{"id": "80f22a0a-d9c8-43eb-bef3-10056befc526", "fitness": 0.2315420439590159, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: Differential Evolution + occasional Lévy jumps + trust-region local search with online step-size adaptation (fast global exploration, focused local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension and budget but kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # make sure population not larger than a fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # internal state for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (careful with budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if we didn't manage to evaluate full population due to tiny budget,\n        # set remaining individuals randomly without evaluation (they won't be used)\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals[valid])\n            # map to absolute index\n            best_indices = np.where(valid)[0]\n            best_idx = best_indices[best_idx]\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # no evals possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight\n        CR_mean = 0.9       # crossover probability\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # limit extreme outliers to avoid numerical blow-up\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        # We'll process population members sequentially; each evaluated candidate consumes one eval.\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # adapt per-generation randomization of F and CR around their means\n            # Using jDE-like simple mutation: sample F_i and CR_i for each target\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # create donor vector\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    # scale by range and dynamic trust_radius\n                    donor = best_x + (step_scale * 0.5 + 0.5 * rng.rand()) * step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                else:\n                    # DE/rand/1-like mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    # adapt F and CR per individual\n                    Fi = np.clip(rng.normal(F_mean, 0.1), 0.1, 1.0)\n                    CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    donor = trial\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # one evaluation\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # move means slightly toward successful parameters\n                    # adjust F_mean and CR_mean toward the Fi/CRi used (approx)\n                    # (we used Fi/CRi only in DE branch; in Lévy, we nudge toward exploration)\n                    # small adaptation:\n                    F_mean = 0.9 * F_mean + 0.1 * (Fi if 'Fi' in locals() else F_mean)\n                    CR_mean = 0.9 * CR_mean + 0.1 * (CRi if 'CRi' in locals() else CR_mean)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # clear local Fi/CRi from scope if set to avoid reuse\n                if 'Fi' in locals():\n                    del Fi\n                if 'CRi' in locals():\n                    del CRi\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            # number of local samples depends on remaining budget and dimension\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample count: small handful scaled by dim and remaining budget\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter += 1\n\n            # if many successes, encourage exploration less; if stagnating increase Lévy jumps\n            if successes > 0:\n                p_levy = max(0.01, p_levy * (0.95 if successes > self.pop_size * 0.2 else 0.98))\n                # move F_mean/CR_mean a bit toward successful exploitation\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.1, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                for j in rng.choice(np.arange(self.pop_size), k, replace=False):\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.232 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11035934011028292, 0.18192003466809592, 0.2900111018232636, 0.3174789567974713, 0.2528554061797754, 0.2657336158689808, 0.23751267935247244, 0.2633178080121562, 0.22374998468513996, 0.17248151209252027]}, "task_prompt": ""}
{"id": "699c03e0-f225-4f1b-96c6-a04899b37c6c", "fitness": 0.3234274083314844, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and an online-adapting trust-region local search — balances fast global search with focused local refinement.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n     - Differential Evolution (DE/rand/1 with per-individual Fi, CRi adapted online)\n     - Occasional Lévy-flight jumps centered on the best for long-range exploration\n     - Trust-region Gaussian local search around the current best with adaptive radius\n     - Online adaptation of F_mean, CR_mean and Lévy probability based on recent successes.\n\n    Usage: instantiate with budget and dim (optional seed), then call with a black-box 'func'\n    that exposes .bounds.lb and .bounds.ub (scalars or arrays). The __call__ returns (f_opt, x_opt)\n    and stores them as self.f_opt, self.x_opt.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size scaled by dimension (kept reasonable)\n        if pop_size is None:\n            base = int(8 + 2 * np.sqrt(max(1, self.dim)))\n            self.pop_size = max(6, min(60, base))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # ensure we can evaluate an initial meaningful population within budget\n        pop_size = min(self.pop_size, max(2, self.budget))  # at least 2\n        self.pop_size = pop_size\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * (ub - lb)\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (stop if budget exhausted)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if some individuals could not be evaluated due to tiny budget, shrink population\n        if not np.isfinite(fvals).any():\n            # nothing evaluated (budget 0), return defaults\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # hyper-parameters (adapt online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of performing a Lévy jump instead of DE\n        step_scale = 0.25   # base scale for Lévy steps (fraction of variable range)\n        trust_radius = 0.15 * range_norm  # initial trust radius in absolute units\n        min_trust = 1e-8\n        max_trust = max(1.0, 2.0 * range_norm)\n\n        stagnation_counter = 0\n        gen = 0\n\n        # memory for adaptation (moving averages)\n        adapt_alpha = 0.1\n\n        # helper: generate Levy-like heavy-tailed step using symmetric Cauchy approx\n        def levy_step():\n            # Mantegna-like heavy tails can be approximated; use standard Cauchy scaled\n            s = rng.standard_cauchy(self.dim)\n            # scale by step_scale and normalize relative to average range\n            s = s * (step_scale * (range_vec / max(range_vec.mean(), 1e-12)))\n            # clip extreme outliers to remain numerically stable\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # For each individual attempt an offspring (stop if budget exhausted)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual Fi and CRi around their means (jDE-like)\n                Fi = np.clip(rng.normal(F_mean, 0.2), 0.05, 0.95)\n                CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # decide exploration mode: Lévy jump centered on best (global exploration) or DE mutation\n                if rng.rand() < p_levy:\n                    # Lévy jump: generate heavy-tailed step, biased by trust radius and random scaling\n                    step = levy_step()\n                    scale = 0.5 * (0.5 + rng.rand())  # moderate random scale\n                    candidate = best_x + scale * step\n                    # add small local gaussian perturbation proportional to trust radius\n                    candidate += rng.normal(0, 1.0, self.dim) * (trust_radius / max(range_norm, 1e-12)) * (rng.rand(self.dim) * 0.5)\n                else:\n                    # DE/rand/1 mutation + binomial crossover\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback: random perturbation if population too small\n                        candidate = pop[i] + Fi * (rng.rand(self.dim) - 0.5) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[rng.randint(0, self.dim)] = True\n                        candidate = np.where(cr_mask, donor, pop[i])\n\n                # project to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # one evaluation (respect budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # nudge parameter means slightly toward successful Fi/CRi\n                    F_mean = (1.0 - adapt_alpha) * F_mean + adapt_alpha * Fi\n                    CR_mean = (1.0 - adapt_alpha) * CR_mean + adapt_alpha * CRi\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful local refinement => shrink trust radius moderately\n                    trust_radius = max(min_trust, trust_radius * 0.9)\n                else:\n                    stagnation_counter += 1\n\n                # stop early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # After one generation, perform a small trust-region local search around best (budget permitting)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample count small, depending on dim and remaining budget\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 8)))\n            improved_locally = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: scale per dimension by a random factor in [0.5,1.0]\n                sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    improved_locally += 1\n                    # successful local step => tighten trust region (focus)\n                    trust_radius = max(min_trust, trust_radius * 0.80)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand trust region a bit to try escape\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n                    stagnation_counter += 1\n\n            # adapt p_levy and means based on success statistics\n            if successes + improved_locally > 0:\n                # reduce chance of Levy if we are making progress\n                p_levy = max(0.01, p_levy * (0.95 if (successes + improved_locally) > (0.2 * self.pop_size) else 0.98))\n                # slowly move F_mean/CR_mean toward exploitation-favoring values\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.7, 0.0, 1.0)\n            else:\n                # stagnation handling: increase Levy probability and slightly increase diversity\n                p_levy = min(0.5, p_levy * 1.06 + 0.005)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n\n            # strong stagnation reset: if no improvement for many steps, re-seed part of population\n            if stagnation_counter > max(80, self.dim * 6):\n                k = max(1, self.pop_size // 2)\n                replace_idx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in replace_idx:\n                    # reinitialize\n                    pop[j] = lb + rng.rand(self.dim) * (ub - lb)\n                    if evals >= self.budget:\n                        break\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius to encourage exploration after reset\n                trust_radius = min(max_trust, trust_radius * 1.6)\n                stagnation_counter = 0\n                # nudge means slightly toward exploration\n                p_levy = min(0.5, p_levy + 0.05)\n                F_mean = max(0.07, F_mean * 0.9)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.323 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15583613211119574, 0.28143031778822025, 0.33175232127405097, 0.4221812737159081, 0.2825575372206187, 0.5073047464577635, 0.2706403653777465, 0.3896183895850407, 0.38360215248654106, 0.20935084729775844]}, "task_prompt": ""}
{"id": "c6e1df10-f41e-452a-81f9-78f5762778b8", "fitness": 0.22442338057065653, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size adaptation for fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=1)\n        f_opt, x_opt = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # population size heuristic if not provided\n        if pop_size is None:\n            ps = max(8, int(8 + 2 * np.sqrt(self.dim)))\n            ps = min(ps, 60)\n            ps = min(ps, max(4, self.budget // 20))\n            self.pop_size = int(max(2, ps))\n        else:\n            self.pop_size = int(max(2, pop_size))\n\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.ndim == 0:\n            return np.full(self.dim, float(b))\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting to dim\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds shape incompatible with dimension\")\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        # protect against zero-range dims\n        range_vec = np.where(range_vec == 0.0, 1.0, range_vec)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate as many as budget allows (initial population)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # find best among evaluated\n        valid_idx = np.isfinite(fvals)\n        if not np.any(valid_idx):\n            # nothing evaluated\n            self.f_opt = np.inf\n            self.x_opt = lb + rng.rand(self.dim) * range_vec\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals[valid_idx])\n        best_indices = np.where(valid_idx)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale multiplier for Levy and trust radius factor\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Lévy-like heavy-tailed step using clipped Cauchy\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # normalize scale to typical magnitude 1\n            m = np.mean(np.abs(s))\n            if m < 1e-12:\n                m = 1.0\n            return (s / m)\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            gen_start_evals = evals\n\n            # per-generation small random perturbation of means for diversity\n            F_mean = np.clip(F_mean + rng.normal(0, 0.01), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + rng.normal(0, 0.02), 0.0, 1.0)\n\n            # iterate population sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # decide whether to do LEVY or DE-based trial\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best (long-range exploration)\n                    s = levy_step()\n                    # scale by current trust radius and base step_scale relative to range\n                    scale = step_scale * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                    trial = best_x + s * scale * range_vec\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1-like mutation; ensure indices not equal to i\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(idxs) < 3:\n                        # fallback to random perturbation\n                        donor = pop[i] + rng.normal(0, 0.1, size=self.dim) * range_vec\n                    else:\n                        r = rng.choice(idxs, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < (CRi if CRi is not None else CR_mean)\n                    # ensure at least one gene from donor\n                    cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds (simple clipping)\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate candidate (consumes one eval)\n                f_candidate = float(func(trial))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = trial.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # nudge means slightly toward successful Fi/CRi\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = trial.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # adapt trust radius modestly if many successes within generation\n                # handled after generation\n\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            remaining = self.budget - evals\n            # trust-region local search around best: sample a few local candidates\n            if remaining > 0:\n                local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma: base trust_radius scaled per-dimension\n                    base = trust_radius / (np.linalg.norm(range_vec) + 1e-12)\n                    sigma = (0.3 + rng.rand(self.dim) * 0.7) * base * step_scale\n                    candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step => shrink trust radius to focus\n                        trust_radius = max(min_trust, trust_radius * 0.85)\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful => expand a bit to escape local traps\n                        trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # Adapt probability of Levy and means according to success rate\n            gen_success_rate = successes / max(1.0, (self.pop_size))\n            if gen_success_rate > 0.2:\n                # good progress; reduce long jumps and slightly tighten trust region\n                p_levy = max(0.01, p_levy * 0.95)\n                trust_radius = max(min_trust, trust_radius * 0.98)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # poor progress; increase chance of long jumps and diversify F\n                p_levy = min(0.5, p_levy * 1.02 + 0.001)\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n\n            # stagnation handling: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(20, 2 * self.dim):\n                k = max(1, self.pop_size // 2)\n                # choose indices to reinitialize (but keep best index)\n                cand_idxs = [j for j in range(self.pop_size) if not np.allclose(pop[j], best_x)]\n                k = min(k, len(cand_idxs))\n                if k > 0:\n                    reinit_idxs = rng.choice(cand_idxs, k, replace=False)\n                    for j in reinit_idxs:\n                        if evals >= self.budget:\n                            break\n                        pop[j] = lb + rng.rand(self.dim) * range_vec\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                        if fvals[j] < best_f:\n                            best_f = fvals[j]\n                            best_x = pop[j].copy()\n                            stagnation_counter = 0\n                # expand trust radius a bit after reset\n                trust_radius = min(max_trust, trust_radius * 1.2)\n                p_levy = min(0.5, p_levy * 1.1)\n                # gentle shake of F_mean/CR_mean\n                F_mean = np.clip(0.6 * F_mean + 0.4 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.6 * CR_mean + 0.4 * 0.9, 0.0, 1.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.224 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1027087573527331, 0.15681966322925467, 0.31708211165981115, 0.40578881890345886, 0.2087102616504839, 0.2923878100687506, 0.2103136490414732, 0.2032537823923246, 0.20259282375899235, 0.14457612764928296]}, "task_prompt": ""}
{"id": "a7988d9c-439c-497b-a1f8-7bfa5bfc9b32", "fitness": 0.2527881257716971, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size adaptation and simple success-based parameter updates.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size scaled with dimension\n        if pop_size is None:\n            ps = int(max(8, min(60, 8 + 2 * np.sqrt(self.dim))))\n            # do not exceed a fraction of budget\n            ps = min(ps, max(4, self.budget // 10))\n            self.pop_size = max(4, ps)\n        else:\n            self.pop_size = max(2, int(pop_size))\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # outputs\n        self.f_opt = None\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # Try to read bounds from func; fallback to [-5,5]\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub =  5.0 * np.ones(self.dim)\n        # ensure shapes\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # ensure arrays length match dim\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n        range_vec = ub - lb\n        # initial trust radius (relative)\n        trust_radius = 0.2 * np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 1.0\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0 + 1e-12\n\n        # initialize population uniformly\n        pop = lb[None, :] + self.rng.random((self.pop_size, self.dim)) * (range_vec[None, :])\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate as many individuals as budget allows\n        n_init = min(self.pop_size, self.budget)\n        for i in range(n_init):\n            x = pop[i].copy()\n            fvals[i] = float(func(x))\n            evals += 1\n            # keep best so far\n        # if population not fully evaluated (tiny budget), remaining fvals stay inf\n\n        # determine current best (only among evaluated)\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n        best_idx = np.argmin(fvals[valid])\n        best_indices = np.nonzero(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        gen = 0\n        stagnation_counter = 0\n        stagnation_limit = max(20, 5 * self.dim)\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy sampling\n        def levy_step():\n            s = self.rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            successful_Fs = []\n            successful_CRs = []\n\n            # iterate through population members for DE operations\n            indices = np.arange(self.pop_size)\n            self.rng.shuffle(indices)\n            for idx in indices:\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi around means\n                Fi = float(np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                if self.rng.random() < p_levy:\n                    # Levy jump centered at best_x scaled by trust radius and range\n                    s = levy_step()\n                    step_scale = 0.5 + 0.5 * self.rng.random()\n                    donor = best_x + step_scale * trust_radius * (s / (np.linalg.norm(s) + 1e-12)) * (range_vec / (np.maximum(range_vec.mean(), 1e-12)))\n                else:\n                    # DE/rand/1 mutation excluding current index\n                    pool = [j for j in range(self.pop_size) if j != idx]\n                    if len(pool) < 3:\n                        # not enough distinct individuals, use random perturbation near current\n                        donor = pop[idx] + Fi * (self.rng.random(self.dim) - 0.5) * range_vec\n                    else:\n                        r1, r2, r3 = self.rng.choice(pool, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cr_mask = self.rng.random(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[self.rng.integers(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[idx])\n                # ensure bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate (if budget allows)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[idx]:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    successful_Fs.append(Fi)\n                    successful_CRs.append(CRi)\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # adapt means based on successful parameters (simple moving average)\n            if len(successful_Fs) > 0:\n                mean_F_succ = float(np.mean(successful_Fs))\n                F_mean = 0.9 * F_mean + 0.1 * mean_F_succ\n            else:\n                # slowly decay toward diversification\n                F_mean = 0.995 * F_mean + 0.005 * 0.5\n\n            if len(successful_CRs) > 0:\n                mean_CR_succ = float(np.mean(successful_CRs))\n                CR_mean = 0.9 * CR_mean + 0.1 * mean_CR_succ\n            else:\n                CR_mean = 0.995 * CR_mean + 0.005 * 0.5\n\n            # trust-region local search around best_x\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic Gaussian noise scaled by trust_radius and relative ranges\n                per_dim_scale = (0.5 + self.rng.random(self.dim) * 0.5) * (trust_radius / (np.maximum(np.linalg.norm(range_vec), 1e-12)))\n                noise = self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_scale * (range_vec / (np.maximum(range_vec.mean(), 1e-12)))\n                candidate = np.clip(best_x + noise, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius (focus)\n                    trust_radius = max(min_trust, trust_radius * 0.8)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand slightly to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n                if evals >= self.budget:\n                    break\n\n            # adapt probability of Levy jumps based on stagnation\n            if successes > 0:\n                # reward exploitation: reduce long-jump frequency slightly\n                p_levy = max(0.01, p_levy * 0.98)\n            else:\n                # if stagnating, increase chance of long jumps\n                p_levy = min(0.6, p_levy * 1.06 + 0.005)\n\n            # strong stagnation reset: if no improvement for a while, re-seed half population\n            if stagnation_counter >= stagnation_limit:\n                k = max(1, self.pop_size // 2)\n                reinit_idx = self.rng.choice(np.arange(self.pop_size), size=k, replace=False)\n                for j in reinit_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # update best if needed\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # after reset, expand trust radius modestly\n                trust_radius = min(max_trust, max(trust_radius * 1.5, 1e-8))\n                stagnation_counter = 0  # reset stagnation after reseed\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.253 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12945105423210113, 0.20980120197538665, 0.34888641955586996, 0.3866199645326933, 0.23099389062281162, 0.27958148051640863, 0.23534665689764778, 0.3466691628517191, 0.1808388113800815, 0.17969261515225177]}, "task_prompt": ""}
{"id": "0eb40020-368e-465b-8753-c54c63ef6a12", "fitness": 0.5924091411712984, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy (Cauchy) long jumps, and a trust-region Gaussian local search with online adaptation of F/CR and jump probability.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (using Cauchy for heavy tails) jumps for\n    long-range exploration, and a trust-region Gaussian local search\n    around the current best. Parameters (F, CR, p_levy) are adapted\n    online based on success history.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # default population size scaled with dimension, user can override\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like; produce float array of length self.dim\n        if b is None:\n            return np.full(self.dim, -5.0)  # fallback\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.copy().astype(float)\n        # try to broadcast if possible\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to dimension {}\".format(self.dim))\n\n    def __call__(self, func):\n        # get bounds (if available), default to [-5,5] per problem statement\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure valid bounds\n        range_vec = ub - lb\n        range_vec[range_vec == 0.0] = 1e-12\n\n        # initialize population in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                f = float(func(pop[i]))\n            except Exception:\n                f = np.inf\n            fvals[i] = f\n            evals += 1\n\n        # determine initial best\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals)\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # if no evaluations possible (budget==0), return defaults\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # Algorithm hyper-parameters (adaptive)\n        F_mean = 0.5       # scale factor mean\n        CR_mean = 0.9      # crossover probability mean\n        p_levy = 0.08      # probability of a Lévy jump\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # absolute radius\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0 + 1e-12\n\n        gen = 0\n        stagnation_counter = 0\n        successes_in_gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step(scale=1.0):\n            # standard Cauchy vector, clipped to avoid numerical blow-ups\n            s = self.rng.standard_cauchy(self.dim)\n            s = np.clip(s, -10.0, 10.0)\n            # normalize and scale to trust region\n            norm = np.linalg.norm(s)\n            if norm == 0:\n                s = self.rng.normal(0, 1, self.dim)\n                norm = np.linalg.norm(s)\n            s = s / (norm + 1e-12)\n            return s * scale\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes_in_gen = 0\n            success_F = []\n            success_CR = []\n\n            # Precompute indices for DE selection convenience\n            idxs = np.arange(self.pop_size)\n\n            # For each individual (target) attempt one offspring\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Occasionally use heavy-tailed jump centered at best\n                if self.rng.rand() < p_levy:\n                    # Levy / Cauchy jump with scale proportional to trust_radius\n                    step = levy_step(scale=trust_radius)\n                    candidate = best_x + step\n                    # project to bounds\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    # Evaluate candidate\n                    f_candidate = float(func(candidate)) if evals < self.budget else np.inf\n                    evals += 1\n\n                    # Selection against worst individual (or replace i if better)\n                    # We'll replace the worst if candidate is good, else ignore\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        successes_in_gen += 1\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                    continue  # move to next individual\n\n                # DE/current-to-rand/1-like with per-individual Fi, CRi\n                # ensure we pick 3 distinct indices distinct from i when possible\n                available = idxs[idxs != i]\n                if available.size >= 3:\n                    r = self.rng.choice(available, 3, replace=False)\n                    r1, r2, r3 = r[0], r[1], r[2]\n                else:\n                    # fallback to random picks with replacement if pop too small\n                    r1 = self.rng.randint(0, self.pop_size)\n                    r2 = self.rng.randint(0, self.pop_size)\n                    r3 = self.rng.randint(0, self.pop_size)\n                    # ensure not equal to i\n                    if r1 == i and self.pop_size > 1:\n                        r1 = (r1 + 1) % self.pop_size\n\n                # sample Fi and CRi around their means (bounded)\n                Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 1.0)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # donor: rand/1 + small attraction to best (encourages directed search)\n                donor = pop[r1] + Fi * (pop[r2] - pop[r3]) + 0.2 * Fi * (best_x - pop[i])\n                # binomial crossover\n                cross = self.rng.rand(self.dim) < CRi\n                if not np.any(cross):\n                    cross[self.rng.randint(0, self.dim)] = True\n                candidate = np.where(cross, donor, pop[i])\n                # projection\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # Evaluate candidate\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: replace if improved\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes_in_gen += 1\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n                    # move the running means slightly toward successful values\n                    # We'll aggregate after generation\n                # Update global best if needed\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # After processing population, adapt F_mean and CR_mean using simple smoothing\n            if success_F:\n                # Lehmer-like weighting: give larger F more weight\n                success_F = np.array(success_F)\n                if success_F.sum() > 0:\n                    F_mean = 0.9 * F_mean + 0.1 * (np.sum(success_F**2) / (np.sum(success_F) + 1e-12))\n            if success_CR:\n                CR_mean = 0.9 * CR_mean + 0.1 * np.mean(success_CR)\n\n            # Trust-region local search around current best: a few Gaussian samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # local samples: small handful depending on dim and remaining budget\n            local_samples = min(max(1, self.dim // 4), remaining, 12)\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian step scaled by trust_radius\n                # step std per-dimension scaled so total expected step length ~ trust_radius\n                step_std = (trust_radius / np.sqrt(self.dim)) * (0.5 + self.rng.rand(self.dim) * 0.5)\n                step = self.rng.normal(0.0, step_std, size=self.dim)\n                candidate = best_x + step\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # Adjust trust radius based on local success\n            if local_success > 0 or successes_in_gen > max(1, self.pop_size // 10):\n                # successes -> focus more (shrink)\n                trust_radius = max(min_trust, trust_radius * 0.8)\n            else:\n                # no improvements -> widen a bit to escape\n                trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # Adapt p_levy: if many successes, reduce global jumps; if stagnating increase it\n            if successes_in_gen + local_success > max(1, self.pop_size // 8):\n                p_levy = max(0.005, p_levy * 0.96)\n            else:\n                p_levy = min(0.5, p_levy * 1.02)\n\n            # slight decay of CR_mean to maintain exploration\n            CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # Strong stagnation reset: re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5) and evals < self.budget:\n                # reinitialize half of population (or at least 1)\n                k = max(1, self.pop_size // 2)\n                reinit_idxs = self.rng.choice(np.arange(self.pop_size), size=k, replace=False)\n                for j in reinit_idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius slightly to encourage escape\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0  # reset\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.592 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12263448359960105, 0.20194408397538888, 0.5911029319059831, 0.9746230498884203, 0.8538950622017794, 0.9608384026171707, 0.3781352560321143, 0.7373150665588082, 0.9019707977203539, 0.2016322772133634]}, "task_prompt": ""}
{"id": "e16260b1-6531-4438-9939-2474935e349e", "fitness": 0.1892993013030811, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer: Differential Evolution with per-individual self-adaptation, occasional Lévy-style long jumps for global exploration, and a trust-region Gaussian local search around the current best with online step-size adaptation and stagnation resets.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Use:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=123)\n        f_opt, x_opt = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        # population size: scale with dim but respect budget\n        if pop_size is None:\n            # base on dimension but don't make population too large relative to budget\n            pop_base = max(6, 4 * self.dim)\n            pop_cap = max(4, int(max(4, self.budget // 20)))\n            self.pop_size = int(min(pop_base, pop_cap))\n        else:\n            self.pop_size = int(pop_size)\n        # internal best results (updated after __call__)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Random generator\n        rng = np.random.default_rng(self.seed)\n\n        # bounds (handle broadcasting)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # default bounds for BBOB tasks\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim,  5.0, dtype=float)\n\n        if lb.size == 1:\n            lb = np.broadcast_to(lb, (self.dim,))\n        if ub.size == 1:\n            ub = np.broadcast_to(ub, (self.dim,))\n\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n        range_vec = ub - lb\n        range_mean = float(np.maximum(range_vec.mean(), 1e-12))\n\n        # Ensure population size is feasible (at least 4)\n        self.pop_size = max(4, min(self.pop_size, self.budget))  # cannot be more than budget realistically\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            xi = pop[i]\n            try:\n                fv = float(func(xi))\n            except Exception:\n                # protect against bad evaluations; treat as high cost\n                fv = np.inf\n            fvals[i] = fv\n            evals += 1\n\n        # If no evaluations possible, return defaults\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # identify best\n        valid_idx = np.argmin(fvals)\n        best_x = pop[valid_idx].copy()\n        best_f = float(fvals[valid_idx])\n\n        # DE-related state: per-individual Fi and CRi for self-adaptation\n        Fi_arr = np.clip(rng.normal(0.6, 0.15, size=self.pop_size), 0.05, 1.0)\n        CR_arr = np.clip(rng.random(self.pop_size), 0.0, 1.0)\n        F_mean = 0.6\n        CR_mean = 0.9\n\n        # exploration parameters\n        step_scale = 0.25  # base scale for levy/trust radius relative to range_mean\n        trust_radius = step_scale * range_mean  # initial trust radius (std dev for local Gaussian)\n        min_trust = 1e-6 * range_mean\n        max_trust = 2.0 * range_mean\n\n        p_levy = 0.05  # initial probability of Levy jump\n        stagnation_counter = 0\n        max_stagnation_before_reset = max(50, self.dim * 5)\n\n        # bookkeeping to adapt p_levy and means\n        last_best_f = best_f\n\n        # helper: Levy-like heavy-tailed step (Cauchy-based, clipped)\n        def levy_step(dim):\n            s = rng.standard_cauchy(size=dim)\n            # clip to avoid extreme numerical blow-ups while preserving heavy tails\n            s = np.clip(s, -15.0, 15.0)\n            return s\n\n        # Main loop: iterate until budget exhausted\n        while evals < self.budget:\n            # Per-generation bookkeeping\n            gen_successes = 0\n            gen_improvement = False\n            gen_evals_before = evals\n            prev_best_f = best_f\n\n            # process each population member sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi for this trial (jDE-style: sample around mean)\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                # decide exploration via Levy jump or DE mutation (probabilities adapt)\n                if rng.random() < p_levy:\n                    # Lévy-inspired long-range jump centered at current best\n                    step = levy_step(self.dim)\n                    # scale step by trust_radius and problem scale\n                    scale = (0.3 + rng.random() * 1.2) * trust_radius\n                    delta = step * (scale * (range_vec / range_mean))\n                    candidate = best_x + delta\n                    candidate = np.clip(candidate, lb, ub)\n\n                    # evaluate\n                    try:\n                        f_candidate = float(func(candidate))\n                    except Exception:\n                        f_candidate = np.inf\n                    evals += 1\n\n                    # greedy replacement for the current target i\n                    if f_candidate < fvals[i]:\n                        pop[i] = candidate\n                        fvals[i] = f_candidate\n                        gen_successes += 1\n                        # nudge means toward exploration if Levy was successful (encourage occasional jumps)\n                        F_mean = np.clip(0.98 * F_mean + 0.02 * 0.6, 0.05, 0.99)\n                        CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.5, 0.0, 1.0)\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        gen_improvement = True\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n\n                else:\n                    # Differential Evolution: rand/1/bin with small best-bias\n                    idxs = np.arange(self.pop_size)\n                    others = idxs[idxs != i]\n                    if others.size < 3:\n                        # safety fallback: random perturbation\n                        donor = pop[i] + Fi * rng.normal(0, 1.0, size=self.dim) * (range_vec / range_mean)\n                    else:\n                        r1, r2, r3 = rng.choice(others, size=3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # small bias toward best for exploitation\n                        donor += 0.05 * rng.random() * (best_x - pop[i])\n\n                    # binomial crossover\n                    jrand = rng.integers(self.dim)\n                    mask = rng.random(self.dim) < CRi\n                    mask[jrand] = True\n                    candidate = np.where(mask, donor, pop[i])\n                    candidate = np.clip(candidate, lb, ub)\n\n                    # evaluate\n                    try:\n                        f_candidate = float(func(candidate))\n                    except Exception:\n                        f_candidate = np.inf\n                    evals += 1\n\n                    # selection\n                    if f_candidate < fvals[i]:\n                        pop[i] = candidate\n                        fvals[i] = f_candidate\n                        gen_successes += 1\n                        Fi_arr[i] = Fi\n                        CR_arr[i] = CRi\n                        # move global means slightly toward successful Fi/CRi\n                        F_mean = np.clip(0.9 * F_mean + 0.1 * Fi, 0.05, 0.99)\n                        CR_mean = np.clip(0.9 * CR_mean + 0.1 * CRi, 0.0, 1.0)\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        gen_improvement = True\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n\n            # End of generation: quick local trust-region search around the best\n            remaining = self.budget - evals\n            # sample a few local candidates depending on dimension and remaining budget\n            local_samples = int(min(max(1, self.dim // 2), remaining, 8 + self.dim // 2))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic Gaussian noise scaled by trust_radius and problem scale\n                noise = rng.normal(0.0, 1.0, size=self.dim) * (trust_radius * (range_vec / range_mean))\n                candidate = np.clip(best_x + noise, lb, ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    gen_improvement = True\n                    # successful local step -> shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful -> slightly expand trust to encourage escape\n                    trust_radius = min(max_trust, trust_radius * 1.03)\n\n            # Adapt exploration probability and global parameters based on generation performance\n            if gen_successes > max(1, int(0.15 * self.pop_size)):\n                # many successes -> reduce need for Levy jumps, focus on exploitation\n                p_levy = max(0.01, p_levy * 0.95)\n                trust_radius = max(min_trust, trust_radius * 0.98)\n            else:\n                # few successes -> increase chance of long jumps and gently increase diversity\n                p_levy = min(0.5, p_levy * 1.06 + 0.005)\n                trust_radius = min(max_trust, trust_radius * 1.02)\n\n            # small conservative drift of means to stable values (avoid runaway)\n            F_mean = np.clip(0.98 * F_mean + 0.02 * 0.6, 0.05, 0.99)\n            CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.9, 0.0, 1.0)\n\n            # if the best hasn't improved this generation, increment stagnation\n            if best_f < last_best_f:\n                last_best_f = best_f\n                # reset stagnation counter if we had improvement\n                stagnation_counter = 0\n            else:\n                # accumulate stagnation\n                stagnation_counter += 1\n\n            # Strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max_stagnation_before_reset and evals < self.budget:\n                k = max(1, self.pop_size // 2)\n                for j in rng.choice(np.arange(self.pop_size), k, replace=False):\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    try:\n                        f_j = float(func(pop[j]))\n                    except Exception:\n                        f_j = np.inf\n                    fvals[j] = f_j\n                    evals += 1\n                    if f_j < best_f:\n                        best_f = f_j\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # after reset, enlarge trust radius to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # slightly increase p_levy to explore after reset\n                p_levy = min(0.5, p_levy + 0.05)\n                stagnation_counter = 0\n\n            # update overall best record\n            self.f_opt = float(best_f)\n            self.x_opt = best_x.copy()\n\n            # Safety: ensure trust_radius remains in bounds\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # if budget exhausted, break\n            if evals >= self.budget:\n                break\n\n        # final assignment\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.189 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10592332471497157, 0.15036120890777016, 0.24130435258262628, 0.1865798478464321, 0.21168754542943236, 0.21681548907681858, 0.20719419722829446, 0.26339592986894433, 0.1800361999198874, 0.12969491745563355]}, "task_prompt": ""}
{"id": "bdb22d96-dd70-479d-851a-4a34ec459ecb", "fitness": 0.24312856860705132, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like adaptation, occasional Lévy (Cauchy) long jumps for exploration, and a trust-region Gaussian local search around the best; online adaptation steers exploration vs. exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line idea: use adaptive DE for global recombination, occasional\n    heavy-tailed (Cauchy) jumps for breaking out of basins, and a\n    trust-region Gaussian local search to refine the best solutions.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        # sensible default population size: scale with dimension but capped by budget\n        if pop_size is None:\n            default = max(4, 8 + 2 * self.dim)\n            self.pop_size = int(min(default, max(4, self.budget // 8)))\n        else:\n            self.pop_size = int(max(2, pop_size))\n\n        # outputs (filled after run)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size != self.dim:\n            # try to broadcast\n            try:\n                return np.broadcast_to(b, (self.dim,)).astype(float)\n            except Exception:\n                raise ValueError(\"Bounds must be scalar or length == dim\")\n        return b\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        # try to get bounds from func if available, otherwise default to [-5,5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        range_vec = ub - lb\n        if np.any(range_vec <= 0):\n            raise ValueError(\"Upper bounds must be greater than lower bounds\")\n\n        # Helper: Levy-like heavy-tailed step using standard Cauchy\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            # clip extreme outliers to avoid numerical blow-up\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # initialize population\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n        evals = 0\n\n        # Evaluate as many initial individuals as budget allows\n        to_eval = min(self.pop_size, self.budget)\n        for i in range(to_eval):\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n        # the rest remain randomly initialized but unevaluated (will be used later)\n\n        # If no evaluations possible return defaults\n        if evals == 0:\n            # store a random solution\n            self.f_opt = np.inf\n            self.x_opt = pop[0].copy()\n            return self.f_opt, self.x_opt\n\n        # initial best\n        best_idx = int(np.argmin(fvals[:to_eval]))\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx].item()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6   # mean mutation factor\n        CR_mean = 0.5  # mean crossover rate\n        p_levy = 0.08  # base probability of doing a Levy jump instead of DE\n        trust_radius = 0.2 * np.linalg.norm(range_vec) / np.sqrt(self.dim)  # initial trust radius (absolute)\n        min_trust = 1e-8\n\n        stagnation_counter = 0\n        gen = 0\n\n        # Main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # Per-generation small perturbation of means to encourage exploration\n            F_mean = np.clip(F_mean * (1 + rng.normal(0, 0.02)), 0.05, 1.0)\n            CR_mean = np.clip(CR_mean + rng.normal(0, 0.02), 0.0, 1.0)\n\n            # Process each individual sequentially (each candidate evaluation consumes budget)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual Fi and CRi (jDE-like simple adaptation)\n                Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                do_levy = rng.random() < p_levy\n\n                if do_levy:\n                    # Lévy-like jump centered on best to promote long-range exploration\n                    step = levy_step()\n                    # scale step by trust_radius and coordinate-wise range\n                    step_scale = trust_radius * (0.5 + rng.random())\n                    donor = best_x + step_scale * step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                else:\n                    # DE/rand/1 mutation (classic)\n                    # select three distinct indices different from i\n                    idxs = np.arange(self.pop_size, dtype=int)\n                    # ensure pop_size >= 4; if small, allow replacement but try to avoid i\n                    if self.pop_size >= 4:\n                        choices = rng.choice(idxs[idxs != i], size=3, replace=False)\n                    else:\n                        # fallback: choose with replacement but not the target index if possible\n                        choices = rng.choice(idxs, size=3, replace=True)\n                        if i in choices and self.pop_size > 1:\n                            # try to correct\n                            for k in range(3):\n                                if choices[k] == i:\n                                    alt = rng.choice(idxs[idxs != i])\n                                    choices[k] = alt\n                    r1, r2, r3 = int(choices[0]), int(choices[1]), int(choices[2])\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # occasionally mix in best influence (small)\n                    if rng.random() < 0.08:\n                        donor = 0.8 * donor + 0.2 * best_x\n\n                # binomial crossover\n                cr_mask = rng.random(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[rng.integers(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate (only if budget allows)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    # successful replacement\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n\n                    # adapt means slightly toward successful Fi/CRi (only when used DE branch)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                    # if Levy branch, slightly lower p_levy (it found improvement)\n                    if do_levy:\n                        p_levy = max(0.001, p_levy * 0.95)\n                else:\n                    # unsuccessful: mild diversification nudges\n                    F_mean = np.clip(F_mean * 0.995, 0.05, 1.0)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                # if we found improvement but didn't replace this index, still reset stagnation\n                elif f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # Trust-region local search around best: a few Gaussian candidates, focused exploitation\n            # number of local samples depends on remaining budget and dimension\n            n_local = min(remaining, max(1, int(np.ceil(self.dim / 2))))\n            local_success = 0\n            for t in range(n_local):\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                local_sigma = (0.4 + rng.random(self.dim) * 0.6) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, local_sigma)\n                candidate = np.clip(candidate, lb, ub)\n\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    # successful local step => shrink trust radius to focus\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    local_success += 1\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand slightly to escape local traps\n                    trust_radius = min(10.0 * np.linalg.norm(range_vec), trust_radius * 1.08)\n\n            # Adjust exploration probability and CR_mean based on successes\n            if successes + local_success > max(1, int(0.15 * self.pop_size)):\n                # good generation: exploit more (reduce Levy frequency), slightly tighten CR\n                p_levy = max(0.001, p_levy * 0.92)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                stagnation_counter = 0\n            else:\n                # stagnating generation: increase chance of long jumps and diversify F\n                p_levy = min(0.9, p_levy * 1.08 + 0.01)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n                stagnation_counter += 1\n\n            # strong stagnation reset: if no improvement for many generations, re-seed part of population\n            if stagnation_counter >= max(8, int(6 + np.sqrt(self.dim))):\n                k = max(1, self.pop_size // 2)\n                # choose indices to reinitialize (avoid destroying the best if it's in the population)\n                idxs = np.arange(self.pop_size)\n                # avoid reinitializing index of best if it exists in pop; try to find an identical row\n                # simple approach: don't touch the global best (if present)\n                # find an index with exact (or very close) match to best_x\n                same_idx = None\n                for ii in range(self.pop_size):\n                    if np.allclose(pop[ii], best_x, atol=1e-12):\n                        same_idx = ii\n                        break\n                if same_idx is not None:\n                    candidates = idxs[idxs != same_idx]\n                else:\n                    candidates = idxs\n                reinit = rng.choice(candidates, size=min(k, candidates.size), replace=False)\n                for j in reinit:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                # slightly enlarge trust radius after reset to encourage exploration\n                trust_radius = min(10.0 * np.linalg.norm(range_vec), trust_radius * 1.5)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.243 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20710286827285618, 0.16270056761982876, 0.2774814830929876, 0.33971213355990326, 0.27787446071237976, 0.26142538844207464, 0.2808941931720843, 0.18145519451561443, 0.27160959380239524, 0.17102980288038905]}, "task_prompt": ""}
{"id": "0c58c4ae-94fc-4bdf-adad-adcbcb0087c0", "fitness": 0.22686067325781067, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: differential evolution with jDE-style adaptation, occasional Lévy (Cauchy) long jumps, and an adaptive trust-region local search around the best solution.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (DE/rand/1 style) with simple jDE-like online adaptation of F and CR,\n      - Occasional Lévy-like (Cauchy) long jumps centered on the current best for global exploration,\n      - Trust-region local search around current best using Gaussian sampling with adaptive radius,\n      - Stagnation detection and partial reseeding.\n    Designed for continuous bounded problems (defaults to [-5,5] if bounds are not provided by func).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population size scaled with dimension but not larger than a fraction of budget\n        if pop_size is None:\n            # basic heuristic: between 6 and 18 individuals, scaled with dimension\n            guess = max(6, min(18 + int(0.6 * dim), max(6, budget // max(10, dim))))\n            self.pop_size = min(max(4, guess), max(4, int(budget // 4)))\n        else:\n            self.pop_size = int(pop_size)\n\n        # placeholders for results\n        self.x_opt = None\n        self.f_opt = float(\"inf\")\n\n    def _get_bounds(self, func):\n        # Try to obtain bounds from func, else use default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast if scalar bounds provided\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n        return lb, ub\n\n    def __call__(self, func):\n        rng = self.rng\n        budget = int(self.budget)\n        dim = self.dim\n        pop_size = max(4, int(self.pop_size))\n\n        if budget <= 0:\n            # nothing to do\n            self.f_opt = float(\"inf\")\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        lb, ub = self._get_bounds(func)\n        # defensive shape\n        lb = lb.reshape((dim,))\n        ub = ub.reshape((dim,))\n        range_vec = ub - lb\n        if np.any(range_vec <= 0):\n            raise ValueError(\"Upper bounds must be strictly greater than lower bounds per dimension.\")\n\n        # initial hyper-parameters for adaptation\n        F_mean = 0.6\n        CR_mean = 0.3\n        p_levy = 0.08\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius in absolute terms\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        # state\n        pop = rng.uniform(lb, ub, size=(pop_size, dim))\n        fvals = np.full(pop_size, np.inf, dtype=float)\n        evals = 0\n\n        # Evaluate initial population until budget exhausted or all evaluated\n        to_eval = min(pop_size, budget - evals)\n        for i in range(to_eval):\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n        # remaining individuals keep fvals=inf\n\n        # initialize best\n        best_idx = int(np.nanargmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy() if np.isfinite(best_f) else pop[0].copy()\n        if np.isfinite(best_f):\n            self.f_opt, self.x_opt = best_f, best_x.copy()\n        else:\n            # no evaluated individual yet (tiny budget)\n            self.f_opt, self.x_opt = float(\"inf\"), None\n\n        # helper: Lévy-like heavy tail using Cauchy\n        def levy_step():\n            # standard Cauchy may produce extreme outliers; clip scale\n            s = rng.standard_cauchy(size=dim)\n            s = np.clip(s, -30.0, 30.0)\n            # scale relative to the problem range\n            step = s * (0.15 * (range_vec + 1e-12))\n            return step\n\n        # stagnation monitoring\n        best_since = 0  # evaluations since last improvement\n        gen = 0\n        # main loop: iterate individuals until budget exhausted\n        while evals < budget:\n            gen += 1\n            successes = 0\n\n            # Per-generation ephemeral parameters: adapt F_i and CR_i around their means with jitter\n            # Process population sequentially\n            order = rng.permutation(pop_size)\n            for idx in order:\n                if evals >= budget:\n                    break\n\n                # select three distinct indices different from idx\n                picks = np.setdiff1d(np.arange(pop_size), idx)\n                if picks.size < 3:\n                    # degenerate small population, skip iteration\n                    continue\n                r = rng.choice(picks, size=3, replace=False)\n                r1, r2, r3 = r[0], r[1], r[2]\n\n                # sample Fi and CRi (jDE-like)\n                Fi = rng.normal(F_mean, 0.12)\n                Fi = float(np.clip(Fi, 0.05, 0.99))\n                CRi = rng.normal(CR_mean, 0.12)\n                CRi = float(np.clip(CRi, 0.0, 1.0))\n\n                # create donor\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on current best (exploration)\n                    donor = best_x + levy_step()\n                    # nudging donor by a rand difference to keep some DE flavor\n                    donor += Fi * (pop[r2] - pop[r3]) * rng.rand()\n                else:\n                    # classical DE/rand/1 mutation\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # crossover (binomial)\n                cr_mask = rng.rand(dim) < CRi\n                # ensure at least one parameter from donor\n                if not np.any(cr_mask):\n                    cr_mask[rng.randint(dim)] = True\n                trial = np.where(cr_mask, donor, pop[idx])\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate candidate\n                f_candidate = float(func(trial))\n                evals += 1\n\n                # selection: greedy\n                if f_candidate <= fvals[idx]:\n                    # accept\n                    pop[idx] = trial\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    # nudge means toward successful params\n                    # small learning rate\n                    F_mean = np.clip(0.9 * F_mean + 0.1 * Fi, 0.05, 0.99)\n                    CR_mean = np.clip(0.9 * CR_mean + 0.1 * CRi, 0.0, 1.0)\n                    # if improved global best, update\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = trial.copy()\n                        best_idx = idx\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        best_since = 0\n                    else:\n                        best_since += 1\n                else:\n                    # unsuccessful: small drift of means toward exploration\n                    F_mean = np.clip(0.995 * F_mean + 0.005 * 0.3, 0.05, 0.99)\n                    CR_mean = np.clip(0.995 * CR_mean + 0.005 * 0.1, 0.0, 1.0)\n                    best_since += 1\n\n                # break early if budget exhausted\n                if evals >= budget:\n                    break\n\n            # End of generation: trust-region local search around best\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: small fraction depending on dim and remaining budget\n            n_local = min(max(1, dim // 2), remaining, max(1, pop_size // 4))\n            improved_local = False\n            for _ in range(n_local):\n                # anisotropic gaussian perturbation scaled by trust_radius and problem range\n                # per-dim scaling allows exploring narrow valleys\n                sigma = (rng.rand(dim) * 0.6 + 0.4) * (trust_radius / (np.sqrt(dim) + 1e-12))\n                candidate = best_x + rng.normal(scale=sigma) * (range_vec + 1e-12)\n                # project\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    improved_local = True\n                    best_since = 0\n                else:\n                    best_since += 1\n                if evals >= budget:\n                    break\n\n            # adjust trust radius: shrink when local improvements, expand on failure\n            if improved_local:\n                trust_radius = max(1e-8, trust_radius * 0.85)\n            else:\n                trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # adaptation of p_levy depending on successes / stagnation\n            if successes > max(1, pop_size * 0.12):\n                # many successes => focus more on exploitation\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                # few successes => encourage exploration\n                p_levy = min(0.4, p_levy * 1.03)\n\n            # If strong stagnation detected, reseed a fraction of population\n            if best_since > max(50, pop_size * 6):\n                # reinitialize half of population randomly\n                k = max(1, pop_size // 2)\n                replace_idx = rng.choice(np.arange(pop_size), size=k, replace=False)\n                for j in replace_idx:\n                    pop[j] = rng.uniform(lb, ub)\n                    # lazily evaluate a subset if budget allows\n                    if evals < budget:\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                    else:\n                        fvals[j] = np.inf\n                # enlarge trust radius to escape trap\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # increase Lévy probability temporarily\n                p_levy = min(0.5, p_levy * 1.5)\n                best_since = 0  # reset stagnation counter\n\n            # safety: update global best from fvals if needed\n            finite_mask = np.isfinite(fvals)\n            if np.any(finite_mask):\n                idx_min = int(np.nanargmin(fvals))\n                if fvals[idx_min] < best_f:\n                    best_f = float(fvals[idx_min])\n                    best_x = pop[idx_min].copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n            # if remaining budget is tiny, do a few random restarts around best to try small improvements\n            if (budget - evals) <= max(2, dim // 4):\n                # quick micro-explorations\n                micro = min(budget - evals, max(1, dim // 4))\n                for _ in range(micro):\n                    if evals >= budget:\n                        break\n                    candidate = best_x + rng.normal(scale=0.02) * range_vec\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        best_since = 0\n\n        # final results\n        # ensure x_opt is set (if never evaluated due to tiny budget, set to a random point)\n        if self.x_opt is None:\n            self.x_opt = rng.uniform(lb, ub)\n            try:\n                self.f_opt = float(func(self.x_opt))\n            except Exception:\n                self.f_opt = float(\"inf\")\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.227 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10866348264002967, 0.16337854737769364, 0.31568147955543124, 0.23404990385080227, 0.23763566150407323, 0.44221834222002365, 0.21809121642780327, 0.20792846707508406, 0.19169972053589213, 0.14925991139127393]}, "task_prompt": ""}
{"id": "e25f5a13-4e19-498a-b5b4-2149475cf030", "fitness": 0.2934076683128118, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining jDE-style Differential Evolution, occasional Lévy (Cauchy) long jumps, and a trust-region Gaussian local search with online parameter adaptation and partial population reseeding.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (modeled with Cauchy) long jumps, and a\n    trust-region local search around the current best. Online adaptation\n    of F and CR is inspired by jDE / Success-History ideas. Periodic\n    partial reseeding combats strong stagnation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # sensible default population size scaled with dimension but not too large\n        if pop_size is None:\n            # base: 4*dim to 10*dim but limited by budget\n            self.pop_size = int(np.clip(6 * self.dim, 6, max(6, int(0.12 * self.budget))))\n        else:\n            self.pop_size = int(pop_size)\n\n        # don't allow population greater than budget (must evaluate at least once)\n        self.pop_size = max(1, min(self.pop_size, max(1, self.budget)))\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # fetch bounds if available, else default [-5,5]\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure bounds shape\n        if lb.shape == () or lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.shape == () or ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # domain info\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        remaining = lambda: self.budget - evals\n\n        # initial evaluation of as many individuals as budget permits (at least 1)\n        initial_evals = min(self.pop_size, max(1, self.budget))\n        for i in range(initial_evals):\n            if evals >= self.budget:\n                break\n            f = float(func(pop[i]))\n            fvals[i] = f\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = pop[i].copy()\n\n        # If budget exhausted during initialization\n        if evals >= self.budget:\n            return self.f_opt, self.x_opt\n\n        # If we couldn't evaluate entire population, shrink population to evaluated part\n        if initial_evals < self.pop_size:\n            pop = pop[:initial_evals].copy()\n            fvals = fvals[:initial_evals].copy()\n            self.pop_size = initial_evals\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.5\n        CR_mean = 0.9\n        p_levy = 0.08  # chance of long jump per candidate\n        tau_F = 0.1\n        tau_CR = 0.1\n\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        min_trust = 1e-6\n        max_trust = range_norm * 0.5\n\n        no_improve_count = 0\n        stagnation_reset_threshold = max(30, 8 * self.dim)\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # draw Cauchy (standard), shape dim, clip extremes to prevent numerical blow-up\n            step = self.rng.standard_cauchy(self.dim)\n            # stabilize scale\n            step = np.clip(step, -1e2, 1e2)\n            # normalize median absolute deviation and scale by 1.0\n            med = np.median(np.abs(step)) + 1e-12\n            step = step / med\n            # limit max magnitude\n            step = np.clip(step, -50, 50)\n            return step\n\n        # main optimization loop: iterate generations until budget exhausted\n        generation = 0\n        while evals < self.budget:\n            generation += 1\n            # per-generation containers for successful parameters\n            succ_F = []\n            succ_CR = []\n            successes = 0\n\n            # shuffle processing order to avoid bias\n            idx_order = np.arange(self.pop_size)\n            self.rng.shuffle(idx_order)\n\n            for idx in idx_order:\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual F and CR (jDE-like)\n                if self.rng.random() < tau_F:\n                    Fi = 0.1 + self.rng.random() * 0.9\n                else:\n                    Fi = F_mean\n                if self.rng.random() < tau_CR:\n                    CRi = self.rng.random()\n                else:\n                    CRi = CR_mean\n\n                # choose mutation strategy: occasional Lévy centered near best, else classical DE/rand/1\n                if self.rng.random() < p_levy:\n                    # exploration: levy jump from current best with some trust scaling\n                    perturb = levy_step()\n                    # scale by trust radius and problem range\n                    scale = trust_radius / (range_norm + 1e-12)\n                    donor = (self.x_opt if self.x_opt is not None else pop[idx]) + (perturb * scale) * range_vec\n                else:\n                    # DE/rand/1: r0 + F*(r1-r2)\n                    # ensure three distinct indices not including idx\n                    candidates = [j for j in range(self.pop_size) if j != idx]\n                    if len(candidates) < 3:\n                        # fallback to random uniform if too small pop\n                        donor = lb + self.rng.random(self.dim) * range_vec\n                    else:\n                        r = self.rng.choice(candidates, size=3, replace=False)\n                        donor = pop[r[0]] + Fi * (pop[r[1]] - pop[r[2]])\n\n                # binomial crossover\n                jrand = self.rng.integers(self.dim)\n                cross_mask = self.rng.random(self.dim) < CRi\n                cross_mask[jrand] = True  # ensure at least one from donor\n                trial = np.where(cross_mask, donor, pop[idx])\n\n                # trust-aware blend with best: small bias toward best if within trust region\n                if self.x_opt is not None:\n                    dist_to_best = np.linalg.norm(pop[idx] - self.x_opt)\n                    if dist_to_best < 2.0 * trust_radius:\n                        # mix slightly with best to encourage local exploitation\n                        trial = 0.85 * trial + 0.15 * self.x_opt\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate candidate if budget left\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(trial))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[idx]:\n                    pop[idx] = trial\n                    old = fvals[idx]\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    succ_F.append(Fi)\n                    succ_CR.append(CRi)\n                    # update best\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = trial.copy()\n                        no_improve_count = 0\n                    else:\n                        no_improve_count += 1\n                else:\n                    # no improvement\n                    no_improve_count += 1\n\n                # small immediate adaptation: nudge means toward used Fi/CR if successful\n                if len(succ_F) > 0:\n                    # keep F_mean and CR_mean bounded\n                    F_mean = np.clip(0.9 * F_mean + 0.1 * np.mean(succ_F), 0.05, 0.99)\n                if len(succ_CR) > 0:\n                    CR_mean = np.clip(0.9 * CR_mean + 0.1 * np.mean(succ_CR), 0.0, 1.0)\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            # update global F/CR more formally if we had successes\n            if len(succ_F) > 0:\n                # Lehmer-like update gives more weight to larger Fi\n                numer = np.sum(np.array(succ_F) ** 2)\n                denom = np.sum(succ_F) + 1e-12\n                F_mean = np.clip(0.8 * F_mean + 0.2 * (numer / denom), 0.05, 0.99)\n            if len(succ_CR) > 0:\n                CR_mean = np.clip(0.8 * CR_mean + 0.2 * np.mean(succ_CR), 0.0, 1.0)\n\n            # trust-region local search around best: sample a few gaussian candidates\n            if evals < self.budget and self.x_opt is not None:\n                remaining_budget = self.budget - evals\n                local_samples = int(min(5, remaining_budget, max(1, 3 + self.dim // 10)))\n                local_success = 0\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma: base trust radius scaled per-dimension\n                    sigma = (trust_radius / (range_norm + 1e-12)) * range_vec * (0.2 + 0.8 * self.rng.random(self.dim))\n                    candidate = self.x_opt + self.rng.normal(0.0, 1.0, self.dim) * sigma\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    # accept if better than best or replace a worse individual\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = candidate.copy()\n                        local_success += 1\n                        no_improve_count = 0\n                    else:\n                        no_improve_count += 1\n                    # optionally replace a random poor individual to inject improved solution\n                    worst_idx = int(np.argmax(fvals))\n                    if f_candidate < fvals[worst_idx]:\n                        pop[worst_idx] = candidate\n                        fvals[worst_idx] = f_candidate\n\n                # adjust trust radius depending on local successes\n                if local_success > 0:\n                    # successful local search -> shrink to focus\n                    trust_radius = max(min_trust, 0.7 * trust_radius)\n                else:\n                    # unsuccessful -> expand slightly to encourage escape\n                    trust_radius = min(max_trust, 1.05 * trust_radius)\n\n            # adapt probability of Lévy jumps based on recent success rate\n            if successes > 0:\n                # if many successes, reduce exploration\n                if successes > max(1, 0.2 * self.pop_size):\n                    p_levy = max(0.01, p_levy * 0.95)\n                else:\n                    p_levy = min(0.5, p_levy * 1.02)\n\n            # slight drift of means toward default if stuck\n            F_mean = np.clip(0.98 * F_mean + 0.02 * 0.5, 0.05, 0.99)\n            CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.9, 0.0, 1.0)\n\n            # stagnation handling: partial reseeding if no improvement for many evals\n            if no_improve_count >= stagnation_reset_threshold and evals < self.budget:\n                # reinitialize half the population randomly (or at least 1)\n                n_reseed = max(1, self.pop_size // 2)\n                reseed_idxs = self.rng.choice(self.pop_size, size=n_reseed, replace=False)\n                for j in reseed_idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # update global best if necessary\n                    if fvals[j] < self.f_opt:\n                        self.f_opt = fvals[j]\n                        self.x_opt = pop[j].copy()\n                        no_improve_count = 0\n                # after reseed, slightly enlarge trust radius to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # reduce p_levy temporarily to let DE explore new area less destructively\n                p_levy = min(0.5, p_levy * 1.2)\n                # reset stagnation counter\n                no_improve_count = 0\n\n        # final results\n        return float(self.f_opt), (self.x_opt.copy() if self.x_opt is not None else None)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.293 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10562862782631821, 0.16464686155175556, 0.2838411423642587, 0.3030948307354192, 0.20938777431538358, 0.943228774141783, 0.21288314101487293, 0.37849462775018583, 0.18326950937696573, 0.14960139405117534]}, "task_prompt": ""}
{"id": "dc03c746-289d-4141-8061-d2a0b0186c61", "fitness": 0.6213159972353479, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy-flight jumps and an online trust-region local search; adapts step sizes and exploration intensity from success history.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    One-line idea: DE for structured population search + sporadic Lévy jumps\n    to escape basins + an adaptive trust-region local search to refine.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size: scales with dimension but bounded by budget\n        if pop_size is None:\n            self.pop_size = int(min(max(10, 4 * self.dim), max(4, self.budget // 6)))\n        else:\n            self.pop_size = int(pop_size)\n        # Keep pop size no larger than budget (cannot evaluate more individuals than budget)\n        self.pop_size = max(1, min(self.pop_size, max(1, self.budget)))\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Ensure bounds become numpy arrays of correct shape (dim,).\n        Accepts scalars, lists, or arrays. If scalar given, broadcast.\n        \"\"\"\n        a = np.asarray(b)\n        if a.shape == ():\n            return np.full(self.dim, float(a))\n        a = a.astype(float)\n        if a.size == 1:\n            return np.full(self.dim, float(a))\n        if a.shape[0] != self.dim:\n            # try to broadcast\n            try:\n                return np.broadcast_to(a, (self.dim,))\n            except Exception:\n                raise ValueError(\"Bounds length does not match dim\")\n        return a\n\n    def __call__(self, func):\n        # Quick guard: no budget\n        if self.budget <= 0:\n            return self.f_opt, self.x_opt\n\n        # Get bounds from func (expected func.bounds.lb and func.bounds.ub)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n        span = ub - lb\n        span_mean = max(1e-12, span.mean())\n\n        # Initialize population uniformly in bounds\n        pop = rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n\n        # Evaluate as many individuals as budget allows (sequentially)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = func(x)\n            fvals[i] = f\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If we didn't evaluate full population due to tiny budget, remaining individuals stay with inf fvals\n        if evals >= self.budget:\n            # budget exhausted during initialization\n            return self.f_opt, self.x_opt\n\n        # Algorithm hyper-parameters (online adapted)\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.06  # base probability of inserting a Lévy jump\n        trust_radius = 0.25 * span_mean  # relative trust radius in problem scale\n        min_trust = 1e-6 * span_mean\n        max_trust = 5.0 * span_mean\n\n        # stagnation counters and adaptation controls\n        no_improve_count = 0\n        global_eval_since_improve = 0\n        gen = 0\n\n        def levy_step(scale=1.0):\n            # Simple heavy-tailed step using Cauchy distribution, clipped to avoid extreme outliers.\n            s = rng.standard_cauchy(self.dim)\n            # Normalize scale of the sample to avoid NaNs or infinite variance effects\n            # Clip large values and rescale to reasonable multiple of problem span\n            s = np.clip(s, -10.0, 10.0)\n            return s * (0.5 * scale)\n\n        # main loop: process generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # per-generation statistics for adaptation\n            succ_F = []\n            succ_CR = []\n            gen_successes = 0\n\n            # Shuffle order to avoid bias\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # Sample individual F_i and CR_i around means with small jitter\n                if rng.rand() < 0.1:\n                    Fi = 0.4 + 0.6 * rng.rand()\n                else:\n                    Fi = np.clip(rng.normal(F_mean, 0.1), 0.01, 1.2)\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # mutation: DE/rand/1-like base\n                # choose three distinct indices different from idx\n                candidates = list(range(self.pop_size))\n                if self.pop_size >= 4:\n                    candidates.remove(idx)\n                    r1, r2, r3 = rng.choice(candidates, 3, replace=False)\n                else:\n                    # small pop edge-case\n                    others = [j for j in range(self.pop_size) if j != idx]\n                    r1 = others[0]\n                    r2 = others[0]\n                    r3 = idx\n\n                donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # Lévy jump centered on current best for exploration\n                if rng.rand() < p_levy:\n                    # scale levy by trust radius and problem span\n                    levy_scale = max(1e-12, trust_radius / span_mean) * span\n                    jump = levy_step(scale=1.0) * levy_scale\n                    donor = pop[idx] + 0.7 * (self.x_opt - pop[idx]) + jump\n\n                # binomial crossover\n                trial = pop[idx].copy()\n                jrand = rng.randint(self.dim)\n                mask = rng.rand(self.dim) < CRi\n                mask[jrand] = True\n                trial[mask] = donor[mask]\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # Evaluate trial (counts toward budget)\n                f_trial = func(trial)\n                evals += 1\n\n                # selection: greedy\n                if f_trial < fvals[idx]:\n                    # success: replace\n                    pop[idx] = trial\n                    fvals[idx] = f_trial\n                    succ_F.append(Fi)\n                    succ_CR.append(CRi)\n                    gen_successes += 1\n\n                    # move means slightly towards successful parameters\n                    if len(succ_F) > 0:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if len(succ_CR) > 0:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                    # tighten trust radius a bit when successful\n                    trust_radius = max(min_trust, trust_radius * 0.95)\n                    no_improve_count = 0\n                    global_eval_since_improve = 0\n                else:\n                    # unsuccessful: slightly relax trust radius to encourage escape\n                    trust_radius = min(max_trust, trust_radius * 1.01)\n                    no_improve_count += 1\n                    global_eval_since_improve += 1\n\n                # update global best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                    # strong encouragement on success\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    no_improve_count = 0\n                    global_eval_since_improve = 0\n\n                # small safeguard to prevent runaway trust_radius\n                trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n                # break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments:\n            # If the generation had several successes, slightly reduce exploration (p_levy)\n            if gen_successes >= max(1, self.pop_size // 10):\n                p_levy = max(0.01, p_levy * 0.9)\n            else:\n                # increase chance of Lévy jumps slowly when not much success\n                p_levy = min(0.5, p_levy * 1.02)\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample count: a small handful scaled by dimension and remaining budget\n            local_samples = int(min(max(1, self.dim // 2), max(1, remaining // max(1, 8))))\n            # anisotropic sigma: base trust_radius scaled by uniform per-dimension factor\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                sigma = (trust_radius / span_mean) * rng.rand(self.dim)\n                cand = self.x_opt + rng.normal(0.0, 1.0, self.dim) * sigma * span\n                cand = np.minimum(np.maximum(cand, lb), ub)\n                f_c = func(cand)\n                evals += 1\n                if f_c < self.f_opt:\n                    self.f_opt = f_c\n                    self.x_opt = cand.copy()\n                    # shrink trust radius on success (focus)\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    no_improve_count = 0\n                    global_eval_since_improve = 0\n                else:\n                    trust_radius = min(max_trust, trust_radius * 1.03)\n                    no_improve_count += 1\n                    global_eval_since_improve += 1\n\n            # stagnation handling: if many evaluations without improvement, increase exploration and reseed some population\n            stagnation_threshold = max(30, 6 * self.dim)\n            if global_eval_since_improve > stagnation_threshold:\n                # intensify long jumps\n                p_levy = min(0.6, p_levy * 1.8)\n                # re-seed half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                for j in range(k):\n                    if evals >= self.budget:\n                        break\n                    new_x = rng.uniform(lb, ub)\n                    # evaluate the new individual; if no budget left, skip re-seeding evaluation\n                    f_new = func(new_x)\n                    evals += 1\n                    # replace worst individuals preferentially\n                    worst_idx = np.argmax(fvals)\n                    if f_new < fvals[worst_idx]:\n                        pop[worst_idx] = new_x\n                        fvals[worst_idx] = f_new\n                        if f_new < self.f_opt:\n                            self.f_opt = f_new\n                            self.x_opt = new_x.copy()\n                # after reseed relax trust radius a bit to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.3)\n                # reset stagnation counter\n                global_eval_since_improve = 0\n                no_improve_count = 0\n\n            # gradual decay of parameter means toward moderate values to avoid extremes\n            F_mean = float(np.clip(0.995 * F_mean + 0.001, 0.01, 1.2))\n            CR_mean = float(np.clip(0.995 * CR_mean + 0.0005, 0.0, 1.0))\n\n            # safety: ensure x_opt exists (in case budget allowed no initial evals - handled earlier)\n            if self.x_opt is None:\n                # fallback to best in population (evaluated)\n                idx_best = int(np.argmin(fvals))\n                self.x_opt = pop[idx_best].copy()\n                self.f_opt = float(fvals[idx_best])\n\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.621 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15415840203819897, 0.24173171573257923, 0.6436846257122011, 0.9439245652868907, 0.8211738866613236, 0.8861382352759914, 0.8113608833743905, 0.6633004435473494, 0.8247747666039382, 0.2229124481206165]}, "task_prompt": ""}
{"id": "30f491cc-3792-4bbb-8687-172beddfda4e", "fitness": 0.21434859071194928, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy flights for global jumps, and an adaptive trust-region local search around the best found solution (online step-size and operator-adaptation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension and budget but kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # make sure population not larger than a fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # internal state for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        # if scalar (0-D) or shape mismatch, broadcast\n        if b.shape == () or b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        b = b.astype(float)\n        if b.shape == (self.dim,):\n            return b\n        # try to broadcast to (dim,)\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except ValueError:\n            # fallback: use first element repeated\n            return np.full(self.dim, float(np.ravel(b)[0]), dtype=float)\n\n    def __call__(self, func):\n        # bounds from func; many BBOB wrappers provide bounds as arrays or scalars\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n\n        # safety: if provided bounds are unreasonable, fall back to [-5,5]\n        safe_lb = -5.0\n        safe_ub = 5.0\n        if not np.all(np.isfinite(lb)): lb = np.full(self.dim, safe_lb)\n        if not np.all(np.isfinite(ub)): ub = np.full(self.dim, safe_ub)\n        # ensure lb < ub per component\n        swapped = lb >= ub\n        if np.any(swapped):\n            # replace swapped components with defaults\n            lb[swapped] = safe_lb\n            ub[swapped] = safe_ub\n\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (careful with budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                fvals[i] = np.inf\n            evals += 1\n\n        # identify initial best among evaluated individuals\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals)\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # no evals possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius (fraction of range)\n        trust_radius = 0.2 * range_norm  # scalar trust radius (in absolute scale)\n        min_trust = 1e-6\n        max_trust = range_norm * 2.0\n\n        gen = 0\n        stagnation_counter = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # limit extreme outliers to avoid numerical blow-up\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation temporary accumulators for successful Fi/CR usage\n            sum_F_success = 0.0\n            sum_CR_success = 0.0\n            count_F_success = 0\n\n            # Process each population member sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Adapt per-individual F and CR (jDE-like sampling around means)\n                # but we'll sample only if DE branch used\n                use_levy = (rng.rand() < p_levy)\n\n                if use_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    # scale by range and dynamic trust_radius\n                    scale = (step_scale * 0.5 + 0.5 * rng.rand()) * (trust_radius / max(range_norm, 1e-12))\n                    donor = best_x + scale * (step * (range_vec / np.maximum(range_vec.mean(), 1e-12)))\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1-like mutation\n                    # pick three distinct indices different from i\n                    idxs = np.arange(self.pop_size)\n                    if self.pop_size >= 4:\n                        idxs = idxs[idxs != i]\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    else:\n                        # degenerate small-pop fallback: pick with replacement but avoid i\n                        choices = [j for j in range(self.pop_size) if j != i]\n                        r1 = rng.choice(choices)\n                        r2 = rng.choice(choices)\n                        r3 = rng.choice(choices)\n                    # adapt F and CR per individual\n                    Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 0.99))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n                    donor_vec = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover to produce trial\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor_vec, pop[i])\n                    donor = trial\n\n                # projection to bounds (ensure feasibility)\n                candidate = np.clip(donor, lb, ub)\n\n                # one evaluation (if budget allows)\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt the means: if DE used, move toward Fi/CRi; if Levy used, slightly reduce CR to favor exploration\n                    if Fi is not None and CRi is not None:\n                        sum_F_success += Fi\n                        sum_CR_success += CRi\n                        count_F_success += 1\n                    else:\n                        # Levy success nudges exploration parameters\n                        F_mean = np.clip(0.98 * F_mean + 0.02 * 0.6, 0.05, 0.99)\n                        CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.5, 0.0, 1.0)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # end of population loop\n\n            # update means from successful DE parameter usages (Lehmer-like learning)\n            if count_F_success > 0:\n                new_Fm = sum_F_success / count_F_success\n                new_CRm = sum_CR_success / count_F_success\n                # smooth update\n                F_mean = 0.9 * F_mean + 0.1 * new_Fm\n                CR_mean = 0.9 * CR_mean + 0.1 * new_CRm\n                # constrain\n                F_mean = float(np.clip(F_mean, 0.05, 0.99))\n                CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample count: small handful scaled by dim and remaining budget\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by rand per-dimension (in relative units)\n                rel_sigma = (0.4 + rng.rand(self.dim) * 0.6) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * rel_sigma * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.80\n                    trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n                    stagnation_counter = 0\n                    # reward exploitation by nudging CR up and F down a bit\n                    CR_mean = float(np.clip(0.95 * CR_mean + 0.05 * 0.95, 0.0, 1.0))\n                    F_mean = float(np.clip(0.95 * F_mean + 0.05 * 0.4, 0.05, 0.99))\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius *= 1.06\n                    trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n                    stagnation_counter += 1\n\n            # adapt probability of Levy jumps and means based on successes and stagnation\n            if successes >= max(1, int(0.15 * self.pop_size)):\n                # good progress -> reduce exploration\n                p_levy = max(0.01, p_levy * 0.94)\n                # move F_mean/CR_mean slightly toward exploitation defaults\n                F_mean = float(np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99))\n                CR_mean = float(np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0))\n            else:\n                # poor progress -> increase jump chance and enlarge F a bit\n                p_levy = min(0.5, p_levy * 1.06 + 0.005)\n                F_mean = float(np.clip(F_mean * 1.015, 0.05, 0.99))\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    # update best if any new point is better\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = float(np.clip(trust_radius * 2.0, min_trust, max_trust))\n                stagnation_counter = 0\n                # encourage exploration after reset\n                p_levy = min(0.5, p_levy + 0.05)\n                # diversify means slightly\n                F_mean = float(np.clip(F_mean * 1.02, 0.05, 0.99))\n                CR_mean = float(np.clip(0.8 * CR_mean + 0.2 * 0.6, 0.0, 1.0))\n\n            # occasionally enforce population best consistency (replace worst with best)\n            if rng.rand() < 0.05:\n                worst_idx = int(np.argmax(fvals))\n                pop[worst_idx] = best_x.copy()\n                fvals[worst_idx] = best_f\n\n            # small safeguard: re-evaluate best if needed (but don't exceed budget)\n            # (we avoid extra evals generally)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.214 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12723174613395927, 0.16078158876242454, 0.31008067658142047, 0.16669750170309428, 0.22105813125077034, 0.3484191301855297, 0.23313990241545102, 0.2559095134623117, 0.16677742192592315, 0.15339029469860843]}, "task_prompt": ""}
{"id": "0c89c1de-6282-4f46-95ec-f2182803c055", "fitness": 0.5320883934490485, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution (jDE-style), occasional Lévy (Cauchy) long jumps, and an online-adaptive trust-region local search — balances fast global exploration with focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (jDE-like),\n    occasional Lévy/Cauchy jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history and stagnation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # population size heuristics: grow mildly with dim but keep within budget\n        if pop_size is None:\n            p = int(8 + 2 * np.sqrt(self.dim))\n            p = max(8, min(60, p))\n            p = min(p, max(4, self.budget // 20))\n            self.pop_size = p\n        else:\n            self.pop_size = int(pop_size)\n\n        # results stored after run\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.shape == (self.dim,):\n            return b.astype(float)\n        # try broadcasting\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        range_norm = max(range_norm, 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n\n        # evaluate initial population but respect budget\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x_i = pop[i]\n            f = float(func(x_i))\n            fvals[i] = f\n            evals += 1\n\n        # determine initial best\n        if np.all(np.isinf(fvals)):\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy (Cauchy) jump\n        step_scale = 0.25   # base scale for Lévy and trust radius (fraction of range)\n        trust_radius = 0.2 * range_norm  # scalar trust radius in absolute scale\n        min_trust = 1e-8\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using truncated Cauchy\n        def levy_step(scale=1.0):\n            # Cauchy (standard) heavy-tail; truncate extreme outliers\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # normalize to roughly unit median absolute deviation and scale\n            med = np.median(np.abs(s)) if np.median(np.abs(s)) > 0 else 1.0\n            return (s / med) * scale\n\n        # main loop: iterate until budget exhausted\n        # We'll produce candidates sequentially; each evaluated candidate consumes one eval.\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # containers for successful Fi/CRi to update means (Lehmer/mean)\n            F_succ = []\n            CR_succ = []\n\n            # iterate over population members\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual F and CR (jDE-like)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 0.99))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # choose mutation: levy jump around best or DE/rand/1\n                if rng.rand() < p_levy:\n                    # Lévy/Cauchy long jump centered on best for exploration\n                    step = levy_step(scale=1.0)\n                    # scale by trust_radius and global range to get absolute step\n                    scale_abs = step_scale * (0.5 + 0.5 * rng.rand()) * (trust_radius / max(range_norm, 1e-12))\n                    donor = best_x + step * scale_abs * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    # no CR typical for Lévy, but we may still crossover lightly\n                    cr_mask = rng.rand(self.dim) < (0.5 * CRi)\n                    # ensure at least one dimension from donor\n                    jrand = rng.randint(self.dim)\n                    cr_mask[jrand] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    candidate = np.clip(trial, lb, ub)\n                else:\n                    # classic DE/rand/1 mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback to random sample around current\n                        donor = pop[i] + Fi * rng.randn(self.dim)\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover (ensure at least one dim from donor)\n                    cr_mask = rng.rand(self.dim) < CRi\n                    jrand = rng.randint(self.dim)\n                    cr_mask[jrand] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement if better than current pop[i]\n                replaced = False\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    replaced = True\n                    successes += 1\n                    # record Fi/CRi only if they were used (DE branch). For Levy we still recorded Fi/CRi but it's okay.\n                    F_succ.append(Fi)\n                    CR_succ.append(CRi)\n\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0  # improvement resets stagnation\n                # else increase stagnation counter slowly\n                # We'll count per generation later\n\n                # If budget exhausted, break\n                if evals >= self.budget:\n                    break\n\n            # End of population loop -> generation-level adjustments\n\n            # Update means F_mean and CR_mean using successes (Lehmer-like for F)\n            if len(F_succ) > 0:\n                # Lehmer mean for F to prefer larger successful F\n                num = np.sum(np.square(F_succ))\n                den = np.sum(F_succ)\n                if den > 0:\n                    F_mean = 0.85 * F_mean + 0.15 * (num / den)\n                else:\n                    F_mean = 0.98 * F_mean\n                # CR mean simple average\n                CR_mean = 0.85 * CR_mean + 0.15 * (np.mean(CR_succ))\n            else:\n                # mild drift if no successes\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n\n            # adapt p_levy: reduce when many successes, increase when stagnating\n            if successes > max(1, int(0.15 * self.pop_size)):\n                p_levy = max(0.01, p_levy * 0.95)\n            elif successes == 0:\n                p_levy = min(0.6, p_levy * 1.10 + 0.01)\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 8)))\n\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension randomness\n                sigma = (0.4 + rng.rand(self.dim) * 0.6) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # successful local step => shrink trust radius a bit to focus search\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                else:\n                    # unsuccessful => slightly expand trust radius to escape\n                    trust_radius *= 1.03\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n\n            # adjust trust radius based on local successes\n            if local_success > 0:\n                trust_radius *= 0.95\n            else:\n                trust_radius *= 1.02\n            trust_radius = max(min(trust_radius, max_trust), min_trust)\n\n            # stagnation bookkeeping: if no improvement this generation increment counter\n            # we check best improvement by comparing to stored best in fvals\n            # Use a simple heuristic: if there were zero improvements (global) in this gen, increment\n            if successes == 0 and local_success == 0:\n                stagnation_counter += 1\n            else:\n                stagnation_counter = max(0, stagnation_counter - 1)\n\n            # strong stagnation reset: if no improvement for many generations, re-seed part of population\n            if stagnation_counter > max(8, self.dim // 2):\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 1.8)\n                # encourage more Levy jumps temporarily\n                p_levy = min(0.5, p_levy + 0.05)\n                stagnation_counter = 0\n\n            # safety: ensure we never do more than budget (loop conditions enforce it)\n            # continue until budget exhausted\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.532 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1488800003102283, 0.2539551876143381, 0.8619030919711738, 0.48274906808571216, 0.9255535537071593, 0.6467856961172966, 0.31471511192960866, 0.4765886448128468, 0.9430759113432923, 0.2666776685988286]}, "task_prompt": ""}
{"id": "60ae092f-16af-4aea-9b98-9388766b570a", "fitness": 0.21978336871448353, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy (Cauchy) long jumps, and a trust-region local search with online adaptation of strategy parameters.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight-like (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Online\n    adaptation of F and CR uses simple success-based nudging.\n    Designed for continuous box-bounded optimization.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size: scaled with dimension but limited by budget\n        if pop_size is None:\n            # base population on dimension, but not larger than a fraction of budget\n            self.pop_size = int(min(max(6, 4 + int(3 * np.sqrt(self.dim))), max(6, self.budget // 20)))\n        else:\n            self.pop_size = int(pop_size)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float).copy()\n        # try broadcasting if possible\n        try:\n            out = np.broadcast_to(b, (self.dim,))\n            return np.array(out, dtype=float)\n        except Exception:\n            raise ValueError(\"Bounds must be scalar or of length dim\")\n\n    def __call__(self, func):\n        # prepare bounds (func is assumed to provide bounds.lb and bounds.ub; Many BBOB uses [-5,5])\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # protect against degenerate ranges\n        range_vec = np.maximum(range_vec, 1e-12)\n\n        rng = self.rng  # local alias\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population until budget or done\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            f = float(func(pop[i]))\n            fvals[i] = f\n            evals += 1\n\n        # If at least one evaluation succeeded, get best, otherwise fail early\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals)\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # base probability of a Lévy (Cauchy) jump\n        step_scale = 0.25   # base scale for Lévy and trust radius relative to range_vec\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius (in absolute units)\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step(dimension):\n            # use standard Cauchy distributed steps (heavy-tailed)\n            s = rng.standard_cauchy(size=dimension)\n            # clip extreme outliers to avoid numerical blow-up but keep heavy tail\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # Main optimization loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # adapt jump probability modestly with stagnation\n            p_current_levy = np.clip(p_levy * (1.0 + stagnation_counter / max(100.0, self.dim)), 0.01, 0.5)\n\n            # shuffle order to reduce bias\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                target = pop[idx].copy()\n                target_f = fvals[idx]\n\n                # decide operator: Levy jump or DE variant\n                if rng.rand() < p_current_levy:\n                    # Lévy-style jump centered around current best (long-range exploration)\n                    # scale relative to trust_radius and global range\n                    scale = step_scale * (0.5 + rng.rand() * 2.0)\n                    step = levy_step(self.dim)\n                    donor = best_x + (trust_radius * scale) * (step / (np.linalg.norm(step) + 1e-12)) * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    # small random recombination with current target to preserve some features\n                    mix_mask = rng.rand(self.dim) < 0.2\n                    if not np.any(mix_mask):\n                        mix_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(mix_mask, donor, target)\n                    operator = 'levy'\n                else:\n                    # DE/rand/1 with per-individual Fi and CRi (jDE-like)\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != idx]\n                    r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    # Fi and CRi sampled around means with small perturbation\n                    Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n                    donor_vec = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor_vec, target)\n                    operator = 'de'\n                    # keep Fi/CRi for adaptation if successful\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection (greedy)\n                replaced = False\n                if f_candidate < target_f:\n                    # accept\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    replaced = True\n\n                    # adapt F_mean and CR_mean slightly toward the used Fi / CRi when DE used\n                    if operator == 'de':\n                        # Fi and CRi exist in this branch\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    else:\n                        # if Levy gave success, slightly decrease p_levy to exploit\n                        p_levy = max(0.01, 0.98 * p_levy)\n                        # and nudge F_mean toward smaller values (more conservative)\n                        F_mean = 0.98 * F_mean + 0.02 * 0.5\n\n                else:\n                    # unsuccessful: punish parameters slightly (increase exploration)\n                    if operator == 'de':\n                        F_mean = 0.995 * F_mean + 0.005 * 0.8\n                        CR_mean = 0.995 * CR_mean + 0.005 * 0.2\n                    else:\n                        p_levy = min(0.5, 1.02 * p_levy)\n\n                # update global best if needed\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    if not replaced:\n                        stagnation_counter += 1\n\n                # small trust radius adaptation on successes\n                if replaced:\n                    # successful local step: focus search a bit\n                    trust_radius = max(min_trust, trust_radius * 0.97)\n                else:\n                    # no success: expand slightly to escape\n                    trust_radius = min(max_trust, trust_radius * 1.01)\n\n            # End of generation adjustments\n            success_rate = successes / max(1.0, float(self.pop_size))\n            # gradually move CR_mean toward modest exploitation if many successes\n            if success_rate > 0.2:\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                # reduce chance of Levy as we find improvements\n                p_levy = max(0.01, p_levy * 0.97)\n            else:\n                # encourage more exploration\n                p_levy = min(0.5, p_levy * 1.03)\n                F_mean = np.clip(F_mean * 1.01, 0.05, 1.2)\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # determine local samples: a handful depending on dimension and remaining budget\n            local_samples = int(min(max(1, self.dim // 3), remaining, max(1, int(6 * (1.0 - success_rate)))))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (0.4 + rng.rand(self.dim) * 0.6) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # stagnation handling: partial reinitialization if no improvement for a while\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population randomly to diversify\n                reinit_count = max(1, self.pop_size // 2)\n                reinit_indices = rng.choice(np.arange(self.pop_size), size=reinit_count, replace=False)\n                for j in reinit_indices:\n                    if evals >= self.budget:\n                        # if no budget left, still re-seed but don't evaluate\n                        pop[j] = lb + rng.rand(self.dim) * range_vec\n                        fvals[j] = np.inf\n                        continue\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # also mildly increase Levy probability\n                p_levy = min(0.5, p_levy * 1.2)\n\n            # safety: ensure means in reasonable ranges\n            F_mean = float(np.clip(F_mean, 0.05, 1.2))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.220 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11166748073015054, 0.16087794202406291, 0.3483007494688347, 0.2618643696232198, 0.2207464303163451, 0.2550787402323018, 0.22520892529738068, 0.23941305280989067, 0.21882800616181863, 0.15584799048083053]}, "task_prompt": ""}
{"id": "f3114c9e-30c4-46b5-9c45-4ce9e834f52f", "fitness": 0.4098458238312184, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history. Designed for box-bounded\n    continuous optimization (expects func.bounds.lb / ub).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: scales with dim but capped by budget\n        if pop_size is None:\n            self.pop_size = max(4, min(20 + 2 * self.dim, max(4, self.budget // 20)))\n        else:\n            self.pop_size = int(pop_size)\n        # RNG\n        self.rng = np.random.default_rng(seed)\n        # results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to dimension {}\".format(self.dim))\n\n    def __call__(self, func):\n        # get bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # safety: if bounds are inf or identical, fallback to [-5,5] as specified\n        if not np.all(np.isfinite(lb)) or not np.all(np.isfinite(ub)) or np.any(ub <= lb):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        range_vec = ub - lb\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if we didn't get any evals, return nothing\n        if evals == 0:\n            self.x_opt = None\n            self.f_opt = np.inf\n            return self.f_opt, self.x_opt\n\n        # pick initial best among evaluated entries\n        valid_mask = np.isfinite(fvals)\n        best_idx = np.argmin(np.where(valid_mask, fvals, np.inf))\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.3       # crossover mean\n        p_levy = 0.05       # chance of levy/global jump\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 1.0\n        trust_radius = max_trust * 0.25  # initial trust radius (norm scale)\n\n        stagnation_counter = 0\n        gen = 0\n        max_stagnation = max(20, 5 * self.pop_size)\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step(scale=1.0):\n            # Cauchy (heavy tailed). clamp extremes to avoid NaNs\n            s = rng.standard_cauchy(self.dim) * scale\n            s = np.clip(s, -1e6, 1e6)\n            return s\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            per_gen_success = 0\n\n            # iterate over population sequentially (one eval per candidate attempted)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # prepare mutation branch: Levy jump or DE/rand/1\n                if rng.random() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    # scale by trust_radius and relative range per-dimension\n                    step = levy_step(scale=1.0)\n                    # normalize step to have roughly trust_radius norm\n                    step = step / (np.linalg.norm(step) + 1e-12) * trust_radius\n                    donor = best_x + step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    # use no Fi/CRi (mark with None)\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1-like mutation\n                    idxs = np.arange(self.pop_size)\n                    if self.pop_size <= 3:\n                        # fallback small-pop behavior: sample with replacement\n                        r1, r2, r3 = rng.integers(0, self.pop_size, size=3)\n                    else:\n                        idxs = idxs[idxs != i]\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover to produce trial candidate\n                if CRi is None:\n                    # in Levy branch, mix best and donor a bit\n                    mix_mask = rng.random(self.dim) < 0.5\n                else:\n                    mix_mask = rng.random(self.dim) < CRi\n                # ensure at least one component from donor\n                jrand = rng.integers(0, self.dim)\n                mix_mask[jrand] = True\n\n                candidate = np.where(mix_mask, donor, pop[i])\n                # small gaussian perturbation proportional to trust_radius to encourage exploitation\n                candidate = candidate + rng.normal(0, 0.01, size=self.dim) * (trust_radius / max_trust)\n\n                # projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # one evaluation (if budget permits)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population (if current has been evaluated)\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    per_gen_success += 1\n                    # adapt means slightly toward successful parameters (simple learning)\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    successes += 1\n                    stagnation_counter = 0\n                    # successful local improvement should shrink trust region a bit\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    stagnation_counter += 1\n                    # unsuccessful mutation slightly expands trust to encourage escapes\n                    trust_radius = min(max_trust, trust_radius * 1.01)\n\n                # small protective adjustments\n                F_mean = np.clip(F_mean, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean, 0.0, 0.99)\n                p_levy = np.clip(p_levy, 0.01, 0.5)\n\n                if evals >= self.budget:\n                    break\n\n            # End of generation: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # choose number of local samples (budget-aware)\n            local_samples = int(min(max(1, self.dim), remaining, max(1, self.pop_size // 3)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian noise: per-dim sigma proportional to trust radius and range_vec\n                sigma = (0.1 + rng.random(self.dim) * 0.9) * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    successes += 1\n                    # shrink trust radius to focus locally\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                else:\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n                if evals >= self.budget:\n                    break\n\n            # Adapt probability of Levy jumps based on success / stagnation\n            if per_gen_success >= max(1, self.pop_size * 0.15):\n                # good progress: reduce exploration\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                # limited progress: increase chance of long jumps\n                p_levy = min(0.5, p_levy * 1.03 + 0.001)\n\n            # stagnation handling: if no improvement for long time, partially re-seed population\n            if stagnation_counter >= max_stagnation:\n                # reinitialize half of the population (or as many as budget allows)\n                nreset = max(1, self.pop_size // 2)\n                for j in range(nreset):\n                    if evals >= self.budget:\n                        break\n                    idx = rng.integers(0, self.pop_size)\n                    pop[idx] = lb + rng.random(self.dim) * range_vec\n                    fvals[idx] = float(func(pop[idx]))\n                    evals += 1\n                    if fvals[idx] < best_f:\n                        best_f = fvals[idx]\n                        best_x = pop[idx].copy()\n                # increase exploration after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                p_levy = min(0.5, p_levy * 1.2)\n                stagnation_counter = 0\n\n            # safety termination if best is already very good (optional heuristic)\n            # (no strict threshold given; we won't early stop here)\n\n            # continue next generation\n            # small generation cap just to prevent pathological long loops (not enforced by budget)\n            if gen > 10000:\n                break\n\n        # final results\n        self.x_opt = best_x.copy()\n        self.f_opt = float(best_f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.410 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19964605362880106, 0.2068067210933242, 0.41178382590534224, 0.9255917978687809, 0.4316220446890515, 0.4871160010080804, 0.2788033855082087, 0.419232502329443, 0.40097580899033936, 0.33688009729081236]}, "task_prompt": ""}
{"id": "f3114ebf-2e58-41f5-a76e-d912ddcbf35d", "fitness": 0.5319254328638827, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DE with occasional Lévy jumps and trust-region local search — combines population-based Differential Evolution, rare heavy-tailed long jumps for escape, and an online-adapting trust-region local search to balance fast global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    are adapted online based on success history.\n\n    Usage: instance(bbob_function) where bbob_function has .bounds.lb/.ub\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default pop size relative to dim and budget\n        if pop_size is None:\n            # keep population modest relative to budget but scale with dim\n            pop_guess = max(8, min(12 * self.dim, max(8, self.budget // 20)))\n            self.pop_size = int(pop_guess)\n        else:\n            self.pop_size = int(pop_size)\n        # ensure population is not larger than budget - 1\n        self.pop_size = max(2, min(self.pop_size, max(2, self.budget - 1)))\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n\n        # fallback if bounds degenerate (many BBOB are [-5,5] but use provided)\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        # initialize population\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate initial population within budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # If nothing evaluated\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # initialize global best\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.3       # crossover mean\n        p_levy = 0.05       # base probability of Levy jump per trial\n        trust_radius = 0.2  # relative fraction of range_vec (0..1)\n        min_trust = 1e-4\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        successes_since_update = 0\n        history_F = []\n        history_CR = []\n\n        gen = 0\n        # helper: Levy-like heavy-tailed step using standard Cauchy\n        def levy_step(scale=1.0):\n            # Cauchy distributed steps per-dimension; clip extremes\n            s = self.rng.standard_cauchy(size=self.dim) * scale\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate until budget exhausted\n        # We'll iterate over population repeatedly (generations)\n        while evals < self.budget:\n            gen += 1\n            # per-generation small jitter to means to maintain variation\n            F_mean = np.clip(F_mean * (1.0 + 0.02 * (self.rng.rand() - 0.5)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + 0.05 * (self.rng.rand() - 0.5), 0.0, 1.0)\n\n            # process each individual sequentially (consumes 1 eval per candidate)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi (jDE-like)\n                Fi = np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.0)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.15), 0.0, 1.0)\n\n                # choose mutation strategy: Levy jump or DE/rand/1\n                if self.rng.rand() < p_levy:\n                    # Levy jump centered at current best to explore far\n                    # scale by trust radius and range\n                    step_scale = max(0.5 * trust_radius, 1e-6)\n                    s = levy_step(scale=1.0)\n                    donor = best_x + (0.5 + self.rng.rand() * 1.0) * s * (range_vec / np.maximum(range_vec.mean(), 1e-12)) * step_scale\n                else:\n                    # DE/rand/1 mutation: r1 + Fi*(r2 - r3)\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cr_mask = self.rng.rand(self.dim) < CRi\n                # ensure at least one dimension from donor\n                cr_mask[self.rng.randint(self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(trial))\n                evals += 1\n\n                # selection\n                if f_candidate < fvals[i]:\n                    # accept\n                    pop[i] = trial\n                    fvals[i] = f_candidate\n                    # adapt parameter means toward successful Fi/CRi (small step)\n                    history_F.append(Fi)\n                    history_CR.append(CRi)\n                    successes_since_update += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = trial.copy()\n                    # shrink trust radius on improvement\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    # reward smaller p_levy slightly\n                    p_levy = max(0.005, p_levy * 0.92)\n                    # update stored best\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n                # periodic update of F_mean and CR_mean using Lehmer/mean-like update\n                if successes_since_update >= max(1, int(0.05 * self.pop_size)):\n                    # take means of successful Fi/CRi\n                    if history_F:\n                        m_F = np.mean(history_F)\n                        # Lehmer-like: bias toward larger F that produced successes\n                        F_mean = 0.9 * F_mean + 0.1 * m_F\n                    if history_CR:\n                        CR_mean = 0.9 * CR_mean + 0.1 * np.mean(history_CR)\n                    history_F.clear()\n                    history_CR.clear()\n                    successes_since_update = 0\n\n                # stagnation handling in inner loop: if many failures, consider a Levy jump next time\n                if stagnation_counter > max(50, self.dim * 5):\n                    p_levy = min(0.5, p_levy * 1.2 + 0.02)\n                    # small random reinitialization of one individual to inject diversity (if budget allows future evals)\n                    j = self.rng.randint(self.pop_size)\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    # mark its fitness as unknown and evaluate if budget permits\n                    if evals < self.budget:\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                        if fvals[j] < best_f:\n                            best_f = fvals[j]\n                            best_x = pop[j].copy()\n                            self.f_opt = best_f\n                            self.x_opt = best_x.copy()\n                    stagnation_counter = 0\n\n            # End of generation: perform a trust-region local search around best (small batch)\n            if evals >= self.budget:\n                break\n\n            # number of local samples depends on remaining budget and dimension\n            remaining = self.budget - evals\n            local_budget = min(max(1, self.dim // 2), remaining, max(1, self.pop_size // 6))\n            for _ in range(local_budget):\n                # anisotropic gaussian noise scaled by trust_radius and range_vec\n                sigma = trust_radius * (0.2 + 0.8 * self.rng.rand())  # slight anisotropy\n                candidate = best_x + self.rng.normal(0, 1.0, size=self.dim) * (sigma * range_vec)\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                if evals >= self.budget:\n                    break\n                f_c = float(func(candidate))\n                evals += 1\n                if f_c < best_f:\n                    best_f = f_c\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful local steps slightly expand trust radius\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # generational adaptation of exploration/exploitation\n            if stagnation_counter > max(100, 10 * self.dim):\n                # strong stagnation: increase chance of Levy jumps and slightly expand trust region\n                p_levy = min(0.6, p_levy * 1.3 + 0.02)\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # replace a fraction of population randomly to diversify (but careful with budget)\n                k = max(1, self.pop_size // 4)\n                for j in self.rng.choice(self.pop_size, k, replace=False):\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    # try to evaluate new seeds if budget allows\n                    if evals < self.budget:\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                        if fvals[j] < best_f:\n                            best_f = fvals[j]\n                            best_x = pop[j].copy()\n                            self.f_opt = best_f\n                            self.x_opt = best_x.copy()\n                stagnation_counter = 0\n\n            # decay p_levy slowly if we have been improving\n            if self.f_opt < np.inf and np.isfinite(self.f_opt):\n                p_levy = max(0.005, p_levy * 0.995)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.532 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14146059774703357, 0.17672054017946748, 0.42556403516943897, 0.7977801134429412, 0.7595282957836358, 0.8004001930422987, 0.6740710918563415, 0.6119635644206518, 0.7437727397689986, 0.1879931572280188]}, "task_prompt": ""}
{"id": "875b62c4-73ef-447d-bec6-1536e2485fbd", "fitness": 0.26641629596604777, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like online parameter adaptation, occasional Lévy-flight global jumps, and a shrinking/expanding trust-region local search around the current best (fast global exploration + focused local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, seed=1)\n        f_opt, x_opt = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # Bounds for Many Affine BBOB (and as required by the prompt)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.range = self.ub - self.lb\n\n        # sensible default population size: scale with dimension but limited by budget\n        if pop_size is None:\n            # scale with dim but not too large relative to budget\n            candidate = max(8, 6 * self.dim)\n            self.pop_size = min(candidate, max(4, self.budget // 4))\n        else:\n            self.pop_size = max(4, int(pop_size))\n\n        # internal best values\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = self.rng\n        lb = self.lb\n        ub = self.ub\n        dim = self.dim\n        pop_size = self.pop_size\n        budget = self.budget\n\n        # helper: sample levy-like heavy-tailed vector (Cauchy truncated)\n        def levy_step(scale=0.5):\n            # standard Cauchy gives heavy tails; truncate extremes\n            v = rng.standard_cauchy(size=dim)\n            # clip to avoid numerical blow-ups but keep heavy tails\n            v = np.clip(v, -1e2, 1e2)\n            return scale * v\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(pop_size, dim) * self.range\n\n        # evaluate as many initial individuals as budget allows\n        fvals = np.full(pop_size, np.inf, dtype=float)\n        evals = 0\n        for i in range(pop_size):\n            if evals >= budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # initialize best\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n\n        # DE / adaptation hyperparameters\n        F_mean = 0.6  # mean mutation factor\n        CR_mean = 0.9  # mean crossover\n        tau_F = 0.1  # adaptation rate for Fi (jDE-like)\n        tau_CR = 0.1  # adaptation rate for CRi (jDE-like)\n\n        # levy probability and scaling\n        p_levy = 0.05\n        levy_scale = 0.7  # fraction of range\n\n        # trust-region variables\n        trust_radius = 0.2 * self.range  # absolute scale in variable space\n        min_trust = 1e-6 * self.range\n        max_trust = 2.0 * self.range\n\n        stagnation_counter = 0\n        stagnation_threshold = max(50, dim * 5)\n\n        generation = 0\n        # Main loop: until budget exhausted\n        while evals < budget:\n            generation += 1\n            successes = 0\n            gen_best_improved = False\n\n            # per-generation arrays for Fi and CRi (for adaptation)\n            Fi_used = []\n            CRi_used = []\n            succ_F = []\n            succ_CR = []\n\n            # Shuffle order of targets to avoid bias\n            order = rng.permutation(pop_size)\n\n            for idx in order:\n                if evals >= budget:\n                    break\n\n                target = pop[idx].copy()\n                f_target = fvals[idx]\n\n                # sample individual Fi and CRi (jDE-like)\n                if rng.rand() < tau_F:\n                    Fi = rng.rand() * 0.9 + 0.05  # random in (0.05,0.95)\n                else:\n                    Fi = F_mean\n                # log-normal-ish jitter\n                Fi = np.clip(Fi * (1 + 0.3 * (rng.rand() - 0.5)), 0.01, 1.2)\n\n                if rng.rand() < tau_CR:\n                    CRi = np.clip(rng.rand(), 0.0, 1.0)\n                else:\n                    CRi = CR_mean\n                CRi = np.clip(CRi + 0.1 * (rng.rand() - 0.5), 0.0, 1.0)\n\n                candidate = None\n                # With some probability do a Levy jump rather than DE mutation\n                if rng.rand() < p_levy:\n                    # Levy jump centered on best with scaling influenced by trust radius\n                    step = levy_step(scale=levy_scale) * (self.range)\n                    # also occasionally bias by trust radius to encourage escapes\n                    step *= (1.0 + trust_radius / (self.range + 1e-12))\n                    candidate = best_x + step\n                else:\n                    # Differential Evolution rand/1/bin style: pick r1,r2,r3 distinct and != idx\n                    inds = [i for i in range(pop_size) if i != idx]\n                    if len(inds) < 3:\n                        # fallback to random perturbation\n                        candidate = target + Fi * rng.randn(dim)\n                    else:\n                        r = rng.choice(inds, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        mask = rng.rand(dim) < CRi\n                        # ensure at least one component crosses\n                        jrand = rng.randint(dim)\n                        mask[jrand] = True\n                        candidate = np.where(mask, donor, target)\n\n                # project to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate if budget allows\n                if evals >= budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy\n                if f_candidate <= f_target:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    Fi_used.append(Fi)\n                    CRi_used.append(CRi)\n                    succ_F.append(Fi)\n                    succ_CR.append(CRi)\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        gen_best_improved = True\n                        stagnation_counter = 0\n                else:\n                    # keep old individual, count stagnation\n                    stagnation_counter += 1\n\n                # update recorded global best for return\n                if best_f < self.f_opt:\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n            # End of generation: adapt means slightly toward successful Fi/CRi\n            if len(succ_F) > 0:\n                # weighted update: move means toward median of successful factors\n                new_F = np.median(succ_F)\n                F_mean = 0.9 * F_mean + 0.1 * new_F\n            else:\n                # small decay toward exploration\n                F_mean = max(0.2, 0.98 * F_mean)\n\n            if len(succ_CR) > 0:\n                new_CR = np.median(succ_CR)\n                CR_mean = 0.9 * CR_mean + 0.1 * new_CR\n            else:\n                CR_mean = max(0.1, 0.98 * CR_mean)\n\n            # adjust probability of Levy: increase when stagnating, decrease when many successes\n            if successes > max(1, pop_size * 0.15):\n                p_levy = max(0.01, p_levy * 0.95)\n                # slightly contract trust radius to focus\n                trust_radius = max(min_trust, trust_radius * 0.98)\n            else:\n                # not many successes: increase chance of large jumps\n                p_levy = min(0.6, p_levy * 1.06 + 0.005)\n                trust_radius = min(max_trust, trust_radius * 1.02)\n\n            # Trust-region local search around best: use some of the remaining budget\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: small handful depending on dim and remaining budget\n            local_samples = min(remaining, max(1, int(round(dim * 1.5))))\n            # anisotropic gaussian scaled by trust_radius and by range norm\n            for _ in range(local_samples):\n                if evals >= budget:\n                    break\n                # anisotropic per-dimension sigma around trust_radius\n                per_dim_scale = (0.3 + rng.rand(dim) * 0.7) * (trust_radius / (self.range + 1e-12))\n                candidate = best_x + rng.randn(dim) * per_dim_scale * self.range\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    # improvement: accept and shrink trust radius\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    stagnation_counter = 0\n                    # adapt global records\n                    if best_f < self.f_opt:\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                else:\n                    # no improvement: slight expansion to escape local minima\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed a portion\n            if stagnation_counter > stagnation_threshold:\n                # reinitialize half of population randomly to diversify\n                k = max(1, pop_size // 2)\n                re_idxs = rng.choice(np.arange(pop_size), k, replace=False)\n                for j in re_idxs:\n                    if evals >= budget:\n                        break\n                    pop[j] = lb + rng.rand(dim) * self.range\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = float(fvals[j])\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                        if best_f < self.f_opt:\n                            self.f_opt = best_f\n                            self.x_opt = best_x.copy()\n                # after reset encourage exploration and expand trust radius a bit\n                p_levy = min(0.9, p_levy * 1.2 + 0.02)\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # relax stagnation counter so we don't repeatedly reset\n                stagnation_counter = 0\n\n        # final return: best found\n        # Ensure x_opt set\n        if self.x_opt is None:\n            # fallback: pick best among fvals (in case none evaluated earlier)\n            best_idx = int(np.argmin(fvals))\n            self.x_opt = pop[best_idx].copy()\n            self.f_opt = float(fvals[best_idx])\n\n        return float(self.f_opt), self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.266 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08633827163770535, 0.14804753412644667, 0.23444858587789863, 0.17174075661001986, 0.18929902003652077, 0.659856527147636, 0.22553301592006503, 0.16840967345172309, 0.6412608308871746, 0.13922874396528762]}, "task_prompt": ""}
{"id": "f9c52893-8d10-45a3-bce6-c59577990bcd", "fitness": 0.2565881810607216, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-driven long jumps, and a trust-region local search; online adapts DE parameters and jump probability to balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps (via truncated Cauchy) for long-range exploration,\n    and a trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One may instantiate with HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=None)\n    and call it with a black-box function `func` that exposes func.bounds.lb/ub arrays.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size scaled with dimension\n        if pop_size is None:\n            pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n        self.pop_size = int(max(4, pop_size))\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Ensure lb/ub are numpy arrays of length dim (broadcast scalars).\n        \"\"\"\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size != self.dim:\n            # attempt to broadcast if possible, otherwise raise\n            try:\n                return np.broadcast_to(b, (self.dim,))\n            except Exception:\n                raise ValueError(\"Bounds size mismatch with dimension.\")\n        return b.astype(float)\n\n    def __call__(self, func):\n        # Prepare bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # If range is zero anywhere, set a small non-zero to avoid division by zero\n        safe_range = np.maximum(range_vec, 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * safe_range\n\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # Evaluate initial population within budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                fvals[i] = np.inf\n            evals += 1\n\n        # If no evaluated individuals (tiny budget), pick random x as no-eval return\n        finite_mask = np.isfinite(fvals)\n        if not np.any(finite_mask):\n            # if we couldn't evaluate anything, return inf and a random point\n            self.f_opt = np.inf\n            self.x_opt = lb + self.rng.rand(self.dim) * safe_range\n            return self.f_opt, self.x_opt\n\n        # Fill unevaluated individuals with random positions (they remain unevaluated)\n        for i in range(self.pop_size):\n            if not np.isfinite(fvals[i]):\n                pop[i] = lb + self.rng.rand(self.dim) * safe_range\n\n        # global best\n        best_idx = np.nanargmin(fvals)\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.5    # mean mutation factor for jDE-like adaptation\n        CR_mean = 0.5   # mean crossover probability\n        p_levy = 0.05   # initial probability of taking a Lévy jump\n        step_scale = 0.25  # base scale for Levy and trust radius relative to range\n        trust_radius_rel = 0.25  # relative trust radius (fraction of full range norm)\n        max_trust_rel = 2.0\n        stagnation_counter = 0\n\n        # helper: truncated Cauchy (Levy-like heavy tail) limited to avoid blow-ups\n        def levy_step(truncate=10.0):\n            # standard Cauchy via tan(pi*(u-0.5)); then truncate magnitude\n            u = self.rng.rand()\n            s = np.tan(np.pi * (u - 0.5))\n            # limit extremes\n            s = np.sign(s) * min(abs(s), truncate)\n            # produce per-dimension scaling between -1..1 but heavy-tailed\n            return s * (0.5 + self.rng.rand(self.dim) * 1.5)  # mix per-dim variability\n\n        gen = 0\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # per-generation small randomization of means to avoid lock\n            F_mean = np.clip(F_mean * (1.0 + (self.rng.rand() - 0.5) * 0.02), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean * (1.0 + (self.rng.rand() - 0.5) * 0.02), 0.0, 1.0)\n\n            # iterate population sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi via a truncated Cauchy around F_mean (jDE-like)\n                # try a few draws to get a valid Fi in (0.01,1.5)\n                Fi = None\n                for _ in range(5):\n                    draw = F_mean + 0.1 * np.tan(np.pi * (self.rng.rand() - 0.5))\n                    if 0.01 < draw < 1.5:\n                        Fi = draw\n                        break\n                if Fi is None:\n                    Fi = np.clip(self.rng.normal(F_mean, 0.2), 0.05, 1.0)\n\n                # sample CRi\n                CRi = np.clip(self.rng.normal(CR_mean, 0.15), 0.0, 1.0)\n\n                use_levy = (self.rng.rand() < p_levy)\n                if use_levy:\n                    # Lévy jump centered at best\n                    ls = levy_step()\n                    # scale by trust radius relative and global range\n                    candidate = best_x + (ls * (step_scale * trust_radius_rel)) * safe_range\n                else:\n                    # DE/rand/1 mutation with optional best influence\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    a, b, c = self.rng.choice(idxs, 3, replace=False)\n                    donor = pop[a] + Fi * (pop[b] - pop[c])\n                    # small chance to bias toward best (current-to-best-like behavior)\n                    if self.rng.rand() < 0.2:\n                        donor = donor + 0.3 * Fi * (best_x - pop[i])\n                    # binomial crossover\n                    mask = self.rng.rand(self.dim) < CRi\n                    if not np.any(mask):\n                        # ensure at least one dimension changes\n                        mask[self.rng.randint(0, self.dim)] = True\n                    candidate = pop[i].copy()\n                    candidate[mask] = donor[mask]\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                replaced = False\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    replaced = True\n                    # small adaptation toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # decay means slowly to encourage diversity if many failures\n                    F_mean = 0.999 * F_mean\n                    CR_mean = 0.999 * CR_mean\n\n                # update global best if needed\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    stagnation_counter = 0\n                    # successful global improvement encourages exploitation\n                    trust_radius_rel = max(trust_radius_rel * 0.9, 1e-6)\n                    # reduce chance of Levy after improvement\n                    p_levy = max(0.005, p_levy * 0.85)\n                else:\n                    stagnation_counter += 1\n\n                # slight nudging of p_levy depending on short-run success\n                if replaced:\n                    p_levy = max(0.005, p_levy * 0.98)\n                else:\n                    p_levy = min(0.5, p_levy * 1.002 + 0.0005)\n\n                # End individual loop\n\n            # End of generation adjustments (trust-region local search)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # determine number of local samples (small handful)\n            local_samples = min(remaining, max(1, int(max(1, self.dim // 2))))\n            local_successes = 0\n\n            # anisotropic sigma per-dimension drawn around trust_radius_rel\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # per-dim sigma: base trust_radius_rel scaled by per-dim random factor\n                sigma_rel = (0.3 + self.rng.rand(self.dim) * 0.7) * trust_radius_rel\n                candidate = best_x + self.rng.normal(0, 1.0, size=self.dim) * sigma_rel * safe_range\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    local_successes += 1\n                    # shrink trust radius to focus\n                    trust_radius_rel = max(1e-6, trust_radius_rel * 0.8)\n                    stagnation_counter = 0\n                    # reward exploitation by nudging means\n                    F_mean = np.clip(0.95 * F_mean + 0.05 * 0.5, 0.05, 0.99)\n                    CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                else:\n                    # unsuccessful local step => slightly expand to escape\n                    trust_radius_rel = min(max_trust_rel, trust_radius_rel * 1.05)\n                    # encourage more exploration if local steps fail\n                    p_levy = min(0.5, p_levy * 1.02 + 0.001)\n\n            # adjust p_levy depending on local success rate\n            if local_successes > 0:\n                p_levy = max(0.005, p_levy * 0.9)\n            else:\n                p_levy = min(0.5, p_levy * 1.03 + 0.002)\n\n            # strong stagnation reset: if no improvement for long, re-seed half population\n            if stagnation_counter > max(50, self.dim * 5) and evals < self.budget:\n                to_replace = max(1, self.pop_size // 2)\n                indices = self.rng.choice(self.pop_size, to_replace, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * safe_range\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    # update global best if any of the reseeded are good\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                # slightly enlarge trust radius to encourage exploration after reset\n                trust_radius_rel = min(max_trust_rel, trust_radius_rel * 1.5)\n                stagnation_counter = 0\n                # increase chance of Levy jumps after reset\n                p_levy = min(0.5, p_levy * 1.4 + 0.02)\n\n            # safeguard to avoid overly tiny or huge trust radius\n            trust_radius_rel = np.clip(trust_radius_rel, 1e-6, max_trust_rel)\n\n            # generation loop end\n\n        # Final results\n        # ensure x_opt and f_opt are set\n        if self.x_opt is None:\n            # fallback: pick best from current fvals\n            idx = int(np.nanargmin(fvals))\n            self.f_opt = float(fvals[idx])\n            self.x_opt = pop[idx].copy()\n\n        return float(self.f_opt), self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.257 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11146459712998058, 0.1965307117062276, 0.4889374675207091, 0.31615319624653726, 0.2619749690045631, 0.29095001341451565, 0.25017083688881747, 0.2588926100272493, 0.21434650316632464, 0.17646090550229154]}, "task_prompt": ""}
{"id": "6a702fcb-534d-442f-82ca-39dc1885304b", "fitness": 0.34136755116687867, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and an online-adaptive trust-region local search for fast global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (rand/1/bin style) with jDE-like online adaptation of F and CR\n      - Occasional Lévy-like (Cauchy) jumps centered on the current best for long-range exploration\n      - A trust-region local search (Gaussian samples around current best) with adaptive radius\n    Designed to work with continuous box-bounded problems (default bounds [-5,5]) and obeys a strict\n    function-evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: scale with dim but not exceed half budget\n        if pop_size is None:\n            self.pop_size = max(4, min(20 + 2 * self.dim, max(4, self.budget // 2)))\n        else:\n            self.pop_size = max(4, int(pop_size))\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # try to obtain bounds from func.bounds if provided, else default [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            # handle scalar bounds\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb))\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        range_vec[range_vec == 0.0] = 1e-8\n\n        rng = self.rng\n        budget = int(self.budget)\n        dim = self.dim\n        pop_size = self.pop_size\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(pop_size, dim) * range_vec\n\n        # evaluation bookkeeping\n        fvals = np.full(pop_size, np.inf)\n        evals = 0\n\n        # Evaluate as many of the initial population as allowed by budget\n        for i in range(pop_size):\n            if evals >= budget:\n                break\n            xi = pop[i]\n            fi = float(func(xi))\n            evals += 1\n            fvals[i] = fi\n            # update global best\n            if fi < self.f_opt:\n                self.f_opt = fi\n                self.x_opt = xi.copy()\n\n        # If no evaluations possible (budget==0), return current placeholders\n        if evals == 0:\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters and online-adapted means\n        F_mean = 0.6   # mean of scaling factor F\n        CR_mean = 0.2  # mean of crossover probability\n        p_levy = 0.05  # base probability of performing a Lévy jump mutation (adapts with stagnation)\n        trust_radius = 0.08  # relative trust radius (fraction of range_vec)\n        min_trust = 1e-5\n        max_trust = 1.5  # can be larger than 1 to allow broader local search occasionally\n\n        stagnation_counter = 0\n        last_best = self.f_opt\n\n        generation = 0\n        # To adapt means each generation: collect successful Fi and CRi\n        while evals < budget:\n            generation += 1\n            # per-generation accumulators for adaptation (simple moving averages)\n            succ_F = []\n            succ_CR = []\n            successes = 0\n\n            # shuffle order to reduce bias\n            order = rng.permutation(pop_size)\n            for idx in order:\n                if evals >= budget:\n                    break\n                # if this individual had no evaluation (possible if budget tiny), skip it\n                if not np.isfinite(fvals[idx]):\n                    continue\n\n                # jDE-like sampling of Fi and CRi around means\n                Fi = rng.normal(F_mean, 0.1)\n                Fi = np.clip(Fi, 0.05, 0.95)\n                CRi = rng.normal(CR_mean, 0.1)\n                CRi = np.clip(CRi, 0.0, 1.0)\n\n                # choose whether to do a Lévy jump mutation (global heavy-tail) or DE mutation\n                if rng.rand() < p_levy:\n                    # Lévy-like jump centered on best with Cauchy steps scaled by trust and range\n                    # Cauchy has heavy tails; limit extremes\n                    cauchy = rng.standard_cauchy(size=dim)\n                    cauchy = np.clip(cauchy, -1e3, 1e3)\n                    step_scale = Fi * trust_radius  # use Fi as extra multiplier\n                    donor = self.x_opt + (step_scale * range_vec) * cauchy\n                    # add a bit of diversity by mixing with a random individual\n                    j = rng.randint(pop_size)\n                    donor = 0.7 * donor + 0.3 * pop[j]\n                else:\n                    # standard DE rand/1 mutation: pick three distinct indices i!=r1!=r2!=r3\n                    idxs = np.arange(pop_size)\n                    # ensure we have at least 3 other candidates\n                    choices = np.setdiff1d(idxs, np.array([idx]))\n                    if choices.size < 3:\n                        # fallback: small gaussian perturbation around current\n                        donor = pop[idx] + Fi * (rng.randn(dim) * range_vec * 0.01)\n                    else:\n                        r = rng.choice(choices, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cross_mask = rng.rand(dim) < CRi\n                # ensure at least one dimension is taken from donor\n                if not np.any(cross_mask):\n                    cross_mask[rng.randint(dim)] = True\n                trial = np.where(cross_mask, donor, pop[idx])\n\n                # projection to bounds (clip to box)\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # Evaluate candidate (respect budget)\n                if evals >= budget:\n                    break\n                f_trial = float(func(trial))\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_trial < fvals[idx]:\n                    # accept\n                    pop[idx] = trial\n                    fvals[idx] = f_trial\n                    successes += 1\n                    succ_F.append(Fi)\n                    succ_CR.append(CRi)\n                    # Minor online nudging of means towards successful Fi/CRi (exponential moving average)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # small random drift to means to maintain diversity\n                    F_mean = 0.999 * F_mean + 0.001 * Fi\n                    CR_mean = 0.999 * CR_mean + 0.001 * CRi\n\n                # Update global best if improved\n                if fvals[idx] < self.f_opt:\n                    self.f_opt = fvals[idx]\n                    self.x_opt = pop[idx].copy()\n\n                # stagnation tracking\n                if self.f_opt < last_best - 1e-12:\n                    last_best = self.f_opt\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # small adaptation of Lévy probability depending on recent improvements\n                if successes > 0:\n                    p_levy = max(0.01, p_levy * 0.995)\n                else:\n                    p_levy = min(0.5, p_levy * 1.002)\n\n                # If budget exhausted, break early\n                if evals >= budget:\n                    break\n\n            # End of generation: maybe apply trust-region local search around the best\n            if evals >= budget:\n                break\n\n            # Local sampling budget is small and scaled by dimension and remaining budget\n            remaining = budget - evals\n            local_samples = min(remaining, max(2, int(1 + 1.0 * dim * 0.6)))\n            # trust radius is relative to range_vec; get absolute sigma per-dim\n            abs_trust = np.maximum(trust_radius * range_vec, 1e-12)\n\n            local_successes = 0\n            for _ in range(local_samples):\n                if evals >= budget:\n                    break\n                # anisotropic gaussian noise scaled by abs_trust\n                step = rng.randn(dim) * abs_trust\n                local_candidate = self.x_opt + step\n                local_candidate = np.minimum(np.maximum(local_candidate, lb), ub)\n                f_local = float(func(local_candidate))\n                evals += 1\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = local_candidate.copy()\n                    local_successes += 1\n                    # shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n                else:\n                    # expand slightly to escape if no improvement\n                    trust_radius *= 1.05\n                    trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # adapt DE means based on successes collected in this generation (simple Lehmer-style)\n            if len(succ_F) > 0:\n                # Lehmer mean encourages larger successful F\n                lev_num = np.sum(np.array(succ_F) ** 2)\n                lev_den = np.sum(succ_F)\n                if lev_den > 0:\n                    F_mean = 0.85 * F_mean + 0.15 * (lev_num / lev_den)\n                CR_mean = 0.85 * CR_mean + 0.15 * (np.mean(succ_CR) if len(succ_CR) else CR_mean)\n\n            # stagnation-based diversification\n            if stagnation_counter > max(50, dim * 5):\n                # re-seed half of the worst individuals randomly (diversify)\n                k = max(1, pop_size // 2)\n                worst_idx = np.argsort(fvals)[-k:]\n                for j in worst_idx:\n                    if evals >= budget:\n                        break\n                    pop[j] = lb + rng.rand(dim) * range_vec\n                    # evaluate new individuals if budget allows\n                    f_new = float(func(pop[j]))\n                    evals += 1\n                    fvals[j] = f_new\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = pop[j].copy()\n                # enlarge trust radius a bit to escape basin\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # slightly increase probability of Lévy jumps\n                p_levy = min(0.5, p_levy * 1.2)\n\n            # safety: enforce bounds and repair any NaNs\n            pop = np.minimum(np.maximum(pop, lb), ub)\n            pop = np.nan_to_num(pop, nan=lb + 0.5 * range_vec)\n\n        # Final results\n        # Ensure x_opt is not None: pick best from evaluated population if needed\n        if self.x_opt is None:\n            finite = np.isfinite(fvals)\n            if np.any(finite):\n                ib = np.argmin(fvals)\n                self.x_opt = pop[ib].copy()\n                self.f_opt = fvals[ib]\n\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.341 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16834410228508, 0.23492255727313194, 0.36398993470094376, 0.4982316053710243, 0.3461932829418445, 0.4732863315276302, 0.347336717499208, 0.3689536730711259, 0.3097630798364308, 0.30265422716236756]}, "task_prompt": ""}
{"id": "f8f02d8c-99c7-40c6-8694-4a0a800874f9", "fitness": 0.5068359180650965, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size adaptation and population reseeding to maintain exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # population size heuristic: grows with dim but bounded by budget\n        if pop_size is None:\n            p = max(6, min(40, int(6 + 2 * self.dim)))\n            # ensure not larger than a fraction of budget\n            p = min(p, max(4, self.budget // 5))\n            self.pop_size = p\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        a = np.asarray(b, dtype=float)\n        if a.ndim == 0:\n            a = np.full(self.dim, a)\n        return a\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # fallback if bounds unspecified or scalar mismatch\n        if lb.size != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n        if ub.size != self.dim:\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * (range_vec)\n        fvals = np.full(self.pop_size, np.inf)\n        valid = np.zeros(self.pop_size, dtype=bool)\n\n        evals = 0\n        # evaluate initial population but do not exceed budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            fvals[i] = f\n            valid[i] = True\n            evals += 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # if none evaluated (very tiny budget), return current best (inf)\n        if not np.any(valid):\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.3\n        step_scale = 0.25  # base scale for Lévy and trust radius\n        trust_radius = 0.5  # fraction of search range\n        p_levy = 0.02  # base chance for a long jump\n        stagnation_limit = max(20, self.budget // 20)\n        no_improve_count = 0\n\n        # helper: generate Levy-like heavy-tailed step using truncated Cauchy\n        def levy_step():\n            # sample per-dimension from Cauchy (location 0, scale 1), clipped\n            # produce vector with typical size ~1 but heavy tails\n            raw = self.rng.standard_cauchy(self.dim)\n            # clip extreme outliers but keep heavy tail\n            raw = np.clip(raw, -25.0, 25.0)\n            # normalize then scale by heavy factor\n            s = np.abs(raw).mean() + 1e-12\n            step = raw / s\n            return step\n\n        # helper to sample Fi and CRi (jDE-like small adaptation)\n        def sample_F_CR():\n            # sample Fi from log-normal-like symmetric around F_mean with bounds\n            Fi = self.rng.normal(F_mean, 0.1)\n            Fi = np.clip(Fi, 0.1, 0.9)\n            CRi = self.rng.normal(CR_mean, 0.2)\n            CRi = np.clip(CRi, 0.0, 1.0)\n            return Fi, CRi\n\n        # main loop: iterate generating candidates until budget exhausted\n        # We will sequentially process candidate generations\n        successful_F = []\n        successful_CR = []\n\n        # convenience list of indices that are valid/populated\n        def valid_indices():\n            return np.where(valid)[0]\n\n        # small utility to enforce bounds by reflection then clip\n        def bound_project(x):\n            # reflection for moderate violations\n            y = x.copy()\n            for k in range(self.dim):\n                if y[k] < lb[k]:\n                    y[k] = lb[k] + (lb[k] - y[k])\n                    if y[k] > ub[k]:\n                        y[k] = lb[k] + self.rng.random() * (ub[k] - lb[k])\n                elif y[k] > ub[k]:\n                    y[k] = ub[k] - (y[k] - ub[k])\n                    if y[k] < lb[k]:\n                        y[k] = lb[k] + self.rng.random() * (ub[k] - lb[k])\n            # final clip\n            np.clip(y, lb, ub, out=y)\n            return y\n\n        # main while loop until budget consumed\n        # We'll repeatedly loop over population indices to propose candidates\n        pop_order = np.arange(self.pop_size)\n        while evals < self.budget:\n            # shuffle order each pass to reduce bias\n            self.rng.shuffle(pop_order)\n            for i in pop_order:\n                if evals >= self.budget:\n                    break\n                # skip individuals that were never evaluated (if any)\n                if not valid[i]:\n                    # if we have budget and a not-evaluated individual, evaluate it\n                    if evals < self.budget:\n                        x = pop[i]\n                        try:\n                            f = func(x)\n                        except Exception:\n                            f = np.inf\n                        fvals[i] = f\n                        valid[i] = True\n                        evals += 1\n                        if f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = x.copy()\n                            no_improve_count = 0\n                        else:\n                            no_improve_count += 1\n                    continue\n\n                # Adapt chance of Levy with stagnation\n                p_use_levy = min(0.5, p_levy * (1.0 + no_improve_count / max(10, stagnation_limit)))\n                do_levy = (self.rng.random() < p_use_levy)\n                Fi, CRi = sample_F_CR()\n\n                # Choose strategy: Levy long jump or DE variant\n                if do_levy:\n                    # long jump centered on current best\n                    best_idx = int(np.argmin(fvals[valid]))\n                    best = pop[valid_indices()[best_idx]]\n                    step = levy_step()\n                    # scale with trust radius and problem range to produce a candidate far away\n                    candidate = best + (step * (step_scale * 3.0) * trust_radius) * (range_vec / range_norm)\n                    # small random local perturbation as well\n                    candidate += self.rng.normal(0, 0.02 * trust_radius, size=self.dim) * range_vec\n                    candidate = bound_project(candidate)\n                else:\n                    # Differential Evolution mutation + crossover\n                    # choose three distinct indices different from i and having evaluations\n                    idxs = valid_indices()\n                    if idxs.size < 4:\n                        # fallback to local gaussian around best\n                        best_idx = int(np.argmin(fvals[valid]))\n                        best = pop[valid_indices()[best_idx]]\n                        candidate = best + self.rng.normal(0, 0.3 * trust_radius, size=self.dim) * range_vec\n                        candidate = bound_project(candidate)\n                    else:\n                        # ensure we pick indices distinct from i\n                        choices = idxs[idxs != i]\n                        # sample r1, r2, r3\n                        r = self.rng.choice(choices, size=3, replace=False)\n                        x_r1, x_r2, x_r3 = pop[r[0]], pop[r[1]], pop[r[2]]\n                        # base vector: rand/1 + current-to-best mixing for exploitation\n                        best_idx = int(np.argmin(fvals[valid]))\n                        best = pop[valid_indices()[best_idx]]\n                        # mutation\n                        v = x_r1 + Fi * (x_r2 - x_r3) + 0.2 * Fi * (best - pop[i])\n                        # crossover binomial\n                        jrand = self.rng.integers(0, self.dim)\n                        mask = (self.rng.random(self.dim) < CRi)\n                        mask[jrand] = True\n                        trial = np.where(mask, v, pop[i])\n                        candidate = bound_project(trial)\n\n                # one evaluation\n                try:\n                    f_cand = func(candidate)\n                except Exception:\n                    f_cand = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                if f_cand < fvals[i]:\n                    # success\n                    pop[i] = candidate\n                    fvals[i] = f_cand\n                    valid[i] = True\n                    successful_F.append(Fi)\n                    successful_CR.append(CRi)\n                    # move global best pointer\n                    if f_cand < self.f_opt:\n                        self.f_opt = f_cand\n                        self.x_opt = candidate.copy()\n                        no_improve_count = 0\n                    else:\n                        no_improve_count += 1\n                    # small adaptation: shrink trust region upon consistent success\n                    trust_radius = max(1e-6, trust_radius * 0.98)\n                else:\n                    # failed candidate\n                    no_improve_count += 1\n                    # slight exploration increase on failure\n                    trust_radius = min(2.0, trust_radius * 1.001)\n\n                # update means if we have some successes\n                if len(successful_F) >= 5:\n                    # Lehmer mean (gives more weight to larger Fi)\n                    sF = np.array(successful_F[-20:])\n                    if sF.sum() > 0:\n                        F_mean = (np.sum(sF ** 2) / (np.sum(sF) + 1e-12))\n                        F_mean = np.clip(F_mean, 0.1, 0.9)\n                    sCR = np.array(successful_CR[-20:])\n                    if sCR.size > 0:\n                        CR_mean = np.clip(sCR.mean(), 0.0, 1.0)\n\n                # occasional clean-up of local names\n                if 'Fi' in locals():\n                    pass  # noop, kept for compatibility\n\n                # break if budget is exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of a population pass: perform trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # calculate local samples (small handful scaled by dim)\n            local_samples = min(remaining, max(2, int(round(self.dim * 0.7))))\n            best_idx = int(np.argmin(fvals[valid]))\n            best = pop[valid_indices()[best_idx]]\n\n            successful_local = 0\n            for _ in range(local_samples):\n                # anisotropic sigma: base trust_radius scaled by per-dim random\n                sigma = (0.25 + self.rng.random(self.dim) * 0.75) * trust_radius\n                # sample anisotropic gaussian around best\n                perturb = self.rng.normal(0.0, sigma, size=self.dim) * (range_vec / np.maximum(range_norm, 1e-12))\n                candidate = bound_project(best + perturb)\n                try:\n                    f_cand = func(candidate)\n                except Exception:\n                    f_cand = np.inf\n                evals += 1\n                if f_cand < fvals[valid_indices()[best_idx]]:\n                    # accepted: replace worst individual to preserve population diversity\n                    worst_idx = int(np.argmax(fvals))\n                    pop[worst_idx] = candidate\n                    fvals[worst_idx] = f_cand\n                    valid[worst_idx] = True\n                    successful_local += 1\n                    if f_cand < self.f_opt:\n                        self.f_opt = f_cand\n                        self.x_opt = candidate.copy()\n                        no_improve_count = 0\n                    else:\n                        no_improve_count += 1\n                else:\n                    no_improve_count += 1\n\n                if evals >= self.budget:\n                    break\n\n            # adjust trust radius depending on local success rate\n            if local_samples > 0:\n                succ_rate = successful_local / local_samples\n                if succ_rate > 0.2:\n                    # focus more (shrink)\n                    trust_radius = max(1e-6, trust_radius * (0.9 - 0.1 * succ_rate))\n                    # move F_mean & CR_mean slightly toward exploitation\n                    F_mean = np.clip(0.9 * F_mean + 0.1 * 0.4, 0.1, 0.9)\n                    CR_mean = np.clip(0.9 * CR_mean + 0.1 * 0.9, 0.0, 1.0)\n                else:\n                    # expand region a bit\n                    trust_radius = min(2.0, trust_radius * (1.02 + 0.5 * (1 - succ_rate)))\n\n            # stagnation handling: if many consecutive no improvements, increase Levy chance and partially re-seed\n            if no_improve_count > stagnation_limit:\n                # amplify long jump probability\n                p_levy = min(0.5, p_levy * 1.5)\n                # re-seed a portion of the population to diversify\n                n_reseed = max(1, self.pop_size // 2)\n                to_reseed = self.rng.choice(np.arange(self.pop_size), size=n_reseed, replace=False)\n                for idx in to_reseed:\n                    newx = lb + self.rng.random(self.dim) * range_vec\n                    pop[idx] = newx\n                    fvals[idx] = np.inf\n                    valid[idx] = False\n                # slightly enlarge trust radius to escape basin\n                trust_radius = min(2.0, trust_radius * 1.5)\n                no_improve_count = 0  # reset counter after reseed\n\n        # Final check: if some individuals were never evaluated (shouldn't matter), ensure best known returned\n        if self.x_opt is None:\n            # pick best among evaluated\n            if np.any(valid):\n                idx = int(np.argmin(fvals[valid]))\n                self.x_opt = pop[valid_indices()[idx]].copy()\n                self.f_opt = fvals[valid_indices()[idx]]\n            else:\n                # fallback random\n                self.x_opt = lb + self.rng.random(self.dim) * range_vec\n                try:\n                    self.f_opt = func(self.x_opt)\n                except Exception:\n                    self.f_opt = np.inf\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.507 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1617656278063878, 0.2604789629167096, 0.6594287514561374, 0.7034388914703735, 0.8400306223025439, 0.8313376442315241, 0.26315875192009464, 0.272800022950347, 0.8312024145197903, 0.2447174910770581]}, "task_prompt": ""}
{"id": "6a221e5e-d780-46eb-ab9f-4127ef0c7956", "fitness": 0.2403281946567783, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style parameter adaptation, occasional Lévy-flight jumps for long-range exploration, and a trust-region local search around the current best (fast global exploration + focused local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history (jDE-like).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        rng = np.random.RandomState(seed)\n        self.rng = rng\n\n        # Bounds (BBOB style is [-5,5])\n        self.lb = -5.0\n        self.ub = 5.0\n        self.range = self.ub - self.lb\n\n        # population size: scale with dimension but limited by budget\n        if pop_size is None:\n            # base on dim but keep reasonable relative to budget\n            p = max(8, int(4 + 2 * self.dim))\n            p = min(p, max(4, self.budget // 4))\n            pop_size = p\n        self.pop_size = max(4, int(pop_size))\n\n        # algorithm state placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Ensure bounds can be broadcast to (dim,)\n        b = np.asarray(b)\n        if b.shape == ():\n            return np.full((self.dim,), float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        raise ValueError(\"Bounds must be scalar or length dim\")\n\n    def __call__(self, func):\n        # sanity\n        if self.budget <= 0:\n            return np.inf, None\n\n        rng = self.rng\n        dim = self.dim\n        pop_size = self.pop_size\n\n        # Initialize population uniformly in bounds\n        pop = rng.uniform(self.lb, self.ub, size=(pop_size, dim))\n\n        # Evaluate initial population (respect budget)\n        fitness = np.full(pop_size, np.inf)\n        evals = 0\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            fitness[i] = f\n            evals += 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n        # If budget was tiny and we couldn't evaluate whole pop, leave rest eval=inf.\n        # They will be used later if budget available.\n\n        # jDE-style parameter adaptation initial means\n        F_mean = 0.6\n        CR_mean = 0.2\n        tau_F = 0.1\n        tau_CR = 0.1\n\n        # trust-region radius (fraction of range)\n        trust_radius = 1.0  # in same units as variable range\n        trust_min = 1e-6\n        trust_max = 5.0 * self.range\n\n        # stagnation detection\n        since_improve = 0\n        stagnation_limit = max(100, 20 * dim)\n\n        # counters for adapting\n        adapt_factor = 0.1\n\n        # base probability of Levy jump\n        base_levy_prob = 0.02\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step(scale=1.0):\n            # Cauchy (stable heavy tail). Scale to variable range and trust radius.\n            # We cap extremes to avoid numerical blow-up.\n            # Using location 0, scale small; then multiply.\n            # Per-dimension independent cauchy sample\n            step = rng.standard_cauchy(size=dim) * 0.5  # reduce raw extremes\n            # cap extreme values\n            limit = 10.0\n            step = np.clip(step, -limit, limit)\n            return step * scale\n\n        # main loop\n        generation = 0\n        # maintain success lists for adaptation\n        while evals < self.budget:\n            generation += 1\n            # prepare per-generation success storage\n            successful_F = []\n            successful_CR = []\n            successes = 0\n\n            # Shuffle indices to avoid bias\n            idxs = np.arange(pop_size)\n            rng.shuffle(idxs)\n\n            for idx in idxs:\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[idx].copy()\n                f_target = fitness[idx]\n\n                # sample per-individual F and CR (jDE-style)\n                if rng.rand() < tau_F:\n                    F_i = 0.1 + 0.9 * rng.rand()\n                else:\n                    F_i = F_mean\n                # ensure in (0,2)\n                F_i = np.clip(F_i, 0.01, 1.9)\n\n                if rng.rand() < tau_CR:\n                    CR_i = rng.rand()\n                else:\n                    CR_i = CR_mean\n                CR_i = np.clip(CR_i, 0.0, 1.0)\n\n                # Decide whether to perform a Levy jump or DE mutation.\n                # Levy prob grows with stagnation\n                stagnation_factor = min(5.0, since_improve / max(1, stagnation_limit))\n                p_levy = base_levy_prob * (1.0 + stagnation_factor)\n                use_levy = (rng.rand() < p_levy)\n\n                if use_levy and (self.x_opt is not None):\n                    # Lévy centered on best for exploration\n                    scale = 0.2 * trust_radius + 0.05 * self.range\n                    step = levy_step(scale=scale)\n                    v = self.x_opt + step\n                    # occasionally add a DE-like perturbation to maintain diversity\n                    if rng.rand() < 0.3:\n                        # one rand/1 perturbation using other members\n                        r1, r2, r3 = rng.choice(pop_size, 3, replace=False)\n                        v = pop[r1] + F_i * (pop[r2] - pop[r3])\n                else:\n                    # DE/rand/1 mutation with optional best influence\n                    # pick distinct indices r1, r2, r3 != idx\n                    all_idx = np.arange(pop_size)\n                    # fallback in case pop_size < 4\n                    if pop_size > 3:\n                        choices = np.setdiff1d(all_idx, idx)\n                        r = rng.choice(choices, size=3, replace=False)\n                        r1, r2, r3 = r\n                    else:\n                        # allow replacement if too small\n                        r1, r2, r3 = rng.randint(0, pop_size, size=3)\n                    base_vec = pop[r1]\n                    diff = pop[r2] - pop[r3]\n                    # mix in best influence occasionally proportional to stagnation\n                    best_influence = 0.25 * min(1.0, stagnation_factor)\n                    v = base_vec + F_i * diff + best_influence * (self.x_opt - base_vec if self.x_opt is not None else 0.0)\n\n                # Binomial crossover\n                jrand = rng.randint(dim)\n                trial = np.empty(dim)\n                mask = rng.rand(dim) < CR_i\n                mask[jrand] = True\n                trial[mask] = v[mask]\n                trial[~mask] = x_target[~mask]\n\n                # project to bounds\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # evaluate candidate (if budget allows)\n                if evals >= self.budget:\n                    break\n                try:\n                    f_trial = func(trial)\n                except Exception:\n                    f_trial = np.inf\n                evals += 1\n\n                # selection: greedy\n                if f_trial <= f_target:\n                    # replace\n                    pop[idx] = trial\n                    fitness[idx] = f_trial\n                    # record for adaptation\n                    successful_F.append(F_i)\n                    successful_CR.append(CR_i)\n                    successes += 1\n\n                    # update global best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        since_improve = 0\n                    else:\n                        since_improve += 1\n                else:\n                    # no replacement\n                    since_improve += 1\n\n                # early break if budget consumed\n                if evals >= self.budget:\n                    break\n\n            # End of generation: adapt F_mean and CR_mean using successful lists\n            if len(successful_F) > 0:\n                # Lehmer-like update gives more weight to large F\n                mean_F = np.mean(successful_F)\n                F_mean = (1 - adapt_factor) * F_mean + adapt_factor * mean_F\n            else:\n                # nudge F_mean slightly to encourage exploration if no success\n                F_mean = F_mean * (1.0 + 0.01 * rng.randn())\n\n            if len(successful_CR) > 0:\n                mean_CR = np.mean(successful_CR)\n                CR_mean = (1 - adapt_factor) * CR_mean + adapt_factor * mean_CR\n            else:\n                CR_mean = CR_mean + 0.01 * (rng.rand() - 0.5)\n\n            F_mean = float(np.clip(F_mean, 0.01, 1.9))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n            # Trust-region local search around best: a few Gaussian samples\n            if evals < self.budget and self.x_opt is not None:\n                # number of local samples limited by remaining budget and dimension\n                remaining = self.budget - evals\n                n_local = min(max(1, dim // 2), remaining, 2 + dim // 4)\n                any_local_success = False\n                for _ in range(n_local):\n                    # anisotropic sigma: base trust_radius scaled randomly per-dim\n                    sigma = max(1e-12, 0.5 * (trust_radius / max(1.0, self.range))) * self.range\n                    anis = sigma * (0.2 + 0.8 * rng.rand(dim))\n                    candidate = self.x_opt + rng.normal(scale=anis)\n                    candidate = np.clip(candidate, self.lb, self.ub)\n                    # evaluate\n                    if evals >= self.budget:\n                        break\n                    try:\n                        f_c = func(candidate)\n                    except Exception:\n                        f_c = np.inf\n                    evals += 1\n                    if f_c < self.f_opt:\n                        self.f_opt = f_c\n                        self.x_opt = candidate.copy()\n                        any_local_success = True\n                        since_improve = 0\n                    else:\n                        since_improve += 1\n                    # optional injection: replace worst individual with good local sample\n                    if f_c < np.max(fitness):\n                        worst = np.argmax(fitness)\n                        pop[worst] = candidate\n                        fitness[worst] = f_c\n\n                # adjust trust radius based on local success\n                if any_local_success:\n                    # shrink to focus\n                    trust_radius = max(trust_min, trust_radius * 0.7)\n                    # reward exploitation: nudge F_mean downward a bit (favor smaller steps)\n                    F_mean = max(0.01, F_mean * 0.95)\n                else:\n                    # expand to escape\n                    trust_radius = min(trust_max, trust_radius * 1.15)\n                    # encourage exploration: nudge F_mean upward slightly\n                    F_mean = min(1.9, F_mean * 1.02)\n\n            # adjust long-jump probability by stagnation\n            if since_improve > stagnation_limit:\n                # strong stagnation: reseed half of the population\n                half = max(1, pop_size // 2)\n                for k in range(half):\n                    if evals >= self.budget:\n                        break\n                    i = rng.randint(0, pop_size)\n                    pop[i] = rng.uniform(self.lb, self.ub, size=dim)\n                    # evaluate reinitialized individuals if budget allows\n                    try:\n                        f_new = func(pop[i])\n                    except Exception:\n                        f_new = np.inf\n                    fitness[i] = f_new\n                    evals += 1\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = pop[i].copy()\n                        since_improve = 0\n                # after reseed, slightly enlarge trust radius to explore\n                trust_radius = min(trust_max, trust_radius * 1.5)\n                # relax stagnation counter a bit\n                since_improve = since_improve // 2\n\n            # Minor diversity injection if very few successes\n            if successes == 0 and rng.rand() < 0.05:\n                # perturb a random individual\n                i = rng.randint(0, pop_size)\n                pop[i] = pop[i] + rng.normal(scale=0.1 * self.range, size=dim)\n                pop[i] = np.clip(pop[i], self.lb, self.ub)\n                # do not immediately evaluate unless budget allows in next iteration\n\n            # If no global best (all inf), seed a random evaluated point if possible\n            if self.x_opt is None and evals < self.budget:\n                candidate = rng.uniform(self.lb, self.ub, size=dim)\n                try:\n                    f_c = func(candidate)\n                except Exception:\n                    f_c = np.inf\n                evals += 1\n                if f_c < self.f_opt:\n                    self.f_opt = f_c\n                    self.x_opt = candidate.copy()\n\n            # small safeguard to not loop infinitely\n            if generation > 100000 and evals < self.budget:\n                # fallback: random sampling of remaining budget\n                while evals < self.budget:\n                    candidate = rng.uniform(self.lb, self.ub, size=dim)\n                    try:\n                        f_c = func(candidate)\n                    except Exception:\n                        f_c = np.inf\n                    evals += 1\n                    if f_c < self.f_opt:\n                        self.f_opt = f_c\n                        self.x_opt = candidate.copy()\n                break\n\n        # final result\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()\n\n\n# Example usage (not run here):\n# opt = HybridDeLevyTrust(budget=1000, dim=10, seed=42)\n# fbest, xbest = opt(func)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.240 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09420800635964044, 0.15948757124829038, 0.3586375929892337, 0.17149474033319612, 0.23523976291163273, 0.3656910816278951, 0.23681761622977293, 0.23366286503874756, 0.3855360693084716, 0.16250664052090247]}, "task_prompt": ""}
{"id": "c2e3c864-660e-400b-838a-18a45c471015", "fitness": 0.3815128525019424, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online step-size and operator adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but kept within sensible bounds\n        if pop_size is None:\n            default = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # ensure population not larger than a fraction of budget\n            self.pop_size = min(default, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # bounds from func (BBOB-style). If not present, default to [-5, 5]\n        try:\n            lb_raw = func.bounds.lb\n            ub_raw = func.bounds.ub\n        except Exception:\n            lb_raw = -5.0\n            ub_raw = 5.0\n\n        lb = self._ensure_array_bounds(lb_raw)\n        ub = self._ensure_array_bounds(ub_raw)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        mean_range = max(range_vec.mean(), 1e-12)\n\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population sequentially (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # determine initial best\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals[valid])\n            best_indices = np.where(valid)[0]\n            best_idx = best_indices[best_idx]\n            best_f = float(fvals[best_idx])\n            best_x = pop[best_idx].copy()\n        else:\n            # cannot evaluate anything\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probabilty mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * range_norm if range_norm > 0 else 0.2\n        min_trust = 1e-6\n        max_trust = max(1e-6, range_norm * 2.0)\n\n        stagnation_counter = 0\n        gen = 0\n        max_no_improve_for_reset = max(10, self.pop_size // 2)\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            Fi_success = []\n            CRi_success = []\n            improved_in_gen = False\n\n            # per-individual operations like DE\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                used_Fi = None\n                used_CRi = None\n\n                if rng.rand() < p_levy:\n                    # Lévy jump centred on current best\n                    step = levy_step()\n                    scale = step_scale * (0.5 + 0.5 * rng.rand())\n                    donor = best_x + scale * step * (range_vec / mean_range)\n                else:\n                    # DE/rand/1/bin style mutation and crossover\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback to random jump if population too small\n                        donor = lb + rng.rand(self.dim) * range_vec\n                    else:\n                        r = rng.choice(idxs, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        # sample Fi and CRi around means\n                        Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n                        used_Fi = Fi\n                        used_CRi = CRi\n                        mutant = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = rng.rand(self.dim) < CRi\n                        # ensure at least one dimension from mutant\n                        cr_mask[rng.randint(0, self.dim)] = True\n                        donor = np.where(cr_mask, mutant, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection - greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    improved_in_gen = True\n                    if used_Fi is not None:\n                        Fi_success.append(used_Fi)\n                        CRi_success.append(used_CRi if used_CRi is not None else CR_mean)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                # no else here; we count stagnation globally below\n\n            # End of generation: adapt means if successes observed\n            if successes > 0 and len(Fi_success) > 0:\n                # Lehmer-like adaptation: give more weight to larger Fi if they perform better\n                Fi_avg = float(np.mean(Fi_success))\n                CRi_avg = float(np.mean(CRi_success))\n                # move means slightly toward averages of successful parameters\n                F_mean = 0.85 * F_mean + 0.15 * Fi_avg\n                CR_mean = 0.85 * CR_mean + 0.15 * CRi_avg\n                # reduce exploration probability if many successes\n                if successes > max(1, int(0.2 * self.pop_size)):\n                    p_levy = max(0.01, p_levy * 0.95)\n                else:\n                    p_levy = max(0.01, p_levy * 0.98)\n            else:\n                # no successful DE replacements: encourage larger jumps & increase Fi slightly\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.03, 0.05, 0.99)\n                stagnation_counter += 1\n\n            # trust-region local search around best: small Gaussian perturbations\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma scaled by trust_radius relative to range\n                sigma = (0.4 + rng.rand(self.dim) * 0.6) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step -> tighten trust radius\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                    improved_in_gen = True\n                else:\n                    # unsuccessful -> relax trust region a bit\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # stagnation handling: if many generations without improvement, re-seed half population\n            if stagnation_counter >= max_no_improve_for_reset:\n                k = max(1, self.pop_size // 2)\n                sel = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in sel:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # after reset, encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                p_levy = min(0.5, p_levy * 1.2 + 0.02)\n                stagnation_counter = 0\n\n            # small safety adjustments to keep means in sensible ranges\n            F_mean = float(np.clip(F_mean, 0.05, 0.99))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n            # final check for termination by evals will happen in loop condition\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.382 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1703017331911596, 0.31723384300423263, 0.36114821430807587, 0.46176307986096454, 0.4869034253231651, 0.4866203621570946, 0.300832712834329, 0.308009611749141, 0.5125068389208929, 0.4098087036703686]}, "task_prompt": ""}
{"id": "055d84dd-051a-40ba-997b-19808c9fa81d", "fitness": 0.6665386540607078, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution (jDE-style) with occasional Lévy-flight jumps for global escapes and an adaptive trust-region local search around the current best (fast global exploration, focused local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (jDE-like),\n    occasional Lévy-flight jumps for long-range exploration, and an\n    adaptive trust-region local search around the current best.\n    - Budgeted (counts function evaluations precisely).\n    - Works for continuous box-bounded problems (broadcast lb/ub).\n    - Adaptation: per-individual F and CR (jDE-style) and global p_levy/trust radius.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None,\n                 p_levy_init=0.08, trust_fraction=0.15):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size heuristic\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # algorithm state\n        self.f_opt = np.inf\n        self.x_opt = None\n        # tunables\n        self.p_levy = float(p_levy_init)\n        self.trust_fraction = float(trust_fraction)\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # get bounds from func and ensure arrays\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # safety: if bounds are identical or degenerate, fix tiny range\n        range_vec = ub - lb\n        range_vec = np.where(range_vec == 0.0, 1e-12, range_vec)\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        # per-individual control parameters (jDE-like)\n        F_i = np.full(self.pop_size, 0.6)\n        CR_i = np.full(self.pop_size, 0.9)\n        tau_F = 0.1\n        tau_CR = 0.1\n\n        # trust region scalar (absolute scale relative to range magnitude)\n        global_scale = np.linalg.norm(range_vec) / np.sqrt(max(1, self.dim))\n        trust_radius = max(1e-9, self.trust_fraction * global_scale)\n        min_trust = 1e-9\n        max_trust = 2.0 * global_scale\n\n        evals = 0\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if no evaluations possible return\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # find best\n        best_idx = np.argmin(fvals)\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        gen = 0\n        stagnation_counter = 0\n        last_improve_eval = evals\n\n        # helper: Levy step using Cauchy (heavy tail), clipped\n        def levy_step():\n            s = self.rng.standard_cauchy(self.dim)\n            # clip extremely large leaps\n            s = np.clip(s, -1e3, 1e3)\n            # normalize to unit typical scale\n            s = s / (np.linalg.norm(s) + 1e-12)\n            return s\n\n        # main generational loop but limited by budget (we may break mid-generation)\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # randomize order to avoid positional bias\n            order = self.rng.permutation(self.pop_size)\n\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[idx]\n                # jDE style update of Fi and CRi with small probabilities\n                if self.rng.rand() < tau_F:\n                    Fi = 0.1 + 0.9 * self.rng.rand()\n                    F_i[idx] = Fi\n                else:\n                    Fi = F_i[idx]\n\n                if self.rng.rand() < tau_CR:\n                    CRi = self.rng.rand()\n                    CR_i[idx] = CRi\n                else:\n                    CRi = CR_i[idx]\n\n                # decide between Levy jump or DE mutation\n                if self.rng.rand() < self.p_levy:\n                    # Lévy jump centered on best (global exploration)\n                    step = levy_step()\n                    scale = (0.2 + 0.8 * self.rng.rand()) * (trust_radius / (global_scale + 1e-12))\n                    donor = best_x + step * scale * range_vec\n                else:\n                    # classic DE/rand/1 mutation\n                    idxs = [i for i in range(self.pop_size) if i != idx]\n                    r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover (ensure at least one component from donor)\n                cross = self.rng.rand(self.dim) < CRi\n                jrand = self.rng.randint(self.dim)\n                cross[jrand] = True\n                trial = np.where(cross, donor, x_target)\n\n                # project to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate (count)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection\n                if f_candidate <= fvals[idx]:\n                    # accept\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    # update per-individual parameters toward Fi/CRi used (jDE success learning)\n                    F_i[idx] = Fi\n                    CR_i[idx] = CRi\n                    # inform global means (light smoothing)\n                    # slightly nudge p_levy down if this was DE success (exploit)\n                    if self.p_levy > 0.01 and (self.rng.rand() > 0.2):\n                        self.p_levy = max(0.01, self.p_levy * 0.995)\n                else:\n                    # unsuccessful: small perturbation back (optional) - we keep x_target\n                    pass\n\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    last_improve_eval = evals\n                    # shrink trust region to intensify local search\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    # if not improved, tiny expansion tendency for exploration\n                    trust_radius = min(max_trust, trust_radius * 1.001)\n\n                # stagnation logic\n                if evals - last_improve_eval > max(10, int(0.05 * self.budget)):\n                    stagnation_counter += 1\n                else:\n                    stagnation_counter = 0\n\n                # occasional small in-place local polish when budget remaining low (decrease to not abuse budget)\n                if (self.rng.rand() < 0.02) and (evals < self.budget):\n                    # one gaussian perturb around candidate\n                    sigma = trust_radius * (0.3 + 0.7 * self.rng.rand())\n                    cand2 = np.clip(candidate + self.rng.normal(0, 1.0, size=self.dim) * (sigma * range_vec), lb, ub)\n                    if evals < self.budget:\n                        f_cand2 = float(func(cand2))\n                        evals += 1\n                        if f_cand2 < fvals[idx]:\n                            pop[idx] = cand2\n                            fvals[idx] = f_cand2\n                            if f_cand2 < best_f:\n                                best_f = f_cand2\n                                best_x = cand2.copy()\n                                last_improve_eval = evals\n                                trust_radius = max(min_trust, trust_radius * 0.85)\n\n                # early break if budget used\n                if evals >= self.budget:\n                    break\n\n            # End of generation: perform trust-region local sampling around best\n            remaining = self.budget - evals\n            # number of local samples: small handful scaled by dim and remaining budget\n            local_samples = min(max(1, self.dim // 4), remaining, max(1, self.pop_size // 4))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (0.5 + self.rng.rand(self.dim) * 0.5) * trust_radius / (global_scale + 1e-12)\n                candidate = best_x + self.rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.80)\n                    last_improve_eval = evals\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.06)\n\n                if evals >= self.budget:\n                    break\n\n            # Adapt global exploration probability based on recent successes\n            if successes > max(1, 0.05 * self.pop_size):\n                # many successes -> rely on DE/exploitation\n                self.p_levy = max(0.01, self.p_levy * 0.95)\n            else:\n                # few successes -> increase chance of long jumps\n                self.p_levy = min(0.5, self.p_levy * (1.02 + 0.01 * (stagnation_counter > 5)))\n\n            # Strong stagnation reset: re-seed part of population if no improvement for long\n            if stagnation_counter > max(30, int(0.1 * self.pop_size)):\n                k = max(1, self.pop_size // 2)\n                # replace worst k individuals with random samples near best (or fully random if very stagnant)\n                worst_idx = np.argsort(fvals)[-k:]\n                for j in worst_idx:\n                    if evals >= self.budget:\n                        break\n                    if stagnation_counter > max(60, int(0.2 * self.pop_size)):\n                        # fully random reinit across bounds\n                        pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    else:\n                        # reinit near best with large variance\n                        jitter = self.rng.normal(0, 1.0, size=self.dim) * (0.5 * max(trust_radius, 1e-9)) * range_vec\n                        pop[j] = np.clip(best_x + jitter, lb, ub)\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        last_improve_eval = evals\n                # reset stagnation counter and slightly enlarge trust radius after reset\n                stagnation_counter = 0\n                trust_radius = min(max_trust, trust_radius * 1.25)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.667 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.24946101223702422, 0.34683600082649857, 0.8942386390284742, 0.6071964441454989, 0.9498181529450387, 0.9576478669540317, 0.9121775390562835, 0.5727218667984244, 0.9435978438824745, 0.23169117473332956]}, "task_prompt": ""}
{"id": "580fafcf-73b8-4bea-ba88-3e68a8376163", "fitness": 0.45186665679708515, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: Differential Evolution with occasional Lévy long-jumps and an online trust-region local search; parameters adapt from recent successes to balance global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent successes.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but bounded\n        if pop_size is None:\n            pop_size = int(max(8, min(60, 8 + 2 * np.sqrt(self.dim))))\n            pop_size = min(pop_size, max(4, self.budget // 20))\n        self.pop_size = int(pop_size)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # accepts scalar or array-like, returns 1D array of length dim\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to dimension {}\".format(self.dim))\n\n    def __call__(self, func):\n        # bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        norm_range = max(np.linalg.norm(range_vec), 1e-12)\n\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        # function values (initialize to +inf)\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        gen = 0\n\n        # evaluate as many initial individuals as budget allows (sequential)\n        n_init = min(self.pop_size, self.budget)\n        for i in range(n_init):\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # If no evals possible\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # get current best\n        best_idx = int(np.nanargmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump per target\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * norm_range  # scalar trust radius in absolute units\n        min_trust = 1e-6\n        max_trust = 2.0 * norm_range\n\n        # adaptation memory for successful F/CR values (for simple Lehmer-style update)\n        success_F = []\n        success_CR = []\n\n        stagnation_counter = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy with limits\n        def levy_step():\n            # generate per-dimension Cauchy (heavy tails). Clip extremes to avoid blow-ups.\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -50.0, 50.0)\n            # normalize to have moderate scale\n            s = s / (np.linalg.norm(s) / np.sqrt(self.dim) + 1e-12)\n            return s\n\n        # Main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            used_F = []\n            used_CR = []\n\n            # iterate over population as targets\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # choose per-individual F and CR (jDE-like small perturbation)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # decide exploration method: Lévy jump around best or DE mutation\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on current best for long-range exploration\n                    step = levy_step()\n                    # scale by step_scale, trust_radius and normalized range per-dim\n                    scale = step_scale * (trust_radius / (norm_range + 1e-12))\n                    candidate = best_x + scale * step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                else:\n                    # DE/rand/1/bin mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback: small random perturbation\n                        candidate = pop[i] + rng.normal(0, 0.01, size=self.dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover ensuring at least one gene from donor\n                        jrand = rng.randint(self.dim)\n                        cr_mask = (rng.rand(self.dim) < CRi)\n                        cr_mask[jrand] = True\n                        candidate = np.where(cr_mask, donor, pop[i])\n\n                # project to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if we have budget\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population (if better than current individual)\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # record used parameters for adaptation\n                    used_F.append(Fi)\n                    used_CR.append(CRi)\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                        # successful local improvement -> shrink trust radius a bit\n                        trust_radius = max(min_trust, 0.9 * trust_radius)\n                    else:\n                        # small successful replacement but not global improvement\n                        stagnation_counter = max(0, stagnation_counter - 1)\n                else:\n                    # unsuccessful trial\n                    stagnation_counter += 1\n                    # unsuccessful => small expansion to encourage exploration\n                    trust_radius = min(max_trust, trust_radius * (1.0 + 0.001))\n\n                # early termination if perfect\n                if best_f <= 0.0:\n                    break\n\n            # trust-region local search around best: focused Gaussian samples\n            remaining = self.budget - evals\n            if remaining > 0:\n                local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma: base trust_radius scaled per-dimension and relative to range\n                    sigma_rel = (0.4 + rng.rand(self.dim) * 0.6) * (trust_radius / (norm_range + 1e-12))\n                    candidate = best_x + rng.normal(0, sigma_rel) * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step => shrink trust radius to focus\n                        trust_radius = max(min_trust, 0.85 * trust_radius)\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                        # unsuccessful => slightly expand trust radius to escape\n                        trust_radius = min(max_trust, trust_radius * 1.02)\n\n            # Adaptation of F_mean, CR_mean, p_levy using successes\n            if len(used_F) > 0:\n                # Lehmer-like update (give more weight to larger F that succeeded)\n                used_F = np.array(used_F)\n                used_CR = np.array(used_CR)\n                # Lehmer mean for F\n                F_mean = float((np.sum(used_F**2) / (np.sum(used_F) + 1e-12)) * 0.8 + F_mean * 0.2)\n                CR_mean = float(0.8 * np.mean(used_CR) + 0.2 * CR_mean)\n                # reduce chance of Lévy if many successes (favor exploitation)\n                if successes > max(1, self.pop_size // 10):\n                    p_levy = max(0.01, p_levy * 0.95)\n            else:\n                # no successes: encourage exploration\n                p_levy = min(0.5, p_levy * 1.08)\n                # slightly relax crossover to increase diversity\n                CR_mean = float(np.clip(0.98 * CR_mean + 0.02, 0.0, 1.0))\n                # slightly increase F_mean to encourage bigger steps\n                F_mean = float(np.clip(F_mean * 1.01, 0.05, 0.99))\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 8):\n                k = max(1, self.pop_size // 2)\n                replace_idx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in replace_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # update best if any new sample is better\n                    if fvals[j] < best_f:\n                        best_f = float(fvals[j])\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 1.8)\n                stagnation_counter = 0\n                # encourage exploration after reset\n                p_levy = min(0.5, p_levy * 1.2)\n                F_mean = min(0.9, F_mean * 1.05)\n\n            # safety: keep parameters in sensible ranges\n            F_mean = float(np.clip(F_mean, 0.05, 0.99))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n            p_levy = float(np.clip(p_levy, 0.01, 0.5))\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # quick progress print could be added here (commented out)\n            # if gen % 10 == 0:\n            #     print(f\"gen {gen}, evals {evals}/{self.budget}, best_f {best_f:.4e}, p_levy {p_levy:.3f}\")\n\n            # termination if best is good enough (function-specific; here we keep going)\n            # continue until budget exhausted\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.452 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13244276665706534, 0.25694055832873874, 0.6737323972870675, 0.9622458570335545, 0.3173323369347698, 0.9354872236641897, 0.28974147064635825, 0.474106084950744, 0.29181234524877264, 0.18482552721959078]}, "task_prompt": ""}
{"id": "881e51bf-9fd9-4f89-8052-4136246b1a24", "fitness": 0.2110849656077865, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online parameter adaptation (fast global exploration, focused local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, seed=0)\n        f_opt, x_opt = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size: scale with dimension but never exceed a reasonable\n        # fraction of the budget so we can do local search as well.\n        if pop_size is None:\n            # baseline: between 8 and 4+2*dim but limited by budget/4\n            approx = max(8, 4 + 2 * self.dim)\n            self.pop_size = int(min(approx, max(4, self.budget // 4)))\n        else:\n            self.pop_size = int(max(4, pop_size))\n            self.pop_size = min(self.pop_size, max(1, self.budget))  # cannot exceed budget\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.array(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast or repeat\n        try:\n            out = np.broadcast_to(b, (self.dim,))\n            return out.astype(float)\n        except Exception:\n            # fallback: repeat first element\n            return np.full(self.dim, float(b.flat[0]), dtype=float)\n\n    def __call__(self, func):\n        rng = self.rng\n        # bounds (func.bounds.lb / ub can be scalars or arrays)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_vec = np.maximum(range_vec, 1e-12)  # avoid zeros\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population, but do not exceed budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                # if the func expects float32 or other shape, ensure we pass right type\n                fvals[i] = float(func(np.asarray(pop[i], dtype=float)))\n            evals += 1\n\n        # determine current best among evaluated individuals\n        if evals > 0:\n            best_idx = int(np.argmin(fvals[:min(evals, self.pop_size)]))\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # no evaluations possible\n            best_x = lb + rng.rand(self.dim) * range_vec\n            best_f = np.inf\n\n        # algorithm hyper-parameters and adaptive statistics\n        F_mean = 0.6\n        CR_mean = 0.5\n        p_levy = 0.08       # probability of a Lévy jump\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0 + 1e-12\n        trust_radius = np.linalg.norm(range_vec) * 0.2 + 1e-12\n\n        stagnation_counter = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # scale down by median absolute to avoid too extreme jumps in high-d\n            mad = np.median(np.abs(s - np.median(s))) + 1e-12\n            return s / mad\n\n        # main loop: run until budget exhausted\n        while evals < self.budget:\n            successes = 0\n            # adapt per-generation randomization of F and CR around their means\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample individual DE parameters (jDE-like lightweight)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # choose mutation style\n                if rng.rand() < p_levy:\n                    # Lévy-centered jump around best for long-range exploration\n                    step = levy_step()\n                    # scale by trust_radius and relative variable scales\n                    local_scale = trust_radius / (np.linalg.norm(range_vec) + 1e-12)\n                    donor = best_x + (0.5 + 0.5 * rng.rand()) * step * (range_vec / np.maximum(range_vec.mean(), 1e-12)) * local_scale\n                else:\n                    # DE/rand/1-like mutation\n                    idxs = np.arange(self.pop_size)\n                    if idxs.size <= 3:\n                        # fallback small random perturb\n                        donor = pop[i] + Fi * (rng.rand(self.dim) - 0.5)\n                    else:\n                        idxs = idxs[idxs != i]\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cr_mask = rng.rand(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[rng.randint(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                trial = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(trial))\n                except Exception:\n                    f_candidate = float(func(np.asarray(trial, dtype=float)))\n                evals += 1\n\n                # selection\n                if f_candidate < fvals[i]:\n                    pop[i] = trial.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # nudge parameter means toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = trial.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation: trust-region local search around best (sample small local moves)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # allocate a few local samples, scaled by dimension and remaining budget but keep them small\n            local_samples = int(min(remaining, max(1, self.dim // 2 + 1)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by random per-dimension factor\n                sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.randn(self.dim) * sigma\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = float(func(np.asarray(candidate, dtype=float)))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus search\n                    trust_radius *= 0.85\n                    trust_radius = max(trust_radius, min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = min(max_trust, max(trust_radius, min_trust))\n                    stagnation_counter += 1\n\n            # adjust exploration probability and parameter means based on success\n            if successes > 0:\n                # slightly reduce p_levy if we're making progress (less need for long jumps)\n                factor = 0.95 if successes > max(1, self.pop_size * 0.15) else 0.98\n                p_levy = max(0.01, p_levy * factor)\n                # small damping to means to avoid runaway\n                F_mean = np.clip(F_mean * 0.999, 0.05, 1.0)\n                CR_mean = np.clip(CR_mean * 0.999, 0.0, 1.0)\n            else:\n                # stagnation: increase chance of long jumps and slightly decrease CR to encourage exploration\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n                # small random perturbation to F_mean to inject diversity\n                if rng.rand() < 0.2:\n                    F_mean = np.clip(F_mean + rng.normal(0, 0.05), 0.05, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                for j in rng.choice(np.arange(self.pop_size), k, replace=False):\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = float(func(np.asarray(pop[j], dtype=float)))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.211 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1288357483381718, 0.1600630492120706, 0.30855295086319534, 0.29886515084585696, 0.20159494209992468, 0.22747223426606478, 0.22661965787670746, 0.2095652057554026, 0.19328451970833282, 0.15599619711213786]}, "task_prompt": ""}
{"id": "37a8f3c6-8e58-4c18-b661-5408f10a3999", "fitness": 0.40874757227632036, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size and operator adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Adaptive hybrid: Differential Evolution + occasional Lévy jumps + trust-region local search.\n    - Fast global exploration via DE + occasional heavy-tailed Lévy jumps from the best.\n    - Focused local exploitation via a trust-region Gaussian search around the best.\n    - Online adaptation of DE parameters (F, CR), Levy probability and trust radius based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size (scaled with dim but bounded by budget)\n        if pop_size is None:\n            # base on dimension but keep small if budget is small\n            self.pop_size = int(min(max(6, 4 * self.dim), max(4, self.budget // 10)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_bounds(self, vec, lb, ub):\n        # project to bounds\n        return np.minimum(np.maximum(vec, lb), ub)\n\n    def __call__(self, func):\n        rng = self.rng\n\n        # Determine bounds from func if available, else use [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            # broadcast if needed\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item(), dtype=float)\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item(), dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim,  5.0, dtype=float)\n\n        # ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        # If any dimension has zero range, set small positive to avoid divide by zero\n        range_vec = np.where(range_vec <= 0.0, 1e-8, range_vec)\n\n        # initialize population\n        pop = np.empty((self.pop_size, self.dim), dtype=float)\n        for i in range(self.pop_size):\n            pop[i] = lb + rng.random(self.dim) * range_vec\n\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # If no evaluations could be performed return trivial\n        if np.all(np.isinf(fvals)):\n            self.f_opt = np.inf\n            self.x_opt = pop[0].copy() if self.pop_size > 0 else np.zeros(self.dim)\n            return self.f_opt, self.x_opt\n\n        # set current best\n        valid = np.isfinite(fvals)\n        best_idx = np.argmin(fvals[valid])\n        best_indices = np.where(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Levy jump instead of DE\n        trust_radius = 0.2  # relative (0..1) trust radius (proportion of range_vec)\n        min_trust = 1e-6\n        max_trust = float(np.linalg.norm(range_vec) * 2.0)\n\n        gen = 0\n        stagnation_counter = 0\n        successes = 0  # per-gen successes\n        max_stagnation_for_reset = max(50, self.dim * 6)\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (clipped)\n        def levy_step():\n            # Cauchy (standard) per-dimension, clipped to avoid numerical extremes\n            s = rng.standard_cauchy(self.dim)\n            # clip extreme values to something proportional to dimensional scale\n            clip_val = 10.0\n            s = np.clip(s, -clip_val, clip_val)\n            return s\n\n        # Main loop: process generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # Shuffle order to avoid bias\n            idx_order = rng.permutation(self.pop_size)\n\n            # Per-individual operations (each evaluation counts)\n            for idx in idx_order:\n                if evals >= self.budget:\n                    break\n\n                # choose whether to do Levy exploration or DE\n                do_levy = (rng.random() < p_levy)\n\n                # sample Fi and CRi around means (small normal jitter)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 0.99))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                target = pop[idx]\n                if do_levy:\n                    # generate a Levy-like jump centered on best_x scaled by trust radius and global range\n                    step = levy_step()\n                    # scale by trust_radius and per-dimension range\n                    candidate = best_x + (trust_radius * 5.0) * (step * range_vec / (1.0 + np.linalg.norm(step)))\n                    # add a small gaussian perturbation as well\n                    candidate += rng.normal(0, 0.05, size=self.dim) * range_vec\n                else:\n                    # DE/rand/1/bin or DE/best/1 occasionally\n                    # pick three distinct indices different from idx\n                    choices = np.arange(self.pop_size)\n                    choices = choices[choices != idx]\n                    r = rng.choice(choices, size=3, replace=False)\n                    r1, r2, r3 = r\n                    # mix whether to use best-based or rand-based mutation\n                    if rng.random() < 0.4:\n                        # DE/best/1\n                        donor = best_x + Fi * (pop[r1] - pop[r2])\n                    else:\n                        # DE/rand/1\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    cross_mask = rng.random(self.dim) < CRi\n                    # ensure at least one gene from donor\n                    if not np.any(cross_mask):\n                        cross_mask[rng.integers(0, self.dim)] = True\n                    candidate = np.where(cross_mask, donor, target)\n\n                # project candidate to bounds\n                candidate = self._ensure_bounds(candidate, lb, ub)\n\n                # One evaluation\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_candidate < fvals[idx]:\n                    pop[idx] = candidate.copy()\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    # move means slightly toward successful Fi/CRi\n                    F_mean = 0.95 * F_mean + 0.05 * Fi\n                    CR_mean = 0.95 * CR_mean + 0.05 * CRi\n                # Update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful local improvement encourage focusing\n                    trust_radius = max(min_trust, trust_radius * 0.92)\n                else:\n                    stagnation_counter += 1\n                    # small expansion on failures\n                    trust_radius = min(max_trust, trust_radius * 1.002 + 1e-12)\n\n                # occasionally \"forget\" local Fi/CRi variables (clean)\n                # Not strictly necessary in python; kept to reflect logic\n\n                # budget check inside loop\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            # If there were successes, slightly reduce Levy chance and stabilize means,\n            # otherwise encourage more global exploration via Levy jumps\n            if successes > 0:\n                # stabilize means\n                F_mean = np.clip(0.97 * F_mean + 0.03 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.97 * CR_mean + 0.03 * 0.9, 0.0, 1.0)\n                # decrease Levy probability a bit\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                # stagnation: increase Levy jumps modestly\n                p_levy = min(0.5, p_levy * 1.12 + 0.01)\n                # nudge F_mean upward to allow larger steps\n                F_mean = np.clip(F_mean * 1.03, 0.05, 0.99)\n\n            # trust-region local search around the best (small set of Gaussian samples)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # number of local samples depends on remaining budget and dimension\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            # anisotropic sigma: base trust_radius scaled by random per-dim perturbation\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # a Gaussian local probe: sigma proportional to trust_radius and range_vec\n                sigma = max(min_trust, trust_radius) * rng.random()  # uniform factor 0..1\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * (sigma * range_vec)\n                candidate = self._ensure_bounds(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                # update best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.6)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slight expansion to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # strong stagnation reset: re-seed part of population if no improvement for long\n            if stagnation_counter > max_stagnation_for_reset and evals < self.budget:\n                k = max(1, self.pop_size // 2)\n                # replace k worst individuals with random ones\n                worst_idx = np.argsort(fvals)[-k:]\n                for j in worst_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # if this new one is better update best\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # slightly increase levy chance to encourage exploration after reset\n                p_levy = min(0.8, p_levy * 1.2 + 0.02)\n                # reset stagnation counter partially\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.409 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13321784527972935, 0.2090898650376034, 0.5521615889720152, 0.9767017346473212, 0.33504106660464006, 0.7550818738360703, 0.28248450913713385, 0.3846279630413022, 0.286199476850739, 0.17286979935664848]}, "task_prompt": ""}
{"id": "52d678b8-0fb0-4c82-967d-2c20f715b4e6", "fitness": 0.5575717004618429, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy-flight leaps and an online-adapting trust-region local search for fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=1)\n        f_opt, x_opt = opt(func)\n    The black-box `func` is expected to accept a 1-D numpy array of length `dim`.\n    If `func` provides `.bounds.lb` and `.bounds.ub`, these are used; otherwise [-5,5]^dim is used.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: scale with dim and budget but keep reasonable limits\n        if pop_size is None:\n            # prefer bigger populations for higher-dim problems, but do not exceed budget/4\n            self.pop_size = int(min(100, max(4, min(max(4, 4 * self.dim), self.budget // 8))))\n        else:\n            self.pop_size = int(max(4, pop_size))\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = self.rng\n        budget = self.budget\n        dim = self.dim\n        pop_size = self.pop_size\n\n        # determine bounds (support func.bounds if present)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(dim, float(lb))\n                ub = np.full(dim, float(ub))\n        except Exception:\n            lb = np.full(dim, -5.0)\n            ub = np.full(dim, 5.0)\n\n        # ensure correct shape\n        if lb.shape != (dim,):\n            lb = np.broadcast_to(lb.ravel(), (dim,)).astype(float)\n        if ub.shape != (dim,):\n            ub = np.broadcast_to(ub.ravel(), (dim,)).astype(float)\n\n        range_vec = ub - lb\n        # prevent zero range\n        range_vec[range_vec == 0.0] = 1.0\n\n        # initialization\n        pop = rng.random((pop_size, dim)) * range_vec + lb\n        fvals = np.full(pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # Evaluate initial population until budget exhausted or all evaluated\n        for i in range(pop_size):\n            if evals >= budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                # on any evaluation failure, penalize\n                fvals[i] = np.inf\n            evals += 1\n\n        # if we couldn't evaluate any individual, return empty result\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6   # differential weight mean\n        CR_mean = 0.9  # crossover probability mean\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = max(1e-3, 0.2 * np.linalg.norm(range_vec))\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        successes = 0\n        generation = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # sample Cauchy (heavy-tailed), scaled and clipped to avoid numerical blow-up\n            # typical heavy tail for exploration: location 0, scale 1\n            step = rng.standard_cauchy(size=dim)\n            # attenuate very large outliers smoothly\n            limit = 1e3\n            step = np.clip(step, -limit, limit)\n            # add a small Gaussian jitter to avoid pathological symmetric patterns\n            step = step + 0.05 * rng.standard_normal(size=dim)\n            return step\n\n        # Main loop: generations until budget exhausted\n        # We'll loop over population sequentially (like classic DE)\n        while evals < budget:\n            generation += 1\n            # adjust chance of Levy based on stagnation (more stagnation => more Levy jumps)\n            p_levy = np.clip(0.02 + 0.002 * stagnation_counter, 0.02, 0.4)\n\n            # adapt per-generation randomness of F and CR around their means (like jDE)\n            for i in range(pop_size):\n                if evals >= budget:\n                    break\n\n                # pick per-individual F and CR\n                Fi = float(np.clip(rng.normal(F_mean, 0.15), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0))\n\n                use_levy = (rng.random() < p_levy)\n                if use_levy:\n                    # Levy jump centered on best for exploration\n                    step = levy_step()\n                    # scale by range and dynamic trust_radius\n                    scale = step_scale * (0.5 + 0.5 * rng.random())\n                    donor = best_x + scale * step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    # to mix, also incorporate a bit of current individual\n                    donor = 0.7 * donor + 0.3 * pop[i]\n                else:\n                    # DE/rand/1-like mutation\n                    idxs = np.arange(pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback to small gaussian perturbation\n                        donor = pop[i] + Fi * rng.standard_normal(size=dim) * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover to create trial vector\n                cr_mask = rng.random(dim) < CRi\n                # ensure at least one dimension is from donor\n                if not np.any(cr_mask):\n                    cr_mask[rng.integers(0, dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # Evaluate candidate if budget allows\n                if evals >= budget:\n                    break\n                try:\n                    f_trial = float(func(trial))\n                except Exception:\n                    f_trial = np.inf\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_trial < fvals[i]:\n                    pop[i] = trial\n                    fvals[i] = f_trial\n                    successes += 1\n                    stagnation_counter = 0\n                    # move means slightly toward successful parameters (only if DE branch)\n                    if not use_levy:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    else:\n                        # slight nudge toward exploration when Levy succeeds\n                        F_mean = 0.98 * F_mean + 0.02 * 0.5\n                        CR_mean = 0.98 * CR_mean + 0.02 * 0.5\n                else:\n                    stagnation_counter += 1\n\n                # update global best\n                if fvals[i] < best_f:\n                    best_f = float(fvals[i])\n                    best_x = pop[i].copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n                # occasionally, when a Levy jump was attempted but not inserted into population,\n                # consider accepting it into the population index to maintain diversity\n                # (already handled by selection above)\n\n            # End of generation adjustments: trust-region local search around best\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n\n            # dynamic number of local samples: small handful scaled by dim and remaining budget\n            local_samples = int(min(max(1, dim // 2), max(1, remaining // max(1, (10 * dim)))))\n            local_samples = max(1, local_samples)\n            # but limit to a small number to preserve global exploration\n            local_samples = min(local_samples, 10)\n\n            improved_in_local = False\n            for _ in range(local_samples):\n                if evals >= budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                sigma = (0.5 + rng.random(dim) * 0.5) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.standard_normal(size=dim) * sigma\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    stagnation_counter = 0\n                    improved_in_local = True\n                else:\n                    # unsuccessful => expand slightly to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # adapt F_mean/CR_mean slowly towards exploitation if many successes\n            if successes > 0:\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            # slightly increase chance of stronger global moves if stagnating\n            if stagnation_counter > 5:\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, 10 * dim):\n                k = max(1, pop_size // 2)\n                # pick k indices to reinitialize\n                reinit_idxs = rng.choice(np.arange(pop_size), size=k, replace=False)\n                for j in reinit_idxs:\n                    if evals >= budget:\n                        break\n                    pop[j] = rng.random(dim) * range_vec + lb\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    # if any new individual is better, update best\n                    if fvals[j] < best_f:\n                        best_f = float(fvals[j])\n                        best_x = pop[j].copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                # slightly enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                successes = 0\n\n            # safety: keep F_mean and CR_mean in bounds\n            F_mean = float(np.clip(F_mean, 0.05, 0.99))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n            # if best hasn't been updated in a long while, occasionally perform a direct Levy probe\n            if stagnation_counter > max(20, 5 * dim) and evals < budget:\n                # direct probe\n                probe = best_x + levy_step() * step_scale * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                probe = np.minimum(np.maximum(probe, lb), ub)\n                try:\n                    f_probe = float(func(probe))\n                except Exception:\n                    f_probe = np.inf\n                evals += 1\n                if f_probe < best_f:\n                    best_f = f_probe\n                    best_x = probe.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    trust_radius = max(min_trust, trust_radius * 0.6)\n                    stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.558 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.24464158788080514, 0.18740785902515433, 0.3782845227929498, 0.7961943668646841, 0.6301496907139057, 0.9104129116862418, 0.8443623249157891, 0.4525994808677102, 0.9013199377972558, 0.23034432207393285]}, "task_prompt": ""}
{"id": "c4c62488-8145-40ad-9654-d9d66a6a8ef1", "fitness": 0.5539768595275393, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining jDE-like Differential Evolution, occasional Lévy (Cauchy) long jumps, and an adaptive trust-region local search — balances fast global exploration and focused local exploitation with online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid DE + Lévy jumps + Trust-region local search with online adaptation.\n    - budget: number of function evaluations allowed\n    - dim: problem dimension\n    Optional:\n    - pop_size: number of individuals in population (auto-scaled if None)\n    - seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n\n        # sensible default population size scaled with dimension\n        if pop_size is None:\n            pop_size = int(np.clip(8 + 2 * np.sqrt(self.dim), 8, 60))\n        self.pop_size = max(2, int(pop_size))\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Determine bounds: prefer func.bounds if present, else [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item())\n                ub = np.full(self.dim, ub.item())\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure shapes\n        if lb.size != self.dim:\n            lb = np.broadcast_to(lb.ravel()[0], self.dim)\n        if ub.size != self.dim:\n            ub = np.broadcast_to(ub.ravel()[0], self.dim)\n\n        range_vec = ub - lb\n        range_mean = max(range_vec.mean(), 1e-12)\n\n        # Adjust population if budget too small\n        if self.budget < self.pop_size:\n            self.pop_size = max(1, int(self.budget))\n\n        # Initialize population uniformly\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # Evaluate initial population (as many as budget allows)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n            if fvals[i] < self.f_opt:\n                self.f_opt = fvals[i]\n                self.x_opt = pop[i].copy()\n\n        # If nothing evaluated (budget==0) return\n        if evals == 0:\n            return self.f_opt, self.x_opt\n\n        # jDE-like means for F and CR\n        F_mean = 0.6\n        CR_mean = 0.3\n\n        # Lévy (Cauchy) jump probability\n        p_levy = 0.05\n\n        # trust-region parameters\n        trust_radius = 0.1  # fraction of range_vec\n        trust_min = 1e-6\n        trust_max = 1.0\n\n        # adaptation memory\n        c_adapt = 0.1  # learning rate for parameter means\n        stagnation_counter = 0\n        best_global = self.x_opt.copy()\n        f_best_global = self.f_opt\n\n        gen = 0\n        # main loop: process generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n\n            # storage for successful F and CR in this generation\n            S_F = []\n            S_CR = []\n\n            # shuffle order for fairness\n            order = np.arange(self.pop_size)\n            self.rng.shuffle(order)\n\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                xi = pop[idx]\n                fi = fvals[idx]\n\n                # sample per-individual F and CR (jDE-styled noisy sampling)\n                Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 0.95)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # With small probability perform a Lévy-like jump centered on current best\n                if self.rng.random() < p_levy:\n                    # Use Cauchy (standard_cauchy) as a practical heavy-tailed proxy\n                    step = self.rng.standard_cauchy(self.dim)\n                    step = np.clip(step, -1e3, 1e3)\n                    # scale depends on trust radius and range\n                    scale = (0.5 + 0.5 * self.rng.random()) * (trust_radius + 0.2)  # slightly encourage exploration\n                    donor = best_global + scale * (step / (np.std(step) + 1e-12)) * (range_vec / range_mean)\n                    # small chance to mix with xi (crossover)\n                    mask = self.rng.random(self.dim) < CRi\n                    if not np.any(mask):\n                        mask[self.rng.integers(0, self.dim)] = True\n                    trial = np.where(mask, donor, xi)\n                else:\n                    # Differential Evolution mutation: DE/rand/1\n                    # pick three distinct indices different from idx\n                    if self.pop_size >= 4:\n                        candidates = [j for j in range(self.pop_size) if j != idx]\n                        r1, r2, r3 = self.rng.choice(candidates, size=3, replace=False)\n                    else:\n                        # small pop special handling: allow replacement\n                        r1, r2, r3 = self.rng.integers(0, self.pop_size, size=3)\n                    mutant = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    mask = self.rng.random(self.dim) < CRi\n                    if not np.any(mask):\n                        mask[self.rng.integers(0, self.dim)] = True\n                    trial = np.where(mask, mutant, xi)\n\n                # project to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # Evaluate candidate if we still have budget\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_candidate < fi:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    # success memory\n                    S_F.append(Fi)\n                    S_CR.append(CRi)\n                    # update global best if improved\n                    if f_candidate < f_best_global:\n                        f_best_global = f_candidate\n                        best_global = candidate.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 0\n                else:\n                    stagnation_counter += 0\n\n                # update record of best found so far\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    # also update best_global\n                    if f_candidate < f_best_global:\n                        f_best_global = f_candidate\n                        best_global = candidate.copy()\n                        stagnation_counter = 0\n\n            # End of generation: adapt F_mean and CR_mean using successes (Lehmer-like for F)\n            if len(S_F) > 0:\n                # Lehmer mean effect for F to favor larger F that are successful\n                numer = sum(np.array(S_F) ** 2)\n                denom = sum(S_F)\n                if denom > 0:\n                    F_new = numer / denom\n                    F_mean = (1 - c_adapt) * F_mean + c_adapt * np.clip(F_new, 0.05, 0.95)\n                else:\n                    F_mean = (1 - c_adapt) * F_mean + c_adapt * np.mean(S_F)\n                CR_mean = (1 - c_adapt) * CR_mean + c_adapt * np.mean(S_CR)\n                # reward focused local search if many successes\n                trust_radius = max(trust_min, trust_radius * (0.9 if len(S_F) > max(1, self.pop_size // 10) else 1.0))\n                stagnation_counter = 0\n            else:\n                # no DE successes this generation -> encourage exploration\n                p_levy = min(0.5, p_levy * 1.15)\n                trust_radius = min(trust_max, trust_radius * 1.05)\n                stagnation_counter += 1\n\n            # Clamp p_levy to a minimum to keep some exploration\n            p_levy = max(0.01, min(0.5, p_levy))\n            F_mean = float(np.clip(F_mean, 0.05, 0.95))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n            # Trust-region local search around current best_global\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # choose number of local samples adaptively\n            local_samples = min(remaining, max(1, int(min(5 + self.dim // 2, remaining))))\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian noise scaled by trust_radius and range_vec\n                noise = self.rng.normal(0, 1.0, size=self.dim)\n                candidate = best_global + noise * (trust_radius * range_vec)\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < f_best_global:\n                    f_best_global = f_candidate\n                    best_global = candidate.copy()\n                    local_success += 1\n                    # shrink trust region to focus exploitation\n                    trust_radius = max(trust_min, trust_radius * 0.7)\n                else:\n                    # if not improved, slightly expand to escape local minima\n                    trust_radius = min(trust_max, trust_radius * 1.05)\n\n                # update global best record\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    stagnation_counter = 0\n\n            # adjust p_levy based on local successes\n            if local_success > max(1, local_samples // 4):\n                # many successes -> less long jumps\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                p_levy = min(0.5, p_levy * 1.02)\n\n            # Stagnation handling: if too many generations without improvement, re-seed part of population\n            if stagnation_counter > max(5, self.dim):\n                # reinitialize half of the population away from best_global\n                n_reseed = max(1, self.pop_size // 2)\n                for j in range(n_reseed):\n                    idx = self.rng.integers(0, self.pop_size)\n                    pop[idx] = lb + self.rng.random(self.dim) * range_vec\n                    # evaluate re-seeded individual if budget allows\n                    if evals < self.budget:\n                        fvals[idx] = float(func(pop[idx]))\n                        evals += 1\n                        if fvals[idx] < self.f_opt:\n                            self.f_opt = fvals[idx]\n                            self.x_opt = pop[idx].copy()\n                # increase exploration probability and reset trust radius moderately\n                p_levy = min(0.5, p_levy * 1.5)\n                trust_radius = min(trust_max, trust_radius * 1.2)\n                stagnation_counter = 0\n\n            # small decay to avoid saturating exploration\n            p_levy = max(0.01, p_levy * 0.999)\n\n        # final bests already stored in self.f_opt, self.x_opt\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.554 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1971229186015241, 0.21492808953539233, 0.5809293355340932, 0.9264238488825203, 0.8291215894364048, 0.9031072975532816, 0.3042075674341347, 0.5050036106390232, 0.8384323370210873, 0.24049200063793108]}, "task_prompt": ""}
{"id": "0ca5a264-16d4-4991-8f3a-5d5c8be4b8d1", "fitness": 0.5891990074354541, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy flights and a trust-region local search; online adapts DE parameters and exploration intensity for fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent successes (jDE-like adaptation).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # default population size scaled with dimension but not too large relative to budget\n        if pop_size is None:\n            approx = max(4, int(6 + 1.5 * self.dim))\n            pop_size = approx\n        self.pop_size = int(pop_size)\n        # ensure not using enormous population relative to budget\n        self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        # output placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Ensure b is an array of length dim. b may be scalar or array-like.\n        \"\"\"\n        b = np.asarray(b)\n        if b.shape == ():  # scalar\n            return np.full(self.dim, float(b))\n        if b.shape[0] != self.dim:\n            # try broadcasting\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        return b.astype(float)\n\n    def __call__(self, func):\n        # bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # fallback to [-5,5] if bounds are degenerate\n        if np.any(np.isnan(lb)) or np.any(np.isnan(ub)):\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n        range_vec = ub - lb\n        # avoid zero ranges\n        range_vec = np.where(range_vec <= 0.0, 1.0, range_vec)\n\n        # initialize algorithm hyper-parameters\n        F_mean = 0.6    # mean differential weight\n        CR_mean = 0.3   # mean crossover prob\n        p_levy = 0.05   # initial chance for a Levy jump\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # global trust radius (in absolute units)\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        min_trust = 1e-6\n\n        # population initialization\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                f = func(pop[i])\n            except Exception:\n                # safety: if evaluation fails, keep inf and continue\n                f = np.inf\n            fvals[i] = f\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = pop[i].copy()\n\n        # If we couldn't evaluate full pop due to tiny budget, remaining individuals stay but won't be used much\n        # adaptive state\n        stagnation_counter = 0\n        best_since_reset = self.f_opt\n        generation = 0\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            generation += 1\n            successes_F = []\n            successes_CR = []\n            generation_successes = 0\n\n            # iterate individuals sequentially (each evaluation uses one budget)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                target = pop[i].copy()\n                use_levy = (self.rng.rand() < p_levy)\n                if use_levy:\n                    # Lévy-like heavy-tailed step using Cauchy (simple)\n                    # scale step by current trust radius and range_vec anisotropically\n                    step = self.rng.standard_cauchy(self.dim)\n                    # clip extreme outliers for numerical stability\n                    step = np.clip(step, -1e3, 1e3)\n                    step = step / (np.mean(np.abs(step)) + 1e-9)  # normalize magnitude\n                    # anisotropic scaling: random per-dim factor\n                    anis = 0.5 + self.rng.rand(self.dim)\n                    trial = self.x_opt + step * (trust_radius / np.sqrt(self.dim)) * anis\n                    Fi = None; CRi = None\n                else:\n                    # DE mutation: choose three distinct indices\n                    idxs = np.arange(self.pop_size)\n                    # ensure at least 3 other indices exist\n                    if self.pop_size >= 4:\n                        idxs = idxs[idxs != i]\n                        a, b, c = self.rng.choice(idxs, 3, replace=False)\n                    else:\n                        # fallback to random perturbation if tiny pop\n                        a = b = c = i\n                    # adapt Fi and CRi per individual (jDE-like sampling)\n                    Fi = float(np.clip(self.rng.normal(F_mean, 0.1), 0.05, 1.0))\n                    CRi = float(np.clip(self.rng.normal(CR_mean, 0.15), 0.0, 1.0))\n                    # with some probability use best-guided mutation to accelerate exploitation\n                    p_best_guided = 0.25 if trust_radius < 0.05 * np.linalg.norm(range_vec) else 0.08\n                    if self.rng.rand() < p_best_guided:\n                        donor = self.x_opt + Fi * (pop[a] - pop[b])\n                    else:\n                        donor = pop[a] + Fi * (pop[b] - pop[c])\n                    # binomial crossover\n                    jrand = self.rng.randint(self.dim)\n                    mask = (self.rng.rand(self.dim) < CRi)\n                    mask[jrand] = True\n                    trial = np.where(mask, donor, target)\n\n                # project to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # one evaluation (if budget allows)\n                try:\n                    f_trial = func(trial) if evals < self.budget else np.inf\n                except Exception:\n                    f_trial = np.inf\n                evals += 1\n\n                # selection (greedy)\n                # If target wasn't evaluated before (inf), treat as replaceable\n                if f_trial < fvals[i]:\n                    pop[i] = trial\n                    fvals[i] = f_trial\n                    generation_successes += 1\n                    if Fi is not None:\n                        successes_F.append(Fi)\n                    if CRi is not None:\n                        successes_CR.append(CRi)\n                    # update global best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 0  # small change; keep prior\n                else:\n                    stagnation_counter += 1\n\n                # small immediate adaptation: nudge means toward used Fi/CRi if successful\n                if len(successes_F) > 0 and len(successes_CR) > 0 and len(successes_F) + len(successes_CR) > 0:\n                    # we'll update at generation end more stably\n                    pass\n\n                # if we used levy and it improved, slightly reduce p_levy; otherwise maybe increase later\n                if use_levy and f_trial < self.f_opt:\n                    p_levy = max(0.005, p_levy * 0.92)\n\n                # break early if budget exhausted checked at top of loop\n\n            # End of generation: adapt F_mean and CR_mean using successful parameters (simple moving average)\n            if len(successes_F) > 0:\n                # Lehmer-like: larger Fi contribute more\n                num = np.sum(np.array(successes_F) ** 2)\n                den = np.sum(np.array(successes_F)) + 1e-12\n                F_new = num / den\n                F_mean = 0.9 * F_mean + 0.1 * np.clip(F_new, 0.05, 1.0)\n            else:\n                # small drift back to moderate exploration\n                F_mean = 0.995 * F_mean + 0.005 * 0.6\n\n            if len(successes_CR) > 0:\n                CR_new = np.mean(successes_CR)\n                CR_mean = 0.9 * CR_mean + 0.1 * np.clip(CR_new, 0.0, 1.0)\n            else:\n                CR_mean = 0.995 * CR_mean + 0.005 * 0.3\n\n            # trust-region local search around best: budget permitting\n            remaining = self.budget - evals\n            if remaining > 0:\n                # sample a small number of local candidates relative to dim and remaining budget\n                n_local = min(max(1, self.dim // 3), remaining, 5)\n                local_success = 0\n                for _ in range(n_local):\n                    # anisotropic Gaussian noise scaled by trust_radius and range_vec\n                    per_dim_sigma = (0.5 + self.rng.rand(self.dim)) * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                    noise = self.rng.randn(self.dim) * per_dim_sigma * range_vec\n                    local_trial = np.minimum(np.maximum(self.x_opt + noise, lb), ub)\n                    try:\n                        f_local = func(local_trial)\n                    except Exception:\n                        f_local = np.inf\n                    evals += 1\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = local_trial.copy()\n                        local_success += 1\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                    if evals >= self.budget:\n                        break\n                # adjust trust radius based on local success\n                if local_success > 0:\n                    # successful local search -> focus: shrink trust radius\n                    trust_radius = max(min(trust_radius * (0.7 ** local_success), max_trust), min_trust)\n                else:\n                    # unsuccessful local search -> expand a bit to escape local traps\n                    trust_radius = min(trust_radius * 1.12, max_trust)\n\n            # adapt p_levy based on generation successes / stagnation\n            if generation_successes > 0:\n                # if many successes, slightly reduce long jumps\n                p_levy = max(0.005, p_levy * (0.98 if generation_successes > (0.2 * self.pop_size) else 0.995))\n            else:\n                # stagnating generation: increase chance of long jumps\n                p_levy = min(0.5, p_levy * 1.08)\n\n            # global stagnation handling: if no improvement for long, re-seed half population\n            if stagnation_counter > max(50, self.budget // 50):\n                k = max(1, self.pop_size // 2)\n                idxs = self.rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in idxs:\n                    # reinitialize\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    # evaluate if budget allows\n                    if evals < self.budget:\n                        try:\n                            fnew = func(pop[j])\n                        except Exception:\n                            fnew = np.inf\n                        fvals[j] = fnew\n                        evals += 1\n                        if fnew < self.f_opt:\n                            self.f_opt = fnew\n                            self.x_opt = pop[j].copy()\n                # slightly enlarge trust radius after reset and increase exploration\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                p_levy = min(0.5, p_levy * 1.2)\n                stagnation_counter = 0  # reset\n\n            # safety clamps\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n            p_levy = float(np.clip(p_levy, 0.005, 0.8))\n            F_mean = float(np.clip(F_mean, 0.05, 1.2))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n            # small safeguard to avoid infinite loops in degenerate cases\n            if generation > 10000:\n                break\n\n        # final results\n        # ensure x_opt is at least some valid candidate (if never evaluated)\n        if self.x_opt is None:\n            # pick best from evaluated fvals\n            finite = np.isfinite(fvals)\n            if np.any(finite):\n                idx = np.argmin(fvals)\n                self.x_opt = pop[idx].copy()\n                self.f_opt = fvals[idx]\n            else:\n                # fallback random\n                self.x_opt = lb + self.rng.rand(self.dim) * range_vec\n                try:\n                    self.f_opt = func(self.x_opt)\n                except Exception:\n                    self.f_opt = np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.589 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15658630024095754, 0.19412965912311908, 0.7487016489399637, 0.8345597339034129, 0.8643875988438552, 0.9280728895892709, 0.3306124081607269, 0.7062381039369329, 0.8726996279950593, 0.25600210362124187]}, "task_prompt": ""}
{"id": "082bb208-45c0-4062-a5b9-ba95052808d6", "fitness": 0.18865312490248048, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy-flight jumps and a shrinking/expanding trust-region local search; online adapts F/CR and jump-rate to balance global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line idea: jDE-style per-individual adaptation for DE, occasional\n    heavy-tailed Lévy/Cauchy jumps from the best to escape basins, and a\n    dynamic trust-region local search that contracts on success and expands\n    on failure; partial re-seeding prevents stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # scale pop with dimension but keep reasonable relative to budget\n            pop_size = max(8, min(12 * self.dim, max(8, self.budget // 10)))\n        self.pop_size = int(pop_size)\n        self.seed = seed\n\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # bounds handling; Many BBOB provides func.bounds.{lb,ub}\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure correct shapes\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n\n        # clamp shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        range_max = float(np.max(range_vec))\n\n        # initialize population\n        pop = lb + rng.rand(self.pop_size, self.dim) * (ub - lb)\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # evaluate initial individuals (as many as budget allows)\n        n_init = min(self.pop_size, self.budget)\n        for i in range(n_init):\n            x = pop[i].copy()\n            f = func(x)\n            evals += 1\n            fvals[i] = float(f)\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # un-evaluated individuals remain in pop but with inf fvals\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6       # mean scaling factor for DE\n        CR_mean = 0.9      # mean crossover probability\n        p_levy = 0.08      # base probability of a Lévy jump\n        trust_radius = 0.2  # relative to variable range (0..1)\n        min_trust = 1e-6\n        max_trust = 5.0\n\n        stagnation_counter = 0\n        stagnation_limit = max(50, 10 * self.dim)\n        generation = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step():\n            # standard Cauchy per-dimension, clipped to avoid numerical blow-up\n            step = rng.standard_cauchy(self.dim)\n            # clip extremes and scale slightly\n            step = np.clip(step, -10.0, 10.0)\n            # normalize magnitude a bit to avoid overwhelmingly large jumps\n            norm = np.linalg.norm(step)\n            if norm == 0:\n                return step\n            return step / (norm + 1e-12)\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            generation += 1\n\n            # store successful parameter values for adaptation\n            succ_Fs = []\n            succ_CRs = []\n            improvements = 0\n\n            # iterate population sequentially (each candidate may consume 1 eval)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi around means (jDE-like)\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.2)\n                CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                target = pop[i].copy()\n                target_f = fvals[i]\n\n                # choose random distinct indices for mutation\n                idxs = list(range(self.pop_size))\n                idxs.remove(i)\n                r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n\n                # decide whether to perform a Lévy jump\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on current best\n                    step = levy_step()\n                    # scale based on trust radius and global range\n                    scale = trust_radius * range_max * (0.5 + rng.rand())\n                    donor = self.x_opt + scale * step\n                    # small DE-like perturbation blended\n                    donor = np.clip(donor + Fi * (pop[r1] - pop[r2]), lb, ub)\n                    is_levy = True\n                else:\n                    # classical DE/rand/1\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    is_levy = False\n\n                # binomial crossover\n                jrand = rng.randint(0, self.dim)\n                cross = rng.rand(self.dim) < CRi\n                cross[jrand] = True\n                trial = np.where(cross, donor, target)\n                # projection to bounds\n                trial = np.clip(trial, lb, ub)\n\n                # evaluation\n                f_trial = func(trial)\n                evals += 1\n                f_trial = float(f_trial)\n\n                # selection: greedy replacement\n                replaced = False\n                if f_trial < target_f:\n                    pop[i] = trial\n                    fvals[i] = f_trial\n                    replaced = True\n                    improvements += 1\n                    # adapt F_mean and CR_mean slightly toward successful Fi/CRi\n                    succ_Fs.append(Fi)\n                    succ_CRs.append(CRi)\n                    # if Levy branch succeeded, reward: reduce p_levy a bit to focus\n                    if is_levy:\n                        p_levy = max(0.01, p_levy * 0.92)\n                    else:\n                        # small nudges\n                        p_levy = min(0.3, p_levy * 1.0)\n                    # update global best if improved\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        stagnation_counter = 0\n                else:\n                    # no replacement -> small penalty encouraging exploration\n                    p_levy = min(0.5, p_levy * 1.002)\n\n                if not replaced:\n                    stagnation_counter += 1\n\n                # limit trust_radius to avoid collapse or explosion\n                trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n                # early exit if budget reached\n                if evals >= self.budget:\n                    break\n\n            # End of generation: adapt F_mean and CR_mean using successful values\n            if len(succ_Fs) > 0:\n                # Lehmer-like mean (gives weight to larger Fi)\n                nums = np.sum(np.array(succ_Fs) ** 2)\n                dens = np.sum(np.array(succ_Fs)) + 1e-12\n                F_mean = 0.85 * F_mean + 0.15 * (nums / dens)\n                CR_mean = 0.85 * CR_mean + 0.15 * (np.mean(succ_CRs))\n            else:\n                # small random nudges if nothing succeeded\n                F_mean = np.clip(F_mean * (1.0 + 0.02 * (rng.rand() - 0.5)), 0.05, 1.2)\n                CR_mean = np.clip(CR_mean * (1.0 + 0.02 * (rng.rand() - 0.5)), 0.0, 1.0)\n\n            # trust-region local search around current best: a few focused samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # number of local attempts scaled with dimension and remaining budget\n            n_local = min(max(2, self.dim // 2), max(1, remaining // max(10, self.dim)))\n            local_improved = 0\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian sampling around best\n                per_dim_scale = rng.rand(self.dim) * 0.6 + 0.2  # between 0.2 and 0.8\n                sigma = trust_radius * range_vec * per_dim_scale\n                cand = self.x_opt + rng.normal(0.0, 1.0, self.dim) * sigma\n                cand = np.clip(cand, lb, ub)\n                f_cand = func(cand)\n                evals += 1\n                f_cand = float(f_cand)\n                if f_cand < self.f_opt:\n                    # accept and insert into population replacing worst member\n                    worst_idx = np.argmax(fvals)\n                    pop[worst_idx] = cand\n                    fvals[worst_idx] = f_cand\n                    self.f_opt = f_cand\n                    self.x_opt = cand.copy()\n                    local_improved += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # adjust trust radius based on local improvement\n            if local_improved > 0:\n                # shrink to focus\n                trust_radius *= max(0.6, 0.9 ** local_improved)\n            else:\n                # expand a bit to attempt escape\n                trust_radius *= 1.05\n\n            # adjust exploration/exploitation pressure after generation\n            if improvements > max(1, self.pop_size // 10):\n                # many improvements -> favor exploitation: shrink trust, reduce Levy\n                trust_radius *= 0.95\n                p_levy = max(0.01, p_levy * 0.94)\n            elif improvements == 0:\n                # stagnating generation -> more global exploration\n                trust_radius *= 1.08\n                p_levy = min(0.5, p_levy * 1.18)\n\n            # strong stagnation reset\n            if stagnation_counter >= stagnation_limit:\n                # re-seed half of the population to diversify\n                n_reseed = max(1, self.pop_size // 2)\n                for j in rng.choice(range(self.pop_size), n_reseed, replace=False):\n                    pop[j] = lb + rng.rand(self.dim) * (ub - lb)\n                    fvals[j] = np.inf\n                # try evaluating a few of the re-seeded ones if budget allows\n                for j in range(self.pop_size):\n                    if evals >= self.budget:\n                        break\n                    if fvals[j] == np.inf:\n                        x = pop[j].copy()\n                        f = func(x)\n                        evals += 1\n                        fvals[j] = float(f)\n                        if f < self.f_opt:\n                            self.f_opt = float(f)\n                            self.x_opt = x.copy()\n                # enlarge trust radius a bit to escape\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n                # slightly encourage Levy jumps after reset\n                p_levy = min(0.5, p_levy * 1.5)\n\n        # final best\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.189 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1021801567181615, 0.14892362000796378, 0.21924710503061273, 0.16907405453710855, 0.20690069240111741, 0.2771114089879839, 0.2151892064093872, 0.19559795532093238, 0.21670791720866134, 0.13559913240287602]}, "task_prompt": ""}
{"id": "9817a60b-9bb6-4ff1-a242-dda79c418382", "fitness": 0.5562419505730442, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy jumps and a trust-region local search; online adapts DE parameters and trust radius to balance global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line: Adaptive DE + sporadic Lévy jumps + trust-region local refinement with online step-size adaptation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # sensible default population size scaled with dimension but limited by budget\n        if pop_size is None:\n            p = max(8, 4 * self.dim)             # baseline scale\n            p = min(p, 80)                       # cap\n            p = min(p, max(4, self.budget // 6)) # don't spend too much on initial pop\n            self.pop_size = int(p)\n        else:\n            self.pop_size = int(pop_size)\n\n        # internal best result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # Try to read bounds from func, else use [-5,5] as specified\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # Broadcast to dim if scalars provided\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        rng = self.rng\n        dim = self.dim\n        pop_size = self.pop_size\n        budget = self.budget\n\n        # range\n        span = ub - lb\n        span[span == 0.0] = 1.0  # avoid zero spans\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(pop_size, dim) * span\n        fitness = np.full(pop_size, np.inf)\n\n        evals = 0\n\n        # Evaluate as many initial individuals as budget allows\n        to_eval = min(pop_size, budget - evals)\n        for i in range(to_eval):\n            fitness[i] = float(func(pop[i]))\n            evals += 1\n            if fitness[i] < self.f_opt:\n                self.f_opt = float(fitness[i])\n                self.x_opt = pop[i].copy()\n\n        # If budget too small to evaluate whole pop, remaining individuals keep inf fitness\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.3\n        tau_F = 0.1   # probability to re-sample F\n        tau_CR = 0.1  # probability to re-sample CR\n\n        # trust region radius (fraction of span)\n        trust_radius = 0.25 * np.mean(span)\n        trust_min = 1e-6 * np.mean(span)\n        trust_max = np.mean(span) * 2.0\n\n        # Levy jump probability and scale\n        p_levy = 0.05 + 0.3 * min(1.0, np.log1p(budget) / 10.0)  # moderate chance early\n        levy_scale = 1.0\n\n        # counters for adaptation/stagnation\n        no_improve_count = 0\n        best_so_far = self.f_opt\n        stagnation_reset_threshold = max(200, budget // 10)\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step(scale=1.0, limit=10.0):\n            \"\"\"Return a vector step sampled from a truncated Cauchy distribution.\"\"\"\n            # Draw per-dimension Cauchy samples\n            step = rng.standard_cauchy(size=dim) * scale\n            # truncate large outliers to avoid numeric blow-up\n            step = np.clip(step, -limit, limit)\n            return step\n\n        # Main loop: generations until budget exhausted\n        generation = 0\n        while evals < budget:\n            generation += 1\n            # adaptive per-generation parameter nudges\n            successful_F = []\n            successful_CR = []\n            successes = 0\n\n            # Shuffle order to avoid bias\n            indices = np.arange(pop_size)\n            rng.shuffle(indices)\n\n            for idx in indices:\n                if evals >= budget:\n                    break\n\n                target = pop[idx].copy()\n                target_f = fitness[idx]\n\n                # jDE-like parameter adaptation per individual\n                if rng.rand() < tau_F:\n                    Fi = 0.1 + 0.9 * rng.rand()\n                else:\n                    Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 0.95)\n\n                if rng.rand() < tau_CR:\n                    CRi = rng.rand()\n                else:\n                    CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # Decide if we'll do a Levy jump (centered on best) to generate donor\n                if rng.rand() < p_levy:\n                    # global long jump: best + levy * trust_radius * scaled span\n                    best_center = self.x_opt.copy() if self.x_opt is not None else (lb + ub) / 2.0\n                    levy = levy_step(scale=levy_scale, limit=5.0)\n                    donor = best_center + levy * (trust_radius / (np.mean(span) + 1e-12)) * span\n                    # small random perturbation to preserve diversity\n                    donor += rng.normal(0, 0.02 * np.mean(span), size=dim)\n                else:\n                    # standard DE/rand/1-like mutation biased by best occasionally\n                    # pick three distinct indices different from idx\n                    ids = np.arange(pop_size)\n                    ids = ids[ids != idx]\n                    if ids.size < 3:\n                        # fallback random vector in bounds\n                        donor = lb + rng.rand(dim) * span\n                    else:\n                        a, b, c = rng.choice(ids, 3, replace=False)\n                        # sometimes use best to encourage exploitation\n                        if rng.rand() < 0.2 and self.x_opt is not None:\n                            base = self.x_opt\n                            donor = base + Fi * (pop[a] - pop[b])\n                        else:\n                            base = pop[a]\n                            donor = base + Fi * (pop[b] - pop[c])\n\n                # binomial crossover\n                cross = rng.rand(dim) < CRi\n                if not np.any(cross):\n                    # ensure at least one gene from donor\n                    cross[rng.randint(dim)] = True\n                trial = np.where(cross, donor, target)\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # One evaluation (consume one budget)\n                f_trial = float(func(trial))\n                evals += 1\n\n                # Selection: greedy\n                if f_trial <= target_f:\n                    pop[idx] = trial\n                    fitness[idx] = f_trial\n                    successes += 1\n                    successful_F.append(Fi)\n                    successful_CR.append(CRi)\n                    # move means slightly toward successful parameters\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        no_improve_count = 0\n                    else:\n                        no_improve_count += 1\n                else:\n                    # unsuccessful: keep target, increment stagnation marker\n                    no_improve_count += 1\n\n                # quick stagnation early break\n                if evals >= budget:\n                    break\n\n            # End of generation adjustments: update F_mean and CR_mean using successes (Lehmer-like)\n            if successful_F:\n                # Lehmer mean for F (gives higher weight to larger successful F)\n                sf = np.array(successful_F)\n                F_mean = 0.8 * F_mean + 0.2 * (np.sum(sf ** 2) / (np.sum(sf) + 1e-12))\n            else:\n                # small drift toward exploration if no successes\n                F_mean = np.clip(0.95 * F_mean + 0.05 * (0.2 + 0.8 * rng.rand()), 0.05, 0.99)\n\n            if successful_CR:\n                CR_mean = 0.8 * CR_mean + 0.2 * np.mean(successful_CR)\n            else:\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * rng.rand(), 0.0, 1.0)\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            rem = budget - evals\n            if rem <= 0:\n                break\n\n            # choose number of local samples: small handful scaled by dim and remaining budget\n            n_local = int(min(max(1, dim // 2), rem, 8))\n            local_success = 0\n\n            # Anisotropic sigma: base trust_radius scaled per-dimension by random factors near 1\n            for _ in range(n_local):\n                if evals >= budget:\n                    break\n                if self.x_opt is None:\n                    center = lb + rng.rand(dim) * span\n                else:\n                    center = self.x_opt\n\n                scales = trust_radius * (0.3 + 0.7 * rng.rand(dim))\n                candidate = center + rng.normal(0, scales)\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_c = float(func(candidate))\n                evals += 1\n\n                if f_c < self.f_opt:\n                    # successful local refinement: shrink trust radius slightly\n                    self.f_opt = f_c\n                    self.x_opt = candidate.copy()\n                    trust_radius = max(trust_min, trust_radius * 0.85)\n                    local_success += 1\n                    no_improve_count = 0\n                else:\n                    # unsuccessful local step -> slightly expand trust radius to explore more\n                    trust_radius = min(trust_max, trust_radius * 1.03)\n                    no_improve_count += 1\n\n            # If many successes in DE stage, encourage exploitation by shrinking trust radius\n            if successes > max(1, pop_size // 10):\n                trust_radius = max(trust_min, trust_radius * 0.9)\n            elif successes == 0 and local_success == 0:\n                # stagnation: encourage exploration by increasing Levy jumps probability & trust radius\n                p_levy = min(0.9, p_levy * 1.05 + 0.01)\n                trust_radius = min(trust_max, trust_radius * 1.05)\n            else:\n                # slight cooling of levy probability as we make progress\n                p_levy = max(0.02, p_levy * 0.995)\n\n            # strong stagnation reset\n            if no_improve_count >= stagnation_reset_threshold:\n                # re-seed half of population randomly to diversify\n                n_reset = max(1, pop_size // 2)\n                reset_indices = rng.choice(pop_size, n_reset, replace=False)\n                for ridx in reset_indices:\n                    pop[ridx] = lb + rng.rand(dim) * span\n                    fitness[ridx] = np.inf\n                # evaluate a few new individuals if budget allows\n                for ridx in reset_indices:\n                    if evals >= budget:\n                        break\n                    fitness[ridx] = float(func(pop[ridx]))\n                    evals += 1\n                    if fitness[ridx] < self.f_opt:\n                        self.f_opt = float(fitness[ridx])\n                        self.x_opt = pop[ridx].copy()\n                        no_improve_count = 0\n                # enlarge trust radius a bit after reset\n                trust_radius = min(trust_max, trust_radius * 1.4)\n                # relieve stagnation counter\n                no_improve_count = 0\n                # slightly increase exploration parameter\n                p_levy = min(0.9, p_levy * 1.2)\n\n            # safety to not exceed budget (loop condition will handle but keep tidy)\n            if evals >= budget:\n                break\n\n        # Final results\n        if self.x_opt is None:\n            # fallback: return best evaluated individual if any\n            idx_best = np.argmin(fitness)\n            self.f_opt = float(fitness[idx_best])\n            self.x_opt = pop[idx_best].copy()\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.556 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12116733580695316, 0.17752798654595503, 0.5976675965176761, 0.8718414650734995, 0.8503147420258738, 0.8408282577546651, 0.31423552649758757, 0.7556970213672448, 0.8284582920116117, 0.20468128212937486]}, "task_prompt": ""}
{"id": "f5d5258e-5505-423c-96ff-03a82977df6f", "fitness": 0.25320035313150974, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and a trust-region local search with online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # internal state for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # read bounds if available, otherwise default to [-5,5] (BBOB typical)\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        mean_range = max(range_vec.mean(), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # find best among evaluated\n        valid = np.isfinite(fvals)\n        if valid.any():\n            best_idx_relative = np.argmin(fvals[valid])\n            best_indices = np.where(valid)[0]\n            best_idx = best_indices[best_idx_relative]\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # no evaluations were possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * (range_norm if range_norm > 0 else 1.0)\n        max_trust = (range_norm if range_norm > 0 else 1.0) * 2.0\n        min_trust = 1e-6\n        p_levy = 0.08       # probability of Lévy-centered jump\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step():\n            # standard Cauchy per-dimension; clip extremes\n            s = rng.standard_cauchy(size=self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # small scaling to avoid huge jumps; allow heavy-tailed behaviour\n            return s * 0.8\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            success_Fs = []\n            success_CRs = []\n            # process population sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # decide path: Lévy jump centered on best (exploration) or DE step (exploitation)\n                if rng.rand() < p_levy:\n                    # Lévy exploration: centered on current best with heavy-tail\n                    step = levy_step()\n                    # random scale in [0.5,1.0] times step_scale and trust radius fraction\n                    scale = (0.5 + 0.5 * rng.rand()) * step_scale * (trust_radius / max(range_norm, 1e-12))\n                    donor = best_x + scale * (step * (range_vec / mean_range))\n                    candidate = np.clip(donor, lb, ub)\n                    # mark Fi/CRi not set for levy path\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1/bin mutation + binomial crossover\n                    # draw Fi and CRi per individual (jDE-like perturbation)\n                    Fi = np.clip(rng.normal(F_mean, 0.15), 0.01, 1.0)\n                    CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n                    # random indices distinct from i\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # not enough individuals, fall back to random perturbation around best\n                        donor = best_x + rng.normal(0, 1.0, size=self.dim) * (trust_radius / max(range_norm, 1e-12)) * (range_vec / mean_range)\n                        candidate = np.clip(donor, lb, ub)\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        mutant = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        trial = pop[i].copy()\n                        jrand = rng.randint(0, self.dim)\n                        mask = rng.rand(self.dim) < CRi\n                        mask[jrand] = True\n                        trial[mask] = mutant[mask]\n                        candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # record parameter values if DE branch\n                    if Fi is not None:\n                        success_Fs.append(Fi)\n                    if CRi is not None:\n                        success_CRs.append(CRi)\n                    # small immediate adaptation toward successful Fi/CRi\n                    if Fi is not None:\n                        F_mean = 0.92 * F_mean + 0.08 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.92 * CR_mean + 0.08 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation: adapt means using successful parameter lists (Lehmer-style)\n            if len(success_Fs) > 0:\n                # give more weight to larger Fi (Lehmer mean idea)\n                num = np.sum(np.array(success_Fs) ** 2)\n                den = np.sum(success_Fs)\n                if den > 0:\n                    F_lehmer = num / den\n                    F_mean = 0.9 * F_mean + 0.1 * F_lehmer\n                else:\n                    F_mean = np.clip(F_mean, 0.01, 1.0)\n            if len(success_CRs) > 0:\n                CR_mean = 0.9 * CR_mean + 0.1 * np.mean(success_CRs)\n\n            # trust-region local search around best: small anisotropic Gaussian perturbations\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (0.3 + rng.rand(self.dim) * 0.7) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * (range_vec / mean_range)\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min(trust_radius * 0.85, max_trust), min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    trust_radius = max(trust_radius, min_trust)\n                    stagnation_counter += 1\n\n            # adapt Lévy probability and some global params based on success\n            if successes > 0:\n                # successful generation: reduce tendency to long jumps slightly\n                p_levy = max(0.01, p_levy * (0.95 if successes > self.pop_size * 0.2 else 0.98))\n                # nudge F_mean/CR_mean slightly toward conservative defaults\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.01, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnating: increase chance of long jumps and diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.01, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many steps, re-seed half of the population\n            if stagnation_counter > max(50, self.dim * 6):\n                k = max(1, self.pop_size // 2)\n                ids = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in ids:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset to re-explore\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.253 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14123090142180694, 0.19916055045891745, 0.37452129357960606, 0.24581864232961625, 0.2553122422752545, 0.32283549575001225, 0.2560806747505665, 0.3067141639419022, 0.23600589492925528, 0.1943236718781598]}, "task_prompt": ""}
{"id": "a6e2e001-b040-4f85-8815-625c034803b0", "fitness": 0.2983248512285481, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with per-individual jDE-like adaptation, occasional Lévy (Cauchy) long jumps for exploration, and a trust-region Gaussian local search around the best; all components adapt online from success history.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n    - Differential Evolution (rand/1-style) with per-individual F and CR sampled around running means (jDE-like adaptation).\n    - Occasional Lévy-like (Cauchy) jumps centered on the current best to escape basins and explore globally.\n    - Trust-region local Gaussian sampling around the best to exploit promising regions.\n    - Online adaptation of F_mean, CR_mean and p_levy; stagnation-triggered partial re-seeding.\n    Designed to work with continuous box-bounded functions (func.bounds.lb, func.bounds.ub).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # default population size scaled with problem dimension (bounded by budget)\n        if pop_size is None:\n            p = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            max_from_budget = max(4, self.budget // 20)\n            self.pop_size = min(p, max_from_budget)\n        else:\n            self.pop_size = max(4, int(pop_size))\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.ravel()[0]))\n        if b.size == self.dim:\n            return b.reshape(self.dim)\n        # try flatten then broadcast if possible\n        if b.ndim == 2 and b.shape[0] == 1 and b.shape[1] == self.dim:\n            return b.ravel()\n        raise ValueError(\"Bounds must be scalar or of length dim\")\n\n    def __call__(self, func):\n        # get bounds and range\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # ensure ub>lb\n        if np.any(ub <= lb):\n            raise ValueError(\"Upper bound must be greater than lower bound for all dimensions.\")\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if no evaluations possible\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # determine current best among evaluated individuals\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-parameters\n        F_mean = 0.6        # mean differential weight\n        CR_mean = 0.9       # mean crossover prob\n        p_levy = 0.05       # initial chance of Lévy jump per trial\n        step_scale = 0.25   # base multiplier for Levy steps\n        trust_radius = 0.2 * range_norm\n        min_trust = 1e-8\n        max_trust = max(1e-12, 2.0 * range_norm)\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: Levy-like step using Cauchy distribution (heavy-tailed)\n        def levy_step():\n            s = self.rng.standard_cauchy(self.dim)\n            # clip extremely large outliers (keeps heavy tail but avoids overflow)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: generate candidates until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            S_F = []  # successful F values in this generation\n            S_CR = []  # successful CR values in this generation\n\n            # per-generation small jitter to means (encourages exploration)\n            F_mean = np.clip(F_mean * (1.0 + 0.01 * (self.rng.rand() - 0.5)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean * (1.0 + 0.01 * (self.rng.rand() - 0.5)), 0.0, 1.0)\n\n            # process population sequentially, each candidate evaluation consumes budget\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # choose strategy: Levy jump centered on best or DE/rand/1 mutation\n                if self.rng.rand() < p_levy:\n                    # Levy exploration jump centered on best_x\n                    step = levy_step()\n                    scale = step_scale * (0.5 + 0.5 * self.rng.rand())\n                    donor = best_x + scale * step * (range_vec / max(range_vec.mean(), 1e-12))\n                    # small random perturbation to encourage variety\n                    donor = donor + 0.05 * (self.rng.rand(self.dim) - 0.5) * range_vec\n                    # no Fi/CR used in Levy branch\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1 style mutation with per-individual Fi and binomial crossover\n                    # pick 3 distinct indices not equal to i\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(idxs) < 3:\n                        # fallback: small random perturbation if population too small\n                        donor = pop[i] + 0.1 * (self.rng.rand(self.dim) - 0.5) * range_vec\n                        Fi = None\n                        CRi = None\n                    else:\n                        r = self.rng.choice(idxs, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        # sample Fi and CRi (jDE-like)\n                        Fi = float(np.clip(self.rng.normal(F_mean, 0.1), 0.05, 1.0))\n                        CRi = float(np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0))\n                        donor_vec = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = self.rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[self.rng.randint(0, self.dim)] = True\n                        donor = np.where(cr_mask, donor_vec, pop[i])\n\n                # project to bounds\n                candidate = np.minimum(np.maximum(donor, lb), ub)\n\n                # evaluate if still budget\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # record successful Fi/CR for adaptation if they exist\n                    if Fi is not None:\n                        S_F.append(Fi)\n                    if CRi is not None:\n                        S_CR.append(CRi)\n                    # nudge global means slightly toward successful values\n                    if Fi is not None:\n                        F_mean = float(np.clip(0.9 * F_mean + 0.1 * Fi, 0.05, 0.99))\n                    if CRi is not None:\n                        CR_mean = float(np.clip(0.9 * CR_mean + 0.1 * CRi, 0.0, 1.0))\n\n                # Update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # adapt p_levy mildly after each evaluation (favor exploration if stagnating)\n                if stagnation_counter > max(10, self.dim):\n                    p_levy = min(0.5, p_levy * 1.02)\n                else:\n                    p_levy = max(0.01, p_levy * 0.995)\n\n            # End of a generation: trust-region local exploitation (small batch)\n            if evals < self.budget:\n                remaining = self.budget - evals\n                local_samples = min(5 + max(0, self.dim // 10), remaining, 8)\n                for _ in range(int(local_samples)):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic Gaussian noise scaled by trust_radius and problem range\n                    sigma = (0.5 + self.rng.rand(self.dim) * 0.5) * (trust_radius / max(range_norm, 1e-12))\n                    candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step => shrink trust radius to focus\n                        trust_radius = max(min_trust, trust_radius * 0.7)\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful => expand a bit to encourage escape\n                        trust_radius = min(max_trust, trust_radius * 1.1)\n\n            # generation-level adaptation using collected successful Fi/CRi\n            if len(S_F) > 0:\n                mean_SF = float(np.mean(S_F))\n                F_mean = float(np.clip(0.85 * F_mean + 0.15 * mean_SF, 0.05, 0.99))\n            if len(S_CR) > 0:\n                mean_SCR = float(np.mean(S_CR))\n                CR_mean = float(np.clip(0.85 * CR_mean + 0.15 * mean_SCR, 0.0, 1.0))\n\n            # stagnation handling: partial re-seed if stuck\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population (random positions)\n                half = max(1, self.pop_size // 2)\n                reinit_indices = self.rng.choice(np.arange(self.pop_size), half, replace=False)\n                for j in reinit_indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius to give wider search after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # slightly increase chance of Levy jumps after reset\n                p_levy = min(0.5, p_levy * 1.5)\n\n            # ensure boundaries for trust radius and p_levy\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n            p_levy = float(np.clip(p_levy, 0.01, 0.5))\n\n            # small generation-level randomization to avoid premature convergence on means\n            F_mean = float(np.clip(F_mean * (1.0 + 0.005 * (self.rng.rand() - 0.5)), 0.05, 0.99))\n            CR_mean = float(np.clip(CR_mean * (1.0 + 0.005 * (self.rng.rand() - 0.5)), 0.0, 1.0))\n\n        # assign final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.298 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13009555913753112, 0.2724224561341654, 0.3461620143245352, 0.465881794158962, 0.2899817039755326, 0.4004012314430111, 0.2678763234641185, 0.30779512625631444, 0.29117014698581, 0.21146215640550114]}, "task_prompt": ""}
{"id": "b92ea13d-cb1d-499e-aa7a-d5778c8e6c47", "fitness": 0.2100194164689893, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like adaptation, occasional Lévy (Cauchy) long jumps for global exploration, and a trust-region Gaussian local search around the best with online step-size and jump-probability adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (mutation factor F, crossover CR, Lévy probability p_levy and trust\n    radius) are adapted online based on success history.\n\n    Usage:\n        alg = HybridDeLevyTrust(budget=10000, dim=10, seed=42)\n        f_opt, x_opt = alg(func)\n    Notes:\n        - func.bounds.lb and func.bounds.ub are used (can be scalars or arrays).\n        - The algorithm will not evaluate func more than `budget` times.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # sensible default population size if not provided\n        if pop_size is None:\n            # scale with dimension but limited by budget\n            self.pop_size = int(max(4, min(12 * self.dim, max(4, self.budget // 10))))\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        # try to broadcast to dimension\n        return np.reshape(b.astype(float), (self.dim,))\n\n    def __call__(self, func):\n        # get bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # safety: if bounds invalid, fallback to [-5,5]\n        if np.any(ub <= lb):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        range_vec = ub - lb\n\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population until budget or full population\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # check any evaluations performed\n        valid = ~np.isinf(fvals)\n        if not np.any(valid):\n            # no evaluation possible within budget\n            self.x_opt = None\n            self.f_opt = float(np.inf)\n            return self.f_opt, self.x_opt\n\n        # find best among evaluated individuals\n        best_idx = np.argmin(fvals[valid])\n        best_indices = np.nonzero(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6    # mean mutation factor for DE\n        CR_mean = 0.5   # mean crossover rate\n        p_levy = 0.06   # probability of performing a Levy jump instead of DE mutation\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius\n        max_trust = np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 1.0\n        min_trust = 1e-8\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # per-generation randomized center perturbations for diversity\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Decide branch: Levy jump focused on best (exploration) or DE mutation (exploitation & recombination)\n                if rng.random() < p_levy:\n                    # Lévy jump centered on best for long-range exploration\n                    s = levy_step()\n                    # scale relative to problem range, with a randomized scaling factor\n                    scale = step_scale * (0.5 + 0.5 * rng.random())\n                    candidate = best_x + scale * s * (range_vec / max(range_vec.mean(), 1e-12))\n                else:\n                    # DE/rand/1-like mutation with binomial crossover and jDE-like parameter sampling\n                    idxs = np.arange(self.pop_size)\n                    if idxs.size < 4:\n                        # trivial fallback: gaussian perturbation\n                        candidate = pop[i] + 0.1 * (rng.standard_normal(self.dim) * (range_vec / np.maximum(range_vec.mean(), 1e-12)))\n                    else:\n                        idxs = idxs[idxs != i]\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        # sample per-individual Fi and CRi from normals around means\n                        Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        jrand = rng.integers(self.dim)\n                        mask = rng.random(self.dim) < CRi\n                        mask[jrand] = True\n                        candidate = np.where(mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    # accept into population\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt means toward successful Fi/CRi if present (DE branch)\n                    if 'Fi' in locals():\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if 'CRi' in locals():\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                else:\n                    # unsuccessful candidate gives slight pressure to diversify\n                    stagnation_counter += 1\n\n                # clear local Fi/CRi if they exist to avoid accidental reuse\n                if 'Fi' in locals():\n                    del Fi\n                if 'CRi' in locals():\n                    del CRi\n\n            # trust-region local search around best: sample a small number of local Gaussian candidates\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: trust_radius scaled per-dimension by random factor\n                sigma = trust_radius * (0.2 + 0.8 * rng.random(self.dim))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus search\n                    trust_radius *= 0.85\n                    trust_radius = np.clip(trust_radius, min_trust, max_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = np.clip(trust_radius, min_trust, max_trust)\n                    stagnation_counter += 1\n\n            # adapt Lévy probability and DE means based on success rate\n            if successes > 0:\n                # if many successes, reduce exploration (Lévy) slowly\n                p_levy = max(0.01, p_levy * (0.95 if successes > self.pop_size * 0.2 else 0.98))\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase mutation scaling\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                reinit_idxs = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reinit_idxs:\n                    if evals >= self.budget:\n                        break\n                    # reinitialize and evaluate\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius to encourage exploration after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.210 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10775585383646447, 0.16159553961682716, 0.2703414398648806, 0.27368331912282506, 0.22123500524151274, 0.26272499907858526, 0.2294154737610905, 0.2325130410069779, 0.20800333884692013, 0.13292615431380939]}, "task_prompt": ""}
{"id": "096545d7-de1e-4728-919a-24ff9baf5987", "fitness": 0.30370982116273915, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining population-based Differential Evolution, occasional Lévy-flight global jumps, and a trust-region Gaussian local search with online adaptation of mutation/crossover and restart on stagnation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent successes. Boundaries are assumed\n    to be [-5, 5] in each dimension (Many Affine BBOB standard).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # set bounds consistent with Many Affine BBOB\n        self.lb = -5.0\n        self.ub = 5.0\n        self.range_vec = self.ub - self.lb\n\n        # population size scaled with dimension if not provided\n        if pop_size is None:\n            # keep population modest so small budgets still useful\n            base = int(max(4, min(40, 6 + int(np.sqrt(self.dim)*2))))\n            self.pop_size = min(base, max(2, self.budget // 5))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n        # output placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _clip(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def __call__(self, func):\n        rng = self.rng\n        dim = self.dim\n        lb = self.lb\n        ub = self.ub\n        range_vec = np.full(dim, self.range_vec)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n        # evaluate initial population until budget exhausted\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            xi = pop[i].copy()\n            f = float(func(xi))\n            fvals[i] = f\n            evals += 1\n\n        # if no evaluations were possible (budget==0)\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # establish current best\n        finite_mask = np.isfinite(fvals)\n        if np.any(finite_mask):\n            best_idx = np.argmin(fvals[finite_mask])\n            best_indices = np.where(finite_mask)[0]\n            best_i = best_indices[best_idx]\n            best_x = pop[best_i].copy()\n            best_f = float(fvals[best_i])\n        else:\n            # if somehow none finite, pick random individual and evaluate if budget allows\n            j = 0\n            best_x = pop[j].copy()\n            if evals < self.budget:\n                best_f = float(func(best_x)); evals += 1\n            else:\n                best_f = np.inf\n\n        # adaptive hyperparameters\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.06  # base probability for Levy long jump\n        step_scale = 0.25  # base scale for Levy relative to range\n        trust_radius = 0.2 * np.linalg.norm(range_vec)\n        min_trust = 1e-6\n        max_trust = 2.0 * np.linalg.norm(range_vec)\n\n        # success memories for small-learn adaptation (like SHADE/jDE lite)\n        S_F = []\n        S_CR = []\n        c_adapt = 0.1  # learning rate for mean updates\n\n        stagnation_counter = 0\n        gen = 0\n        max_stagnation = max(20, self.pop_size * 3)\n\n        # helper: Levy-like heavy-tail step via scaled Cauchy\n        def levy_step():\n            s = rng.standard_cauchy(size=dim)\n            # clamp extreme outliers to avoid blow-ups\n            s = np.clip(s, -1e3, 1e3)\n            # normalize to unit-ish scale\n            s = s / (np.linalg.norm(s) + 1e-12)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # per-generation temporary success lists\n            gen_S_F = []\n            gen_S_CR = []\n            improved_this_gen = False\n\n            # shuffle order to avoid biases\n            order = rng.permutation(self.pop_size)\n\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi around means\n                # Fi drawn from Cauchy around mean (like jDE/JADE practice), truncated\n                Fi = rng.standard_cauchy() * 0.1 + F_mean\n                Fi = np.clip(Fi, 0.05, 1.0)\n                CRi = rng.normal(CR_mean, 0.1)\n                CRi = np.clip(CRi, 0.0, 1.0)\n\n                # decide exploration via Levy\n                if rng.rand() < p_levy:\n                    # Levy jump centered on best_x\n                    step = levy_step()\n                    scale = step_scale * (0.5 + rng.rand() * 1.0) * (np.linalg.norm(range_vec) / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                    candidate = best_x + scale * step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    # to add diversity, sometimes add a small component of diff of two random individuals\n                    if self.pop_size >= 2 and rng.rand() < 0.5:\n                        a, b = rng.choice(self.pop_size, 2, replace=False)\n                        candidate += 0.1 * Fi * (pop[a] - pop[b])\n                else:\n                    # DE/rand/1 mutation\n                    # pick r1,r2,r3 distinct from idx\n                    if self.pop_size >= 4:\n                        r = rng.choice([j for j in range(self.pop_size) if j != idx], 3, replace=False)\n                        r1, r2, r3 = r\n                    else:\n                        # fallback when small population: use random draws allowing repeats\n                        r1, r2, r3 = rng.randint(0, self.pop_size, 3)\n                        if r1 == idx: r1 = (r1 + 1) % self.pop_size\n\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # crossover (binomial)\n                    cr_mask = rng.rand(dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, dim)] = True\n                    candidate = np.where(cr_mask, donor, pop[idx])\n\n                # projection to bounds\n                candidate = self._clip(candidate)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                # If current fval is inf (never evaluated), treat as replaceable\n                if not np.isfinite(fvals[idx]) or f_candidate < fvals[idx]:\n                    pop[idx] = candidate.copy()\n                    fvals[idx] = f_candidate\n                    gen_S_F.append(Fi)\n                    gen_S_CR.append(CRi)\n                    improved_this_gen = True\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # adapt immediate (small nudges) if success\n                if f_candidate < best_f:\n                    F_mean = np.clip(0.95 * F_mean + 0.05 * Fi, 0.05, 0.99)\n                    CR_mean = np.clip(0.95 * CR_mean + 0.05 * CRi, 0.0, 1.0)\n\n            # end of population loop\n\n            # update means from generation successes (Lehmer-like or arithmetic)\n            if len(gen_S_F) > 0:\n                # Lehmer mean gives more weight to larger Fi\n                gen_S_F = np.array(gen_S_F)\n                gen_S_CR = np.array(gen_S_CR)\n                lehmer = (np.sum(gen_S_F**2) / np.sum(gen_S_F + 1e-12))\n                meanF = np.clip(lehmer, 0.05, 1.0)\n                meanCR = np.clip(np.mean(gen_S_CR), 0.0, 1.0)\n                F_mean = (1.0 - c_adapt) * F_mean + c_adapt * meanF\n                CR_mean = (1.0 - c_adapt) * CR_mean + c_adapt * meanCR\n                # reward reduced levy probability when exploitation is working\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                # no successes this generation: encourage exploration\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                # slightly increase F_mean to encourage larger steps\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # Trust-region local search around best_x\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # choose number of local samples modestly\n            local_samples = min( max(1, dim//2), remaining, 6 )\n            local_improved = False\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by random per-dim factor\n                sigma = (0.4 + rng.rand(dim) * 0.6) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=dim) * sigma * range_vec\n                candidate = self._clip(candidate)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # accept and shrink trust radius for focused exploitation\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    local_improved = True\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful local step => expand slightly to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            if local_improved:\n                # reward exploitation by nudging F_mean/CR_mean\n                F_mean = np.clip(0.98 * F_mean + 0.02 * 0.5, 0.05, 0.99)\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.9, 0.0, 1.0)\n            else:\n                # no local improvement -> raise chance of Levy moves gradually\n                p_levy = min(0.6, p_levy * 1.03 + 0.005)\n\n            # stagnation handling: if many evals without improvement, partially re-seed\n            if stagnation_counter > max_stagnation:\n                # reinitialize half of the population but try to keep the best\n                k = max(1, self.pop_size // 2)\n                replaced = rng.choice([i for i in range(self.pop_size)], k, replace=False)\n                for j in replaced:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # update global best if any replacement produced improvement\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset to explore new region\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # reset counters and make exploration more likely\n                stagnation_counter = 0\n                p_levy = min(0.9, p_levy * 1.2)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.304 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11811459332602159, 0.1672183836537604, 0.4254417104602072, 0.4545952602423501, 0.27983807002384276, 0.449204204805554, 0.2533267461468004, 0.3771659629952693, 0.3711598862166461, 0.14103339375693946]}, "task_prompt": ""}
{"id": "f6ccaa00-bc5c-4443-9e86-ff9385672158", "fitness": 0.1781365054698044, "name": "AdaptiveHybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online parameter adaptation for fast global exploration and focused local refinement.", "code": "import numpy as np\n\nclass AdaptiveHybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n      - Differential Evolution style population search with per-individual Fi/CRi (jDE-like adaptation)\n      - Occasional Lévy-like (Cauchy) jumps centered on the best for long-range exploration\n      - Trust-region Gaussian local search around the current best for focused exploitation\n      - Online adaptation of F_mean, CR_mean, p_levy and trust radius based on success history\n\n    Usage:\n      optimizer = AdaptiveHybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=1)\n      fopt, xopt = optimizer(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: scale with dimension but limited by budget\n        if pop_size is None:\n            pop_size = int(np.clip(10 * self.dim, 6, max(6, self.budget // 5)))\n        self.pop_size = max(4, int(pop_size))\n        # RNG\n        self.rng = np.random.RandomState(seed)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-likes and return array of length self.dim\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.copy().astype(float)\n        # try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be interpreted as length-dim arrays\")\n\n    def __call__(self, func):\n        # extract bounds (the BBOB interface provides func.bounds.lb/ub)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # Some test suites might give global bounds not exactly -5/5; use provided.\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        rng = self.rng\n        pop_size = self.pop_size\n        dim = self.dim\n        budget = self.budget\n\n        # Initialize population\n        pop = lb + rng.rand(pop_size, dim) * range_vec\n        fvals = np.full(pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # Evaluate initial population sequentially until budget allows\n        for i in range(pop_size):\n            if evals >= budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # If none evaluated (zero budget) return empty\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # Fill any unevaluated individuals with random values (not evaluated)\n        for i in range(pop_size):\n            if np.isinf(fvals[i]):\n                pop[i] = lb + rng.rand(dim) * range_vec\n                # leave fvals[i] as inf so they won't be chosen as best until evaluated\n\n        # Best-so-far\n        valid = ~np.isinf(fvals)\n        best_idx = np.argmin(fvals[valid]) if np.any(valid) else 0\n        best_indices = np.where(valid)[0]\n        best_idx = best_indices[best_idx] if np.any(valid) else 0\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # Algorithm hyperparameters and adaptive state\n        F_mean = 0.6        # mean differential weight\n        CR_mean = 0.9       # mean crossover\n        p_levy = 0.05       # probability of performing a Lévy jump\n        trust_radius = 0.1  # relative radius (fraction of search range)\n        min_trust = 1e-6\n        max_trust = 2.0     # can expand a bit\n        stagnation_counter = 0\n\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step():\n            s = rng.standard_cauchy(size=dim)\n            s = np.clip(s, -1e3, 1e3)\n            # scale down by a moderate factor so not always huge\n            return s / 10.0\n\n        # helper to clip to bounds\n        def project(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # main loop: generations until budget exhausted\n        while evals < budget:\n            gen += 1\n            successes = 0\n            successful_F = []\n            successful_CR = []\n            # Shuffle processing order each generation to avoid bias\n            order = rng.permutation(pop_size)\n\n            for idx in order:\n                if evals >= budget:\n                    break\n\n                # If this individual has inf value (no eval initially), evaluate and continue\n                if np.isinf(fvals[idx]):\n                    x_eval = pop[idx].copy()\n                    f_candidate = float(func(x_eval))\n                    fvals[idx] = f_candidate\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = x_eval.copy()\n                        stagnation_counter = 0\n                    continue\n\n                # sample per-individual Fi and CRi (normal jitter around means, clipped)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 0.99))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # Decide whether to do Lévy jump or DE mutation\n                if rng.rand() < p_levy:\n                    # Lévy jump centered at best, scaled by search range and trust radius\n                    step = levy_step()\n                    donor = best_x + step * (trust_radius * range_vec)\n                else:\n                    # DE/rand/1 donor (choose 3 distinct others)\n                    ids = np.arange(pop_size)\n                    ids = ids[ids != idx]\n                    r1, r2, r3 = rng.choice(ids, size=3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover -> trial vector\n                cr_mask = rng.rand(dim) < CRi\n                # ensure at least one dimension is taken from donor\n                if not np.any(cr_mask):\n                    cr_mask[rng.randint(0, dim)] = True\n                trial = np.where(cr_mask, donor, pop[idx])\n\n                # project to bounds\n                trial = project(trial)\n\n                # evaluate trial (count evaluation)\n                f_candidate = float(func(trial))\n                evals += 1\n\n                # greedy selection\n                if f_candidate < fvals[idx]:\n                    # accept\n                    pop[idx] = trial\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    successful_F.append(Fi)\n                    successful_CR.append(CRi)\n                    # nudge means towards successful parameters (simple adaptation)\n                    # will compute aggregated update after loop\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = trial.copy()\n                        stagnation_counter = 0\n                else:\n                    # unsuccessful -> mild stagnation increment\n                    stagnation_counter += 1\n\n            # End of population iteration (one generation)\n            # Adapt F_mean and CR_mean using Lehmer-like means of successes (jDE-ish)\n            if len(successful_F) > 0:\n                # Lehmer mean favors larger F if they helped\n                num = np.sum(np.array(successful_F) ** 2)\n                den = np.sum(successful_F)\n                if den > 0:\n                    F_mean = 0.9 * F_mean + 0.1 * (num / den)\n                CR_mean = 0.9 * CR_mean + 0.1 * (np.mean(successful_CR))\n                # reduce p_levy slightly if successes are many\n                p_levy *= 0.95 if successes > max(1, 0.2 * pop_size) else 0.98\n            else:\n                # no successful DE moves: encourage exploration\n                p_levy = min(0.8, p_levy * 1.1 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n\n            # Trust-region local search: perform a few local Gaussian samples around best\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n            # local budget per generation is limited\n            local_budget = min( max(1, dim // 2), remaining, 2 + dim // 5 )\n            improved_local = False\n            for _ in range(local_budget):\n                if evals >= budget:\n                    break\n                # anisotropic sigma scaled by trust radius and normalized range\n                # relative sigma per-dim in [0.3, 1.0] times trust_radius\n                sigma_rel = (0.3 + rng.rand(dim) * 0.7) * trust_radius\n                candidate = best_x + rng.normal(0.0, 1.0, size=dim) * (sigma_rel * range_vec)\n                candidate = project(candidate)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    improved_local = True\n                    stagnation_counter = 0\n                    # shrink trust radius to focus more\n                    trust_radius = max(min_trust, trust_radius * 0.8)\n                else:\n                    # slightly expand to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # adjust p_levy by success rate (reduce if many successes, increase if stagnating)\n            success_rate = successes / max(1, pop_size)\n            if success_rate > 0.2:\n                p_levy = max(0.01, p_levy * 0.9)\n            else:\n                p_levy = min(0.5, p_levy * (1.0 + 0.03 * (1.0 - success_rate)))\n\n            # stagnation handling: if many evaluations with no improvement, re-seed some individuals\n            if stagnation_counter > max(50, 5 * dim):\n                k = max(1, pop_size // 2)\n                reinit_indices = rng.choice(pop_size, size=k, replace=False)\n                for j in reinit_indices:\n                    if evals >= budget:\n                        break\n                    pop[j] = lb + rng.rand(dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # enlarge trust radius a bit to allow wide search after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # reduce p_levy burst after reset to keep exploration moderate\n                p_levy = min(0.5, p_levy * 1.2)\n\n            # update global stored results\n            self.f_opt = best_f\n            self.x_opt = best_x.copy()\n\n            # safety: if best is already near global minimum threshold (optional), could break early\n            # (Not used here; leave outer loop to exhaust budget)\n\n        # final results\n        return float(self.f_opt), self.x_opt.copy() if self.x_opt is not None else None", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveHybridDeLevyTrust scored 0.178 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09310714799837982, 0.1535968536029516, 0.21652052650177922, 0.23137347371198003, 0.19274130811991996, 0.2065935777522021, 0.20402899008099384, 0.17507622487790064, 0.1687810450872681, 0.13954590696466873]}, "task_prompt": ""}
{"id": "2b164fe7-384a-4560-8501-d9dc2e767ad3", "fitness": 0.27992038139729025, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for rare long-range exploration, and a trust-region local search with online step-size adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters (F, CR,\n    trust radius, and Lévy probability) are adapted online based on recent\n    success history to balance fast global exploration and focused local\n    exploitation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size scaled with dimensionality and budget\n        if pop_size is None:\n            # Ensure at least 4 individuals; limit proportional to budget\n            self.pop_size = int(np.clip(4 * max(1, self.dim), 4, max(4, self.budget // 8)))\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Accept scalar or array-like lower/upper bounds and return a 1D float array of length self.dim.\n        \"\"\"\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # Try broadcast if possible\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            # fallback: fill with first element\n            return np.full(self.dim, float(b.flat[0]), dtype=float)\n\n    def __call__(self, func):\n        # Prepare bounds and range\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        rng = self.rng\n\n        # Initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # Evaluate initial population until budget exhausted or full pop evaluated\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # If no evaluations possible\n        valid_mask = np.isfinite(fvals)\n        if not np.any(valid_mask):\n            self.f_opt = float(np.inf)\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # Identify current best among evaluated individuals\n        best_idx = np.argmin(fvals[valid_mask])\n        # map back to absolute index\n        valid_idx = np.flatnonzero(valid_mask)\n        best_idx = valid_idx[best_idx]\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # Adaptive hyper-parameters (start values)\n        F_mean = 0.6\n        CR_mean = 0.9\n        step_scale = 0.25  # base scale for levy/trust radius as fraction of search range\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        trust_radius = max(min_trust, step_scale * np.linalg.norm(range_vec))\n\n        # Levy jump probability and scaling\n        p_levy = 0.05\n        levy_scale = 0.8  # multiplier for the Lévy step amplitude in units of range_vec\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step(dim):\n            # Cauchy(0,1) via tan(pi*(u-0.5)), truncate outliers\n            u = rng.rand(dim)\n            step = np.tan(np.pi * (u - 0.5))\n            # truncate extreme outliers to avoid numerical blow-up, keep tails but bounded\n            step = np.clip(step, -50.0, 50.0)\n            return step\n\n        # Main loop: proceed until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            improved_in_gen = False\n\n            # process the population in random order to avoid bias\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # Decide operation: occasional Levy, otherwise DE\n                if rng.rand() < p_levy:\n                    # Lévy-centered exploration around the current best\n                    L = levy_step(self.dim)\n                    # scale relative to trust radius and problem range\n                    scale = levy_scale * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                    donor = best_x + L * scale * range_vec\n                    # Crossover: use larger CR for pure exploration\n                    CRi = np.clip(rng.normal(CR_mean + 0.1, 0.15), 0.0, 1.0)\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[idx])\n                else:\n                    # DE/rand/1 with jDE-like adaptation of Fi and CRi\n                    # choose three distinct indices different from idx\n                    candidates = np.delete(np.arange(self.pop_size), idx)\n                    if candidates.size < 3:\n                        # fallback: small Gaussian perturbation\n                        donor = pop[idx] + rng.normal(0, 0.1, size=self.dim) * range_vec\n                        CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                    else:\n                        r1, r2, r3 = rng.choice(candidates, 3, replace=False)\n                        # adapt Fi and CRi by sampling around means\n                        Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0))\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # occasional tiny Lévy perturbation to donor to increase diversity\n                        if rng.rand() < 0.02:\n                            donor += levy_scale * 0.1 * levy_step(self.dim) * range_vec\n                        cr_mask = rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[rng.randint(0, self.dim)] = True\n                        trial = np.where(cr_mask, donor, pop[idx])\n\n                # Projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(trial))\n                evals += 1\n\n                # Selection: greedy replacement\n                if not np.isfinite(fvals[idx]) or (f_candidate < fvals[idx]):\n                    # successful replacement in population\n                    pop[idx] = trial.copy()\n                    fvals[idx] = f_candidate\n                    # nudge means toward the successful Fi/CRi if available\n                    if 'Fi' in locals() and 'CRi' in locals():\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    else:\n                        # in Levy case, prefer slightly more exploratory F_mean/CR_mean drift\n                        F_mean = np.clip(0.98 * F_mean, 0.05, 0.99)\n                        CR_mean = np.clip(0.98 * CR_mean, 0.0, 1.0)\n\n                # Update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = trial.copy()\n                    improved_in_gen = True\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # Clean up locals Fi/CRi to avoid accidental reuse\n                if 'Fi' in locals():\n                    del Fi\n                if 'CRi' in locals():\n                    del CRi\n\n            # End of generation adjustments\n\n            # Trust-region local search around best: small Gaussian samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # Choose number of local evaluations: small handful scaled by dim and remaining budget\n            n_local = int(np.clip(2 + self.dim // 4, 2, min(10, remaining)))\n            local_success = False\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (0.3 + rng.rand(self.dim) * 0.7) * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success = True\n                    stagnation_counter = 0\n                    # also possibly insert candidate into population replacing worst\n                    worst_idx = int(np.argmax(fvals))\n                    pop[worst_idx] = candidate.copy()\n                    fvals[worst_idx] = f_candidate\n\n            # Adapt trust region according to local success\n            if local_success:\n                # shrink trust radius to focus further\n                trust_radius = max(min_trust, trust_radius * 0.7)\n                # reward decreasing exploration probability slightly\n                p_levy = max(0.01, p_levy * 0.9)\n            else:\n                # expand trust radius to escape local traps\n                trust_radius = min(max_trust, trust_radius * 1.08)\n                # if stagnating, increase chance of long jumps\n                if stagnation_counter > 3 * self.pop_size:\n                    p_levy = min(0.6, p_levy * 1.5)\n                else:\n                    p_levy = max(0.01, p_levy * 0.98)\n\n            # Gentle drift of means to prefer exploitation if many successes, else exploration\n            if improved_in_gen:\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.5, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n\n            # Stagnation handling: if no improvement for long, re-seed part of the population\n            if stagnation_counter > 8 * self.pop_size:\n                # reinitialize half of the population away from the best\n                n_reseed = max(1, self.pop_size // 2)\n                indices = rng.choice(np.arange(self.pop_size), n_reseed, replace=False)\n                for j in indices:\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = np.inf  # will be evaluated in subsequent generations if budget remains\n                # slightly enlarge trust radius to encourage escape\n                trust_radius = min(max_trust, trust_radius * 1.4)\n                # increase levy chance\n                p_levy = min(0.5, p_levy * 1.5)\n                stagnation_counter = 0  # reset\n\n            # Update class-level best and record\n            self.f_opt = float(best_f)\n            self.x_opt = best_x.copy()\n\n            # Small safety break in case of pathological loops (should not be reached due to evals check)\n            if gen > 100000:\n                break\n\n        # Final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.280 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1183034103697217, 0.15821640328087772, 0.45236335499854496, 0.2592862204086861, 0.2085064174063187, 0.44487872806811113, 0.29374410283402175, 0.38224370920569095, 0.341280229772968, 0.14038123762796184]}, "task_prompt": ""}
{"id": "7e8bfa1f-2d36-4ecc-a3eb-2961fd8670cd", "fitness": 0.243785934743373, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and trust-region local searches with online step-size and operator adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Adaptive hybrid optimizer: Differential Evolution + occasional Lévy jumps +\n    trust-region local search with online step-size/adaptation.\n    Designed for continuous box-constrained problems (bounds provided by func).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size heuristic: grows mildly with dimension but limited by budget\n        if pop_size is None:\n            guessed = int(8 + 2 * np.log(self.dim + 1) * self.dim**0.25)\n            guessed = max(6, guessed)\n            # don't allocate more than a reasonable fraction of budget\n            guessed = min(guessed, max(4, self.budget // 10))\n            self.pop_size = guessed\n        else:\n            self.pop_size = int(pop_size)\n        # output placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full((self.dim,), float(b.item()))\n        if b.shape == (self.dim,):\n            return b.astype(float)\n        # try broadcasting scalar-like\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # prepare bounds and range\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialization\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n        evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # If no budget, return defaults\n        if self.budget <= 0:\n            return self.f_opt, self.x_opt\n\n        # Evaluate initial population sequentially until budget runs out\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                f = float(func(pop[i]))\n            except Exception:\n                # if evaluation fails, keep inf and continue\n                f = np.inf\n            fvals[i] = f\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = pop[i].copy()\n\n        # If we couldn't evaluate any candidate, return None\n        if evals == 0:\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6      # mean mutation factor\n        CR_mean = 0.9     # mean crossover probability\n        p_levy = 0.08     # probability of a Lévy jump for an individual\n        trust_radius = 0.2 * range_norm\n        min_trust = 1e-8\n        max_trust = 5.0 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        # Levy-like heavy-tailed step (Cauchy surrogate) with clipping\n        def levy_step(scale_factor=1.0):\n            # Cauchy distributed steps per-dimension, scaled by trust radius and range\n            scale = 0.7 * (trust_radius / max(range_norm, 1e-12)) * scale_factor\n            # sample standard Cauchy; clip to avoid numerical blow-up\n            s = self.rng.standard_cauchy(size=self.dim) * scale\n            # clip extremely large values (rare)\n            clip_val = max(10.0 * scale, 1e3 * np.finfo(float).eps)\n            s = np.clip(s, -clip_val, clip_val)\n            # scale with problem range so step is in coordinate units\n            return s * range_vec\n\n        # main optimization loop (generational but respects budget tightly)\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # adapt per-generation perturbation to diversity\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi (jDE-like mild adaptation)\n                Fi = np.clip(self.rng.normal(F_mean, 0.12), 0.05, 0.95)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                # choose DE or Lévy exploration\n                if self.rng.rand() < p_levy:\n                    # Lévy centered on current best (long jump exploration)\n                    base = self.x_opt.copy() if self.x_opt is not None else pop[i]\n                    step = levy_step(scale_factor=1.0)\n                    candidate = base + step\n                else:\n                    # DE/rand/1 mutation + binomial crossover\n                    idxs = np.arange(self.pop_size)\n                    # ensure we have at least 3 distinct indices\n                    if self.pop_size >= 4:\n                        # exclude current index\n                        idxs = idxs[idxs != i]\n                        r = self.rng.choice(idxs, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    else:\n                        # fallback to rand/1-like around best\n                        r1 = self.rng.randint(self.pop_size)\n                        donor = pop[r1] + Fi * (self.x_opt - pop[r1])\n\n                    # binomial crossover\n                    trial = pop[i].copy()\n                    jrand = self.rng.randint(self.dim)\n                    mask = self.rng.rand(self.dim) < CRi\n                    mask[jrand] = True\n                    trial[mask] = donor[mask]\n                    candidate = trial\n\n                # projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                try:\n                    f_cand = float(func(candidate))\n                except Exception:\n                    f_cand = np.inf\n                evals += 1\n\n                # selection: greedy replacement (per-target)\n                replaced = False\n                if f_cand < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_cand\n                    replaced = True\n\n                # global best update\n                if f_cand < self.f_opt:\n                    self.f_opt = f_cand\n                    self.x_opt = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    # increment stagnation for lack of global improvement\n                    stagnation_counter += 1\n\n                # adaptation of means based on individual success\n                if replaced:\n                    successes += 1\n                    F_mean = np.clip(0.9 * F_mean + 0.1 * Fi, 0.05, 0.99)\n                    CR_mean = np.clip(0.9 * CR_mean + 0.1 * CRi, 0.0, 1.0)\n                else:\n                    # small decay to encourage exploration slowly\n                    F_mean = np.clip(0.995 * F_mean + 0.0005, 0.05, 0.99)\n                    CR_mean = np.clip(0.995 * CR_mean + 0.0001, 0.0, 1.0)\n\n                # if we used Levy, slightly increase exploration propensity if not successful\n                # (increase p_levy a little when stagnating)\n                if not replaced and (self.rng.rand() < 0.02):\n                    p_levy = min(0.5, p_levy * 1.02)\n\n                # small safeguard to avoid stale local vars\n                del Fi, CRi\n\n            # After a generation over population, do a trust-region local search around best\n            if evals >= self.budget:\n                break\n\n            # decide number of local samples (small, scaled by dim and remaining budget)\n            remaining = self.budget - evals\n            local_budget = min( max(1, self.dim // 2 + 1), remaining, 6 )\n            local_successes = 0\n            # anisotropic per-dim sigma based on trust_radius and randomization\n            for _ in range(local_budget):\n                if evals >= self.budget:\n                    break\n                # sigma per-dim in fractions of bounds\n                sigma = (0.3 * trust_radius / max(range_norm, 1e-12)) * (0.5 + self.rng.rand(self.dim) * 0.5)\n                # propose candidate (Gaussian around best)\n                step = self.rng.normal(loc=0.0, scale=sigma, size=self.dim) * range_vec\n                candidate = self.x_opt + step\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_cand = float(func(candidate))\n                except Exception:\n                    f_cand = np.inf\n                evals += 1\n\n                if f_cand < self.f_opt:\n                    self.f_opt = f_cand\n                    self.x_opt = candidate.copy()\n                    local_successes += 1\n                    stagnation_counter = 0\n                # optionally inject into population (replace worst)\n                worst_idx = np.argmax(fvals)\n                if f_cand < fvals[worst_idx]:\n                    pop[worst_idx] = candidate.copy()\n                    fvals[worst_idx] = f_cand\n\n            # adjust trust radius based on local search results\n            if local_successes > 0:\n                trust_radius = max(min_trust, trust_radius * 0.85)\n                # slightly consolidate exploration probability\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                trust_radius = min(max_trust, trust_radius * 1.12)\n                p_levy = min(0.6, p_levy * 1.02)\n\n            # generation-level adaptation: if many DE successes, bias toward exploitation\n            if successes > max(1, self.pop_size // 8):\n                F_mean = np.clip(0.96 * F_mean + 0.04 * 0.5, 0.05, 0.99)\n                CR_mean = np.clip(0.96 * CR_mean + 0.04 * 0.9, 0.0, 1.0)\n            else:\n                # encourage exploration a bit if few successes\n                F_mean = np.clip(1.02 * F_mean, 0.05, 0.99)\n\n            # strong stagnation reset: re-seed part of population\n            if stagnation_counter > max(50, self.dim * 6):\n                k = max(1, self.pop_size // 2)\n                # choose indices to replace excluding current best if possible\n                candidate_idxs = np.arange(self.pop_size)\n                # avoid replacing individual matching global best if present in pop\n                # find index in pop closest to best_x (L2)\n                if self.x_opt is not None:\n                    dists = np.linalg.norm(pop - self.x_opt, axis=1)\n                    keep_idx = np.argmin(dists)\n                else:\n                    keep_idx = None\n                if keep_idx is not None:\n                    candidate_idxs = candidate_idxs[candidate_idxs != keep_idx]\n                replace_idxs = self.rng.choice(candidate_idxs, size=min(k, candidate_idxs.size), replace=False)\n                for j in replace_idxs:\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    # Evaluate re-seeded individuals as budget allows\n                    if evals < self.budget:\n                        try:\n                            fnew = float(func(pop[j]))\n                        except Exception:\n                            fnew = np.inf\n                        fvals[j] = fnew\n                        evals += 1\n                        if fnew < self.f_opt:\n                            self.f_opt = fnew\n                            self.x_opt = pop[j].copy()\n                # enlarge trust radius moderately after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # slightly increase Levy probability to favor exploration after reset\n                p_levy = min(0.5, p_levy * 1.3)\n\n            # safety clamp on parameters\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n            F_mean = np.clip(F_mean, 0.05, 0.99)\n            CR_mean = np.clip(CR_mean, 0.0, 1.0)\n            p_levy = np.clip(p_levy, 0.0, 0.6)\n\n        # final results\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.244 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10289980969702572, 0.1582937847555581, 0.41575687508076653, 0.212713486714881, 0.22784729636593848, 0.3963992410834116, 0.24390774781811764, 0.3254136192731433, 0.2169176840872853, 0.1377098025576028]}, "task_prompt": ""}
{"id": "bf580aa2-b0d2-445a-bf19-7117c97e858b", "fitness": 0.18281334174462976, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: population DE with jDE-like adaptation, occasional Lévy/Cauchy long jumps for global exploration, and a trust-region Gaussian local search around the current best with online step-size control.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (DE) with per-individual Fi/CRi adaptation (jDE-style),\n      - Occasional Lévy-like (Cauchy) long jumps centered on the current best for exploration,\n      - Trust-region Gaussian local search around the current best with adaptive trust radius,\n      - Stagnation detection and partial re-seeding.\n\n    Interface:\n      opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=None)\n      f_best, x_best = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population size: scale with dim but limited by budget\n        if pop_size is None:\n            pop = max(6, int(10 * self.dim))            # baseline proportional to dim\n            pop = min(pop, max(4, self.budget // 8))    # don't exceed a fraction of budget\n            self.pop_size = int(pop)\n        else:\n            self.pop_size = int(pop_size)\n\n        # outputs to be set after run\n        self.f_opt = None\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.ndim == 0:\n            b = np.full(self.dim, b, dtype=float)\n        elif b.size == 1:\n            b = np.full(self.dim, b.item(), dtype=float)\n        else:\n            b = b.reshape(self.dim)\n        return b\n\n    def __call__(self, func):\n        # Read bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        # safety if bounds degenerate\n        if range_norm == 0:\n            range_norm = 1.0\n\n        # Bookkeeping\n        budget = max(0, int(self.budget))\n        evals = 0\n\n        # Initialize population uniformly in bounds, but do not exceed budget when evaluating\n        pop = np.empty((self.pop_size, self.dim), dtype=float)\n        pop[:] = self.rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        # Evaluate initial population respecting budget\n        for i in range(self.pop_size):\n            if evals >= budget:\n                break\n            xi = pop[i].copy()\n            fvals[i] = func(xi)\n            evals += 1\n\n        # If budget too small to evaluate full pop, unused individuals stay with inf fvals\n        # Find initial best\n        best_idx = int(np.nanargmin(fvals))\n        if not np.isfinite(fvals[best_idx]):\n            # If none evaluated, create one candidate and evaluate\n            if evals < budget:\n                x0 = self.rng.uniform(lb, ub)\n                f0 = func(x0)\n                evals += 1\n                best_x = x0.copy()\n                best_f = f0\n            else:\n                # No evaluations possible at all: return trivial\n                self.f_opt = np.inf\n                self.x_opt = None\n                return self.f_opt, self.x_opt\n        else:\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n\n        # DE / adaptation parameters\n        F_mean = 0.6   # mean mutation factor\n        CR_mean = 0.5  # mean crossover rate\n        p_levy = 0.05  # initial probability of a long jump per trial\n        max_p_levy = 0.5\n\n        # per-individual Fi and CRi (initialized around means)\n        Fi = np.clip(self.rng.normal(F_mean, 0.1, size=self.pop_size), 0.1, 0.95)\n        CRi = np.clip(self.rng.normal(CR_mean, 0.2, size=self.pop_size), 0.0, 1.0)\n\n        # Trust-region parameters\n        trust_radius = 0.2 * range_norm\n        min_trust = 1e-8\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        last_improvement_at = evals\n\n        generation = 0\n        # Helper: Levy-like step using Cauchy heavy-tailed distribution, clipped\n        def levy_step(scale=0.25):\n            # returns a vector of dimension self.dim\n            # Cauchy(0,1) has heavier tails than Gaussian and approximates Levy jumps for exploration\n            raw = self.rng.standard_cauchy(self.dim)\n            # scale relative to range_vec and scale factor; clip extreme outliers\n            step = np.tanh(raw) * (scale * range_vec)\n            return step\n\n        # Main loop: iterate until budget exhausted\n        while evals < budget:\n            generation += 1\n            # Shuffle order to reduce biases\n            order = self.rng.permutation(self.pop_size)\n            # Per-generation success logs for adaptation\n            succ_F = []\n            succ_CR = []\n            succ_count = 0\n\n            for idx in order:\n                if evals >= budget:\n                    break\n\n                # If individual has not been evaluated (fvals inf), we treat it as target but allow evaluation\n                x_target = pop[idx].copy()\n                f_target = fvals[idx]\n\n                # Draw per-individual Fi and CRi around means (jDE-like)\n                if self.rng.rand() < 0.1:\n                    Fi[idx] = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 1.0)\n                if self.rng.rand() < 0.1:\n                    CRi[idx] = np.clip(self.rng.normal(CR_mean, 0.2), 0.0, 1.0)\n                Fi_i = float(Fi[idx])\n                CR_i = float(CRi[idx])\n\n                # Decide whether to perform a Lévy jump (global exploration)\n                do_levy = (self.rng.rand() < p_levy)\n\n                if do_levy:\n                    # Create candidate around best using a heavy-tailed Cauchy step\n                    step = levy_step(scale=0.5)\n                    donor = best_x + step\n                    # small DE-like mixing: sometimes mix in a vector difference for diversity\n                    if self.pop_size >= 3 and self.rng.rand() < 0.4:\n                        r1, r2 = self.rng.choice([j for j in range(self.pop_size) if j != idx and np.isfinite(fvals[j])], size=2, replace=False) \\\n                                if np.sum(np.isfinite(fvals)) >= 3 else (idx, idx)\n                        donor = donor + Fi_i * (pop[r1] - pop[r2])\n                else:\n                    # DE/rand/1 mutation (classical) using three distinct indices\n                    # ensure we pick three distinct indices different from idx\n                    candidates = [j for j in range(self.pop_size) if j != idx]\n                    if len(candidates) >= 3 and np.sum(np.isfinite(fvals[candidates])) >= 3:\n                        r = self.rng.choice(candidates, size=3, replace=False)\n                        donor = pop[r[0]] + Fi_i * (pop[r[1]] - pop[r[2]])\n                    else:\n                        # fallback: rand in bounds near target\n                        donor = x_target + Fi_i * self.rng.normal(0, 1, self.dim) * (range_vec * 0.1)\n\n                # Binomial crossover\n                cross_mask = self.rng.rand(self.dim) < CR_i\n                # ensure at least one dimension is from donor\n                jrand = self.rng.randint(self.dim)\n                cross_mask[jrand] = True\n                trial = np.where(cross_mask, donor, x_target)\n\n                # Projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # Evaluate trial\n                f_trial = func(trial)\n                evals += 1\n\n                # Selection: greedy\n                if (not np.isfinite(f_target)) or (f_trial <= f_target):\n                    pop[idx] = trial\n                    fvals[idx] = f_trial\n                    succ_count += 1\n                    succ_F.append(Fi_i)\n                    succ_CR.append(CR_i)\n                    # Update global best if improved\n                    if f_trial < best_f:\n                        best_f = float(f_trial)\n                        best_x = trial.copy()\n                        last_improvement_at = evals\n                        stagnation_counter = 0\n                else:\n                    # unsuccessful trial\n                    pass\n\n                # Increase exploration probability slightly if we haven't improved recently\n                if (evals - last_improvement_at) > max(100, 5 * self.dim):\n                    p_levy = min(max_p_levy, p_levy * 1.02 + 0.001)\n\n                # small safety break\n                if evals >= budget:\n                    break\n\n            # End of generation: adapt F_mean and CR_mean toward successful parameters if any\n            if len(succ_F) > 0:\n                # Lehmer-like emphasis on larger values for F (works well in DE literature)\n                num = sum([f * f for f in succ_F])\n                den = sum(succ_F)\n                if den > 0:\n                    F_mean = 0.85 * F_mean + 0.15 * (num / den)\n                CR_mean = 0.85 * CR_mean + 0.15 * (sum(succ_CR) / len(succ_CR))\n            else:\n                # decay towards exploration if no successes\n                F_mean = min(0.95, F_mean * 1.02)\n                CR_mean = max(0.05, CR_mean * 0.98)\n\n            # Trust-region local search around best: use some evaluations if budget allows\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n            # Determine local_samples based on remaining budget and dim\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 8)))\n            local_success = 0\n            for _ in range(local_samples):\n                # anisotropic Gaussian noise scaled by trust_radius\n                if trust_radius < min_trust:\n                    trust_radius = min_trust\n                sigma = (0.4 + self.rng.rand(self.dim) * 0.6) * (trust_radius / range_norm)\n                candidate = best_x + self.rng.normal(0.0, 1.0, self.dim) * (sigma * range_vec)\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = func(candidate)\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = float(f_candidate)\n                    best_x = candidate.copy()\n                    local_success += 1\n                    last_improvement_at = evals\n                    stagnation_counter = 0\n                    # successful exploitation: shrink trust region to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    # nudge means toward exploitation\n                    F_mean = 0.9 * F_mean + 0.1 * 0.5\n                    CR_mean = 0.9 * CR_mean + 0.1 * 0.8\n                else:\n                    # unsuccessful: slightly widen trust region to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n                if evals >= budget:\n                    break\n\n            # update stagnation counter\n            if evals - last_improvement_at > max(50, 4 * self.dim):\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n\n            # If strong stagnation, partially re-seed and increase exploration\n            if stagnation_counter > max(5, self.dim // 2):\n                # reinitialize half of the population randomly\n                n_reseed = max(1, self.pop_size // 2)\n                reseed_idx = self.rng.choice(self.pop_size, size=n_reseed, replace=False)\n                for ridx in reseed_idx:\n                    if evals >= budget:\n                        break\n                    pop[ridx] = self.rng.uniform(lb, ub)\n                    fvals[ridx] = func(pop[ridx])\n                    evals += 1\n                    # maybe update best\n                    if fvals[ridx] < best_f:\n                        best_f = float(fvals[ridx])\n                        best_x = pop[ridx].copy()\n                        last_improvement_at = evals\n                # encourage more Lévy jumps and enlarge trust region a bit\n                p_levy = min(max_p_levy, p_levy * 1.5 + 0.02)\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n\n            # Gentle decay to reduce over-exploration as search proceeds\n            p_levy = max(0.005, p_levy * 0.995)\n            CR_mean = np.clip(CR_mean, 0.0, 1.0)\n            F_mean = np.clip(F_mean, 0.05, 1.0)\n\n        # Finalize outputs\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.183 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10754618745167177, 0.15632846818896218, 0.24411974871448505, 0.18620752947673824, 0.1986369669102559, 0.20433797309531843, 0.21811050414168942, 0.17525519353426733, 0.19202443211431064, 0.1455664138185988]}, "task_prompt": ""}
{"id": "6f226694-0ab0-4e2b-9c4d-5acf7e925b12", "fitness": 0.7005117443786929, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like online parameter adaptation, occasional Lévy-flight long jumps centered on the best, and a trust-region Gaussian local search — balancing fast global exploration with focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line: Adaptive DE with occasional Lévy jumps and trust-region local search.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size default: scale with dimension but not exceed a fraction of budget\n        if pop_size is None:\n            self.pop_size = int(np.clip(10 + 2 * self.dim, 4, max(4, self.budget // max(5, self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # ensure minimal viable population\n        self.pop_size = max(4, self.pop_size)\n        # internal best results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like bounds and broadcast to dimension\n        b = np.asarray(b)\n        if b.shape == ():\n            return np.full(self.dim, float(b))\n        if b.ndim == 1 and b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be interpreted for dimension {}\".format(self.dim))\n\n    def __call__(self, func):\n        # bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # fallback if function provides -5..5 not as arrays\n        range_vec = ub - lb\n        # handle degenerate range\n        range_vec[range_vec == 0] = 1e-12\n\n        evals = 0\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        pop_f = np.full(self.pop_size, np.inf)\n\n        # jDE style parameter adaptation state\n        F_mean = 0.6\n        CR_mean = 0.5\n        tau1 = 0.1  # prob to change F\n        tau2 = 0.1  # prob to change CR\n        Fl, Fu = 0.2, 0.95\n\n        # trust-region parameters\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # absolute scale\n        trust_min = 1e-6\n        trust_max = np.linalg.norm(range_vec) * 2.0\n\n        # Levy step helper (Cauchy ~ Lévy-like heavy tail)\n        def levy_step(scale=1.0):\n            # sample from Cauchy(0, scale), but cap extremes to avoid numerical blow-up\n            s = self.rng.standard_cauchy(self.dim) * scale\n            # cap based on range\n            cap = 10.0 * np.maximum(range_vec, 1e-12)\n            s = np.clip(s, -cap, cap)\n            return s\n\n        # Evaluate initial population as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            evals += 1\n            pop_f[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # if we couldn't evaluate all individuals because budget tiny,\n        # fill leftover slots with randoms (they won't be evaluated)\n        # no further action needed\n\n        generation = 0\n        no_improve_iters = 0\n        stagnation_threshold = max(10, 5 * self.dim)\n        # memory for successful F/CR to update means via Lehmer mean\n        success_F = []\n        success_CR = []\n\n        # main loop: process generations until budget exhausted\n        while evals < self.budget:\n            generation += 1\n            success_in_gen = 0\n            # adapt per-generation jitter for mean sampling\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Sample Fi and CRi with jDE-like mechanism\n                if self.rng.rand() < tau1:\n                    Fi = Fl + self.rng.rand() * (Fu - Fl)\n                else:\n                    # sample around mean with small jitter\n                    Fi = np.clip(F_mean + 0.1 * self.rng.randn(), Fl, Fu)\n\n                if self.rng.rand() < tau2:\n                    CRi = self.rng.rand()\n                else:\n                    CRi = np.clip(CR_mean + 0.1 * self.rng.randn(), 0.0, 1.0)\n\n                # Select indices for DE/rand/1 mutation (exclude i)\n                idxs = np.arange(self.pop_size)\n                idxs = idxs[idxs != i]\n                self.rng.shuffle(idxs)\n                r1, r2, r3 = idxs[:3]\n                # create donor by DE/rand/1 plus occasional Levy-centered jump\n                if self.rng.rand() < 0.08:\n                    # Occasional Levy-based global exploration centered on best\n                    best_x = self.x_opt if self.x_opt is not None else pop[r1]\n                    step = levy_step(scale=0.5)  # heavy-tailed direction\n                    # scale by trust radius and range vector\n                    donor = best_x + (0.5 + self.rng.rand()) * step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                else:\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                jrand = self.rng.randint(self.dim)\n                mask = self.rng.rand(self.dim) < CRi\n                mask[jrand] = True\n                trial = np.where(mask, donor, pop[i])\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_trial = float(func(trial))\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_trial <= pop_f[i]:\n                    # successful - replace and record successful parameters\n                    pop[i] = trial\n                    pop_f[i] = f_trial\n                    success_in_gen += 1\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n                    # move F_mean and CR_mean slightly towards Fi/CRi (simple adaptation)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # update global best if improved\n                    if f_trial < self.f_opt:\n                        improvement = self.f_opt - f_trial\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        no_improve_iters = 0\n                else:\n                    # unsuccessful - small negative reinforcement (push means slightly away)\n                    F_mean = 0.999 * F_mean + 0.001 * Fi\n                    CR_mean = 0.999 * CR_mean + 0.001 * CRi\n\n            # End of generation adjustments\n            # Slightly shrink trust radius if many successes, otherwise expand\n            if success_in_gen > (0.2 * self.pop_size):\n                trust_radius = max(trust_min, trust_radius * 0.92)\n            else:\n                trust_radius = min(trust_max, trust_radius * 1.05)\n\n            # trust-region local search around best: sample a small number of Gaussian candidates\n            if evals < self.budget and self.x_opt is not None:\n                # number of local samples depends on dimension and remaining budget, but keep small\n                remaining = self.budget - evals\n                local_samples = int(np.clip(min(8 + self.dim // 2, remaining, 2 * self.dim), 1, remaining))\n                local_success = 0\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma using trust radius and per-dim randomness\n                    sigma = (0.5 + self.rng.rand(self.dim) * 0.5) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                    candidate = self.x_opt + self.rng.randn(self.dim) * sigma\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    f_cand = float(func(candidate))\n                    evals += 1\n                    if f_cand < self.f_opt:\n                        self.f_opt = f_cand\n                        self.x_opt = candidate.copy()\n                        local_success += 1\n                        no_improve_iters = 0\n                # adjust trust radius from local search outcome\n                if local_success > 0:\n                    trust_radius = max(trust_min, trust_radius * 0.85)\n                else:\n                    trust_radius = min(trust_max, trust_radius * 1.07)\n\n            # stagnation handling\n            if success_in_gen == 0:\n                no_improve_iters += 1\n            else:\n                no_improve_iters = 0\n\n            # if stagnating, increase chance of long jumps and slightly increase diversity\n            if no_improve_iters > stagnation_threshold:\n                # perform Lévy resets: reinitialize a fraction of population\n                reinit_count = max(1, int(0.4 * self.pop_size))\n                for j in range(reinit_count):\n                    if evals >= self.budget:\n                        break\n                    idx = self.rng.randint(self.pop_size)\n                    pop[idx] = lb + self.rng.rand(self.dim) * range_vec\n                    # immediately evaluate the new individual if budget allows\n                    if evals < self.budget:\n                        fnew = float(func(pop[idx]))\n                        evals += 1\n                        pop_f[idx] = fnew\n                        if fnew < self.f_opt:\n                            self.f_opt = fnew\n                            self.x_opt = pop[idx].copy()\n                # enlarge trust radius a bit to encourage exploration\n                trust_radius = min(trust_max, trust_radius * 1.5)\n                no_improve_iters = 0\n                # slightly nudge F_mean to encourage exploration\n                F_mean = np.clip(F_mean * 1.05, Fl, Fu)\n                CR_mean = np.clip(CR_mean * 1.05, 0.0, 1.0)\n\n            # Recompute range_vec in case bounds change externally (unlikely) - keep consistent\n            # Update Lehmer mean if we collected successful Fs\n            if len(success_F) > 0:\n                # Lehmer mean emphasizes larger F values that succeeded\n                num = np.sum(np.square(success_F))\n                den = np.sum(success_F) + 1e-12\n                F_mean = np.clip(num / den, Fl, Fu)\n                CR_mean = np.clip(np.mean(success_CR), 0.0, 1.0)\n                success_F = []\n                success_CR = []\n\n            # quick population diversification if best cluster too tight and budget remains\n            if evals < self.budget and generation % max(10, 2 * self.dim) == 0:\n                # replace worst few individuals\n                worst_k = max(1, int(0.1 * self.pop_size))\n                worst_idx = np.argsort(pop_f)[-worst_k:]\n                for idx in worst_idx:\n                    pop[idx] = lb + self.rng.rand(self.dim) * range_vec\n                    # optionally evaluate one of them if small budget allows\n                    if evals < self.budget:\n                        fnew = float(func(pop[idx]))\n                        evals += 1\n                        pop_f[idx] = fnew\n                        if fnew < self.f_opt:\n                            self.f_opt = fnew\n                            self.x_opt = pop[idx].copy()\n\n        # final return\n        return float(self.f_opt), self.x_opt.copy() if self.x_opt is not None else None", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.701 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.25388954801834795, 0.2953328359714621, 0.7447952101354167, 0.9174192618852586, 0.8519896840408556, 0.8979423940555905, 0.6995178467502419, 0.7853728254911174, 0.8254900641559141, 0.733367773282724]}, "task_prompt": ""}
{"id": "7da5b188-cc35-45a4-947a-0b191320b82d", "fitness": 0.1918913622402866, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy-like Cauchy jumps and a trust-region local search; online adapts F/CR and trust radius, re-seeds on stagnation for robust global-local balance.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (jDE-like per-individual F/CR sampling + population-based DE/rand/1)\n      - Occasional heavy-tailed Lévy-like (Cauchy) jumps centered on best for long-range exploration\n      - Trust-region local Gaussian search around current best with online step-size adaptation\n      - Stagnation detection with partial re-seeding to restore diversity\n\n    Meant for continuous bound-constrained optimization (default bounds [-5,5] per dimension).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # Population size scaled with dimension but not too large\n        if pop_size is None:\n            self.pop_size = max(6, min(50, int(6 + 2 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        # internal state result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like; return arrays length dim\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b.ravel(), (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # derive bounds from func if present, else default [-5,5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # helper: clip into bounds\n        def project(x):\n            return np.minimum(ub, np.maximum(lb, x))\n\n        budget = self.budget\n        evals = 0\n\n        # initialize population uniformly in bounds\n        pop = self.rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fpop = np.full(self.pop_size, np.inf)\n\n        # evaluate initial population but careful with budget\n        n_init = min(self.pop_size, budget)\n        for i in range(n_init):\n            fpop[i] = func(pop[i])\n            evals += 1\n            if fpop[i] < self.f_opt:\n                self.f_opt = fpop[i]\n                self.x_opt = pop[i].copy()\n        # If not all evaluated, leave rest as inf (they won't be selected until evaluated)\n        # initialize algorithm hyper-parameters (online adapted)\n        F_mean = 0.6\n        CR_mean = 0.5\n        levy_prob = 0.05  # base chance to perform a long Lévy-like jump\n        trust_radius = 0.5 * (ub - lb).mean()  # initial trust radius\n        trust_shrink = 0.8\n        trust_expand = 1.2\n        stagnation_counter = 0\n        stagnation_threshold = max(50, 10 * self.dim)\n        best_seen_at = evals\n\n        # adaptation memory\n        success_Fs = []\n        success_CRs = []\n\n        # Levy/Cauchy helper: heavy-tailed step but clipped to avoid blow-up\n        def levy_step(scale=1.0):\n            # Cauchy samples produce heavy tails; scale by provided factor\n            step = self.rng.standard_cauchy(self.dim) * scale\n            # clip extreme outliers to avoid numerical blow-up\n            max_abs = 10.0 * (ub - lb).mean()\n            step = np.clip(step, -max_abs, max_abs)\n            return step\n\n        # Main loop: iterate generations until budget exhausted\n        gen = 0\n        while evals < budget:\n            gen += 1\n            # process population sequentially\n            indices = np.arange(self.pop_size)\n            self.rng.shuffle(indices)\n            for idx in indices:\n                if evals >= budget:\n                    break\n\n                x = pop[idx]\n                # sample per-individual F and CR (jDE-like)\n                if self.rng.random() < 0.1:\n                    Fi = 0.1 + 0.9 * (self.rng.random() ** 2)  # occasional small F\n                else:\n                    Fi = max(0.05, min(0.95, self.rng.normal(F_mean, 0.1)))\n                CRi = min(1.0, max(0.0, self.rng.normal(CR_mean, 0.15)))\n\n                # Decide mutation: with probability levy_prob or if stagnating -> Levy-centered jump\n                do_levy = (self.rng.random() < levy_prob) or (stagnation_counter > stagnation_threshold and self.rng.random() < 0.5)\n                if do_levy and self.x_opt is not None:\n                    # long jump centered on best with some DE differential + Cauchy perturbation\n                    best = self.x_opt\n                    # pick two random distinct indices for differential component\n                    idxs = self.rng.choice(self.pop_size, size=2, replace=False)\n                    r1, r2 = pop[idxs[0]], pop[idxs[1]]\n                    donor = best + Fi * (r1 - r2) + levy_step(scale=trust_radius * 0.8)\n                else:\n                    # DE/rand/1-like mutation: pick three distinct indices different from idx\n                    all_idx = np.delete(np.arange(self.pop_size), idx)\n                    if all_idx.size < 3:\n                        # fallback: random step\n                        donor = x + Fi * levy_step(scale=trust_radius * 0.5)\n                    else:\n                        r = self.rng.choice(all_idx, size=3, replace=False)\n                        r0, r1, r2 = pop[r[0]], pop[r[1]], pop[r[2]]\n                        donor = r0 + Fi * (r1 - r2)\n\n                # binomial crossover to form trial\n                cross_mask = self.rng.random(self.dim) < CRi\n                # ensure at least one dimension from donor\n                jrand = self.rng.integers(0, self.dim)\n                cross_mask[jrand] = True\n                trial = np.where(cross_mask, donor, x)\n                trial = project(trial)\n\n                # evaluate trial (one function evaluation)\n                f_trial = func(trial)\n                evals += 1\n\n                # selection: greedy\n                if f_trial <= fpop[idx]:\n                    # replace\n                    pop[idx] = trial\n                    fpop[idx] = f_trial\n                    success_Fs.append(Fi)\n                    success_CRs.append(CRi)\n                    # if improvement globally\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        best_seen_at = evals\n                        stagnation_counter = 0\n                else:\n                    # unsuccessful -> nothing replaced\n                    stagnation_counter += 1\n\n                # slight moving average adaptation of F_mean and CR_mean using success lists\n                if len(success_Fs) >= 5:\n                    # Lehmer-like update (gives weight to larger Fi)\n                    sf = np.array(success_Fs)\n                    F_mean = 0.9 * F_mean + 0.1 * (np.sum(sf ** 2) / (1e-9 + np.sum(sf)))\n                    CR_mean = 0.9 * CR_mean + 0.1 * (np.mean(success_CRs) if success_CRs else CR_mean)\n                    # decay histories\n                    success_Fs.clear()\n                    success_CRs.clear()\n\n                # adjust levy_prob based on stagnation: more leaps when stagnating\n                if stagnation_counter > stagnation_threshold:\n                    levy_prob = min(0.5, levy_prob * 1.05 + 0.01)\n                else:\n                    levy_prob = max(0.01, levy_prob * 0.995)\n\n                # break if budget exhausted\n                if evals >= budget:\n                    break\n\n            # End of generation adjustments: trust-region local search around best\n            if evals < budget and self.x_opt is not None:\n                # number of local samples small, scaled by dim and remaining budget\n                remaining = budget - evals\n                n_local = int(min(max(2, self.dim // 2), remaining, 6))\n                local_success = 0\n                for _ in range(n_local):\n                    # anisotropic Gaussian noise: per-dim random scaling\n                    per_dim_scale = trust_radius * (0.25 + self.rng.random(self.dim) * 1.25)\n                    cand = self.x_opt + self.rng.normal(0.0, per_dim_scale)\n                    cand = project(cand)\n                    f_cand = func(cand)\n                    evals += 1\n                    if f_cand < self.f_opt:\n                        self.f_opt = f_cand\n                        self.x_opt = cand.copy()\n                        local_success += 1\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                    if evals >= budget:\n                        break\n                # trust region adaptation\n                if local_success > 0:\n                    # shrink to focus\n                    trust_radius = max(1e-6, trust_radius * (trust_shrink ** local_success))\n                    # encourage exploitation by slightly moving means inward\n                    F_mean = max(0.05, F_mean * 0.98)\n                    CR_mean = min(0.98, CR_mean * (1.0 + 0.01 * local_success))\n                else:\n                    # expand to escape local trap\n                    trust_radius = min((ub - lb).mean(), trust_radius * trust_expand)\n                    # nudge means to larger exploration\n                    F_mean = min(0.95, F_mean * 1.02)\n                    CR_mean = max(0.05, CR_mean * 0.99)\n\n            # stagnation handling: strong reset if no improvement for long time\n            if evals - best_seen_at > 5 * stagnation_threshold and evals < budget:\n                # re-seed half of the population randomly\n                n_reseed = max(1, self.pop_size // 2)\n                re_indices = self.rng.choice(self.pop_size, size=n_reseed, replace=False)\n                for ri in re_indices:\n                    new_x = self.rng.uniform(lb, ub)\n                    pop[ri] = new_x\n                    # evaluate if budget allows\n                    if evals < budget:\n                        fpop[ri] = func(new_x)\n                        evals += 1\n                        if fpop[ri] < self.f_opt:\n                            self.f_opt = fpop[ri]\n                            self.x_opt = new_x.copy()\n                            best_seen_at = evals\n                            stagnation_counter = 0\n                # enlarge trust radius moderately after reset\n                trust_radius = min((ub - lb).mean(), trust_radius * 1.5)\n                # slightly increase exploratory tendency\n                F_mean = min(0.9, F_mean + 0.05)\n                levy_prob = min(0.4, levy_prob + 0.05)\n                # reset stagnation counters\n                best_seen_at = evals\n\n        # final results\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.192 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09791402966063256, 0.15766480163100582, 0.25225599882630356, 0.21044163238046565, 0.21491291843493576, 0.2358163264073012, 0.22170131590045294, 0.19271835127008163, 0.19512935056925573, 0.14035889732243134]}, "task_prompt": ""}
{"id": "32658aa0-0f25-4cc1-9f9e-e9ea1f050229", "fitness": 0.33920811318458644, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online step-size adaptation for robust exploration-exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (DE) for population-based recombination,\n      - occasional Lévy-like (Cauchy) jumps for long-range exploration,\n      - trust-region Gaussian local search around the current best for focused exploitation,\n      - online adaptation of DE parameters (F, CR) and Lévy probability based on success history.\n\n    Intended for continuous black-box optimization (e.g., BBOB). The algorithm\n    ensures it never exceeds the given evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # RNG: keep backward-compatible RandomState but allow numpy integer seeds\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but bounded\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # also ensure population not larger than a fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n\n        # final best values (populated after run)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Ensure bounds are arrays of length dim (broadcast scalars).\n        Accepts float, list/array of size dim or 1.\n        \"\"\"\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to problem dimension.\")\n\n    def __call__(self, func):\n        # Prepare bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # Ensure standard BBOB-like [-5,5] if user passed None or identical\n        # (but we respect provided bounds)\n        range_vec = ub - lb\n        # safeguard: if any zero range, set tiny epsilon\n        range_vec = np.where(range_vec <= 0.0, 1e-12, range_vec)\n\n        rng = self.rng\n\n        # bookkeeping for evaluations\n        evals = 0\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        # evaluate initial population (but not exceeding budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # if no evaluations happened (budget==0), return defaults\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # determine current best among evaluated individuals\n        finite_mask = np.isfinite(fvals)\n        if not np.any(finite_mask):\n            # shouldn't happen, but guard\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals)\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover prob mean\n        p_levy = 0.08       # probability of a Lévy jump (long jump)\n        step_scale = 0.25   # base scale for Lévy and trust radius relative to range\n        # trust radius expressed as fraction of norm(range_vec)\n        trust_radius = 0.2 * np.linalg.norm(range_vec)\n        max_trust = max(1.0, 0.5 * np.linalg.norm(range_vec))\n        min_trust = 1e-6\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # use Cauchy (standard) to create heavy tails; clip extremes\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # local sampling budget per generation: choose modest number\n        def compute_local_samples(remaining):\n            # small handful scaled by dimension but limited by remaining evals\n            return int(min(max(1, self.dim // 2), remaining, 12))\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            gen_improved = False\n            # per-generation arrays for adaptation (collect successful Fi/CR)\n            Fi_success = []\n            CRi_success = []\n\n            # iterate over population members sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # decide mutation type: Levy jump centered on best OR DE/rand/1\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on current best for exploration\n                    step = levy_step()\n                    # scale by range and trust radius; add randomness\n                    scale = step_scale * (0.3 + 0.7 * rng.rand()) * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                    donor = best_x + scale * step * (range_vec / (np.maximum(range_vec.mean(), 1e-12)))\n                    # No CR/F used in Levy branch\n                else:\n                    # DE/rand/1 mutation with per-individual Fi\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback: small random perturbation if pop too small\n                        donor = pop[i] + rng.normal(0, 0.1, size=self.dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                        donor_vec = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # crossover\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n                        cr_mask = rng.rand(self.dim) < CRi\n                        # ensure at least one dimension crosses\n                        if not np.any(cr_mask):\n                            cr_mask[rng.randint(0, self.dim)] = True\n                        trial = np.where(cr_mask, donor_vec, pop[i])\n                        donor = trial\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    gen_improved = True\n                    # if this was from DE branch, record Fi/CRi for adaptation\n                    if 'Fi' in locals():\n                        Fi_success.append(Fi)\n                    if 'CRi' in locals():\n                        CRi_success.append(CRi)\n                    # nudge means slightly toward successful params if available\n                    if Fi_success:\n                        F_mean = 0.9 * F_mean + 0.1 * np.mean(Fi_success)\n                    if CRi_success:\n                        CR_mean = 0.9 * CR_mean + 0.1 * np.mean(CRi_success)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0  # reset stagnation on improvement\n                # else, if no improvement, will be handled later\n\n                # clean local Fi/CRi to avoid accidental reuse next iteration\n                if 'Fi' in locals():\n                    del Fi\n                if 'CRi' in locals():\n                    del CRi\n\n            # End of generation DE pass\n\n            # trust-region local search around best: sample a few candidates with anisotropic Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            local_samples = compute_local_samples(remaining)\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (0.3 + rng.rand(self.dim) * 0.7) * (trust_radius / (np.maximum(np.linalg.norm(range_vec), 1e-12)))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                    gen_improved = True\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter += 1\n\n            # Adaptation of Lévy probability and global F/CR means based on success\n            if successes > 0 or gen_improved:\n                # if we had success, gently reduce exploration (fewer Lévy jumps)\n                p_levy = max(0.01, p_levy * (0.95 if successes > max(1, self.pop_size * 0.15) else 0.98))\n                # bias CR_mean slightly toward higher exploitation if success\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                stagnation_counter = max(0, stagnation_counter - 1)\n            else:\n                # stagnation handling: increase chance of long jumps and diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n                stagnation_counter += 1\n\n            # strong stagnation reset: if no improvement for many local tries, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                reinit_idxs = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reinit_idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # reduce Lévy probability a bit since we just diversified\n                p_levy = max(0.02, p_levy * 0.7)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.339 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.22426695137482977, 0.2595589670597114, 0.33308885216752493, 0.4290575191983501, 0.3305044313855585, 0.3906612734562669, 0.25631232659472303, 0.30081275420810827, 0.7083650114132424, 0.15945304498754864]}, "task_prompt": ""}
{"id": "5751ea6a-c90f-40c2-84a0-75ae60a17405", "fitness": 0.2803156265700083, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy jumps and a trust-region local search; online adapts step sizes and jump probability to balance global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, seed=42)\n        f_best, x_best = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # population size scaled with dimension and budget but kept reasonable\n        if pop_size is None:\n            pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            pop_size = min(pop_size, max(4, self.budget // 20))\n        self.pop_size = int(pop_size)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full((self.dim,), float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # read bounds and prepare\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        # protect against degenerate bounds\n        range_vec = np.where(range_vec == 0.0, 1.0, range_vec)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # if no evaluations possible, return\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # determine initial best\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius (fraction of range)\n        trust_radius = step_scale * np.linalg.norm(range_vec)  # scalar trust radius\n        max_trust = 0.8 * np.linalg.norm(range_vec)\n        min_trust = 1e-8\n\n        stagnation_counter = 0\n        success_history = 0  # successes in recent generation(s)\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # Main generational loop. We'll allow multiple inner generations until budget exhausted.\n        # Each candidate evaluation increments evals and we never exceed self.budget.\n        generation = 0\n        # We'll maintain a sliding window of recent successes to adapt parameters\n        recent_successes = []\n\n        while evals < self.budget:\n            generation += 1\n            gen_successes = 0\n\n            # For each target in population, attempt to create a trial and evaluate it\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual parameters (jDE-like)\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                # decide mutation type: Levy jump centered at best or DE/rand/1\n                if rng.rand() < p_levy:\n                    # Lévy jump: center around best_x with heavy-tail step scaled by trust_radius & range\n                    step = levy_step()\n                    # scale: normalized by Cauchy scale, then by trust_radius and range_vec\n                    step_norm = step / (np.linalg.norm(step) + 1e-12)\n                    donor = best_x + (step * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))) * range_vec\n                    # small random offset so not always exactly symmetric\n                    donor += rng.normal(0, 0.01, size=self.dim) * range_vec\n                    # we don't have Fi/CR influence here, but still treat as exploration move\n                    used_Fi = None\n                    used_CRi = None\n                else:\n                    # DE/rand/1 mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback to random perturbation if not enough distinct indices\n                        donor = pop[i] + Fi * rng.randn(self.dim) * range_vec\n                        used_Fi = Fi\n                        used_CRi = CRi\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor_base = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover to produce trial\n                        cr_mask = rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[rng.randint(0, self.dim)] = True\n                        donor = np.where(cr_mask, donor_base, pop[i])\n                        used_Fi = Fi\n                        used_CRi = CRi\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate (check budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    # replace in population\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    gen_successes += 1\n                    # update parameter means if DE used\n                    if used_Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * used_Fi\n                    if used_CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * used_CRi\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # end of generation processing\n            recent_successes.append(gen_successes)\n            if len(recent_successes) > 10:\n                recent_successes.pop(0)\n            avg_recent_success = float(np.mean(recent_successes)) if recent_successes else 0.0\n\n            # Adapt diversity parameters based on success\n            if avg_recent_success > 0.3 * self.pop_size:\n                # many successes -> exploit more\n                p_levy = max(0.01, p_levy * 0.9)\n                F_mean = np.clip(0.98 * F_mean, 0.05, 0.99)\n                CR_mean = np.clip(1.01 * CR_mean, 0.0, 1.0)\n            else:\n                # few successes -> encourage exploration\n                p_levy = min(0.5, p_levy * 1.05 + 0.005)\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # Trust-region local search around best_x: allocate a small fraction of remaining budget\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # determine number of local samples (bounded and scaled by dim and remaining)\n            local_samples = int(min(max(2, self.dim), max(1, remaining // max(1, self.dim // 2))))\n            # local gaussian sigma proportional to trust_radius and to search range\n            # normalize trust_radius to relative scale for each dimension\n            rel_scale = trust_radius / (np.linalg.norm(range_vec) + 1e-12)\n            sigma = max(1e-8, rel_scale)\n\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian step: scaled per-dim by range_vec\n                step = rng.normal(0.0, sigma, size=self.dim) * range_vec\n                candidate = np.clip(best_x + step, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # successful local step => shrink trust radius to focus search\n                    trust_radius = max(min_trust, trust_radius * 0.75)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand trust radius slightly to escape local minima\n                    trust_radius = min(max_trust, trust_radius * 1.07)\n                    stagnation_counter += 1\n\n            # if local search had some success, promote exploitation (reduce p_levy slightly)\n            if local_success > 0:\n                p_levy = max(0.01, p_levy * 0.95)\n                # nudge CR_mean / F_mean toward exploitation-friendly values\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.9, 0.0, 1.0)\n                F_mean = np.clip(0.98 * F_mean + 0.02 * 0.5, 0.05, 0.99)\n            else:\n                # stagnation adaptation\n                p_levy = min(0.9, p_levy * 1.03 + 0.002)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                reinit_indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reinit_indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                trust_radius = min(max_trust, max(min_trust, trust_radius * 1.5))\n                stagnation_counter = 0\n                # after reset, encourage local search to refine\n                p_levy = max(0.02, p_levy * 0.7)\n                # slightly decrease F_mean to encourage smaller steps\n                F_mean = np.clip(F_mean * 0.9, 0.05, 0.99)\n\n            # safety: ensure trust radius stays in bounds\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # update best tracked values into object (so intermediate restarts can access)\n            self.f_opt = best_f\n            self.x_opt = best_x.copy()\n\n        # final store and return\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n        return float(self.f_opt), self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.280 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17780978891464239, 0.2094245376192989, 0.360979030745213, 0.4176032166749776, 0.24699786836946425, 0.37259405012022073, 0.2541934147132764, 0.3001035844321568, 0.27778898539201424, 0.18566178871881867]}, "task_prompt": ""}
{"id": "9c3854d1-936d-44ed-8a4f-c4c451fd1c3a", "fitness": 0.1642155623545126, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid: Differential Evolution with occasional Lévy-flight jumps and an adaptive trust-region local search — balances fast global exploration and focused local exploitation with online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (DE scale F, crossover CR, Lévy probability, trust radius) are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size: scale with dimension but limited by budget\n        if pop_size is None:\n            self.pop_size = int(min(max(4, 8 * self.dim), max(4, self.budget // 10)))\n        else:\n            self.pop_size = int(pop_size)\n            # ensure it's reasonable relative to budget and dimension\n            self.pop_size = max(4, min(self.pop_size, max(4, self.budget // 5)))\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        # attempt broadcasting\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # obtain bounds from func (Many BBOB uses [-5,5], but take func.bounds if provided)\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            # fallback to default [-5, 5]\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        # hyperparameters (adapted online)\n        F_mean = 0.6    # mean differential weight\n        CR_mean = 0.5   # mean crossover prob\n        p_levy = 0.08   # initial probability of Levy jump\n        step_scale = 0.2   # base Lévy/trust step scale (relative to range)\n        trust_radius = 0.2 * range_norm\n        min_trust = 1e-8\n        max_trust = 2.0 * range_norm\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population until budget exhausted or population done\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n\n        # If we couldn't evaluate any individual (tiny budget), do a single random sample if possible\n        if evals == 0 and self.budget > 0:\n            x0 = lb + rng.rand(self.dim) * range_vec\n            f0 = float(func(x0))\n            evals = 1\n            self.f_opt = f0\n            self.x_opt = x0.copy()\n            return self.f_opt, self.x_opt\n\n        # find current best\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # stagnation & adaptation bookkeeping\n        stagnation_counter = 0    # counts consecutive non-improving evaluations (coarse)\n        gen = 0\n\n        # helper: Levy-like heavy-tailed step using capped Cauchy\n        def levy_step(scale=1.0):\n            # generate Cauchy (standard) and cap extreme values for numerical stability\n            s = rng.standard_cauchy(self.dim)\n            # cap at +/- 10 to avoid huge leaps; scale afterwards\n            s = np.clip(s, -10.0, 10.0)\n            return scale * s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation small jitter for means to encourage exploration\n            F_mean = np.clip(F_mean * (1.0 + rng.normal(0, 0.01)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean * (1.0 + rng.normal(0, 0.01)), 0.0, 1.0)\n\n            # iterate population sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # determine whether to use Levy jump or DE for this target\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on the best for large exploration\n                    scale_rel = step_scale * (0.5 + rng.rand())  # randomize scale a bit\n                    raw_step = levy_step(scale=scale_rel)\n                    # scale by range_vec to be coordinate-aware\n                    candidate = best_x + raw_step * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                    # evaluate\n                    f_candidate = float(func(candidate))\n                    evals += 1\n\n                    # selection: replace a random/worst individual or the worst among pair\n                    # Here we replace the target i if better; otherwise we may replace worst with some prob\n                    if f_candidate < fvals[i]:\n                        pop[i] = candidate\n                        fvals[i] = f_candidate\n                        successes += 1\n                        # nudge means slightly toward exploratory behavior\n                        F_mean = np.clip(0.98 * F_mean + 0.02 * 0.9, 0.05, 0.99)\n                        CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.3, 0.0, 1.0)\n                else:\n                    # DE/rand/1-like mutation with jDE-like per-individual F and CR sampling\n                    # choose r1,r2,r3 distinct and not equal to i\n                    idxs = list(range(self.pop_size))\n                    idxs.remove(i)\n                    r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0))\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    candidate = np.clip(trial, lb, ub)\n\n                    # evaluate\n                    f_candidate = float(func(candidate))\n                    evals += 1\n\n                    # selection and adaptation\n                    if f_candidate < fvals[i]:\n                        pop[i] = candidate\n                        fvals[i] = f_candidate\n                        successes += 1\n                        # adapt means toward successful parameters (small learning rate)\n                        F_mean = np.clip(0.9 * F_mean + 0.1 * Fi, 0.05, 0.99)\n                        CR_mean = np.clip(0.9 * CR_mean + 0.1 * CRi, 0.0, 1.0)\n                    else:\n                        # small decay away from unsuccessful Fi/CRi\n                        F_mean = np.clip(0.995 * F_mean, 0.05, 0.99)\n                        CR_mean = np.clip(0.995 * CR_mean, 0.0, 1.0)\n\n                # update global best\n                # find the index with best fval among those we touched\n                # But simply compare f_candidate to global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # adjust levy probability slightly based on recent improvements\n                if successes == 0:\n                    # if no successes yet this generation, gently increase chance of levy\n                    p_levy = min(0.6, p_levy * 1.02 + 0.002)\n                else:\n                    # if there are successes, reduce chance of wild jumps\n                    p_levy = max(0.01, p_levy * 0.98 - 0.001)\n\n                if evals >= self.budget:\n                    break\n\n            # End of generation: trust-region local search around best (exploit)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # decide local sample count proportional to dimension but limited by remaining budget\n            local_samples = min(remaining, max(1, self.dim // 2 + 1))\n            # anisotropic sigma per-dimension relative to trust_radius and range\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # draw gaussian perturbation scaled by trust radius relative to range_vec\n                rel_sigma = (trust_radius / (range_norm + 1e-12))\n                # per-dim sigma to allow anisotropic steps\n                sigma = rel_sigma * (0.5 + rng.rand(self.dim))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * (sigma * range_vec)\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => tighten trust region to focus\n                    trust_radius = max(min_trust, trust_radius * 0.75)\n                    # reward DE parameters for exploitation\n                    F_mean = np.clip(0.95 * F_mean + 0.05 * 0.4, 0.05, 0.99)\n                    CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slightly expand to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.1)\n                    stagnation_counter += 1\n\n            # If there were multiple successes this generation, slightly encourage exploitation\n            if successes > max(1, self.dim // 4):\n                F_mean = np.clip(F_mean * 0.98, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 1.02, 0.0, 1.0)\n                p_levy = max(0.01, p_levy * 0.9)\n\n            # Strong stagnation reset: if many non-improvements, re-seed half the population\n            if stagnation_counter > max(50, self.dim * 8):\n                n_reinit = max(1, self.pop_size // 2)\n                reinit_indices = rng.choice(self.pop_size, n_reinit, replace=False)\n                for j in reinit_indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # update global best if needed\n                    if fvals[j] < best_f:\n                        best_f = float(fvals[j])\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # enlarge trust radius moderately to encourage exploration after reset\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # increase levy probability to escape other traps\n                p_levy = min(0.8, p_levy * 1.3)\n\n            # safety clamps\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n            p_levy = float(np.clip(p_levy, 0.0, 0.9))\n\n        # finalize\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.164 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08671137843028587, 0.17559904019838857, 0.2179689547667676, 0.15833999333152649, 0.16795134893875963, 0.17492836606637396, 0.18907815286071372, 0.17937789421442762, 0.16200353314774796, 0.1301969615901345]}, "task_prompt": ""}
{"id": "09dd25f8-3dfe-4558-aa9e-06cc2a13d97c", "fitness": 0.3910851014174529, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional heavy-tailed (Lévy-like / Cauchy) jumps for global exploration, and an online-adaptive trust-region local search for focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy/Cauchy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters (F,CR)\n    are adapted online using a jDE-like scheme and success histories.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension, kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar, sequence, numpy array; return numpy array of length dim\n        if np.isscalar(b):\n            return np.full(self.dim, float(b), dtype=float)\n        arr = np.asarray(b, dtype=float)\n        if arr.shape == ():\n            return np.full(self.dim, float(arr), dtype=float)\n        if arr.size == self.dim:\n            return arr.reshape(self.dim,)\n        # try broadcasting\n        return np.broadcast_to(arr, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # Bounds (fallback to [-5,5] if func doesn't provide)\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # safety: ensure valid bounds\n        ub = np.maximum(ub, lb + 1e-12)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        max_trust = max(1e-6, range_norm * 2.0)\n        min_trust = 1e-9\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population until budget exhausted\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if no evaluations possible, return\n        if evals == 0 or not np.any(np.isfinite(fvals)):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # set best\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        trust_radius = max(0.05 * range_norm, 0.5)  # initial trust radius (in absolute units)\n        stagnation_counter = 0\n        successes = 0\n\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step(scale=1.0):\n            s = self.rng.standard_cauchy(size=self.dim) * scale\n            # limit extreme outliers to avoid numerical blow-up, but keep heavy tails\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n\n            # small generational noise to mean parameters (prevents stagnation)\n            F_mean = np.clip(F_mean * (1.0 + 0.005 * (self.rng.rand() - 0.5)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean * (1.0 + 0.01 * (self.rng.rand() - 0.5)), 0.0, 1.0)\n\n            # per-generation counters\n            gen_successes = 0\n\n            # dynamic probability of Levy jumps increases with stagnation\n            p_levy = np.clip(0.12 + 0.0005 * stagnation_counter, 0.05, 0.65)\n\n            # iterate population sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                target = pop[i].copy()\n\n                # jDE-like parameter adaptation: occasionally resample Fi and CRi\n                if self.rng.rand() < 0.1:\n                    Fi = np.clip(self.rng.normal(F_mean, 0.15), 0.05, 0.99)\n                else:\n                    Fi = F_mean\n                if self.rng.rand() < 0.1:\n                    CRi = np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0)\n                else:\n                    CRi = CR_mean\n\n                # decide operator: Levy jump (centered on best) or DE mutation\n                if self.rng.rand() < p_levy:\n                    # Levy/Cauchy jump: global exploration centered on best, scaled by trust radius and problem scale\n                    # scale is smaller initially for focused jumps and larger if heavy stagnation\n                    scale_multiplier = 0.5 + 1.5 * (stagnation_counter / max(1, 50 + self.pop_size))\n                    cauchy = levy_step(scale=scale_multiplier)\n                    # normalize cauchy direction but keep elementwise heavy tails for anisotropy\n                    denom = np.maximum(np.linalg.norm(cauchy), 1e-12)\n                    step = (cauchy / denom) * (trust_radius * (range_norm + 1e-12))\n                    candidate = best_x + step * (0.2 + self.rng.rand() * 1.0)\n                else:\n                    # DE/rand/1-like mutation\n                    # choose three distinct indices different from i when possible\n                    idxs = np.arange(self.pop_size)\n                    if self.pop_size >= 4:\n                        choices = self.rng.choice(np.delete(idxs, i), 3, replace=False)\n                    else:\n                        choices = self.rng.choice(idxs, 3, replace=False)\n                    r1, r2, r3 = choices\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    cr_mask = self.rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[self.rng.randint(0, self.dim)] = True\n                    candidate = np.where(cr_mask, donor, target)\n\n                # projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # one evaluation (respect budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # greedy replacement (population-level selection)\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    gen_successes += 1\n                    successes += 1\n                    # adapt means toward successful parameters\n                    F_mean = np.clip(0.9 * F_mean + 0.1 * Fi, 0.05, 0.99)\n                    CR_mean = np.clip(0.9 * CR_mean + 0.1 * CRi, 0.0, 1.0)\n                else:\n                    stagnation_counter += 1\n\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0  # reset stagnation on improvement\n                # limit runaway stagnation counter\n                stagnation_counter = min(stagnation_counter, 10**6)\n\n                # stop if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n\n            # trust-region local search around best: sample a few Gaussian-noise candidates\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # adaptive number of local samples: small handful scaled by dim and remaining budget\n            local_samples = int(min(max(1, self.dim // 2), 10, remaining))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (0.3 + self.rng.rand(self.dim) * 0.7) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    successes += 1\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = min(max(trust_radius, min_trust), max_trust)\n                    stagnation_counter += 1\n\n                if evals >= self.budget:\n                    break\n\n            # population-level bookkeeping: if no progress for long, reseed part of population\n            if stagnation_counter > max(200, 10 * self.dim):\n                # reinitialize half of population randomly\n                k = max(1, self.pop_size // 2)\n                idxs = self.rng.choice(self.pop_size, k, replace=False)\n                for j in idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n\n            # mild global adaptation based on per-generation successes\n            if gen_successes > 0:\n                # slowly move F_mean/CR_mean towards stable exploitation values\n                F_mean = np.clip(0.97 * F_mean + 0.03 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.97 * CR_mean + 0.03 * 0.9, 0.0, 1.0)\n            else:\n                # encourage exploration if stagnating\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # safety: ensure trust radius stays in bounds\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # small generator-level diversification if too many successes recently (prevent premature convergence)\n            if successes > 5 * self.dim:\n                # inject a single larger jump\n                if evals < self.budget:\n                    jump = levy_step(scale=2.0)\n                    jump = np.clip(jump, -5e2, 5e2)\n                    candidate = best_x + (jump / max(np.linalg.norm(jump), 1e-12)) * (0.5 * range_norm) * self.rng.rand()\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                    successes = 0  # reset to avoid repeated injections\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.391 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.23030967119485923, 0.2365655302725871, 0.39791470815072505, 0.9761596542516192, 0.34514858933091797, 0.5390856270955499, 0.31154214969224947, 0.3761155646372001, 0.25326539841864837, 0.2447441211301733]}, "task_prompt": ""}
{"id": "9ae161a0-b9d7-45f1-89ef-b85585643cb4", "fitness": 0.17408333996356085, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DE-Levy-Trust — a population-based Differential Evolution with occasional Lévy jumps and an online-adaptive trust-region local search to balance fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        # sensible default population scaling with dimension; user can override\n        if pop_size is None:\n            pop_size = int(min(max(4, 6 * self.dim), 60))\n        # ensure population is not larger than a reasonable fraction of budget\n        pop_size = int(max(4, min(pop_size, max(4, self.budget // 5))))\n        self.pop_size = pop_size\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Convert/expand provided bound (scalar or array-like) to a 1-D numpy array of length self.dim.\n        \"\"\"\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full((self.dim,), float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        # obtain and normalize bounds\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            # fallback to [-5,5] if function doesn't expose bounds explicitly\n            lb = np.full((self.dim,), -5.0)\n            ub = np.full((self.dim,), 5.0)\n\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initial population\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # Evaluate initial population (as many as budget allows)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # If no evaluations could be performed, return trivial result\n        finite_mask = np.isfinite(fvals)\n        if not np.any(finite_mask):\n            return float(self.f_opt), self.x_opt\n\n        # determine current best\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # hyper-parameters (adapted online)\n        F_mean = 0.6        # mean differential weight\n        CR_mean = 0.9       # mean crossover rate\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.20   # base scale for Lévy and trust radius (fraction of range)\n        trust_radius = 0.2 * range_norm  # initial trust region radius (in absolute units)\n\n        # trust radius limits (in absolute units)\n        min_trust = max(1e-6 * range_norm, 1e-8)\n        max_trust = 2.0 * range_norm\n\n        # jDE adaptation probabilities\n        tau1 = 0.1\n        tau2 = 0.1\n\n        stagnation_counter = 0\n        stagnation_limit = max(40, 10 * self.dim)\n        recent_success = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (alpha ~1)\n        def levy_step():\n            # Use Cauchy distribution as a simple heavy-tailed step; clip extremes\n            step = rng.standard_cauchy(self.dim)\n            # clip extremely large values to keep numerical stability\n            step = np.clip(step, -10.0, 10.0)\n            # normalize to have robust scale ~1 and apply some random sign scaling\n            med = np.median(np.abs(step)) + 1e-12\n            return step / med\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            successes = 0\n            # per-generation small randomization around means\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                target = pop[i].copy()\n                target_f = fvals[i]\n\n                # jDE-like adaptation: possibly mutate Fi/CRi\n                if rng.random() < tau1:\n                    Fi = 0.1 + 0.9 * rng.random()  # in (0.1,1.0)\n                else:\n                    Fi = F_mean\n                if rng.random() < tau2:\n                    CRi = rng.random()\n                else:\n                    CRi = CR_mean\n\n                # decide between long Lévy jump (exploration) or DE mutation (exploit/explore)\n                if rng.random() < p_levy:\n                    # Lévy jump centered on current best (long-range exploration)\n                    ls = levy_step()\n                    scale = step_scale * (0.5 + rng.random()) * (trust_radius / max(range_norm, 1e-12))\n                    candidate = best_x + ls * scale * range_vec\n                else:\n                    # DE/rand/1-like mutation with binomial crossover\n                    # select r1,r2,r3 distinct from i\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback small random step if population too small\n                        candidate = target + 0.01 * rng.standard_normal(self.dim) * range_vec\n                    else:\n                        r = rng.choice(idxs, size=3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = rng.random(self.dim) < CRi\n                        # ensure at least one component from donor\n                        cr_mask[rng.integers(0, self.dim)] = True\n                        candidate = np.where(cr_mask, donor, target)\n\n                # project candidate to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_cand = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_cand < target_f:\n                    pop[i] = candidate\n                    fvals[i] = f_cand\n                    successes += 1\n                    recent_success += 1\n                    # nudge means toward successful Fi/CRi (if used DE branch)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # small negative adaptation to avoid stagnation\n                    F_mean = 0.995 * F_mean + 0.0005\n                    CR_mean = 0.995 * CR_mean + 0.0005\n\n                # update global best if improved\n                if f_cand < best_f:\n                    best_f = f_cand\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # if budget exhausted break early\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n\n            # TRUST-REGION local search around best: small Gaussian anisotropic samples\n            remaining = self.budget - evals\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension and normalized by range\n                sigma = (0.5 + rng.random(self.dim) * 0.5) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius (focus)\n                    trust_radius = max(min_trust, 0.8 * trust_radius)\n                    stagnation_counter = 0\n                    recent_success += 1\n                else:\n                    # unsuccessful => expand a bit to try escaping\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # adapt exploration probability based on recent success\n            if recent_success > max(1, self.pop_size // 8):\n                # many successes -> exploit more (reduce Levy jumps)\n                p_levy = max(0.01, p_levy * 0.85)\n                # slowly decay trust radius to focus\n                trust_radius = max(min_trust, trust_radius * 0.95)\n            else:\n                # few successes -> promote exploration\n                p_levy = min(0.5, p_levy * 1.05 + 0.005)\n                # gently expand trust region if stagnating\n                if stagnation_counter > (stagnation_limit // 4):\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # regularize means into bounds\n            F_mean = float(np.clip(F_mean, 0.05, 0.99))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n            # stagnation handling: partial re-seeding when stuck\n            if stagnation_counter > stagnation_limit and evals < self.budget:\n                k = max(1, self.pop_size // 2)\n                to_replace = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in to_replace:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # keep best if new one is better\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius slightly to escape region\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n                recent_success = 0\n\n            # small decay/reset of recent_success to keep adaptability\n            recent_success = int(max(0, recent_success - max(1, self.pop_size // 6)))\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.174 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09120221577020715, 0.1608738073871231, 0.22286811760537195, 0.1709418791313766, 0.196867154028193, 0.19599223982383351, 0.20604349564925872, 0.18880344558498585, 0.16770953309427106, 0.1395315115609873]}, "task_prompt": ""}
{"id": "a6deecdb-0e4e-4691-894f-9d0d4c1faa52", "fitness": 0.34252032011254696, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: Differential Evolution with jDE-style adaptation, occasional Lévy-flight long jumps, and a trust-region Gaussian local search around the incumbent; online adaptation of jump probability, step sizes and trust radius to mix fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (jDE-style),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n      algo = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=123)\n      f_best, x_best = algo(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # choose population size sensibly with respect to dimension and budget\n        if pop_size is None:\n            # baseline population: 4*dim but not too large relative to budget\n            pop_default = max(8, 4 * self.dim)\n            pop_size = min(pop_default, max(4, self.budget // 5))\n        self.pop_size = int(max(4, pop_size))\n\n        # state to expose after run\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = self.rng\n        dim = self.dim\n\n        # determine bounds if available, else default to [-5, 5] per problem statement\n        if hasattr(func, \"bounds\"):\n            b = getattr(func, \"bounds\")\n            # try common attribute names\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            elif hasattr(b, \"lower\") and hasattr(b, \"upper\"):\n                lb = np.asarray(b.lower, dtype=float)\n                ub = np.asarray(b.upper, dtype=float)\n            else:\n                # fallback\n                lb = np.full(dim, -5.0)\n                ub = np.full(dim, 5.0)\n        else:\n            # Most BBOB problems use [-5, 5]\n            lb = np.full(dim, -5.0)\n            ub = np.full(dim, 5.0)\n\n        # ensure shapes\n        if lb.size == 1:\n            lb = np.full(dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(dim, float(ub.item()))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population\n        pop = rng.uniform(lb, ub, size=(self.pop_size, dim))\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        best_idx = None\n        best_x = None\n        best_f = np.inf\n\n        # evaluate initial population until budget exhausted\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            xi = pop[i].copy()\n            fi = float(func(xi))\n            evals += 1\n            fvals[i] = fi\n            if fi < best_f:\n                best_f = fi\n                best_x = xi.copy()\n                best_idx = i\n\n        # if no evaluations were possible (budget == 0)\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6      # baseline scaling factor mean\n        CR_mean = 0.9     # baseline crossover mean\n        p_levy = 0.06     # initial chance for a Levy jump per candidate\n        p_levy_base = p_levy\n\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        min_trust = 1e-6\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate simple Levy-like heavy-tailed step using Cauchy\n        def levy_step():\n            s = rng.standard_cauchy(dim)\n            # normalize to unit direction but keep heavy-tail scale through a random scale\n            norm = np.linalg.norm(s) + 1e-12\n            direction = s / norm\n            # heavy-tailed scale: sample from Pareto-like exponential to vary jump lengths\n            scale = max(1e-12, rng.pareto(a=1.5) + 0.5)\n            return direction * scale\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            # shuffle order for fairness\n            order = rng.permutation(self.pop_size)\n            for ii in order:\n                if evals >= self.budget:\n                    break\n\n                # generate Fi and CRi (jDE-like simple adaptation)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # choose operation: Levy jump or DE mutation/crossover\n                if rng.rand() < p_levy:\n                    # Lévy-centered jump around the incumbent best for exploration\n                    step = levy_step()\n                    # scale by trust radius and global range to create candidate\n                    scale = (rng.rand() * 2.0 + 0.5) * trust_radius / (range_norm + 1e-12)\n                    candidate = best_x + step * scale * range_vec\n                    # small local jitter to increase diversity\n                    candidate += rng.normal(0, 0.01, size=dim) * range_vec\n                else:\n                    # DE/rand/1 mutation + binomial crossover\n                    idxs = [j for j in range(self.pop_size) if j != ii]\n                    if len(idxs) < 3:\n                        # fallback to random perturbation if not enough individuals\n                        candidate = pop[ii] + rng.normal(0, 0.05, size=dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = rng.rand(dim) < CRi\n                        # ensure at least one dimension crosses\n                        jrand = rng.randint(dim)\n                        cr_mask[jrand] = True\n                        candidate = np.where(cr_mask, donor, pop[ii])\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # one evaluation if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[ii]:\n                    # accept\n                    pop[ii] = candidate.copy()\n                    fvals[ii] = f_candidate\n                    # adapt means slightly toward used Fi/CRi\n                    F_mean = 0.92 * F_mean + 0.08 * Fi\n                    CR_mean = 0.92 * CR_mean + 0.08 * CRi\n                    # successful local improvement -> shrink trust radius (focus)\n                    trust_radius = max(min_trust, trust_radius * 0.88)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful -> encourage exploration a bit\n                    trust_radius = min(max_trust, trust_radius * 1.02)\n                    stagnation_counter += 1\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    best_idx = ii\n                    stagnation_counter = 0\n\n                # if budget exhausted break\n                if evals >= self.budget:\n                    break\n\n            # End of generation: do a trust-region local search around best if budget allows\n            if evals < self.budget:\n                # number of local samples depends on dim and remaining budget\n                remaining = self.budget - evals\n                local_budget = min( max(1, dim // 2), remaining, 8 )\n                for _ in range(local_budget):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma per-dimension (fraction of trust radius scaled by range)\n                    anis = (0.4 + rng.rand(dim) * 0.8) * (trust_radius / (range_norm + 1e-12))\n                    candidate = best_x + rng.normal(0, 1.0, size=dim) * anis * range_vec\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step => shrink trust radius to focus\n                        trust_radius = max(min_trust, trust_radius * 0.7)\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful => expand a bit to escape local traps\n                        trust_radius = min(max_trust, trust_radius * 1.06)\n                        stagnation_counter += 1\n\n            # adaptation of Levy probability based on stagnation\n            if stagnation_counter > max(10, dim * 2):\n                # more stagnation -> increase long jumps chance\n                p_levy = min(0.5, p_levy * 1.18 + 0.01)\n            else:\n                # slowly decay toward base probability when things improve\n                p_levy = max(p_levy_base, p_levy * 0.98)\n\n            # occasional partial re-seed on strong stagnation\n            if stagnation_counter > max(60, dim * 8) and evals < self.budget:\n                # reinitialize half of the worst individuals (or as many as budget allows)\n                k = max(1, self.pop_size // 2)\n                # sort indices by their fvals (worst first)\n                worst_idx = np.argsort(fvals)[-k:]\n                for j in worst_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = rng.uniform(lb, ub)\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # increase trust radius after restart to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.4)\n                stagnation_counter = 0\n\n            # small safeguard: keep F_mean and CR_mean reasonable\n            F_mean = float(np.clip(F_mean, 0.05, 1.0))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n            gen += 1\n\n        # finished\n        self.f_opt = float(best_f)\n        self.x_opt = None if best_x is None else best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.343 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18652797746992367, 0.17298947545862842, 0.41727255908700034, 0.5459193399259307, 0.3217662439595034, 0.5870284474680713, 0.24955815301290307, 0.4293228722046839, 0.34248686806016326, 0.17233126447866143]}, "task_prompt": ""}
{"id": "55de16c7-591e-4840-80bc-d150b78c2c51", "fitness": 0.4489377351978329, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with per-individual F/CR adaptation, occasional Lévy-flight long jumps for global escape, and a trust-region local search around the incumbent to refine solutions (fast exploration + focused exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (DE/rand/1/bin) with online F/CR adaptation (jDE-like)\n      - Occasional Lévy-like (Cauchy) jumps centered on the best for long-range exploration\n      - Trust-region Gaussian local search around the incumbent for focused exploitation\n    Designed for continuous box-bounded black-box functions. Stops exactly when budget hits zero.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population size heuristic: grows with dim but not larger than budget\n        if pop_size is None:\n            # base 10 + 2*dim but limited by budget/2 to reserve evals for local search\n            pop_guess = max(4, 10 + 2 * self.dim)\n            self.pop_size = min(pop_guess, max(4, self.budget // 2))\n        else:\n            self.pop_size = int(max(4, min(pop_size, self.budget)))\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # final results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        rng = self.rng\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # get bounds and range vector\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        # initialize DE-related parameters\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        min_trust = 1e-8\n\n        # trust-region parameters\n        range_mean = float(np.mean(range_vec))\n        trust_radius = max(1e-6, 0.1 * range_mean)   # initial trust radius relative to box size\n        max_trust = 5.0 * range_mean\n\n        # probability of Lévy jump (increases when stagnating)\n        base_p_levy = 0.05\n\n        # bookkeeping\n        evals = 0\n        pop_size = self.pop_size\n\n        # initialize population\n        pop = lb + rng.random((pop_size, self.dim)) * range_vec\n\n        # Evaluate initial individuals up to budget\n        fvals = np.full(pop_size, np.inf, dtype=float)\n        initial_eval_n = min(pop_size, self.budget)\n        for i in range(initial_eval_n):\n            x = pop[i]\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n        # If budget exhausted during initialization, return best found\n        if evals >= self.budget:\n            return self.f_opt, self.x_opt\n\n        # If we couldn't evaluate full population (because budget < pop_size), do not attempt DE\n        if initial_eval_n < pop_size:\n            # fill remaining individuals with random points (unevaluated) and exit, because no budget\n            for j in range(initial_eval_n, pop_size):\n                pop[j] = lb + rng.random(self.dim) * range_vec\n            return self.f_opt, self.x_opt\n\n        # main loop variables\n        stagnation_counter = 0\n        successes_epoch = 0\n        epoch = 0\n\n        def levy_step():\n            # simple heavy-tailed step using standard Cauchy per-dimension (approx Lévy)\n            s = rng.standard_cauchy(self.dim)\n            # clip extreme outliers to avoid numerical blow-up while keeping heavy tails\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main evolution loop: process generations until budget exhausted\n        while evals < self.budget:\n            epoch += 1\n            successes_epoch = 0\n\n            # dynamic chance of Levy jumps increases with stagnation\n            p_levy = base_p_levy * (1.0 + stagnation_counter / max(10.0, 20.0 + 2.0 * self.dim))\n            p_levy = min(0.5, p_levy)\n\n            # one generation: attempt to update each individual (consuming up to pop_size evals)\n            # we iterate indices in random order to reduce bias\n            indices = rng.permutation(pop_size)\n            for idx in indices:\n                if evals >= self.budget:\n                    break\n\n                target = pop[idx].copy()\n                target_f = float(fvals[idx])\n\n                # find current global best and its index\n                best_idx = int(np.argmin(fvals))\n                best_x = pop[best_idx].copy()\n                best_f = float(fvals[best_idx])\n\n                # sample whether to do a Levy jump or DE mutation\n                if rng.random() < p_levy:\n                    # Levy jump around the best (global search)\n                    scale = trust_radius * (0.5 + 0.5 * rng.random())  # base scaling\n                    step = levy_step()\n                    donor = best_x + scale * step * (range_vec / max(range_mean, 1e-12))\n                else:\n                    # DE/rand/1 mutation with per-trial Fi and CRi (normal perturbation around means)\n                    Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                    # pick a,b,c distinct from idx\n                    idxs = list(range(pop_size))\n                    idxs.remove(idx)\n                    a, b, c = rng.choice(idxs, size=3, replace=False)\n                    donor = pop[a] + Fi * (pop[b] - pop[c])\n\n                    # binomial crossover to form trial\n                    trial = target.copy()\n                    jrand = rng.integers(self.dim)\n                    for j in range(self.dim):\n                        if rng.random() < CRi or j == jrand:\n                            trial[j] = donor[j]\n                    # ensure trial bounds\n                    trial = np.minimum(np.maximum(trial, lb), ub)\n                    donor = trial  # reuse naming so next steps handle trial evaluation uniformly\n\n                # project to bounds before evaluation\n                trial = np.minimum(np.maximum(donor, lb), ub)\n\n                # evaluate candidate if budget remains\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(trial))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < target_f:\n                    pop[idx] = trial.copy()\n                    fvals[idx] = f_candidate\n                    successes_epoch += 1\n                    # adapt means slightly toward successful trial parameters if this was DE\n                    if rng.random() >= p_levy:\n                        # adjust F_mean and CR_mean based on last Fi/CRi sampled (we used Fi/CRi in DE branch)\n                        # small learning rate\n                        alpha = 0.08\n                        # Recompute Fi/CRi approximations: Fi and CRi were local variables only present in DE branch.\n                        # For simplicity, we approximate the Fi that produced donor by using ratio of steps:\n                        # However to keep stable, nudge F_mean slightly downward if many successes (focusing)\n                        F_mean = float(np.clip(F_mean * (1.0 - alpha) + 0.6 * alpha, 0.05, 0.99))\n                        CR_mean = float(np.clip(CR_mean * (1.0 - alpha) + 0.9 * alpha, 0.0, 0.999))\n                    else:\n                        # If Levy produced success, subtly increase exploration tendency\n                        F_mean = float(np.clip(F_mean * 0.995, 0.05, 0.99))\n                        CR_mean = float(np.clip(CR_mean * 1.001, 0.0, 0.999))\n                else:\n                    # unsuccessful: small relaxation of means to encourage exploration\n                    F_mean = float(np.clip(F_mean * 1.001, 0.05, 0.99))\n                    CR_mean = float(np.clip(CR_mean * 0.999, 0.0, 0.999))\n\n                # update global best if improved\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = trial.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # keep current best index coherent (in case replaced)\n                # if the target was the best and got worse (shouldn't happen with greedy), we ensure best_f accurate:\n                # (we will recompute best index at next iteration)\n\n            # End of generation adjustments\n            # Trust-region local search around current best: small Gaussian perturbations\n            remaining = self.budget - evals\n            if remaining > 0:\n                # allocate a few local samples in proportion to dim but not to exceed remaining\n                local_candidates = min(max(1, self.dim // 2), remaining)\n                # anisotropic sigma: per-dimension random scaling times trust_radius scaled by box size\n                for _ in range(local_candidates):\n                    if evals >= self.budget:\n                        break\n                    # sample anisotropic gaussian noise\n                    sigma = trust_radius * (0.5 + rng.random(self.dim) * 0.5)\n                    candidate = self.x_opt + rng.normal(0.0, sigma) * (range_vec / max(range_mean, 1e-12))\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    # if success, accept and shrink trust region to exploit\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = candidate.copy()\n                        trust_radius = max(min_trust, trust_radius * 0.6)\n                        stagnation_counter = 0\n                    else:\n                        # no improvement -> relax/increase trust to escape\n                        trust_radius = min(max_trust, trust_radius * 1.05)\n                        stagnation_counter += 1\n\n            # adapt strategy after generation\n            if successes_epoch > max(1, pop_size // 10):\n                # if many successes we become a bit more exploitative\n                F_mean = float(np.clip(F_mean * 0.98, 0.05, 0.99))\n                CR_mean = float(np.clip(CR_mean * 0.995, 0.0, 0.999))\n                trust_radius = max(min_trust, trust_radius * 0.95)\n            else:\n                # if few successes encourage exploration mildly\n                F_mean = float(np.clip(F_mean * 1.01, 0.05, 0.99))\n                CR_mean = float(np.clip(CR_mean * 1.001, 0.0, 0.999))\n                trust_radius = min(max_trust, trust_radius * 1.02)\n\n            # strong stagnation reset: reseed part of population randomly\n            if stagnation_counter > max(50, 8 * self.dim):\n                k = max(1, pop_size // 2)\n                indices_reinit = rng.choice(np.arange(pop_size), size=k, replace=False)\n                for j in indices_reinit:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    f_new = float(func(pop[j]))\n                    evals += 1\n                    fvals[j] = f_new\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = pop[j].copy()\n                # slightly enlarge trust radius to encourage escape\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0  # reset stagnation after reseed\n\n        # End main loop\n        # Final safeguard: ensure x_opt not None (should not happen)\n        if self.x_opt is None:\n            # fallback: return best in population (if any evaluated)\n            best_idx = int(np.argmin(fvals))\n            if np.isfinite(fvals[best_idx]):\n                self.f_opt = float(fvals[best_idx])\n                self.x_opt = pop[best_idx].copy()\n            else:\n                # random point if absolutely nothing evaluated\n                x = lb + rng.random(self.dim) * range_vec\n                self.x_opt = x\n                self.f_opt = float(func(x))\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.449 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11463782573846681, 0.1534149044361629, 0.8896627462164468, 0.9663951929159366, 0.20977991379070382, 0.904954783269412, 0.2901052245145389, 0.5911060249484554, 0.21858160870516052, 0.15073912744304618]}, "task_prompt": ""}
{"id": "561a6b9c-6194-49a2-ba5b-d0e216b30fad", "fitness": 0.27650974399076944, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy-flight global jumps and an online-adaptive trust-region local search (fast global exploration, focused local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Adaptive hybrid: Differential Evolution + occasional Lévy jumps + trust-region local search\n    with online step-size adaptation. Balanced exploration via DE + Levy and exploitation via\n    trust-region Gaussian searches around the current best. Designed for continuous domains\n    (default bounds [-5, 5] for each dimension) and to respect a strict evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # Default population size: scale with dimension but keep reasonable relative to budget\n        if pop_size is None:\n            # typical DE uses 5-15 * dim; keep smaller if budget limited\n            default = max(6, min(12 * self.dim, max(6, self.budget // max(10, self.dim))))\n            self.pop_size = int(default)\n        else:\n            self.pop_size = int(pop_size)\n        # ensure pop_size at least 4\n        self.pop_size = max(4, self.pop_size)\n        # internal result holders\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # Try to obtain bounds from func, fall back to [-5, 5]\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # ensure shapes\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        # enforce dimension match\n        if lb.size != self.dim or ub.size != self.dim:\n            # try to broadcast scalars\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item())\n            else:\n                raise ValueError(\"Bounds dimension mismatch with self.dim\")\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item())\n            else:\n                raise ValueError(\"Bounds dimension mismatch with self.dim\")\n\n        # helper: clip to bounds\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # initialize population\n        pop = self.rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # Evaluate initial population as budget allows\n        init_count = min(self.pop_size, self.budget)\n        for i in range(init_count):\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n            if fvals[i] < self.f_opt:\n                self.f_opt = fvals[i]\n                self.x_opt = pop[i].copy()\n        # If budget too small to evaluate entire population, leave remaining individuals unevaluated (they will be used only after further eval budget remains)\n        # Set indices of valid (evaluated) individuals\n        valid = fvals != np.inf\n        if not np.any(valid):\n            # No evaluations could be done at all\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # fill unevaluated individuals later when budget allows by generating randoms (their fvals remain inf)\n        # DE hyper-parameters with online adaptation\n        F_mean = 0.6\n        CR_mean = 0.3\n        # jDE-style individual adaptation probabilities\n        tau_F = 0.1\n        tau_CR = 0.1\n\n        # Levy jump probability and trust-region radius\n        p_levy = 0.05 + 0.2 * min(1.0, np.log1p(self.budget) / 10.0)\n        # initial trust radius: fraction of domain\n        domain_range = ub - lb\n        trust_radius = 0.2 * np.linalg.norm(domain_range)  # scalar radius in Euclidean sense\n\n        stagnation_counter = 0\n        max_stagnation = max(50, self.dim * 6)\n\n        gen = 0\n        # history of successful F and CR to update means\n        success_F = []\n        success_CR = []\n\n        # Levy-like step generator using Cauchy distribution (heavy-tailed)\n        def levy_step(scale=1.0):\n            # Sample Cauchy and clip extremes\n            step = self.rng.standard_cauchy(self.dim) * scale\n            step = np.clip(step, -1e3, 1e3)\n            # normalize to have a controlled Euclidean size; allow heavy-tail by scaling with random factor\n            norm = np.linalg.norm(step)\n            if norm == 0:\n                return step\n            # scale so that typical magnitude ~ scale\n            return step / norm * (scale * (1.0 + self.rng.rand()))\n\n        # Main loop: proceed generation by generation until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # Make sure fvals has valid numeric entries for selection; if some are inf but budget allows, evaluate them now randomly\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n                if fvals[i] == np.inf:\n                    fvals[i] = float(func(pop[i]))\n                    evals += 1\n                    if fvals[i] < self.f_opt:\n                        self.f_opt = fvals[i]\n                        self.x_opt = pop[i].copy()\n\n            # compute indices for DE operations\n            indices = np.arange(self.pop_size)\n            # shuffle for processing order\n            self.rng.shuffle(indices)\n\n            successes_in_gen = 0\n            success_F.clear()\n            success_CR.clear()\n\n            for idx in indices:\n                if evals >= self.budget:\n                    break\n\n                i = int(idx)\n                xi = pop[i]\n                fi = fvals[i]\n\n                # sample Fi and CRi per individual (jDE style)\n                if self.rng.rand() < tau_F:\n                    Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 1.0)\n                else:\n                    Fi = np.clip(self.rng.rand() * 0.9 + 0.05, 0.05, 1.0)\n                if self.rng.rand() < tau_CR:\n                    CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                else:\n                    CRi = np.clip(self.rng.rand(), 0.0, 1.0)\n\n                # Choose mutation strategy probabilistically\n                r = self.rng.rand()\n                # ensure we have three distinct indices different from i\n                all_idx = list(range(self.pop_size))\n                all_idx.remove(i)\n                r1, r2, r3 = self.rng.choice(all_idx, size=3, replace=False)\n                # standard DE/rand/1 donor as baseline\n                donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # Occasionally use a Levy-centered jump around best or around current to perform long-range exploration\n                if self.rng.rand() < p_levy:\n                    # center at best to enforce directed long jumps\n                    center = self.x_opt.copy() if self.x_opt is not None else xi\n                    # scale levy step by trust radius and domain span\n                    levy_scale = max(1e-12, trust_radius) + 0.2 * np.linalg.norm(domain_range)\n                    donor = center + levy_step(scale=levy_scale)\n                    # also blend with DE donor to keep some structure\n                    blend = 0.5 + 0.5 * self.rng.rand()\n                    donor = blend * donor + (1 - blend) * donor  # (keeps donor same) placeholder for potential mixing\n\n                # Binomial crossover\n                cr_mask = self.rng.rand(self.dim) < CRi\n                # ensure at least one dimension crosses\n                if not np.any(cr_mask):\n                    cr_mask[self.rng.randint(self.dim)] = True\n                trial = np.where(cr_mask, donor, xi)\n                # Occasionally mix a bit with the best (directed exploitation)\n                if self.rng.rand() < 0.1:\n                    trial = 0.8 * trial + 0.2 * self.x_opt\n\n                # project to bounds\n                trial = clip(trial)\n\n                # one evaluation if budget allows\n                if evals >= self.budget:\n                    break\n                ft = float(func(trial))\n                evals += 1\n\n                # selection: greedy replacement\n                if ft < fi:\n                    pop[i] = trial\n                    fvals[i] = ft\n                    successes_in_gen += 1\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n                    # update global best\n                    if ft < self.f_opt:\n                        self.f_opt = ft\n                        self.x_opt = trial.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 0\n                else:\n                    stagnation_counter += 1\n\n                # small online nudging of the parameter means toward successful Fi/CRi\n                if len(success_F) > 0:\n                    F_mean = 0.9 * F_mean + 0.1 * np.mean(success_F)\n                if len(success_CR) > 0:\n                    CR_mean = 0.9 * CR_mean + 0.1 * np.mean(success_CR)\n                # adaptive adjustment of p_levy based on stagnation\n                if stagnation_counter > 0 and (stagnation_counter % max(5, self.dim // 2) == 0):\n                    p_levy = min(0.4, p_levy * (1.0 + stagnation_counter / (10.0 + self.dim)))\n                p_levy = np.clip(p_levy, 0.01, 0.6)\n\n            # End of generation: trust-region local search around best\n            # Use a small budget fraction for local refinement\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # local samples at most a small multiple of dim, but not exceeding remaining\n            local_samples = int(min(max(2, self.dim // 2), remaining, 4 + self.dim // 2))\n            # anisotropic Gaussian noise scaled by trust_radius\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # per-dimension sigma is random fraction times trust_radius normalized by domain norm\n                domain_norm = np.maximum(np.linalg.norm(domain_range), 1e-12)\n                sigma_vec = (0.2 + self.rng.rand(self.dim) * 0.8) * (trust_radius / domain_norm)\n                candidate = self.x_opt + self.rng.normal(0.0, 1.0, size=self.dim) * sigma_vec\n                candidate = clip(candidate)\n                fc = float(func(candidate))\n                evals += 1\n                if fc < self.f_opt:\n                    self.f_opt = fc\n                    self.x_opt = candidate.copy()\n                    # shrink trust radius to focus\n                    trust_radius *= 0.85\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful local step => small expansion (to escape)\n                    trust_radius *= 1.05\n                    stagnation_counter += 1\n                # keep trust_radius in sensible bounds\n                trust_radius = np.clip(trust_radius, 1e-6 * np.linalg.norm(domain_range), 2.0 * np.linalg.norm(domain_range))\n\n            # Post-generation adjustments\n            # If we had many successes, slightly decrease exploration (reduce p_levy) and shrink trust region\n            if successes_in_gen > max(1, self.pop_size // 10):\n                p_levy = max(0.01, p_levy * 0.95)\n                trust_radius = max(1e-6, trust_radius * 0.95)\n                stagnation_counter = max(0, stagnation_counter - successes_in_gen // 2)\n            else:\n                # few successes -> more exploration\n                p_levy = min(0.6, p_levy * 1.05)\n                trust_radius = min(np.linalg.norm(domain_range), trust_radius * 1.02)\n\n            # stagnation handling: if stuck for too long, re-seed half the population\n            if stagnation_counter > max_stagnation:\n                reinit_count = max(1, self.pop_size // 2)\n                for j in range(reinit_count):\n                    idx = self.rng.randint(self.pop_size)\n                    pop[idx] = self.rng.uniform(lb, ub)\n                    if evals >= self.budget:\n                        break\n                    fvals[idx] = float(func(pop[idx]))\n                    evals += 1\n                    if fvals[idx] < self.f_opt:\n                        self.f_opt = fvals[idx]\n                        self.x_opt = pop[idx].copy()\n                # enlarge trust radius slightly after reset\n                trust_radius = min(np.linalg.norm(domain_range), trust_radius * 1.5)\n                stagnation_counter = 0\n                # encourage more Levy jumps temporarily\n                p_levy = min(0.6, p_levy * 1.3)\n\n        # Final results\n        self.f_opt = float(self.f_opt)\n        if self.x_opt is not None:\n            self.x_opt = np.array(self.x_opt, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.277 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10503474553990555, 0.189312792602789, 0.30513976193024694, 0.7831220529860317, 0.21949636724844346, 0.3412875977384162, 0.22125830582505646, 0.24330854529335277, 0.20819003946371395, 0.14894723127973875]}, "task_prompt": ""}
{"id": "0e15c09b-664a-4f60-9884-a2bc616d1ca8", "fitness": 0.24458237307875547, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy (Cauchy) long jumps and a trust-region local search; online adapts DE parameters and jump probability to balance global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    are adapted online based on recent success history.\n\n    Usage: instantiate with budget and dim. Call instance with the black-box\n    function object `func` (assumed to accept a 1D numpy array and return a scalar).\n    The black-box may expose bounds via func.bounds.lb and func.bounds.ub (arrays or scalars);\n    otherwise bounds default to [-5, 5] as requested by the task.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # sensible default population size: scaled to dim but not too large\n        if pop_size is None:\n            self.pop_size = max(6, min(40, 8 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n\n        # internal outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        rng = self.rng\n\n        # determine bounds (support scalar or array)\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        rng = self.rng\n\n        # cap population by budget (we need at least pop_size initial evaluations)\n        self.pop_size = min(self.pop_size, max(2, self.budget))  # at least 2\n        # but ensure not exceed budget (we need some budget for algorithm progress)\n        self.pop_size = min(self.pop_size, max(2, self.budget // 5) if self.budget < 200 else self.pop_size)\n\n        # initialize population uniformly\n        pop = rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n\n        # Evaluate initial population sequentially until budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            f = func(pop[i])\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = pop[i].copy()\n\n        # If budget was too small to evaluate entire pop, shrink population to evaluated part\n        if np.isinf(fvals).any():\n            # keep only evaluated individuals\n            mask = ~np.isinf(fvals)\n            if not np.any(mask):\n                # nothing evaluated! fallback: sample a random point and evaluate\n                x0 = rng.uniform(lb, ub)\n                f0 = func(x0); evals += 1\n                self.f_opt, self.x_opt = f0, x0.copy()\n                pop = pop[[0]]\n                fvals = np.array([f0])\n                self.pop_size = 1\n            else:\n                pop = pop[mask]\n                fvals = fvals[mask]\n                self.pop_size = pop.shape[0]\n\n        # algorithm hyper-parameters and online-adaptive statistics\n        F_mean = 0.6\n        CR_mean = 0.5\n        p_levy = 0.08             # base probability for Levy jump\n        trust_radius = 0.2        # relative trust radius (fraction of range)\n        range_vec = ub - lb\n\n        # bookkeeping for adaptation and stagnation\n        best_history_eval = evals\n        no_improve_counter = 0\n        stagnation_threshold = max(50, 5 * self.dim)\n        gen = 0\n\n        def levy_step(scale=0.7, clip=10.0):\n            # use a Cauchy as a simple heavy-tailed step (approx Levy)\n            step = rng.standard_cauchy(self.dim) * scale\n            # clip extreme outliers\n            step = np.clip(step, -clip, clip)\n            # scale to problem range\n            return step * range_vec\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            gen_success_F = []\n            gen_success_CR = []\n            gen_improvements = 0\n            gen_success_levy = 0\n\n            # shuffle indices to avoid order bias\n            idxs = np.arange(self.pop_size)\n            rng.shuffle(idxs)\n\n            # per-target DE steps\n            for idx in idxs:\n                if evals >= self.budget:\n                    break\n\n                # per-individual adaptation similar to jDE: sample Fi and CRi around means\n                Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.2)\n                CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # decide if we do Levy jump or DE\n                if rng.random() < p_levy:\n                    # Lévy-centered exploration around the current best\n                    levy = levy_step(scale=0.5 + 0.5 * rng.random())\n                    donor = self.x_opt + levy * (1.0 + 0.5 * rng.random())\n                    # small blend with a random DE-like perturbation to keep diversity\n                    # pick two random distinct indices different from idx\n                    r = rng.choice(self.pop_size, 2, replace=False)\n                    donor = 0.6 * donor + 0.4 * (pop[r[0]] + Fi * (pop[r[1]] - pop[r[0]]))\n                    # mark that this trial was a levy attempt for adaptation bookkeeping\n                    used_levy = True\n                else:\n                    # classic DE/rand/1 mutation plus a tiny pull to the best scaled by trust_radius\n                    r1, r2, r3 = rng.choice(np.delete(np.arange(self.pop_size), idx), 3, replace=False)\n                    base = pop[r1]\n                    mutant = base + Fi * (pop[r2] - pop[r3])\n                    # pull toward best proportional to trust radius to encourage exploitation\n                    mutant = mutant + (trust_radius * (self.x_opt - pop[idx])) * rng.random()\n                    donor = mutant\n                    used_levy = False\n\n                # binomial crossover\n                cr_mask = rng.random(self.dim) < CRi\n                # ensure at least one dimension from donor (standard DE)\n                jrand = rng.integers(0, self.dim)\n                cr_mask[jrand] = True\n                trial = np.where(cr_mask, donor, pop[idx])\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = func(trial)\n                evals += 1\n\n                # selection\n                if f_candidate < fvals[idx]:\n                    # accept\n                    pop[idx] = trial\n                    fvals[idx] = f_candidate\n                    gen_success_F.append(Fi)\n                    gen_success_CR.append(CRi)\n                    gen_improvements += 1\n                    if used_levy:\n                        gen_success_levy += 1\n                    # update global best\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = trial.copy()\n                        best_history_eval = evals\n                        no_improve_counter = 0\n                else:\n                    # unsuccessful: small random relaxation (to encourage diversity)\n                    if rng.random() < 0.03:\n                        j = rng.integers(0, self.dim)\n                        pop[idx, j] = lb[j] + rng.random() * range_vec[j]\n\n            # end of population loop (one \"generation\")\n            # Adapt F_mean and CR_mean using simple moving averages of successful Fi/CRi\n            if gen_success_CR:\n                CR_mean = np.clip(0.9 * CR_mean + 0.1 * np.mean(gen_success_CR), 0.01, 0.99)\n            else:\n                # slowly revert to moderate exploration\n                CR_mean = np.clip(0.99 * CR_mean + 0.01 * 0.5, 0.01, 0.99)\n\n            if gen_success_F:\n                # Lehmer-like emphasis for F (gives more weight to larger F if they succeed)\n                sF = np.array(gen_success_F)\n                # avoid zero division\n                num = np.sum(sF * sF)\n                den = np.sum(sF) + 1e-12\n                F_mean = np.clip(0.85 * F_mean + 0.15 * (num / den), 0.05, 1.2)\n            else:\n                F_mean = np.clip(0.995 * F_mean + 0.005 * 0.6, 0.05, 1.2)\n\n            # Adjust p_levy based on improvements: if no improvement increase long jumps, else slightly reduce\n            if gen_improvements == 0:\n                p_levy = np.clip(p_levy * 1.04, 0.02, 0.5)\n            else:\n                # if many Levy successes, keep or favour a bit more Levy\n                if gen_success_levy > 0 and gen_success_levy >= max(1, gen_improvements // 2):\n                    p_levy = np.clip(p_levy * 1.02, 0.02, 0.5)\n                else:\n                    p_levy = np.clip(p_levy * 0.97, 0.02, 0.5)\n\n            # trust-region local search around best: use remaining budget conservatively\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # choose number of local samples; scale with dimension but keep small\n            num_local = int(min(max(1, self.dim // 2), max(1, remaining // max(5, self.dim))))\n            num_local = min(num_local, remaining)\n\n            if num_local > 0:\n                local_success = 0\n                for _ in range(num_local):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic Gaussian noise scaled by trust_radius and range\n                    sigma = trust_radius * (0.6 + 0.8 * rng.random()) * range_vec\n                    cand = self.x_opt + rng.normal(0.0, 1.0, size=self.dim) * sigma\n                    cand = np.minimum(np.maximum(cand, lb), ub)\n                    f_c = func(cand)\n                    evals += 1\n                    if f_c < self.f_opt:\n                        local_success += 1\n                        self.f_opt = f_c\n                        self.x_opt = cand.copy()\n                        best_history_eval = evals\n                        no_improve_counter = 0\n                    # try to insert candidate into population replacing worst if better\n                    worst_idx = np.argmax(fvals)\n                    if f_c < fvals[worst_idx]:\n                        pop[worst_idx] = cand\n                        fvals[worst_idx] = f_c\n\n                # adjust trust radius: successful local search -> shrink to focus; else expand slightly\n                if local_success > 0:\n                    trust_radius = max(0.01, trust_radius * (0.6 ** (local_success)))\n                else:\n                    trust_radius = min(1.5, trust_radius * 1.05)\n\n            # stagnation: if no improvement for a while, partially re-seed population and enlarge trust region\n            if evals - best_history_eval >= stagnation_threshold:\n                no_improve_counter += 1\n            else:\n                no_improve_counter = 0\n\n            if no_improve_counter > 0 and no_improve_counter % 3 == 0 and evals < self.budget:\n                # re-seed up to half of population\n                k = max(1, self.pop_size // 2)\n                replace_idx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in replace_idx:\n                    # keep the current best as anchor, otherwise random reinit\n                    if rng.random() < 0.3:\n                        pop[j] = self.x_opt + rng.normal(0, 1.0, size=self.dim) * (0.8 * trust_radius * range_vec)\n                    else:\n                        pop[j] = rng.uniform(lb, ub)\n                    # evaluate if budget permits\n                    if evals < self.budget:\n                        fnew = func(pop[j])\n                        evals += 1\n                        fvals[j] = fnew\n                        if fnew < self.f_opt:\n                            self.f_opt = fnew\n                            self.x_opt = pop[j].copy()\n                            best_history_eval = evals\n                trust_radius = min(1.0, trust_radius * 1.3)\n                # increase exploration chance a bit\n                p_levy = min(0.5, p_levy * 1.2)\n\n            # gentle diversification to maintain spread: occasionally jitter worst individuals\n            if rng.random() < 0.08 and evals < self.budget:\n                widx = np.argmax(fvals)\n                jitter = rng.normal(0, 0.5, size=self.dim) * range_vec * trust_radius\n                cand = pop[widx] + jitter\n                cand = np.minimum(np.maximum(cand, lb), ub)\n                f_c = func(cand)\n                evals += 1\n                if f_c < fvals[widx]:\n                    pop[widx] = cand\n                    fvals[widx] = f_c\n                    if f_c < self.f_opt:\n                        self.f_opt = f_c\n                        self.x_opt = cand.copy()\n                        best_history_eval = evals\n\n            # safety break in case budget exact\n            if evals >= self.budget:\n                break\n\n        return float(self.f_opt), np.array(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.245 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09052013087871058, 0.16654026157030932, 0.22489008654671372, 0.4099631114917305, 0.20774579232200774, 0.40500853370908163, 0.2528488425245903, 0.22636164546059223, 0.2902444131833345, 0.17170091310048408]}, "task_prompt": ""}
{"id": "af709d11-9e60-493f-a10f-84b0f7aa82bf", "fitness": 0.25893303773918963, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: population DE with per-individual jDE-like adaptation, occasional Lévy (Cauchy) long jumps, and a trust-region Gaussian local search around the best; online adaptation of step-sizes and restart-on-stagnation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) long jumps, and a trust-region\n    Gaussian local search around the current best. Uses simple online\n    adaptation of F and CR (jDE-like), adapts Lévy rate and trust radius,\n    and performs partial re-seeding on stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size heuristics: scale with dim but not too large relative to budget\n        if pop_size is None:\n            # baseline population = max(6, 8 + dim//2) but limited by budget\n            base = max(6, 8 + self.dim // 2)\n            max_allowed = max(4, self.budget // 20)\n            self.pop_size = int(min(base, max_allowed))\n            self.pop_size = max(4, self.pop_size)\n        else:\n            self.pop_size = int(pop_size)\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b, default_low=-5.0, default_high=5.0):\n        # Accept scalars or array-likes and broadcast to length dim\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # If mismatch, try to broadcast or fallback to default\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            return np.full(self.dim, float(default_low if np.all(b < 0) else default_high))\n\n    def __call__(self, func):\n        # get bounds from func if present; otherwise default [-5,5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        if np.any(range_vec <= 0):\n            # fallback if bounds degenerate\n            range_vec = np.maximum(range_vec, 1.0)\n\n        # initialize population uniformly in bounds\n        X = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n        # evaluate initial population sequentially but do not exceed budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            xi = X[i].copy()\n            try:\n                fv = func(xi)\n            except Exception:\n                fv = np.inf\n            fvals[i] = fv\n            evals += 1\n            if fv < self.f_opt:\n                self.f_opt = fv\n                self.x_opt = xi.copy()\n                last_improve_at = evals\n        # if no evals were possible, return placeholder\n        if evals == 0:\n            return self.f_opt, self.x_opt\n\n        # Algorithm hyper-parameters (online adapted)\n        F_mean = 0.6\n        CR_mean = 0.5\n        tau_F = 0.1  # chance to sample new F\n        tau_CR = 0.1\n        c_adapt = 0.1  # learning rate for means\n\n        # Lévy (Cauchy) long jump parameters\n        p_levy = 0.05\n        p_levy_min = 0.01\n        p_levy_max = 0.4\n\n        # trust region\n        trust_radius = 0.25 * np.mean(range_vec)  # initial trust radius\n        trust_min = 1e-6 * np.mean(range_vec)\n        trust_max = np.mean(range_vec) * 2.0\n\n        stagnation = 0\n        stagnation_threshold = max(200, 10 * self.dim)\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step(scale=1.0):\n            # standard Cauchy is heavy-tailed; cap extremes\n            s = self.rng.standard_cauchy(size=self.dim)\n            # limit extreme outliers to avoid numerical blow-up\n            s = np.clip(s, -1e3, 1e3)\n            # scale relative to search range\n            return scale * s\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            # per-generation randomization of F_mean and CR_mean jitter\n            # process population sequentially\n            idxs = np.arange(self.pop_size)\n            self.rng.shuffle(idxs)\n            # store successful parameters for learning\n            succ_F = []\n            succ_CR = []\n            succ_counts = 0\n\n            for ii in idxs:\n                if evals >= self.budget:\n                    break\n                # target vector and current fitness\n                x_t = X[ii]\n                f_t = fvals[ii]\n\n                # sample Fi and CRi (jDE-like)\n                if self.rng.rand() < tau_F:\n                    Fi = 0.1 + 0.9 * self.rng.rand()\n                else:\n                    Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 0.95)\n                if self.rng.rand() < tau_CR:\n                    CRi = self.rng.rand()\n                else:\n                    CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # choose mutation strategy\n                # pick three distinct indices different from ii\n                choices = [j for j in range(self.pop_size) if j != ii]\n                if len(choices) < 3:\n                    # degenerate small population: random perturbation\n                    mutant = x_t + Fi * levy_step(scale=0.1) * range_vec\n                else:\n                    r = self.rng.choice(choices, 3, replace=False)\n                    r1, r2, r3 = r[0], r[1], r[2]\n\n                    # with some probability use \"best\" guidance\n                    if self.rng.rand() < 0.2:\n                        # DE/best/1\n                        best_idx = np.argmin(fvals)\n                        mutant = X[best_idx] + Fi * (X[r1] - X[r2])\n                    else:\n                        # DE/rand/1\n                        mutant = X[r1] + Fi * (X[r2] - X[r3])\n\n                    # occasional Levy-centered exploration around best\n                    if self.rng.rand() < p_levy:\n                        # levy vector scaled by search range and current trust radius fraction\n                        lev_scale = 0.5 * trust_radius / (np.mean(range_vec) + 1e-12)\n                        mutant = X[np.argmin(fvals)] + levy_step(scale=lev_scale) * range_vec\n                        # incorporate some random local perturbation\n                        mutant = mutant + self.rng.normal(0, 0.5 * trust_radius, size=self.dim)\n\n                # crossover (binomial)\n                trial = np.empty_like(x_t)\n                jrand = self.rng.randint(self.dim)\n                rand_mask = self.rng.rand(self.dim) < CRi\n                for j in range(self.dim):\n                    if rand_mask[j] or j == jrand:\n                        trial[j] = mutant[j]\n                    else:\n                        trial[j] = x_t[j]\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # one evaluation\n                try:\n                    f_trial = func(trial)\n                except Exception:\n                    f_trial = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                if f_trial < f_t:\n                    X[ii] = trial\n                    fvals[ii] = f_trial\n                    succ_F.append(Fi)\n                    succ_CR.append(CRi)\n                    succ_counts += 1\n                    # move means slightly toward successful parameters\n                    F_mean = (1 - c_adapt) * F_mean + c_adapt * Fi\n                    CR_mean = (1 - c_adapt) * CR_mean + c_adapt * CRi\n                    # update global best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        stagnation = 0\n                else:\n                    # unsuccessful => slight diversity nudges\n                    F_mean = (1 - 0.01) * F_mean + 0.01 * Fi\n                    CR_mean = (1 - 0.01) * CR_mean + 0.01 * CRi\n\n                # update best of population for strategies needing it\n                best_idx = np.argmin(fvals)\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n\n            # trust-region local search around best: small anisotropic Gaussian sampling\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # decide number of local samples (keep it small)\n            n_local = int(min(max(1, self.dim // 2), remaining))\n            # if population very small, do a couple more local tries\n            n_local = max(1, min(n_local, remaining))\n            local_improved = False\n            best_idx = np.argmin(fvals)\n            best_x = X[best_idx].copy()\n            best_f = fvals[best_idx]\n\n            for _ in range(n_local):\n                # anisotropic sigma: base trust_radius scaled by random per-dim\n                sigma = trust_radius * (0.2 + 0.8 * self.rng.rand(self.dim))\n                cand = best_x + self.rng.normal(0, sigma)\n                cand = np.minimum(np.maximum(cand, lb), ub)\n                try:\n                    f_cand = func(cand)\n                except Exception:\n                    f_cand = np.inf\n                evals += 1\n                if f_cand < best_f:\n                    best_f = f_cand\n                    best_x = cand.copy()\n                    local_improved = True\n                    # shrink trust radius to focus\n                    trust_radius = max(trust_min, trust_radius * 0.7)\n                    # also nudge global adaptation to exploit\n                    F_mean = max(0.05, F_mean * 0.95)\n                    CR_mean = min(1.0, CR_mean * 1.05)\n                    # update global best\n                    if f_cand < self.f_opt:\n                        self.f_opt = f_cand\n                        self.x_opt = cand.copy()\n                        stagnation = 0\n                else:\n                    # unsuccessful local step: expand a bit to escape\n                    trust_radius = min(trust_max, trust_radius * 1.02)\n\n                if evals >= self.budget:\n                    break\n\n            # if local improved, replace one worst individual with new best candidate\n            if local_improved:\n                worst_idx = np.argmax(fvals)\n                X[worst_idx] = best_x.copy()\n                fvals[worst_idx] = best_f\n\n            # adaptation of p_levy based on recent success / stagnation\n            if succ_counts > 0 or local_improved:\n                # success: reduce long jump probability a bit\n                p_levy = max(p_levy_min, p_levy * (0.95))\n                stagnation = 0\n            else:\n                stagnation += 1\n                # if stagnating, increase chance of long jumps gradually\n                if stagnation % 5 == 0:\n                    p_levy = min(p_levy_max, p_levy * 1.1)\n                # minor expansion to explore more\n                trust_radius = min(trust_max, trust_radius * 1.02)\n\n            # if many successes, encourage exploitation less (slightly)\n            if succ_counts > max(1, self.pop_size // 4):\n                F_mean = max(0.05, F_mean * 0.98)\n                CR_mean = min(1.0, CR_mean * 1.02)\n\n            # stagnation handling: partial re-seed\n            if stagnation > stagnation_threshold and evals < self.budget:\n                # reinitialize half population randomly\n                n_reseed = max(1, self.pop_size // 2)\n                inds = self.rng.choice(self.pop_size, n_reseed, replace=False)\n                for ii in inds:\n                    X[ii] = lb + self.rng.rand(self.dim) * range_vec\n                    # evaluate new members if budget allows\n                    if evals < self.budget:\n                        try:\n                            f_new = func(X[ii])\n                        except Exception:\n                            f_new = np.inf\n                        fvals[ii] = f_new\n                        evals += 1\n                        if f_new < self.f_opt:\n                            self.f_opt = f_new\n                            self.x_opt = X[ii].copy()\n                # slightly enlarge trust region to escape traps\n                trust_radius = min(trust_max, trust_radius * 1.5)\n                p_levy = min(p_levy_max, p_levy * 1.5)\n                stagnation = 0  # reset stagnation after reset\n\n            # safety clamp for F_mean and CR_mean\n            F_mean = float(np.clip(F_mean, 0.05, 0.95))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n            p_levy = float(np.clip(p_levy, p_levy_min, p_levy_max))\n\n            # small housekeeping: if global best in population lost (due to un-evaluated individuals),\n            # ensure best is tracked\n            if self.x_opt is None:\n                best_idx = np.argmin(fvals)\n                self.x_opt = X[best_idx].copy()\n                self.f_opt = fvals[best_idx]\n\n        # final return\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.259 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2337473276860289, 0.18993295089560136, 0.34603005708482126, 0.22081841335541885, 0.3059799666794858, 0.31769567760264805, 0.31403950645232803, 0.25565392112130814, 0.24639920999180498, 0.1590333465224507]}, "task_prompt": ""}
{"id": "5ad198c4-3ac1-4e2d-a1ef-a706fa5f4850", "fitness": 0.273592024510031, "name": "AHDL", "description": "Adaptive Hybrid Differential Evolution with Lévy-driven jumps and trust-region local refinement — combines DE-style population search, occasional heavy-tailed Lévy jumps for global escapes, and an adaptive trust-region local sampler with online parameter adaptation and targeted restarts.", "code": "import numpy as np\n\nclass AHDL:\n    \"\"\"\n    Adaptive Hybrid Differential Evolution + Levy jumps + Trust-region local refinement.\n    Designed for continuous bounded optimization (e.g., BBOB noiseless functions).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size if not provided\n        if pop_size is None:\n            self.pop_size = int(max(4, min(60, 8 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # ensure pop size not larger than budget (we must be able to evaluate at least once)\n        self.pop_size = min(self.pop_size, max(1, self.budget))\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # get bounds from func; BBOB typical bounds are [-5,5], but we use provided ones\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 1.0\n\n        rng = self.rng\n        budget = self.budget\n\n        # Initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (stop if budget exhausted)\n        for i in range(self.pop_size):\n            if evals >= budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # If no evaluations possible, return default\n        if not np.isfinite(fvals).any():\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # set initial best\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-params and adaptive stats\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.08\n        step_scale = 0.25\n        trust_radius = 0.2 * range_norm\n        min_trust = 1e-8 * range_norm\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        # History buffers for successful Fi/CRi\n        succ_F = []\n        succ_CR = []\n\n        # Levy-like heavy-tailed step (Cauchy-ish)\n        def levy_step(dim):\n            s = rng.standard_cauchy(dim)\n            # limit extreme outliers to keep stable steps\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # Main optimization loop\n        while evals < budget:\n            gen += 1\n            successes = 0\n            succ_F.clear()\n            succ_CR.clear()\n\n            # For selection and mutation, ensure we pick indices among valid/evaluated individuals\n            # We'll treat all population indices; if some fvals are inf they are simply poor and can be replaced.\n            indices = np.arange(self.pop_size)\n\n            for i in range(self.pop_size):\n                if evals >= budget:\n                    break\n\n                # Decide between Levy exploration and DE-style offspring\n                if rng.rand() < p_levy:\n                    # Levy centered on current best for long-range exploration\n                    step = levy_step(self.dim)\n                    # scale: combine global step_scale, trust_radius, and search range\n                    scale = step_scale * (0.2 + 0.8 * rng.rand()) * (trust_radius / (range_norm + 1e-12))\n                    donor = best_x + scale * step * (range_vec / (range_vec.mean() + 1e-12))\n                    # simple local mixing with current individual to keep some diversity\n                    if rng.rand() < 0.5:\n                        j = rng.randint(0, self.pop_size)\n                        donor = 0.6 * donor + 0.4 * pop[j]\n                    candidate = np.clip(donor, lb, ub)\n                    used_Fi = None\n                    used_CRi = None\n                else:\n                    # DE/rand/1-like mutation with jDE-like per-individual Fi and CRi sampling\n                    # Choose three distinct indices not equal to i\n                    choices = indices[indices != i]\n                    if choices.size < 3:\n                        # fallback: random sample with replacement\n                        r1, r2, r3 = rng.randint(0, self.pop_size, 3)\n                    else:\n                        r1, r2, r3 = rng.choice(choices, 3, replace=False)\n                    # sample Fi and CRi around means\n                    Fi = np.clip(rng.normal(F_mean, 0.15), 0.05, 1.0)\n                    CRi = np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0)\n                    mutant = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, mutant, pop[i])\n                    candidate = np.clip(trial, lb, ub)\n                    used_Fi = Fi\n                    used_CRi = CRi\n\n                # Evaluate candidate if budget remains\n                if evals >= budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Greedy selection: replace target if better\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # record successful Fi/CRi if present (for adaptation)\n                    if used_Fi is not None:\n                        succ_F.append(used_Fi)\n                    if used_CRi is not None:\n                        succ_CR.append(used_CRi)\n                    # nudge parameter means toward successful values (small step)\n                    if succ_F:\n                        F_mean = 0.9 * F_mean + 0.1 * np.mean(succ_F)\n                    if succ_CR:\n                        CR_mean = 0.9 * CR_mean + 0.1 * np.mean(succ_CR)\n\n                # Update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of population pass\n\n            # Trust-region local refinement around best_x\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n            # local samples: a few attempts relative to dimension and remaining budget\n            local_samples = min(remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= budget:\n                    break\n                # anisotropic gaussian sigma per-dim scaled by trust_radius and range\n                sigma = (0.25 + rng.rand(self.dim) * 0.75) * (trust_radius / (range_norm + 1e-12))\n                noise = rng.randn(self.dim) * sigma\n                candidate = np.clip(best_x + noise * (range_vec / (range_vec.mean() + 1e-12)), lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local refinement: shrink trust radius to focus\n                    trust_radius *= 0.8\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful: relax trust radius a bit to encourage escapes\n                    trust_radius *= 1.07\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter += 1\n\n            # Adapt exploration probability and parameter means based on success\n            if successes > 0:\n                # reduce levy probability slightly if exploitation is working\n                p_levy = max(0.01, p_levy * (0.97 if successes > max(1, self.pop_size * 0.15) else 0.99))\n                # move means slightly toward conservative defaults\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation: increase chance of long jumps and diversify F a little\n                p_levy = min(0.6, p_levy * 1.06 + 0.005)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # Strong stagnation handling: periodic targeted reinitialization\n            if stagnation_counter > 8 * max(1, self.dim):\n                # reinitialize a fraction of population (half) near random/global region\n                k = max(1, self.pop_size // 2)\n                reinit_idx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reinit_idx:\n                    if evals >= budget:\n                        break\n                    # either uniform random or perturbed best to keep some good structure\n                    if rng.rand() < 0.5:\n                        pop[j] = lb + rng.rand(self.dim) * range_vec\n                    else:\n                        # perturb best with larger gaussian\n                        pop[j] = np.clip(best_x + rng.randn(self.dim) * (0.5 * trust_radius / (range_norm + 1e-12)) * (range_vec / (range_vec.mean() + 1e-12)), lb, ub)\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius a bit after reset to re-explore\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n                # slightly boost Levy chance\n                p_levy = min(0.5, p_levy + 0.05)\n\n        # finalize results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AHDL scored 0.274 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1666603727174818, 0.2272799572551304, 0.3052815330715498, 0.2555024681161866, 0.4079364464841926, 0.23462036721276847, 0.27892210946219176, 0.3213964665579473, 0.3434229506960744, 0.19489757352678727]}, "task_prompt": ""}
{"id": "9752de64-3cc3-4d54-aa81-1a9918e06058", "fitness": 0.32211007732476016, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps, and a trust-region local search with online parameter adaptation for robust global-to-local search.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent successes.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size heuristic (kept modest for budget-limited runs)\n        if pop_size is None:\n            ps = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            ps = min(ps, max(4, self.budget // 20))\n            self.pop_size = int(ps)\n        else:\n            self.pop_size = int(pop_size)\n\n        # best found values (to be set in __call__)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # read bounds, ensure arrays of length dim\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        # ensure reasonable range (many BBOB problems have [-5,5])\n        range_vec = ub - lb\n        max_trust = max(np.linalg.norm(range_vec), 1e-12)\n        trust_radius = 0.2 * max_trust\n        min_trust = 1e-8\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population until budget or full population\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if nothing was evaluated (budget == 0), return defaults\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # determine current best among evaluated individuals\n        valid = np.isfinite(fvals)\n        best_idx = np.argmin(fvals[valid])\n        best_indices = np.where(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6         # mean differential weight\n        CR_mean = 0.9        # mean crossover probability\n        p_levy = 0.08        # probability of a Lévy jump\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (approx.)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: process until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # Each generation: sequentially attempt to improve each individual\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # prepare index pool for DE mutation\n                idxs = np.arange(self.pop_size)\n                idxs = idxs[idxs != i]\n                # occasional Lévy jump centered on best for exploration\n                if rng.rand() < p_levy:\n                    step = levy_step()\n                    # scale the step: relative to trust radius and coordinate-wise range\n                    step_scale = rng.uniform(0.2, 1.2)\n                    norm_step = np.linalg.norm(step) + 1e-12\n                    # candidate around best_x with heavy-tailed displacement\n                    candidate = best_x + (step_scale * (trust_radius / max_trust)) * (step / norm_step) * range_vec\n                else:\n                    # DE/rand/1-like mutation\n                    if idxs.size < 3:\n                        # not enough individuals for DE, fallback to random perturbation\n                        candidate = pop[i] + rng.normal(0, 0.1, size=self.dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[rng.randint(0, self.dim)] = True\n                        candidate = np.where(cr_mask, donor, pop[i])\n\n                # project to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n\n                    # adapt means if DE parameters were used\n                    if 'Fi' in locals():\n                        # move means slightly toward successful Fi/CRi\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                        # remove temporary names to avoid accidental reuse\n                        del Fi\n                        del CRi\n                    else:\n                        # Levy success -> slightly reduce p_levy and nudge CR_mean\n                        p_levy = max(0.01, p_levy * 0.95)\n                        CR_mean = 0.95 * CR_mean + 0.05 * 0.5\n\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # Trust-region local search around current best (a few samples)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample a small number of local candidates: scale with dim but limited by remaining budget\n            local_samples = int(min(max(1, self.dim // 3), remaining, 6))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dimension relative to trust_radius and problem range\n                sigma = (0.2 + rng.rand(self.dim) * 0.8) * (trust_radius / max_trust)\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful contraction to focus search\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful -> slight expansion to escape shallow traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # Adapt population-level parameters based on successes this generation\n            if successes > 0:\n                # reward exploitation: slightly contract F_mean/CR_mean toward conservative values\n                F_mean = float(np.clip(0.95 * F_mean + 0.05 * 0.55, 0.05, 0.99))\n                CR_mean = float(np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0))\n                # slightly reduce probability of long jumps when making progress\n                p_levy = max(0.01, p_levy * 0.98)\n            else:\n                # stagnation-handling: increase chance of Lévy jumps and diversify F_mean\n                p_levy = min(0.6, p_levy * 1.06 + 0.005)\n                F_mean = float(np.clip(F_mean * 1.02, 0.05, 0.99))\n                # enlarge trust radius modestly to encourage exploration if stagnant\n                if stagnation_counter > int(max(5, self.dim * 0.5)):\n                    trust_radius = min(max_trust, trust_radius * 1.2)\n\n            # strong stagnation reset: re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                chosen = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in chosen:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # after reset, enlarge trust region to escape previous basin\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # slightly increase exploration probability\n                p_levy = min(0.7, p_levy * 1.2)\n\n        # final bookkeeping\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.322 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15436064897617396, 0.17401652547568225, 0.38957121652944626, 0.43590576811722614, 0.3582945797834357, 0.4886977921396962, 0.2803400625142347, 0.29419829106930584, 0.3261112848005502, 0.3196046038418503]}, "task_prompt": ""}
{"id": "cd5ea2da-1ff7-41ff-bb66-89b1dfaa7df6", "fitness": 0.3243711486723837, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range escapes, and trust-region local searches with online step-size adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage: instantiate with budget and dim and call with a black-box\n    function object `func` that exposes `.bounds.lb` and `.bounds.ub`\n    (scalars or arrays). The search respects the budget of function\n    evaluations exactly (never exceeds it).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population size scaled with dimension but bounded\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # don't allow population to be larger than budget\n            self.pop_size = min(self.pop_size, max(2, self.budget // 5))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.default_rng(seed)\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # attempt broadcasting to dimension\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # get bounds (func.bounds.lb/ub may be scalars or arrays)\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            # Fallback to [-5,5] if bounds not provided\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure valid ranges\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        # initialize population uniformly inside bounds\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate as much of initial population as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # determine current best among evaluated individuals\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            valid_idx = np.where(valid)[0]\n            best_idx_rel = np.argmin(fvals[valid_idx])\n            best_idx = valid_idx[best_idx_rel]\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n            self.x_opt = best_x.copy()\n            self.f_opt = best_f\n        else:\n            # no evaluations possible (budget==0)\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump per trial\n        trust_radius = 0.2 * range_norm  # trust radius in absolute terms\n        max_trust = 2.0 * range_norm\n        min_trust = 1e-6\n\n        stagnation_counter = 0\n        gen = 0\n        max_stagnation = max(20, int(0.02 * self.budget))\n        # main loop: generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # per-generation small jitter to means for diversity\n            F_mean = np.clip(F_mean * (1 + 0.01 * (self.rng.random() - 0.5)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean * (1 + 0.02 * (self.rng.random() - 0.5)), 0.05, 0.999)\n\n            # shuffle evaluation order to avoid bias\n            order = np.arange(self.pop_size)\n            self.rng.shuffle(order)\n\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                i = int(idx)\n                # adapt individual parameters jDE-like\n                Fi = np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.0)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                # decide if we do a Levy jump (exploration) or DE mutation (exploitation)\n                if self.rng.random() < p_levy:\n                    # Levy-like heavy-tailed step: use Cauchy (alpha-stable approx)\n                    # center on best_x to bias exploration toward promising regions\n                    # step magnitude scales with trust_radius and global range\n                    # limit extremes to avoid overflow\n                    step = self.rng.standard_cauchy(self.dim)\n                    step = np.clip(step, -1e2, 1e2)\n                    # scale step so typical magnitude is proportional to trust_radius\n                    step_scale = (self.rng.random() * 1.5 + 0.5) * (trust_radius / max(range_norm, 1e-12))\n                    donor = best_x + step * step_scale * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    # less aggressive crossover: mix with target to keep diversity\n                    cr_mask = self.rng.random(self.dim) < (0.5 * CRi)\n                    if not np.any(cr_mask):\n                        cr_mask[self.rng.integers(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    donor = trial\n                else:\n                    # DE/rand/1/bin style\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback random perturbation\n                        donor = pop[i] + Fi * self.rng.normal(0, 1.0, size=self.dim) * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    else:\n                        r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = self.rng.random(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[self.rng.integers(0, self.dim)] = True\n                        trial = np.where(cr_mask, donor, pop[i])\n                        donor = trial\n\n                # ensure bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # nudge parameter means towards successful Fi/CRi\n                    F_mean = 0.92 * F_mean + 0.08 * Fi\n                    CR_mean = 0.92 * CR_mean + 0.08 * CRi\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.x_opt = best_x.copy()\n                    self.f_opt = best_f\n                    stagnation_counter = 0\n                    # slightly tighten trust region on success\n                    trust_radius = max(min_trust, 0.9 * trust_radius)\n                else:\n                    stagnation_counter += 1\n\n                # early termination if optimum reached (optional small tolerance)\n                # many benchmarks use target ~1e-8 etc; we don't hardcode a target here.\n\n            # after processing population: do trust-region local sampling (focused exploitation)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples depends on remaining budget and dim\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: per-dim factor around trust_radius\n                sigma = (0.5 + self.rng.random(self.dim) * 0.5) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + self.rng.normal(0, 1.0, size=self.dim) * sigma * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.x_opt = best_x.copy()\n                    self.f_opt = best_f\n                    # successful local step: shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.75)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful: increase a bit to escape local minima\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n                    stagnation_counter += 1\n\n            # adapt p_levy: reduce if many successes else increase if stagnating\n            if successes > max(1, int(0.15 * self.pop_size)):\n                p_levy = max(0.01, p_levy * 0.96)\n                # encourage a slight move of means toward exploitation baseline\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.05, 0.999)\n            else:\n                # if stagnating, raise exploration rate a bit\n                p_levy = min(0.5, p_levy * 1.04 + 0.005)\n\n            # stagnation handling: perturb population if no improvement for a while\n            if stagnation_counter > max_stagnation:\n                # reinitialize half of the population (but do not exceed budget)\n                n_reinit = max(1, self.pop_size // 2)\n                for j in range(n_reinit):\n                    if evals >= self.budget:\n                        break\n                    idx = self.rng.integers(0, self.pop_size)\n                    pop[idx] = lb + self.rng.random(self.dim) * range_vec\n                    # evaluate the new individual\n                    f_new = float(func(pop[idx]))\n                    evals += 1\n                    fvals[idx] = f_new\n                    if f_new < best_f:\n                        best_f = f_new\n                        best_x = pop[idx].copy()\n                        self.x_opt = best_x.copy()\n                        self.f_opt = best_f\n                # enlarge trust radius after reset to encourage exploring new areas\n                trust_radius = min(max_trust, max(trust_radius * 1.5, range_norm * 0.05))\n                stagnation_counter = 0\n                # slightly boost exploration probability\n                p_levy = min(0.5, p_levy * 1.15 + 0.02)\n\n        # final bookkeeping\n        # self.f_opt and self.x_opt already updated during search\n        # ensure types\n        if self.x_opt is not None:\n            self.x_opt = np.asarray(self.x_opt, dtype=float)\n            self.f_opt = float(self.f_opt)\n        else:\n            self.f_opt = float(self.f_opt)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.324 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2296013980760344, 0.20485028977670994, 0.3909969699887986, 0.37433911666245445, 0.30403638580799697, 0.4308047788357595, 0.26083088962156387, 0.49104196173862136, 0.26787975926205077, 0.289329936953847]}, "task_prompt": ""}
{"id": "0e90ab91-a253-4c2d-a4b8-8d905a4c273f", "fitness": 0.31672328252918835, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy (Cauchy) long jumps, and a trust-region local search with online step-size adaptation and population reseeding to balance global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (F, CR, p_levy, trust radius) are adapted online based on success history.\n    Designed for box-bounded continuous optimization.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size: scale with dim but limited by budget\n        if pop_size is None:\n            self.pop_size = int(np.clip(6 * self.dim, 4, max(4, min(40, self.budget // 5))))\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # bounds from func (Many BBOB has -5..5 but accept func.bounds)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # protect against degenerate bounds\n        range_vec = np.where(range_vec <= 0, 1.0, range_vec)\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population while respecting budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if we didn't evaluate any candidate, return\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            self.f_opt = float(np.inf)\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy (Cauchy) jump per trial\n        step_scale = 0.25   # base scale for Lévy and trust radius (fraction of range)\n        min_trust = 1e-6\n        max_trust = 2.0\n\n        # trust region initial radius (fraction of max range)\n        trust_radius = step_scale * np.max(range_vec)\n        trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n        stagnation_counter = 0\n        gen = 0\n\n        def levy_step_vector(scale):\n            \"\"\"Generate a heavy-tailed Cauchy-like vector (per-dim) scaled by 'scale'.\"\"\"\n            # Standard Cauchy via tan(pi*(u-0.5))\n            u = self.rng.rand(self.dim)\n            cauchy = np.tan(np.pi * (u - 0.5))\n            # limit extremes to avoid blow-up\n            cauchy = np.clip(cauchy, -1e3, 1e3)\n            return scale * cauchy\n\n        # Main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # generation: iterate over population\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Adapt Fi and CRi per individual (jDE-like sampling around means)\n                Fi = float(np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # choose between Lévy jump (exploration) and DE mutation (exploitation)\n                if self.rng.rand() < p_levy:\n                    # Lévy/Cauchy jump centered at best, scaled by trust_radius and global range\n                    scale = trust_radius * (0.5 + self.rng.rand() * 1.5)  # vary a bit\n                    step = levy_step_vector(scale / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                    donor = best_x + step * range_vec  # directional scaling per-dim\n                    # add a small random perturbation to avoid staying identical\n                    donor += 1e-6 * (self.rng.rand(self.dim) - 0.5)\n                else:\n                    # DE/rand/1 mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover to create trial\n                # Ensure at least one dimension from donor\n                cr_mask = (self.rng.rand(self.dim) < CRi)\n                if not np.any(cr_mask):\n                    cr_mask[self.rng.randint(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # project to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if there is budget\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                replaced = False\n                if not np.isfinite(fvals[i]) or f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    replaced = True\n                    # nudge global parameter means toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # on success shrink trust radius a bit to focus local search\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    # only count as stagnation if no improvement from any trial recently\n                    stagnation_counter += 0 if replaced else 1\n\n                # if many evaluations consumed, maybe break\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            # adapt probability of Lévy jumps based on successes\n            if successes > 0:\n                # slightly reduce exploration probability if many successes\n                if successes > max(1, int(0.2 * self.pop_size)):\n                    p_levy = max(0.01, p_levy * 0.95)\n                else:\n                    p_levy = max(0.01, p_levy * 0.98)\n                # slowly return F_mean/CR_mean toward nominal exploitation values\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # increase exploration probability under stagnation\n                p_levy = min(0.5, p_levy * 1.08 + 0.005)\n                # widen trust radius a bit to help escape\n                trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # Trust-region local search if stagnating for a while\n            if stagnation_counter > max(8, self.dim):\n                # perform a few local Gaussian samples around best\n                local_samples = min(max(2, self.dim // 2), max(1, self.budget - evals))\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic gaussian noise scaled by trust radius and relative dimension scales\n                    sigma = (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                    perturb = self.rng.normal(0, sigma, size=self.dim) * range_vec\n                    candidate = np.clip(best_x + perturb, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local search -> shrink trust radius to focus more\n                        trust_radius = max(min_trust, trust_radius * 0.75)\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful -> slightly expand trust radius to try different scales\n                        trust_radius = min(max_trust, trust_radius * 1.12)\n\n            # Strong stagnation handling: re-seed part of population\n            if stagnation_counter > max(40, 6 * self.dim) and evals < self.budget:\n                # reinitialize a subset of population (diversify)\n                k = max(1, self.pop_size // 2)\n                to_replace = self.rng.choice(self.pop_size, k, replace=False)\n                for j in to_replace:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # after reseeding, enlarge trust radius a bit to encourage broader search\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0  # give a new chance\n\n            # update global tracking\n            # adapt minimal safe trust radius\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.317 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13821686864811433, 0.20684660752662898, 0.43152311141383126, 0.3446807693923464, 0.23948626342410484, 0.7155470184877188, 0.2556999464024391, 0.2914103343016423, 0.3241298891310651, 0.21969201656399262]}, "task_prompt": ""}
{"id": "7a3b9627-ea6a-429d-8606-4792b92a26fc", "fitness": 0.23644720883029127, "name": "HybridDeLevyTrust", "description": "HybridDeLevyTrust — an adaptive hybrid: population-based Differential Evolution with jDE-like online control, occasional Lévy-flight long jumps for global exploration, and a trust-region focused local search around the current best; parameters and jump probability adapt to stagnation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success/stagnation history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # choose a reasonable population size relative to dimension and budget\n        if pop_size is None:\n            # keep it compact if budget is small\n            self.pop_size = int(min(max(8, 4 * self.dim), max(8, self.budget // 10)))\n        else:\n            self.pop_size = max(4, int(pop_size))\n        # result placeholders\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.ones(self.dim, dtype=float) * float(b)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast to dim\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds must be scalar or an array of length dim\")\n\n    def _levy_step(self):\n        # simple heavy-tailed step using Cauchy; clip extreme outliers\n        s = self.rng.standard_cauchy(self.dim)\n        s = np.clip(s, -1e2, 1e2)\n        return s\n\n    def __call__(self, func):\n        # get bounds and ensure arrays\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # typical ManyBBOB uses bounds [-5,5] but use provided bounds\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population until budget exhausted or pop fully evaluated\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # if no evaluations possible, return\n        if np.all(np.isinf(fvals)):\n            self.x_opt = None\n            self.f_opt = np.inf\n            return self.f_opt, self.x_opt\n\n        # get current best\n        best_idx = int(np.nanargmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adaptive)\n        F_mean = 0.6         # initial scale factor mean\n        CR_mean = 0.9        # initial crossover probability mean\n        p_levy = 0.08        # initial probability of a Lévy jump\n        trust_radius = 0.2 * range_norm\n        max_trust = 2.0 * range_norm\n        min_trust = 1e-8 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        # main loop until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # per-generation adjustments (slight jitter)\n            # number of individuals to process this generation: all\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi around current means (jDE-like)\n                # Use Cauchy for Fi to keep heavy tails; normal for CR\n                Fi = F_mean + 0.1 * self.rng.standard_cauchy()\n                Fi = float(np.clip(Fi, 0.05, 1.0))\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.15), 0.0, 1.0))\n\n                # decide exploration mode: Levy jump centered on best OR DE mutation\n                if self.rng.rand() < p_levy:\n                    # Lévy long jump exploration\n                    step = self._levy_step()\n                    # scale step by trust_radius relative to global range\n                    scale = (trust_radius / (range_norm + 1e-12))\n                    candidate = best_x + step * scale * (range_vec * 0.5)\n                    candidate = np.clip(candidate, lb, ub)\n                else:\n                    # Differential Evolution mutation (rand/1 + slight current-to-best bias)\n                    # choose distinct indices different from i from evaluated subset if possible\n                    valid_idx = np.where(np.isfinite(fvals))[0]\n                    choose_pool = valid_idx[valid_idx != i]\n                    if choose_pool.size >= 3:\n                        r = self.rng.choice(choose_pool, 3, replace=False)\n                    else:\n                        # fallback to any indices excluding i (allowing possibly inf fvals)\n                        pool = np.delete(np.arange(self.pop_size), i)\n                        r = self.rng.choice(pool, 3, replace=False)\n                    r1, r2, r3 = int(r[0]), int(r[1]), int(r[2])\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # bias donor toward best a little (current-to-best influence)\n                    donor = donor + 0.3 * Fi * (best_x - pop[i])\n                    # binomial crossover\n                    cr_mask = self.rng.rand(self.dim) < CRi\n                    # ensure at least one dimension is from donor\n                    cr_mask[self.rng.randint(0, self.dim)] = True\n                    candidate = np.where(cr_mask, donor, pop[i])\n                    candidate = np.clip(candidate, lb, ub)\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # greedy selection: replace if improved\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt means gently toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # if candidate failed, slightly decay means to encourage exploration\n                    F_mean = np.clip(0.995 * F_mean, 0.05, 1.0)\n                    CR_mean = np.clip(0.995 * CR_mean, 0.0, 1.0)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # very small maintenance: keep param means in bounds\n                F_mean = float(np.clip(F_mean, 0.05, 0.99))\n                CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n            # End of generation adjustments\n\n            # trust-region local search around best: sample a small handful proportional to dim\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample count: a few samples, but don't exceed remaining budget\n            local_samples = int(min(max(1, self.dim // 2), remaining, 2 * self.dim))\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dimension: fraction of trust radius scaled to range\n                sigma = (0.3 + self.rng.rand(self.dim) * 0.7) * (trust_radius / (range_norm + 1e-12))\n                candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    stagnation_counter = 0\n                    # successful local -> shrink trust radius to focus search\n                    trust_radius *= 0.7\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                else:\n                    # unsuccessful local step -> slight expansion to try escaping\n                    trust_radius *= 1.05\n                    trust_radius = min(trust_radius, max_trust)\n\n            # adapt p_levy based on stagnation: increase when stagnating, reduce on success\n            if stagnation_counter > max(20, self.dim * 3):\n                p_levy = min(0.5, p_levy * 1.15)\n            else:\n                p_levy = max(0.01, p_levy * (0.995 if successes > 0 or local_success > 0 else 1.005))\n\n            # encourage exploration if few successes, encourage exploitation if many successes\n            if successes + local_success > max(1, self.pop_size * 0.15):\n                # good progress -> tighten trust region slightly and reduce levy chance\n                trust_radius = max(min_trust, trust_radius * 0.95)\n                p_levy = max(0.005, p_levy * 0.95)\n            else:\n                # poor progress -> open up exploration\n                trust_radius = min(max_trust, trust_radius * 1.02)\n\n            # strong stagnation reset: re-seed a portion of population\n            if stagnation_counter > max(50, self.dim * 6):\n                k = max(1, self.pop_size // 2)\n                # choose indices to replace that are not the best index\n                replace_candidates = np.arange(self.pop_size)\n                # prefer to keep best individual\n                replace_candidates = replace_candidates[replace_candidates != best_idx]\n                to_replace = self.rng.choice(replace_candidates, size=min(k, replace_candidates.size), replace=False)\n                for j in to_replace:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # immediate improvement handling\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # after reset, enlarge trust radius a bit to explore new regions\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # slightly increase levy probability to jump more\n                p_levy = min(0.5, p_levy * 1.2)\n                # reset stagnation counter\n                stagnation_counter = 0\n\n            # keep track of final best in arrays\n            self.x_opt = best_x.copy()\n            self.f_opt = best_f\n\n        # End while budget\n\n        # final assignment and return\n        self.x_opt = best_x.copy()\n        self.f_opt = float(best_f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.236 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10759296652872863, 0.15216509919248045, 0.26706893376522234, 0.24042557739647907, 0.2052585626258745, 0.21943308559297958, 0.21845238764887576, 0.6415071421884364, 0.1782840332351493, 0.13428430012868664]}, "task_prompt": ""}
{"id": "cd208f44-a170-4433-aa0f-ea698a7f5ad8", "fitness": 0.19214035855968725, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and a trust-region Gaussian local search with online step-size and operator-adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Online adaptation\n    of DE parameters (F, CR) and Lévy probability is performed based on\n    recent successes. Works in the fixed box [-5, 5]^dim.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        # bounds for Many Affine BBOB as specified\n        self.lb = -5.0\n        self.ub = 5.0\n        self.range = self.ub - self.lb\n\n        # sensible default population size scaled to dimension (but not exceeding budget)\n        if pop_size is None:\n            ps = int(min(max(8, 4 * self.dim), 60))\n        else:\n            ps = int(pop_size)\n        # cannot have population larger than budget (at least 1)\n        ps = max(1, min(ps, max(1, self.budget)))\n        self.pop_size = ps\n\n        # placeholders for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, x):\n        x = np.asarray(x, dtype=float)\n        if x.shape != (self.dim,):\n            x = np.broadcast_to(x, (self.dim,)).astype(float)\n        return x\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # initialize population uniformly in bounds\n        pop = rng.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n        # bookkeeping\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # evaluate as many as allowed by budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # if nothing could be evaluated (budget == 0)\n        if evals == 0:\n            # return trivial defaults\n            self.f_opt = np.inf\n            self.x_opt = self._ensure_array_bounds(pop[0])\n            return self.f_opt, self.x_opt\n\n        # set current best\n        valid_idx = np.argmin(fvals[:min(self.pop_size, evals)])\n        best_idx = int(valid_idx)\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.3       # crossover prob mean\n        p_levy = 0.06       # initial probability of Lévy jumps\n        trust_radius = 0.2 * self.range  # absolute trust radius (in variable space)\n        min_trust = 1e-6\n\n        stagnation_counter = 0\n        success_history = []\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step(scale=1.0):\n            # Cauchy approximates heavy tails; cap extreme outliers to avoid blow-up\n            step = rng.standard_cauchy(self.dim)\n            # cap to avoid infinite or enormous steps\n            step = np.clip(step, -10.0, 10.0)\n            return step * float(scale)\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            # per-generation operations\n            # shuffle order to avoid bias\n            order = rng.permutation(self.pop_size)\n\n            for ii in order:\n                if evals >= self.budget:\n                    break\n\n                # decide branch: Lévy jump centered on best vs DE mutation\n                if rng.rand() < p_levy:\n                    # Lévy jump: center on best with heavy-tailed perturbation\n                    lev = levy_step()\n                    # scale relative to current trust radius and problem range\n                    candidate = best_x + (lev / (1.0 + np.linalg.norm(lev))) * trust_radius\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1/bin mutation with per-individual Fi and CRi (jDE-like)\n                    idxs = np.arange(self.pop_size)\n                    # ensure r1,r2,r3 different from ii\n                    others = np.delete(idxs, ii)\n                    if others.size < 3:\n                        r = rng.choice(idxs, 3, replace=True)\n                    else:\n                        r = rng.choice(others, 3, replace=False)\n                    r1, r2, r3 = r[0], r[1], r[2]\n\n                    # adapt F and CR per individual around the running means\n                    Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    # ensure at least one gene from donor\n                    jrand = rng.randint(self.dim)\n                    cr_mask[jrand] = True\n                    candidate = np.where(cr_mask, donor, pop[ii])\n\n                # projection to bounds\n                candidate = np.clip(candidate, self.lb, self.ub)\n\n                # one evaluation if budget remains\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population (target-to-trial)\n                if f_candidate < fvals[ii]:\n                    pop[ii] = candidate.copy()\n                    fvals[ii] = f_candidate\n                    # adapt means slightly toward successful parameters (simple exponential moving average)\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                    # improvement relative to global best?\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                        success_history.append(1)\n                    else:\n                        # successful local improvement but not global\n                        success_history.append(1)\n                else:\n                    success_history.append(0)\n                    # small nudge for exploration if DE failed\n                    if Fi is not None:\n                        F_mean = np.clip(F_mean * 1.002, 0.05, 0.99)\n\n                # maintain short success history\n                if len(success_history) > 50:\n                    success_history.pop(0)\n\n                # adaptive adjustments inside generation\n                # if many recent successes, reduce p_levy; if stagnating increase it\n                recent_success_rate = (sum(success_history[-20:]) / max(1, min(20, len(success_history))))\n                if recent_success_rate > 0.25:\n                    p_levy = max(0.01, p_levy * 0.9)\n                elif recent_success_rate < 0.05:\n                    p_levy = min(0.5, p_levy * 1.15)\n\n                # update global best info kept already\n\n            # End of generation: do trust-region local search around best if budget remains\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: keep it modest and dependent on dim and remaining budget\n            local_samples = max(1, min(3 + self.dim // 4, remaining))\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic Gaussian noise scaled by trust_radius\n                sigma = trust_radius * (0.5 + rng.rand(self.dim) * 0.5) / max(1.0, np.sqrt(self.dim))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma\n                candidate = np.clip(candidate, self.lb, self.ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # if local success, tighten trust region to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    # if no improvement, relax trust region a bit to explore wider\n                    trust_radius = min(self.range, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n                # optionally replace worst in population to inject good local solutions\n                worst_idx = int(np.argmax(fvals))\n                if f_candidate < fvals[worst_idx]:\n                    pop[worst_idx] = candidate.copy()\n                    fvals[worst_idx] = f_candidate\n\n            # adjust global adaptation rates based on local outcomes\n            if local_success > 0:\n                # encourage exploitation (slightly lower F, narrower CR)\n                F_mean = np.clip(F_mean * 0.98, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n                stagnation_counter = 0\n            else:\n                # encourage exploration if no local success\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 1.01, 0.0, 1.0)\n                stagnation_counter += 1\n\n            # strong stagnation reset: re-seed part of the population if stuck for long\n            if stagnation_counter > max(10, 3 + self.dim // 2) and evals < self.budget:\n                k = max(1, self.pop_size // 2)\n                idxs = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = rng.uniform(self.lb, self.ub, size=self.dim)\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # update best if any new point is better\n                    if fvals[j] < best_f:\n                        best_f = float(fvals[j])\n                        best_x = pop[j].copy()\n                # after reseed, encourage exploration a bit\n                trust_radius = min(self.range, trust_radius * 1.5)\n                p_levy = min(0.5, p_levy * 1.3)\n                stagnation_counter = 0\n                success_history = []\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = self._ensure_array_bounds(best_x)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.192 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09957496641574615, 0.16171878187495214, 0.267334598388303, 0.20803426016773663, 0.20986338109072855, 0.2298334335723985, 0.21233168222816579, 0.2077393295267188, 0.1823642965234934, 0.14260885580862948]}, "task_prompt": ""}
{"id": "b1f888c6-8dd1-42dc-81d1-269bca73a52c", "fitness": 0.3182497690076894, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and a trust-region local search with online step-size adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # population size heuristic: scale with dimension but keep reasonable relative to budget\n        if pop_size is None:\n            # At least 6, at most budget//3, typical 4*dim\n            self.pop_size = int(min(max(6, 4 * self.dim), max(6, self.budget // 3)))\n        else:\n            self.pop_size = int(pop_size)\n\n        # results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        raise ValueError(\"Bounds must be scalar or length dim.\")\n\n    def _levy_step(self):\n        # simple heavy-tailed step using Cauchy (a 1D-stable distribution)\n        s = self.rng.standard_cauchy(self.dim)\n        # clip extreme outliers to avoid numerical blow-up\n        return np.clip(s, -1e2, 1e2)\n\n    def __call__(self, func):\n        # bounds from the function object (BBOB style)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population but respect budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # If we did not manage to evaluate any individual (tiny budget), return best seen (maybe none)\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # set initial best from evaluated individuals\n        best_idx = np.argmin(fvals)\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6      # mutation scale mean (jDE-like adaptation)\n        CR_mean = 0.2     # crossover rate mean\n        p_levy = 0.05     # base probability to attempt a Levy jump\n        trust_radius = 0.2 * range_norm  # initial trust region radius (absolute scale)\n        min_trust = 1e-8\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation small randomization around means\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample individual control parameters (jDE-like)\n                Fi = np.clip(self.rng.normal(F_mean, 0.15), 0.05, 1.0)\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.15), 0.0, 1.0))\n\n                # choose exploration type: Levy-centered on best or DE mutation\n                if self.rng.rand() < p_levy:\n                    # Levy jump around current best (long-range)\n                    step = self._levy_step()\n                    # Scale relative to problem range and trust radius\n                    step_scale = (0.5 + 0.5 * self.rng.rand())\n                    donor = best_x + step_scale * (trust_radius / np.maximum(range_norm, 1e-12)) * step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                else:\n                    # DE/rand/1-like mutation (ensure distinct indices)\n                    idxs = list(range(self.pop_size))\n                    idxs.remove(i)\n                    a, b, c = pop[self.rng.choice(idxs, 3, replace=False)]\n                    donor = a + Fi * (b - c)\n\n                # binomial crossover\n                jrand = self.rng.randint(self.dim)\n                mask = self.rng.rand(self.dim) < CRi\n                mask[jrand] = True\n                trial = np.where(mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate candidate (one evaluation)\n                f_cand = float(func(candidate))\n                evals += 1\n\n                # replacement: greedy\n                if f_cand < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_cand\n                    successes += 1\n                    # adapt means slightly toward successful parameters\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # if it improved global best, update\n                    if f_cand < best_f:\n                        best_f = f_cand\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # early exit if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation: trust-region local search around current best\n            # spend a small fraction of remaining budget on focused local samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # number of local evaluations: scale with dim but limited by remaining\n            local_n = min(remaining, max(1, self.dim // 2))\n            improved_local = False\n            for _ in range(local_n):\n                # anisotropic sigma: dimension-wise scaling smaller with trust radius\n                sigma_vec = (0.2 + self.rng.rand(self.dim) * 0.8) * (trust_radius / np.maximum(range_norm, 1e-12)) * (range_vec / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                cand = best_x + self.rng.randn(self.dim) * sigma_vec\n                cand = np.minimum(np.maximum(cand, lb), ub)\n                f_cand = float(func(cand))\n                evals += 1\n                if f_cand < best_f:\n                    best_f = f_cand\n                    best_x = cand.copy()\n                    improved_local = True\n                    stagnation_counter = 0\n                if evals >= self.budget:\n                    break\n\n            # adapt trust region based on local improvement\n            if improved_local:\n                # successful local search -> focus (shrink)\n                trust_radius = max(trust_radius * 0.8, min_trust)\n            else:\n                # unsuccessful -> allow broader moves\n                trust_radius = min(trust_radius * 1.08, max_trust)\n\n            # adaptation of Levy probability from stagnation and recent success\n            if successes == 0:\n                # no success this generation -> increase chance of long jumps progressively\n                p_levy = min(0.5, p_levy * 1.12 + 0.01)\n                # slightly widen DE scale to increase diversity\n                F_mean = min(0.99, F_mean * 1.03)\n                CR_mean = max(0.05, CR_mean * 0.98)\n            else:\n                # successful exploitation -> reduce Levy behavior\n                p_levy = max(0.02, p_levy * 0.9)\n                # encourage exploitation parameters\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.5, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n\n            # stagnation handling: if many evaluations without global improvement, re-seed part of population\n            if stagnation_counter > max(50, 5 * self.dim) and evals < self.budget:\n                # reinitialize half of the population (or as budget allows)\n                k = max(1, self.pop_size // 2)\n                for j in range(k):\n                    idx = (j + gen) % self.pop_size\n                    pop[idx] = lb + self.rng.rand(self.dim) * range_vec\n                    # Evaluate new individual if budget allows\n                    if evals < self.budget:\n                        fvals[idx] = float(func(pop[idx]))\n                        evals += 1\n                        if fvals[idx] < best_f:\n                            best_f = fvals[idx]\n                            best_x = pop[idx].copy()\n                            stagnation_counter = 0\n                    else:\n                        fvals[idx] = np.inf\n                # slightly enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.2)\n                # reduce Levy jump aggressiveness a bit to avoid too many huge jumps after reset\n                p_levy = max(0.02, p_levy * 0.9)\n                stagnation_counter = 0\n\n        # final results\n        self.x_opt = best_x.copy()\n        self.f_opt = float(best_f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.318 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14516979697662125, 0.23722292094534614, 0.47046249218778, 0.4216866743369353, 0.3074035120685904, 0.31207195495822027, 0.29563408693002813, 0.3744353764197652, 0.42772032315185093, 0.19069055210175645]}, "task_prompt": ""}
{"id": "2b415e2f-5967-445a-8207-c8ae9aceb6ce", "fitness": 0.32198525227367314, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for global exploration, and an online-adaptive trust-region local search for focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # suggested default pop size: moderate but bounded by budget\n        suggested = 10 + 2 * self.dim\n        if pop_size is None:\n            pop_size = suggested\n        self.pop_size = int(min(pop_size, max(4, max(4, self.budget // 20))))\n        # results placeholders\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, bound):\n        \"\"\"\n        Convert a scalar or array-like bound to a numpy array of length dim.\n        If bound is None, default to [-5,5] problem-wide assumption.\n        \"\"\"\n        if bound is None:\n            return np.full(self.dim, 5.0)\n        arr = np.asarray(bound, dtype=float)\n        if arr.ndim == 0:\n            return np.full(self.dim, float(arr))\n        if arr.size == self.dim:\n            return arr.copy()\n        # if provided as single value in a 1-length array\n        if arr.size == 1:\n            return np.full(self.dim, float(arr.ravel()[0]))\n        # try broadcast\n        try:\n            return np.broadcast_to(arr, (self.dim,)).astype(float).copy()\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to problem dimension.\")\n\n    def __call__(self, func):\n        # quick sanity\n        if self.budget <= 0:\n            return np.inf, None\n\n        lb = self._ensure_array_bounds(getattr(func.bounds, \"lb\", None))\n        ub = self._ensure_array_bounds(getattr(func.bounds, \"ub\", None))\n        # fallback to canonical -5..5 if bounds are degenerate\n        lb = np.where(np.isfinite(lb), lb, -5.0)\n        ub = np.where(np.isfinite(ub), ub, 5.0)\n\n        # dimension might be provided by func.bounds length; otherwise use self.dim\n        # ensure shapes\n        self.dim = int(self.dim)\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # evaluate as many as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = func(x)\n            evals += 1\n            pop_f[i] = float(f)\n\n        # if we didn't evaluate anyone (tiny budget), we must at least evaluate one random candidate\n        if evals == 0:\n            x0 = lb + rng.random(self.dim) * range_vec\n            f0 = func(x0)\n            evals += 1\n            self.f_opt = float(f0)\n            self.x_opt = x0.copy()\n            return self.f_opt, self.x_opt\n\n        # set best from evaluated\n        best_idx = int(np.argmin(pop_f))\n        best_f = float(pop_f[best_idx])\n        best_x = pop[best_idx].copy()\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.5       # crossover mean\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        p_levy = 0.08       # initial probability for a Lévy long jump\n        trust_radius = 0.25 # relative to range_vec\n        stagnation_counter = 0\n\n        # success memory for adaptation\n        success_F = []\n        success_CR = []\n        success_count = 0\n\n        # Levy-like heavy-tail step (simple truncated Cauchy)\n        def levy_step():\n            # standard Cauchy for heavy tail; truncate extreme outliers\n            step = rng.standard_cauchy(self.dim)\n            # cap extremes to avoid overflow\n            step = np.clip(step, -10.0, 10.0)\n            # scale with a small randomness factor\n            scale = step_scale * (0.5 + rng.random())  # between 0.5 and 1.5 of step_scale\n            return step * scale\n\n        # helper: ensure at least one index different\n        def distinct_indices(exclude_idx, k):\n            idxs = np.arange(self.pop_size)\n            idxs = idxs[idxs != exclude_idx]\n            rng.shuffle(idxs)\n            return idxs[:k]\n\n        # main loop: iterate until budget exhausted\n        # We'll sequentially process population members, each candidate consumes one eval.\n        generation = 0\n        while evals < self.budget:\n            generation += 1\n            # per-generation small jitter to adaptation and probabilities\n            CR_mean = np.clip(CR_mean, 0.05, 0.95)\n            F_mean = np.clip(F_mean, 0.05, 0.95)\n            # ensure a small chance to globally jump if stagnating\n            levy_prob = min(0.5, p_levy * (1.0 + 0.01 * stagnation_counter))\n\n            # iterate through population\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                target = pop[i]\n                target_f = pop_f[i]\n\n                # decide branch: Levy global jump vs DE mutation\n                if rng.random() < levy_prob:\n                    # Levy jump centered on best, but offset also from target for diversity\n                    step = levy_step()\n                    # anisotropic scaling by range_vec\n                    candidate = best_x + step * range_vec * (0.5 + rng.random(self.dim))\n                    # small mix with the target to retain diversity\n                    mix = rng.random(self.dim) < 0.2\n                    candidate = np.where(mix, 0.5 * (candidate + target), candidate)\n                    branch = \"levy\"\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE mutation (rand/1 + current-to-best mixing)\n                    # simple jDE-like adaptation: with small prob resample Fi,CRi around means\n                    if rng.random() < 0.1:\n                        Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 0.95)\n                    else:\n                        Fi = np.clip(rng.normal(F_mean, 0.07), 0.05, 0.95)\n                    CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                    r1, r2, r3 = distinct_indices(i, 3)\n                    v_rand = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # incorporate a 'toward-best' bias with small weight to speed convergence\n                    toward_best = 0.15 * (best_x - target)\n                    donor = v_rand + toward_best\n                    # scale mutation by trust radius to localize exploration when trust is small\n                    donor = target + trust_radius * (donor - target)\n                    # binomial crossover\n                    cr_mask = rng.random(self.dim) < CRi\n                    # ensure at least one dimension is from donor\n                    cr_mask[rng.integers(0, self.dim)] = True\n                    candidate = np.where(cr_mask, donor, target)\n                    branch = \"de\"\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_cand = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                replaced = False\n                if f_cand < target_f:\n                    pop[i] = candidate\n                    pop_f[i] = f_cand\n                    replaced = True\n                    stagnation_counter = 0\n                    success_count += 1\n                    # record successful Fi/CR for adaptation if applicable\n                    if branch == \"de\" and Fi is not None:\n                        success_F.append(Fi)\n                        success_CR.append(CRi)\n                else:\n                    stagnation_counter += 1\n\n                # update best\n                if f_cand < best_f:\n                    best_f = f_cand\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n                # online adaptation: move means toward successful values (Lehmer-average like)\n                if len(success_F) >= 5:\n                    # Lehmer mean gives more weight to larger Fi when successful (common trick)\n                    sf = np.array(success_F)\n                    F_mean = 0.9 * F_mean + 0.1 * (np.sum(sf**2) / np.sum(sf))\n                    cr = np.array(success_CR)\n                    CR_mean = 0.9 * CR_mean + 0.1 * np.mean(cr)\n                    # clear memory partially (keep recent)\n                    success_F = success_F[-10:]\n                    success_CR = success_CR[-10:]\n\n                # small exploration nudges for levy branch\n                if branch == \"levy\" and replaced:\n                    # successful levy -> reduce probability slightly to encourage exploitation\n                    p_levy = max(0.02, p_levy * 0.95)\n                elif branch == \"levy\" and not replaced:\n                    # unsuccessful leve -> boost chance mildly\n                    p_levy = min(0.5, p_levy * 1.03 + 0.005)\n\n                # if we made many successful replacements recently, slightly reduce trust radius to focus\n                if replaced:\n                    trust_radius = max(0.01, trust_radius * 0.97)\n                else:\n                    # otherwise slowly expand the trust radius to escape local minima\n                    trust_radius = min(1.0, trust_radius * 1.0015)\n\n                # clamp trust radius\n                trust_radius = float(np.clip(trust_radius, 0.01, 1.0))\n                p_levy = float(np.clip(p_levy, 0.01, 0.5))\n\n                # early break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: small handful scaled by dim and remaining budget\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            local_success = 0\n            for _ in range(local_samples):\n                # anisotropic sigma: base trust_radius scaled per-dimension with mild randomness\n                sigma = trust_radius * (0.5 + rng.random(self.dim))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                if evals >= self.budget:\n                    break\n                f_cand = float(func(candidate))\n                evals += 1\n                if f_cand < best_f:\n                    best_f = f_cand\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    local_success += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # adapt trust radius after local batch\n            if local_success > 0:\n                # successful local steps => shrink to focus\n                trust_radius = max(0.005, trust_radius * (0.8 ** local_success))\n                # nudge adaptation to exploit: move CR_mean a bit toward exploitation\n                CR_mean = 0.98 * CR_mean + 0.02 * 0.2\n            else:\n                # expand slightly to escape\n                trust_radius = min(1.0, trust_radius * 1.08)\n                # encourage more Levy jumps if stagnating\n                p_levy = min(0.5, p_levy * 1.06 + 0.005)\n\n            # stagnation handling: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, 10 * self.dim):\n                # reinitialize half of the population except best\n                to_reinit = max(1, self.pop_size // 2)\n                indices = np.arange(self.pop_size)\n                # do not reinitialize the current best if present in population\n                # find an index of best in pop (if any)\n                pop_best_idx = None\n                for ii in range(self.pop_size):\n                    if np.allclose(pop[ii], best_x):\n                        pop_best_idx = ii\n                        break\n                candidates = indices\n                if pop_best_idx is not None:\n                    candidates = candidates[candidates != pop_best_idx]\n                rng.shuffle(candidates)\n                for j in candidates[:to_reinit]:\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    # evaluate newly reinitialized individual if budget allows\n                    if evals < self.budget:\n                        pop_f[j] = float(func(pop[j]))\n                        evals += 1\n                # enlarge trust radius to escape\n                trust_radius = min(1.0, trust_radius * 2.0)\n                # increase Levy chance a bit\n                p_levy = min(0.5, p_levy * 1.2 + 0.02)\n                stagnation_counter = 0\n                # clear success lists lightly\n                success_F = []\n                success_CR = []\n\n            # generation end housekeeping: reduce influence of very old success memory\n            if len(success_F) > 50:\n                success_F = success_F[-50:]\n                success_CR = success_CR[-50:]\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.322 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1956396093295193, 0.28402245182538577, 0.37686907575571194, 0.49713184010839073, 0.3460925009571556, 0.3925658844045349, 0.3363806572942193, 0.28888244950163566, 0.28365882688799116, 0.21860922667218707]}, "task_prompt": ""}
{"id": "efd1d938-488e-4d23-a6af-10627fa7dbfd", "fitness": 0.26230366546108125, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution (population-based), occasional Lévy-flight long jumps for exploration, and a trust-region Gaussian local search around the best; parameters are adapted online and partial re-seeding is used on stagnation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n    - Differential Evolution style population search with self-adapting F_i and CR_i\n    - Occasional Lévy/Cauchy long jumps centered on current best to escape basins\n    - Trust-region Gaussian local search around the current best to intensify search\n    - Online adaptation of F_mean, CR_mean and trust radius; partial population re-seeding on stagnation\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None,\n                 lb=None, ub=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # default population size scaling\n        if pop_size is None:\n            p = max(4, int(10 * np.log(self.dim + 1)))  # base population\n            p = max(p, int(6 * self.dim / 10))  # scale with dimension slightly\n            # don't exceed half the budget (leave room for local search etc.)\n            p = min(p, max(4, self.budget // 5))\n            pop_size = p\n        self.pop_size = int(max(4, pop_size))\n\n        # explicit bounds if provided; else will be taken from func.bounds when calling\n        if lb is None:\n            self._lb = None\n        else:\n            self._lb = np.asarray(lb, dtype=float)\n        if ub is None:\n            self._ub = None\n        else:\n            self._ub = np.asarray(ub, dtype=float)\n\n        # internal results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _get_bounds(self, func):\n        # Respect func.bounds if provided; fallback to provided / default [-5,5]\n        if hasattr(func, \"bounds\") and getattr(func.bounds, \"lb\", None) is not None:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n            # in some APIs bounds are scalars; make arrays\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        if self._lb is not None:\n            if self._lb.shape == ():\n                self._lb = np.full(self.dim, float(self._lb))\n            lb = np.maximum(lb, self._lb)\n        if self._ub is not None:\n            if self._ub.shape == ():\n                self._ub = np.full(self.dim, float(self._ub))\n            ub = np.minimum(ub, self._ub)\n\n        return lb, ub\n\n    def __call__(self, func):\n        # initialize\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        # protect against zero-range dims\n        range_vec[range_vec == 0.0] = 1.0\n\n        # population initialization\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fpop = np.full(self.pop_size, np.inf, dtype=float)\n        valid = np.zeros(self.pop_size, dtype=bool)\n\n        evals = 0\n        # Evaluate as many as budget allows (sequentially)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fpop[i] = f\n            valid[i] = True\n            evals += 1\n\n        # if budget too tiny, leave remaining individuals unevaluated (fpop = inf)\n        # find best\n        if np.any(valid):\n            best_idx = int(np.argmin(fpop))\n            best_x = pop[best_idx].copy()\n            best_f = float(fpop[best_idx])\n        else:\n            # no eval made (budget == 0)\n            self.f_opt = np.inf\n            self.x_opt = pop[0].copy()\n            return self.f_opt, self.x_opt\n\n        # algorithm hyperparameters and adaptive state\n        F_mean = 0.6\n        CR_mean = 0.3\n        # per-individual parameters (jDE-like)\n        Fi = np.full(self.pop_size, F_mean)\n        CRi = np.full(self.pop_size, CR_mean)\n        tau_F = 0.1\n        tau_CR = 0.1\n        F_min, F_max = 0.2, 0.95\n\n        # Lévy / Cauchy jump chance\n        p_levy = 0.05\n        # trust-region radius (fraction of range)\n        trust_radius = 0.2  # start moderately large (0..1 relative to full range)\n        trust_radius_min = 1e-3\n        trust_radius_max = 2.0\n\n        # adaptation memory\n        successful_F = []\n        successful_CR = []\n\n        # stagnation counters\n        no_improve_evals = 0\n        best_seen = best_f\n        stagnation_limit = max(50, 5 * self.dim)\n\n        gen = 0\n\n        def levy_step(scale=1.0):\n            # simple heavy-tailed step using Cauchy; clipped to avoid extreme numbers\n            s = self.rng.standard_cauchy(self.dim) * scale\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # lightly randomize generation-wise means (encourage exploration)\n            if self.rng.rand() < 0.1:\n                F_mean = np.clip(F_mean * (1 + 0.1 * (self.rng.rand() - 0.5)), F_min, F_max)\n                CR_mean = np.clip(CR_mean + 0.05 * (self.rng.rand() - 0.5), 0.0, 1.0)\n\n            # process individuals sequentially (target-to-target)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[i].copy()\n                f_target = fpop[i]\n\n                # generate per-individual Fi and CRi (jDE style)\n                if self.rng.rand() < tau_F:\n                    Fi[i] = self.rng.uniform(F_min, F_max)\n                else:\n                    Fi[i] = Fi[i]\n\n                if self.rng.rand() < tau_CR:\n                    CRi[i] = self.rng.rand()\n                else:\n                    CRi[i] = CRi[i]\n\n                # mutation + crossover\n                if self.rng.rand() < p_levy:\n                    # Lévy jump centered on best\n                    # scale step by range and trust region\n                    scale = 0.25 * trust_radius  # baseline\n                    step = levy_step(scale=scale) * range_vec\n                    candidate = best_x + step\n                    # small additional DE-like perturbation to mix\n                    if self.rng.rand() < 0.5:\n                        # add a small differential perturbation from two randoms if available\n                        idxs = [j for j in range(self.pop_size) if j != i]\n                        if len(idxs) >= 2:\n                            r1, r2 = self.rng.choice(idxs, size=2, replace=False)\n                            candidate += Fi[i] * (pop[r1] - pop[r2])\n                else:\n                    # classic DE/rand/1 mutation\n                    idxs = [j for j in range(self.pop_size) if j != i]\n                    if len(idxs) < 3:\n                        # fallback: gaussian local perturbation\n                        candidate = x_target + self.rng.normal(0, 0.01, size=self.dim) * range_vec\n                    else:\n                        r1, r2, r3 = self.rng.choice(idxs, size=3, replace=False)\n                        donor = pop[r1] + Fi[i] * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cross_points = self.rng.rand(self.dim) < CRi[i]\n                        if not np.any(cross_points):\n                            cross_points[self.rng.randint(0, self.dim)] = True\n                        candidate = np.where(cross_points, donor, x_target)\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # Evaluate candidate if we have evaluation budget\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy\n                replaced = False\n                if f_candidate < f_target or (not valid[i]):\n                    # replace individual\n                    pop[i] = candidate.copy()\n                    fpop[i] = f_candidate\n                    valid[i] = True\n                    replaced = True\n\n                # update best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    no_improve_evals = 0\n                else:\n                    no_improve_evals += 1\n\n                # adaptation of means based on success\n                if replaced:\n                    successful_F.append(Fi[i])\n                    successful_CR.append(CRi[i])\n                    # nudge global means toward successful params (small step)\n                    if len(successful_F) > 0:\n                        F_mean = 0.9 * F_mean + 0.1 * np.mean(successful_F[-10:])\n                    if len(successful_CR) > 0:\n                        CR_mean = 0.9 * CR_mean + 0.1 * np.mean(successful_CR[-10:])\n                else:\n                    # slight decay to avoid stagnation at extremes\n                    F_mean = np.clip(F_mean * 0.999, F_min, F_max)\n                    CR_mean = np.clip(CR_mean * 0.999, 0.0, 1.0)\n\n                # adjust per-individual Fi/CRi slightly toward means if successful less often\n                Fi[i] = np.clip(0.95 * Fi[i] + 0.05 * F_mean, F_min, F_max)\n                CRi[i] = np.clip(0.95 * CRi[i] + 0.05 * CR_mean, 0.0, 1.0)\n\n                # clear locals if necessary (not needed in Python but keep semantics)\n                # end individual\n\n            # end for population (one generation)\n            # Trust-region local search around best (small batch)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # sample count: a few candidates depending on remaining budget and dimension\n            # at least 1, at most min(remaining, dim)\n            n_local = int(min(max(1, self.dim // 2), remaining, 8))\n            local_successes = 0\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dimension\n                sigma = trust_radius * (0.5 + self.rng.rand(self.dim) * 0.5)\n                candidate = best_x + self.rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_successes += 1\n                    no_improve_evals = 0\n                else:\n                    no_improve_evals += 1\n\n            # adjust trust radius based on local search results\n            if local_successes > 0:\n                # successful local search => focus: shrink trust region\n                trust_radius = max(trust_radius * (0.85 ** local_successes), trust_radius_min)\n                # reward: slightly reduce p_levy to exploit\n                p_levy = max(0.01, p_levy * 0.9)\n            else:\n                # unsuccessful local => expand trust region a bit to escape\n                trust_radius = min(trust_radius * 1.15, trust_radius_max)\n                p_levy = min(0.5, p_levy * 1.05)\n\n            # adaptation of global params after generation\n            # move means a bit toward means of last successes for gradual learning\n            if len(successful_F) > 0:\n                F_mean = 0.95 * F_mean + 0.05 * np.mean(successful_F[-20:])\n            if len(successful_CR) > 0:\n                CR_mean = 0.95 * CR_mean + 0.05 * np.mean(successful_CR[-20:])\n\n            # stagnation handling\n            if no_improve_evals >= stagnation_limit:\n                # partial re-seed: reinitialize half of the population randomly (but limit by remaining budget)\n                k = max(1, self.pop_size // 2)\n                # select worst k individuals to replace, or random if not enough valid\n                order = np.argsort(-fpop)  # descending (worst first)\n                replace_idx = order[:k]\n                for idx in replace_idx:\n                    if evals >= self.budget:\n                        break\n                    new_x = lb + self.rng.rand(self.dim) * range_vec\n                    new_f = float(func(new_x))\n                    evals += 1\n                    pop[idx] = new_x\n                    fpop[idx] = new_f\n                    valid[idx] = True\n                    # small increase in trust radius to help exploration after reset\n                    trust_radius = min(trust_radius * 1.2, trust_radius_max)\n                no_improve_evals = 0\n                # also increase chance of long jumps\n                p_levy = min(0.5, p_levy * 1.5)\n\n            # safety adjust: ensure best_x aligned with best in fpop\n            current_best_idx = int(np.argmin(fpop))\n            if fpop[current_best_idx] < best_f:\n                best_f = float(fpop[current_best_idx])\n                best_x = pop[current_best_idx].copy()\n\n            # small generation-level diversity injection if stuck in narrow search\n            if trust_radius < 1e-2 and p_levy < 0.02:\n                # jitter a few individuals slightly\n                jitter_k = max(1, min(self.pop_size // 10, self.dim))\n                for _ in range(jitter_k):\n                    j = self.rng.randint(0, self.pop_size)\n                    pop[j] = np.minimum(np.maximum(pop[j] + self.rng.normal(0, 0.01, size=self.dim) * range_vec, lb), ub)\n\n        # finished search\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.262 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11509937746770915, 0.16138859056045107, 0.30805943634754407, 0.40216849050857717, 0.28286561291027856, 0.35435593249982145, 0.2517524533603226, 0.3094891504540839, 0.25802564248661375, 0.17983196801541101]}, "task_prompt": ""}
{"id": "2cff89d8-a1e2-4383-84c6-19f1c54e5165", "fitness": 0.2276751539152367, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: Differential Evolution with per-individual parameter adaptation, occasional truncated Lévy (Cauchy) jumps for long-range exploration, and a trust-region Gaussian local search around the best solution with online step-size adaptation and stagnation-driven reseeding.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid optimizer: Differential Evolution (adaptive), occasional Lévy-like\n    (Cauchy) jumps for global moves, and a trust-region local search around\n    the current best. Online adaptation of F and CR means and trust radius,\n    plus stagnation-driven reseeding.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # Default population size: scale with dimension but never exceed half the budget\n        if pop_size is None:\n            p = int(max(8, 4 * self.dim))\n            p = min(p, max(4, self.budget // 3))  # keep reasonable compared to budget\n            self.pop_size = max(4, p)\n        else:\n            self.pop_size = max(4, int(pop_size))\n        # Internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # broadcast bounds to vector length dim\n        b = np.asarray(b)\n        if b.shape == ():\n            return np.full(self.dim, b)\n        if b.shape[0] != self.dim:\n            raise ValueError(\"Bounds length mismatch with dimension\")\n        return b\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize function `func` with a fixed number of function evaluations = self.budget.\n        Expects func(x) -> float and func.bounds.lb / func.bounds.ub to be available.\n        \"\"\"\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # enforce search box as [-5,5] if wrapper doesn't provide\n        lb = np.maximum(lb, -5.0)\n        ub = np.minimum(ub, 5.0)\n\n        # Budget accounting\n        evals = 0\n        max_evals = self.budget\n\n        D = self.dim\n        NP = min(self.pop_size, max(4, max_evals))  # ensure not larger than budget trivial\n        # Initialize population uniformly; evaluate as budget allows\n        pop = self.rng.uniform(lb, ub, size=(NP, D))\n        fitness = np.full(NP, np.inf)\n\n        # Evaluate initial members up to budget\n        for i in range(NP):\n            if evals >= max_evals:\n                break\n            x = pop[i].copy()\n            fitness[i] = func(x)\n            evals += 1\n            # update best\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = x.copy()\n\n        # If we couldn't evaluate full population, leave remaining as-is (won't be selected)\n        # Algorithm hyper-parameters (adaptive)\n        F_mean = 0.6\n        CR_mean = 0.5\n        tau_F = 0.1   # adaptation step-size for F\n        tau_CR = 0.1  # adaptation step-size for CR\n\n        trust_radius = 0.2 * (ub - lb)  # per-dimension trust radius (vector)\n        # clamp trust radius to a fraction of domain\n        trust_min = 1e-6\n        trust_max = 0.5 * (ub - lb)\n\n        p_levy = 0.08  # initial probability to perform a Levy-like jump (Cauchy)\n        p_levy_min = 0.01\n        p_levy_max = 0.4\n\n        stagnation_evals = 0\n        stagnation_threshold = max(50, 10 * D)\n        consecutive_no_improve = 0\n\n        # Helper: truncated Cauchy / Lévy-like step generator\n        def levy_step(scale=1.0):\n            # standard Cauchy (heavy tail); truncate extreme values to avoid numeric blow-up\n            # returns vector length D\n            # scale is relative to domain range\n            # limit to +/- 10*scale per-dimension\n            raw = self.rng.standard_cauchy(size=D)\n            max_abs = 10.0\n            raw = np.clip(raw, -max_abs, max_abs)\n            # scale by domain size\n            return raw * scale\n\n        # main loop: generations until budget exhausted\n        gen = 0\n        # Protect against infinite loops\n        while evals < max_evals:\n            gen += 1\n            # For each target in population (sequential)\n            indices = np.arange(NP)\n            self.rng.shuffle(indices)\n            for i in indices:\n                if evals >= max_evals:\n                    break\n\n                x_target = pop[i].copy()\n                f_target = fitness[i]\n\n                # sample per-individual F_i and CR_i around their means, with small randomness\n                # jDE inspired but simpler: sample log-normal-like for F\n                Fi = F_mean * (1.0 + 0.3 * (self.rng.rand() - 0.5))\n                Fi = np.clip(Fi, 0.05, 1.0)\n                CRi = CR_mean + 0.2 * (self.rng.rand() - 0.5)\n                CRi = np.clip(CRi, 0.0, 1.0)\n\n                # mutation: choose three distinct indices different from i\n                idx = list(range(NP))\n                idx.remove(i)\n                r1, r2, r3 = self.rng.choice(idx, size=3, replace=False)\n                base = pop[r1]\n                diff = pop[r2] - pop[r3]\n                donor = base + Fi * diff\n\n                # occasional levy jump centered on best for exploration\n                if self.rng.rand() < p_levy:\n                    # scale of levy relative to domain size and current trust radius mean\n                    domain_scale = (ub - lb)\n                    # average trust radius scalar\n                    tr_scale = np.maximum(1e-12, np.mean(trust_radius / np.maximum(domain_scale, 1e-12)))\n                    # combine scales\n                    levy_scale = 0.3 * domain_scale * (1.0 + 2.0 * tr_scale)\n                    donor = self.x_opt + levy_step(scale=levy_scale)\n\n                # trust-region nudging to encourage local search around donor/best occasionally\n                if self.rng.rand() < 0.25:\n                    # add small gaussian proportional to trust radius (per-dimension)\n                    gaussian = self.rng.normal(scale=trust_radius)\n                    donor = donor + gaussian\n\n                # binomial crossover\n                cross = self.rng.rand(D) < CRi\n                # ensure at least one component from donor\n                jrand = self.rng.randint(D)\n                cross[jrand] = True\n                trial = np.where(cross, donor, x_target)\n\n                # project to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate trial if budget left\n                if evals >= max_evals:\n                    break\n                f_trial = func(trial)\n                evals += 1\n\n                # selection (greedy)\n                if f_trial < f_target:\n                    # success: replace in population\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    # adapt means: move them slightly toward successful Fi and CRi\n                    F_mean = (1 - tau_F) * F_mean + tau_F * Fi\n                    CR_mean = (1 - tau_CR) * CR_mean + tau_CR * CRi\n                    # encourage smaller trust radius when local improvements happen\n                    # use success magnitude relative to current best\n                    if f_trial < self.f_opt - 1e-12:\n                        # global improvement\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        # shrink trust region to intensify exploitation\n                        trust_radius = np.maximum(trust_min, 0.8 * trust_radius)\n                        consecutive_no_improve = 0\n                        stagnation_evals = 0\n                    else:\n                        # local improvement in population but not global best\n                        trust_radius = np.maximum(trust_min, 0.95 * trust_radius)\n                        consecutive_no_improve = 0\n                        stagnation_evals = 0\n                else:\n                    # failure: mild adaptation to encourage exploration\n                    F_mean = (1 - 0.01) * F_mean + 0.01 * (0.5 + 0.5 * self.rng.rand())\n                    CR_mean = (1 - 0.01) * CR_mean + 0.01 * self.rng.rand()\n                    # expand trust region slightly on failures to escape\n                    trust_radius = np.minimum(trust_max, 1.02 * trust_radius)\n                    consecutive_no_improve += 1\n                    stagnation_evals += 1\n\n                # update global best if someone else improved it previously in population\n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = pop[i].copy()\n                    consecutive_no_improve = 0\n                    stagnation_evals = 0\n\n                # adjust Levy probability slowly based on stagnation\n                if stagnation_evals > 0:\n                    p_levy = np.clip(p_levy * (1.0 + 0.0008 * stagnation_evals), p_levy_min, p_levy_max)\n                else:\n                    p_levy = max(p_levy_min, p_levy * 0.999)\n\n                # safety: keep means in valid ranges\n                F_mean = float(np.clip(F_mean, 0.05, 1.0))\n                CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n                # Break early if budget exhausted\n                if evals >= max_evals:\n                    break\n\n            # End of generation: trust-region local search around current best\n            if evals >= max_evals:\n                break\n\n            # Number of local samples: a small handful scaled by dimension and remaining budget\n            remaining = max_evals - evals\n            local_budget = min( max(1, D), remaining )  # at least 1, <= D\n            # but try a few more if cheap budget remains\n            local_budget = min(local_budget + (remaining // 50), remaining, 5 * D)\n\n            for _ in range(int(local_budget)):\n                if evals >= max_evals:\n                    break\n                # anisotropic gaussian: per-dimension sigma proportional to trust_radius\n                sigma = 0.5 * trust_radius  # moderate exploration in trust region\n                local_candidate = self.x_opt + self.rng.normal(scale=sigma, size=D)\n                local_candidate = np.minimum(np.maximum(local_candidate, lb), ub)\n                f_local = func(local_candidate)\n                evals += 1\n                if f_local < self.f_opt:\n                    # success: accept and shrink trust region\n                    self.f_opt = f_local\n                    self.x_opt = local_candidate.copy()\n                    # optionally inject into population replacing worst\n                    worst_idx = np.argmax(fitness)\n                    pop[worst_idx] = local_candidate.copy()\n                    fitness[worst_idx] = f_local\n                    trust_radius = np.maximum(trust_min, 0.7 * trust_radius)\n                    consecutive_no_improve = 0\n                    stagnation_evals = 0\n                else:\n                    # no improvement: expand small amount\n                    trust_radius = np.minimum(trust_max, 1.03 * trust_radius)\n                    consecutive_no_improve += 1\n                    stagnation_evals += 1\n\n            # End local search adjustments\n            # If many consecutive no improvements, increase exploration and maybe reseed half\n            if consecutive_no_improve > stagnation_threshold:\n                # reseed half of the population with new random points\n                n_reseed = max(1, NP // 2)\n                indices_reseed = self.rng.choice(NP, size=n_reseed, replace=False)\n                for j in indices_reseed:\n                    if evals >= max_evals:\n                        break\n                    new_x = self.rng.uniform(lb, ub)\n                    new_f = func(new_x)\n                    evals += 1\n                    pop[j] = new_x\n                    fitness[j] = new_f\n                    if new_f < self.f_opt:\n                        self.f_opt = new_f\n                        self.x_opt = new_x.copy()\n                        trust_radius = np.maximum(trust_min, 0.8 * trust_radius)\n                        consecutive_no_improve = 0\n                        stagnation_evals = 0\n                # after reseed, encourage larger Levy probability temporarily\n                p_levy = min(p_levy_max, p_levy * 1.3)\n                # reset counters\n                consecutive_no_improve = 0\n                stagnation_evals = 0\n\n            # small decays to avoid infinite growth/shrink\n            trust_radius = np.minimum(trust_max, np.maximum(trust_min, trust_radius))\n            # generation done, continue until budget exhausted\n        # end main loop\n\n        # final return of best found\n        return float(self.f_opt), np.asarray(self.x_opt).copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.228 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10946347892250619, 0.1620900102914289, 0.27991949399900595, 0.24120485706651418, 0.24510538110147884, 0.37493026384170447, 0.21507863157447926, 0.26186964229314136, 0.2322582751399116, 0.15483150492219622]}, "task_prompt": ""}
{"id": "9e9d11b4-7f09-4f28-9f68-21337c4b316a", "fitness": 0.22969072043251887, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and an online trust-region local search with adaptive parameters for robust exploration–exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=42)\n        fopt, xopt = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # internal state for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # bounds from func (Many BBOB uses -5..5, but use func.bounds)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (careful with budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = int(np.nanargmin(fvals))\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # no evals possible (budget==0)\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius\n        min_trust = 1e-9\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        idxs_all = np.arange(self.pop_size)\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # adapt per-generation randomness: but sampled per individual below\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi (jDE-like)\n                Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n\n                # create donor vector: Levy-centered exploration or DE mutation\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration (heavy tails)\n                    step = levy_step()\n                    # scale by trust radius and range_vec to be sensible across dims\n                    # ensure non-zero divisor\n                    denom = max(np.linalg.norm(range_vec), 1e-12)\n                    scale = (step_scale * 0.5 + 0.5 * rng.rand()) * (trust_radius / denom)\n                    donor = best_x + scale * step * (range_vec / denom)\n                    # binomial crossover with the target\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    candidate = np.clip(trial, lb, ub)\n                else:\n                    # classic DE/rand/1 mutation\n                    choices = np.delete(idxs_all, i)\n                    if choices.size >= 3:\n                        r1, r2, r3 = rng.choice(choices, 3, replace=False)\n                        mutant = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    else:\n                        # fallback when pop small\n                        mutant = pop[i].copy()\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, mutant, pop[i])\n                    candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # greedy selection into population at index i\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt parameter means slightly toward used Fi and CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation adjustments: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                denom = max(np.linalg.norm(range_vec), 1e-12)\n                sigma = (trust_radius / denom) * (0.2 + 0.8 * rng.rand(self.dim))\n                candidate = best_x + rng.normal(0.0, 1.0, self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand slightly to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter += 1\n\n            # Adapt exploration probability and parameter means based on successes\n            if successes > 0:\n                p_levy = max(0.01, p_levy * (0.95 if successes > self.pop_size * 0.2 else 0.98))\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.1, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5) and evals < self.budget:\n                k = max(1, self.pop_size // 2)\n                for j in rng.choice(np.arange(self.pop_size), k, replace=False):\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.230 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11739207453434752, 0.17625971851309674, 0.28983769409330007, 0.3418878546411842, 0.231205811338833, 0.2853963361268874, 0.23131461931977992, 0.26171425267634507, 0.20674471604828504, 0.15515412703313003]}, "task_prompt": ""}
{"id": "2977116a-7733-4feb-a383-fc37311c6470", "fitness": 0.2970289608664768, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DE with occasional Lévy jumps and a shrinking/expanding trust-region local search — fast global exploration via DE+Lévy and focused local exploitation via adaptive trust radii and online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but bounded reasonably\n        if pop_size is None:\n            pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            pop_size = min(pop_size, max(4, self.budget // 20))\n        self.pop_size = int(pop_size)\n        # internal best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Accept scalar or array-like and return 1D numpy array of length self.dim.\n        If b is not provided or None, return default [-5, ... , -5] or [5,...,5] externally.\n        \"\"\"\n        if b is None:\n            return None\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.copy()\n        # try broadcasting if possible\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            # fallback: repeat first element\n            return np.full(self.dim, float(b.flat[0]), dtype=float)\n\n    def __call__(self, func):\n        # get bounds from func if present, otherwise default [-5,5]\n        lb = self._ensure_array_bounds(getattr(func, \"bounds\", None) and getattr(func.bounds, \"lb\", None))\n        ub = self._ensure_array_bounds(getattr(func, \"bounds\", None) and getattr(func.bounds, \"ub\", None))\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # safety: ensure shapes and ub>lb\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        rng = self.rng\n        budget = int(self.budget)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population (stop if budget exhausted)\n        valid = np.zeros(self.pop_size, dtype=bool)\n        for i in range(self.pop_size):\n            if evals >= budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n            valid[i] = True\n\n        # If no evaluations possible, return None\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # determine current best\n        best_idx = int(np.argmin(fvals[valid]))\n        best_indices = np.where(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6         # initial differential weight mean\n        CR_mean = 0.9        # crossover probability mean\n        p_levy = 0.08        # probability of a Lévy jump\n        step_scale = 0.25    # base scale for Lévy and trust radius\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # initial scalar trust radius\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        min_trust = 1e-8\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # generate from standard Cauchy and limit extreme outliers to avoid numerical blow-up\n            s = rng.standard_cauchy(self.dim)\n            # clip extreme values (still heavy-tailed but bounded)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # Main loop: iterate until budget exhausted\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            successes = 0\n            # per-generation adaptation spreads for F and CR\n            # simple jDE-like sampling: each individual samples Fi and CRi around means\n            indices = np.arange(self.pop_size)\n\n            for i in range(self.pop_size):\n                if evals >= budget:\n                    break\n\n                Fi = None\n                CRi = None\n                # choose mutation strategy\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    scale = step_scale * (0.3 + rng.rand() * 1.0)  # randomize scale a bit\n                    donor = best_x + scale * step * (range_vec / max(range_vec.mean(), 1e-12))\n                    # create trial by mixing donor and current member (to keep some heritage)\n                    CRi = np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0)\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                else:\n                    # DE/rand/1-like mutation (ensure distinct indices)\n                    if self.pop_size < 4:\n                        # fallback to rand/1 using perturbation around best\n                        r1 = r2 = r3 = 0\n                        donor = pop[i] + rng.normal(0, 0.1, size=self.dim) * range_vec\n                    else:\n                        # choose three distinct indices not equal to i\n                        choices = np.setdiff1d(indices, np.array([i]))\n                        r = rng.choice(choices, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        Fi = np.clip(rng.normal(F_mean, 0.15), 0.05, 1.0)\n                        CRi = np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[rng.randint(0, self.dim)] = True\n                        trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # greedy selection\n                if f_candidate < fvals[i]:\n                    fvals[i] = f_candidate\n                    pop[i] = candidate.copy()\n                    successes += 1\n                    # update per-generation means modestly\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                    # update global best\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation: trust-region local search around best\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (0.25 + rng.rand(self.dim) * 0.75) * (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    trust_radius = min(trust_radius, max_trust)\n                    stagnation_counter = 0\n                    successes += 1\n                else:\n                    # unsuccessful => expand a bit to escape local traps (but keep within bounds)\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n                    trust_radius = max(trust_radius, min_trust)\n                    stagnation_counter += 1\n\n            # per-generation parameter drift\n            if successes > 0:\n                # slightly favor exploitation settings\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                # reduce levy probability gently\n                p_levy = max(0.01, p_levy * 0.98)\n            else:\n                # stagnation handling: increase chance of long jumps and a touch more exploration\n                p_levy = min(0.7, p_levy * 1.08 + 0.01)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: re-seed part of population when stuck\n            if stagnation_counter > max(60, self.dim * 6):\n                k = max(1, self.pop_size // 2)\n                reinit_idx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reinit_idx:\n                    if evals >= budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius after reset to encourage broad search\n                trust_radius = min(max_trust, max(trust_radius * 2.0, 0.1 * np.linalg.norm(range_vec)))\n                stagnation_counter = 0\n                # nudge parameters toward exploration\n                p_levy = min(0.5, p_levy * 1.2)\n                F_mean = np.clip(0.5 * F_mean + 0.5 * 0.7, 0.05, 0.99)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.297 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1712046063788526, 0.2767240958041688, 0.376612469187825, 0.4934679223645867, 0.2787975232341804, 0.35409342732512095, 0.25596058735396277, 0.3149468349209319, 0.27561239833639906, 0.17286974375873965]}, "task_prompt": ""}
{"id": "d9663d5f-0c1f-49da-8367-6147eedae464", "fitness": 0.6816532452899204, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: Differential Evolution with per-individual jDE-style adaptation, occasional Lévy-flight long jumps for exploration, and a trust-region Gaussian local search around the current best; online control of jump probability and trust radius to balance global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (rand/1/bin) with per-individual jDE-like adaptation of F and CR\n      - Occasional Lévy-like (Cauchy) jumps centered on the best for long-range exploration\n      - A trust-region Gaussian local search around the current best (adaptive trust radius)\n    Designed to work with optimization problems whose func exposes bounds via func.bounds.lb/ub.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # sensible default population size scaled by dimension but limited by budget\n        if pop_size is None:\n            default = max(10, int(8 + 2 * np.sqrt(self.dim)))\n            self.pop_size = min(default, max(4, self.budget // 8))\n        else:\n            self.pop_size = max(4, int(pop_size))\n\n        # internal best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # handle trivial budgets\n        if self.budget <= 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # ensure we have a reasonable finite range (Many BBOB uses [-5,5])\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        rng = self.rng\n\n        # if budget is very small, reduce population so we don't waste eval attempts\n        pop_size = min(self.pop_size, max(1, self.budget))\n        pop = lb + rng.rand(pop_size, self.dim) * range_vec\n        fvals = np.full(pop_size, np.inf, dtype=float)\n\n        # jDE parameters for per-individual adaptation\n        F_i = 0.5 + 0.3 * rng.rand(pop_size)  # scaling factors (~0.5-0.8)\n        CR_i = rng.rand(pop_size)             # crossover rates ~U(0,1)\n        tau_F = 0.1\n        tau_CR = 0.1\n\n        # global control\n        p_levy = 0.06\n        trust_radius = 0.25 * np.linalg.norm(range_vec)   # initial trust-region size (absolute)\n        min_trust = 1e-6\n        max_trust = 2.0 * np.linalg.norm(range_vec)\n        stagnation_counter = 0\n        stagnation_limit = max(5 * pop_size, 50)\n\n        evals = 0\n\n        # evaluate initial population as much as budget allows\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # set initial best\n        best_idx = int(np.argmin(fvals))\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        # helper: levy-like heavy-tail step (use clipped Cauchy)\n        def levy_step():\n            step = rng.standard_cauchy(self.dim)\n            # scale down heavy outliers to avoid numerical blow-up\n            step = np.clip(step, -1e3, 1e3)\n            return step\n\n        gen = 0\n        # main loop: process generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # shuffle order to reduce bias\n            order = rng.permutation(pop_size)\n            for ii in order:\n                if evals >= self.budget:\n                    break\n\n                # jDE-style occasional resampling of Fi and CRi\n                if rng.rand() < tau_F:\n                    Fi = 0.1 + 0.9 * rng.rand()\n                else:\n                    Fi = F_i[ii]\n                if rng.rand() < tau_CR:\n                    CRi = rng.rand()\n                else:\n                    CRi = CR_i[ii]\n\n                # Choose operation: Levy jump around best (exploration) or DE mutation+crossover (exploitation)\n                if rng.rand() < p_levy:\n                    # Lévy-like jump centered on best (Cauchy scaled by trust radius and global range)\n                    step = levy_step()\n                    # scale: combine trust radius and relative component sizes\n                    scale = (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                    candidate = best_x + (0.5 + rng.rand() * 1.5) * step * scale * (range_vec)\n                    # small local perturbation occasionally\n                    if rng.rand() < 0.05:\n                        candidate += rng.normal(0, 0.1, size=self.dim) * trust_radius * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                else:\n                    # DE/rand/1/bin mutation + binomial crossover\n                    # pick three distinct indices different from ii\n                    idxs = np.arange(pop_size)\n                    idxs = idxs[idxs != ii]\n                    if idxs.size < 3:\n                        # fallback to random restart candidate if population too small\n                        candidate = lb + rng.rand(self.dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        mutant = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # ensure mutant not too wildly out of domain by blending toward center\n                        mutant = np.clip(mutant, lb - 0.5 * range_vec, ub + 0.5 * range_vec)\n                        # binomial crossover\n                        cross = rng.rand(self.dim) < CRi\n                        if not np.any(cross):\n                            cross[rng.randint(self.dim)] = True\n                        candidate = np.where(cross, mutant, pop[ii])\n\n                # projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_cand = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement (target-to-trial)\n                if f_cand < fvals[ii]:\n                    pop[ii] = candidate\n                    fvals[ii] = f_cand\n                    successes += 1\n                    # commit Fi and CRi if successful\n                    F_i[ii] = Fi\n                    CR_i[ii] = CRi\n                    # slight encouragement: nudge Fi/CRi toward used values (already set)\n                else:\n                    # small decay to encourage diversity for unsuccessful individuals\n                    F_i[ii] = 0.9 * F_i[ii] + 0.1 * (0.1 + 0.9 * rng.rand())\n                    CR_i[ii] = 0.9 * CR_i[ii] + 0.1 * rng.rand()\n\n                # update global best\n                if f_cand < best_f:\n                    best_f = f_cand\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful local improvement: tighten trust radius moderately\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    stagnation_counter += 1\n\n                # if many improvements quickly, slightly reduce Levy probability to exploit\n                if successes > 0 and successes > 0.25 * pop_size:\n                    p_levy = max(0.01, p_levy * 0.94)\n\n            # End of generation adjustments (after iterating population)\n            # Trust-region local search: attempt a few local Gaussian samples around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # local samples depends on dim and remaining budget\n            local_samples = min( max(1, int(2 + self.dim // 10)), remaining, 8 )\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic Gaussian: per-dimension sigma ~ trust_radius scaled by a random factor\n                sigma = (0.3 + rng.rand(self.dim) * 0.7) * (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_cand = float(func(candidate))\n                evals += 1\n                if f_cand < best_f:\n                    best_f = f_cand\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # successful local search => shrink trust region to intensify search\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a little to escape narrow traps\n                    trust_radius = min(max_trust, trust_radius * 1.06)\n\n            # adjust p_levy and F/CR global tendencies based on success\n            if successes + local_success > 0:\n                # encourage exploitation slightly when successful\n                p_levy = max(0.01, p_levy * (0.96 if successes + local_success > 0.25 * pop_size else 0.98))\n            else:\n                # stagnation: increase chance of long jumps and diversify parameters\n                p_levy = min(0.6, p_levy * 1.08 + 0.02)\n                # slight randomization of some individuals to escape stagnation\n                if stagnation_counter > stagnation_limit:\n                    # re-seed a portion of population (but respect budget)\n                    reinit_count = max(1, pop_size // 2)\n                    indices = rng.choice(pop_size, reinit_count, replace=False)\n                    for j in indices:\n                        if evals >= self.budget:\n                            break\n                        pop[j] = lb + rng.rand(self.dim) * range_vec\n                        f_new = float(func(pop[j]))\n                        fvals[j] = f_new\n                        evals += 1\n                        if f_new < best_f:\n                            best_f = f_new\n                            best_x = pop[j].copy()\n                    # after reset, broaden trust radius and increase exploration\n                    trust_radius = min(max_trust, trust_radius * 1.5)\n                    p_levy = min(0.9, p_levy * 1.3)\n                    stagnation_counter = 0\n\n            # small safeguard: keep trust_radius in bounds\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.682 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14544043203931867, 0.17501130457866765, 0.758115865247379, 0.9608901388683035, 0.5796494553615376, 0.9648373568900437, 0.837646593932274, 0.8918910848535581, 0.8864758124405528, 0.6165744086875695]}, "task_prompt": ""}
{"id": "dc40ff10-15e6-496e-9b3b-aa5c3589f791", "fitness": 0.18962397198720007, "name": "HybridDeLevyTrust", "description": "Hybrid DE with occasional Lévy flights and an adaptive trust-region local search — combines global DE exploration, Cauchy/Lévy long jumps when stagnating, and focused Gaussian trust-region refinement with online adaptation of F/CR and jump probability.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy/Cauchy jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history. Designed for continuous\n    optimization in box [-5,5] (or func.bounds if provided).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # default population size scales with dimension but limited by budget\n        if pop_size is None:\n            self.pop_size = max(6, min(60, int(6 + 2 * np.sqrt(self.dim))))\n            # but not larger than a reasonable fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 10))\n        else:\n            self.pop_size = int(pop_size)\n            self.pop_size = max(4, min(self.pop_size, max(4, self.budget // 5)))\n        # solution placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = self.rng\n        # bounds\n        if hasattr(func, \"bounds\") and getattr(func.bounds, \"lb\", None) is not None:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            # support scalar or vector bounds\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item())\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # ensure shapes\n        lb = lb.flatten()\n        ub = ub.flatten()\n        if lb.size != self.dim:\n            lb = np.broadcast_to(lb, (self.dim,))\n        if ub.size != self.dim:\n            ub = np.broadcast_to(ub, (self.dim,))\n        range_vec = ub - lb\n        # internal counters\n        evals = 0\n        budget = self.budget\n\n        # initialize population uniformly in bounds (but don't exceed budget for evaluations)\n        pop = np.empty((self.pop_size, self.dim), dtype=float)\n        for i in range(self.pop_size):\n            pop[i] = lb + rng.rand(self.dim) * range_vec\n\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        # evaluate initial population (as many as budget allows)\n        for i in range(self.pop_size):\n            if evals >= budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                # in case func expects 1d arrays or other types, ensure conversion\n                fvals[i] = float(func(np.asarray(pop[i], dtype=float)))\n            evals += 1\n\n        # find initial best\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = int(np.nanargmin(fvals))\n            best_f = float(fvals[best_idx])\n            best_x = pop[best_idx].copy()\n        else:\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # mean differential weight\n        CR_mean = 0.3       # mean crossover prob\n        p_levy = 0.08       # base probability of a Lévy jump\n        trust_radius = 0.25 * np.linalg.norm(range_vec)  # scalar trust radius (Euclidean)\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        gen = 0\n        stagnation_counter = 0\n        success_history = []\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            # cap extreme outliers to keep numerics stable\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < budget:\n            gen += 1\n            gen_successes = 0\n            # per-generation arrays for adaptation (jDE-like)\n            F_list = np.clip(rng.normal(F_mean, 0.1, size=self.pop_size), 0.05, 1.0)\n            CR_list = np.clip(rng.normal(CR_mean, 0.1, size=self.pop_size), 0.0, 1.0)\n\n            # Shuffle order to reduce positional bias\n            order = rng.permutation(self.pop_size)\n\n            for idx in order:\n                if evals >= budget:\n                    break\n\n                target = pop[idx].copy()\n                f_target = fvals[idx]\n\n                # decide exploration or DE\n                use_levy = rng.rand() < p_levy\n                if use_levy:\n                    # Lévy jump centered on current global best\n                    # scale jump size with trust radius and problem range\n                    step = levy_step()\n                    # scale per-dimension by range_vec to keep units consistent\n                    # also multiply by a random magnitude factor\n                    mag = 0.5 + rng.rand() * 2.0\n                    candidate = best_x + (step * mag) * (trust_radius / (1.0 + np.linalg.norm(step))) * (range_vec / (np.linalg.norm(range_vec) + 1e-12))\n                    # small local jitter as well\n                    candidate += rng.normal(0, 0.01, size=self.dim) * range_vec\n                    method = \"levy\"\n                else:\n                    # classical DE/rand/1/bin with per-individual F_i and CR_i\n                    Fi = float(F_list[idx])\n                    CRi = float(CR_list[idx])\n                    # pick distinct indices r1, r2, r3 distinct from idx\n                    all_idx = np.arange(self.pop_size)\n                    choices = np.setdiff1d(all_idx, np.array([idx]), assume_unique=True)\n                    if choices.size < 3:\n                        # fallback: do a random perturbation\n                        donor = target + Fi * (rng.randn(self.dim) * range_vec * 0.01)\n                    else:\n                        r = rng.choice(choices, size=3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    # ensure at least one element from donor\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    candidate = np.where(cr_mask, donor, target)\n                    method = \"de\"\n\n                # ensure bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # one evaluation\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = float(func(np.asarray(candidate, dtype=float)))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate <= f_target:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    gen_successes += 1\n                    # adapt means slightly toward successful Fi/CRi if used\n                    if method == \"de\":\n                        # move means toward Fi/CRi used (simple exponential smoothing)\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    else:\n                        # if levy jump succeeded, slightly increase exploration probability decay\n                        p_levy = max(0.02, p_levy * 0.95)\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                    else:\n                        # slight progress, still reset stagnation a little\n                        stagnation_counter = max(0, stagnation_counter - 1)\n                else:\n                    # unsuccessful: small adjustments to encourage diversity\n                    if method == \"de\":\n                        F_mean = np.clip(F_mean * (1.0 + 0.002), 0.05, 0.99)\n                        CR_mean = np.clip(CR_mean * (1.0 - 0.001), 0.0, 0.99)\n                    else:\n                        p_levy = min(0.5, p_levy * 1.02)\n                    stagnation_counter += 1\n\n                # book-keeping for overall best\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                # break early guards handled by while condition\n\n            # End of generation: apply trust-region local search around best\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n\n            # local sample budget: a small fraction of remaining but at least 1\n            n_local = int(min(max(1, self.dim // 2), remaining, 8))\n            local_success = 0\n            # anisotropic sigma: base on trust_radius and per-dim range\n            for _ in range(n_local):\n                if evals >= budget:\n                    break\n                # sample Gaussian around best with scaled sigma\n                # use rand per-dimension to create anisotropic samples\n                sigma = (trust_radius / (np.linalg.norm(range_vec) + 1e-12)) * (0.5 + rng.rand(self.dim) * 0.5)\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = float(func(np.asarray(candidate, dtype=float)))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    trust_radius = max(min_trust, trust_radius * 0.85)  # focus\n                    local_success += 1\n                    stagnation_counter = 0\n                else:\n                    # expand slightly to escape local minima\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n                # update overall best\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n\n            # adapt exploration probability based on generation success\n            if gen_successes > max(1, self.pop_size * 0.05) or local_success > 0:\n                # encourage exploitation: reduce levy chance\n                p_levy = max(0.01, p_levy * 0.9)\n                # nudge means toward exploitation\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                F_mean = np.clip(0.98 * F_mean + 0.02 * 0.6, 0.05, 0.99)\n            else:\n                # stagnation: increase chance of long jumps and diversity\n                p_levy = min(0.6, p_levy * 1.12)\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # strong stagnation: partial population reseed\n            if stagnation_counter > max(50, self.dim * 6):\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                reinit_idx = rng.choice(np.arange(self.pop_size), size=k, replace=False)\n                for j in reinit_idx:\n                    if evals >= budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = float(func(np.asarray(pop[j], dtype=float)))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # enlarge trust radius to encourage broader exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # reset some adaptation parameters gently\n                F_mean = 0.6\n                CR_mean = 0.3\n                p_levy = min(0.3, p_levy * 1.2)\n                # continue main loop\n\n        # final results\n        # ensure x_opt and f_opt reflect best found\n        if self.x_opt is None:\n            self.f_opt = float(best_f)\n            self.x_opt = best_x.copy()\n        else:\n            # ensure consistency: if recorded best is worse than best_f, update\n            if best_f < self.f_opt:\n                self.f_opt = float(best_f)\n                self.x_opt = best_x.copy()\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.190 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10674600137487811, 0.15444025419080631, 0.24007532875173598, 0.17309143681069628, 0.2134149573233729, 0.2567560878028877, 0.2153387258439624, 0.18934207251852886, 0.19475224055446771, 0.1522826147006643]}, "task_prompt": ""}
{"id": "0f940aad-d444-46ca-8b66-2bd27d544407", "fitness": 0.5079093274444999, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and a trust-region local search with online step-size and parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # Population scaled with dimension but kept feasible with budget.\n            # Try to keep at least a few generations possible.\n            guess = max(8, 4 * self.dim)\n            guess = min(guess, max(4, self.budget // 8))\n            self.pop_size = int(max(4, guess))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        # broadcast scalar or per-dim vector to length dim\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full((self.dim,), float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast if possible\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to dimension {}\".format(self.dim))\n\n    def __call__(self, func):\n        # prepare bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec) + 1e-12\n\n        rng = self.rng\n        evals = 0\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        # evaluate initial population as much as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # if no evaluations possible, return\n        if evals == 0:\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        min_trust = 1e-6 * range_norm\n        max_trust = 2.0 * range_norm\n\n        # DE adaptation means\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.05  # chance of long Lévy jump per candidate\n\n        stagnation_counter = 0\n        no_improve_evals = 0\n\n        gen = 0\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # shuffle order to avoid bias\n            order = rng.permutation(self.pop_size)\n            best_idx = int(np.argmin(fvals))\n            best_f = float(fvals[best_idx])\n            best_x = pop[best_idx].copy()\n\n            for i in order:\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual F and CR (jDE-like small perturbation)\n                Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0))\n\n                if rng.random() < p_levy:\n                    # Lévy-like jump centered on current best (Cauchy as proxy)\n                    step = rng.standard_cauchy(self.dim)\n                    # truncate extreme outliers\n                    step = np.clip(step, -10.0, 10.0)\n                    # scale by trust_radius and per-dim range to keep in sensible scale\n                    scale = (0.5 * trust_radius) / (np.linalg.norm(step) / np.sqrt(self.dim) + 1e-12)\n                    donor = best_x + step * scale * (range_vec / (np.maximum(range_vec.mean(), 1e-12)))\n                    # also add a small random perturbation to encourage diversity\n                    donor += rng.normal(0, 0.01, size=self.dim) * range_vec\n                else:\n                    # DE/rand/1 mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cr_mask = rng.random(self.dim) < CRi\n                # ensure at least one dimension changes\n                cr_mask[rng.integers(0, self.dim)] = True\n                trial = pop[i].copy()\n                trial[cr_mask] = donor[cr_mask]\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # one evaluation if budget permits\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt means toward successful parameters\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                    # update best\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local improvement: shrink trust region slightly\n                        trust_radius = max(min_trust, 0.8 * trust_radius)\n                        stagnation_counter = 0\n                        no_improve_evals = 0\n                    else:\n                        # successful but not global best => slight diversification\n                        trust_radius = min(max_trust, trust_radius * 1.02)\n                else:\n                    # unsuccessful => slightly expand trust to encourage escape\n                    trust_radius = min(max_trust, trust_radius * 1.005)\n\n                # track global best improvement\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    stagnation_counter = 0\n                    no_improve_evals = 0\n                else:\n                    no_improve_evals += 1\n\n            # End of generation: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining > 0:\n                # number of local samples: small handful scaled by dimension and remaining budget\n                local_budget = min( max(1, int(2 + self.dim // 2)), remaining )\n                # anisotropic sigma: base trust_radius scaled per-dim\n                sigma = (trust_radius / (range_norm + 1e-12)) * (0.5 + rng.random(self.dim) * 0.5)\n                for _ in range(local_budget):\n                    if evals >= self.budget:\n                        break\n                    candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = candidate.copy()\n                        # successful local step => shrink trust radius to focus search\n                        trust_radius = max(min_trust, trust_radius * 0.7)\n                        stagnation_counter = 0\n                        no_improve_evals = 0\n                    else:\n                        # unsuccessful local attempt: slightly increase trust to escape\n                        trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # Adapt high-level controls based on successes\n            if successes > max(1, int(0.2 * self.pop_size)):\n                # good generation: reduce exploration (less Lévy)\n                p_levy = max(0.01, p_levy * 0.95)\n                # encourage exploitation F/CR to go toward smaller steps/higher crossover\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.5, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # poor generation: increase chance of long jumps\n                p_levy = min(0.5, p_levy * 1.05 + 0.005)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n\n            # stagnation handling: if many evals without improvement, re-seed a fraction\n            if no_improve_evals > max(50, self.pop_size * 5):\n                stagnation_counter += 1\n                # reinitialize half of population\n                k = max(1, self.pop_size // 2)\n                idxs = rng.choice(self.pop_size, k, replace=False)\n                for j in idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < self.f_opt:\n                        self.f_opt = fvals[j]\n                        self.x_opt = pop[j].copy()\n                # enlarge trust radius to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                no_improve_evals = 0\n\n            # small global diversification if strong stagnation\n            if stagnation_counter > 5:\n                # re-seed a smaller portion\n                k = max(1, self.pop_size // 4)\n                idxs = rng.choice(self.pop_size, k, replace=False)\n                for j in idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < self.f_opt:\n                        self.f_opt = fvals[j]\n                        self.x_opt = pop[j].copy()\n                stagnation_counter = 0\n                trust_radius = min(max_trust, trust_radius * 1.5)\n\n            # Ensure recorded best is current\n            if self.x_opt is None:\n                best_idx = int(np.argmin(fvals))\n                self.x_opt = pop[best_idx].copy()\n                self.f_opt = float(fvals[best_idx])\n\n        # final results\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.508 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2375046492078957, 0.2457694874040265, 0.46051649470712663, 0.9078228875920156, 0.7374416965297812, 0.6240941846820112, 0.3253812855401087, 0.5151009318391908, 0.4224684841045585, 0.6029931728382839]}, "task_prompt": ""}
{"id": "029577d7-a3ee-47e2-bd57-9ba3047f8af7", "fitness": 0.18422672874013016, "name": "AdaptiveHybridDELevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and an adaptive trust-region local search; parameters (F, CR, trust radius, jump chance) adapt online from success history.", "code": "import numpy as np\n\nclass AdaptiveHybridDELevyTrust:\n    \"\"\"\n    Adaptive hybrid optimizer:\n      - Population-based Differential Evolution (jDE-like per-individual F and CR)\n      - Occasional Lévy-like (Cauchy) jumps centered on the best for long-range exploration\n      - Lightweight trust-region local search around the current best; trust radius adapts\n      - Online adaptation of global F_mean and CR_mean using successful trials\n    Designed to work with black-box functions that expose func.bounds.lb and func.bounds.ub\n    (falls back to [-5, 5] if not provided). Respects the evaluation budget strictly.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # Set population size proportional to dim but bounded by budget and practical limits\n        if pop_size is None:\n            self.pop_size = max(8, min(12 * self.dim, max(8, self.budget // 10)))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.default_rng(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_bounds(self, b, fallback):\n        if b is None:\n            return np.asarray(fallback, dtype=float)\n        arr = np.asarray(b, dtype=float)\n        if arr.ndim == 0:\n            # scalar bound to vector\n            return np.full(self.dim, float(arr))\n        if arr.size == 1:\n            return np.full(self.dim, float(arr))\n        return arr.astype(float)\n\n    def __call__(self, func):\n        # Prepare bounds (fall back to [-5,5] if func has no bounds attribute)\n        lb = self._ensure_bounds(getattr(func, \"bounds\", None) and getattr(func.bounds, \"lb\", None), np.full(self.dim, -5.0))\n        ub = self._ensure_bounds(getattr(func, \"bounds\", None) and getattr(func.bounds, \"ub\", None), np.full(self.dim, 5.0))\n        # ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        # initialize population\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n        # Evaluate as many as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            xi = pop[i]\n            try:\n                fv = float(func(xi))\n            except Exception:\n                fv = float(\"inf\")\n            fvals[i] = fv\n            evals += 1\n\n        # If budget exhausted before full population evaluation: trim active population\n        active = self.pop_size\n        if np.isinf(fvals).all():\n            # no evaluations possible\n            self.f_opt = float(\"inf\")\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # Find initial best among evaluated\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # Algorithm hyper-parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.08\n        trust_radius = np.linalg.norm(range_vec) * 0.15  # absolute scale\n        stagnation_counter = 0\n\n        # Helper: Levy-like heavy-tailed step using clipped Cauchy\n        def levy_step(dim):\n            # standard Cauchy gives heavy tails; clip extremes to avoid numeric blow-up\n            s = self.rng.standard_cauchy(dim)\n            s = np.clip(s, -50.0, 50.0)\n            # normalize by typical scale to make step magnitudes interpretable\n            med = np.median(np.abs(s)) + 1e-12\n            return s / med\n\n        # Main loop: iterate generations until budget exhausted\n        gen = 0\n        while evals < self.budget:\n            gen += 1\n            successes_F = []\n            successes_CR = []\n            gen_successes = 0\n\n            # iterate population members (target vectors)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Option 1: Lévy jump centered on current best (exploration)\n                if self.rng.random() < p_levy:\n                    step = levy_step(self.dim)\n                    candidate = best_x + (step * trust_radius) * (range_vec / (np.linalg.norm(range_vec) + 1e-12))\n                    # clip to bounds\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    # evaluate\n                    try:\n                        f_candidate = float(func(candidate))\n                    except Exception:\n                        f_candidate = float(\"inf\")\n                    evals += 1\n\n                    # greedy replacement: replace worst in neighborhood or replace current i if improves\n                    if f_candidate < fvals[i]:\n                        pop[i] = candidate\n                        fvals[i] = f_candidate\n                        successes_F.append(0.5)  # nudge F_mean slightly towards exploration\n                        successes_CR.append(0.5)\n                        gen_successes += 1\n                        if f_candidate < best_f:\n                            best_f = f_candidate\n                            best_x = candidate.copy()\n                            stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                    continue  # next individual\n\n                # Option 2: DE-style mutation + binomial crossover\n                # sample Fi and CRi around means (jDE-like)\n                Fi = self.rng.normal(F_mean, 0.12)\n                Fi = float(np.clip(Fi, 0.05, 0.95))\n                CRi = self.rng.normal(CR_mean, 0.12)\n                CRi = float(np.clip(CRi, 0.0, 1.0))\n\n                # mutation: DE/rand/1\n                idxs = np.arange(self.pop_size)\n                idxs = idxs[idxs != i]\n                a, b, c = self.rng.choice(idxs, 3, replace=False)\n                donor = pop[a] + Fi * (pop[b] - pop[c])\n                # projection of donor to bounds (simple)\n                donor = np.minimum(np.maximum(donor, lb), ub)\n\n                # binomial crossover to produce candidate\n                jrand = self.rng.integers(self.dim)\n                mask = self.rng.random(self.dim) < CRi\n                mask[jrand] = True\n                candidate = pop[i].copy()\n                candidate[mask] = donor[mask]\n                # small trust-region nudges occasionally: if close to best, bias toward best\n                if self.rng.random() < 0.12:\n                    candidate = 0.9 * candidate + 0.1 * best_x\n\n                # clip candidate\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = float(\"inf\")\n                evals += 1\n\n                # selection\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes_F.append(Fi)\n                    successes_CR.append(CRi)\n                    gen_successes += 1\n                    # small adaptation nudges toward successful Fi/CRi handled after generation\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation: adapt means based on successes\n            if len(successes_F) > 0:\n                # Lehmer-like mean for F (gives more weight to larger Fi)\n                sf = np.array(successes_F, dtype=float)\n                F_mean = 0.85 * F_mean + 0.15 * (np.sum(sf * sf) / (np.sum(sf) + 1e-12))\n                F_mean = float(np.clip(F_mean, 0.05, 0.99))\n                CR_mean = 0.85 * CR_mean + 0.15 * (np.mean(successes_CR))\n                CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n            else:\n                # no successes: slightly diversify\n                F_mean = float(np.clip(F_mean * 1.02, 0.05, 0.99))\n                CR_mean = float(np.clip(CR_mean * 0.98, 0.0, 1.0))\n\n            # Trust-region local search around best\n            if evals < self.budget:\n                remaining = self.budget - evals\n                # number of local samples: at most dim, but small and depends on remaining budget\n                n_local = min(max(1, self.dim // 2), remaining, 5)\n                local_improved = False\n                for _ in range(n_local):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic gaussian noise scaled by trust_radius and per-dim range\n                    sigma = (trust_radius / (np.linalg.norm(range_vec) + 1e-12)) * (range_vec + 1e-12)\n                    # random anisotropic scaling for exploration\n                    anis = 0.5 + self.rng.random(self.dim)\n                    candidate = best_x + self.rng.normal(0.0, 1.0, self.dim) * sigma * anis\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    try:\n                        f_candidate = float(func(candidate))\n                    except Exception:\n                        f_candidate = float(\"inf\")\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        local_improved = True\n                        stagnation_counter = 0\n                    # optionally insert into population replacing worst\n                    worst_idx = np.argmax(fvals)\n                    if f_candidate < fvals[worst_idx]:\n                        pop[worst_idx] = candidate\n                        fvals[worst_idx] = f_candidate\n\n                # adapt trust radius based on local improvement\n                if local_improved:\n                    trust_radius = max(1e-9, trust_radius * 0.7)  # focus\n                else:\n                    trust_radius = min(max_trust, trust_radius * 1.15)  # broaden\n\n            # If many successes this generation encourage exploitation, else exploration\n            if gen_successes >= max(1, self.pop_size // 6):\n                p_levy = max(0.01, p_levy * 0.9)\n            else:\n                p_levy = min(0.35, p_levy * 1.08)\n\n            # Stagnation handling: re-seed part of the population if no improvement for a while\n            if stagnation_counter > max(50, self.dim * 6):\n                k = max(1, self.pop_size // 2)\n                for j in self.rng.choice(np.arange(self.pop_size), k, replace=False):\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = float(\"inf\")\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # after reseed, slightly enlarge trust region to allow escape\n                trust_radius = min(max_trust, trust_radius * 1.8)\n                stagnation_counter = 0  # reset after reseeding\n\n            # optional small diversification: occasionally nudge some individuals toward best or random\n            if self.rng.random() < 0.12 and evals < self.budget:\n                j = self.rng.integers(self.pop_size)\n                pop[j] = 0.7 * pop[j] + 0.3 * best_x + 0.05 * self.rng.standard_normal(self.dim) * range_vec\n                pop[j] = np.minimum(np.maximum(pop[j], lb), ub)\n                try:\n                    fvals[j] = float(func(pop[j]))\n                except Exception:\n                    fvals[j] = float(\"inf\")\n                evals += 1\n                if fvals[j] < best_f:\n                    best_f = fvals[j]\n                    best_x = pop[j].copy()\n                    stagnation_counter = 0\n\n            # Keep best tracked strictly\n            current_best_idx = np.argmin(fvals)\n            if fvals[current_best_idx] < best_f:\n                best_f = float(fvals[current_best_idx])\n                best_x = pop[current_best_idx].copy()\n                stagnation_counter = 0\n\n            # break if no budget left\n            if evals >= self.budget:\n                break\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveHybridDELevyTrust scored 0.184 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0848267520659548, 0.1672290249046383, 0.22411646383603934, 0.1700187183737103, 0.1780836621574251, 0.2549180794281083, 0.19814333408430063, 0.24413618826108174, 0.1779537807550523, 0.14284128353499093]}, "task_prompt": ""}
{"id": "1c328aa4-f4f3-48c5-aae7-86bed07d7a22", "fitness": 0.3335908113740746, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with per-individual parameter adaptation, occasional Lévy-flight long jumps for exploration, and an adaptive trust-region local search around the best solution.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=1)\n        f_best, x_best = opt(func)\n    Notes:\n        - The black-box func should accept a 1D numpy array and return a scalar.\n        - If func has attribute `bounds` with `.lb` and `.ub`, they are used.\n          Otherwise the search bounds default to [-5, 5] per-dimension.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: between 4*dim and 10*dim but not exceeding budget/2\n        if pop_size is None:\n            self.pop_size = max(4 * self.dim, min(10 * self.dim, max(20, int(self.budget // 20))))\n        else:\n            self.pop_size = int(pop_size)\n        # RNG\n        self.rng = np.random.default_rng(seed)\n        # results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _get_bounds(self, func):\n        # prefer func.bounds if present\n        if hasattr(func, \"bounds\"):\n            try:\n                lb = np.asarray(func.bounds.lb, dtype=float)\n                ub = np.asarray(func.bounds.ub, dtype=float)\n                # broadcast if singletons provided\n                if lb.size == 1:\n                    lb = np.full(self.dim, float(lb))\n                if ub.size == 1:\n                    ub = np.full(self.dim, float(ub))\n                return lb, ub\n            except Exception:\n                pass\n        # default bounds [-5, 5]\n        return np.full(self.dim, -5.0), np.full(self.dim, 5.0)\n\n    def __call__(self, func):\n        rng = self.rng\n        lb, ub = self._get_bounds(func)\n        # ensure shapes\n        lb = np.asarray(lb, dtype=float).reshape(self.dim)\n        ub = np.asarray(ub, dtype=float).reshape(self.dim)\n        range_vec = ub - lb\n        # initialize storage\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n        evals = 0\n\n        # Evaluate initial population as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i].copy()))\n            except Exception as e:\n                # propagate error but keep consistent counters\n                raise\n            evals += 1\n\n        # If we couldn't evaluate full pop due to tiny budget, reduce pop size effectively\n        if evals < self.pop_size:\n            # keep only evaluated individuals\n            pop = pop[:evals]\n            fvals = fvals[:evals]\n            self.pop_size = evals\n            if self.pop_size == 0:\n                # nothing evaluated; return empty\n                self.x_opt = None\n                self.f_opt = np.inf\n                return self.f_opt, self.x_opt\n\n        # initial best\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n        self.x_opt = best_x.copy()\n        self.f_opt = best_f\n\n        # DE hyper-parameters and adaptive stats\n        F_mean = 0.6\n        CR_mean = 0.2\n        p_levy = 0.08\n        # trust-region parameters (fractions of range)\n        trust_radius = 0.15  # start medium\n        min_trust = 1.0e-4\n        max_trust = 1.5\n        # counters\n        gen = 0\n        stagnation_counter = 0\n        no_improve_evals = 0\n        # bounds for F and CR\n        F_min, F_max = 0.1, 1.0\n\n        # helper: truncated Cauchy-based levy-like step (heavy-tailed)\n        def levy_step(scale=1.0):\n            # Use standard Cauchy (heavy tail); truncate extremes to avoid blow-ups\n            step = rng.standard_cauchy(self.dim)\n            # cap extremes to avoid numerical issues\n            cap = 20.0\n            step = np.clip(step, -cap, cap)\n            # scale and random rotate direction\n            dir_unit = rng.normal(0, 1.0, size=self.dim)\n            dir_unit /= np.linalg.norm(dir_unit) + 1e-12\n            return dir_unit * step * float(scale)\n\n        # main loop\n        # we will treat generations, but obey eval budget strictly\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            prev_best_f = best_f\n            # per-generation indices\n            idxs = np.arange(self.pop_size)\n            # Shuffle order to avoid biases\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[idx].copy()\n                f_target = float(fvals[idx])\n\n                # Choose whether to do a Lévy jump (global exploration) vs DE step\n                if rng.random() < p_levy:\n                    # Levy jump centered on best with trust scaling\n                    scale = trust_radius * np.mean(range_vec)\n                    candidate = best_x + levy_step(scale=scale)\n                    # small isotropic jitter to avoid clones\n                    candidate += rng.normal(0.0, 1e-6, size=self.dim)\n                    # clip to bounds\n                    candidate = np.clip(candidate, lb, ub)\n                    # mark that no Fi/CR used\n                    used_Fi = None\n                    used_CRi = None\n                else:\n                    # Differential Evolution rand/1/bin with jDE-style per-individual F and CR\n                    # pick r1,r2,r3 distinct from idx\n                    choices = np.delete(idxs, idx)\n                    if choices.size < 3:\n                        # not enough individuals -> fallback to random within bounds\n                        candidate = lb + rng.random(self.dim) * range_vec\n                        used_Fi = None\n                        used_CRi = None\n                    else:\n                        r1, r2, r3 = rng.choice(choices, 3, replace=False)\n                        # per-individual parameters drawn around means\n                        Fi = np.clip(rng.normal(F_mean, 0.1), F_min, F_max)\n                        CRi = np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0)\n                        used_Fi = Fi\n                        used_CRi = CRi\n                        # mutation: rand/1\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # incorporate trust-region scaling toward best occasionally\n                        if rng.random() < 0.15:\n                            # bias donor slightly toward best_x proportionally to trust_radius\n                            donor = donor * (1 - 0.2 * trust_radius) + best_x * (0.2 * trust_radius)\n                        # binomial crossover\n                        cr_mask = rng.random(self.dim) < CRi\n                        # ensure at least one dimension crosses\n                        if not np.any(cr_mask):\n                            cr_mask[rng.integers(0, self.dim)] = True\n                        candidate = np.where(cr_mask, donor, x_target)\n                        # clip\n                        candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < f_target:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    # adapt means slightly toward successful Fi/CRi if used\n                    if used_Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * used_Fi\n                    if used_CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * used_CRi\n                    # if it was a Levy jump success, slightly reduce p_levy (we found good region)\n                    if used_Fi is None and rng.random() < 0.5:\n                        p_levy = max(0.01, p_levy * 0.9)\n                else:\n                    # small nudges to increase variability when failing\n                    F_mean = np.clip(F_mean * (1.0 + 0.001), F_min, F_max)\n                    CR_mean = np.clip(CR_mean * (1.0 + 0.001), 0.0, 1.0)\n\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    stagnation_counter = 0\n                    no_improve_evals = 0\n                    # contract trust region if improving (focus search)\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    no_improve_evals += 1\n\n            # end of generation adjustments\n            # trust-region local search around best: consume a small number of extra local evaluations\n            remaining = self.budget - evals\n            if remaining > 0:\n                # sample count: prefer a small handful, scaled with dim and remaining budget\n                n_local = min( max(1, self.dim // 2), remaining, 8 )\n                # anisotropic sigma: base trust_radius scaled per-dim\n                for _ in range(n_local):\n                    if evals >= self.budget:\n                        break\n                    sigma = trust_radius * (0.5 + rng.random(self.dim) * 0.5)\n                    candidate = best_x + rng.normal(0, 1.0, size=self.dim) * (sigma * range_vec)\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    # selection against worst individual to keep pop diverse\n                    worst_idx = int(np.argmax(fvals))\n                    if f_candidate < fvals[worst_idx]:\n                        pop[worst_idx] = candidate\n                        fvals[worst_idx] = f_candidate\n                    # update global best\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        # successful local step => shrink trust radius to focus more\n                        trust_radius = max(min_trust, trust_radius * 0.7)\n                        stagnation_counter = 0\n                        no_improve_evals = 0\n                    else:\n                        # unsuccessful local => expand slightly to escape\n                        trust_radius = min(max_trust, trust_radius * 1.05)\n                        stagnation_counter += 1\n\n            # adapt p_levy based on success or stagnation\n            if successes > max(1, int(0.15 * self.pop_size)):\n                # many successes: decrease long jumps\n                p_levy = max(0.01, p_levy * 0.95)\n                # tighten trust region lightly\n                trust_radius = max(min_trust, trust_radius * 0.95)\n            elif successes == 0:\n                # stagnation handling: increase chance of long jumps and diversifying F_mean slightly\n                p_levy = min(0.5, p_levy * 1.12)\n                F_mean = np.clip(F_mean * 1.02, F_min, F_max)\n                CR_mean = np.clip(CR_mean * 1.02, 0.0, 1.0)\n                stagnation_counter += 1\n            else:\n                # mild success: small decay of p_levy\n                p_levy = max(0.01, p_levy * 0.98)\n\n            # strong stagnation reset\n            if stagnation_counter > max(20, int(2 * self.dim)):\n                # re-seed half of the population randomly to diversify\n                n_reset = max(1, self.pop_size // 2)\n                reset_idxs = rng.choice(self.pop_size, n_reset, replace=False)\n                for j in reset_idxs:\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    # evaluate if budget allows and replace worst if necessary\n                    if evals < self.budget:\n                        f_new = float(func(pop[j].copy()))\n                        evals += 1\n                        fvals[j] = f_new\n                        if f_new < best_f:\n                            best_f = f_new\n                            best_x = pop[j].copy()\n                            self.f_opt = best_f\n                            self.x_opt = best_x.copy()\n                # enlarge trust radius to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n                no_improve_evals = 0\n\n            # safety: ensure bounds on trust_radius\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # update stored bests to population-best to keep selection behavior consistent\n            best_idx = int(np.argmin(fvals))\n            if fvals.size > 0:\n                if fvals[best_idx] < best_f:\n                    best_f = float(fvals[best_idx])\n                    best_x = pop[best_idx].copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n        # End main loop\n        # Ensure we have final best recorded\n        if self.x_opt is None and fvals.size > 0:\n            best_idx = int(np.argmin(fvals))\n            self.x_opt = pop[best_idx].copy()\n            self.f_opt = float(fvals[best_idx])\n\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.334 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10790583883503424, 0.16774557340662188, 0.5162717839105224, 0.24365826979359884, 0.23757067871534265, 0.6081398742758358, 0.2509532433507664, 0.4697214178563168, 0.5840978104871638, 0.14984362310954336]}, "task_prompt": ""}
{"id": "409b3f73-b9cb-4d39-a575-c71321f5561f", "fitness": 0.4479277994741343, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and an online-adaptive trust-region local search (fast exploration + focused exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line idea: Use adaptive DE as a backbone, inject occasional heavy-tailed\n    Lévy steps centered on the best to escape basins, and refine the best\n    with an online-adjusted trust-region local sampler.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but bounded\n        if pop_size is None:\n            pref = int(8 + 2 * np.sqrt(self.dim))\n            self.pop_size = max(6, min(60, pref))\n        else:\n            self.pop_size = int(pop_size)\n        # never have population larger than budget (can't evaluate more individuals than budget)\n        self.pop_size = max(2, min(self.pop_size, max(2, self.budget)))\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.ndim == 0:\n            return np.full(self.dim, b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.ravel()[0], dtype=float)\n        if b.size != self.dim:\n            # try to broadcast: if vector given shorter/longer, reshape if possible\n            return np.resize(b, self.dim).astype(float)\n        return b.astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_mean = max(range_vec.mean(), 1e-12)\n\n        # initialize population uniformly\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fpop = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population but don't exceed budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fpop[i] = float(func(x))\n            evals += 1\n\n        # if we couldn't evaluate any individuals (budget == 0)\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # best so far\n        best_idx = int(np.argmin(fpop))\n        best_x = pop[best_idx].copy()\n        best_f = float(fpop[best_idx])\n\n        # adaptive DE parameters (jDE-like)\n        F_mean = 0.6\n        CR_mean = 0.9\n\n        # trusts and exploration parameters\n        trust_radius = 0.08 * range_mean  # relative to domain\n        trust_min = 1e-6 * range_mean\n        trust_max = 1.0 * range_mean\n        levy_base_prob = 0.03\n        levy_prob = levy_base_prob\n        stagnation_counter = 0\n        stagnation_limit = max(20 * self.dim, 100)\n\n        # helper: Levy-like heavy tails using Cauchy stabilized by tanh\n        def levy_step():\n            s = self.rng.standard_cauchy(size=self.dim)\n            s = np.tanh(s)  # limit extreme outliers while keeping heavy tails\n            # scale to range units\n            return s\n\n        # utility: ensure within bounds with simple reflection-like clipping\n        def project_bounds(x):\n            x = np.array(x, dtype=float)\n            # reflect trick: if out-of-bounds, reflect back into range once\n            below = x < lb\n            above = x > ub\n            x[below] = lb[below] + (lb[below] - x[below])  # reflect\n            x[above] = ub[above] - (x[above] - ub[above])\n            # final clip as fallback\n            x = np.minimum(np.maximum(x, lb), ub)\n            return x\n\n        # main loop: generations until budget exhausted\n        gen = 0\n        # small memory of successful parameters for adaptation\n        w_success_F = []\n        w_success_CR = []\n        while evals < self.budget:\n            gen += 1\n            idxs = np.arange(self.pop_size)\n            self.rng.shuffle(idxs)\n\n            # iterate target vectors\n            for i in idxs:\n                if evals >= self.budget:\n                    break\n\n                xi = pop[i]\n\n                # sample adaptive Fi and CRi\n                Fi = np.clip(self.rng.normal(F_mean, 0.15), 0.05, 1.0)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # choose mutation strategy\n                use_levy = (self.rng.rand() < levy_prob)\n                if use_levy:\n                    # Lévy-centered jump around best: heavy-tailed displacement scaled by trust radius and domain\n                    step = levy_step()\n                    scale = (0.3 + 0.7 * self.rng.rand()) * (trust_radius / (range_mean + 1e-12))\n                    donor = best_x + scale * step * range_vec\n                else:\n                    # Differential Evolution variants: choose three distinct indices\n                    r = self.rng.choice(np.delete(np.arange(self.pop_size), i), size=3, replace=False)\n                    r1, r2, r3 = r\n                    if self.rng.rand() < 0.5:\n                        # DE/rand/1\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    else:\n                        # current-to-best/1\n                        donor = xi + Fi * (best_x - xi) + Fi * (pop[r1] - pop[r2])\n\n                # binomial crossover\n                cr_mask = (self.rng.rand(self.dim) < CRi)\n                if not cr_mask.any():\n                    cr_mask[self.rng.randint(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, xi)\n                trial = project_bounds(trial)\n\n                # evaluate trial (consumes one eval)\n                f_trial = float(func(trial))\n                evals += 1\n\n                # selection\n                if f_trial < fpop[i]:\n                    # success: replace\n                    pop[i] = trial\n                    fpop[i] = f_trial\n                    w_success_F.append(Fi)\n                    w_success_CR.append(CRi)\n                    # move means slightly toward successful parameters (we'll aggregate per generation)\n                    if f_trial < best_f:\n                        best_f = f_trial\n                        best_x = trial.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                else:\n                    # no improvement on this target\n                    stagnation_counter += 1\n\n                # if budget exhausted break\n                if evals >= self.budget:\n                    break\n\n            # generation-end adaptation of F_mean and CR_mean using Lehmer mean-like weighting\n            if w_success_F:\n                # Lehmer mean emphasizes larger Fi values\n                w = np.array(w_success_F)\n                if w.sum() > 0:\n                    F_mean = 0.9 * F_mean + 0.1 * (np.sum(w**2) / (np.sum(w) + 1e-12))\n                else:\n                    F_mean = 0.99 * F_mean + 0.01 * 0.5\n            else:\n                # slightly nudge to encourage exploration if no success\n                F_mean = min(0.95, 1.02 * F_mean + 0.001)\n\n            if w_success_CR:\n                CR_mean = 0.9 * CR_mean + 0.1 * (np.mean(w_success_CR))\n            else:\n                CR_mean = max(0.05, 0.98 * CR_mean)\n\n            # clear success memory for next gen\n            w_success_F = []\n            w_success_CR = []\n\n            # trust-region local search around best: small gaussian perturbations\n            # number of local samples depends on remaining budget and dimension\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(max(1, self.dim // 2), remaining, 8)\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic Gaussian noise: per-dim sigma ~ trust_radius * (0.5 + rand)\n                per_dim_sigma = trust_radius * (0.5 + self.rng.rand(self.dim))\n                x_local = best_x + self.rng.normal(0.0, per_dim_sigma)\n                x_local = project_bounds(x_local)\n                f_local = float(func(x_local))\n                evals += 1\n                if f_local < best_f:\n                    best_f = f_local\n                    best_x = x_local.copy()\n                    local_success += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # adjust trust-radius based on local success\n            if local_success > 0:\n                # successful local search -> shrink trust region (focus)\n                trust_radius = max(trust_min, trust_radius * (0.8 ** local_success))\n                # slightly favor exploitation -> lower levy probability\n                levy_prob = max(0.005, levy_prob * 0.9)\n            else:\n                # no local improvement -> expand trust radius to escape\n                trust_radius = min(trust_max, trust_radius * 1.08)\n                # increase chance of Levy jumps when stagnating\n                levy_prob = min(0.3, levy_prob * 1.05)\n\n            # stagnation handling: if no improvement for many evals, diversify\n            if stagnation_counter >= stagnation_limit:\n                stagnation_counter = 0\n                # re-seed a portion of population (replace worst members)\n                k = max(1, self.pop_size // 2)\n                worst_idx = np.argsort(fpop)[-k:]\n                for j in worst_idx:\n                    if evals >= self.budget:\n                        break\n                    new_x = lb + self.rng.rand(self.dim) * range_vec\n                    new_f = float(func(new_x))\n                    evals += 1\n                    pop[j] = new_x\n                    fpop[j] = new_f\n                    if new_f < best_f:\n                        best_f = new_f\n                        best_x = new_x.copy()\n                # after reset, enlarge trust radius and increase levy prob to encourage exploration\n                trust_radius = min(trust_max, max(trust_radius, 0.2 * range_mean))\n                levy_prob = min(0.5, levy_prob * 1.5)\n                # also slightly randomize F_mean to escape bias\n                F_mean = min(0.9, max(0.1, F_mean * (0.8 + 0.4 * self.rng.rand())))\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.448 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15205306531728102, 0.20307434315401607, 0.38798514324859856, 0.800210529120067, 0.7395686284602578, 0.9454960385025953, 0.3287978384752084, 0.43202818235812535, 0.2986178717926711, 0.19144635431252377]}, "task_prompt": ""}
{"id": "b86b42f7-8ab4-421c-85e6-8231d5a94e54", "fitness": 0.7060722492679999, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online step-size adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line: Adaptive DE with occasional Lévy global jumps and a shrinking/growing trust-region local search.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # default population scales with dimension but limited by budget\n        if pop_size is None:\n            # try to keep population useful but not too large relative to budget\n            self.pop_size = int(min(max(8, 6 * self.dim), max(4, self.budget // 10)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.default_rng(self.seed)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.ndim == 0:\n            return np.full(self.dim, float(b))\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size != self.dim:\n            # attempt broadcasting\n            return np.resize(b.astype(float), (self.dim,))\n        return b.astype(float)\n\n    def __call__(self, func):\n        # bounds (Many BBOB provides func.bounds.lb / ub)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        norm_range = np.linalg.norm(range_vec)\n        # safety: if range is zero (degenerate) set small positive\n        if norm_range == 0:\n            norm_range = 1.0\n\n        # If budget extremely small: do simple random sampling and return best\n        if self.budget <= 0:\n            return self.f_opt, self.x_opt\n\n        # If budget smaller than population, sample only budget many points uniformly\n        if self.budget < self.pop_size:\n            # limited evaluations only\n            fbest = np.inf\n            xbest = None\n            for _ in range(self.budget):\n                x = lb + self.rng.random(self.dim) * range_vec\n                f = func(x)\n                if f < fbest:\n                    fbest = f\n                    xbest = x.copy()\n            self.f_opt = fbest\n            self.x_opt = xbest\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = func(pop[i])\n            evals += 1\n\n        # If we ran out of budget during initialization\n        if evals >= self.budget:\n            # find best among evaluated\n            idx = np.argmin(fvals)\n            self.f_opt = float(fvals[idx])\n            self.x_opt = pop[idx].copy()\n            return self.f_opt, self.x_opt\n\n        # initialize strategy/adaptation parameters\n        F_mean = 0.5\n        CR_mean = 0.9\n        tau_F = 0.1  # probability to mutate F in jDE style\n        tau_CR = 0.1\n        trust_radius = 0.2 * norm_range  # scalar trust radius\n        p_levy = 0.05  # initial chance of performing a Lévy jump instead of DE\n        stagnation_counter = 0\n        stagnation_threshold = max(20, 20 * self.dim)\n        gen = 0\n\n        # helpers\n        def levy_step(scale=1.0):\n            # Use Cauchy (heavy-tailed) as a simple Lévy-like step, clipped for stability\n            s = self.rng.standard_cauchy(self.dim)\n            s = np.clip(s, -10.0, 10.0)\n            # normalize to avoid extremely small magnitude and scale by given factor\n            norm = np.linalg.norm(s)\n            if norm == 0:\n                s = self.rng.normal(0, 1.0, self.dim)\n                norm = max(1e-8, np.linalg.norm(s))\n            s = s / norm\n            # add random amplitude drawn from a Pareto-like tail using power law\n            amp = (1.0 + self.rng.pareto(1.5))  # heavy tail amplitude\n            return s * amp * scale\n\n        # global best\n        best_idx = int(np.argmin(fvals))\n        f_best = float(fvals[best_idx])\n        x_best = pop[best_idx].copy()\n\n        # main loop: run generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # one generation: iterate over population members\n            indices = np.arange(self.pop_size)\n            self.rng.shuffle(indices)\n            improved_in_gen = False\n            for i in indices:\n                if evals >= self.budget:\n                    break\n\n                # Adapt individual parameters (jDE-like)\n                if self.rng.random() < tau_F:\n                    Fi = 0.1 + 0.8 * self.rng.random()\n                else:\n                    # normal jitter around mean\n                    Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 0.95)\n                if self.rng.random() < tau_CR:\n                    CRi = self.rng.random()\n                else:\n                    CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # Decide whether to do Lévy centered exploration (global jump) or DE\n                if self.rng.random() < p_levy:\n                    # Lévy-based candidate around best with some random scaling\n                    scale = trust_radius / (1.0 + gen * 0.05)  # gradually dampen amplitude across generations\n                    step = levy_step(scale=scale)\n                    candidate = x_best + step * (range_vec / max(1.0, np.linalg.norm(range_vec)))\n                    # occasionally combine with small differential vector to keep diversity\n                    if self.rng.random() < 0.3:\n                        r1, r2 = self.rng.choice(self.pop_size, size=2, replace=False)\n                        candidate += Fi * (pop[r1] - pop[r2]) * 0.1\n                else:\n                    # DE/rand/1 with binomial crossover (target is pop[i])\n                    # pick r1,r2,r3 distinct from i\n                    others = [idx for idx in range(self.pop_size) if idx != i]\n                    r1, r2, r3 = self.rng.choice(others, size=3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    jrand = self.rng.integers(0, self.dim)\n                    mask = self.rng.random(self.dim) < CRi\n                    mask[jrand] = True\n                    candidate = np.where(mask, donor, pop[i])\n                    # small local exploitation bias pushing toward best with small prob\n                    if self.rng.random() < 0.15:\n                        candidate = candidate * 0.85 + x_best * 0.15\n\n                # projection/clipping to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = func(candidate)\n                evals += 1\n\n                # Selection: greedy replacement (DE style)\n                if f_candidate < fvals[i]:\n                    # accept candidate\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    # nudge global parameter means toward successful params (simple exponential moving avg)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    improved_in_gen = True\n                    # if improvement was substantial, slightly reduce trust radius to focus\n                    if f_candidate < f_best - 1e-12:\n                        # local improvement\n                        trust_radius = max(1e-8, trust_radius * 0.90)\n                        stagnation_counter = 0\n                    else:\n                        # small move, mild contraction\n                        trust_radius = max(1e-8, trust_radius * 0.97)\n                else:\n                    # unsuccessful -> slightly increase diversity\n                    trust_radius = min(2.0 * norm_range, trust_radius * 1.002)\n\n                # update global best if needed\n                if f_candidate < f_best:\n                    f_best = float(f_candidate)\n                    x_best = candidate.copy()\n                    # successful global improvement reduces Levy chance\n                    p_levy = max(0.01, p_levy * 0.6)\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # clear local Fi/CRi (just let them go out of scope)\n\n            # End of generation: perform trust-region local search near current best\n            # Number of local samples limited by remaining budget and dimension\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            n_local = min( max(1, int(self.dim // 2)), remaining, 8 )\n            # anisotropic sigma: base trust_radius scaled per-dimension\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # per-dim sigma: trust_radius times small random anisotropy / sqrt(dim)\n                per_dim_scale = (0.3 + 0.7 * self.rng.random(self.dim))\n                sigma = (trust_radius / max(1.0, np.sqrt(self.dim))) * per_dim_scale\n                local_candidate = x_best + self.rng.normal(0.0, 1.0, self.dim) * sigma\n                local_candidate = np.clip(local_candidate, lb, ub)\n                f_local = func(local_candidate)\n                evals += 1\n                if f_local < f_best:\n                    # successful local step: accept and shrink trust region\n                    f_best = float(f_local)\n                    x_best = local_candidate.copy()\n                    trust_radius = max(1e-9, trust_radius * 0.85)\n                    # gently reduce Levy frequency since we're exploiting successfully\n                    p_levy = max(0.01, p_levy * 0.7)\n                    stagnation_counter = 0\n                    improved_in_gen = True\n                else:\n                    # unsuccessful local sample: slightly expand trust radius to escape\n                    trust_radius = min(2.0 * norm_range, trust_radius * 1.03)\n                    stagnation_counter += 1\n\n            # Adjust exploration parameters based on stagnation\n            if stagnation_counter > stagnation_threshold:\n                # strong stagnation: increase global exploration and re-seed half pop\n                p_levy = min(0.6, p_levy * 1.5)\n                num_reseed = max(1, self.pop_size // 2)\n                for j in range(num_reseed):\n                    if evals >= self.budget:\n                        break\n                    idx = self.rng.integers(0, self.pop_size)\n                    pop[idx] = lb + self.rng.random(self.dim) * range_vec\n                    fvals[idx] = func(pop[idx])\n                    evals += 1\n                # enlarge trust radius to escape basin\n                trust_radius = min(2.0 * norm_range, trust_radius * 1.5)\n                stagnation_counter = 0\n                # adjust means to encourage exploration\n                F_mean = np.clip(F_mean * 0.9 + 0.1 * self.rng.random(), 0.05, 0.95)\n                CR_mean = np.clip(CR_mean * 0.9 + 0.1 * self.rng.random(), 0.0, 1.0)\n\n            # Insure we keep a record of the best in pop if we found better while reseeding\n            idx = int(np.argmin(fvals))\n            if fvals[idx] < f_best:\n                f_best = float(fvals[idx])\n                x_best = pop[idx].copy()\n\n            # decay/increase p_levy slowly based on recent progress\n            if not improved_in_gen:\n                # no improvement in generation -> encourage more Lévy jumps\n                p_levy = min(0.5, p_levy * 1.05)\n            else:\n                # progress -> slightly favor DE exploitation\n                p_levy = max(0.01, p_levy * 0.98)\n\n        # finished budget or loop\n        self.f_opt = float(f_best)\n        self.x_opt = x_best.copy() if x_best is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.706 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.24369199054875612, 0.23684996082603715, 0.505462462130592, 0.9462896583199361, 0.9056584325551437, 0.9090692023771285, 0.8594419287899415, 0.6843223239356291, 0.9015275779325783, 0.8684089552642569]}, "task_prompt": ""}
{"id": "14224501-7481-481d-99cf-129e2fc9608e", "fitness": 0.6117411600948257, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style self-adaptation, occasional Lévy (Cauchy) long jumps, and an adaptive trust-region local search around the current best to balance fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best.\n    Online adaptation of F and CR (jDE-like) and trust radius based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # Population size heuristic: scale with dim, but do not exceed a fraction of budget.\n        if pop_size is None:\n            base = max(8, 4 + 2 * self.dim)\n            # keep population reasonable relative to budget\n            pop_size = int(min(base, max(4, self.budget // 8)))\n        self.pop_size = max(1, int(pop_size))\n        # state holders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # Try to read bounds from func if available, else default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            # ensure arrays of correct length\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item())\n            lb = lb.reshape(self.dim)\n            ub = ub.reshape(self.dim)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def __call__(self, func):\n        if self.budget <= 0:\n            return np.inf, None\n\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        # initialize dynamic parameters\n        evals = 0\n        # If budget is tiny, reduce population accordingly\n        pop = min(self.pop_size, max(1, self.budget // 3))\n        # random initial population (clip if budget too small to evaluate all)\n        X = self.rng.uniform(lb, ub, size=(pop, self.dim))\n        Fvals = np.full(pop, np.inf, dtype=float)\n        # per-individual self-adaptive parameters (jDE-style)\n        F_mean = 0.6\n        CR_mean = 0.5\n        Fi = np.clip(self.rng.normal(F_mean, 0.1, size=pop), 0.01, 0.95)\n        CRi = np.clip(self.rng.normal(CR_mean, 0.1, size=pop), 0.0, 1.0)\n\n        # evaluate initial population until budget allows\n        for i in range(pop):\n            if evals >= self.budget:\n                break\n            x = X[i]\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            Fvals[i] = f\n            evals += 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n                last_improve_at = evals\n\n        # if no evaluations were possible (budget==0 handled earlier), return\n        if evals == 0:\n            return self.f_opt, self.x_opt\n\n        # Levy/Cauchy helper (vectorized)\n        def levy_step():\n            # Use Cauchy (heavy-tailed) steps; limit extremes to avoid numerical issues\n            step = self.rng.standard_cauchy(size=self.dim)\n            # scale by 0.1*span per-dim and clip\n            scale = 0.6 * span  # allow long jumps relative to range, scaled by trust_radius later\n            step = np.clip(step, -50.0, 50.0)  # hard clamp\n            return step * scale * 0.02  # default moderate magnitude\n\n        # dynamic controls\n        trust_radius = 0.2  # relative fraction of span (0-1)\n        levy_prob = 0.06  # base chance of long jump\n        stagnation_counter = 0\n        best_since_reset = self.f_opt\n        # parameters for adaptation\n        adapt_lr = 0.15\n        gen = 0\n\n        # main loop: iterate until budget exhausted\n        # We'll go generation by generation (one pass over population)\n        while evals < self.budget:\n            gen += 1\n            success_flags_F = []\n            success_F_values = []\n            success_CR_values = []\n            success_count = 0\n            # permutation for sequential processing to avoid bias\n            order = np.arange(pop)\n            self.rng.shuffle(order)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n                target = X[idx]\n                target_f = Fvals[idx]\n                # adapt per-individual parameters (jDE-style random perturb)\n                # occasionally re-draw Fi/CRi\n                if self.rng.rand() < 0.1:\n                    Fi[idx] = np.clip(self.rng.standard_cauchy() * 0.05 + F_mean, 0.01, 0.95)\n                if self.rng.rand() < 0.1:\n                    CRi[idx] = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                f_i = Fi[idx]\n                cr_i = CRi[idx]\n\n                # Mutation: DE/rand/1\n                # pick r1,r2,r3 distinct from idx\n                if pop >= 4:\n                    choices = [i for i in range(pop) if i != idx]\n                    r1, r2, r3 = self.rng.choice(choices, size=3, replace=False)\n                    donor = X[r1] + f_i * (X[r2] - X[r3])\n                else:\n                    # fallback: gaussian perturb\n                    donor = target + f_i * self.rng.normal(size=self.dim) * span\n\n                # Occasional Levy jump centered on best for exploration\n                if self.rng.rand() < levy_prob:\n                    # Levy step scaled by trust_radius\n                    lev = levy_step() * (0.5 + trust_radius)\n                    donor = (self.x_opt + lev)\n                    # nudge Fi/CR to encourage exploration\n                    f_i = min(0.95, f_i * (1.0 + 0.5 * self.rng.rand()))\n                    cr_i = max(0.0, cr_i * 0.5)\n\n                # crossover binomial\n                cross = self.rng.rand(self.dim) < cr_i\n                if not np.any(cross):\n                    cross[self.rng.randint(0, self.dim)] = True\n                trial = np.where(cross, donor, target)\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # Evaluate trial\n                if evals >= self.budget:\n                    break\n                try:\n                    f_trial = func(trial)\n                except Exception:\n                    f_trial = np.inf\n                evals += 1\n\n                # Selection greedy\n                if f_trial < target_f:\n                    # successful replacement\n                    X[idx] = trial\n                    Fvals[idx] = f_trial\n                    success_count += 1\n                    success_flags_F.append(True)\n                    success_F_values.append(f_i)\n                    success_CR_values.append(cr_i)\n                    # update global best immediately\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        last_improve_at = evals\n                        stagnation_counter = 0\n                else:\n                    success_flags_F.append(False)\n\n                # small online adaptation of means toward successful Fi/CRi\n                # We'll do aggregated update at generation end using success lists\n\n                # break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation: update F_mean and CR_mean towards successful params\n            if success_count > 0:\n                # weighted average update\n                mean_F_succ = np.mean(success_F_values) if success_F_values else F_mean\n                mean_CR_succ = np.mean(success_CR_values) if success_CR_values else CR_mean\n                F_mean = (1.0 - adapt_lr) * F_mean + adapt_lr * mean_F_succ\n                CR_mean = (1.0 - adapt_lr) * CR_mean + adapt_lr * mean_CR_succ\n                # tighten trust radius slightly if many successes\n                trust_radius = max(0.02, trust_radius * (0.9 if success_count > pop * 0.2 else 0.98))\n            else:\n                # no success -> encourage exploration\n                trust_radius = min(2.0, trust_radius * 1.08)\n                levy_prob = min(0.5, levy_prob * 1.05)\n                stagnation_counter += 1\n\n            # Trust-region local search around best: sample a small handful\n            remaining = self.budget - evals\n            # number of local samples scaled by dimension and remaining budget, but small\n            n_local = 1 + min(max(1, self.dim // 3), remaining // max(1, (self.dim // 2)))\n            n_local = min(n_local, remaining)\n            local_successes = 0\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian noise around best\n                sigma = trust_radius * (0.8 + 0.4 * self.rng.rand(self.dim)) * span\n                local = self.x_opt + self.rng.normal(scale=sigma)\n                local = np.minimum(np.maximum(local, lb), ub)\n                try:\n                    f_local = func(local)\n                except Exception:\n                    f_local = np.inf\n                evals += 1\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = local.copy()\n                    local_successes += 1\n                    stagnation_counter = 0\n                # adapt trust radius: successful local steps shrink (focus), failures expand (explore)\n                if local_successes > 0:\n                    trust_radius = max(0.01, trust_radius * 0.7)\n                else:\n                    trust_radius = min(3.0, trust_radius * 1.07)\n\n            # Adjust levy probability based on stagnation\n            if stagnation_counter > max(5, 2 * self.dim):\n                levy_prob = min(0.5, levy_prob * 1.15)\n            else:\n                # slowly decay back to base\n                levy_prob = max(0.02, levy_prob * 0.995)\n\n            # Strong stagnation reset: if no improvement for a long time, re-seed half population\n            if stagnation_counter > max(50, 10 * self.dim):\n                num_reseed = max(1, pop // 2)\n                idxs = self.rng.choice(pop, size=num_reseed, replace=False)\n                for ii in idxs:\n                    X[ii] = self.rng.uniform(lb, ub)\n                    try:\n                        Fvals[ii] = func(X[ii])\n                    except Exception:\n                        Fvals[ii] = np.inf\n                    evals += 1\n                    # prevent overshoot budget\n                    if evals >= self.budget:\n                        break\n                trust_radius = min(2.0, trust_radius * 1.5)\n                levy_prob = min(0.7, levy_prob * 2.0)\n                stagnation_counter = 0\n\n            # safety: if improvement seen recently, slightly reduce exploration and encourage exploitation\n            if evals - locals().get('last_improve_at', 0) < max(5, self.dim):\n                F_mean = max(0.02, F_mean * 0.98)\n                CR_mean = min(0.95, CR_mean * 1.02)\n\n            # small population maintenance: if we have spare budget and small population, add random individuals\n            if pop < self.pop_size and (self.budget - evals) > 2 * self.dim:\n                add = min(self.pop_size - pop, max(1, (self.budget - evals) // (5 * self.dim)))\n                if add > 0:\n                    newX = self.rng.uniform(lb, ub, size=(add, self.dim))\n                    newF = np.full(add, np.inf)\n                    for j in range(add):\n                        if evals >= self.budget:\n                            newF[j] = np.inf\n                        else:\n                            try:\n                                newF[j] = func(newX[j])\n                            except Exception:\n                                newF[j] = np.inf\n                            evals += 1\n                            if newF[j] < self.f_opt:\n                                self.f_opt = newF[j]\n                                self.x_opt = newX[j].copy()\n                    X = np.vstack([X, newX])\n                    Fvals = np.concatenate([Fvals, newF])\n                    Fi = np.concatenate([Fi, np.clip(self.rng.normal(F_mean, 0.1, size=add), 0.01, 0.95)])\n                    CRi = np.concatenate([CRi, np.clip(self.rng.normal(CR_mean, 0.1, size=add), 0.0, 1.0)])\n                    pop = X.shape[0]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.612 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17739561041252583, 0.17009463659250335, 0.5237503300255502, 0.9484682393641141, 0.9226905990160036, 0.9386215434368497, 0.4127003778839369, 0.49153217596531107, 0.7024628907950463, 0.8296951974564158]}, "task_prompt": ""}
{"id": "8ecad622-9914-4f24-991c-15c8be3e1222", "fitness": 0.19981539443129828, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and an online-adaptive trust-region local search for robust global-to-local search.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension and budget but kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # make sure population not larger than a fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # internal state for results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # get bounds and prepare\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_mean = max(range_vec.mean(), 1e-12)\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (careful with budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # if no evals possible\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # determine current best\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n        best_idx = np.argmin(fvals[valid])\n        best_indices = np.where(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius multiplier\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # limit extreme outliers to avoid numerical blow-up\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # adapt per-generation randomness to encourage exploration early\n            gen_factor = 1.0 / (1.0 + 0.01 * gen)\n\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                used_DE = False\n                Fi = None\n                CRi = None\n\n                # create donor vector\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    # dynamic scale: combine trust radius and global range\n                    per_dim_scale = (trust_radius / (range_mean + 1e-12)) * (step_scale * (0.5 + 0.5 * rng.rand()))\n                    donor = best_x + per_dim_scale * step * (range_vec / (range_mean + 1e-12))\n                else:\n                    # DE/rand/1-like mutation with jDE-style adaptation sampling\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # not enough other individuals -> random restart for this slot\n                        donor = lb + rng.rand(self.dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                        CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = rng.rand(self.dim) < CRi\n                        cr_mask[rng.randint(0, self.dim)] = True  # ensure at least one gene from donor\n                        trial = np.where(cr_mask, donor, pop[i])\n                        donor = trial\n                        used_DE = True\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # one evaluation\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate <= fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # move means slightly toward successful parameters (if DE used)\n                    if used_DE and Fi is not None and CRi is not None:\n                        # small adaptation step\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    else:\n                        # Lévy success nudges exploration parameters slightly\n                        F_mean = 0.97 * F_mean + 0.03 * 0.6\n                        CR_mean = 0.97 * CR_mean + 0.03 * 0.9\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful improvement => shrink trust radius to focus\n                    trust_radius *= 0.90\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                else:\n                    stagnation_counter += 1\n\n                # clear local Fi/CRi from scope if set to avoid reuse\n                Fi = None\n                CRi = None\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by random per-dimension factors\n                sigma = trust_radius * (0.3 + 0.7 * rng.rand(self.dim)) / (range_mean + 1e-12)\n                candidate_local = best_x + sigma * rng.randn(self.dim)\n                candidate_local = np.clip(candidate_local, lb, ub)\n                f_candidate = float(func(candidate_local))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate_local.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter += 1\n\n            # adapt exploration/exploitation probabilities and means based on success\n            if successes > 0:\n                # small reduction on Levy prob if successful exploitation occurred\n                p_levy = max(0.01, p_levy * (0.95 if successes > max(1, self.pop_size * 0.2) else 0.98))\n                # slightly move means towards typical good values\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # no successes -> increase chance of Levy jumps to escape stagnation\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                # slightly increase mutation strength\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 1.01, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, 5 * self.dim):\n                k = max(1, self.pop_size // 2)\n                # reinitialize up to k individuals (but ensure budget)\n                idxs = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset, encourage exploration\n                trust_radius = min(max_trust, max(trust_radius * 1.8, 1e-12))\n                stagnation_counter = 0\n                # nudge means to encourage exploration after reset\n                F_mean = np.clip(0.6 + 0.4 * rng.rand(), 0.05, 0.99)\n                CR_mean = np.clip(0.4 + 0.6 * rng.rand(), 0.0, 1.0)\n                p_levy = min(0.4, p_levy + 0.05)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.200 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1143136491704565, 0.1733370456836224, 0.29876325296231576, 0.16449713392500898, 0.2161156965523766, 0.26249388002695995, 0.21675812878939493, 0.2184459099754127, 0.1898321313947966, 0.14359711583263834]}, "task_prompt": ""}
{"id": "dd63135c-e0a1-4b60-b333-ea136832f8ac", "fitness": 0.23136427769729062, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size/parameter adaptation for robust global-to-local search.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but constrained\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        # safety: if bounds not provided or degenerate, fall back to [-5,5]\n        if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # find best among evaluated individuals\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.nanargmin(np.where(valid, fvals, np.inf))\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # no evals possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        # initial trust radius (fraction of search size)\n        trust_radius = np.linalg.norm(range_vec) * 0.08\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # standard Cauchy heavy-tailed steps, clipped to avoid numerical blow-up\n            s = rng.standard_cauchy(self.dim)\n            # compress extremes smoothly\n            s = np.tanh(s / 10.0) * 10.0\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # For each target vector in the population create a trial\n            # Use sequential processing so each evaluation counts towards budget\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # choose operator: Levy jump with probability p_levy, else DE mutation + crossover\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    # scale relative to trust radius and search range\n                    scale = rng.uniform(0.05, 0.6) * (trust_radius / max(1e-12, np.linalg.norm(range_vec)))\n                    candidate = best_x + scale * step * range_vec\n                    Fi = None\n                    CRi = None\n                else:\n                    # sample Fi and CRi around the running means (simple jDE-like)\n                    Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                    # pick three distinct indices different from i\n                    idxs = list(range(self.pop_size))\n                    idxs.pop(i)\n                    r = rng.choice(idxs, 3, replace=False)\n                    r1, r2, r3 = r[0], r[1], r[2]\n\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    candidate = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # one evaluation (respect budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                # if candidate is better than individual's current, replace\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # small adaptation of means toward successful Fi/CRi (if applicable)\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # if budget exhausted, break\n                if evals >= self.budget:\n                    break\n\n            # After processing population, perform trust-region local sampling (few samples)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: small handful scaled with dim and remaining budget\n            local_samples = min(max(1, int(max(1, self.dim // 3))), remaining)\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (0.2 + rng.rand(self.dim) * 0.8) * (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.75)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slightly expand to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.06)\n                    stagnation_counter += 1\n\n            # adapt exploration/exploitation probabilities and parameter means\n            # reduce levy probability if many successes, otherwise slowly increase\n            if successes > max(1, int(0.2 * self.pop_size)):\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                p_levy = min(0.5, p_levy * 1.02 + 0.001)\n            # gently nudge F_mean and CR_mean toward reasonable defaults\n            F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n            CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n\n            # stagnation handling: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, 5 * self.dim):\n                k = max(1, self.pop_size // 2)\n                indices = rng.choice(self.pop_size, k, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset and reduce levy to avoid endless random jumps\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                p_levy = min(0.5, p_levy * 1.1)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.231 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12462133266321951, 0.16886763497647972, 0.3409017001379552, 0.2601957128072838, 0.21592901498397044, 0.3322692210664392, 0.2341156432397099, 0.2525269575117083, 0.21804896470785684, 0.16616659487828378]}, "task_prompt": ""}
{"id": "3aaf4f9f-6890-49dc-904c-65c3e37a9cab", "fitness": 0.2184063979002652, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining population-based Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size adaptation and restarts.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line: Adaptive hybrid combining DE for structured exploration,\n    Lévy jumps for global escapes, and trust-region local search for focused exploitation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # population size: sensible default scaling with dim, clipped\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # make sure population not larger than a fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.ndim == 0:\n            return np.full(self.dim, float(b), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast to dimension\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be interpreted for dimension {}\".format(self.dim))\n\n    def __call__(self, func):\n        rng = self.rng\n\n        # read bounds from provided func (BBOB style)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        norm_range = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n        evals = 0\n\n        # evaluate initial population sequentially (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if no evaluations possible, return defaults\n        if not np.isfinite(fvals).any():\n            self.f_opt = float(np.inf)\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # get current best from evaluated individuals\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump per trial\n        step_scale = 0.25   # base scale for Lévy and trust radius (fraction of range)\n        min_trust = 1e-6\n        max_trust = norm_range * 2.0\n        trust_radius = max(min_trust, step_scale * norm_range)\n\n        gen = 0\n        stagnation_counter = 0\n        successes = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # normalize heavy-tailed step to unit scale median-like\n            med = np.median(np.abs(s)) + 1e-12\n            return s / med\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes_this_gen = 0\n\n            # adapt per-generation randomization of F and CR around their means\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi per individual (jDE-like)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # decide whether to perform a Levy jump instead of DE for this trial\n                if rng.random() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    # scale by trust radius fraction of range\n                    scale = (trust_radius / norm_range) * step_scale\n                    candidate = best_x + step * scale * range_vec\n                else:\n                    # Differential Evolution mutation + binomial crossover\n                    # pick three distinct indices different from i\n                    idxs = np.arange(self.pop_size)\n                    if self.pop_size > 3:\n                        choices = rng.choice(idxs[idxs != i], 3, replace=False)\n                    else:\n                        # fallback when tiny populations\n                        choices = rng.choice(idxs, 3, replace=True)\n                    r1, r2, r3 = choices\n                    # mix mutation strategies: rand/1 and current-to-best/1\n                    if rng.random() < 0.3:\n                        # current-to-best/1: encourages exploitation\n                        donor = pop[i] + Fi * (best_x - pop[i]) + Fi * (pop[r1] - pop[r2])\n                    else:\n                        # rand/1: exploration\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    jrand = rng.integers(0, self.dim)\n                    mask = rng.random(self.dim) < CRi\n                    mask[jrand] = True\n                    candidate = np.where(mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate (counting the eval)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    successes_this_gen += 1\n                    # move means slightly toward successful parameters (online adaptation)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples (small handful depending on dim and remaining budget)\n            local_samples = int(min(max(1, self.dim // 3), remaining, max(1, self.pop_size // 3)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: per-dimension randomization scaled by trust_radius\n                sigma = (0.5 + rng.random(self.dim) * 0.5) * (trust_radius / norm_range)\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    # reward exploitation by reducing p_levy slightly\n                    p_levy = max(0.01, p_levy * 0.95)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.12)\n                    stagnation_counter += 1\n\n            # generation-level adaptation\n            if successes_this_gen > max(1, self.pop_size // 10):\n                # many successes: focus more on exploitation slightly\n                p_levy = max(0.01, p_levy * 0.95)\n                F_mean = np.clip(0.98 * F_mean + 0.02 * 0.5, 0.05, 0.99)\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase diversity\n                p_levy = min(0.6, p_levy * 1.07 + 0.005)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                idxs = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # possibly update best\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, max(trust_radius * 1.5, step_scale * norm_range))\n                stagnation_counter = 0\n                # encourage exploration after reset\n                p_levy = min(0.5, p_levy * 1.2 + 0.02)\n                # slightly relax means to encourage diversity\n                F_mean = np.clip(F_mean * 0.98 + 0.02 * 0.7, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98 + 0.02 * 0.5, 0.0, 1.0)\n\n            # safe-guard trust radius bounds\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # small random jitter to means to avoid lock-in\n            F_mean = np.clip(F_mean + rng.normal(0, 0.002), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + rng.normal(0, 0.002), 0.0, 1.0)\n\n        # final results stored\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.218 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10378660569044951, 0.16170978687186865, 0.3184159403740634, 0.36126449876090405, 0.2137621234161856, 0.22604014340023215, 0.21953992618306062, 0.2206753556513561, 0.2063639379417378, 0.15250566071279403]}, "task_prompt": ""}
{"id": "2fe56012-6a5b-41b2-bb85-82bd3d733019", "fitness": 0.22313382820043248, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with per-individual parameter adaptation, occasional Lévy-flight long jumps, and an online trust-region local search around the current best.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Adaptive hybrid optimizer:\n      - Differential Evolution (rand/1/bin) with per-individual Fi/CRi sampling and jDE-like online adaptation.\n      - Occasional Lévy-like (Cauchy) jumps centered on the current best for long-range exploration.\n      - Trust-region Gaussian local search around the current best for focused exploitation, with adaptive trust radius.\n      - Stagnation-driven population reseeding.\n    Usage: instantiate with budget and dim; call with a black-box func that exposes func.bounds.lb/ub.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        if pop_size is None:\n            # scale population with dimension but keep reasonable bounds\n            self.pop_size = max(8, min(80, int(8 + 3 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # determine bounds (fallback to [-5,5] if not provided)\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        range_vec = ub - lb\n        # avoid zero-range issues\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        rng = self.rng\n\n        # initialize population\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # Evaluate as many as budget allows from initial population\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # if no evaluations possible (budget == 0)\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters and adaptive state\n        F_mean = 0.6\n        CR_mean = 0.5\n        p_levy = 0.08\n        trust_radius = 0.08 * range_norm  # absolute scale for trust region\n        min_trust = 1e-8 * range_norm\n        max_trust = 2.0 * range_norm\n\n        # bookkeeping\n        best_idx = np.argmin(fvals)\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n        stagnation_counter = 0\n\n        gen = 0\n        # stochastic helper: Lévy-like heavy-tailed step using Cauchy\n        def levy_step(dim):\n            s = rng.standard_cauchy(size=dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main generation loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # stores of successful Fi/CRi for adaptation\n            S_F = []\n            S_CR = []\n\n            # Per-generation slight damping for p_levy drift\n            p_levy = max(0.0, min(0.5, p_levy * 0.995))\n\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual parameters (jDE-like)\n                Fi = float(np.clip(rng.normal(F_mean, 0.15), 0.05, 1.2))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.18), 0.0, 1.0))\n\n                # Decide to do a Lévy jump (centered on best_x) or DE mutation\n                if rng.rand() < p_levy:\n                    # long-range jump: combine a Cauchy (heavy tail) and trust radius for scaling\n                    step = levy_step(self.dim)\n                    scale = trust_radius / (range_norm + 1e-12)\n                    donor = best_x + (scale * step) * range_vec\n                else:\n                    # standard DE/rand/1 mutation (use three distinct indices excluding i)\n                    idxs = list(range(self.pop_size))\n                    if self.pop_size <= 3:\n                        r1 = r2 = r3 = i\n                    else:\n                        idxs.remove(i)\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cross_mask = rng.rand(self.dim) < CRi\n                # ensure at least one dimension taken from donor\n                if not np.any(cross_mask):\n                    cross_mask[rng.randint(0, self.dim)] = True\n                trial = np.where(cross_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    S_F.append(Fi)\n                    S_CR.append(CRi)\n\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                else:\n                    # no improvement on this individual's slot\n                    stagnation_counter += 1\n\n                # If time, we can do an immediate small local trust adjustment on success\n                # (we keep it simple and manage trust at generation end)\n\n            # End of population loop: adapt F_mean and CR_mean using successful params\n            if len(S_F) > 0:\n                # Lehmer mean for Fi (gives larger weight to larger F)\n                S_F = np.array(S_F)\n                new_F = (np.sum(S_F**2) / (np.sum(S_F) + 1e-12))\n                F_mean = 0.8 * F_mean + 0.2 * np.clip(new_F, 0.05, 1.2)\n            else:\n                # slightly drift F_mean to encourage larger steps when no successes\n                F_mean = min(1.2, F_mean * 1.01)\n\n            if len(S_CR) > 0:\n                # simple mean for CR\n                new_CR = float(np.mean(S_CR))\n                CR_mean = 0.85 * CR_mean + 0.15 * new_CR\n            else:\n                CR_mean = max(0.02, CR_mean * 0.995)\n\n            # Adjust p_levy depending on stagnation: if many gens without improvement increase chance\n            if stagnation_counter > max(5, self.dim // 2):\n                p_levy = min(0.5, p_levy * 1.06 + 0.01)\n            else:\n                p_levy = max(0.02, p_levy * 0.995)\n\n            # Trust-region local search around best (sample a few Gaussian perturbations)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic noise scaled by trust_radius (relative to range)\n                sigma = trust_radius / (range_norm + 1e-12)\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # adapt trust radius: shrink on local improvements, expand on failures\n            if local_success > 0:\n                trust_radius *= 0.8 ** local_success\n            else:\n                trust_radius *= 1.06\n            trust_radius = max(min_trust, min(max_trust, trust_radius))\n\n            # Strong stagnation handling: partial reseed of population\n            if stagnation_counter > max(50, self.dim * 6):\n                # reinitialize half the population (or at least one), evaluate as budget allows\n                num_reseed = max(1, self.pop_size // 2)\n                for _ in range(num_reseed):\n                    if evals >= self.budget:\n                        break\n                    j = rng.randint(0, self.pop_size)\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # slightly enlarge trust radius after reseed to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n\n            # Safety: keep a valid best from population if someone has better fvals\n            cur_best_idx = np.argmin(fvals)\n            if fvals[cur_best_idx] < best_f:\n                best_f = float(fvals[cur_best_idx])\n                best_x = pop[cur_best_idx].copy()\n\n            # update global outputs if improved\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n            else:\n                # if no better than previously stored, keep previous best stored in object\n                # but update object values to current best\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n            # small protection to avoid extreme trust radius drift\n            trust_radius = max(min_trust, min(max_trust, trust_radius))\n\n        # final return\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.223 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12511399449799576, 0.1531737856954587, 0.3765277701529488, 0.15757650620308583, 0.2659189918649356, 0.30536181145715513, 0.21669226139328834, 0.2478106649966627, 0.24391925445649099, 0.13924324128630283]}, "task_prompt": ""}
{"id": "88ac5157-c6fa-43a7-ad5e-512f9385afa8", "fitness": 0.1820868375483952, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and an online-adaptive trust-region local search — balances fast global exploration with focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Online adaptation\n    of DE parameters (F, CR) and trust radius is used based on recent\n    successes. Designed for continuous box-bounded problems.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population size: scale with dimension but limited by budget\n        if pop_size is None:\n            # baseline population related to dim but not exceeding budget/4\n            self.pop_size = max(4, min(10 * self.dim, max(4, self.budget // 8)))\n        else:\n            self.pop_size = int(max(4, pop_size))\n        self.seed = seed\n\n        # Results placeholders\n        self.f_opt = None\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        # bounds handling: try using func.bounds if available (BBOB style),\n        # else default to [-5, 5] per-dimension\n        try:\n            lb = np.atleast_1d(np.array(func.bounds.lb, dtype=float))\n            ub = np.atleast_1d(np.array(func.bounds.ub, dtype=float))\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast scalar bounds to vector if necessary\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # check shapes\n        assert lb.size == self.dim and ub.size == self.dim, \"Bounds must match dimension\"\n\n        range_vec = ub - lb\n        # prevent degenerate ranges\n        eps = 1e-12\n        range_vec = np.where(range_vec < eps, 1.0, range_vec)\n\n        # Initialize population uniformly in bounds\n        pop = rng.random((self.pop_size, self.dim)) * range_vec + lb\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate initial population sequentially (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # If no evaluations possible, return None\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # determine initial best among evaluated\n        evaluated_mask = np.isfinite(fvals)\n        best_idx = np.argmin(fvals[evaluated_mask])\n        best_indices = np.where(evaluated_mask)[0]\n        best_idx = best_indices[best_idx]\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        # Algorithm hyper-parameters (adaptive)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # base probability of Levy jump\n        step_scale = 0.25   # base multiplier for Levy and trust radius\n\n        # Trust-region parameters (relative to range_vec)\n        trust_radius = 0.1 * np.mean(range_vec)  # initial absolute trust radius\n        min_trust = 1e-6 * np.mean(range_vec)\n        max_trust = np.mean(range_vec) * 0.8\n\n        stagnation_counter = 0\n        no_improve_evals = 0\n\n        gen = 0\n        # helper: generate Levy-like heavy-tailed step using Cauchy approximation\n        def levy_step():\n            # Cauchy generates heavy tails. Scale and clip extremes.\n            s = rng.standard_cauchy(self.dim)\n            # scale to the problem range\n            s = s / np.maximum(1.0, np.abs(s) / 10.0)  # limit extreme outliers\n            return s * (step_scale * np.maximum(range_vec, 1e-8))\n\n        # helpers for bounds projection (reflective)\n        def reflect_bounds(x):\n            # Reflective projection keeps points in [lb, ub]\n            xr = x.copy()\n            below = xr < lb\n            xr[below] = lb[below] + (lb[below] - xr[below])\n            above = xr > ub\n            xr[above] = ub[above] - (xr[above] - ub[above])\n            # second pass clipping to ensure numeric stability\n            xr = np.minimum(np.maximum(xr, lb), ub)\n            return xr\n\n        # store recent successful Fi and CRi for moving averages (like jDE)\n        success_Fs = []\n        success_CRs = []\n\n        # main loop: generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # Per-generation small perturbation to means to encourage diversity\n            F_mean = np.clip(F_mean * (1.0 + rng.normal(0.0, 0.02)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + rng.normal(0.0, 0.02), 0.0, 1.0)\n\n            # Process each target individual sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                xi = pop[i].copy()\n                fi = fvals[i]\n\n                # Sample Fi and CRi per-individual (jDE-like)\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                # Decide whether to do a Levy jump, DE mutation, or DE/best style\n                do_levy = rng.random() < p_levy\n                do_best = rng.random() < 0.15  # sometimes use best-guided mutation\n\n                if do_levy:\n                    # long-range exploratory jump centered on best_x with Cauchy-like step\n                    step = levy_step()\n                    candidate = best_x + step\n                else:\n                    # Differential Evolution donor creation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback to random reinitialization\n                        candidate = rng.random(self.dim) * range_vec + lb\n                    else:\n                        r = rng.choice(idxs, size=3, replace=False)\n                        r1, r2, r3 = r\n                        if do_best:\n                            donor = pop[r1] + Fi * (best_x - pop[r2])\n                        else:\n                            donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                        # Binomial crossover\n                        cr_mask = rng.random(self.dim) < CRi\n                        # Ensure at least one dimension is taken from donor\n                        if not np.any(cr_mask):\n                            cr_mask[rng.integers(0, self.dim)] = True\n                        candidate = np.where(cr_mask, donor, xi)\n\n                # Local trust-region micro-search: with small probability try small gaussian around xi\n                if rng.random() < 0.03 and evals + 1 < self.budget:\n                    local_sigma = (trust_radius / max(1e-12, np.mean(range_vec))) * range_vec\n                    local_step = rng.normal(0.0, 1.0, size=self.dim) * local_sigma\n                    candidate = xi + local_step\n\n                # Protect against numerical extremes and project to bounds\n                candidate = np.asarray(candidate, dtype=float)\n                # if any nan or inf, reinitialize candidate\n                if not np.isfinite(candidate).all():\n                    candidate = rng.random(self.dim) * range_vec + lb\n\n                candidate = reflect_bounds(candidate)\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement\n                replaced = False\n                if f_candidate <= fi:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    replaced = True\n\n                # Update success archives and adapt means if replaced\n                if replaced:\n                    successes += 1\n                    success_Fs.append(Fi)\n                    success_CRs.append(CRi)\n                    # Move global means slightly toward successful Fi/CRi\n                    F_mean = np.clip(0.9 * F_mean + 0.1 * Fi, 0.05, 0.99)\n                    CR_mean = np.clip(0.9 * CR_mean + 0.1 * CRi, 0.0, 1.0)\n                else:\n                    # slight decay of means to maintain exploration\n                    F_mean = np.clip(0.995 * F_mean + 0.0005, 0.05, 0.99)\n                    CR_mean = np.clip(0.995 * CR_mean + 0.0002, 0.0, 1.0)\n\n                # Update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    no_improve_evals = 0\n                    # shrink trust radius to focus exploitation\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    no_improve_evals += 1\n\n                # occasional small cleanup to avoid memory blow-up of success arrays\n                if len(success_Fs) > 200:\n                    success_Fs = success_Fs[-100:]\n                if len(success_CRs) > 200:\n                    success_CRs = success_CRs[-100:]\n\n                # If we exhausted budget inside loop, break\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n\n            # If we had some successes, update the means using Lehmer-like averaging (gives weight to larger Fs)\n            if success_Fs:\n                # Lehmer mean for F: sum(F^2)/sum(F)\n                Fs = np.array(success_Fs)\n                try:\n                    lehmer = np.sum(Fs * Fs) / (np.sum(Fs) + 1e-12)\n                    F_mean = np.clip(0.8 * F_mean + 0.2 * lehmer, 0.05, 0.99)\n                except Exception:\n                    pass\n                CRs = np.array(success_CRs)\n                CR_mean = np.clip(0.8 * CR_mean + 0.2 * np.mean(CRs), 0.0, 1.0)\n                # clear successes for next generation\n                success_Fs.clear()\n                success_CRs.clear()\n\n            # Trust-region local search phase: spend a small budget to intensify around best\n            remaining = self.budget - evals\n            if remaining > 0:\n                # number of local samples is small and scales with dim and spare budget\n                local_samples = min(max(1, self.dim // 2), max(1, remaining // max(1, 10 - min(9, gen))))\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma per-dimension\n                    sigma = (trust_radius / max(1e-12, np.mean(range_vec))) * range_vec\n                    local_step = rng.normal(0.0, 1.0, size=self.dim) * sigma\n                    candidate = best_x + local_step\n                    candidate = reflect_bounds(candidate)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step => shrink trust radius to focus\n                        trust_radius = max(min_trust, trust_radius * 0.7)\n                        stagnation_counter = 0\n                        no_improve_evals = 0\n                    else:\n                        # unsuccessful => expand a bit to escape local traps\n                        trust_radius = min(max_trust, trust_radius * 1.05)\n                        stagnation_counter += 1\n                        no_improve_evals += 1\n\n            # Stagnation handling: if too many evaluations without improvement, increase Levy probability and diversify\n            if no_improve_evals > max(50, 5 * self.dim):\n                p_levy = min(0.5, p_levy * 1.15 + 0.02)\n                # re-seed half of the population randomly to diversify (but keep best)\n                num_repop = max(1, self.pop_size // 2)\n                replace_indices = rng.choice([i for i in range(self.pop_size) if not np.all(pop[i] == best_x)], size=num_repop, replace=False)\n                for idx in replace_indices:\n                    if evals >= self.budget:\n                        break\n                    pop[idx] = rng.random(self.dim) * range_vec + lb\n                    fvals[idx] = float(func(pop[idx]))\n                    evals += 1\n                    # update global best if needed\n                    if fvals[idx] < best_f:\n                        best_f = fvals[idx]\n                        best_x = pop[idx].copy()\n                        trust_radius = max(min_trust, trust_radius * 0.7)\n                        no_improve_evals = 0\n                # slightly enlarge trust radius after reset to encourage escapes\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # reset stagnation counters partially\n                no_improve_evals = 0\n\n            # Gentle decay of levy chance if we're improving well\n            if successes > max(1, self.pop_size // 8):\n                p_levy = max(0.01, p_levy * 0.95)\n\n        # Final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.182 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1016426346685948, 0.15159686666993677, 0.23890124521909284, 0.19737402046844932, 0.19786823581151936, 0.20221642960880237, 0.20603294813881745, 0.20007513345397088, 0.18216171667191605, 0.14299914477285192]}, "task_prompt": ""}
{"id": "fe97331e-db7d-4d53-a438-71d630d1af03", "fitness": 0.24090194751236355, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with per-individual parameter adaptation, occasional Lévy-flight long jumps for global exploration, and a trust-region Gaussian local search around the current best (fast global search + focused local refinement).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # bounds: Many BBOB problems are on [-5,5] per dimension\n        self.lb = np.full(self.dim, -5.0)\n        self.ub = np.full(self.dim, 5.0)\n        self.range_vec = self.ub - self.lb\n        self.range_norm = max(1e-12, np.linalg.norm(self.range_vec))\n\n        # population size default: scale with dimension but keep reasonable and related to budget\n        if pop_size is None:\n            default_ps = int(8 + 2 * np.sqrt(self.dim))\n            default_ps = max(8, min(60, default_ps))\n            default_ps = min(default_ps, max(4, self.budget // 20))\n            self.pop_size = max(4, default_ps)\n        else:\n            self.pop_size = max(4, int(pop_size))\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _levy_step(self):\n        # simple heavy-tail via standard Cauchy, clipped to avoid numerical blowups\n        s = self.rng.standard_cauchy(size=self.dim)\n        s = np.clip(s, -1e3, 1e3)\n        # scale by a factor relative to the search range\n        return s * (0.3 * (self.range_norm / np.sqrt(self.dim)) / (np.mean(np.abs(s)) + 1e-12))\n\n    def __call__(self, func):\n        # if budget is nonpositive just return initial defaults\n        if self.budget <= 0:\n            return self.f_opt, self.x_opt\n\n        # allow using func.bounds if provided (but default to [-5,5])\n        try:\n            lb_external = np.array(func.bounds.lb, dtype=float)\n            ub_external = np.array(func.bounds.ub, dtype=float)\n            if lb_external.size == self.dim and ub_external.size == self.dim:\n                self.lb = lb_external.copy()\n                self.ub = ub_external.copy()\n                self.range_vec = self.ub - self.lb\n                self.range_norm = max(1e-12, np.linalg.norm(self.range_vec))\n        except Exception:\n            # ignore and keep default bounds\n            pass\n\n        rng = self.rng\n        npop = self.pop_size\n\n        # initialize population uniformly in bounds\n        pop = self.lb + rng.rand(npop, self.dim) * self.range_vec\n        fvals = np.full(npop, np.inf)\n        valid = np.zeros(npop, dtype=bool)\n\n        evals = 0\n\n        # Evaluate initial population as budget allows\n        for i in range(npop):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            try:\n                f = float(func(x))\n            except Exception:\n                # On unexpected errors, set inf and continue; do not consume more budget\n                f = np.inf\n            fvals[i] = f\n            valid[i] = True\n            evals += 1\n\n        # If no evaluated individuals (budget exhausted), pick any random as x_opt (no evaluations)\n        if not np.any(valid):\n            self.f_opt = np.inf\n            self.x_opt = pop[rng.randint(0, npop)].copy()\n            return self.f_opt, self.x_opt\n\n        # initial best\n        best_idx = np.argmin(fvals[valid])\n        valid_indices = np.nonzero(valid)[0]\n        best_idx = valid_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover mean\n        p_levy = 0.08       # initial probability of a Lévy jump\n        trust_radius = 0.2 * self.range_norm  # scalar trust radius\n        min_trust = 1e-6\n        max_trust = 2.0 * self.range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        # Precompute indices for sampling\n        idxs = np.arange(npop)\n\n        # Main loop: generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            gen_best_improved = False\n\n            # adapt per-generation exploration noise amplitude\n            for i in range(npop):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi (jDE-like but simple)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 0.99))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # choose three distinct indices different from i\n                choices = np.setdiff1d(idxs, np.array([i], dtype=int))\n                if choices.size < 3:\n                    # degenerate small-pop case: skip\n                    continue\n                r1, r2, r3 = rng.choice(choices, 3, replace=False)\n\n                # mutation: DE/rand/1 variant scaled by range\n                donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # With probability p_levy, perform a Lévy-centered jump from the global best\n                if rng.rand() < p_levy:\n                    # leverage heavy-tail step to escape basins\n                    levy = self._levy_step()\n                    # scale by trust radius but keep within bounds scale\n                    donor = best_x + (levy * (trust_radius / (self.range_norm + 1e-12)))\n\n                # binomial crossover -> candidate\n                candidate = pop[i].copy()\n                jrand = rng.randint(0, self.dim)\n                mask = rng.rand(self.dim) < CRi\n                mask[jrand] = True  # ensure at least one from donor\n                candidate[mask] = donor[mask]\n\n                # enforce bounds\n                candidate = np.minimum(np.maximum(candidate, self.lb), self.ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate <= fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    valid[i] = True\n                    successes += 1\n                    # nudge means towards successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # small decay to avoid overconfidence if failing\n                    F_mean = 0.995 * F_mean + 0.005 * 0.5\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    gen_best_improved = True\n                    stagnation_counter = 0\n                    # on improvement, shrink trust region to focus exploitation\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    stagnation_counter += 1\n\n            # end of generation adjustments\n            # encourage exploration pressure when few successes\n            if successes > 0:\n                # move parameter means slightly toward exploitation (conservative)\n                F_mean = np.clip(0.92 * F_mean + 0.08 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.92 * CR_mean + 0.08 * 0.9, 0.0, 1.0)\n                # when many successes, slightly reduce chance of Levy jumps\n                p_levy = max(0.01, p_levy * (0.98 if successes > max(1, npop // 10) else 0.995))\n            else:\n                # stagnation: increase chance of long jumps and diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                trust_radius = min(max_trust, trust_radius * 1.06)\n\n            # periodic trust-region local search around current best if budget allows\n            # allocate a small adaptive number of local samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # decide how many local samples to evaluate: a small handful\n            local_budget = min( max(1, self.dim // 2), remaining, max(1, npop // 6) )\n            # use anisotropic Gaussian noise scaled by trust_radius and problem range\n            for _ in range(local_budget):\n                # anisotropic sigma per-dimension: random fraction of trust_radius normalized by range\n                frac = 0.2 + rng.rand(self.dim) * 0.8\n                sigma = frac * (trust_radius / (self.range_norm + 1e-12))\n                candidate = best_x + rng.randn(self.dim) * sigma\n                candidate = np.minimum(np.maximum(candidate, self.lb), self.ub)\n\n                # evaluate\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    gen_best_improved = True\n                    stagnation_counter = 0\n                    # shrink trust region if successful local step\n                    trust_radius = max(min_trust, trust_radius * 0.75)\n                else:\n                    # unsuccessful local step => relax/shrink less aggressively\n                    trust_radius = np.clip(trust_radius * 1.02, min_trust, max_trust)\n\n                if evals >= self.budget:\n                    break\n\n            # stagnation handling: if many evaluations since last improvement, perform partial reseed\n            if stagnation_counter > max(50, self.dim * 10):\n                # reseed half population to diversify\n                k = max(1, npop // 2)\n                replace_idx = rng.choice(npop, k, replace=False)\n                for j in replace_idx:\n                    pop[j] = self.lb + rng.rand(self.dim) * self.range_vec\n                    try:\n                        fnew = float(func(pop[j]))\n                    except Exception:\n                        fnew = np.inf\n                    fvals[j] = fnew\n                    valid[j] = True\n                    evals += 1\n                    if fnew < best_f:\n                        best_f = fnew\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                    if evals >= self.budget:\n                        break\n                # expand trust radius a bit after reseed\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # reduce Levy probability a bit to allow DE to exploit new diversity\n                p_levy = min(0.5, p_levy * 0.7)\n                # reset stagnation counter moderately\n                stagnation_counter = 0\n\n            # additional small safeguard: if trust_radius collapses to zero-ish, reset a bit\n            if trust_radius < 1e-8:\n                trust_radius = max(1e-6, 0.1 * self.range_norm)\n\n            # enforce bounds for trust radius\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n            # if budget nearly exhausted, break\n            if evals >= self.budget:\n                break\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.241 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.24320177088666473, 0.26755183861592147, 0.42130777710867795, 0.17476113661812487, 0.16691036782815338, 0.2972983254738699, 0.17885596128311998, 0.18462287303470415, 0.20670125045870136, 0.2678081738156973]}, "task_prompt": ""}
{"id": "b7624896-e25b-4d36-a742-398a52206217", "fitness": 0.37235676861004696, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online step-size / parameter adaptation for robust exploration-exploitation on continuous problems.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Main design points:\n    - DE-style population search with jDE-like online adaptation of F and CR.\n    - Occasional Lévy (Cauchy-like) jumps centered at the best to escape large basins.\n    - Small trust-region local search around the current best to exploit.\n    - Budget-aware: never calls func() more than self.budget times.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # default population size grows with dimension but stays reasonable\n        default_pop = max(4, 10 + int(1.5 * self.dim))\n        # do not allow population larger than a fraction of the budget\n        max_pop_by_budget = max(4, self.budget // 20)\n        self.pop_size = int(pop_size) if pop_size is not None else min(default_pop, max_pop_by_budget)\n\n        # final results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # obtain bounds if available, otherwise default to [-5, 5] per-dimension\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure bounds have correct shape\n        if lb.ndim == 0:\n            lb = np.full(self.dim, float(lb))\n        if ub.ndim == 0:\n            ub = np.full(self.dim, float(ub))\n\n        range_vec = (ub - lb)\n        # protection if someone gives degenerate bounds\n        range_vec[range_vec == 0.0] = 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n        # Evaluate as many initial individuals as budget allows\n        to_eval = min(self.pop_size, self.budget)\n        for i in range(to_eval):\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        if evals == 0:\n            # no budget to evaluate anything\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # If we couldn't evaluate all initial individuals, leave the rest as inf\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        min_trust = 1e-6\n        trust_radius = 0.25  # relative (0..1) trust-region radius\n        levy_prob = 0.04     # chance of making a Lévy jump in mutation\n        stagnation_counter = 0\n        last_improve_eval = evals\n\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (limited)\n        def levy_step():\n            # Cauchy-like heavy-tailed step, limited to avoid numerical blow-up\n            step = rng.standard_cauchy(size=self.dim)\n            # clamp extreme outliers and scale moderately\n            step = np.clip(step, -6.0, 6.0) * 0.15\n            return step\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # process population in random order each generation\n            order = rng.permutation(self.pop_size)\n            successes = 0\n\n            for i in order:\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual F and CR around current means (like jDE)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.2))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # choose three distinct indices a,b,c != i\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                if len(idxs) < 3:\n                    # fallback: sample with replacement\n                    a, b, c = rng.randint(0, self.pop_size, size=3)\n                    while a == i:\n                        a = rng.randint(0, self.pop_size)\n                else:\n                    a, b, c = rng.choice(idxs, 3, replace=False)\n\n                # DE/rand/1 donor\n                donor = pop[a] + Fi * (pop[b] - pop[c])\n\n                # small probability to bias mutation toward best (current-to-best)\n                if rng.rand() < 0.28:\n                    donor = pop[i] + Fi * (best_x - pop[i]) + Fi * (pop[b] - pop[c])\n\n                # occasional Lévy jump centered on the best (global exploration)\n                if rng.rand() < levy_prob:\n                    step = levy_step() * range_vec\n                    # scale Lévy jump magnitude by trust radius (if trust small, allow larger leaps)\n                    scale = max(1.0, 0.5 + 1.5 * (1.0 - trust_radius))\n                    donor = best_x + step * scale\n\n                # binomial crossover\n                cross_mask = rng.rand(self.dim) < CRi\n                # ensure at least one dimension is crossed\n                jrand = rng.randint(self.dim)\n                cross_mask[jrand] = True\n                trial = np.where(cross_mask, donor, pop[i])\n\n                # trust-region effect: damp moves that go far from best when trust_radius small\n                # (pulls candidate toward best if trust is small)\n                trial = best_x + (trial - best_x) * np.clip(trust_radius, min_trust, 1.0)\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate trial (respect budget)\n                f_candidate = float(func(trial))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate <= fvals[i]:\n                    pop[i] = trial\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # move means slightly toward successful parameters (simple exponential smoothing)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # update global best\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = trial.copy()\n                        stagnation_counter = 0\n                        last_improve_eval = evals\n                else:\n                    # unsuccessful: slight drift to encourage exploration\n                    F_mean = min(1.2, F_mean * 1.0005 + 0.0001)\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments: small trust-region local search around best\n            remaining = self.budget - evals\n            if remaining > 0:\n                local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma: trust_radius scaled per-dimension\n                    sigma = max(min_trust, trust_radius)\n                    candidate = best_x + rng.normal(0, sigma, size=self.dim) * range_vec\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step => shrink trust radius to focus\n                        trust_radius = max(min_trust, trust_radius * 0.7)\n                        stagnation_counter = 0\n                        last_improve_eval = evals\n                        successes += 1\n                    else:\n                        # unsuccessful => slightly expand to try to escape\n                        trust_radius = min(1.0, trust_radius * 1.05)\n\n            # adapt exploration intensity based on success / stagnation\n            if successes > 0:\n                # more exploitation: reduce levy probability, shrink trust slightly\n                levy_prob = max(0.01, levy_prob * 0.95)\n                trust_radius = max(min_trust, trust_radius * 0.98)\n            else:\n                # stagnation handling: increase chance of long jumps and diversity\n                levy_prob = min(0.5, levy_prob * 1.15 + 0.01)\n                trust_radius = min(1.0, trust_radius * 1.05)\n                stagnation_counter += 1\n\n            # strong stagnation reset: re-seed part of population if no improvement for long\n            if (evals - last_improve_eval) > max(100, self.budget // 10):\n                k = max(1, self.pop_size // 2)\n                idxs = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in idxs:\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    # evaluate re-seeded individuals while respecting budget\n                    if evals < self.budget:\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                    else:\n                        fvals[j] = np.inf\n                # recompute best after reseed\n                best_idx = int(np.argmin(fvals))\n                best_f = float(fvals[best_idx])\n                best_x = pop[best_idx].copy()\n                # slightly enlarge trust radius to encourage exploration after reset\n                trust_radius = min(1.0, trust_radius * 2.0)\n                stagnation_counter = 0\n                last_improve_eval = evals\n\n            # keep the parameter means in sensible bounds\n            F_mean = float(np.clip(F_mean, 0.05, 1.2))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n        # final results\n        self.f_opt = best_f\n        self.x_opt = best_x\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.372 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12403125675646365, 0.1516546831445611, 0.6641266410257218, 0.4591637199336458, 0.8566719806889626, 0.6478641239138093, 0.2805927872631402, 0.22868237605687292, 0.15455031569997035, 0.15622980161732247]}, "task_prompt": ""}
{"id": "fd66bde1-7b7a-4c3f-98dd-a2f52f5f8c43", "fitness": 0.39729480983086557, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution (jDE-like adaptation), occasional Lévy/Cauchy long jumps for global exploration, and an adaptive trust-region local search around the current best for focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n     - Differential Evolution (DE/rand/1/bin) with simple online adaptation of F and CR (jDE-like),\n     - Occasional Lévy-like (Cauchy) jumps centered on the best for long-range exploration,\n     - An adaptive trust-region local search around the current best with step-size adaptation,\n     - Stagnation detection and partial re-seeding.\n\n    Designed for continuous optimization in box [-5,5]^dim by default (works with other box bounds\n    if func.bounds exists and provides lb/ub).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # default population size if not provided: depends on dimension but bounded by budget\n        if pop_size is None:\n            pop_guess = 10 + 2 * self.dim\n        else:\n            pop_guess = int(pop_size)\n        # keep population reasonable relative to budget\n        self.pop_size = int(min(max(4, pop_guess), max(4, self.budget // 10)))\n\n        # internal state to return later\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Determine bounds: prefer func.bounds if available, else use [-5,5] each dim.\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item())\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        # Initialize population uniformly in bounds\n        pop = self.rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n\n        # helper to evaluate while respecting budget\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None\n            # ensure array is float and shaped\n            x = np.asarray(x, dtype=float)\n            # call function and increment count\n            f = float(func(x))\n            evals += 1\n            return f\n\n        # Evaluate initial population (may be truncated by budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            f = safe_eval(pop[i])\n            if f is not None:\n                fvals[i] = f\n\n        # If some individuals were not evaluated due to tiny budget, leave them at inf.\n        valid = fvals < np.inf\n        if np.any(valid):\n            best_idx = np.argmin(fvals)\n            best_f = float(fvals[best_idx])\n            best_x = pop[best_idx].copy()\n        else:\n            # No evals possible; return a random point (but per problem, budget>=1 typical)\n            rnd = self.rng.uniform(lb, ub)\n            f = safe_eval(rnd)\n            if f is None:\n                return float(np.inf), np.zeros(self.dim)\n            return f, rnd\n\n        # Algorithm hyper-parameters (adapted online)\n        p_levy = 0.08  # initial probability of a Lévy jump\n        F_mean = 0.6\n        CR_mean = 0.3\n\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # initial trust radius\n        trust_radius = max(trust_radius, 1e-6)\n\n        stagnation_counter = 0\n        gen = 0\n\n        # Generate Levy-like heavy-tailed step using Cauchy (simple and robust)\n        def levy_step(scale=1.0):\n            # Cauchy-like heavy tail; clip extreme outliers for numerical stability\n            step = self.rng.standard_cauchy(self.dim)\n            step = np.clip(step, -1e3, 1e3)\n            return scale * step\n\n        # Main loop: generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            successful_Fs = []\n            successful_CRs = []\n            # per-generation small random jitter to means to encourage exploration\n            F_mean = np.clip(F_mean + self.rng.normal(0.0, 0.01), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + self.rng.normal(0.0, 0.02), 0.0, 1.0)\n\n            # iterate individuals sequentially (each trial consumes an eval)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Sample Fi and CRi jDE-like: mostly around means but random occasionally\n                if self.rng.random() < 0.1:\n                    Fi = np.clip(self.rng.random() * 0.9 + 0.05, 0.05, 0.99)\n                else:\n                    Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 0.99)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # Decide between Levy jump (global) or DE (local/global mix)\n                if self.rng.random() < p_levy:\n                    # Lévy/Cauchy jump centered at best_x, scaled by trust_radius and ranges\n                    scale = 0.3 * trust_radius / (1.0 + np.linalg.norm(range_vec))\n                    candidate = best_x + levy_step(scale=scale) * range_vec\n                    # small random perturbation to some dims\n                    mask = self.rng.random(self.dim) < 0.2\n                    if not np.any(mask):  # ensure at least one dimension changes\n                        mask[self.rng.integers(0, self.dim)] = True\n                    candidate = np.where(mask, candidate, pop[i])\n                else:\n                    # DE/rand/1 mutation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(idxs) < 3:\n                        # fallback: perturb current individual if population tiny\n                        mutant = pop[i] + Fi * self.rng.normal(0, 1.0, self.dim) * (range_vec)\n                    else:\n                        r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                        mutant = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cross = self.rng.random(self.dim) < CRi\n                    if not np.any(cross):\n                        cross[self.rng.integers(0, self.dim)] = True\n                    candidate = np.where(cross, mutant, pop[i])\n\n                # Projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # Evaluate candidate\n                f_candidate = safe_eval(candidate)\n                if f_candidate is None:\n                    # budget exhausted exactly\n                    break\n\n                # Selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    successful_Fs.append(Fi)\n                    successful_CRs.append(CRi)\n                # Update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation: adapt means using successes\n            if successful_Fs:\n                # move F_mean toward mean of successful Fs (robust)\n                medF = float(np.median(successful_Fs))\n                F_mean = np.clip(0.9 * F_mean + 0.1 * medF, 0.05, 0.99)\n            else:\n                # slowly decay F_mean toward 0.6 if no success\n                F_mean = np.clip(0.98 * F_mean + 0.02 * 0.6, 0.05, 0.99)\n\n            if successful_CRs:\n                meanCR = float(np.mean(successful_CRs))\n                CR_mean = np.clip(0.9 * CR_mean + 0.1 * meanCR, 0.0, 1.0)\n            else:\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.2, 0.0, 1.0)\n\n            # Trust-region local search around best: small anisotropic Gaussian samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, 3 + self.dim // 10))\n            improved_local = False\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                per_dim_sigma = (0.3 + self.rng.random(self.dim) * 0.7) * (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + self.rng.normal(0.0, 1.0, self.dim) * per_dim_sigma * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = safe_eval(candidate)\n                if f_candidate is None:\n                    break\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local => shrink trust radius to focus\n                    trust_radius = max(1e-8, trust_radius * 0.6)\n                    improved_local = True\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slight expansion to escape basin\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n                    stagnation_counter += 1\n\n            # adjust global p_levy based on stagnation: more jumps if stagnating, else fewer\n            if stagnation_counter > max(5, 2 * self.dim):\n                p_levy = min(0.5, p_levy + 0.02)\n            else:\n                p_levy = max(0.01, p_levy * 0.995)\n\n            # if many successes in generation, slightly encourage exploitation (reduce trust radius)\n            if successes >= max(1, self.pop_size // 6):\n                trust_radius = max(1e-8, trust_radius * 0.9)\n            else:\n                # otherwise slowly increase a bit to explore\n                trust_radius = min(max_trust, trust_radius * 1.02)\n\n            # strong stagnation reset: if no improvement for a long time, re-seed half of population\n            if stagnation_counter > max(200, 20 * self.dim):\n                # reinitialize half of the population (worst half) randomly\n                reorder = np.argsort(fvals)\n                to_reinit = reorder[self.pop_size // 2 :]\n                for idx in to_reinit:\n                    if evals >= self.budget:\n                        break\n                    pop[idx] = self.rng.uniform(lb, ub)\n                    fvals[idx] = safe_eval(pop[idx]) if evals < self.budget else np.inf\n                # enlarge trust radius a bit to escape\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n                # nudge means to encourage exploration\n                F_mean = np.clip(0.8 * F_mean + 0.2 * 0.7, 0.05, 0.99)\n                CR_mean = np.clip(0.8 * CR_mean + 0.2 * 0.4, 0.0, 1.0)\n\n            # update run-best state for return\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n        # final return\n        # If for some reason no best_x set, pick best from fvals\n        if self.x_opt is None or np.isnan(self.f_opt):\n            valid = fvals < np.inf\n            if np.any(valid):\n                idx = np.argmin(fvals)\n                self.f_opt = float(fvals[idx])\n                self.x_opt = pop[idx].copy()\n            else:\n                # evaluate a random point if nothing else\n                rnd = self.rng.uniform(lb, ub)\n                f = safe_eval(rnd)\n                self.f_opt = float(f) if f is not None else np.inf\n                self.x_opt = rnd\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.397 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11892361006537877, 0.17759870685543078, 0.5656195750780244, 0.3699336170841824, 0.44327715255215805, 0.9076682471607888, 0.2557052572142361, 0.7224428415077531, 0.24148406216462026, 0.17029502862608326]}, "task_prompt": ""}
{"id": "ffd6db12-7d30-44ef-81a3-48e67be944a4", "fitness": 0.380169408095866, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and a trust-region local search with online step-size/parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (DE/rand/1-like) with per-individual F and CR adaptation,\n      - Occasional Lévy-like (Cauchy) jumps centered on the current best for long-range exploration,\n      - Trust-region local Gaussian search around the current best (adaptive trust radius),\n      - Online adaptation of F_mean, CR_mean and exploration probability; re-seeding on stagnation.\n    Designed for continuous bounded problems (default bounds [-5,5] per dimension).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # choose population size heuristically, but not too large relative to budget\n        if pop_size is None:\n            estimated = max(8, int(6 * np.log(1 + self.dim) * self.dim))  # scale with dim\n            self.pop_size = max(4, min(estimated, max(4, self.budget // 8)))\n        else:\n            self.pop_size = max(4, int(pop_size))\n\n        # state placeholders to return later\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_bounds(self, x, lb, ub):\n        # project to bounds\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def __call__(self, func):\n        # Determine bounds: try to read from func.bounds, else use [-5,5] as specified\n        if hasattr(func, \"bounds\") and getattr(func.bounds, \"lb\", None) is not None:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item())\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # range used for scaling steps\n        range_ = ub - lb\n        range_clip = np.maximum(range_, 1e-12)\n\n        # init population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_clip\n\n        # evaluation bookkeeping\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # Evaluate as many initial individuals as budget allows\n        to_eval = min(self.pop_size, self.budget)\n        for i in range(to_eval):\n            x = pop[i].copy()\n            f = func(x)\n            fvals[i] = float(f)\n            evals += 1\n\n        # If budget too small, leave some individuals unevaluated (they won't be used)\n        if to_eval < self.pop_size:\n            # keep remaining individuals random but fvals stay inf\n            pass\n\n        # initial best\n        best_idx = int(np.nanargmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n        self.f_opt, self.x_opt = best_f, best_x.copy()\n\n        # Hyper-parameters (adapted online)\n        F_mean = 0.6       # mean differential weight\n        CR_mean = 0.9      # mean crossover prob\n        tau_F = 0.1        # jDE-like mutation probability of resetting Fi\n        tau_CR = 0.1\n        levy_prob = 0.06   # initial probability to perform a Lévy jump\n        levy_prob_min = 0.01\n        levy_prob_max = 0.4\n\n        trust_radius = 0.08 * np.mean(range_clip)  # initial trust radius as fraction of range\n        trust_min = 1e-6 * np.mean(range_clip)\n        trust_max = 0.5 * np.mean(range_clip)\n\n        # adaptation smoothing factors\n        adapt_alpha = 0.1\n        adapt_beta = 0.08\n\n        stagnation_counter = 0\n        stagnation_limit = max(50, self.budget // 20)\n        generation = 0\n\n        # helper: generate a Levy-like step using Cauchy (heavy tails)\n        def levy_step(scale=1.0, clip=20.0):\n            # Sample from Cauchy (standard): location 0, scale 'scale'\n            step = self.rng.standard_cauchy(self.dim) * scale\n            # Clip extreme outliers to avoid numerical blow-up (but keep heavy tails)\n            step = np.clip(step, -clip, clip)\n            return step\n\n        # main loop\n        while evals < self.budget:\n            generation += 1\n            gen_successes = 0\n            F_successes = []\n            CR_successes = []\n\n            # adapt per-generation jitter for means\n            # we'll process population members sequentially; break if budget exhausted\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Sample Fi and CRi: jDE-inspired stochastic per-individual variation\n                if self.rng.rand() < tau_F:\n                    Fi = np.clip(self.rng.rand() * 0.9 + 0.05, 0.05, 1.0)\n                else:\n                    Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 1.0)\n\n                if self.rng.rand() < tau_CR:\n                    CRi = np.clip(self.rng.rand(), 0.0, 1.0)\n                else:\n                    CRi = np.clip(self.rng.normal(CR_mean, 0.15), 0.0, 1.0)\n\n                # Decide whether to perform Lévy jump or DE\n                if self.rng.rand() < levy_prob:\n                    # Lévy-based candidate centered on best; heavy-tailed step\n                    scale = 0.5 * np.mean(range_clip) + trust_radius  # scale relative to problem range and trust radius\n                    step = levy_step(scale=scale)\n                    candidate = best_x + step\n                    # small smoothing towards population individual to keep diversity\n                    candidate = 0.5 * candidate + 0.5 * pop[i]\n                else:\n                    # DE/rand/1-like mutation with small trust bias toward best\n                    # choose r1,r2,r3 all distinct and != i\n                    idxs = list(range(self.pop_size))\n                    idxs.remove(i)\n                    # If population has many inf fvals (unevaluated), still allow selection by index\n                    r = self.rng.choice(idxs, size=3, replace=False)\n                    r1, r2, r3 = r[0], r[1], r[2]\n                    base = pop[r1]\n                    donor = base + Fi * (pop[r2] - pop[r3])\n\n                    # add small trust-region bias toward the best to allow intensification\n                    bias = (best_x - pop[i]) * (self.rng.rand(self.dim) * (trust_radius / (np.mean(range_clip) + 1e-12)))\n                    donor = donor + bias\n\n                    # binomial crossover\n                    cross = self.rng.rand(self.dim) < CRi\n                    if not np.any(cross):\n                        # enforce at least one gene\n                        cross[self.rng.randint(self.dim)] = True\n                    candidate = np.where(cross, donor, pop[i])\n\n                # project to bounds\n                candidate = self._ensure_bounds(candidate, lb, ub)\n\n                # Evaluate candidate\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    # accept\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    gen_successes += 1\n                    F_successes.append(Fi)\n                    CR_successes.append(CRi)\n\n                    # Update means towards successful parameter values (exponential moving average)\n                    if len(F_successes) > 0:\n                        F_mean = (1 - adapt_alpha) * F_mean + adapt_alpha * np.mean(F_successes)\n                    if len(CR_successes) > 0:\n                        CR_mean = (1 - adapt_alpha) * CR_mean + adapt_alpha * np.mean(CR_successes)\n                else:\n                    # small random nudges on failure to maintain exploration\n                    if self.rng.rand() < 0.02:\n                        pop[i] = self._ensure_bounds(pop[i] + self.rng.normal(0, 0.01 * range_clip, self.dim), lb, ub)\n\n                # Update global best if improved\n                if fvals[i] < best_f:\n                    best_f = float(fvals[i])\n                    best_x = pop[i].copy()\n                    self.f_opt, self.x_opt = best_f, best_x.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 0\n\n                # quick stagnation increment if we evaluated many individuals without improvement\n                # (we count generation successes below)\n\n            # End of generation: local trust-region search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: small handful, scaled by dim and remaining budget\n            local_budget = max(1, min(5 + self.dim // 4, remaining))\n            local_success = 0\n            for _ in range(local_budget):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                sigma = trust_radius * (0.5 + self.rng.rand(self.dim))\n                local_candidate = best_x + self.rng.normal(0, sigma, self.dim)\n                local_candidate = self._ensure_bounds(local_candidate, lb, ub)\n                f_local = float(func(local_candidate))\n                evals += 1\n                if f_local < best_f:\n                    best_f = f_local\n                    best_x = local_candidate.copy()\n                    self.f_opt, self.x_opt = best_f, best_x.copy()\n                    local_success += 1\n\n            # adjust trust radius based on local search outcome\n            if local_success > 0:\n                # successful exploitation: shrink trust radius to hone in\n                trust_radius = max(trust_min, trust_radius * (0.6 - 0.05 * local_success))\n            else:\n                # unsuccessful: expand a bit to escape local traps\n                trust_radius = min(trust_max, trust_radius * 1.12)\n\n            # adaptation of levy_prob and other meta-parameters depending on generation successes\n            if gen_successes > max(1, self.pop_size // 10):\n                # many successes: favor exploitation slightly\n                levy_prob = max(levy_prob_min, levy_prob * 0.95)\n                # reduce trust radius slowly if population is finding improvements\n                trust_radius = max(trust_min, trust_radius * 0.98)\n            else:\n                # few successes: encourage exploration\n                levy_prob = min(levy_prob_max, levy_prob * 1.06)\n                trust_radius = min(trust_max, trust_radius * 1.02)\n\n            # small drift of CR_mean toward mid-range if stuck\n            CR_mean = np.clip(CR_mean * (1 - adapt_beta) + 0.5 * adapt_beta, 0.0, 1.0)\n            F_mean = np.clip(F_mean * (1 - adapt_beta) + 0.5 * adapt_beta, 0.05, 1.0)\n\n            # stagnation handling: if no improvement for many generations, re-seed half of the population\n            # We'll detect stagnation by comparing current best to stored self.f_opt (already updated). Use a counter:\n            # If no improvement in many generations, reinitialize half of pop\n            if gen_successes == 0:\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n\n            if stagnation_counter >= stagnation_limit:\n                # Re-seed: replace half of the population with random samples (but keep best)\n                half = max(1, self.pop_size // 2)\n                replace_idx = self.rng.choice([i for i in range(self.pop_size) if not np.all(pop[i] == best_x)], size=half, replace=False)\n                for j in replace_idx:\n                    pop[j] = lb + self.rng.rand(self.dim) * range_clip\n                    fvals[j] = np.inf\n                # Evaluate as many re-seeded individuals as budget allows (up to remaining)\n                for j in replace_idx:\n                    if evals >= self.budget:\n                        break\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = float(fvals[j])\n                        best_x = pop[j].copy()\n                        self.f_opt, self.x_opt = best_f, best_x.copy()\n\n                # after re-seed, enlarge trust radius slightly to promote exploration\n                trust_radius = min(trust_max, trust_radius * 1.5)\n                # lower levy probability to focus on local searches after reseed\n                levy_prob = max(levy_prob_min, levy_prob * 0.7)\n\n                stagnation_counter = 0  # reset\n\n            # hard stop if budget used\n            if evals >= self.budget:\n                break\n\n        # Final results\n        # Ensure x_opt is numpy array of correct shape\n        if self.x_opt is None:\n            # fallback: pick best from fvals if any finite\n            finite_mask = np.isfinite(fvals)\n            if np.any(finite_mask):\n                idx = int(np.nanargmin(fvals))\n                self.x_opt = pop[idx].copy()\n                self.f_opt = float(fvals[idx])\n            else:\n                # randomly return a point in bounds\n                self.x_opt = lb + self.rng.rand(self.dim) * range_clip\n                self.f_opt = float(func(self.x_opt)) if evals < self.budget else np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.380 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13129532322435622, 0.20393024370927393, 0.3946292778577398, 0.7888784157313888, 0.4182895456555207, 0.5046026605452953, 0.2810598699868152, 0.39964055470013526, 0.4504917867533603, 0.22887640279477495]}, "task_prompt": ""}
{"id": "968c26d6-29b5-49f6-9c98-50d2c027f4db", "fitness": 0.2613657124072371, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DeLevyTrust — a Differential Evolution with per-individual online F/CR adaptation, occasional Lévy-style long jumps for global exploration, and a trust-region Gaussian local search around the best solution with dynamic trust-radius and stagnation resets.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (per-individual adaptive F and CR, jDE-like),\n      - Occasional Lévy-style (Cauchy) long jumps centered on the best,\n      - Trust-region Gaussian local search around the current best with\n        dynamic trust radius adaptation and partial population reseeding.\n    Designed for continuous problems with bounds (default [-5,5]) and a\n    budgeted number of function evaluations.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size: scale with dim, but not too large vs budget\n        if pop_size is None:\n            pop = int(6 + 4 * np.sqrt(self.dim))  # modest baseline\n            # ensure we can at least sample a few generations\n            max_pop = max(4, int(self.budget // max(10, self.dim)))\n            self.pop_size = max(4, min(pop, max_pop))\n        else:\n            self.pop_size = int(pop_size)\n\n        # internal state placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like and return 1D arrays of length dim\n        b_arr = np.atleast_1d(b)\n        if b_arr.size == 1:\n            return np.full(self.dim, float(b_arr[0]))\n        if b_arr.size == self.dim:\n            return b_arr.astype(float)\n        # If wrong shape, try to broadcast\n        return np.broadcast_to(b_arr.ravel(), (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # determine bounds\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure lb < ub\n        span = ub - lb\n        span[span <= 0] = 1.0\n\n        # helpers\n        def clamp(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # initial global best\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n        # initial population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * span\n        pop_f = np.full(self.pop_size, np.inf)\n\n        # per-individual F and CR (initialize around reasonable defaults)\n        Fi = self.rng.normal(0.6, 0.1, size=self.pop_size)\n        Fi = np.clip(Fi, 0.1, 1.2)\n        CRi = np.clip(self.rng.normal(0.2, 0.1, size=self.pop_size), 0.0, 1.0)\n\n        # population evaluation as budget allows\n        for i in range(self.pop_size):\n            if self.evals >= self.budget:\n                break\n            x = pop[i].copy()\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            pop_f[i] = f\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # if not all individuals evaluated (tiny budget), fill remaining with inf\n        # not evaluated individuals exist but won't be used for selection\n        # algorithm hyper-parameters\n        F_mean = 0.6\n        CR_mean = 0.2\n        tau_F = 0.1  # probability to redefine Fi (jDE-like)\n        tau_CR = 0.1\n        levy_base_prob = 0.05  # base chance for a levy jump when mutating\n        levy_increase_on_stagnation = 2.5\n        trust_radius = 0.2  # relative to search range (0..1)\n        trust_shrink = 0.7\n        trust_expand = 1.2\n        trust_min = 1e-4\n        trust_max = 2.0\n\n        no_improve_count = 0\n        stagnation_threshold = max(10, 3 * self.dim)\n        strong_stagnation = 6 * self.dim\n\n        # levy step generator using Cauchy (heavy-tail)\n        def levy_step(scale=1.0):\n            # generate Cauchy distributed vector, limit extremes\n            step = self.rng.standard_cauchy(self.dim)\n            # clip extreme outliers to avoid numerical blow-up then scale\n            step = np.clip(step, -1e3, 1e3)\n            # small random re-scaling per-dimension to induce anisotropy\n            anis = 0.5 + self.rng.rand(self.dim)\n            return step * anis * float(scale)\n\n        # evaluate candidate with budget check\n        def evaluate_candidate(x):\n            if self.evals >= self.budget:\n                return None\n            x = clamp(x)\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            self.evals += 1\n            return f, x\n\n        # main generational loop\n        gen = 0\n        # limit number of generations roughly by budget/pop_size\n        while self.evals < self.budget:\n            gen += 1\n            # prepare storage for successful Fi/CR for online adaptation\n            successful_Fs = []\n            successful_CRs = []\n            successes = 0\n\n            # dynamic probability of levy jumps increases when stagnating\n            if no_improve_count > stagnation_threshold:\n                levy_prob = min(0.5, levy_base_prob * levy_increase_on_stagnation)\n            else:\n                levy_prob = levy_base_prob\n\n            # iterate through population sequentially (each candidate consumes an eval on trial)\n            for i in range(self.pop_size):\n                if self.evals >= self.budget:\n                    break\n\n                # sample or mutate Fi/CR in jDE style\n                if self.rng.rand() < tau_F:\n                    Fi[i] = np.clip(self.rng.rand() * 0.9 + 0.1, 0.05, 1.2)\n                if self.rng.rand() < tau_CR:\n                    CRi[i] = np.clip(self.rng.rand(), 0.0, 1.0)\n\n                # F and CR to use\n                f_i = Fi[i]\n                cr_i = CRi[i]\n\n                # mutation indices for rand/1-like (distinct indices different from i)\n                idxs = np.arange(self.pop_size)\n                idxs = idxs[idxs != i]\n                if idxs.size < 3:\n                    # fallback small-pop handling: sample from the population with replacement\n                    r1, r2, r3 = (i, (i+1)%self.pop_size, (i+2)%self.pop_size)\n                    xr1 = pop[r1]; xr2 = pop[r2]; xr3 = pop[r3]\n                else:\n                    r = self.rng.choice(idxs, size=3, replace=False)\n                    xr1, xr2, xr3 = pop[r[0]], pop[r[1]], pop[r[2]]\n\n                # DE/rand/1 donor\n                donor = xr1 + f_i * (xr2 - xr3)\n\n                # occasional Levy jump centered on the current best (long-range exploration)\n                if self.rng.rand() < levy_prob:\n                    # scale levy by span and trust radius to avoid leaving domain too extremely\n                    levy_scale = 0.5 * trust_radius\n                    levy_vec = levy_step(scale=levy_scale)\n                    # center around best if available otherwise around random\n                    if self.x_opt is not None:\n                        donor = self.x_opt + levy_vec * span\n                    else:\n                        donor = lb + self.rng.rand(self.dim) * span + levy_vec * span\n\n                # blend donor toward best by trust radius to mix exploration/exploitation\n                if self.x_opt is not None:\n                    # compute difference scaled by trust radius relative to span\n                    mix = trust_radius\n                    donor = self.x_opt + mix * (donor - self.x_opt)\n\n                # binomial crossover\n                cross = self.rng.rand(self.dim) < cr_i\n                # ensure at least one dimension from donor\n                jrand = self.rng.randint(self.dim)\n                cross[jrand] = True\n                trial = np.where(cross, donor, pop[i])\n                # project to bounds\n                trial = clamp(trial)\n\n                # evaluate trial\n                res = evaluate_candidate(trial)\n                if res is None:\n                    break\n                trial_f, trial_x = res\n\n                # selection\n                if trial_f < pop_f[i] or np.isinf(pop_f[i]):\n                    # successful replacement\n                    pop[i] = trial_x\n                    pop_f[i] = trial_f\n                    successful_Fs.append(f_i)\n                    successful_CRs.append(cr_i)\n                    successes += 1\n                    # update global best if improved\n                    if trial_f < self.f_opt:\n                        self.f_opt = trial_f\n                        self.x_opt = trial_x.copy()\n                        no_improve_count = 0\n                    else:\n                        # small improvement (population-level) still considered success\n                        no_improve_count += 0\n                else:\n                    # unsuccessful: mild penalty increase to encourage change\n                    no_improve_count += 0\n\n                # small adaptation of means after each trial (can be done later per gen)\n                # but keep light-weight: update moving average if success\n                if len(successful_Fs) > 0:\n                    # Lehmer-like mean for F (emphasize larger F that succeed)\n                    F_mean = (0.9 * F_mean + 0.1 * (np.mean(np.array(successful_Fs)**2) / (np.mean(successful_Fs) + 1e-12)))\n                    CR_mean = 0.9 * CR_mean + 0.1 * np.mean(successful_CRs)\n                    # clip\n                    F_mean = np.clip(F_mean, 0.05, 1.2)\n                    CR_mean = np.clip(CR_mean, 0.0, 1.0)\n\n                # clear any ephemeral large Fi/CR usage not to carry forward angrily\n                # (we keep Fi[i], CRi[i] for next generation as jDE style)\n\n            # End per-population pass\n\n            # Generation-level adjustments: trust-region local search around best\n            remaining = self.budget - self.evals\n            if remaining <= 0:\n                break\n\n            # decide number of local samples: small handful scaled by dim & remaining budget\n            n_local = min(remaining, max(1, int(max(1, self.dim // 2))))\n            # but keep to modest numbers\n            n_local = int(min(n_local, max(1, 2 + self.dim // 3)))\n\n            local_success = 0\n            # local sigma scaled to span and trust_radius\n            sigma_vec = (span * (0.5 * trust_radius))\n            for _ in range(n_local):\n                if self.evals >= self.budget:\n                    break\n                if self.x_opt is None:\n                    break\n                # anisotropic Gaussian perturbation\n                perturb = self.rng.normal(0.0, 1.0, size=self.dim) * sigma_vec\n                cand = clamp(self.x_opt + perturb)\n                res = evaluate_candidate(cand)\n                if res is None:\n                    break\n                cf, cx = res\n                if cf < self.f_opt:\n                    # strong success: adopt, shrink trust radius to focus\n                    self.f_opt = cf\n                    self.x_opt = cx.copy()\n                    local_success += 1\n                    trust_radius = max(trust_min, trust_radius * trust_shrink)\n                    no_improve_count = 0\n                # also allow replacement in population: replace worst member to propagate improvement\n                worst_idx = np.argmax(pop_f)\n                if cf < pop_f[worst_idx]:\n                    pop[worst_idx] = cx.copy()\n                    pop_f[worst_idx] = cf\n\n            if local_success == 0:\n                # no local improvements -> expand trust radius to try escaping\n                trust_radius = min(trust_max, trust_radius * trust_expand)\n                no_improve_count += 1\n            else:\n                # reward successful exploitation by tightening further a bit\n                trust_radius = max(trust_min, trust_radius * (trust_shrink**0.5))\n\n            # stagnation handling: if sustained no improvement, increase exploration\n            if no_improve_count > stagnation_threshold:\n                # nudge Fi and CR distributions to diversify\n                Fi += self.rng.normal(0.0, 0.05, size=Fi.shape)\n                CRi += self.rng.normal(0.0, 0.05, size=CRi.shape)\n                Fi = np.clip(Fi, 0.05, 1.2)\n                CRi = np.clip(CRi, 0.0, 1.0)\n\n            # strong stagnation: reseed half of population randomly to diversify\n            if no_improve_count > strong_stagnation:\n                n_reseed = max(1, self.pop_size // 2)\n                for k in range(n_reseed):\n                    idx = self.rng.randint(self.pop_size)\n                    # reinitialize\n                    new_x = lb + self.rng.rand(self.dim) * span\n                    pop[idx] = new_x\n                    # evaluate re-seeded individuals only if we have budget\n                    if self.evals < self.budget:\n                        res = evaluate_candidate(new_x)\n                        if res is None:\n                            break\n                        nf, nx = res\n                        pop_f[idx] = nf\n                        if nf < self.f_opt:\n                            self.f_opt = nf\n                            self.x_opt = nx.copy()\n                # after reseed, increase trust_radius to recover global search\n                trust_radius = min(trust_max, trust_radius * 1.5)\n                no_improve_count = 0  # reset stagnation counter\n\n            # small safety: avoid infinite loop if nothing improves but budget remains; will continue until budget exhausted\n            # protect F_mean/CR_mean bounds\n            F_mean = float(np.clip(F_mean, 0.05, 1.2))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n            # minimal update of per-individual Fi/CR towards population means to avoid stagnation\n            Fi = 0.9 * Fi + 0.1 * F_mean\n            CRi = 0.9 * CRi + 0.1 * CR_mean\n\n        # Return best found (value, x)\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()\n\n# Example wrapper compatibility:\n# The optimizer expects func(x) to evaluate and optionally func.bounds.lb / ub to exist.\n# The class returns (f_opt, x_opt).", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.261 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10525154125710356, 0.15343712773965945, 0.27595568472706977, 0.24758209482733584, 0.2889909402197355, 0.5111040291510207, 0.2641629580355188, 0.3246120349624144, 0.26993058767131584, 0.17263012548119705]}, "task_prompt": ""}
{"id": "475ebec0-d78d-4e97-af20-074da764d559", "fitness": 0.30761866169635177, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style parameter adaptation, occasional Lévy-flight global jumps, and an adaptive trust-region local search around the current best.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n      - Differential Evolution (rand/1-like) with per-individual jDE-like Fi/CRi adaptation,\n      - Occasional Levy-flight (Cauchy) jumps centered on the best for long-range exploration,\n      - Trust-region Gaussian local search around the best with an adaptive trust radius,\n      - Online adaptation of F_mean, CR_mean and p_levy based on observed successes,\n      - Re-seeding on strong stagnation.\n\n    Designed for continuous box-constrained optimization (supports broadcasting scalar bounds).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but bounded\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # not larger than a fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast to (dim,)\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        range_mean = float(np.mean(range_vec))\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            # best among evaluated\n            best_idx = np.argmin(fvals[valid])\n            best_indices = np.where(valid)[0]\n            best_idx = best_indices[best_idx]\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover prob mean\n        p_levy = 0.08       # probability of a Lévy jump (global exploration)\n        step_scale = 0.2    # base scale factor for levy and trust radius\n        min_trust = 1e-6\n        max_trust = max(1.0, range_norm * 2.0)\n\n        # initial trust radius (fraction of the search range)\n        trust_radius = max(min_trust, min(max_trust, step_scale * range_norm))\n\n        stagnation_counter = 0\n        gen = 0\n\n        # Levy-like heavy-tailed step (Cauchy)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            # clip extreme outliers for numerical stability\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: stop when budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # per-generation sampled indices for DE\n            idxs_base = np.arange(self.pop_size)\n\n            # For each target vector (sequential evaluations)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # prepare to possibly use Fi/CRi; default None (used only if DE branch executed)\n                Fi = None\n                CRi = None\n\n                # decide exploration move\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best_x for global exploration\n                    s = levy_step()\n                    # scale: combine trust_radius and range mean to adapt to problem scale\n                    scale = (0.5 * trust_radius / max(range_norm, 1e-12) + 0.2 * rng.rand()) * range_mean\n                    donor = best_x + s * scale\n                    # crossover-like mixing with current member to keep some diversity\n                    cr_mask = rng.rand(self.dim) < 0.5\n                    trial = np.where(cr_mask, donor, pop[i])\n                    candidate = np.clip(trial, lb, ub)\n                else:\n                    # DE/rand/1 with jDE-like individual Fi/CRi sampling\n                    others = idxs_base[idxs_base != i]\n                    # ensure enough individuals for selection; if small pop, sample with replacement\n                    replace = False if others.size >= 3 else True\n                    r1, r2, r3 = rng.choice(others, 3, replace=replace)\n                    # sample Fi and CRi around current means (bounded)\n                    Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    # ensure at least one gene from donor\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    candidate = np.clip(trial, lb, ub)\n\n                # Evaluate candidate (respect budget)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                    # adapt global means slightly toward successful Fi/CRi if present\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # unsuccessful - slight penalty to encourage exploration\n                    stagnation_counter += 1\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation: local trust-region search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # local samples count scaled by dimension and remaining budget\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dimension: random factor times trust_radius expressed relative to range\n                rand_factor = 0.5 + rng.rand(self.dim) * 0.5\n                sigma = rand_factor * (trust_radius / max(range_norm, 1e-12))\n                # gaussian perturbation in normalized space times range vector\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_x = candidate.copy()\n                    best_f = f_candidate\n                    # successful local search => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slightly expand trust radius to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # Adapt exploration probability and parameter means based on successes and stagnation\n            if successes > 0:\n                # reduce p_levy if many successes (favor exploitation), but keep some baseline\n                decay = 0.98 if successes <= (0.2 * self.pop_size) else 0.95\n                p_levy = max(0.01, p_levy * decay)\n                # nudge F_mean/CR_mean slowly toward default exploitation settings\n                F_mean = np.clip(0.97 * F_mean + 0.03 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.97 * CR_mean + 0.03 * 0.9, 0.0, 1.0)\n                # slight increase in exploration probability if small successes but many gens (diversify)\n                if successes < max(1, int(0.05 * self.pop_size)):\n                    p_levy = min(0.5, p_levy * 1.05 + 0.005)\n                    F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n            else:\n                # no successes this generation: encourage more long jumps and broaden Fi a bit\n                p_levy = min(0.6, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n\n            # Strong stagnation reset: re-seed half the population if stuck for too long\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                to_reset = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in to_reset:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius a bit after reset to explore newly opened area\n                trust_radius = min(max_trust, max(trust_radius * 1.5, step_scale * range_norm))\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.308 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14047494965707796, 0.21081839204592356, 0.34566242199922725, 0.4602268128366255, 0.4312476235393443, 0.4144311843689581, 0.2761629662605778, 0.3223904757071794, 0.25566395286191723, 0.21910783768668718]}, "task_prompt": ""}
{"id": "65621ca6-61e8-41f5-90f1-d2ca228cc6eb", "fitness": 0.21237651727544532, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for global escapes, and a trust-region Gaussian local search; parameters are adapted online from success history.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # base population scaling with dimension\n            self.pop_size = max(6, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # ensure not larger than a fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.copy()\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # RNG\n        rng = np.random.default_rng(self.seed)\n\n        # bounds (BBOB style func.bounds.lb / ub) - fallback to [-5,5]\n        try:\n            lb_raw = func.bounds.lb\n            ub_raw = func.bounds.ub\n        except Exception:\n            lb_raw = -5.0\n            ub_raw = 5.0\n        lb = self._ensure_array_bounds(lb_raw)\n        ub = self._ensure_array_bounds(ub_raw)\n        range_vec = ub - lb\n        # protect against zero-range dimensions\n        range_vec = np.where(range_vec == 0.0, 1.0, range_vec)\n\n        # initialize population\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # initial best\n        if np.isfinite(fvals).any():\n            best_idx = int(np.nanargmin(np.where(np.isfinite(fvals), fvals, np.nan)))\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # nothing evaluated\n            self.f_opt = np.inf\n            self.x_opt = lb + rng.random(self.dim) * range_vec\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius (fraction of range)\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        trust_radius = step_scale * np.linalg.norm(range_vec)\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using standard Cauchy (simple and effective)\n        def levy_step():\n            # standard Cauchy gives heavy tails; clip extremes to avoid numerical blow-ups\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e6, 1e6)\n            return s\n\n        # keep track of successful parameter values for adaptation\n        succ_Fs = []\n        succ_CRs = []\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation temporary success counters (for adaptation)\n            succ_Fs.clear()\n            succ_CRs.clear()\n\n            # Shuffle order to avoid positional bias\n            order = rng.permutation(self.pop_size)\n\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                target = pop[idx].copy()\n                target_f = fvals[idx]\n\n                # decide operation: Levy jump centered on best (exploration) or DE mutation (exploitation)\n                if rng.random() < p_levy:\n                    # Lévy-centered jump around best\n                    step = levy_step()\n                    # scale by trust radius and per-dimension range\n                    scale = trust_radius * (0.5 + rng.random() * 1.5)  # vary scale\n                    donor = best_x + step * (scale * range_vec / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                    # occasional modest crossover to maintain some target structure\n                    CRi = np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0)\n                    mask = rng.random(self.dim) < CRi\n                    if not mask.any():\n                        mask[rng.integers(0, self.dim)] = True\n                    trial = np.where(mask, donor, target)\n                    Fi = None  # not used in Levy\n                else:\n                    # Differential evolution mutation: rand/1 + binomial crossover with per-individual adaptation (jDE-style)\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != idx]\n                    # ensure there are at least 3 other individuals\n                    if idxs.size >= 3:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        # sample per-individual Fi and CRi\n                        Fi = np.clip(rng.normal(F_mean, 0.1), 0.01, 0.99)\n                        CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = rng.random(self.dim) < CRi\n                        if not cr_mask.any():\n                            cr_mask[rng.integers(0, self.dim)] = True\n                        trial = np.where(cr_mask, donor, target)\n                    else:\n                        # fallback to small gaussian perturbation\n                        Fi = None\n                        CRi = None\n                        trial = target + rng.normal(0, 0.01, size=self.dim) * range_vec\n\n                # project to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < target_f:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    # record successful control parameters for adaptation\n                    if Fi is not None:\n                        succ_Fs.append(Fi)\n                    if 'CRi' in locals() and CRi is not None:\n                        succ_CRs.append(CRi)\n                    # update local best\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                else:\n                    # no improvement\n                    stagnation_counter += 1\n\n                # enforce budget\n                if evals >= self.budget:\n                    break\n\n            # End of generation: adapt F_mean and CR_mean using simple Lehmer-like update\n            if len(succ_Fs) > 0:\n                # Lehmer mean (gives larger weight to big values) -> helps F adaptation\n                num = sum(f*f for f in succ_Fs)\n                den = sum(succ_Fs)\n                if den > 0:\n                    newF = num / den\n                    F_mean = 0.9 * F_mean + 0.1 * np.clip(newF, 0.01, 0.99)\n            else:\n                # slowly drift towards exploration if no successes\n                F_mean = np.clip(F_mean * 1.02, 0.01, 0.99)\n\n            if len(succ_CRs) > 0:\n                CR_mean = 0.9 * CR_mean + 0.1 * (sum(succ_CRs) / len(succ_CRs))\n            else:\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # adapt probability of levy jumps based on recent success/stagnation\n            if successes > max(1, self.pop_size // 8):\n                p_levy = max(0.01, p_levy * 0.97)\n                # slightly focus trust region if many local successes\n                trust_radius *= 0.92\n            else:\n                # increase jump probability slowly if not many successes\n                p_levy = min(0.6, p_levy * 1.03 + 0.005)\n                trust_radius *= 1.02\n\n            # clamp trust radius\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # trust-region local search around current best: sample a few gaussian candidates\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # number of local samples depends on remaining budget and dimension (keep small)\n            local_samples = min(5, remaining, max(1, int(2 + self.dim // 10)))\n            improved_local = False\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: combine trust radius and per-dimension random scaling\n                sigma = (0.5 + rng.random(self.dim) * 0.5) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    improved_local = True\n                    # shrink trust region when successful local steps found\n                    trust_radius *= 0.85\n                    trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful: small expansion to escape\n                    trust_radius *= 1.05\n                    trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n                    stagnation_counter += 1\n\n                if evals >= self.budget:\n                    break\n\n            # if many stagnation steps, perform partial re-seeding\n            if stagnation_counter > max(50, 2 * self.dim):\n                # reinitialize half of population (but keep best)\n                k = max(1, self.pop_size // 2)\n                indices = list(range(self.pop_size))\n                # remove best index if present\n                # find index of best in pop (could be multiple)\n                cur_best_idx = int(np.nanargmin(fvals))\n                if cur_best_idx in indices:\n                    indices.remove(cur_best_idx)\n                # choose to reinit k indices (or fewer if not enough)\n                reinit = rng.choice(indices, min(k, len(indices)), replace=False)\n                for j in reinit:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # update best if found\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # after reset, reduce stagnation counter and slightly increase trust radius\n                stagnation_counter = 0\n                trust_radius *= 1.2\n                trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # small safeguard: if best is not represented in population (because we used candidate best_x not in pop),\n            # try to inject it into the worst member to keep population anchored\n            worst_idx = int(np.argmax(fvals))\n            if fvals[worst_idx] > best_f and rng.random() < 0.05:\n                pop[worst_idx] = best_x.copy()\n                fvals[worst_idx] = best_f\n\n            # small diversification if p_levy became too small/large\n            p_levy = float(np.clip(p_levy, 0.005, 0.6))\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.212 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10464569488739062, 0.17250143214890956, 0.27473860563384545, 0.2885854068939905, 0.20380864214598948, 0.2669544421487172, 0.23315396792647602, 0.23840635478108163, 0.19932524078374292, 0.1416453854043096]}, "task_prompt": ""}
{"id": "ae0d726a-ccf4-4230-ae4c-c5e15ea3ec81", "fitness": 0.29753538944251395, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range escapes, and a trust-region local search with online step-size adaptation and population resets.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        if pop_size is None:\n            # scale pop with dim but keep reasonable limits\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast or repeat\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # safety\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # find best among evaluated\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # nothing could be evaluated\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.nanargmin(fvals)\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.05       # probability of Lévy jump per trial\n        step_scale = 0.25   # base scale for Lévy and trust radius relative to range\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        min_trust = 1e-8\n        max_trust = range_norm * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step():\n            # truncated cauchy to avoid numerical blow-ups\n            s = rng.standard_cauchy(self.dim)\n            # clip extreme values\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            gen_improved = False\n\n            # per-generation small jitter to means to encourage exploration\n            F_mean = np.clip(F_mean * (1.0 + rng.normal(0, 0.01)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + rng.normal(0, 0.02), 0.0, 1.0)\n\n            # iterate population\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample DE/F and CR for this individual\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 0.99)\n                CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                if rng.rand() < p_levy:\n                    # Lévy-centered jump around best_x: exploratory long jump\n                    step = levy_step()\n                    # scale by trust radius and global range\n                    donor = best_x + (step_scale * trust_radius / (range_norm + 1e-12)) * step * range_vec\n                    # add a tiny gaussian jitter too\n                    donor = donor + rng.normal(0, 0.01 * trust_radius / (range_norm + 1e-12), size=self.dim) * range_vec\n                    # produce trial via simple mixing with target to allow partial acceptance\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                else:\n                    # classic DE/rand/1 mutation and binomial crossover\n                    idxs = np.arange(self.pop_size)\n                    # exclude i\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback: random jitter around target\n                        donor = pop[i] + Fi * rng.normal(0, 1, size=self.dim)\n                    else:\n                        r = rng.choice(idxs, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget left\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt means slightly toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    gen_improved = True\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # after population loop: trust-region local search around best_x\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample count: small handful scaled by dim and remaining budget\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            improved_local = False\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by random per-dimension\n                sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / (range_norm + 1e-12))\n                candidate = best_x + rng.normal(0.0, sigma, size=self.dim) * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    improved_local = True\n                    gen_improved = True\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand slightly to encourage escape\n                    trust_radius *= 1.05\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n\n            # adaptation of Levy probability based on success rate\n            if successes > 0 or improved_local:\n                # fewer Levy jumps if we are making progress\n                p_levy = max(0.005, p_levy * (0.96 if successes > self.pop_size * 0.15 else 0.98))\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase F_mean\n                p_levy = min(0.5, p_levy * 1.05 + 0.005)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n            # end generation adjustments: small decay to encourage convergence\n            F_mean = np.clip(F_mean * 0.995, 0.05, 0.99)\n            CR_mean = np.clip(CR_mean * 0.999, 0.0, 1.0)\n\n            # If we improved in the generation, reduce trust radius slightly to refine\n            if gen_improved:\n                trust_radius = max(min_trust, trust_radius * 0.95)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.298 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1552976493295415, 0.1665487421234454, 0.32856679490920204, 0.6211882786988923, 0.27689422637448835, 0.3838592800094205, 0.2556334916316143, 0.2545976625742389, 0.27959556565337684, 0.2531722031209196]}, "task_prompt": ""}
{"id": "d31af16f-7474-4fde-a57f-ee249e9b080a", "fitness": 0.15991491246787187, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style parameter control, occasional Lévy (Cauchy) long jumps for exploration, and an adaptive trust-region local search around the best solution.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (jDE-like parameter\n    self-adaptation), occasional Lévy/Cauchy long jumps for global exploration,\n    and an adaptive trust-region Gaussian local search around the current best.\n    Designed for continuous bound-constrained black-box problems.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size relative to dimension and budget\n        if pop_size is None:\n            self.pop_size = max(4, min(10 + 4 * self.dim, max(4, self.budget // 8)))\n        else:\n            self.pop_size = int(pop_size)\n        # never allow pop_size larger than portion of budget (we need evaluations for iterations)\n        self.pop_size = min(self.pop_size, max(4, self.budget // 4))\n        self.seed = None if seed is None else int(seed)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Accepts scalar or array-like bounds and returns an array of shape (dim,).\n        \"\"\"\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.copy().astype(float)\n        # try to broadcast\n        try:\n            b = np.broadcast_to(b, (self.dim,))\n            return b.astype(float)\n        except Exception:\n            raise ValueError(\"Bounds could not be broadcast to problem dimension\")\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # get bounds and range\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # ensure non-zero range\n        range_vec[range_vec == 0.0] = 1e-12\n\n        # initialize population\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # find best among evaluated (if none evaluated, return immediate)\n        evaluated_mask = np.isfinite(fvals)\n        if not np.any(evaluated_mask):\n            # no evaluations possible (budget == 0)\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals[evaluated_mask])\n        # map to absolute index\n        eval_indices = np.where(evaluated_mask)[0]\n        best_idx = eval_indices[best_idx]\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy/Cauchy long jump\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        trust_radius = max_trust * 0.2 if max_trust > 0 else 0.1\n\n        # jDE-like mutation probabilities to refresh Fi/CRi\n        tau1 = 0.1\n        tau2 = 0.1\n\n        gen = 0\n        stagnation_counter = 0\n        successes = 0\n        max_stagnation = max(20, 5 * self.dim)\n\n        # helper: Levy-like heavy-tailed step using Cauchy, limited to avoid blow-up\n        def levy_step(scale=1.0):\n            s = rng.standard_cauchy(size=self.dim)\n            # limit extreme outliers (clip large values)\n            s = np.clip(s, -15.0, 15.0)\n            return s * float(scale)\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            gen_successes = 0\n\n            # iterate over population (sequential, each evaluation counts)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # produce Fi and CRi via jDE-like adaptation\n                if rng.rand() < tau1:\n                    Fi = 0.1 + rng.rand() * 0.9  # in (0.1,1.0]\n                else:\n                    Fi = F_mean\n                if rng.rand() < tau2:\n                    CRi = rng.rand()\n                else:\n                    CRi = CR_mean\n\n                # Decide between Lévy jump centered on best or DE mutation\n                if rng.rand() < p_levy:\n                    # Lévy/Cauchy exploration jump around best_x\n                    # scale jump by trust_radius and range_vec\n                    step = levy_step(scale=trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                    candidate = best_x + step * range_vec\n                else:\n                    # DE/rand/1/bin mutation variant\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback to random candidate if insufficient others\n                        candidate = lb + rng.rand(self.dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        trial = pop[i].copy()\n                        jrand = rng.randint(0, self.dim)\n                        mask = rng.rand(self.dim) < CRi\n                        mask[jrand] = True\n                        trial[mask] = donor[mask]\n                        candidate = trial\n\n                # project to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # greedy selection against target\n                # if target hasn't been evaluated (inf), accept if candidate finite\n                if not np.isfinite(fvals[i]) or f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    gen_successes += 1\n                    # move parameter means slightly toward successful Fi/CRi\n                    # only update if they were actually used (i.e., DE branch or we consider Fi/CRi anyway)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                    # update global best if necessary\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                else:\n                    stagnation_counter += 1\n\n                # small safety to keep means bounded\n                F_mean = np.clip(F_mean, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean, 0.0, 1.0)\n\n                # exit early if budget used up\n                if evals >= self.budget:\n                    break\n\n            successes = gen_successes\n\n            # Trust-region local search around best_x\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(int(local_samples)):\n                # anisotropic per-dimension sigma: random scale in [0.5, 1.0] times trust_radius\n                sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => focus: shrink trust radius (but not below min)\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    gen_successes += 1\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slightly expand to help escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n                    stagnation_counter += 1\n\n            # adjust exploration probability and means based on success count\n            if gen_successes > 0:\n                # reduce p_levy slightly to exploit if many successes\n                if gen_successes > max(1, self.pop_size * 0.2):\n                    p_levy *= 0.95\n                else:\n                    p_levy *= 0.98\n                p_levy = np.clip(p_levy, 0.01, 0.5)\n                # nudge means a bit toward exploitation defaults\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation: increase exploration probability and slightly increase F_mean to promote diversity\n                p_levy = min(0.6, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n                trust_radius = min(max_trust, trust_radius * 1.2)\n\n            # strong stagnation reset: re-seed part of population\n            if stagnation_counter >= max_stagnation and evals < self.budget:\n                # reinitialize half of the population (at most as budget allows)\n                k = max(1, self.pop_size // 2)\n                # determine how many we can evaluate given remaining budget\n                remaining = self.budget - evals\n                # we will at least create new individuals, but only evaluate as many as budget allows\n                to_replace = min(k, remaining)\n                replace_indices = rng.choice(np.arange(self.pop_size), to_replace, replace=False)\n                for j in replace_indices:\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    if evals >= self.budget:\n                        fvals[j] = np.inf\n                    else:\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                        if fvals[j] < best_f:\n                            best_f = fvals[j]\n                            best_x = pop[j].copy()\n                # after reset allow broader search\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n            # if best improved update stored result\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n            # safety guard to ensure trust radius stays in bounds\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n            # small generation-level cap to avoid infinitely small progress\n            if gen > 100000:\n                break\n\n        # final results\n        # ensure x_opt is set\n        if self.x_opt is None and np.isfinite(best_f):\n            self.x_opt = best_x.copy()\n            self.f_opt = best_f\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.160 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08283001719620853, 0.15267077450349487, 0.2067713242330952, 0.17380079983713947, 0.14929681306278286, 0.17878824998596832, 0.1876952808314475, 0.17263500591376646, 0.1624480778276347, 0.13221278128718095]}, "task_prompt": ""}
{"id": "d031cd0d-c086-4c8b-8be3-d6a252cf203a", "fitness": 0.300959379558351, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy (Cauchy) long jumps, and an online-adaptive trust-region local search for fast global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (adaptive jDE-like per-individual F/CR),\n      - occasional heavy-tailed jumps (Cauchy as a Levy-like step) centered on the best,\n      - a trust-region Gaussian local search around the current best.\n    The algorithm adapts F and CR online from successful trials, varies the\n    Lévy-jump probability based on stagnation, and re-seeds population parts\n    if stagnation persists.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        # default population: scale with dim but limited by budget\n        if pop_size is None:\n            p = int(max(8, min(100, 8 + 2 * int(np.sqrt(self.dim)))))\n            p = min(p, max(4, self.budget // 10))\n            self.pop_size = max(4, p)\n        else:\n            self.pop_size = int(pop_size)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # Determine bounds: prefer func.bounds if provided, else default [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb))\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        range_norm = np.maximum(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # if nothing was evaluated (very tiny budget), return best known defaults\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # initialize best\n        best_idx = int(np.nanargmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.06  # initial probability of long jump\n        trust_radius = 0.15  # fraction of search range used for local Gaussian steps\n        min_trust = 1e-4\n        max_trust = 2.0\n        stagnation_counter = 0\n        gen = 0\n\n        # bookkeeping for adaptation\n        successes = 0\n        recent_successes = 0\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            gen_success = 0\n\n            # adapt number of inner iterations to available budget\n            inner_iters = self.pop_size\n            for i in range(inner_iters):\n                if evals >= self.budget:\n                    break\n\n                # choose strategy: long Lévy jump or DE\n                if rng.rand() < p_levy:\n                    # Lévy-like step: use Cauchy (standard_cauchy) clipped\n                    # create a jump centered on best_x scaled by trust_radius and range\n                    cauchy = rng.standard_cauchy(size=self.dim)\n                    # clip to avoid numeric explosions\n                    cauchy = np.clip(cauchy, -1e2, 1e2)\n                    scale = (0.5 + rng.rand() * 1.5) * trust_radius\n                    candidate = best_x + scale * cauchy * (range_vec / range_norm)\n                    # small chance to mix with current individual to keep diversity\n                    mix = rng.rand(self.dim) < 0.2\n                    candidate = np.where(mix, candidate, pop[i])\n                else:\n                    # Differential Evolution / rand/1/bin with per-individual F and CR\n                    # pick distinct indices\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    # jDE-like sampling of Fi and CRi\n                    Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                    CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    cr_mask[rng.randint(0, self.dim)] = True  # ensure at least one\n                    candidate = np.where(cr_mask, donor, pop[i])\n                    # small chance of occasional directed jump toward best\n                    if rng.rand() < 0.03:\n                        lam = rng.rand() * 0.6\n                        candidate = candidate * (1 - lam) + best_x * lam\n\n                # projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # greedy selection replacement (replace target if better)\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    gen_success += 1\n                    recent_successes += 1\n                    successes += 1\n                    # if this was DE branch with Fi/CRi, nudge means toward successful values\n                    if 'Fi' in locals():\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if 'CRi' in locals():\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    stagnation_counter += 1\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0  # reset stagnation on improvement\n\n                # clean local Fi/CRi to avoid accidental reuse\n                if 'Fi' in locals():\n                    del Fi\n                if 'CRi' in locals():\n                    del CRi\n\n                # tiny safeguard: if budget exhausted inside loop, break\n                if evals >= self.budget:\n                    break\n\n            # Trust-region local search around best: do a few local Gaussian probes\n            if evals < self.budget:\n                local_budget = min(self.budget - evals, max(1, self.dim // 2, self.pop_size // 4))\n                for _ in range(local_budget):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma scaled by trust_radius and per-dimension randomness\n                    sigma = (0.4 + rng.rand(self.dim) * 0.6) * trust_radius\n                    candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step -> contract trust region\n                        trust_radius = max(min_trust, trust_radius * 0.7)\n                        stagnation_counter = 0\n                        recent_successes += 1\n                    else:\n                        # unsuccessful local attempt -> expand slightly (to escape)\n                        trust_radius = min(max_trust, trust_radius * 1.05)\n                        stagnation_counter += 1\n\n            # generation-level adaptation of long-jump probability\n            if recent_successes == 0:\n                p_levy = min(0.5, p_levy * 1.15 + 0.01)  # encourage exploration if no success\n                # small random perturbation to means to avoid getting stuck\n                F_mean = np.clip(F_mean * (1.0 + rng.randn() * 0.01), 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * (1.0 + rng.randn() * 0.01), 0.0, 1.0)\n            else:\n                p_levy = max(0.01, p_levy * (0.98 if recent_successes < (0.2 * self.pop_size) else 0.95))\n                # reward stability\n                F_mean = np.clip(0.98 * F_mean + 0.02 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.9, 0.0, 1.0)\n\n            # if stagnation persists, re-seed part of population\n            if stagnation_counter > max(40, self.dim * 4):\n                k = max(1, self.pop_size // 2)\n                ids = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in ids:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # if a re-seeded individual is immediately good, update best\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # enlarge trust radius to explore more after re-seed\n                trust_radius = min(max_trust, trust_radius * 1.8)\n                stagnation_counter = 0\n\n            # safety clamp trust_radius\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n            # reset generation counters\n            recent_successes = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.301 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14718485573489337, 0.2060335933313472, 0.35808075789368166, 0.4913162665602664, 0.2722294580972554, 0.45497445283119176, 0.2700987473348492, 0.3293923473481265, 0.2603189013379683, 0.21996441511392972]}, "task_prompt": ""}
{"id": "bdba0745-0228-46ed-b103-a12b1b8ab208", "fitness": "-inf", "name": "AdaptiveHybridDELevyTrust", "description": "AdaptiveHybridDELevyTrust — a Differential Evolution core with jDE-style adaptation, occasional Lévy-flight long jumps when stagnating, and an online trust-region local search around the best solution (fast global exploration + focused local exploitation).", "code": "import numpy as np\n\nclass AdaptiveHybridDELevyTrust:\n    \"\"\"\n    Adaptive hybrid optimizer:\n      - Differential Evolution (DE/rand/1/bin) with simple jDE-like online adaptation of F and CR\n      - Occasional Lévy-like (Cauchy) jumps centered on current best for long-range exploration\n      - Trust-region local searches (Gaussian sampling around best) shrinking/expanding based on success\n      - Stagnation-driven increases in jump probability and occasional re-seeding\n\n    Usage: instantiate with budget (number of function evaluations) and problem dim.\n    Call the instance with the black-box func; func should support func(x) and\n    ideally provide func.bounds.lb / func.bounds.ub arrays (if missing, [-5,5]^dim is used).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # sensible default population size: grows with dim but bounded by budget\n        if pop_size is None:\n            default = max(8, 4 + 2 * self.dim)\n            self.pop_size = min(default, max(4, self.budget // 10))\n        else:\n            self.pop_size = int(pop_size)\n            self.pop_size = max(4, min(self.pop_size, max(4, self.budget // 5)))\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # try to read bounds; else use [-5,5]^dim\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0\n            ub = 5.0\n        # broadcast to dim\n        def ensure(b):\n            b = np.asarray(b, dtype=float)\n            if b.size == 1:\n                return np.full(self.dim, float(b))\n            if b.size == self.dim:\n                return b.copy()\n            # try broadcast\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        return ensure(lb), ensure(ub)\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        # parameters\n        F_mean = 0.6     # base differential weight\n        CR_mean = 0.9    # base crossover prob\n        tau_F = 0.1      # probability to resample F per individual (jDE-like)\n        tau_CR = 0.1     # probability to resample CR per individual\n        step_scale = 0.25\n        min_trust = 1e-8\n        max_trust = max(1e-8, range_norm * 2.0)\n        trust_radius = step_scale * max(1e-12, range_norm)\n\n        # Lévy / jump control\n        base_jump_prob = 0.03\n        jump_prob = base_jump_prob\n        max_jump_prob = 0.5\n\n        # bookkeeping\n        evals = 0\n        pop = np.empty((self.pop_size, self.dim), dtype=float)\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        # initialize population\n        for i in range(self.pop_size):\n            pop[i] = lb + self.rng.rand(self.dim) * range_vec\n\n        # Evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # If no budget to evaluate anything, return default\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # find best\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n\n        stagnation_counter = 0\n        gen = 0\n        max_stagnation_reset = max(50, self.dim * 10)\n\n        # generation loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            gen_successes = 0\n\n            # per-generation random ordering to avoid bias\n            order = np.arange(self.pop_size)\n            self.rng.shuffle(order)\n\n            for i in order:\n                if evals >= self.budget:\n                    break\n\n                xi = pop[i].copy()\n                # sample Fi and CRi (jDE-like)\n                if self.rng.rand() < tau_F:\n                    Fi = 0.4 + 0.55 * self.rng.rand()  # in (0.4,0.95)\n                else:\n                    Fi = F_mean\n                if self.rng.rand() < tau_CR:\n                    CRi = self.rng.rand()\n                else:\n                    CRi = CR_mean\n\n                # decide branch: Lévy jump vs DE\n                if self.rng.rand() < jump_prob:\n                    # Lévy-like Cauchy jump centered at best\n                    s = self.rng.standard_cauchy(self.dim)\n                    s = np.clip(s, -1e2, 1e2)\n                    # normalize scale to prevent huge jumps; scale by trust_radius\n                    s_norm = np.linalg.norm(s) + 1e-12\n                    step = (s / s_norm) * (trust_radius / max(1.0, np.sqrt(self.dim)))\n                    donor = best_x + Fi * step * range_vec\n                else:\n                    # DE/rand/1 mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback to random perturbation\n                        donor = xi + Fi * (self.rng.rand(self.dim) - 0.5) * 2.0 * trust_radius * range_vec\n                    else:\n                        r = self.rng.choice(idxs, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                trial = xi.copy()\n                jrand = self.rng.randint(self.dim)\n                mask = (self.rng.rand(self.dim) < CRi)\n                mask[jrand] = True\n                trial[mask] = donor[mask]\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate candidate\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection\n                if f_candidate <= fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    gen_successes += 1\n                    # nudge means toward successful parameters\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # small decay toward exploration\n                    F_mean = 0.999 * F_mean + 0.001 * Fi\n\n                # global best update\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation: trust-region local search around best\n            # decide number of local samples based on remaining budget and dimension\n            rem = self.budget - evals\n            if rem <= 0:\n                break\n            # local sample count: at most dim, but not exceed remaining budget\n            local_samples = min(max(1, self.dim), rem)\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: per-dim random scale * trust_radius\n                sigma = (0.2 + self.rng.rand(self.dim) * 0.8) * (trust_radius / max(1e-12, range_norm))\n                candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    local_success += 1\n                    stagnation_counter = 0\n                # If successful local step -> shrink trust radius to focus; else expand slightly\n                if local_success > 0:\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                else:\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # adapt jump probability depending on stagnation and recent successes\n            if gen_successes > 0 or local_success > 0:\n                # successful generation -> slightly reduce jump probability\n                jump_prob = max(base_jump_prob, jump_prob * 0.95)\n            else:\n                # stagnating -> increase jump probability to encourage escapes\n                jump_prob = min(max_jump_prob, jump_prob + 0.02)\n\n            # occasional re-seeding if strong stagnation\n            if stagnation_counter > max_stagnation_reset:\n                # reinitialize half of the population\n                k = max(1, self.pop_size // 2)\n                indices = self.rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices:\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    if evals >= self.budget:\n                        break\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # update best if needed\n                    if fvals[j] < best_f:\n                        best_f = float(fvals[j])\n                        best_x = pop[j].copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                # enlarge trust radius a bit after reset\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n                # slightly increase exploration after reseed\n                jump_prob = min(max_jump_prob, jump_prob + 0.05)\n\n            # safety clamps\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n            F_mean = float(np.clip(F_mean, 0.05, 1.0))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n        # final result\n        self.f_opt = float(self.f_opt)\n        self.x_opt = None if self.x_opt is None else np.asfarray(self.x_opt)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 778, in __getattr__, the following error occurred:\nAttributeError: `np.asfarray` was removed in the NumPy 2.0 release. Use `np.asarray` with a proper dtype instead.", "error": "In the code, line 778, in __getattr__, the following error occurred:\nAttributeError: `np.asfarray` was removed in the NumPy 2.0 release. Use `np.asarray` with a proper dtype instead.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a0a824e5-f527-4f1b-ad65-3ba5b715f070", "fitness": 0.18006263113074672, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style adaptation, occasional Lévy (Cauchy) long jumps, and an online trust-region local search around the current best.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (F and CR) are adapted online in a jDE-like fashion and trust-radius\n    is adapted from local search successes/failures.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default: scale population with dim but not larger than fraction of budget\n        if pop_size is None:\n            self.pop_size = min(max(8, 4 * self.dim), max(4, self.budget // 8))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = self.rng\n\n        # determine bounds from func if available, else default to [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure correct shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n        range_vec = ub - lb\n        if np.any(range_vec <= 0):\n            raise ValueError(\"Upper bounds must be greater than lower bounds per-dimension.\")\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial individuals until budget exhausted or population filled\n        init_count = min(self.pop_size, self.budget)\n        for i in range(init_count):\n            x = pop[i].copy()\n            fvals[i] = float(func(x))\n            evals += 1\n            # store best\n            if fvals[i] < self.f_opt:\n                self.f_opt = float(fvals[i])\n                self.x_opt = x.copy()\n\n        # if budget is zero or no evaluations possible\n        if evals == 0:\n            # nothing to do\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # mean differential weight\n        CR_mean = 0.5       # mean crossover prob\n        step_scale = 0.18   # base scale for Lévy and trust radius relative to search range\n        trust_radius = step_scale * np.mean(range_vec)  # initial trust radius (absolute)\n        min_trust = 1e-8\n        levy_prob = 0.06    # base probability for long Lévy/Cauchy jumps\n        max_levy_prob = 0.5\n        stagnation_counter = 0\n        stagnation_limit = max(30, 5 * self.dim)  # if no improvement for this many evals, escalate\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step(scale):\n            # standard Cauchy, clipped to avoid numerical blow-up\n            step = rng.standard_cauchy(self.dim)\n            # clip extreme tails\n            step = np.clip(step, -1e2, 1e2)\n            return step * scale\n\n        # main loop: process until budget exhausted\n        # iterate generations: we'll sequentially attempt to improve each member\n        while evals < self.budget:\n            improved_in_generation = False\n\n            # shuffle order to avoid bias\n            order = rng.permutation(self.pop_size)\n\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # decide branch: Levy jump (global exploration) vs DE (population-driven)\n                do_levy = (rng.rand() < levy_prob)\n                do_local = False  # local trust-region samples done per-generation below\n\n                candidate = None\n                Fi = None\n                CRi = None\n\n                if do_levy:\n                    # long-range exploration centered on the global best\n                    # scale relative to range and current trust radius\n                    scale = (0.6 * np.mean(range_vec)) * (1.0 + 0.5 * rng.rand())\n                    step = levy_step(scale)\n                    candidate = self.x_opt + step\n                else:\n                    # DE/rand/1 mutation with jDE-like adaptation of F and CR\n                    # choose three indices distinct from idx\n                    if self.pop_size < 4:\n                        # fallback to simple random perturbation if too small population\n                        Fi = np.clip(rng.normal(F_mean, 0.1), 0.1, 1.0)\n                        CRi = np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0)\n                        donor = pop[idx] + Fi * (rng.rand(self.dim) - 0.5) * range_vec\n                    else:\n                        idxs = list(range(self.pop_size))\n                        idxs.remove(idx)\n                        r = rng.choice(idxs, size=3, replace=False)\n                        r1, r2, r3 = r\n                        # adapt Fi and CRi\n                        Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0))\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    mask = rng.rand(self.dim) < CRi\n                    if not np.any(mask):\n                        mask[rng.randint(0, self.dim)] = True\n                    trial = pop[idx].copy()\n                    trial[mask] = donor[mask]\n\n                    # occasional modest bias toward best to speed convergence\n                    if rng.rand() < 0.08:\n                        trial = 0.85 * trial + 0.15 * self.x_opt\n\n                    candidate = trial\n\n                # project to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[idx]:\n                    # replace\n                    pop[idx] = candidate.copy()\n                    fvals[idx] = f_candidate\n                    improved_in_generation = True\n\n                    # adapt F_mean/CR_mean slightly toward successful Fi/CRi (if present)\n                    if Fi is not None:\n                        F_mean = 0.92 * F_mean + 0.08 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.92 * CR_mean + 0.08 * CRi\n\n                else:\n                    # small drift of CR_mean/F_mean away from failing parameters to keep diversity\n                    if Fi is not None:\n                        F_mean = 0.995 * F_mean + 0.005 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.995 * CR_mean + 0.005 * CRi\n\n                # update global best\n                if f_candidate < self.f_opt:\n                    self.f_opt = float(f_candidate)\n                    self.x_opt = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # dynamic adjustment of levy probability: if many failures, increase chance of jumps\n                if stagnation_counter > (stagnation_limit // 3):\n                    levy_prob = min(max_levy_prob, levy_prob * 1.03)\n                else:\n                    # slowly decay back to base\n                    levy_prob = max(0.03, levy_prob * 0.9995)\n\n            # End of generation: trust-region local search around best (if budget remains)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: a small handful scaled by dim and remaining budget\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            successes = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = trust_radius * (0.4 + 0.6 * rng.rand(self.dim))\n                local_step = rng.normal(0.0, sigma, size=self.dim)\n                cand_local = self.x_opt + local_step\n                cand_local = np.minimum(np.maximum(cand_local, lb), ub)\n                f_local = float(func(cand_local))\n                evals += 1\n                if f_local < self.f_opt:\n                    successes += 1\n                    # accept and shrink trust radius to focus search\n                    self.f_opt = f_local\n                    self.x_opt = cand_local.copy()\n                    trust_radius = max(min_trust, trust_radius * 0.72)\n                    stagnation_counter = 0\n                else:\n                    # expand a bit to escape local minima\n                    trust_radius = min(np.mean(range_vec), trust_radius * 1.08)\n\n            # adjust global parameters according to local successes\n            if successes > 0:\n                # reward exploitation: nudge means toward more exploitation\n                F_mean = np.clip(0.96 * F_mean, 0.05, 0.99)\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.9, 0.0, 1.0)\n            else:\n                # encourage exploration if no local improvements\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # stagnation handling: if long stagnation, re-seed part of the population\n            if stagnation_counter >= stagnation_limit:\n                k = max(1, self.pop_size // 2)\n                idxs_to_reset = rng.choice(self.pop_size, size=k, replace=False)\n                for j in idxs_to_reset:\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    # evaluate new member if budget allows\n                    if evals < self.budget:\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                        if fvals[j] < self.f_opt:\n                            self.f_opt = float(fvals[j])\n                            self.x_opt = pop[j].copy()\n                    else:\n                        fvals[j] = np.inf\n                # enlarge trust radius slightly after reset to encourage exploration\n                trust_radius = min(np.mean(range_vec), trust_radius * 1.5)\n                # make levy jumps more probable temporarily\n                levy_prob = min(max_levy_prob, levy_prob * 1.5)\n                stagnation_counter = 0  # reset stagnation after re-seeding\n\n        # final results\n        # ensure x_opt is numpy array\n        if self.x_opt is None:\n            # fallback: return best in current fvals\n            valid = np.isfinite(fvals)\n            if np.any(valid):\n                bi = np.argmin(fvals[valid])\n                self.x_opt = pop[bi].copy()\n                self.f_opt = float(fvals[bi])\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.180 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0837718550454084, 0.16601893989243288, 0.2146854148784677, 0.17889360078129624, 0.1571354436916369, 0.3577618225416903, 0.1885248178060367, 0.16902687390803384, 0.15318530295565835, 0.13162223980680576]}, "task_prompt": ""}
{"id": "d7afa242-8897-4cbf-86ee-62fca759f026", "fitness": 0.3228045675395482, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy-flight global jumps and an online-adaptive trust-region local search (explore globally with Lévy/DE, exploit locally with trust-region), with per-individual parameter adaptation and stagnation-driven restarts.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line idea: Use adaptive DE for steady progress, occasional heavy-tailed\n    Lévy jumps to escape basins, and a shrinking/expanding trust-region local\n    search around the best to quickly refine solutions.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population: scaled with dim but limited by budget\n        if pop_size is None:\n            self.pop_size = max(4, min(40, int(6 * self.dim)))\n            # make sure not larger than fraction of budget (each initial eval)\n            self.pop_size = min(self.pop_size, max(1, self.budget // 5))\n        else:\n            self.pop_size = int(pop_size)\n\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Ensure bounds are arrays of length self.dim. Accept scalars or arrays.\n        \"\"\"\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds must be scalar or of length dim\")\n\n    def __call__(self, func):\n        # Prepare bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # safety: if any zero-range dimension, avoid divide by zero later\n        range_vec[range_vec == 0.0] = 1.0\n\n        # bookkeeping\n        evals = 0\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n\n        # evaluate initial population carefully with budget\n        fvals = np.full(self.pop_size, np.inf)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = func(x)\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If budget was too small to evaluate any individual, do a random search min(budget, ) fallback\n        if evals == 0:\n            # try to evaluate a few random points until budget exhausted\n            for _ in range(self.budget):\n                x = lb + self.rng.rand(self.dim) * range_vec\n                f = func(x)\n                evals += 1\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n            return self.f_opt, self.x_opt\n\n        # If some individuals not evaluated, fill with random guesses (they won't be used until evaluated)\n        for i in range(self.pop_size):\n            if not np.isfinite(fvals[i]):\n                pop[i] = lb + self.rng.rand(self.dim) * range_vec\n                fvals[i] = np.inf\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.3\n        # probability of attempting a Lévy jump on a candidate\n        p_levy = 0.02\n        # trust region parameters\n        trust_radius = 0.2  # relative to variable range (0..1)\n        min_trust = 1e-3\n        max_trust = 1.0\n\n        stagnation_counter = 0\n        best_history = [self.f_opt]\n\n        # Helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step(scale=1.0):\n            # Use a Cauchy (alpha=1) heavy-tailed step, but limit extremes\n            z = self.rng.standard_cauchy(self.dim) * scale\n            # clip extreme outliers\n            z = np.clip(z, -1e3, 1e3)\n            return z\n\n        # Helper: project vector into bounds\n        def project_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # Main loop: iterate until budget exhausted\n        # We'll perform generation-style updates but not exceed budget\n        gen = 0\n        # memory for simple parameter adaptation\n        success_F = []\n        success_CR = []\n\n        while evals < self.budget:\n            gen += 1\n\n            # per-generation small random walk of means to avoid getting stuck\n            F_mean = np.clip(F_mean + self.rng.normal(0, 0.01), 0.05, 1.0)\n            CR_mean = np.clip(CR_mean + self.rng.normal(0, 0.02), 0.0, 1.0)\n\n            # iterate individuals sequentially\n            indices = np.arange(self.pop_size)\n            self.rng.shuffle(indices)\n            for idx in indices:\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[idx]\n                f_target = fvals[idx]\n\n                # sample individual strategy parameters (jDE-style)\n                Fi = np.clip(self.rng.normal(F_mean, 0.15), 0.05, 1.0)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.2), 0.0, 1.0)\n\n                # Build donor vector:\n                # with small probability (increases with stagnation) do Levy jump centered on best\n                best_idx = np.argmin(fvals)\n                best_x = pop[best_idx].copy()\n                best_f = fvals[best_idx]\n\n                # Choose two other distinct vectors\n                idxs = [j for j in range(self.pop_size) if j != idx and j != best_idx]\n                if len(idxs) >= 2:\n                    r1, r2 = self.rng.choice(idxs, 2, replace=False)\n                else:\n                    # fallback to random generation\n                    r1 = self.rng.randint(0, self.pop_size)\n                    r2 = self.rng.randint(0, self.pop_size)\n\n                # create base for donor\n                if self.rng.rand() < 0.5:\n                    # current-to-best/1 style\n                    donor = x_target + Fi * (best_x - x_target) + Fi * (pop[r1] - pop[r2])\n                else:\n                    # rand/1 style\n                    donor = pop[r1] + Fi * (pop[r2] - pop[idx])\n\n                # occasional Lévy jump to donor around best, probability increases if stagnating\n                adaptive_p_levy = np.clip(p_levy + 0.0005 * stagnation_counter, 0.01, 0.5)\n                if self.rng.rand() < adaptive_p_levy:\n                    # scale according to trust_radius and global range\n                    scale = 0.6 * trust_radius\n                    donor = best_x + levy_step(scale=scale) * range_vec\n\n                # Crossover (binomial)\n                cross = self.rng.rand(self.dim) < CRi\n                # ensure at least one dimension from donor\n                cross[self.rng.randint(0, self.dim)] = True\n                trial = np.where(cross, donor, x_target)\n\n                # small local trust-region interpolation to bias trial near best occasionally\n                if self.rng.rand() < 0.1:\n                    # move a bit toward best\n                    trial = trial + (best_x - trial) * (0.3 * trust_radius)\n\n                # project to bounds\n                trial = project_to_bounds(trial)\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_trial = func(trial)\n                evals += 1\n\n                # Selection: greedy\n                if f_trial < f_target:\n                    pop[idx] = trial\n                    fvals[idx] = f_trial\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n                    # update global best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        stagnation_counter = 0\n                else:\n                    # keep target (no change)\n                    stagnation_counter += 1\n\n                # Slightly adapt global means toward successful Fi/CRi periodically\n                if len(success_F) >= max(1, self.pop_size // 6):\n                    mean_success_F = float(np.mean(success_F))\n                    mean_success_CR = float(np.mean(success_CR))\n                    # move means a bit toward success\n                    F_mean = 0.85 * F_mean + 0.15 * mean_success_F\n                    CR_mean = 0.85 * CR_mean + 0.15 * mean_success_CR\n                    # clear memory but keep some history\n                    success_F = success_F[-self.pop_size // 4:]\n                    success_CR = success_CR[-self.pop_size // 4:]\n\n            # End of generation adjustments\n\n            # Trust-region local search around best: spend a small number of evals\n            # Number of local samples depends on remaining budget and dimension\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample_count chosen conservatively\n            sample_count = int(np.clip(2 + self.dim // 6, 1, min(10, remaining)))\n            # anisotropic sigma: base trust_radius scaled by rand per-dim\n            for _ in range(sample_count):\n                if evals >= self.budget:\n                    break\n                sigma = trust_radius * (0.5 + self.rng.rand(self.dim))\n                candidate = best_x + self.rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = project_to_bounds(candidate)\n                f_candidate = func(candidate)\n                evals += 1\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # adaptive adjustments based on progress in recent generations\n            best_history.append(self.f_opt)\n            if len(best_history) > 10:\n                best_history.pop(0)\n\n            # if many generations without improvement, increase Lévy chance and diversify\n            if len(best_history) >= 8 and best_history[-1] >= best_history[0] - 1e-12:\n                # no meaningful improvement\n                stagnation_counter += 1\n                p_levy = min(0.4, p_levy * 1.05 + 0.001)\n                # slight increase in population-level diversity by jittering some individuals\n                if stagnation_counter % 20 == 0:\n                    n_reseed = max(1, self.pop_size // 4)\n                    sel = self.rng.choice(self.pop_size, n_reseed, replace=False)\n                    for j in sel:\n                        pop[j] = project_to_bounds(best_x + levy_step(scale=0.5 * trust_radius) * range_vec)\n                        # do not evaluate immediately; they will be when chosen\n            else:\n                # progress -> slightly reduce global leapiness\n                p_levy = max(0.01, p_levy * 0.98)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 15):\n                n_reinit = max(1, self.pop_size // 2)\n                sel = self.rng.choice(self.pop_size, n_reinit, replace=False)\n                for j in sel:\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = np.inf  # will be evaluated in subsequent loop\n                # slightly enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.8)\n                stagnation_counter = 0\n                # also nudge parameter means\n                F_mean = min(1.0, F_mean * 1.05)\n                CR_mean = min(1.0, CR_mean * 1.05)\n\n            # safety: ensure trust_radius remains within bounds\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n        # final bookkeeping\n        if self.x_opt is None:\n            # fallback: return best seen in population if not set by local search\n            idx = np.argmin(fvals)\n            self.f_opt = float(fvals[idx])\n            self.x_opt = pop[idx].copy()\n\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.323 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15972992822478282, 0.18546058086952366, 0.41366503091521656, 0.36663606940566573, 0.43180870163388074, 0.4090768248188864, 0.2690714552528142, 0.4916293747785475, 0.3407050035890953, 0.1602627059070696]}, "task_prompt": ""}
{"id": "0773eea3-627f-46b3-9489-f40f8efa9bad", "fitness": "-inf", "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy/Cauchy long jumps, and trust-region local search with online step-size adaptation and simple success-history driven parameter tuning.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight-like (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (mutation F and crossover CR) are adapted online from success history.\n    Works with functions defined on [-5,5]^dim (BBOB style).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population size scaled with dimension but kept reasonable\n            self.pop_size = max(8, min(50, 6 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.default_rng(self.seed)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_bounds(self, x, lb, ub):\n        # project to bounds\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def __call__(self, func):\n        rng = self.rng\n        budget = int(self.budget)\n        dim = self.dim\n        pop_size = self.pop_size\n\n        # determine bounds (BBOB often provides bounds; otherwise assume [-5,5])\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            # ensure correct shape\n            if lb.size == 1:\n                lb = np.full(dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(dim, ub.item())\n        except Exception:\n            lb = np.full(dim, -5.0)\n            ub = np.full(dim, 5.0)\n\n        range_vec = ub - lb\n        if np.any(range_vec <= 0):\n            range_vec = np.maximum(range_vec, 1e-6)\n\n        # initialize population\n        pop = rng.uniform(lb, ub, size=(pop_size, dim))\n        fvals = np.full(pop_size, np.inf)\n        evals = 0\n\n        # evaluate initial population until budget exhausted or all evaluated\n        for i in range(pop_size):\n            if evals >= budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                # if evaluation fails, leave inf\n                fvals[i] = np.inf\n            evals += 1\n\n        # If we couldn't evaluate full population due to tiny budget, leave rest as inf and do best among evaluated.\n        valid_mask = np.isfinite(fvals)\n        if np.any(valid_mask):\n            best_idx = int(np.argmin(fvals))\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = pop[0].copy()\n            return self.f_opt, self.x_opt\n\n        # initialize adaptation parameters\n        F_mean = 0.6     # initial mean for mutation factor\n        CR_mean = 0.9    # initial mean for crossover\n        p_levy = 0.05    # base probability of performing a long jump\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius (Euclidean)\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        # SHADE-like memory for successful parameters (we keep small lists)\n        success_F = []\n        success_CR = []\n        memory_size = max(5, pop_size // 5)\n\n        # stagnation counters\n        no_improve_evals = 0\n        stagnation_reset_threshold = max(200, budget // 10)\n\n        # helper: generate Lévy-like heavy-tailed step using standard Cauchy\n        def levy_step(scale=1.0):\n            # Cauchy vector limited to avoid numerical blow-ups\n            s = rng.standard_cauchy(dim) * scale\n            # clip extreme outliers\n            s = np.clip(s, -1e2, 1e2)\n            return s\n\n        # helper: sample Fi and CRi from means with some variability\n        def sample_F_CR():\n            # Fi: sample from log-normal-ish (via normal on logit)\n            # simpler: sample from Cauchy around F_mean then clip\n            Fi = F_mean + 0.1 * rng.standard_cauchy()\n            Fi = float(np.clip(Fi, 0.05, 1.0))\n            # CRi: normal around CR_mean\n            CRi = CR_mean + 0.1 * rng.normal()\n            CRi = float(np.clip(CRi, 0.0, 1.0))\n            return Fi, CRi\n\n        # main loop: iterate over population members sequentially generating trial vectors\n        # We'll stop when budget used up\n        generation = 0\n        while evals < budget:\n            generation += 1\n            # shuffle order for fairness\n            order = rng.permutation(pop_size)\n            gen_successes = 0\n            gen_success_F = []\n            gen_success_CR = []\n\n            for idx in order:\n                if evals >= budget:\n                    break\n\n                target = pop[idx].copy()\n                target_f = fvals[idx]\n\n                # sample Fi and CRi adaptively\n                Fi, CRi = sample_F_CR()\n\n                # decide mutation strategy: with some probability use DE/best/1 to intensify\n                if rng.random() < 0.4:\n                    # DE/best/1: best + F*(r1 - r2)\n                    candidates = np.arange(pop_size)\n                    candidates = candidates[candidates != idx]\n                    if candidates.size < 2:\n                        # fallback to rand\n                        r1, r2 = idx, idx\n                    else:\n                        r1, r2 = rng.choice(candidates, 2, replace=False)\n                    donor = best_x + Fi * (pop[r1] - pop[r2])\n                else:\n                    # DE/rand/1\n                    candidates = np.arange(pop_size)\n                    candidates = candidates[candidates != idx]\n                    if candidates.size < 3:\n                        # fallback\n                        donor = target + Fi * rng.normal(size=dim)\n                    else:\n                        r1, r2, r3 = rng.choice(candidates, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # occasionally perform Lévy-centered jump around best (explorative)\n                if rng.random() < p_levy:\n                    # Lévy step scale proportional to range and trust radius (encourage far jumps early)\n                    scale = 0.5 * np.mean(range_vec) + 0.5 * trust_radius\n                    levy = levy_step(scale=1.0)  # unit Cauchy vector\n                    donor = best_x + levy * scale * rng.random()\n\n                # crossover (binomial)\n                cross = rng.random(dim) < CRi\n                if not np.any(cross):\n                    # ensure at least one component from donor\n                    cross[rng.integers(dim)] = True\n                trial = np.where(cross, donor, target)\n\n                # projection to bounds\n                trial = self._ensure_bounds(trial, lb, ub)\n\n                # evaluate trial if budget allows\n                if evals >= budget:\n                    break\n                try:\n                    f_trial = float(func(trial))\n                except Exception:\n                    f_trial = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                if f_trial < target_f:\n                    # replace\n                    pop[idx] = trial\n                    fvals[idx] = f_trial\n                    gen_successes += 1\n                    gen_success_F.append(Fi)\n                    gen_success_CR.append(CRi)\n                    # update global best\n                    if f_trial < best_f:\n                        best_f = f_trial\n                        best_x = trial.copy()\n                        no_improve_evals = 0\n                    else:\n                        no_improve_evals += 1\n                else:\n                    # failed: slight move by replacing target with small gaussian with low prob to maintain diversity\n                    no_improve_evals += 1\n                    if rng.random() < 0.02:\n                        perturb = rng.normal(scale=0.01 * np.std(pop, axis=0, ddof=1, where=np.isfinite(pop).all(axis=1)), size=dim)\n                        pop[idx] = self._ensure_bounds(target + perturb, lb, ub)\n\n            # end of generation adjustments\n            # update F_mean and CR_mean using successful parameters (Lehmer mean for F, arithmetic for CR)\n            if gen_successes > 0:\n                # update memory lists\n                for Fi in gen_success_F:\n                    success_F.append(Fi)\n                for CRi in gen_success_CR:\n                    success_CR.append(CRi)\n\n                # keep memory bounded\n                if len(success_F) > memory_size:\n                    success_F = success_F[-memory_size:]\n                if len(success_CR) > memory_size:\n                    success_CR = success_CR[-memory_size:]\n\n                # Lehmer mean for F (gives larger weights to larger Fi)\n                Fs = np.array(success_F, dtype=float)\n                if Fs.size > 0 and np.all(Fs >= 0):\n                    F_mean = float(np.sum(Fs**2) / np.sum(Fs))\n                    F_mean = np.clip(F_mean, 0.05, 1.0)\n\n                CRs = np.array(success_CR, dtype=float)\n                if CRs.size > 0:\n                    CR_mean = float(np.mean(CRs))\n                    CR_mean = np.clip(CR_mean, 0.0, 1.0)\n\n            else:\n                # no successes: gently nudge toward more exploration\n                F_mean = min(0.95, F_mean * 1.02)\n                CR_mean = max(0.1, CR_mean * 0.98)\n\n            # trust-region local search around best: try a few local Gaussian samples\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n\n            # determine number of local samples: small handful scaled by dim and remaining budget\n            local_budget = max(0, min(remaining, int(max(1, min(10, dim // 2 + 1)))))\n            local_successes = 0\n            for _ in range(local_budget):\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (trust_radius * (0.5 + rng.random(dim)))  # per-dim\n                candidate = best_x + rng.normal(scale=sigma)\n                candidate = self._ensure_bounds(candidate, lb, ub)\n                if evals >= budget:\n                    break\n                try:\n                    f_c = float(func(candidate))\n                except Exception:\n                    f_c = np.inf\n                evals += 1\n                if f_c < best_f:\n                    best_f = f_c\n                    best_x = candidate.copy()\n                    local_successes += 1\n                    # insert into population by replacing worst\n                    worst_idx = int(np.argmax(fvals))\n                    pop[worst_idx] = candidate.copy()\n                    fvals[worst_idx] = f_c\n\n            # adjust trust radius based on local success\n            if local_successes > 0:\n                # successful local moves: shrink trust to focus\n                trust_radius = max(1e-8, trust_radius * (0.7 ** local_successes))\n                # reward exploitation by nudging CR_mean slightly up\n                CR_mean = min(0.99, CR_mean + 0.02 * local_successes)\n            else:\n                # expand a bit to allow escape\n                trust_radius = min(max_trust, trust_radius * 1.1)\n                # encourage more Lévy jumps if stagnating\n                p_levy = min(0.5, p_levy * 1.05)\n\n            # adjust p_levy based on generation successes from DE\n            if gen_successes > pop_size * 0.2:\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                p_levy = min(0.5, p_levy * 1.02)\n\n            # stagnation handling: if no improvement for a while, re-seed part of population\n            if no_improve_evals > stagnation_reset_threshold:\n                # reinitialize half of population randomly to diversify\n                num_reinit = max(1, pop_size // 2)\n                reinit_idx = rng.choice(pop_size, size=num_reinit, replace=False)\n                for ridx in reinit_idx:\n                    pop[ridx] = rng.uniform(lb, ub)\n                    fvals[ridx] = np.inf  # will be re-evaluated next cycles until budget allows\n                # expand trust radius to encourage escapes\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # increase long jump chance\n                p_levy = min(0.5, p_levy * 2.0)\n                no_improve_evals = 0  # reset counter\n\n            # if population has unevaluated individuals (inf fvals) and we still have budget, evaluate them\n            for i in range(pop_size):\n                if evals >= budget:\n                    break\n                if not np.isfinite(fvals[i]):\n                    try:\n                        fvals[i] = float(func(pop[i]))\n                    except Exception:\n                        fvals[i] = np.inf\n                    evals += 1\n                    # update best if needed\n                    if fvals[i] < best_f:\n                        best_f = fvals[i]\n                        best_x = pop[i].copy()\n                        no_improve_evals = 0\n\n            # slight decay of trust radius over generations to focus search slowly\n            trust_radius = max(1e-12, trust_radius * 0.995)\n\n        # finalize\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 382, in _broadcast_to, the following error occurred:\nValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (12,)  and requested shape (12,2)", "error": "In the code, line 382, in _broadcast_to, the following error occurred:\nValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (12,)  and requested shape (12,2)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "30710b91-188c-44c0-83d9-87ff945425a7", "fitness": 0.46845394192916895, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy (Cauchy) jumps and an online-trust-region local search; adapts mutation/crossover and step sizes and reseeds on stagnation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps (via Cauchy-like steps) for long-range\n    exploration, and an adaptive trust-region local search around the\n    current best. Parameters (F, CR, trust radius) are adapted online.\n    Works within bounds provided by func.bounds or assumes [-5,5]^dim.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # sensible default population size: scale with dimension but keep small if budget small\n        if pop_size is None:\n            pop_size = int(min(max(8, 4 * self.dim), 50, max(4, self.budget // 10)))\n        self.pop_size = max(4, pop_size)\n\n        # internal bookkeeping to return\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or sequence and return array of length dim\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        raise ValueError(\"Bounds must be scalar or length dim\")\n\n    def __call__(self, func):\n        # bounds handling: fallback to [-5,5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # clamp helper\n        def clamp(x):\n            return np.minimum(ub, np.maximum(lb, x))\n\n        # remaining evaluations\n        evals_used = 0\n        remaining = lambda: self.budget - evals_used\n\n        # init population uniformly in bounds but evaluate only as budget allows\n        pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        pop_f = np.full(self.pop_size, np.inf)\n\n        # If budget is tiny, evaluate as many as possible; others remain unevaluated\n        for i in range(self.pop_size):\n            if remaining() <= 0:\n                break\n            pop_f[i] = func(pop[i])\n            evals_used += 1\n            if pop_f[i] < self.f_opt:\n                self.f_opt = pop_f[i].item()\n                self.x_opt = pop[i].copy()\n\n        # If no evaluations possible, just return a random point (but budget==0)\n        if evals_used == 0:\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters and adaptive state\n        F_mean = 0.6  # mean mutation factor\n        CR_mean = 0.3  # mean crossover probability\n        tau_F = 0.1    # probability to change F per individual\n        tau_CR = 0.1   # probability to change CR per individual\n        F_min, F_max = 0.05, 0.95\n\n        # Trust-region radius (fraction of search range)\n        search_range = ub - lb\n        trust_radius = 0.2 * np.linalg.norm(search_range) / np.sqrt(self.dim)  # scalar radius in absolute scale\n        trust_shrink = 0.8\n        trust_expand = 1.2\n        trust_min = 1e-8 * np.linalg.norm(search_range)\n        trust_max = np.linalg.norm(search_range) * 2.0\n\n        # Levy (Cauchy) jump parameters\n        levy_prob = 0.03  # base chance per mutation to do a levy jump\n        levy_scale = 0.5  # scale relative to search_range norm\n\n        # stagnation and resets\n        no_improve_evals = 0\n        stagnation_threshold = max(100, 10 * self.dim)\n        reset_fraction = 0.5\n\n        # Adaptive update rate\n        adapt_rate = 0.1\n\n        gen = 0\n        # main loop: process sequentially until budget exhausted\n        while evals_used < self.budget:\n            gen += 1\n            # per-generation success accumulators\n            success_F = []\n            success_CR = []\n            gen_improvements = 0\n\n            # shuffle order to avoid bias\n            indices = np.random.permutation(self.pop_size)\n            for idx in indices:\n                if evals_used >= self.budget:\n                    break\n\n                x_target = pop[idx].copy()\n                f_target = pop_f[idx]\n\n                # jDE-like adaptive Fi and CRi\n                if np.random.rand() < tau_F:\n                    Fi = F_mean + 0.1 * np.random.randn()\n                else:\n                    Fi = F_mean\n                Fi = np.clip(Fi + 0.2 * np.random.rand(), F_min, F_max)\n\n                if np.random.rand() < tau_CR:\n                    CRi = CR_mean + 0.2 * (np.random.rand() - 0.5)\n                else:\n                    CRi = CR_mean\n                CRi = np.clip(CRi, 0.0, 1.0)\n\n                # mutation: choose three distinct indices not equal to idx\n                ids = [i for i in range(self.pop_size) if i != idx]\n                if len(ids) < 3:\n                    # fallback: small gaussian perturbation\n                    donor = x_target + Fi * np.random.randn(self.dim)\n                else:\n                    a, b, c = np.random.choice(ids, 3, replace=False)\n                    # with small prob use best as base to intensify exploitation\n                    if np.random.rand() < 0.15:\n                        base = self.x_opt.copy()\n                    else:\n                        base = pop[a].copy()\n                    donor = base + Fi * (pop[b] - pop[c])\n\n                # Occasional Levy jump centered on current best to encourage exploration\n                if np.random.rand() < levy_prob:\n                    # draw Cauchy-like heavy tail: standard Cauchy via tan(pi*(u-0.5))\n                    u = np.clip(np.random.rand(self.dim), 1e-12, 1 - 1e-12)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    # scale relative to search range and trust radius\n                    scale_vec = (levy_scale * np.linalg.norm(search_range) / np.sqrt(self.dim))\n                    levy = self.x_opt + np.sign(cauchy) * np.minimum(np.abs(cauchy), 1e4)[:, None].flatten() * scale_vec\n                    # combine donor with levy jump to get a very exploratory candidate\n                    donor = 0.6 * donor + 0.4 * clamp(levy)\n\n                # DE binomial crossover\n                trial = x_target.copy()\n                jrand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CRi or j == jrand:\n                        trial[j] = donor[j]\n\n                # Trust region influence: nudge trial towards best if within trust radius\n                vec_to_best = self.x_opt - trial\n                dist_to_best = np.linalg.norm(vec_to_best)\n                if dist_to_best < max(trust_radius, 1e-12):\n                    # inside trust region: add small gaussian perturbation scaled by trust radius\n                    trial += 0.5 * (trust_radius / (1.0 + dist_to_best)) * np.random.randn(self.dim)\n\n                # project to bounds\n                trial = clamp(trial)\n\n                # evaluate trial (if budget allows)\n                if evals_used >= self.budget:\n                    break\n                f_trial = func(trial)\n                evals_used += 1\n\n                # selection: greedy replacement\n                if f_trial < f_target:\n                    pop[idx] = trial\n                    pop_f[idx] = f_trial\n                    gen_improvements += 1\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n\n                    # update global best\n                    if f_trial < self.f_opt:\n                        improvement = self.f_opt - f_trial\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        no_improve_evals = 0\n                    else:\n                        no_improve_evals += 1\n                else:\n                    no_improve_evals += 1\n\n                # per-individual cleanup (not strictly necessary)\n                del Fi, CRi, donor, trial, f_trial\n\n            # End of generation: adapt means based on successes\n            if len(success_F) > 0:\n                # move F_mean toward mean of successful Fi\n                new_F_mean = (1 - adapt_rate) * F_mean + adapt_rate * np.mean(success_F)\n                F_mean = np.clip(new_F_mean, F_min, F_max)\n            else:\n                # slight decay toward exploration if no successes\n                F_mean = max(F_min, F_mean * (1 - 0.01))\n\n            if len(success_CR) > 0:\n                CR_mean = (1 - adapt_rate) * CR_mean + adapt_rate * np.mean(success_CR)\n            else:\n                CR_mean = CR_mean * (1 - 0.01)\n\n            # trust-region local search around best: sample a small batch if budget allows\n            local_budget = min(max(1, self.dim // 2), remaining())\n            local_success = 0\n            for _ in range(local_budget):\n                if evals_used >= self.budget:\n                    break\n                # anisotropic Gaussian noise scaled by trust radius\n                sigma = trust_radius * (0.2 + np.random.rand(self.dim))\n                candidate = self.x_opt + sigma * np.random.randn(self.dim)\n                candidate = clamp(candidate)\n                f_cand = func(candidate)\n                evals_used += 1\n                if f_cand < self.f_opt:\n                    self.f_opt = f_cand\n                    self.x_opt = candidate.copy()\n                    local_success += 1\n                    no_improve_evals = 0\n                else:\n                    no_improve_evals += 1\n\n            # adjust trust radius based on local success\n            if local_success > 0 or gen_improvements > 0:\n                # shrink to focus\n                trust_radius = max(trust_min, trust_radius * (trust_shrink ** (1 + local_success)))\n            else:\n                # expand a bit to escape\n                trust_radius = min(trust_max, trust_radius * trust_expand)\n\n            # adjust levy probability adaptively: increase when stagnating\n            if no_improve_evals > stagnation_threshold:\n                levy_prob = min(0.5, levy_prob * 1.5)\n            else:\n                levy_prob = max(0.01, levy_prob * 0.99)\n\n            # occasional partial reseed on strong stagnation\n            if no_improve_evals > 3 * stagnation_threshold and remaining() > 0:\n                n_reseed = max(1, int(self.pop_size * reset_fraction))\n                reseed_idx = np.random.choice(self.pop_size, n_reseed, replace=False)\n                for ridx in reseed_idx:\n                    if evals_used >= self.budget:\n                        break\n                    pop[ridx] = np.random.uniform(lb, ub)\n                    pop_f[ridx] = func(pop[ridx])\n                    evals_used += 1\n                    if pop_f[ridx] < self.f_opt:\n                        self.f_opt = pop_f[ridx].item()\n                        self.x_opt = pop[ridx].copy()\n                # after reseed, make trust radius larger to explore\n                trust_radius = min(trust_max, trust_radius * 2.0)\n                no_improve_evals = 0  # reset stagnation counter\n\n            # small safeguard: if population hasn't been fully evaluated and budget allows, try to evaluate missing ones\n            for i in range(self.pop_size):\n                if evals_used >= self.budget:\n                    break\n                if np.isinf(pop_f[i]):\n                    pop_f[i] = func(pop[i])\n                    evals_used += 1\n                    if pop_f[i] < self.f_opt:\n                        self.f_opt = pop_f[i].item()\n                        self.x_opt = pop[i].copy()\n\n        # final return\n        return float(self.f_opt), self.x_opt.copy() if self.x_opt is not None else None", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.468 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12035746735259045, 0.16979933870177877, 0.48882570893080857, 0.7756872202673003, 0.6896109085135187, 0.7584710783866655, 0.2982220924029225, 0.4962231983847267, 0.6518399266804731, 0.23550247967090476]}, "task_prompt": ""}
{"id": "a124d400-23f4-44ca-851a-ae437eb3b2d7", "fitness": 0.22069553135594266, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like parameter adaptation, occasional Lévy-flight global jumps, and a trust-region Gaussian local search — balances fast global exploration and focused local exploitation with online adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (jDE-like),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension and budget but kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # make sure population not larger than a fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        # if scalar\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # prepare bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # fallback to standard -5..5 if bounds are ill-formed\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            # pick best among evaluated\n            valid_indices = np.where(valid)[0]\n            best_idx_in_valid = np.argmin(fvals[valid_indices])\n            best_idx = valid_indices[best_idx_in_valid]\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # no evals possible or budget zero\n            self.f_opt = float(np.inf)\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean (jDE-like adaptation)\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        min_trust = 1e-6\n        max_trust = range_norm * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # limit extreme outliers to avoid numerical blow-up\n            s = self.rng.standard_cauchy(size=self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main generational loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # per-generation small jitter around means to encourage diversity\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # decide branch: Levy global jump vs DE mutation\n                if self.rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    scale = (step_scale * 0.5 + 0.5 * self.rng.rand())\n                    donor = best_x + scale * step * (range_vec / max(range_vec.mean(), 1e-12))\n                    # treat donor as trial directly (no crossover)\n                    trial = donor.copy()\n                    # we did not sample Fi/CRi in this branch\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1-like mutation with jDE-like per-individual Fi and CRi\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # degenerate small-pop fallback: sample random around best\n                        Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.1, 1.0)\n                        CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                        donor = pop[i] + Fi * (best_x - pop[i])\n                    else:\n                        r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                        Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 1.0)\n                        CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover to produce trial\n                    cr_mask = self.rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[self.rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # one evaluation (if budget allows)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population (target-to-trial)\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # move means slightly toward successful parameters if available\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    else:\n                        # nudge towards exploration baseline\n                        F_mean = 0.98 * F_mean + 0.02 * 0.6\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    else:\n                        CR_mean = 0.98 * CR_mean + 0.02 * 0.9\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # clear Fi/CRi local markers (not strictly necessary)\n                Fi = None\n                CRi = None\n\n            # End of generation adjustments\n\n            # trust-region local search around best: sample a few Gaussian-noise candidates\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                sigma = (0.5 + self.rng.rand(self.dim) * 0.5) * (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.8)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # adapt exploration-exploitation tradeoff based on successes\n            if successes > 0:\n                # fewer Levy jumps when making progress\n                decay = 0.95 if successes > max(1, int(self.pop_size * 0.2)) else 0.98\n                p_levy = max(0.01, p_levy * decay)\n                # gently move means toward exploitation-friendly defaults\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation handling: increase chance of long jumps and diversify F\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.1, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many successive checks, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                chosen = self.rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in chosen:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius a bit after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.221 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13510304943858775, 0.17221550242838568, 0.30304848629210046, 0.26426106174237585, 0.2325939151607529, 0.25526568394204985, 0.22744049803941813, 0.24072604281349996, 0.21568067601995133, 0.16062039768230463]}, "task_prompt": ""}
{"id": "5a12d4ab-a471-4411-b4fa-e69a83213ff9", "fitness": 0.24407416381065566, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online parameter adaptation and resets.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population size scaled with dimension but bounded\n        if pop_size is None:\n            ps = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # ensure population not larger than a fraction of budget\n            ps = min(ps, max(4, self.budget // 20))\n            self.pop_size = int(ps)\n        else:\n            self.pop_size = int(pop_size)\n        # RNG\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # internal state for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # bounds from func\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * (ub - lb)\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (honoring budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # find initial best\n        if np.isfinite(fvals).any():\n            best_idx = int(np.nanargmin(fvals))\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.05       # initial probability of Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius multipliers\n        trust_radius = 0.2 * range_norm  # scalar trust radius (in absolute units)\n        min_trust = 1e-8\n        max_trust = max(1e-8, range_norm * 2.0)\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step(d):\n            # Cauchy-like heavy tails; limit extreme outliers\n            s = rng.standard_cauchy(d)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation small exploration of means (jDE style)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample individual Fi and CRi from distributions centered at means\n                Fi = F_mean * (0.5 + rng.random() * 1.0)  # somewhat variable\n                Fi = float(np.clip(Fi, 0.05, 0.99))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n\n                # choose mutation style: with probability p_levy do a long jump\n                if rng.random() < p_levy:\n                    # Levy jump centered on best_x for exploration\n                    step = step_scale * (0.5 + rng.random() * 1.5) * (trust_radius / max(range_norm, 1e-12))\n                    donor = best_x + step * levy_step(self.dim) * (range_vec / max(np.mean(range_vec), 1e-12))\n                    # small chance to mix with DE mutation for diversity\n                    if self.pop_size >= 3 and rng.random() < 0.3:\n                        idxs = np.arange(self.pop_size)\n                        idxs = idxs[idxs != i]\n                        r1, r2 = rng.choice(idxs, 2, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r1])\n                else:\n                    # DE/current-to-rand or DE/rand/1 hybrid\n                    if self.pop_size >= 3:\n                        idxs = np.arange(self.pop_size)\n                        idxs = idxs[idxs != i]\n                        r = rng.choice(idxs, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        # combination: current-to-best plus difference vector\n                        donor = pop[i] + Fi * (best_x - pop[i]) + Fi * (pop[r1] - pop[r2])\n                    else:\n                        # fallback: random perturbation\n                        donor = pop[i] + Fi * rng.normal(0, 1.0, size=self.dim)\n\n                # binomial crossover\n                cr_mask = rng.random(self.dim) < CRi\n                # ensure at least one dimension from donor\n                if not cr_mask.any():\n                    cr_mask[rng.integers(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # one evaluation (if budget permits)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt means towards successful Fi/CRi (small step)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # slight diversification: nudge means away when unsuccessful\n                    F_mean = np.clip(F_mean * 1.0005, 0.05, 0.99)\n                    CR_mean = np.clip(CR_mean * 0.9995, 0.0, 1.0\n\n                                        )\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation adjustments\n\n            # trust-region local search around best: small anisotropic Gaussian samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dim relative to trust radius and overall range\n                sigma = (0.5 + rng.random(self.dim) * 0.5) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    trust_radius = max(trust_radius, min_trust)\n                    stagnation_counter += 1\n\n            # adapt exploration probability and means based on successes\n            if successes > 0:\n                # encourage exploitation a bit\n                p_levy = max(0.01, p_levy * 0.98)\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                choose_idx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in choose_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * (ub - lb)\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # possibly adopt a newly-evaluated better one\n                    if fvals[j] < best_f:\n                        best_f = float(fvals[j])\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.244 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13943035722479102, 0.20272033763009978, 0.28968190255106685, 0.3270088332199832, 0.28643420253961926, 0.2872888885450873, 0.2361426161289646, 0.27751006510946286, 0.2267659924415102, 0.1677584427159715]}, "task_prompt": ""}
{"id": "502d9895-4d35-48ff-a2a7-d189c1924120", "fitness": 0.2283290523877838, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with per-individual F/CR adaptation, occasional Lévy-flight long jumps for global exploration, and a trust-region Gaussian local search around the best solution with online step-size tuning.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # default population size scaled with dimension\n            self.pop_size = max(6, min(80, int(8 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # keep reproducibility option\n        self.seed = seed\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # RNG\n        rng = np.random.RandomState(self.seed)\n\n        # get bounds from func if available, else default [-5,5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        # ensure population not larger than budget (can't evaluate more unique individuals than budget initially)\n        pop_size = min(self.pop_size, max(2, self.budget))\n        pop = lb + rng.rand(pop_size, self.dim) * range_vec\n        fvals = np.full(pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population up to budget\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if no evaluations possible\n        if evals == 0:\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # find current best among evaluated individuals\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n        best_idx = np.argmin(fvals[valid])\n        best_indices = np.where(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.5\n        CR_mean = 0.9\n        p_levy = 0.06           # base probability of attempting a Levy jump\n        step_scale = 0.25      # base scale for Levy and trust radius relative to range_norm\n        trust_radius = 0.18 * range_norm\n        min_trust = 1e-8\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n        max_stagnation = max(50, self.dim * 10)\n\n        # success history for adapting means\n        successes_F = []\n        successes_CR = []\n\n        def levy_step():\n            # use heavy-tailed Cauchy as a simple Levy-like generator\n            s = rng.standard_cauchy(self.dim)\n            # clip to avoid extreme floating behavior\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # for each target vector\n            idxs_all = np.arange(pop_size)\n            # shuffle order to reduce bias\n            order = rng.permutation(pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # per-individual Fi and CRi (jDE-like sampling around means)\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                # DE/rand/1 mutation\n                # pick three distinct indices different from idx\n                choices = idxs_all[idxs_all != idx]\n                r = rng.choice(choices, size=3, replace=False)\n                r1, r2, r3 = r[0], r[1], r[2]\n                donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # crossover (binomial)\n                cr_mask = rng.rand(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[rng.randint(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[idx])\n                trial = np.clip(trial, lb, ub)\n\n                # with some probability perform a Lévy jump around the best instead of the DE trial\n                if rng.rand() < p_levy:\n                    # do a Levy-based exploratory candidate centered at best\n                    step = levy_step()\n                    # scale step by trust radius and problem range\n                    levy_scale = step_scale * (trust_radius / (range_norm + 1e-12))\n                    candidate = best_x + levy_scale * step * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                else:\n                    candidate = trial\n\n                # Evaluate candidate if budget permits\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement (replace target if improved)\n                if f_candidate < fvals[idx]:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes_F.append(Fi)\n                    successes_CR.append(CRi)\n                    # if this also improves global best, update\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful DE -> focus a bit\n                        trust_radius *= 0.90\n                        trust_radius = np.clip(trust_radius, min_trust, max_trust)\n                        stagnation_counter = 0\n                    else:\n                        # modest improvement locally\n                        stagnation_counter = max(0, stagnation_counter - 1)\n                else:\n                    # unsuccessful trial: nothing changes, small penalty to stagnation\n                    stagnation_counter += 1\n\n                # occasional small nudge of global parameters toward successful Fi/CRi\n                if len(successes_F) > 0 and len(successes_CR) > 0 and (len(successes_F) % 20 == 0):\n                    # update means using Lehmer-like averaging (small learning rate)\n                    meanF = np.mean(successes_F[-20:])\n                    meanCR = np.mean(successes_CR[-20:])\n                    F_mean = np.clip(0.9 * F_mean + 0.1 * meanF, 0.05, 0.99)\n                    CR_mean = np.clip(0.9 * CR_mean + 0.1 * meanCR, 0.0, 1.0)\n\n            # end of DE generation\n\n            # adapt F_mean/CR_mean from successes this generation (if any)\n            if len(successes_F) > 0:\n                meanF = np.mean(successes_F)\n                F_mean = np.clip(0.9 * F_mean + 0.1 * meanF, 0.05, 0.99)\n                successes_F.clear()\n            if len(successes_CR) > 0:\n                meanCR = np.mean(successes_CR)\n                CR_mean = np.clip(0.9 * CR_mean + 0.1 * meanCR, 0.0, 1.0)\n                successes_CR.clear()\n\n            # trust-region local search around best: a few Gaussian perturbations\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: trust_radius scaled per-dimension by random factor\n                per_dim_scale = rng.rand(self.dim) * 0.8 + 0.2  # avoid zero\n                sigma_vec = (trust_radius / (range_norm + 1e-12)) * per_dim_scale * range_vec\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma_vec\n                candidate = np.clip(candidate, lb, ub)\n\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local search => shrink trust_radius to intensify search\n                    trust_radius *= 0.80\n                    trust_radius = np.clip(trust_radius, min_trust, max_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful local attempt: expand trust to try escaping local minima\n                    trust_radius *= 1.06\n                    trust_radius = np.clip(trust_radius, min_trust, max_trust)\n                    stagnation_counter += 1\n\n            # adapt exploration rate p_levy based on stagnation\n            if stagnation_counter > self.dim * 3:\n                # increase chance of long jumps\n                p_levy = min(0.6, p_levy * 1.12 + 0.01)\n                # widen F to encourage larger differential steps\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n            else:\n                # gradually reduce levy jumps when progressing\n                p_levy = max(0.01, p_levy * 0.98)\n\n            # strong stagnation reset: re-seed part of the population\n            if stagnation_counter > max_stagnation:\n                k = max(1, pop_size // 2)\n                reinit_indices = rng.choice(np.arange(pop_size), size=k, replace=False)\n                for j in reinit_indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # after reseed, expand trust radius a bit to escape previous basin\n                trust_radius = np.clip(trust_radius * 1.3, min_trust, max_trust)\n                stagnation_counter = 0\n                # slightly increase global diversity parameters\n                F_mean = np.clip(F_mean * 1.05, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.228 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10012162811417935, 0.15602294045185527, 0.27534755087055895, 0.20426301710332317, 0.21140945196077376, 0.46029641452494496, 0.2447219649644452, 0.2669073758060725, 0.21712410530615467, 0.1470760747755302]}, "task_prompt": ""}
{"id": "417146a5-e3ad-4252-929d-d30756a3969a", "fitness": 0.17904736037846025, "name": "AdaptiveHybridDELevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online adaptation of F/CR and jump probability.", "code": "import numpy as np\n\nclass AdaptiveHybridDELevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n    - Population-based Differential Evolution (DE/rand/1-like with per-individual Fi and CRi adaptation).\n    - Occasional Lévy-flight-like jumps centered on the current best for long-range exploration.\n    - Trust-region Gaussian local search around the current best to intensify exploitation.\n    - Online adaptation of F_mean, CR_mean and p_levy based on recent successes.\n    Suitable for continuous box-bounded problems; respects a strict evaluation budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n\n        # Set a reasonable population size based on dimension and budget if not provided\n        if pop_size is None:\n            # keep population moderate but growing with dim, limited by budget\n            self.pop_size = max(4, min(10 * self.dim, max(4, self.budget // 30)))\n        else:\n            self.pop_size = int(pop_size)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.copy().astype(float)\n        # try to broadcast if shape matches other orientation\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to dimension {}\".format(self.dim))\n\n    def __call__(self, func):\n        # read and format bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        if np.any(range_vec <= 0):\n            raise ValueError(\"Upper bound must be greater than lower bound for all dimensions.\")\n\n        # initialize population uniformly\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate as many initial individuals as budget allows\n        init_count = min(self.pop_size, self.budget - evals)\n        for i in range(init_count):\n            x = pop[i].copy()\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # If no evaluations possible, return defaults\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # initialize best\n        best_idx = int(np.argmin(fvals[:init_count]))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyperparameters (adapted online)\n        F_mean = 0.6        # mean differential weight\n        CR_mean = 0.9       # mean crossover prob\n        p_levy = 0.12       # probability of doing a Lévy jump for a candidate\n        levy_scale = 0.6    # scale of Lévy jumps relative to variable range (max)\n        trust_scale = 0.20 * float(np.mean(range_vec))  # initial trust-region std scale\n        min_trust = 1e-8\n        max_trust = float(np.mean(range_vec))\n\n        no_improve_counter = 0\n        gen = 0\n\n        # helper: Levy-like heavy-tailed step (using Cauchy)\n        def levy_step():\n            # produce a heavy-tailed normalized vector and scale by levy_scale and range\n            s = self.rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e4, 1e4)\n            std = np.std(s)\n            if std < 1e-12:\n                s = self.rng.normal(0, 1, size=self.dim)\n                std = np.std(s)\n            s = s / (std + 1e-12)\n            # scale by per-dim range and global levy_scale\n            step = s * (levy_scale * range_vec)\n            return step\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            gen_attempts = 0\n\n            # per-generation small perturbation of means to encourage exploration\n            F_mean = np.clip(F_mean + self.rng.normal(0, 0.01), 0.05, 0.95)\n            CR_mean = np.clip(CR_mean + self.rng.normal(0, 0.02), 0.0, 1.0)\n\n            # iterate population\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi (jDE-like but simple)\n                Fi = np.clip(self.rng.normal(F_mean, 0.12), 0.05, 0.95)\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # choose if levy or DE\n                if self.rng.random() < p_levy:\n                    # Lévy jump centered on best_x\n                    donor = best_x + levy_step()\n                    # slight local bias to preserve some structure\n                    if self.rng.random() < 0.3:\n                        donor = 0.5 * donor + 0.5 * pop[i]\n                else:\n                    # DE/rand/1 with current-to-best influence\n                    # pick three distinct indices != i\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r1, r2, r3 = self.rng.choice(idxs, size=3, replace=False)\n                    # Classic rand/1 donor\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # add a gentle current-to-best component to encourage convergence\n                    donor = donor + 0.3 * Fi * (best_x - pop[i])\n\n                # binomial crossover between target pop[i] and donor\n                cr_mask = self.rng.random(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[self.rng.integers(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # project to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                f_candidate = float(func(candidate))\n                evals += 1\n                gen_attempts += 1\n\n                # selection - greedy replacement of target\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # update best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        no_improve_counter = 0\n                    # move means slightly toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # small drift of means away from unsuccessful Fi to encourage diversity\n                    F_mean = np.clip(0.995 * F_mean + 0.005 * Fi, 0.05, 0.95)\n                    CR_mean = np.clip(0.995 * CR_mean + 0.005 * CRi, 0.0, 1.0)\n                    no_improve_counter += 1\n\n                # break if budget used\n                if evals >= self.budget:\n                    break\n\n            # End of population loop: trust-region local search around best if budget remains\n            if evals < self.budget:\n                # number of local samples depends on remaining budget and dimension\n                remaining = self.budget - evals\n                local_samples = min(max(2, self.dim // 2), remaining, 12)\n                improved_local = 0\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma: scale per-dim by range_vec\n                    per_dim_scale = (range_vec / (np.mean(range_vec) + 1e-12))\n                    noise = self.rng.normal(0, 1.0, size=self.dim) * (trust_scale * per_dim_scale)\n                    candidate = np.clip(best_x + noise, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        improved_local += 1\n                        no_improve_counter = 0\n                        # successful local step => shrink trust region to focus\n                        trust_scale = max(min_trust, trust_scale * 0.85)\n                        trust_scale = min(trust_scale, max_trust)\n                    else:\n                        # unsuccessful => soften (slightly increase) to escape tiny traps\n                        trust_scale = min(max_trust, trust_scale * 1.06)\n\n                # if many local successes, slightly reduce levy probability (focus search)\n                if improved_local > 0:\n                    p_levy = max(0.005, p_levy * (0.96 if improved_local > 1 else 0.98))\n\n            # post-generation adaptation of p_levy and means\n            if gen_attempts > 0:\n                success_rate = successes / max(1, gen_attempts)\n                # if many successes, reduce global jumps; if stagnating, increase\n                if success_rate > 0.2:\n                    p_levy = max(0.005, p_levy * 0.95)\n                else:\n                    p_levy = min(0.5, p_levy * (1.02 + 0.2 * (no_improve_counter > (5 * self.dim))))\n\n                # slightly cool CR_mean when good, else randomize a bit\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.5 * (1 - success_rate), 0.0, 1.0)\n\n            # strong stagnation handling: re-seed part of the population if no improvement for long\n            if no_improve_counter > max(60, self.dim * 6):\n                k = max(1, self.pop_size // 3)\n                # choose individuals to reinitialize\n                to_replace = self.rng.choice(np.arange(self.pop_size), size=k, replace=False)\n                for j in to_replace:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # if any new reinitialized is better update best\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        no_improve_counter = 0\n                # after reseed, slightly enlarge trust region to explore new regions\n                trust_scale = min(max_trust, trust_scale * 1.25)\n                # reduce levy probability a bit to let new population exploit\n                p_levy = max(0.02, p_levy * 0.85)\n                # reset stagnation counter partially\n                no_improve_counter = no_improve_counter // 3\n\n        # finalize\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveHybridDELevyTrust scored 0.179 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11669792291279579, 0.16300575978765275, 0.22891275624032803, 0.17643395802645234, 0.19286110171244886, 0.19936562818153225, 0.21586766045135986, 0.1823715992404905, 0.17135850666600516, 0.14359871056553675]}, "task_prompt": ""}
{"id": "f3221e5e-4613-48bd-a947-6649f764160e", "fitness": 0.23540161523450442, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range escapes, and trust-region local refinement with online step-size adaptation and restart when stagnating.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n        # default population size scaled with dimension but kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # also bound by budget so we don't waste initial evaluations unnecessarily\n            self.pop_size = min(self.pop_size, max(4, self.budget // 10))\n        else:\n            self.pop_size = max(4, int(pop_size))\n        # final results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b_arr = np.asarray(b, dtype=float)\n        if b_arr.size == 1:\n            return np.full(self.dim, float(b_arr.item()), dtype=float)\n        # try to broadcast\n        return np.broadcast_to(b_arr, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        rng = self.rng\n\n        # read bounds from func (many BBOB wrappers provide bounds.lb / bounds.ub)\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            # fallback: use [-5,5] as stated in the problem\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_ = ub - lb\n        # safe minimal nonzero for trust radius\n        max_trust = max(range_) * 0.5\n        min_trust = max(range_) * 1e-6\n        # initialize trust radius as a fraction of domain (start coarse)\n        trust_radius = max(range_) * 0.25\n\n        # initialize population in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * (ub - lb)\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population sequentially up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # if no evaluations possible (budget==0), return\n        if evals == 0:\n            self.f_opt = float(np.inf)\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # initial best\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # adaptive hyper-parameters\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.08\n\n        # jDE-like per-individual parameters\n        Fi_arr = np.full(self.pop_size, F_mean)\n        CRi_arr = np.full(self.pop_size, CR_mean)\n        tau_F = 0.1\n        tau_CR = 0.1\n\n        # counters and bookkeeping\n        stagnation_counter = 0\n        successes = 0\n        generation = 0\n\n        # helper: Lévy-like heavy-tailed step using Cauchy (a simple heavy tail)\n        def levy_vector(scale=1.0):\n            # standard Cauchy via standard_cauchy. Clip extreme outliers for numeric stability.\n            # produce a dim-vector\n            raw = rng.standard_cauchy(size=self.dim)\n            raw = np.clip(raw, -1e2, 1e2)\n            return raw * float(scale)\n\n        # main loop: generations until budget exhausted\n        while evals < self.budget:\n            generation += 1\n\n            # iterate through population as targets\n            indices = np.arange(self.pop_size)\n            rng.shuffle(indices)\n            for ii in indices:\n                if evals >= self.budget:\n                    break\n\n                # update per-individual jDE parameters with small prob\n                if rng.rand() < tau_F:\n                    Fi_arr[ii] = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                if rng.rand() < tau_CR:\n                    CRi_arr[ii] = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                Fi = Fi_arr[ii]\n                CRi = CRi_arr[ii]\n\n                # decide whether to perform a Lévy jump or DE mutation\n                if rng.rand() < p_levy:\n                    # center around best_x to attempt long-range exploration\n                    # scale by trust_radius and domain size\n                    # we include a gaussian envelope to avoid pure one-shot huge steps\n                    levy_scale = (trust_radius / max_trust + 0.05) * 0.5\n                    step = levy_vector(scale=1.0) * levy_scale * range_\n                    # candidate is best_x plus heavy-tailed step\n                    candidate = best_x + step * rng.normal(scale=1.0, size=self.dim)\n                else:\n                    # DE/rand/1 mutation (ensure distinct indices)\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != ii]\n                    if idxs.size < 3:\n                        # fall back to small gaussian perturbation if population too small\n                        donor = pop[ii] + Fi * (rng.randn(self.dim) * range_ * 0.01)\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover with at least one dimension from donor\n                    jrand = rng.randint(self.dim)\n                    cross = rng.rand(self.dim) < CRi\n                    cross[jrand] = True\n                    candidate = np.where(cross, donor, pop[ii])\n\n                # trust-region occasional refinement: with small prob blend with best neighborhood\n                if rng.rand() < 0.05:\n                    candidate = 0.5 * candidate + 0.5 * (best_x + rng.normal(scale=trust_radius, size=self.dim) * range_)\n\n                # project candidate to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy\n                replace = False\n                if not np.isfinite(fvals[ii]) or f_candidate <= fvals[ii]:\n                    replace = True\n\n                if replace:\n                    pop[ii] = candidate.copy()\n                    fvals[ii] = f_candidate\n                    successes += 1\n                    # nudge global means toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # if improvement vs global best\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                        # successful local exploitation -> shrink trust region\n                        trust_radius = max(min_trust, trust_radius * 0.85)\n                    else:\n                        # successful replacement but not global improvement\n                        trust_radius = min(max_trust, trust_radius * 0.98)\n                else:\n                    # unsuccessful mutation\n                    stagnation_counter += 1\n                    # expand trust radius slightly to try escaping\n                    trust_radius = min(max_trust, trust_radius * 1.02)\n\n                # occasionally adjust p_levy based on stagnation\n                if stagnation_counter > max(30, self.dim * 3):\n                    p_levy = min(0.5, p_levy * 1.05)\n                else:\n                    p_levy = max(0.02, p_levy * 0.995)\n\n                # strong stagnation: partial restart\n                if stagnation_counter > max(80, self.dim * 8):\n                    # reinitialize half the population randomly (keep best)\n                    n_replace = max(1, self.pop_size // 2)\n                    replace_idx = rng.choice([i for i in range(self.pop_size) if i != np.argmin(fvals)], n_replace, replace=False)\n                    for j in replace_idx:\n                        pop[j] = lb + rng.rand(self.dim) * range_\n                        fvals[j] = np.inf  # will be evaluated later until budget exhausted\n                    # enlarge trust region to explore more\n                    trust_radius = min(max_trust, trust_radius * 2.0)\n                    stagnation_counter = 0\n                    # reduce p_levy reset to encourage local then re-increase if stagnation continues\n                    p_levy = 0.12\n\n            # end of generation: do a focused trust-region local search around best_x\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # pick a small number of local samples depending on remaining budget and dim\n            local_samples = min(max(1, self.dim // 2), remaining, 10)\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic local gaussian: scale by trust_radius and per-dim range\n                sigma = trust_radius * (0.5 + rng.rand(self.dim))\n                candidate = best_x + rng.normal(scale=sigma) * range_\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.75)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slightly expand to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # mild long-term adaptation of means based on successes\n            if successes > 0:\n                # encourage slightly smaller CR if many successes (fine-tuning)\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.5, 0.0, 1.0)\n                successes = 0  # reset per-generation successes\n\n            # safety: if best_x was NaN or invalid, break\n            if best_x is None or not np.isfinite(best_f):\n                break\n\n        # final assign and return\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.235 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11702500627784318, 0.26324802101791955, 0.291960942340179, 0.28400671824285484, 0.22220269726488184, 0.26365877315223396, 0.25788458873581954, 0.243555836969238, 0.22459651332164898, 0.18587705502242546]}, "task_prompt": ""}
{"id": "b80fc0d3-bb6c-49fc-84b3-46c761f7e054", "fitness": 0.306242283564369, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and trust-region local search with online step-size adaptation for fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population scaled with dimension but bounded\n        if pop_size is None:\n            self.pop_size = min(max(8, int(8 + 1.5 * self.dim)), 60)\n        else:\n            self.pop_size = int(pop_size)\n        # ensure population doesn't exceed budget drastically\n        self.pop_size = max(4, min(self.pop_size, max(4, self.budget // 5)))\n        self.rng = np.random.RandomState(seed)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.reshape(self.dim)\n        # try to broadcast: if scalar-like shape, fill; otherwise raise\n        try:\n            arr = np.broadcast_to(b, (self.dim,))\n            return np.array(arr, dtype=float)\n        except Exception:\n            raise ValueError(\"Bounds must be scalar or length-d arrays compatible with dim\")\n\n    def __call__(self, func):\n        # prepare bounds and range\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_mean = max(range_vec.mean(), 1e-12)\n\n        # helpers\n        def levy_step():\n            # heavy-tailed step using Cauchy; clip extreme outliers for stability\n            s = self.rng.standard_cauchy(self.dim)\n            # scale down extremes smoothly\n            s = np.tanh(s / 5.0)\n            return s\n\n        # initialize population uniformly in bounds\n        pop_size = self.pop_size\n        pop = lb + self.rng.rand(pop_size, self.dim) * range_vec\n        fvals = np.full(pop_size, np.inf, dtype=float)\n\n        evals = 0\n        best_f = np.inf\n        best_x = None\n\n        # evaluate initial population until budget or filled\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n            if f < best_f:\n                best_f = f\n                best_x = x.copy()\n\n        # if no evaluations possible, return default\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # initial probability of a Lévy jump\n        min_trust = 1e-8\n        trust_radius = max(1e-6, 0.08 * range_mean)  # initial trust-region scale\n        max_trust = 2.0 * range_mean\n\n        stagnation_counter = 0\n        gen = 0\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # iterate population sequentially\n            for i in range(pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample individual mutation parameters (jDE-like)\n                Fi = float(np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # choose between Lévy-based global jump or DE operator\n                if (self.rng.rand() < p_levy) and (best_x is not None):\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    scale = trust_radius * (0.5 + 0.5 * self.rng.rand())\n                    donor = best_x + scale * step * (range_vec / range_mean)\n                else:\n                    # DE rand/1 mutation\n                    # ensure distinct indices\n                    idxs = self.rng.choice(pop_size, 3, replace=False)\n                    # if i is among idxs, replace with others until not\n                    if i in idxs:\n                        # draw again until i not included (rare for typical sizes)\n                        avail = [j for j in range(pop_size) if j != i]\n                        idxs = self.rng.choice(avail, 3, replace=False)\n                    r1, r2, r3 = idxs\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cr_mask = self.rng.rand(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[self.rng.randint(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n                candidate = np.clip(trial, lb, ub)\n\n                # one evaluation\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection\n                if f_candidate < fvals[i]:\n                    # successful replacement\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    # adapt means toward successful params\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                        # successful local improvement -> focus\n                        trust_radius = max(min_trust, trust_radius * 0.7)\n                    else:\n                        # slight increase to help exploration if we replace but not best\n                        trust_radius = min(max_trust, trust_radius * 1.02)\n                else:\n                    # unsuccessful -> encourage exploration slowly\n                    stagnation_counter += 1\n                    trust_radius = min(max_trust, trust_radius * 1.01)\n\n                # dynamic adjustment of Lévy probability based on stagnation\n                if stagnation_counter > max(20, self.dim):\n                    p_levy = min(0.6, p_levy * 1.04)\n                else:\n                    p_levy = max(0.01, p_levy * 0.995)\n\n                if evals >= self.budget:\n                    break\n\n            # end of generation: small trust-region local search around best\n            if evals >= self.budget:\n                break\n\n            if best_x is not None:\n                remaining = self.budget - evals\n                # sample count: small handful scaled with dim and remaining budget\n                local_samples = min(remaining, max(1, int(3 + self.dim // 10)))\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic gaussian noise scaled by trust radius and per-dim randomness\n                    sigma = trust_radius * (0.2 + 0.8 * self.rng.rand(self.dim))\n                    perturb = self.rng.normal(0.0, 1.0, size=self.dim) * sigma * (range_vec / range_mean)\n                    candidate = np.clip(best_x + perturb, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        trust_radius = max(min_trust, trust_radius * 0.6)\n                        stagnation_counter = 0\n                    else:\n                        trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # stagnation handling: partial re-seed if stuck\n            if stagnation_counter > max(50, self.dim * 5) and evals < self.budget:\n                k = max(1, pop_size // 2)\n                # pick k individuals to reinitialize\n                to_reinit = self.rng.choice(pop_size, k, replace=False)\n                for idx in to_reinit:\n                    pop[idx] = lb + self.rng.rand(self.dim) * range_vec\n                    if evals < self.budget:\n                        fvals[idx] = float(func(pop[idx]))\n                        evals += 1\n                        if fvals[idx] < best_f:\n                            best_f = fvals[idx]\n                            best_x = pop[idx].copy()\n                            stagnation_counter = 0\n                # broaden trust radius slightly to escape\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n\n            # small safeguard to keep means inside reasonable range\n            F_mean = float(np.clip(F_mean, 0.05, 1.0))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n        # finish\n        self.f_opt = best_f\n        self.x_opt = None if best_x is None else best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.306 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12152704148372517, 0.1667097851163144, 0.3353361398559508, 0.7952704760132311, 0.4686507031539169, 0.27930802873859484, 0.23388366691047446, 0.28774713042050615, 0.2304844048912964, 0.14350545905967926]}, "task_prompt": ""}
{"id": "7d10073e-7ce7-427d-a93a-2007a5818d8d", "fitness": 0.2606054824065317, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online step-size adaptation and simple population reseeding.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters (F, CR,\n    trust radius) are adapted online based on recent success history.\n    Designed for continuous box-bounded problems (e.g., BBOB).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # sensible default population size: scales with dim but not too large relative to budget\n        if pop_size is None:\n            self.pop_size = max(6, min(40, 4 + 3 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n\n        # ensure population not larger than fraction of budget (leave room for local search)\n        self.pop_size = min(self.pop_size, max(2, self.budget // 10))\n\n        # state to be filled on run\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.broadcast_to(b, (self.dim,))\n        if b.size != self.dim:\n            # try to broadcast along first dimension if possible\n            try:\n                return np.broadcast_to(b, (self.dim,))\n            except Exception:\n                raise ValueError(\"Bounds array has incompatible shape.\")\n        return b\n\n    def __call__(self, func):\n        # extract bounds and ensure arrays\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # fallback safety\n        range_vec[range_vec == 0.0] = 1e-8\n\n        rng = self.rng\n\n        # internal parameters with modest adaptive updates\n        F_mean = 0.6   # mutation scale mean\n        CR_mean = 0.5  # crossover mean\n        levy_prob = 0.05  # base probability for long Lévy jump (adapted on stagnation)\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # absolute trust radius in parameter space units\n        trust_min = 1e-6 * np.linalg.norm(range_vec)\n        trust_max = 2.0 * np.linalg.norm(range_vec)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate initial population while respecting budget\n        init_to_eval = min(self.pop_size, self.budget)\n        for i in range(init_to_eval):\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n            # update incumbent\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If budget exhausted during initialization, return\n        if evals >= self.budget:\n            return float(self.f_opt), None if self.x_opt is None else self.x_opt\n\n        # bookkeeping for adaptations\n        stagnation_counter = 0\n        best_f_history = [self.f_opt]\n        recent_successes = 0\n\n        # helper: Levy-like heavy-tail step using truncated Cauchy\n        def levy_step():\n            # standard Cauchy gives heavy tails; truncate to avoid huge jumps\n            s = rng.standard_cauchy(self.dim)\n            # truncate extreme outliers and scale to modest size\n            s = np.clip(s, -10.0, 10.0)\n            # normalize by median absolute deviation to make scale less dependent on dim\n            mad = np.median(np.abs(s - np.median(s))) + 1e-9\n            s = s / mad\n            return s\n\n        # main loop until budget exhausted\n        while evals < self.budget:\n            # per-generation success counters\n            gen_successes = 0\n            gen_attempts = 0\n\n            # adapt per-generation F and CR draws using means\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # decide operator: Levy jump with small probability OR if trust radius tiny -> more leaps\n                prob_levy = levy_prob * (1.0 + stagnation_counter / (50.0 + 1e-9))\n                if rng.rand() < prob_levy:\n                    # Lévy jump centered on current best to explore far-away basins\n                    if self.x_opt is None:\n                        center = lb + rng.rand(self.dim) * range_vec\n                    else:\n                        center = self.x_opt\n                    step = levy_step()\n                    # scale step so that typical jump size is related to trust radius and problem scale\n                    scale = 0.8 * trust_radius / (np.linalg.norm(range_vec) + 1e-12)\n                    candidate = center + (step * scale) * range_vec\n                    operator = \"levy\"\n                else:\n                    # Differential Evolution style mutation + crossover\n                    # select three distinct indices different from i\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    r1, r2, r3 = rng.choice(idxs, size=3, replace=False)\n                    # sample Fi from Cauchy around F_mean (ensures occasional large F)\n                    Fi = rng.standard_cauchy() * 0.1 + F_mean\n                    Fi = float(np.clip(Fi, 0.01, 1.5))\n                    # sample CRi around CR_mean\n                    CRi = float(np.clip(rng.normal(loc=CR_mean, scale=0.15), 0.0, 1.0))\n\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    # ensure at least one dim taken\n                    cr_mask[rng.randint(0, self.dim)] = True\n                    candidate = np.where(cr_mask, donor, pop[i])\n                    operator = \"de\"\n                    gen_attempts += 1\n\n                # project candidate to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate if we still have budget\n                if evals < self.budget:\n                    f_cand = float(func(candidate))\n                    evals += 1\n                else:\n                    break\n\n                # selection: greedy per-individual\n                if operator == \"de\":\n                    # if better than target, replace target and update adaptation parameters\n                    if f_cand < fvals[i]:\n                        # success\n                        pop[i] = candidate\n                        fvals[i] = f_cand\n                        gen_successes += 1\n                        recent_successes += 1\n                        # nudge means toward successful Fi/CRi (simple exponential recency)\n                        # Fi and CRi were defined in this branch\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                        # tighten trust radius modestly on success\n                        trust_radius = max(trust_min, trust_radius * 0.98)\n                    else:\n                        # failure: relax trust radius a little\n                        trust_radius = min(trust_max, trust_radius * 1.005)\n                else:\n                    # Levy jump branch: if improvement inject candidate into population by replacing worst\n                    worst_idx = int(np.argmax(fvals))\n                    if f_cand < fvals[worst_idx]:\n                        pop[worst_idx] = candidate\n                        fvals[worst_idx] = f_cand\n                        gen_successes += 1\n                        recent_successes += 1\n                        # successful large jump -> shrink trust a bit to allow local search\n                        trust_radius = max(trust_min, trust_radius * 0.95)\n                    else:\n                        # unsuccessful large jump: increase trust radius slightly to encourage escapes\n                        trust_radius = min(trust_max, trust_radius * 1.02)\n\n                # update global best if improved\n                if f_cand < self.f_opt:\n                    self.f_opt = f_cand\n                    self.x_opt = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 0.0  # keep stagnation monitoring below\n\n                # adapt levy probability if many successes are zero (stagnation)\n                if (len(best_f_history) > 0) and (self.f_opt >= min(best_f_history[-10:], default=self.f_opt)):\n                    # no improvement in recent history; increase chance of levy\n                    levy_prob = min(0.3, levy_prob * 1.02)\n                else:\n                    levy_prob = max(0.01, levy_prob * 0.995)\n\n                # record best_f_history\n                best_f_history.append(self.f_opt)\n                # limit history length\n                if len(best_f_history) > 200:\n                    best_f_history.pop(0)\n\n                # break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of one generation-like loop across population\n\n            # quick trust-region local search around current best:\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # number of local samples depends on remaining budget and dim\n            local_samples = int(min(remaining, max(1, 3 + self.dim // 10)))\n            # anisotropic gaussian: scale per-dim by a random fraction to allow occasional directionality\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                if self.x_opt is None:\n                    center = lb + rng.rand(self.dim) * range_vec\n                else:\n                    center = self.x_opt\n                # random per-dimension sigma proportional to trust_radius and problem scale\n                per_dim_scale = rng.rand(self.dim) * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                sigma = np.maximum(1e-12, per_dim_scale) * range_vec\n                candidate = center + rng.normal(0.0, 1.0, size=self.dim) * sigma\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                f_cand = float(func(candidate))\n                evals += 1\n\n                # if improved global best, accept and tighten trust region\n                if f_cand < self.f_opt:\n                    self.f_opt = f_cand\n                    self.x_opt = candidate.copy()\n                    trust_radius = max(trust_min, trust_radius * 0.85)\n                    recent_successes += 1\n                else:\n                    # unsuccessful local step => slightly expand to escape shallow trap\n                    trust_radius = min(trust_max, trust_radius * 1.01)\n\n                # optionally insert into population (replace worst)\n                worst_idx = int(np.argmax(fvals))\n                if f_cand < fvals[worst_idx]:\n                    pop[worst_idx] = candidate\n                    fvals[worst_idx] = f_cand\n\n            # end local search\n\n            # stagnation and reseeding checks\n            # if little improvement observed in a long stretch, reseed part of population\n            if len(best_f_history) >= 50:\n                recent_min = min(best_f_history[-50:])\n                if recent_min >= best_f_history[0] and recent_min >= self.f_opt:\n                    # little to no improvement over the window\n                    stagnation_counter += 1\n                else:\n                    stagnation_counter = max(0, stagnation_counter - 1)\n\n            if stagnation_counter >= 5:\n                # reseed half of population to diversify\n                k = max(1, self.pop_size // 2)\n                for j in range(k):\n                    idx = rng.randint(0, self.pop_size)\n                    pop[idx] = lb + rng.rand(self.dim) * range_vec\n                    fvals[idx] = np.inf  # will be evaluated when candidate arises or replaced\n                # slightly enlarge trust radius to promote exploration\n                trust_radius = min(trust_max, trust_radius * 1.3)\n                stagnation_counter = 0\n                levy_prob = min(0.5, levy_prob * 1.5)\n                # if there is budget, evaluate a few newly reseeded individuals to populate scores\n                for j in range(min(k, self.budget - evals)):\n                    # find an inf fval and evaluate it\n                    inf_idxs = np.where(~np.isfinite(fvals))[0]\n                    if inf_idxs.size == 0 or evals >= self.budget:\n                        break\n                    idx = int(inf_idxs[0])\n                    fvals[idx] = float(func(pop[idx]))\n                    evals += 1\n                    if fvals[idx] < self.f_opt:\n                        self.f_opt = fvals[idx]\n                        self.x_opt = pop[idx].copy()\n\n            # conservative decay/growth of means to avoid too much drift\n            F_mean = np.clip(0.98 * F_mean + 0.02 * 0.5, 0.01, 1.5)\n            CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.5, 0.0, 1.0)\n\n            # safety: ensure trust_radius in bounds\n            trust_radius = float(np.clip(trust_radius, trust_min, trust_max))\n\n        # finalize outputs\n        self.f_opt = float(self.f_opt)\n        if self.x_opt is None:\n            # if no evaluated candidate produced a finite value (edge-case), pick best evaluated in population\n            finite_idxs = np.where(np.isfinite(fvals))[0]\n            if finite_idxs.size > 0:\n                idx = int(np.argmin(fvals[finite_idxs]))\n                idx = finite_idxs[idx]\n                self.x_opt = pop[idx].copy()\n                self.f_opt = float(fvals[idx])\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.261 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15764244626890267, 0.26443559869447675, 0.2825646702301071, 0.16760019025068407, 0.661660306723267, 0.27633728719264095, 0.23681120827682067, 0.17195783836231926, 0.22668548296752433, 0.160359795098574]}, "task_prompt": ""}
{"id": "b7e1d1a8-b79a-4e2a-87ae-76138811125f", "fitness": 0.2622759717813581, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy (Cauchy) long jumps, and a trust-region local search with online F/CR adaptation and stagnation-based resets.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (mutation factor F and crossover CR) are adapted online via simple\n    success-based moving averages (jDE-like). Stagnation triggers\n    partial re-seeding and increases chance of Lévy jumps.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # determine pop_size reasonably relative to budget and dimension\n        if pop_size is None:\n            default = 10 + 2 * self.dim\n            # limit pop size so it doesn't consume too much budget initially\n            self.pop_size = int(min(default, max(4, self.budget // 10)))\n        else:\n            self.pop_size = max(4, int(pop_size))\n            # cap to budget/2 to avoid too-large unevaluated populations\n            self.pop_size = int(min(self.pop_size, max(4, self.budget // 2)))\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # public results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = self.rng\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # bounds: prefer func.bounds if present (compat with some benchmarking wrappers),\n        # otherwise use [-5, 5] per problem statement.\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb))\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub))\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure shapes\n        lb = lb.flatten()\n        ub = ub.flatten()\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        range_vec = ub - lb\n        # small epsilon to avoid zero-range\n        range_vec = np.maximum(range_vec, 1e-12)\n\n        # Initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate initial population until budget runs out\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fv = float(func(pop[i]))\n            except Exception as e:\n                # if evaluation fails, set a large value and continue; don't consume budget further\n                fv = np.inf\n            fvals[i] = fv\n            evals += 1\n            if fv < self.f_opt:\n                self.f_opt = fv\n                self.x_opt = pop[i].copy()\n\n        # If no evaluation possible at all\n        if self.x_opt is None:\n            # return default\n            self.f_opt = np.inf\n            self.x_opt = lb + rng.random(self.dim) * range_vec\n            return self.f_opt, self.x_opt\n\n        # Adaptive DE hyper-parameters\n        F_mean = 0.6\n        CR_mean = 0.5\n        tau_F = 0.1   # probability to adjust F (jDE style)\n        tau_CR = 0.1  # probability to adjust CR\n\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        min_trust = 1e-6\n\n        stagnation_counter = 0\n        no_improve_evals = 0\n        last_best = self.f_opt\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # use Cauchy (standard) for heavy tails; clip extremes to avoid blow-up\n            step = rng.standard_cauchy(self.dim)\n            # limit extreme outliers based on problem range and dim\n            cap = 10.0\n            step = np.clip(step, -cap, cap)\n            return step\n\n        # main loop: perform generations until budget exhausted\n        # We'll iterate through population members sequentially as targets\n        generation = 0\n        # control parameters for occasional Levy jumps\n        base_levy_prob = 0.05\n\n        while evals < self.budget:\n            generation += 1\n            # per-generation small diffusion of F_mean/CR_mean can be done implicitly\n            improved_this_gen = False\n\n            # process each individual as target until budget exhausted\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi jDE-style\n                if rng.random() < tau_F:\n                    Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 0.95)\n                else:\n                    Fi = F_mean\n                if rng.random() < tau_CR:\n                    CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                else:\n                    CRi = CR_mean\n\n                # choose mutation style: DE or Levy-centered exploration\n                # increase Levy probability with stagnation\n                levy_prob = min(0.5, base_levy_prob + 0.01 * stagnation_counter)\n                use_levy = rng.random() < levy_prob\n\n                if use_levy:\n                    # center on best and add heavy-tailed jump scaled by trust radius and box range\n                    step = levy_step()\n                    # scale that step: a mix of trust radius and global range\n                    scale = 0.2 + 1.8 * rng.random()\n                    donor = self.x_opt + (step * scale) * (trust_radius / (np.linalg.norm(range_vec) + 1e-12)) * range_vec\n                else:\n                    # DE/rand/1 mutation (ensure distinct indices)\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    a, b, c = rng.choice(idxs, size=3, replace=False)\n                    donor = pop[a] + Fi * (pop[b] - pop[c])\n                    # scale DE mutation by trust-region factor (when trust small make steps smaller)\n                    trust_scale = np.clip(trust_radius / (max_trust * 0.5 + 1e-12), 0.1, 1.0)\n                    donor = self.x_opt * (1 - trust_scale) + donor * trust_scale\n\n                # binomial crossover to produce trial\n                trial = np.copy(pop[i])\n                crossover_mask = rng.random(self.dim) < CRi\n                if not np.any(crossover_mask):\n                    # ensure at least one dimension is taken from donor\n                    jrand = rng.integers(self.dim)\n                    crossover_mask[jrand] = True\n                trial[crossover_mask] = donor[crossover_mask]\n\n                # project to bounds\n                trial = np.clip(trial, lb, ub)\n\n                # evaluate trial if budget allows\n                if evals >= self.budget:\n                    break\n                try:\n                    f_trial = float(func(trial))\n                except Exception:\n                    f_trial = np.inf\n                evals += 1\n\n                # selection: greedy\n                if f_trial < fvals[i]:\n                    # accept\n                    pop[i] = trial\n                    fvals[i] = f_trial\n                    # nudge the means toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    improved_this_gen = True\n                else:\n                    # small decay of means to keep exploration\n                    F_mean = 0.999 * F_mean + 0.001 * Fi\n                    CR_mean = 0.999 * CR_mean + 0.001 * CRi\n\n                # update global best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                    stagnation_counter = 0\n                    no_improve_evals = 0\n                else:\n                    no_improve_evals += 1\n\n                # clear local Fi/CRi if needed (not strictly necessary in Python)\n                # (left as a conceptual placeholder)\n                # end of individual\n\n            # End of a generation: apply trust-region local search around best if budget remains\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # local sample count: small handful scaled with dimension and remaining budget\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            local_successes = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension by rand\n                sigma = (trust_radius / (max_trust + 1e-12)) * (0.5 + rng.random() * 0.5)\n                # use Gaussian perturbation in normalized space then rescale to box\n                candidate = self.x_opt + rng.normal(0.0, sigma, size=self.dim) * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    local_successes += 1\n                    stagnation_counter = 0\n                    no_improve_evals = 0\n                # optionally use local successes to replace worst individuals\n                # if candidate better than worst in population, replace\n                worst_idx = np.argmax(fvals)\n                if f_candidate < fvals[worst_idx]:\n                    pop[worst_idx] = candidate\n                    fvals[worst_idx] = f_candidate\n\n            # adjust trust radius based on local search outcomes\n            if local_successes > 0:\n                # successful local steps => shrink trust to focus\n                trust_radius = max(min_trust, trust_radius * (0.7 ** local_successes))\n                # encourage exploitation: pull means slightly toward exploitation-friendly values\n                F_mean = 0.95 * F_mean\n                CR_mean = np.clip(0.95 * CR_mean + 0.05, 0.0, 1.0)\n            else:\n                # if no local success, expand trust a bit to escape\n                trust_radius = np.clip(trust_radius * 1.08, min_trust, max_trust)\n                # small diversification of CR_mean\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n\n            # stagnation handling: if many evaluations without improvement, increase Levy probability and do partial re-seed\n            if no_improve_evals > max(50, self.dim * 5):\n                stagnation_counter += 1\n                # partial re-seed: reinitialize a fraction of population (but do not exceed budget)\n                reinit_frac = min(0.5, 0.25 + 0.05 * stagnation_counter)\n                k = max(1, int(self.pop_size * reinit_frac))\n                # pick indices to replace, prefer those with worst fitness\n                worst_idxs = np.argsort(-fvals)[:k]\n                for j in worst_idxs:\n                    if evals >= self.budget:\n                        break\n                    newx = lb + rng.random(self.dim) * range_vec\n                    try:\n                        newf = float(func(newx))\n                    except Exception:\n                        newf = np.inf\n                    evals += 1\n                    pop[j] = newx\n                    fvals[j] = newf\n                    if newf < self.f_opt:\n                        self.f_opt = newf\n                        self.x_opt = newx.copy()\n                        stagnation_counter = 0\n                        no_improve_evals = 0\n                # after re-seed, slightly enlarge trust to encourage broader moves\n                trust_radius = np.clip(trust_radius * (1.2 + 0.1 * stagnation_counter), min_trust, max_trust)\n                # nudge F_mean and CR_mean to diversify\n                F_mean = np.clip(0.5 * F_mean + 0.5 * rng.random(), 0.05, 0.95)\n                CR_mean = np.clip(0.5 * CR_mean + 0.5 * rng.random(), 0.0, 1.0)\n\n            # enforce some minimal and maximal trust radius\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n            # update stagnation counters wrt best improvement\n            if self.f_opt < last_best - 1e-12:\n                last_best = self.f_opt\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 0\n\n            # very small random perturbation of population to maintain diversity (occasional)\n            if rng.random() < 0.02 and evals < self.budget:\n                idx = rng.integers(self.pop_size)\n                newx = pop[idx] + rng.normal(0, 0.05, size=self.dim) * range_vec\n                newx = np.clip(newx, lb, ub)\n                try:\n                    newf = float(func(newx))\n                except Exception:\n                    newf = np.inf\n                evals += 1\n                if newf < fvals[idx]:\n                    pop[idx] = newx\n                    fvals[idx] = newf\n                    if newf < self.f_opt:\n                        self.f_opt = newf\n                        self.x_opt = newx.copy()\n\n            # loop until budget exhausted\n\n        # final results\n        # ensure x_opt not None\n        if self.x_opt is None:\n            # fallback\n            self.x_opt = lb + rng.random(self.dim) * range_vec\n            try:\n                self.f_opt = float(func(self.x_opt)) if evals < self.budget else np.inf\n            except Exception:\n                self.f_opt = np.inf\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.262 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11636707077849362, 0.1597371688887077, 0.3427937606314593, 0.41615950173741245, 0.22666345688269862, 0.3773110979008997, 0.26735157689877853, 0.3222633982681403, 0.2472109084820373, 0.1469017773449539]}, "task_prompt": ""}
{"id": "54cc011a-b358-479b-9784-7a05e3691d40", "fitness": 0.22406373272269967, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy flights and a shrinking/expanding trust-region local search; online adaptation of F/CR and population resets for robust global-to-local search.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    One-line idea: fast global mixing via DE + sporadic Lévy jumps and an\n    adaptive trust-region for focused local refinement with online F/CR adaptation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # Population size heuristic: scaled with dimension but limited by budget\n        if pop_size is None:\n            # ensure at least 4 individuals\n            p = max(4, 8 + 2 * self.dim)\n            # don't allocate more population than a reasonable fraction of budget\n            p = min(p, max(4, self.budget // 8))\n            self.pop_size = int(p)\n        else:\n            self.pop_size = int(pop_size)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # broadcast bounds to vector of size dim\n        arr = np.asarray(b, dtype=float)\n        if arr.ndim == 0:\n            arr = np.full(self.dim, float(arr))\n        elif arr.size == 1:\n            arr = np.full(self.dim, float(arr))\n        elif arr.size != self.dim:\n            # try to broadcast by repeating/truncating\n            arr = np.resize(arr, self.dim).astype(float)\n        return arr.copy()\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # safety: if bounds not finite, clamp to [-5,5] per task description\n        lb = np.where(np.isfinite(lb), lb, -5.0)\n        ub = np.where(np.isfinite(ub), ub, 5.0)\n        # ensure lb < ub\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        pop_size = max(4, min(self.pop_size, self.budget))  # at least 4\n        dim = self.dim\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = rng.uniform(lb, ub, size=(pop_size, dim))\n        fvals = np.full(pop_size, np.inf)\n\n        evals = 0\n        # evaluate initial population until budget exhausted\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = func(pop[i])\n            evals += 1\n            if fvals[i] < self.f_opt:\n                self.f_opt = float(fvals[i])\n                self.x_opt = pop[i].copy()\n\n        if evals == 0:\n            # no evaluations possible; return\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n\n        trust_radius = 0.2  # fraction of range_vec\n        trust_shrink = 0.9\n        trust_expand = 1.1\n\n        levy_base_prob = 0.06  # baseline probability for Levy jumps\n        stagnation_counter = 0\n        best_since_reset = 0\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # store recent successful Fi/CRi values for Lehmer-like adaptation (simple list)\n        success_Fs = []\n        success_CRs = []\n\n        # helper: generate Levy-like heavy-tailed step using truncated Cauchy\n        def levy_step(scale=0.4):\n            # Cauchy centered at 0 with scale; truncate to avoid infinities\n            # produces vector of length dim\n            step = rng.standard_cauchy(size=dim) * scale\n            # truncate extreme tails\n            thr = 10.0\n            step = np.clip(step, -thr, thr)\n            return step\n\n        gen = 0\n        # main loop: generations until budget exhausted\n        # We'll iterate over population sequentially\n        while evals < self.budget:\n            gen += 1\n            # per-generation randomness for adaptation\n            per_gen_levy_prob = levy_base_prob * (1.0 + 0.5 * (stagnation_counter / max(1, 20)))\n            per_gen_levy_prob = min(0.6, per_gen_levy_prob)\n\n            # shuffle indices to avoid bias\n            indices = rng.permutation(pop_size)\n\n            for idx in indices:\n                if evals >= self.budget:\n                    break\n                x_target = pop[idx]\n                f_target = fvals[idx]\n\n                # sample Fi and CRi like jDE but with slight gaussian noise around means\n                # Fi via Cauchy centered at F_mean\n                Fi = F_mean + 0.1 * rng.standard_cauchy()\n                Fi = float(np.clip(Fi, 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(loc=CR_mean, scale=0.1), 0.0, 1.0))\n\n                # choose mutation strategy: mostly rand/1, occasionally current-to-best or Levy-based\n                if rng.random() < per_gen_levy_prob:\n                    # Levy jump centered on best plus directional DE/rand/1 perturbation\n                    step = levy_step(scale=0.25)\n                    donor = best_x + step * range_vec  # long-range exploration\n                    # add some rand/1 perturbation scaled by Fi\n                    r1, r2, r3 = rng.choice(pop_size, size=3, replace=False)\n                    donor = donor + Fi * (pop[r1] - pop[r2])\n                else:\n                    # classic DE/rand/1\n                    r1, r2, r3 = rng.choice([i for i in range(pop_size) if i != idx], size=3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # occasionally mix in best guidance for faster convergence\n                    if rng.random() < 0.12:\n                        donor = 0.7 * donor + 0.3 * (best_x + Fi * (pop[r1] - pop[r2]))\n\n                # binomial crossover -> trial vector\n                cr_mask = rng.random(dim) < CRi\n                # ensure at least one dimension from donor\n                jrand = rng.integers(dim)\n                cr_mask[jrand] = True\n                trial = np.where(cr_mask, donor, x_target)\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate trial (counts as one eval)\n                f_trial = func(trial)\n                evals += 1\n\n                # selection greedy\n                if f_trial <= f_target:\n                    # accept\n                    pop[idx] = trial\n                    fvals[idx] = f_trial\n                    # register successes for adaptation\n                    success_Fs.append(Fi)\n                    success_CRs.append(CRi)\n                    # update means slightly toward successful params\n                    # small inertia to avoid sudden jumps\n                    F_mean = 0.92 * F_mean + 0.08 * Fi\n                    CR_mean = 0.92 * CR_mean + 0.08 * CRi\n                else:\n                    # slight drift away from unsuccessful Fi/CRi to encourage exploration\n                    F_mean = 0.995 * F_mean + 0.005 * Fi\n                    CR_mean = 0.995 * CR_mean + 0.005 * CRi\n\n                # update global best\n                if f_trial < best_f:\n                    best_f = float(f_trial)\n                    best_x = trial.copy()\n                    best_since_reset += 1\n                    stagnation_counter = 0\n                    # shrink trust region when making improvements\n                    trust_radius = max(0.02, trust_radius * trust_shrink)\n                else:\n                    stagnation_counter += 1\n                    # slowly expand trust radius if no improvement\n                    if stagnation_counter % (max(5, dim)) == 0:\n                        trust_radius = min(2.0, trust_radius * trust_expand)\n\n                # limit the stored success lists to recent history\n                if len(success_Fs) > 200:\n                    success_Fs = success_Fs[-200:]\n                if len(success_CRs) > 200:\n                    success_CRs = success_CRs[-200:]\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            # perform trust-region local search around best: sample a small handful\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # number of local samples depends on dim and remaining budget, keep it small\n            n_local = int(min(max(1, dim // 4), remaining, 8))\n            # anisotropic sigma per-dimension around trust radius\n            sigma = trust_radius * (0.5 + rng.random(dim) * 1.5)\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                candidate = best_x + rng.normal(0, 1.0, size=dim) * (sigma * range_vec)\n                # clamp\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_c = func(candidate)\n                evals += 1\n                if f_c < best_f:\n                    best_f = float(f_c)\n                    best_x = candidate.copy()\n                    # successful local step -> tighten trust region\n                    trust_radius = max(0.02, trust_radius * trust_shrink)\n                    # nudge global means slightly toward exploitation\n                    F_mean = 0.95 * F_mean + 0.05 * 0.4\n                    CR_mean = 0.95 * CR_mean + 0.05 * 0.9\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful local step -> encourage escape\n                    trust_radius = min(2.0, trust_radius * trust_expand * 0.99)\n\n            # integrate success lists for a small Lehmer-like mean (favor larger Fi)\n            if success_Fs:\n                sFs = np.array(success_Fs)\n                lehmer = (np.sum(sFs ** 2)) / (np.sum(sFs) + 1e-12)\n                F_mean = 0.97 * F_mean + 0.03 * float(np.clip(lehmer, 0.05, 1.0))\n            if success_CRs:\n                CR_mean = 0.97 * CR_mean + 0.03 * float(np.mean(success_CRs))\n\n            # stagnation handling: if too many evaluations without improvement, introduce larger jumps and partial re-seed\n            if stagnation_counter > max(30, self.budget // 50):\n                # re-seed half of the population randomly\n                n_reseed = max(1, pop_size // 2)\n                idxs = rng.choice(pop_size, size=n_reseed, replace=False)\n                for j in idxs:\n                    if evals >= self.budget:\n                        # do not evaluate new individuals beyond budget; simply replace positions and set inf\n                        pop[j] = rng.uniform(lb, ub)\n                        fvals[j] = np.inf\n                        continue\n                    # replace and evaluate immediately to use available budget\n                    newx = rng.uniform(lb, ub)\n                    newf = func(newx)\n                    evals += 1\n                    pop[j] = newx\n                    fvals[j] = newf\n                    if newf < best_f:\n                        best_f = float(newf)\n                        best_x = newx.copy()\n                        stagnation_counter = 0\n                # enlarge trust radius after reset for exploration\n                trust_radius = min(2.0, trust_radius * 1.6)\n                # slightly increase Levy frequency\n                levy_base_prob = min(0.6, levy_base_prob * 1.3)\n                best_since_reset = 0\n                # reduce stored successes to forget old stale info\n                success_Fs = []\n                success_CRs = []\n\n            # small safeguard: ensure best is reflected in population (elitism)\n            worst_idx = int(np.argmax(fvals))\n            if best_f < fvals[worst_idx]:\n                pop[worst_idx] = best_x.copy()\n                fvals[worst_idx] = best_f\n\n            # update class-level best trackers\n            if best_f < self.f_opt:\n                self.f_opt = float(best_f)\n                self.x_opt = best_x.copy()\n\n            # early stopping when near-zero improvement over long period - optional\n            # not strictly necessary; we keep running until budget or strong stagnation triggers reseed\n\n        # final outputs\n        # ensure x_opt is not None: if it is, pick best available\n        if self.x_opt is None:\n            idx = int(np.argmin(fvals))\n            if np.isfinite(fvals[idx]):\n                self.f_opt = float(fvals[idx])\n                self.x_opt = pop[idx].copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.224 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09834170668161246, 0.1491611678620962, 0.27091804234088124, 0.24307057245663966, 0.20665540632645718, 0.3328089312727537, 0.3131761300242515, 0.24579649346402355, 0.20687900809859128, 0.17382986869968953]}, "task_prompt": ""}
{"id": "57552505-6366-422d-a7cc-3d6299e64e01", "fitness": 0.38039233814600404, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer: differential evolution with online F/CR adaptation, occasional Lévy-flight jumps for long-range exploration, and an adaptive trust-region local search around the best point.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region Gaussian local search around the current best.\n    Simple online adaptation of F and CR (jDE-like), adaptive trust radius,\n    and partial re-seeding on stagnation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population size scaled with dimension but bounded by budget fraction\n        if pop_size is None:\n            target = max(6, 6 * self.dim)  # base scaling with dim\n            # don't create more individuals than a reasonable fraction of budget\n            max_by_budget = max(4, self.budget // 10)\n            self.pop_size = int(min(target, max_by_budget))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.RandomState(seed)\n        # State to return\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, lb, ub):\n        lb_a = np.asarray(lb, dtype=float)\n        ub_a = np.asarray(ub, dtype=float)\n        if lb_a.size == 1:\n            lb_a = np.full(self.dim, lb_a.item())\n        if ub_a.size == 1:\n            ub_a = np.full(self.dim, ub_a.item())\n        assert lb_a.shape == (self.dim,)\n        assert ub_a.shape == (self.dim,)\n        return lb_a, ub_a\n\n    def __call__(self, func):\n        lb, ub = self._ensure_array_bounds(func.bounds.lb, func.bounds.ub)\n        rng = self.rng\n\n        # Quick helpers\n        def clamp(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # budget counter\n        evals = 0\n        max_evals = self.budget\n\n        # initialize population\n        pop = rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.full(self.pop_size, np.inf)\n\n        # evaluate initial population until budget allows\n        for i in range(self.pop_size):\n            if evals >= max_evals:\n                # remaining individuals not evaluated (will be ignored)\n                break\n            xi = pop[i].copy()\n            fitness[i] = float(func(xi))\n            evals += 1\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = xi.copy()\n\n        # If too few evaluations allowed, we still should return best seen\n        if evals >= max_evals:\n            return self.f_opt, self.x_opt\n\n        # Algorithm hyper-parameters\n        F_mean = 0.6\n        CR_mean = 0.5\n        tau_F = 0.1  # jDE-like probabilities\n        tau_CR = 0.1\n\n        # Levy/Cauchy jump parameters\n        p_levy = 0.02  # base probability to use Levy jump for a trial\n        levy_scale = 0.8  # relative to domain range\n        levy_cap = 10.0  # cap per-component steps to avoid blow up\n\n        # Trust-region parameters\n        domain_range = ub - lb\n        trust_radius = 0.2  # fraction of domain_range\n        trust_min = 1e-6\n        trust_max = 2.0  # fraction\n        trust_shrink = 0.7\n        trust_expand = 1.15\n\n        # Adaptation memories\n        successful_Fs = []\n        successful_CR = []\n\n        # stagnation detection\n        best_since = 0\n        stagnation_limit = max(20, 5 * self.dim)\n\n        gen = 0\n        # Precompute indices helper\n        indices = np.arange(self.pop_size)\n\n        # Levy generator using standard Cauchy per-dimension, scaled and clipped\n        def levy_step(scale=1.0):\n            step = rng.standard_cauchy(size=self.dim) * scale\n            # clip extreme values to avoid numerical issues\n            step = np.clip(step, -levy_cap, levy_cap)\n            return step\n\n        # Main loop: continue generating trials until budget exhausted\n        while evals < max_evals:\n            gen += 1\n            # per-generation temporary success lists\n            gen_success_F = []\n            gen_success_CR = []\n            gen_success_count = 0\n\n            # iterate population sequentially; each evaluated candidate consumes one eval\n            for i in range(self.pop_size):\n                if evals >= max_evals:\n                    break\n\n                xi = pop[i]\n                fi = fitness[i]\n\n                # sample Fi and CRi (jDE-like adaptation)\n                if rng.rand() < tau_F:\n                    Fi = 0.1 + rng.rand() * 0.9\n                else:\n                    Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 0.95)\n                if rng.rand() < tau_CR:\n                    CRi = rng.rand()\n                else:\n                    CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                use_levy = (rng.rand() < p_levy)\n                if use_levy:\n                    # Levy jump centered at current best, scaled by domain and trust radius\n                    base = self.x_opt.copy() if self.x_opt is not None else xi.copy()\n                    step = levy_step(scale=levy_scale * trust_radius)\n                    donor = base + step * domain_range\n                    # add a small push from a difference vector to keep some DE flavour\n                    # pick two distinct others\n                    r = rng.choice(indices[indices != i], size=min(2, self.pop_size - 1), replace=False)\n                    if r.size >= 2:\n                        donor += 0.2 * Fi * (pop[r[0]] - pop[r[1]])\n                else:\n                    # DE/rand/1-like mutation with small pull towards best (current-to-best)\n                    # choose r1, r2, r3 distinct\n                    idxs = rng.choice(indices[indices != i], size=3, replace=False)\n                    r1, r2, r3 = idxs\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3]) + 0.2 * Fi * (self.x_opt - xi)\n\n                # binomial crossover\n                cr_mask = rng.rand(self.dim) < CRi\n                # ensure at least one element from donor\n                jrand = rng.randint(self.dim)\n                cr_mask[jrand] = True\n                trial = np.where(cr_mask, donor, xi)\n                trial = clamp(trial)\n\n                # Evaluate trial\n                f_trial = float(func(trial))\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_trial < fi:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    gen_success_count += 1\n                    gen_success_F.append(Fi)\n                    gen_success_CR.append(CRi)\n                    # update global best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        best_since = 0\n                    else:\n                        best_since += 1\n                else:\n                    # no improvement: count stagnation\n                    best_since += 1\n\n                # Break early if budget exhausted\n                if evals >= max_evals:\n                    break\n\n            # End of generation: adapt F_mean and CR_mean toward successful parameters\n            if gen_success_F:\n                sF = np.array(gen_success_F)\n                sCR = np.array(gen_success_CR)\n                mean_sF = sF.mean()\n                mean_sCR = sCR.mean()\n                # move means slightly toward successful values\n                F_mean = 0.9 * F_mean + 0.1 * mean_sF\n                CR_mean = 0.9 * CR_mean + 0.1 * mean_sCR\n                # encourage less Levy when we are finding improvements\n                p_levy = max(0.005, p_levy * 0.95)\n            else:\n                # no successes this generation: increase exploration chance a bit\n                p_levy = min(0.5, p_levy * 1.1)\n\n            # Trust-region local search around best\n            if evals < max_evals:\n                # number of local samples depends on remaining budget and dimension\n                remaining = max_evals - evals\n                local_budget = min(remaining, max(1, self.dim))  # sample at most dim candidates per generation\n                # anisotropic sigma: base trust_radius scaled per-dimension random factor\n                base_sigma = np.maximum(trust_min, trust_radius) * domain_range\n                local_success = 0\n                for k in range(local_budget):\n                    # anisotropic gaussian perturbation\n                    sigma_factors = rng.rand(self.dim) ** 2  # bias to small per-dim steps\n                    sigma = base_sigma * sigma_factors\n                    candidate = self.x_opt + rng.normal(loc=0.0, scale=sigma)\n                    candidate = clamp(candidate)\n                    f_cand = float(func(candidate))\n                    evals += 1\n                    if f_cand < self.f_opt:\n                        self.f_opt = f_cand\n                        self.x_opt = candidate.copy()\n                        local_success += 1\n                        best_since = 0\n                    else:\n                        best_since += 1\n                    if evals >= max_evals:\n                        break\n                # adapt trust radius according to local successes\n                if local_success > 0:\n                    # shrink to focus\n                    trust_radius = max(trust_min, trust_radius * (trust_shrink ** (local_success)))\n                    # also reduce levy probability a touch because local exploitation works\n                    p_levy = max(0.001, p_levy * 0.9)\n                else:\n                    # no improvement: expand to escape\n                    trust_radius = min(trust_max, trust_radius * trust_expand)\n                    p_levy = min(0.9, p_levy * 1.2)\n\n            # stagnation handling: if many evaluations without improvement, re-seed half of the population\n            if best_since >= stagnation_limit:\n                # reinitialize half of population randomly\n                num_reseed = max(1, self.pop_size // 2)\n                reseed_indices = rng.choice(self.pop_size, size=num_reseed, replace=False)\n                for idx in reseed_indices:\n                    if evals >= max_evals:\n                        break\n                    newx = rng.uniform(lb, ub)\n                    newf = float(func(newx))\n                    evals += 1\n                    pop[idx] = newx\n                    fitness[idx] = newf\n                    if newf < self.f_opt:\n                        self.f_opt = newf\n                        self.x_opt = newx.copy()\n                        best_since = 0\n                # after reseed, slightly enlarge trust radius to increase exploration\n                trust_radius = min(trust_max, max(1.2 * trust_radius, 0.05))\n                # reset adaptation memory a bit\n                F_mean = 0.6\n                CR_mean = 0.5\n                p_levy = min(0.5, p_levy * 1.5)\n                best_since = 0\n\n            # safety: if population fitness array has inf (because we couldn't evaluate some initial members),\n            # fill them with a copy of current best so they don't break selection\n            inf_mask = np.isinf(fitness)\n            if np.any(inf_mask):\n                for idx in np.where(inf_mask)[0]:\n                    pop[idx] = self.x_opt.copy() if self.x_opt is not None else rng.uniform(lb, ub)\n                    fitness[idx] = self.f_opt if self.x_opt is not None else float(func(pop[idx]))  # careful: ensure valid\n                    # account for evaluation if we had to call func:\n                    if evals < max_evals:\n                        # when we called func above, increment; otherwise rely on previously counted eval\n                        evals += 1\n                        if evals >= max_evals:\n                            break\n\n        # End main loop\n        # return best found\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.380 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10180321666871295, 0.1488064689001357, 0.32139982747038787, 0.8897488231597557, 0.2011102597682567, 0.25195834963499875, 0.5872456522253475, 0.48027275689584814, 0.6910042279082903, 0.13057379882830655]}, "task_prompt": ""}
{"id": "0494f890-bb0f-4435-9531-a99ad733a0d8", "fitness": 0.21100751902874082, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and an adaptive trust-region local search with online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Designed for continuous box-bounded optimization. The func passed in\n    must provide .bounds.lb and .bounds.ub (scalars or arrays).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension, but not exceeding fraction of budget\n        if pop_size is None:\n            pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n        pop_size = int(pop_size)\n        pop_size = min(pop_size, max(4, self.budget // 20))\n        self.pop_size = max(2, pop_size)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # fallback to [-5,5] if bounds are degenerate\n        if np.any(np.isnan(lb)) or np.any(np.isnan(ub)):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                fvals[i] = np.inf\n            evals += 1\n\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals)\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # cannot evaluate even once\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        min_trust = 1e-8\n        max_trust = range_norm * 2.0\n\n        p_levy = 0.05       # initial probability of a Lévy jump\n        stagnation_counter = 0\n        gen = 0\n        evals_at_last_improve = evals\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            # clip extreme outliers\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # Main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # For each population member produce a trial\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # decide whether to do Levy jump (global) or DE mutation (local/global mix)\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best (global exploration)\n                    step = levy_step()\n                    # scale relative to range_vec and trust_radius\n                    scale = step_scale * (0.5 + rng.rand() * 1.5) * (trust_radius / (range_norm + 1e-12))\n                    donor = best_x + scale * step * (range_vec / (np.maximum(range_vec.mean(), 1e-12)))\n                    # small random mix with current member for diversity\n                    alpha = 0.2 + 0.6 * rng.rand()\n                    donor = alpha * donor + (1.0 - alpha) * pop[i]\n                else:\n                    # DE/rand/1 mutation with jDE-style adaptive Fi and CRi per individual\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size >= 3:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    else:\n                        # fallback small population handling\n                        choices = rng.randint(0, self.pop_size, size=3)\n                        r1, r2, r3 = choices\n                    Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                    CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover: ensure at least one dimension from donor\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    donor = trial\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # move means slightly toward successful Fi/CRi when available\n                    if 'Fi' in locals():\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if 'CRi' in locals():\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    evals_at_last_improve = evals\n                else:\n                    stagnation_counter += 1\n\n                # clear locals Fi/CRi if set to avoid accidental reuse\n                if 'Fi' in locals():\n                    del Fi\n                if 'CRi' in locals():\n                    del CRi\n\n            # End of generation adjustments\n\n            # trust-region local search around best: sample a small number of candidates\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dimension derived from trust_radius and range_vec\n                sigma = (0.3 + 0.7 * rng.rand(self.dim)) * (trust_radius / (range_norm + 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * (range_vec)\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step -> shrink trust radius to focus search\n                    trust_radius *= 0.85\n                    trust_radius = max(trust_radius, min_trust)\n                    stagnation_counter = 0\n                    evals_at_last_improve = evals\n                else:\n                    # unsuccessful -> slightly expand to escape\n                    trust_radius *= 1.07\n                    trust_radius = min(max_trust, max(min_trust, trust_radius))\n                    stagnation_counter += 1\n\n            # adapt p_levy and parameter means based on success rate\n            if successes > 0:\n                # reduce chance of Lévy jumps slightly when successful\n                decay = 0.95 if successes > max(1, int(self.pop_size * 0.15)) else 0.98\n                p_levy = max(0.01, p_levy * decay)\n                # nudge means toward exploitation defaults\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation -> increase exploration\n                p_levy = min(0.5, p_levy * 1.08)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evals, re-seed part of population\n            if (evals - evals_at_last_improve) > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        evals_at_last_improve = evals\n                # enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # slightly raise levy probability to escape\n                p_levy = min(0.5, p_levy * 1.5)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.211 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1069518729463691, 0.17949604856733303, 0.26188839940601305, 0.27467711259638206, 0.2086458018961601, 0.2669370044121354, 0.23102072902699977, 0.23315157743224013, 0.20179790272275333, 0.14550874128102254]}, "task_prompt": ""}
{"id": "c5cd984b-f4f5-4e69-a597-2f225eb703d7", "fitness": 0.4213856095187477, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DE with occasional Lévy jumps and trust-region local search — combines jDE-style adaptive Differential Evolution, infrequent heavy-tailed Lévy jumps for global escapes, and a shrinking/enlarging trust-region Gaussian local search around the running best.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (jDE-style online parameter adaptation)\n      - Occasional heavy-tailed Lévy/Cauchy jumps centered on the best\n      - Trust-region Gaussian local search around the current best with adaptive radius\n    The implementation respects the evaluation budget strictly and handles scalar/array bounds.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # population size heuristic if not provided\n        if pop_size is None:\n            ps = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            ps = min(ps, max(4, self.budget // 20))\n            self.pop_size = int(ps)\n        else:\n            self.pop_size = max(4, int(pop_size))\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like and broadcast to dimension\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        # Try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds must be scalar or length == dim\")\n\n    def _levy_step(self, rng, dim):\n        # simple heavy-tailed step using Cauchy (stable alpha=1)\n        s = rng.standard_cauchy(size=dim)\n        # clamp extremely large outliers\n        s = np.clip(s, -1e3, 1e3)\n        return s\n\n    def __call__(self, func):\n        # get bounds from func if available, otherwise default [-5,5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        # small safety for zero-range dims\n        range_vec = np.where(range_vec == 0.0, 1.0, range_vec)\n\n        rng = self.rng\n        budget = self.budget\n        evals = 0\n\n        # initialize population uniformly\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        # jDE-like individual parameter storage\n        Fi = np.clip(rng.normal(0.6, 0.1, size=self.pop_size), 0.1, 1.0)\n        CRi = np.clip(rng.normal(0.9, 0.1, size=self.pop_size), 0.0, 1.0)\n\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # establish current best\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # no evaluations performed\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals[valid])\n        best_indices = np.where(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # overall hyperparams (means for jDE)\n        F_mean = float(np.median(Fi))\n        CR_mean = float(np.median(CRi))\n\n        # exploration parameters\n        p_levy = 0.08\n        step_scale = 0.25  # base for levy steps relative to range\n        trust_radius = 0.25 * np.linalg.norm(range_vec)  # scalar\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        min_trust = 1e-6\n\n        stagnation_counter = 0\n        successes_since_last = 0\n        gen = 0\n\n        # limit on local samples per generation\n        while evals < budget:\n            gen += 1\n            successes = 0\n            # for jDE: occasionally mutate Fi/CRi\n            tau1 = 0.1  # probability to change Fi\n            tau2 = 0.1  # probability to change CRi\n\n            # shuffle order of targets to avoid bias\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= budget:\n                    break\n\n                target = pop[idx].copy()\n                target_f = fvals[idx]\n\n                # jDE parameter self-adaptation: possible change for this individual\n                if rng.rand() < tau1:\n                    Fi[idx] = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                if rng.rand() < tau2:\n                    CRi[idx] = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # decide to do Levy jump or classical DE\n                if rng.rand() < p_levy:\n                    # Levy jump centered around current best with some noise\n                    step = self._levy_step(rng, self.dim)\n                    scale = step_scale * (0.5 + rng.rand() * 1.5) * (np.linalg.norm(range_vec) / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                    donor = best_x + scale * step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    # create trial by mixing donor and target with CRi\n                    cr_mask = rng.rand(self.dim) < CRi[idx]\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, target)\n                else:\n                    # classical DE/rand/1/bin using three distinct indices\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != idx]\n                    r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    donor = pop[r1] + Fi[idx] * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi[idx]\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, target)\n\n                # project trial to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate (respect budget)\n                if evals >= budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection\n                if f_candidate < target_f:\n                    # accept\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    successes_since_last += 1\n                    # encourage Fi/CRi adaptation towards successful values (simple)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi[idx]\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi[idx]\n                else:\n                    # slight penalty: increase Fi a bit to encourage larger steps\n                    Fi[idx] = np.clip(Fi[idx] * 1.01, 0.05, 1.0)\n\n                # global best update\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation: trust-region local search around best if budget remains\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n\n            # adapt number of local samples modestly with dimension and remaining budget\n            local_samples = min(5 + max(0, self.dim // 10), remaining, 8)\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= budget:\n                    break\n                # anisotropic Gaussian around best, scaled by trust_radius and per-dim range\n                sigma = (0.3 + rng.rand(self.dim) * 0.7) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                perturb = rng.normal(0.0, 1.0, size=self.dim) * sigma * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                candidate = np.clip(best_x + perturb, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # shrink trust region (focus)\n                    trust_radius = max(min_trust, trust_radius * 0.8)\n                    stagnation_counter = 0\n                else:\n                    # expand slightly to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # adaptation of levy prob depending on success\n            if successes + local_success > 0:\n                # successful generation: reduce chance of large jumps\n                p_levy = max(0.01, p_levy * (0.95 if successes + local_success > self.pop_size * 0.15 else 0.98))\n                # slightly move means toward observed median parameters\n                F_mean = np.clip(0.98 * F_mean + 0.02 * np.median(Fi), 0.05, 0.99)\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * np.median(CRi), 0.0, 1.0)\n            else:\n                # stagnation: increase chance of long jumps and diversity\n                p_levy = min(0.5, p_levy * 1.07 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n\n            # strong stagnation reset if no improvement for many steps\n            if stagnation_counter > max(50, self.dim * 6):\n                k = max(1, self.pop_size // 2)\n                # re-seed k worst individuals (or random ones) without exceeding budget for evaluations\n                worst_idx = np.argsort(fvals)[-k:]\n                for j in worst_idx:\n                    if evals >= budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    # evaluate re-seeded individual if budget allows\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # gently increase diversity in Fi/CRi\n                Fi = np.clip(Fi * (1.0 + 0.1 * rng.rand(self.pop_size)), 0.05, 1.0)\n                CRi = np.clip(CRi + 0.05 * (rng.rand(self.pop_size) - 0.5), 0.0, 1.0)\n                stagnation_counter = 0\n\n            # safeguard: ensure trust radius bounds\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n            # early termination if best is very good (some tiny threshold)\n            if best_f == 0.0:\n                break\n\n        # final bookkeeping\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.421 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1370137377968842, 0.20194123939806963, 0.6978617994602465, 0.9552241336150381, 0.29542935694379646, 0.8822766929401623, 0.2738183792652201, 0.3255072724714191, 0.2741779626373616, 0.17060552065927914]}, "task_prompt": ""}
{"id": "a6629bfb-0fe5-4f42-852c-3807388c962c", "fitness": 0.5168844999381838, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy (Cauchy) long jumps, and a trust-region Gaussian local search with online adaptation of DE parameters and targeted restarts.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Adaptive hybrid optimizer:\n      - Global search: Differential Evolution (rand/1/bin) with per-individual Fi/CRi (jDE-like adaptation).\n      - Occasional long-range exploration: heavy-tailed jumps using a clipped Cauchy sample (Levy-like).\n      - Local exploitation: trust-region Gaussian sampling around the current best, with adaptive trust radius.\n      - Online adaptation: F_mean, CR_mean, p_levy adjusted based on recent successes/stagnation.\n      - Targeted partial restarts when stagnation detected.\n\n    Intended for continuous bounded optimization problems (e.g., BBOB). The budget (number of\n    function evaluations) is strictly enforced.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # sensible default population size: scale with dimension but keep within budget\n        if pop_size is None:\n            default = max(4, int(10 * self.dim))\n            # ensure at least one generation possible: keep pop_size not too large relative to budget\n            self.pop_size = min(default, max(4, self.budget // 10))\n        else:\n            self.pop_size = int(pop_size)\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # convert bounds to arrays of length dim\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        # if b is length dim already, cast to float\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting if possible\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            # fallback to symmetric [-5,5]\n            return np.full(self.dim, float(b.flat[0]) if b.size > 0 else 5.0)\n\n    def __call__(self, func):\n        # obtain bounds from func if available, else default to [-5, 5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 1.0\n\n        # initialize population\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population sequentially until budget exhausted\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # if no evaluations possible, return defaults\n        if not np.isfinite(fvals).any():\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # find current best among evaluated individuals\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        # hyper-parameters and adaptation state\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.08\n        trust_radius = 0.2 * range_norm\n        min_trust = 1e-6\n        max_trust = 2.0 * range_norm\n\n        # bookkeeping\n        gen = 0\n        stagnation_counter = 0\n        success_window = 0  # successes in last generation\n        global_successes = 0\n\n        # utility: Levy-like heavy-tailed step using clipped Cauchy (fast and simple)\n        def levy_step_vector():\n            # generate Cauchy noise per-dimension and clip extremes\n            s = rng.standard_cauchy(size=self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # normalize and scale to range scale to produce meaningful jumps\n            norm = np.linalg.norm(s)\n            if norm == 0 or not np.isfinite(norm):\n                s = rng.randn(self.dim)\n                norm = np.linalg.norm(s)\n            s = s / (norm + 1e-12)\n            # scale relative to range_vec and trust radius\n            scale = 0.8 * trust_radius / max(range_norm, 1e-12)\n            return s * scale * range_vec\n\n        # main loop until budget exhausted\n        # We'll do generation-wise DE followed by trust-region sampling\n        while evals < self.budget:\n            gen += 1\n            success_window = 0\n\n            # per-generation adaptation noise for Fi/CRi sampling variance\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # build donor/trial\n                if rng.rand() < p_levy:\n                    # long jump around current best\n                    step = levy_step_vector()\n                    # optionally add small random factor to diversify\n                    factor = 0.4 + 0.6 * rng.rand()\n                    donor = best_x + factor * step\n                else:\n                    # DE/rand/1/bin variant\n                    idxs = np.arange(self.pop_size)\n                    # ensure we have at least 3 other indices\n                    if self.pop_size <= 3:\n                        r1 = rng.randint(0, self.pop_size)\n                        r2 = rng.randint(0, self.pop_size)\n                        r3 = rng.randint(0, self.pop_size)\n                    else:\n                        choices = np.delete(idxs, i)\n                        r1, r2, r3 = rng.choice(choices, 3, replace=False)\n\n                    # sample Fi, CRi around the running means (jDE-like)\n                    Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover (ensure at least one component from donor)\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    donor = trial\n\n                # projection to bounds (simple clipping)\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy on population slot i\n                replaced = False\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    replaced = True\n                    success_window += 1\n                    global_successes += 1\n                    # nudge parameter means toward successful Fi/CRi if they exist (DE branch)\n                    if 'Fi' in locals():\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if 'CRi' in locals():\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # small reduction of p_levy when exploitation working\n                    p_levy = max(0.01, p_levy * 0.98)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # clear local Fi/CRi to avoid accidental reuse in Levy branch\n                if 'Fi' in locals():\n                    del Fi\n                if 'CRi' in locals():\n                    del CRi\n\n            # End of generation: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # decide number of local samples (small, depends on dim and remaining budget)\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 15)))\n            improved_local = False\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: per-dim random scaling of trust radius relative to problem range\n                sigma = (0.3 + 0.7 * rng.rand(self.dim)) * (trust_radius / max(range_norm, 1e-12))\n                noise = rng.randn(self.dim) * sigma * (range_vec / max(np.mean(range_vec), 1e-12))\n                candidate = np.clip(best_x + noise, lb, ub)\n\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    improved_local = True\n                    # successful local step => shrink trust radius\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    # also slightly nudge DE parameters toward exploitation\n                    F_mean = np.clip(0.95 * F_mean + 0.05 * 0.5, 0.05, 0.99)\n                    CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 0.99)\n                else:\n                    # unsuccessful => expand trust radius a bit to escape\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # generation-level adaptation of p_levy based on success rate\n            succ_rate = success_window / max(1, self.pop_size)\n            if succ_rate > 0.25:\n                # doing well: reduce long jumps\n                p_levy = max(0.005, p_levy * 0.9)\n            else:\n                # stagnating: increase probability of long jumps gradually\n                p_levy = min(0.5, p_levy * (1.02 + (0.5 - succ_rate)))\n\n            # stagnation handling: targeted partial re-seeding if no improvement for long\n            stagnation_threshold = max(20, self.pop_size * 4)\n            if stagnation_counter > stagnation_threshold:\n                # reinitialize half of the population randomly (but avoid throwing away best)\n                k = max(1, self.pop_size // 2)\n                indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    # do not overwrite the best if it's in population; but we can replace others\n                    new_x = lb + rng.rand(self.dim) * range_vec\n                    new_f = float(func(new_x))\n                    evals += 1\n                    pop[j] = new_x\n                    fvals[j] = new_f\n                    if new_f < best_f:\n                        best_f = new_f\n                        best_x = new_x.copy()\n                # enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # slightly diversify parameters\n                F_mean = np.clip(F_mean * 0.95 + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.95 + 0.05 * 0.9, 0.0, 0.99)\n                p_levy = min(0.5, p_levy * 1.2)\n                stagnation_counter = 0\n\n            # ensure trust radius stays within bounds\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n            # keep track of best in overall record for return\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n            else:\n                # if not improved globally, still store current best found\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n        # final results\n        # Guarantee x_opt is set\n        if self.x_opt is None:\n            # fallback: take best in current population if any finite\n            if np.isfinite(fvals).any():\n                idx = np.nanargmin(fvals)\n                self.x_opt = pop[idx].copy()\n                self.f_opt = float(fvals[idx])\n            else:\n                self.x_opt = None\n                self.f_opt = np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.517 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1573870161985833, 0.2796382348152775, 0.3469625668339752, 0.8859035819858287, 0.7577244162621933, 0.8808448665465362, 0.4407515957847996, 0.5452712886156884, 0.4202553792884498, 0.4541060530505061]}, "task_prompt": ""}
{"id": "58db1e21-b9ac-420a-b415-a8a15eecd12a", "fitness": 0.39031883825811614, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like self-adaptation, occasional Lévy (Cauchy) long jumps for exploration, and an online trust-region local search around the best found (fast global exploration with focused local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Adaptive hybrid: Differential Evolution + occasional Lévy jumps + trust-region local search\n    with online step-size adaptation. Designed for continuous bounded optimization.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size relative to dim and budget\n        if pop_size is None:\n            # at least 4, at most 10*dim, but not exceeding a fraction of budget\n            self.pop_size = max(4, min(10 * self.dim, max(4, self.budget // 10)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed if seed is not None else np.random.randint(1, 2**31 - 1)\n        self.rng = np.random.RandomState(self.seed)\n\n        # results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == self.dim:\n            return b.copy()\n        # broadcast scalar to vector\n        return np.full(self.dim, float(b))\n\n    def __call__(self, func):\n        # bounds from func (Many BBOB gives scalar or per-dim arrays)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # fallback if bounds are None: use [-5,5]\n        if lb is None or ub is None:\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n\n        range_vec = ub - lb\n        range_norm = np.maximum(np.linalg.norm(range_vec), 1e-12)\n\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate as many as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # determine best among evaluated\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals)\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # budget was zero\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6    # mean mutation factor for DE\n        CR_mean = 0.3   # mean crossover probability\n        p_levy = 0.05   # chance to perform a Lévy jump instead of DE mutation\n        trust_radius = 0.2 * range_norm  # initial trust radius (absolute scale)\n        min_trust = 1e-8\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using standard Cauchy\n        def levy_step():\n            # Cauchy (standard) has heavy tails; limit extremes to avoid blow-ups\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # normalize to unit scale (L2) then return\n            s_norm = np.linalg.norm(s)\n            if s_norm < 1e-12:\n                return s\n            return s / s_norm\n\n        # main loop: iterate until budget exhausted\n        # We'll sweep over population repeatedly (generational-like) but accounting strict eval budget\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation adaptation accumulators\n            F_succ_sum = 0.0\n            CR_succ_sum = 0.0\n            n_succ = 0\n\n            # shuffle order to avoid bias\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # if individual hasn't been evaluated (inf), we evaluate it first\n                if not np.isfinite(fvals[idx]):\n                    pop[idx] = lb + rng.rand(self.dim) * range_vec\n                    fvals[idx] = float(func(pop[idx]))\n                    evals += 1\n                    if fvals[idx] < best_f:\n                        best_f = fvals[idx]\n                        best_x = pop[idx].copy()\n                        stagnation_counter = 0\n                    continue\n\n                # jDE-like sampling for this individual\n                Fi = rng.normal(F_mean, 0.15)\n                Fi = float(np.clip(Fi, 0.05, 0.99))\n                CRi = rng.normal(CR_mean, 0.2)\n                CRi = float(np.clip(CRi, 0.0, 1.0))\n\n                # pick three distinct indices different from idx\n                all_idx = list(range(self.pop_size))\n                all_idx.remove(idx)\n                if len(all_idx) < 3:\n                    # fallback: generate random vector from bounds\n                    donor = lb + rng.rand(self.dim) * range_vec\n                else:\n                    r1, r2, r3 = rng.choice(all_idx, 3, replace=False)\n\n                    if rng.rand() < p_levy:\n                        # Lévy jump centered at best_x for long-range exploration\n                        step = levy_step()\n                        # scale step by trust radius (relative to range_norm)\n                        scale = (0.5 + rng.rand() * 2.0) * (trust_radius / max(range_norm, 1e-12))\n                        donor = best_x + step * scale * range_vec\n                    else:\n                        # classic DE/rand/1 mutation\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover to create trial\n                cr_mask = rng.rand(self.dim) < CRi\n                # ensure at least one dimension comes from donor\n                if not np.any(cr_mask):\n                    cr_mask[rng.randint(0, self.dim)] = True\n\n                trial = np.where(cr_mask, donor, pop[idx])\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[idx]:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    n_succ += 1\n                    F_succ_sum += Fi\n                    CR_succ_sum += CRi\n                    # on success slightly reduce trust radius to focus locally\n                    trust_radius = max(min_trust, trust_radius * 0.95)\n                else:\n                    # on failure slightly increase trust radius to encourage exploration\n                    trust_radius = min(max_trust, trust_radius * 1.02)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of sweep: adapt global means based on successes\n            if n_succ > 0:\n                new_F_mean = F_succ_sum / n_succ\n                new_CR_mean = CR_succ_sum / n_succ\n                # blend slowly to avoid oscillation\n                F_mean = 0.9 * F_mean + 0.1 * new_F_mean\n                CR_mean = 0.9 * CR_mean + 0.1 * new_CR_mean\n                # if many successes reduce chance of Lévy jumps (we're exploiting)\n                if successes > max(1, 0.15 * self.pop_size):\n                    p_levy = max(0.005, p_levy * 0.9)\n            else:\n                # no successes: encourage more exploration\n                p_levy = min(0.5, p_levy * 1.15)\n                F_mean = min(0.99, F_mean * 1.02)\n                CR_mean = max(0.05, CR_mean * 0.98)\n\n            # trust-region local search around current best: small Gaussian perturbations\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: random per-dimension scaled by trust_radius and normalized by range_norm\n                sigma = (0.3 + rng.rand(self.dim) * 0.7) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus more\n                    trust_radius = max(min_trust, trust_radius * 0.80)\n                    stagnation_counter = 0\n                    # nudge DE means slightly toward exploitation\n                    F_mean = 0.95 * F_mean + 0.05 * 0.5\n                    CR_mean = 0.95 * CR_mean + 0.05 * 0.6\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n                    stagnation_counter += 1\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                reinit_indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reinit_indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius slightly to try different basins\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n                # increase levy chance to jump to distant basins\n                p_levy = min(0.5, p_levy * 1.5)\n\n            # safety clamps\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n            F_mean = float(np.clip(F_mean, 0.05, 0.99))\n            CR_mean = float(np.clip(CR_mean, 0.0, 0.99))\n            p_levy = float(np.clip(p_levy, 0.0, 0.9))\n\n        # final assignments\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.390 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14548607117106427, 0.22682830449435798, 0.4225212792922346, 0.6266456613080799, 0.45949650562212396, 0.4788141486866776, 0.2926183492219655, 0.5182908234343011, 0.434141296059267, 0.2983459432910901]}, "task_prompt": ""}
{"id": "e66a1334-bf3e-45f4-9b93-128fadbe78a1", "fitness": 0.2673445113103443, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and a trust-region local search with online step-size adaptation (fast global exploration, focused local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (DE) with jDE-like online adaptation of F and CR,\n      - Occasional Lévy (Cauchy) long jumps centered on the best for global escapes,\n      - Periodic trust-region local Gaussian search around the current best with dynamic radius.\n    The algorithm carefully counts function evaluations and never exceeds self.budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n\n        # sensible default population size scaled with dim, but clipped\n        if pop_size is None:\n            self.pop_size = int(np.clip(8 + 2 * np.sqrt(self.dim), 8, 80))\n        else:\n            self.pop_size = int(pop_size)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like, return 1D array of length self.dim\n        arr = np.asarray(b, dtype=float)\n        if arr.ndim == 0:\n            return np.full(self.dim, float(arr))\n        if arr.size == 1:\n            return np.full(self.dim, float(arr.item()))\n        if arr.size == self.dim:\n            return arr.copy().astype(float)\n        # try to broadcast\n        try:\n            return np.broadcast_to(arr, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be interpreted for dimension {}\".format(self.dim))\n\n    def __call__(self, func):\n        # Prepare bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # if some zero ranges, avoid zero division later\n        range_vec = np.maximum(range_vec, 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = self.rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate initial population but respect budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # If no evaluations possible:\n        if not np.isfinite(fvals).any():\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # best so far\n        best_idx = int(np.nanargmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # Algorithmic hyper-parameters (adapted online)\n        F_mean = 0.6        # mean differential weight\n        CR_mean = 0.9       # mean crossover probability\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius (fraction of range)\n        trust_radius = step_scale * np.linalg.norm(range_vec)  # absolute trust radius\n        min_trust = 1e-6\n        max_trust = 2.0 * np.linalg.norm(range_vec) + 1e-12\n\n        stagnation_counter = 0\n        success_history = []  # recent success counts\n        gen = 0\n\n        # Lévy-like heavy-tailed step using Cauchy\n        def levy_step():\n            # Cauchy heavy tails; clip extreme values to avoid numerical blow-up\n            s = self.rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # normalize scale to typical magnitude ~1 by dividing by median abs\n            med = np.median(np.abs(s)) + 1e-12\n            return s / med\n\n        # Helper to ensure at least one index for crossover\n        def ensure_one_true(mask):\n            if not mask.any():\n                mask[self.rng.integers(0, self.dim)] = True\n            return mask\n\n        # Main loop\n        while evals < self.budget:\n            gen += 1\n            gen_successes = 0\n\n            # adapt small jitter to F_mean and CR_mean per generation\n            # jDE-like: individuals will sample Fi and CRi\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Sample Fi and CRi per individual around means with some noise\n                Fi = F_mean + 0.1 * self.rng.normal()\n                Fi = float(np.clip(Fi, 0.05, 0.99))\n                CRi = CR_mean + 0.1 * self.rng.normal()\n                CRi = float(np.clip(CRi, 0.0, 1.0))\n\n                # Decide whether to perform Lévy jump\n                if self.rng.random() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    s = levy_step()\n                    # scale relative to coordinate-wise range and trust radius\n                    scale = (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                    donor = best_x + (scale * range_vec) * s\n                else:\n                    # Standard DE/rand/1 with current-to-best bias occasionally\n                    # pick three distinct indices different from i\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r = self.rng.choice(idxs, size=3, replace=False)\n                    r1, r2, r3 = r\n                    # mixture: with some probability bias toward best (current-to-best)\n                    if self.rng.random() < 0.4:\n                        # current-to-best: move current toward best plus difference\n                        donor = pop[i] + Fi * (best_x - pop[i]) + Fi * (pop[r1] - pop[r2])\n                    else:\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # Binomial crossover\n                cr_mask = self.rng.random(self.dim) < CRi\n                cr_mask = ensure_one_true(cr_mask)\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # Project to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # Evaluate candidate if budget remains\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(trial))\n                evals += 1\n\n                # Selection\n                if f_candidate < fvals[i]:\n                    pop[i] = trial\n                    fvals[i] = f_candidate\n                    gen_successes += 1\n                    # Update population-level F_mean/CR_mean toward successful params (small step)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # Update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = trial.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # quick stagnation break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n\n            # Periodic trust-region local search around best: small Gaussian samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: at most dim+2 but not to exceed remaining\n            local_samples = int(min(max(2, self.dim // 2 + 2), remaining, 8))\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per dimension: trust_radius scaled by relative range\n                per_dim_sigma = (trust_radius / (np.linalg.norm(range_vec) + 1e-12)) * (range_vec * (0.5 + self.rng.random(self.dim)))\n                # sample gaussian step, scaled to maintain reasonable magnitudes\n                step = self.rng.normal(scale=per_dim_sigma)\n                candidate = best_x + step\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # shrink trust region (focus)\n                    trust_radius *= 0.7\n                    trust_radius = max(trust_radius, min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful local try => slightly enlarge to attempt escape\n                    trust_radius *= 1.05\n                    trust_radius = min(trust_radius, max_trust)\n\n            # update global counters and adaptive probabilities\n            success_history.append(gen_successes + local_success)\n            if len(success_history) > 20:\n                success_history.pop(0)\n\n            # Adapt p_levy based on recent successes: fewer successes -> more jumps\n            recent_success = sum(success_history[-5:]) if success_history else 0\n            if recent_success > max(1, 0.1 * self.pop_size):\n                # good progress -> reduce long jumps slowly\n                p_levy = max(0.01, p_levy * 0.97)\n                # slightly stabilize F_mean toward exploitation-friendly value\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.5, 0.05, 0.99)\n            else:\n                # stagnation -> increase exploration\n                p_levy = min(0.5, p_levy * 1.05 + 0.005)\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n\n            # Strong stagnation reset: if no improvement for many evals, re-seed half pop\n            if stagnation_counter > max(200, 10 * self.dim):\n                k = max(1, self.pop_size // 2)\n                # choose indices to reinitialize (avoid the current best index if possible)\n                candidates = np.arange(self.pop_size)\n                # favor replacing worst individuals\n                worst_idx = np.argsort(fvals)[-k:]\n                for j in worst_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = self.rng.uniform(lb, ub)\n                    # evaluate newly seeded individual if budget allows\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                # enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # nudge means toward exploration\n                p_levy = min(0.5, p_levy * 1.2)\n                F_mean = np.clip(F_mean * 1.05, 0.05, 0.99)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.267 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.22711085012741894, 0.20089141083253836, 0.2871629704815287, 0.1789198847263661, 0.2855310720771582, 0.33195200687565796, 0.32751560205207064, 0.3749310996022366, 0.27726956083181575, 0.18216065549665172]}, "task_prompt": ""}
{"id": "a2c1ee7c-6bc3-4f78-9d94-ff77ad8746c6", "fitness": 0.6137484472502047, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: a Differential Evolution core with jDE-style online parameter adaptation, occasional heavy-tailed Lévy/Cauchy jumps for long-range escape, and a trust-region local search around the current best (fast global exploration + focused local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy/Cauchy jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=123)\n        fbest, xbest = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population size heuristic: between 4*dim and something reasonable wrt budget\n        if pop_size is None:\n            self.pop_size = max(6, min(30 + 2 * self.dim, max(6, self.budget // 10)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        # Get bounds from func if possible, otherwise default to [-5, 5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure shapes\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        # allocate population\n        pop = rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # initial evaluation (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # initial best\n        valid = ~np.isinf(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals[valid])\n            best_indices = np.where(valid)[0]\n            best_idx = best_indices[best_idx]\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # if no evaluations possible, return defaults\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # hyperparameters and adaptation state\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.08\n        step_scale = 0.25\n        trust_radius = np.linalg.norm(range_vec) * 0.15  # initial trust radius\n        trust_radius = max(trust_radius, 1e-6)\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: Lévy-like heavy tail using Cauchy (simple approximation)\n        def levy_step():\n            # produce Cauchy-like vector, clipped to avoid numeric blow-up\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: generations\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # permute indices for fairness\n            indices = rng.permutation(self.pop_size)\n            for i in indices:\n                if evals >= self.budget:\n                    break\n\n                # decide branch: Levy jump around best OR DE operator\n                if rng.random() < p_levy:\n                    # Lévy exploration centered on best_x\n                    step = levy_step()  # heavy-tailed\n                    # scale is dynamic: trust radius and global range\n                    scale = step_scale * (trust_radius / (1.0 + np.linalg.norm(step)))  # normalize by step size\n                    donor = best_x + scale * step * (0.5 + rng.random(self.dim))\n                    # keep donor within a ball around best as well\n                else:\n                    # DE/rand/1 mutation with jDE-like Fi, CRi sampling\n                    Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.2)\n                    CRi = np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0)\n                    # pick three distinct indices different from i\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r = rng.choice(idxs, size=3, replace=False)\n                    r1, r2, r3 = r\n                    mutant = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cross = rng.random(self.dim) < CRi\n                    # ensure at least one dimension from mutant\n                    jrand = rng.integers(self.dim)\n                    cross[jrand] = True\n                    donor = np.where(cross, mutant, pop[i])\n\n                # projection to bounds\n                donor = np.clip(donor, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(donor))\n                evals += 1\n\n                # selection (greedy)\n                if f_candidate < fvals[i]:\n                    pop[i] = donor.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt F_mean/CR_mean if DE branch used\n                    # try to detect local Fi/CRi variables\n                    if 'Fi' in locals():\n                        # move means slightly towards the successful parameters\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = np.clip(0.95 * CR_mean + 0.05 * (CRi if 'CRi' in locals() else CR_mean), 0.0, 1.0)\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = donor.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # clear local Fi/CRi to avoid unintended reuse\n                if 'Fi' in locals():\n                    del Fi\n                if 'CRi' in locals():\n                    del CRi\n\n            # End of generation: trust-region local search (focused exploitation)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # decide number of local samples: small handful scaled with dim and remaining budget\n            # keep local effort modest relative to budget\n            local_samples = min(max(1, self.dim // 2), remaining, max(1, self.pop_size // 4))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by random per-dimension factor\n                per_dim_scale = 0.4 + rng.random(self.dim) * 0.8  # between 0.4 and 1.2\n                sigma = (trust_radius / max(1.0, np.linalg.norm(range_vec))) * per_dim_scale * range_vec\n                # For numerical stability, if range_vec very small, scale by trust_radius/ sqrt(dim)\n                if np.all(range_vec == 0):\n                    sigma = np.full(self.dim, trust_radius / np.sqrt(self.dim))\n                candidate = best_x + rng.normal(scale=np.abs(sigma))\n                candidate = np.clip(candidate, lb, ub)\n\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    # successful local step: adopt and shrink trust radius for finer search\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    trust_radius = max(1e-8, trust_radius * 0.7)\n                    stagnation_counter = 0\n                    successes += 1\n                else:\n                    # unsuccessful => expand a bit to try escape\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # Adapt exploration probability and parameter means based on success/stagnation\n            if successes > max(1, self.pop_size // 10):\n                # many successes -> more exploitation; reduce levy probability modestly\n                p_levy = max(0.01, p_levy * 0.96)\n                # move means slightly toward exploitation-favoring defaults\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 1.2)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                # small increase in F to encourage larger steps if many successes\n                F_mean = np.clip(F_mean * 1.01, 0.05, 1.2)\n            else:\n                # few successes -> increase chance of long jumps slowly\n                p_levy = min(0.5, p_levy * (1.0 + 0.002 * (1 + stagnation_counter / (1 + self.pop_size))))\n\n            # stagnation handling: if no improvement for a long time, re-seed part of population\n            if stagnation_counter > max(50, self.pop_size * 8):\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                replace_idx = rng.choice(np.arange(self.pop_size), size=k, replace=False)\n                for j in replace_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = rng.uniform(lb, ub)\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # gently increase exploration probability\n                p_levy = min(0.5, p_levy * 1.5)\n\n            # sanity clamps\n            trust_radius = float(np.clip(trust_radius, 1e-8, max_trust))\n            p_levy = float(np.clip(p_levy, 0.0, 0.8))\n            F_mean = float(np.clip(F_mean, 0.05, 1.2))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n            # optional small random injection to avoid premature convergence\n            if rng.random() < 0.02:\n                j = rng.integers(self.pop_size)\n                if evals < self.budget:\n                    pop[j] = rng.uniform(lb, ub)\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n\n        # finalization\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.614 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17173330911587803, 0.3161680327095032, 0.5777649727783125, 0.9442992722802058, 0.8839353182610369, 0.9157169173988842, 0.3130049244948271, 0.421539083349182, 0.895648719579515, 0.6976739225347028]}, "task_prompt": ""}
{"id": "1ecfc3f2-76f8-47b3-9164-bad6650668db", "fitness": 0.5756654196124048, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy-flight jumps and an online trust-region local search; adapts DE parameters and trust radius from success history for robust global-to-local search.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=42)\n        f_best, x_best = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # Default population size: scaled with dimension but never too large relative to budget\n        if pop_size is None:\n            self.pop_size = int(max(4, min(10 * self.dim, max(4, self.budget // 10))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b, default):\n        # Accept scalar or array-like bounds; return numpy array of length dim\n        if b is None:\n            return np.full(self.dim, default, dtype=float)\n        arr = np.array(b, dtype=float)\n        if arr.ndim == 0:\n            return np.full(self.dim, float(arr), dtype=float)\n        if arr.size == 1:\n            return np.full(self.dim, float(arr.flat[0]), dtype=float)\n        if arr.size != self.dim:\n            # try to broadcast or truncate/extend\n            if arr.size < self.dim:\n                # repeat\n                rep = np.ones(self.dim, dtype=float) * arr.ravel()[-1]\n                rep[:arr.size] = arr.ravel()\n                return rep\n            else:\n                return arr.ravel()[:self.dim].astype(float)\n        return arr.astype(float)\n\n    def __call__(self, func):\n        rng = self.rng\n\n        # Bounds from func if present, else default [-5,5]\n        lb = self._ensure_array_bounds(getattr(func.bounds, \"lb\", None), -5.0)\n        ub = self._ensure_array_bounds(getattr(func.bounds, \"ub\", None), 5.0)\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        # Reduce population if budget too small to allow at least a few generations\n        self.pop_size = min(self.pop_size, max(4, self.budget))\n\n        # Initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate as many initial individuals as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If budget was tiny and some individuals were not evaluated, fill remaining slots with random but set fvals=inf\n        # (they will be candidates for mutation but won't have been evaluated yet; DE will evaluate offspring)\n\n        # Algorithm hyper-parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.08         # base probability of a Lévy jump\n        trust_radius = 0.2    # relative (0-1) radius in units of range_vec\n        min_trust = 1e-4\n        max_trust = 1.0\n\n        # Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step(scale=1.0):\n            s = rng.standard_cauchy(self.dim)\n            # Clip extreme values while retaining heavy tail\n            s = np.clip(s, -50.0, 50.0)\n            # scale to typical magnitudes\n            s = s / (np.std(s) + 1e-12)\n            return scale * s\n\n        gen = 0\n        no_improve_counter = 0\n        stagnation_limit = max(20, self.dim * 5)\n\n        # Main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            success_F = []\n            success_CR = []\n\n            # Per-generation small randomization widths\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # If current individual hasn't been evaluated yet (initialization cut short), treat as high fval\n                if not np.isfinite(fvals[i]):\n                    fvals[i] = np.inf\n\n                # Choose per-individual Fi and CRi around means (jDE-like)\n                Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0)\n\n                # Decide whether to do a Lévy jump candidate centered on best for exploration\n                if rng.rand() < p_levy:\n                    # Levy jump: long-range exploration centered on current best\n                    step = levy_step(scale=1.0)\n                    candidate = self.x_opt + (0.5 + rng.rand() * 1.5) * (step * (range_vec * trust_radius))\n                    # small local jitter as well\n                    if rng.rand() < 0.2:\n                        candidate += rng.normal(0, 0.01, size=self.dim) * range_vec\n                else:\n                    # DE/rand/1 mutation strategy (pick three distinct indices different from i)\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(idxs) < 3:\n                        # fallback: gaussian perturbation around pop[i]\n                        donor = pop[i] + Fi * rng.normal(0, 1.0, size=self.dim) * range_vec * trust_radius\n                    else:\n                        a, b, c = rng.choice(idxs, 3, replace=False)\n                        donor = pop[a] + Fi * (pop[b] - pop[c])\n                        # occasionally bias toward best when stagnating\n                        if no_improve_counter > stagnation_limit and rng.rand() < 0.3:\n                            donor = 0.7 * donor + 0.3 * (self.x_opt + Fi * (pop[b] - pop[c]))\n\n                    # binomial crossover to produce trial vector\n                    cr_mask = rng.rand(self.dim) < CRi\n                    # ensure at least one dimension crosses over\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(self.dim)] = True\n                    candidate = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # Evaluate candidate (consume one budget)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n                    # update global best\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = candidate.copy()\n                        no_improve_counter = 0\n                else:\n                    # no replacement, keep existing pop[i]\n                    pass\n\n                # Early break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of population loop (one generation)\n            # Adapt F_mean and CR_mean from successful parameter choices (Lehmer mean style)\n            if success_F:\n                # Lehmer-like weighted mean to prefer larger F that succeeded\n                numer = sum([f * f for f in success_F])\n                denom = sum(success_F) + 1e-12\n                F_mean = np.clip(0.9 * F_mean + 0.1 * (numer / denom), 0.05, 0.99)\n                CR_mean = np.clip(0.9 * CR_mean + 0.1 * (sum(success_CR) / len(success_CR)), 0.0, 1.0)\n            else:\n                # Slightly explore more if no successes\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # Trust-region local search around best: sample a small number of Gaussian perturbed candidates\n            # Number of local samples depends on remaining budget and dimension\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            n_local = min(max(1, self.dim // 2), remaining)\n            local_improved = 0\n            for _ in range(n_local):\n                sigma = trust_radius * (0.5 + rng.rand() * 0.5)  # anisotropic scaling factor\n                noise = rng.normal(0.0, 1.0, size=self.dim) * (sigma * range_vec)\n                candidate = self.x_opt + noise\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    local_improved += 1\n                    no_improve_counter = 0\n                # adjust budget check\n                if evals >= self.budget:\n                    break\n\n            # Adapt trust region: shrink if local improvements, else expand slightly to escape local traps\n            if local_improved > 0:\n                trust_radius *= max(0.6, 1.0 - 0.25 * local_improved)  # shrink toward min\n            else:\n                trust_radius *= 1.08  # expand if no local improvements\n\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # Adapt chance of Levy jumps: increase if stagnating, reduce if we have many successes\n            if successes > max(1, self.pop_size * 0.15) or local_improved > 0:\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                p_levy = min(0.5, p_levy * 1.03)\n\n            # stagnation and resets\n            if self.x_opt is not None and no_improve_counter < 1:\n                # we found improvement recently\n                pass\n            else:\n                # increment counter if no improvement in generation\n                no_improve_counter += 1\n\n            if no_improve_counter > 3 * stagnation_limit:\n                # strong stagnation reset: re-seed a portion of the population randomly\n                k = max(1, self.pop_size // 2)\n                replace_idx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in replace_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < self.f_opt:\n                        self.f_opt = fvals[j]\n                        self.x_opt = pop[j].copy()\n                        no_improve_counter = 0\n                # slightly enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # relaxation of parameters\n                F_mean = np.clip(F_mean * 0.95, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 1.02, 0.0, 1.0)\n                no_improve_counter = 0\n\n            # soft safeguard: if very few evaluated individuals (budget tiny), break\n            if self.budget <= 0:\n                break\n\n            # If we exhausted budget inside loops, stop\n            if evals >= self.budget:\n                break\n\n        # Final results\n        # Ensure x_opt exists\n        if self.x_opt is None:\n            # fallback: return best among evaluated fvals\n            idx = int(np.nanargmin(fvals))\n            self.x_opt = pop[idx].copy()\n            self.f_opt = float(fvals[idx])\n\n        return float(self.f_opt), self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.576 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16670878574711556, 0.5034914399151627, 0.5410379048248772, 0.8557212064250553, 0.7144517590393586, 0.8409758717629074, 0.33248853805720224, 0.5805285190607098, 0.7056253867804356, 0.5156247845112243]}, "task_prompt": ""}
{"id": "e524114f-2082-43e0-b14a-be9ddb0d1a4d", "fitness": 0.3229045904646725, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution (jDE-style) with occasional Lévy-flight long jumps and an adaptive trust-region local search; online adapts step sizes and restart behavior for robust global-to-local search.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # population size scaled with dimension and budget but kept reasonable\n        if pop_size is None:\n            # base scaling: between 4*dim and 50, but not exceeding budget/3\n            base = max(4 * self.dim, 12)\n            self.pop_size = int(min(max(base, 20), 50, max(4, self.budget // 5)))\n        else:\n            self.pop_size = int(pop_size)\n        # internal state for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = self.rng\n        # try to get bounds from func if available, else default to [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb.item()))\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub.item()))\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure correct shape\n        if lb.size != self.dim:\n            lb = np.broadcast_to(lb.ravel()[0], (self.dim,))\n        if ub.size != self.dim:\n            ub = np.broadcast_to(ub.ravel()[0], (self.dim,))\n\n        range_vec = ub - lb\n        assert np.all(range_vec > 0)\n\n        # initialize population uniformly in bounds\n        pop = rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (careful with budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = func(x)\n            evals += 1\n            fvals[i] = float(f)\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # if we didn't manage to evaluate full population due to tiny budget,\n        # set remaining individuals randomly without evaluation (they won't be used)\n        if evals >= self.budget:\n            # No budget left to run the algorithm\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.5       # crossover rate mean\n        step_scale = 0.25   # base scale for Lévy and trust radius\n\n        # trust-region radius (relative to range_vec)\n        trust_radius = 0.3  # initial fraction of range\n        min_trust = 1e-3\n        max_trust = 1.0\n\n        # Levy jump base probability (increases under stagnation)\n        p_levy = 0.02\n\n        stagnation_counter = 0\n        best_idx = int(np.argmin(fvals))\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # limit extreme outliers to avoid numerical blow-up\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # For adaptation: store successful Fi and CRi and weights\n        S_F = []\n        S_CR = []\n        S_w = []\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # process population members sequentially; each evaluated candidate consumes one eval.\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi (jDE-style mixture)\n                # Fi sampled from Cauchy around F_mean to encourage occasional large steps\n                Fi = F_mean + 0.1 * rng.standard_cauchy()\n                # ensure Fi in (0.05, 0.95)\n                Fi = float(np.clip(Fi, 0.05, 0.95))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n\n                # create donor vector\n                use_levy = (rng.random() < p_levy)\n                if use_levy:\n                    # Lévy jump centered on best for exploration\n                    s = levy_step()\n                    donor = best_x + step_scale * trust_radius * s * range_vec\n                    # small additional jitter\n                    donor += 0.01 * rng.normal(0, 1.0, size=self.dim) * range_vec\n                else:\n                    # DE/rand/1-like mutation with occasional target-to-best bias\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r1, r2, r3 = rng.choice(idxs, size=3, replace=False)\n                    if rng.random() < 0.2:\n                        # DE/best/1 variant occasionally\n                        donor = pop[r1] + Fi * (best_x - pop[r2]) + Fi * (pop[r3] - pop[i])\n                    else:\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # ensure donor uses trust-region for fine search occasionally\n                    if rng.random() < 0.15:\n                        donor = best_x + trust_radius * rng.normal(0, 1.0, size=self.dim) * range_vec * 0.5\n\n                # binomial crossover\n                jrand = rng.integers(0, self.dim)\n                cr_mask = rng.random(self.dim) < CRi\n                cr_mask[jrand] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # one evaluation\n                f_candidate = func(candidate)\n                evals += 1\n\n                # selection: greedy replacement in population\n                replaced = False\n                if f_candidate < fvals[i]:\n                    # success\n                    S_F.append(Fi)\n                    S_CR.append(CRi)\n                    # weight: improvement magnitude (Lehmer-like)\n                    w = float(abs(fvals[i] - f_candidate) + 1e-12)\n                    S_w.append(w)\n\n                    pop[i] = candidate\n                    fvals[i] = float(f_candidate)\n                    replaced = True\n\n                    # move means slightly toward successful parameters (adaptive)\n                    # we'll update means per generation using S_F/S_CR\n                else:\n                    # if levy used and failed, slightly increase p_levy to encourage different jump later\n                    if use_levy:\n                        p_levy = min(0.5, p_levy * 1.01)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = float(f_candidate)\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.9)\n                    # reward exploration reduction\n                    p_levy = max(0.01, p_levy * 0.9)\n                else:\n                    stagnation_counter += 1\n\n                # clear local Fi/CRi from scope if set to avoid reuse\n                # (not strictly necessary in python but kept for clarity)\n                del Fi, CRi\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            # adapt F_mean and CR_mean using weighted Lehmer mean if we had successes\n            if len(S_F) > 0:\n                # Lehmer mean weighted by improvement magnitude\n                w = np.array(S_w, dtype=float)\n                F_arr = np.array(S_F, dtype=float)\n                CR_arr = np.array(S_CR, dtype=float)\n                if w.sum() > 0:\n                    F_mean = float((w * (F_arr ** 2)).sum() / (w * F_arr).sum())\n                    CR_mean = float((w * CR_arr).sum() / w.sum())\n                else:\n                    F_mean = float(F_arr.mean())\n                    CR_mean = float(np.clip(CR_arr.mean(), 0.0, 1.0))\n                # reset success lists\n                S_F = []\n                S_CR = []\n                S_w = []\n                # keep means in sensible ranges\n                F_mean = float(np.clip(F_mean, 0.05, 0.95))\n                CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                sigma = trust_radius * (0.2 + 0.8 * rng.random(self.dim))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = func(candidate)\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = float(f_candidate)\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    # encourage exploitation -> slightly reduce Levy prob\n                    p_levy = max(0.01, p_levy * 0.85)\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # if many successes, encourage exploration less; if stagnating increase Lévy jumps\n            if stagnation_counter > (20 + 2 * self.dim):\n                # increase chance of long jumps and slightly increase diversity\n                p_levy = min(0.5, p_levy * 1.15)\n                trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(200, 20 * self.dim) and evals < self.budget:\n                # reinitialize half of the population randomly to diversify\n                n_reset = max(1, self.pop_size // 2)\n                reset_idx = rng.choice(np.arange(self.pop_size), size=n_reset, replace=False)\n                for idx in reset_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[idx] = rng.uniform(lb, ub)\n                    # evaluate new individual\n                    f_new = func(pop[idx])\n                    evals += 1\n                    fvals[idx] = float(f_new)\n                    if f_new < best_f:\n                        best_f = float(f_new)\n                        best_x = pop[idx].copy()\n                        stagnation_counter = 0\n                # slightly enlarge trust radius after reset to allow broader exploration\n                trust_radius = min(max_trust, trust_radius * 1.2)\n                # reduce Levy prob to not immediately jump too much after reset\n                p_levy = max(0.02, p_levy * 0.7)\n\n            # periodically ensure best is stored in population (elitism)\n            worst_idx = int(np.argmax(fvals))\n            if best_f < fvals[worst_idx]:\n                pop[worst_idx] = best_x.copy()\n                fvals[worst_idx] = best_f\n\n            # update global best record for returning\n            if best_f < self.f_opt:\n                self.f_opt = float(best_f)\n                self.x_opt = best_x.copy()\n\n            # termination safety: if trust radius tiny and Levy prob tiny, nudge them to avoid stagnation\n            if trust_radius < 1e-4:\n                trust_radius = 1e-4\n            if p_levy < 1e-4:\n                p_levy = 1e-4\n\n            # guard: if remaining budget is very small, break to avoid overhead\n            if (self.budget - evals) <= 0:\n                break\n\n        # final results\n        # Ensure x_opt is not None\n        if self.x_opt is None:\n            # fallback: return best known from population if any\n            idx = int(np.argmin(fvals))\n            self.x_opt = pop[idx].copy()\n            self.f_opt = float(fvals[idx])\n        return float(self.f_opt), self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.323 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09835279880016956, 0.15902929638643515, 0.6012737470560581, 0.41092075493735747, 0.22409837525309284, 0.738849190784527, 0.20789749538060232, 0.253006963553863, 0.3676590576392641, 0.1679582248553554]}, "task_prompt": ""}
{"id": "ab68a75c-de05-460c-aa93-95ea4cf0efaa", "fitness": 0.21155720270878908, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style online parameter adaptation, occasional Lévy (Cauchy) long jumps for global exploration, and a trust-region Gaussian local search around the best point (fast exploration, focused exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (DE/rand/1 + binomial crossover),\n      - jDE-like online adaptation of F and CR,\n      - occasional Lévy-like (Cauchy) long jumps centered on the best,\n      - trust-region Gaussian local search around current best with adaptive radius,\n      - stagnation detection and partial population re-seeding.\n\n    Designed for continuous bounded optimization (assumes bounds available via func.bounds\n    or defaults to [-5, 5] per dimension).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n\n        # Choose population size sensibly based on dimension and budget\n        if pop_size is None:\n            # base on dim but ensure small budgets keep pop <= budget\n            guess = max(8, 4 + 2 * self.dim)\n            self.pop_size = int(min(guess, max(4, self.budget // 6)))\n        else:\n            self.pop_size = int(pop_size)\n            self.pop_size = min(self.pop_size, max(4, self.budget))  # cannot exceed budget meaningfully\n\n        # state to return\n        self.x_opt = None\n        self.f_opt = np.inf\n\n        # internal adaptive means\n        self.F_mean = 0.6\n        self.CR_mean = 0.5\n        self.p_levy = 0.08\n\n        # trust region radius as fraction of the search range\n        self.trust_radius_frac = 0.20\n\n    def __call__(self, func):\n        # determine bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n\n        # ensure shape\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        # initial trust radius absolute\n        trust_radius = float(self.trust_radius_frac * np.mean(range_vec))\n\n        # utility: Levy-like heavy-tailed step (use Cauchy)\n        def levy_step(scale=1.0):\n            # standard Cauchy can be extremely heavy-tailed; clip extremes\n            step = self.rng.standard_cauchy(size=self.dim)\n            step = np.clip(step, -10.0, 10.0)\n            return scale * step\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # Evaluate as many as budget allows (we must not exceed budget)\n        to_eval = min(self.pop_size, self.budget - evals)\n        for i in range(to_eval):\n            x = pop[i].copy()\n            f = func(x)\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # if budget exhausted after initialization, return\n        if evals >= self.budget:\n            return self.f_opt, self.x_opt\n\n        # if some individuals remain unevaluated, leave them (fvals=inf)\n        # but they may be used as donors; ok as they'll be replaced when evaluated later.\n\n        # bookkeeping for adaptation\n        stagnation_counter = 0\n        best_idx = int(np.argmin(fvals))\n        if np.isfinite(fvals[best_idx]):\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # fallback best: pick arbitrary evaluated with finite f or random\n            finite_idx = np.where(np.isfinite(fvals))[0]\n            if finite_idx.size > 0:\n                best_idx = int(finite_idx[0])\n                best_x = pop[best_idx].copy()\n                best_f = fvals[best_idx]\n            else:\n                # No evaluations yet (shouldn't happen), set a random\n                best_x = pop[0].copy()\n                best_f = np.inf\n\n        # main loop: generate candidate for each target until budget exhausted\n        gen = 0\n        # minimal useful population for random picks\n        min_idx_pool = max(3, min(self.pop_size, self.pop_size))\n        while evals < self.budget:\n            gen += 1\n            # per-generation lists to accumulate successful Fi/CRi\n            succ_F = []\n            succ_CR = []\n            succ_levy = 0\n            succ_local = 0\n\n            # shuffle order for fairness\n            order = np.arange(self.pop_size)\n            self.rng.shuffle(order)\n\n            for t in order:\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[t].copy()\n                f_target = fvals[t]\n\n                # sample per-individual F and CR (jDE-like but simpler)\n                # F: sample from Cauchy centered on F_mean, then reflect into (0,1]\n                Fi = self.rng.standard_cauchy() * 0.1 + self.F_mean\n                # clip to (0.05,0.95)\n                Fi = float(np.clip(Fi, 0.05, 0.95))\n\n                CRi = float(np.clip(self.rng.normal(self.CR_mean, 0.1), 0.0, 1.0))\n\n                do_levy = (self.rng.rand() < self.p_levy)\n\n                if do_levy:\n                    # Levy jump centered on best (long-range exploration)\n                    levy_scale = 0.5 + self.rng.rand()  # scale relative to range\n                    candidate = best_x + levy_scale * levy_step() * range_vec\n                    # occasional direction bias: small local perturbation mixing with target\n                    mix = self.rng.rand()\n                    candidate = mix * candidate + (1 - mix) * x_target\n                else:\n                    # DE/rand/1 mutation: pick r1, r2, r3 distinct and != t if possible\n                    idxs = np.arange(self.pop_size)\n                    # allow picking even unevaluated individuals; indices must be distinct\n                    choices = np.setdiff1d(idxs, np.array([t], dtype=int))\n                    if choices.size < 3:\n                        # fallback: pure random recombine from bounds\n                        donor = lb + self.rng.rand(self.dim) * range_vec\n                    else:\n                        r = self.rng.choice(choices, size=3, replace=False)\n                        donor = pop[r[0]] + Fi * (pop[r[1]] - pop[r[2]])\n\n                    # binomial crossover\n                    cr_mask = self.rng.rand(self.dim) < CRi\n                    # ensure at least one dimension comes from donor\n                    jrand = self.rng.randint(self.dim)\n                    cr_mask[jrand] = True\n                    candidate = np.where(cr_mask, donor, x_target)\n\n                # bound projection\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate (if budget remains)\n                if evals < self.budget:\n                    f_candidate = func(candidate)\n                    evals += 1\n                else:\n                    break\n\n                # selection: greedy replacement\n                replaced = False\n                if f_candidate <= f_target or not np.isfinite(f_target):\n                    pop[t] = candidate\n                    fvals[t] = f_candidate\n                    replaced = True\n                    # adapt means a bit toward successful params\n                    succ_F.append(Fi)\n                    succ_CR.append(CRi)\n                    if do_levy:\n                        succ_levy += 1\n\n                # update best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = float(best_f)\n                    self.x_opt = best_x.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # minimal online adaptation of F_mean and CR_mean (Lehmer-like)\n                if len(succ_F) > 0:\n                    # small step toward average successful\n                    meanF_succ = float(np.mean(succ_F))\n                    self.F_mean = 0.9 * self.F_mean + 0.1 * meanF_succ\n                if len(succ_CR) > 0:\n                    meanCR_succ = float(np.mean(succ_CR))\n                    self.CR_mean = 0.9 * self.CR_mean + 0.1 * meanCR_succ\n\n                # dynamic adjust p_levy: if many successes from levy, slightly reduce prob;\n                # if long stagnation, increase\n                if gen % 5 == 0:\n                    if succ_levy > 0:\n                        self.p_levy = max(0.01, self.p_levy * 0.95)\n                    if stagnation_counter > (5 * self.pop_size):\n                        self.p_levy = min(0.5, self.p_levy * 1.2)\n\n                # avoid using stale local Fi/CRi variables later in loop\n\n            # End of generation: perform trust-region local search around current best\n            if evals >= self.budget:\n                break\n\n            remaining = self.budget - evals\n            # number of local samples: a few, scaled with dim but limited by remaining budget\n            n_local = int(min(max(2, self.dim // 2), max(1, remaining // 6)))\n            # anisotropic Gaussian perturbations\n            local_success = False\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dim\n                sigma = trust_radius * (0.2 + 0.8 * self.rng.rand(self.dim))\n                candidate = best_x + sigma * self.rng.randn(self.dim)\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = func(candidate)\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = float(best_f)\n                    self.x_opt = best_x.copy()\n                    # successful local step -> shrink trust region to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(trust_radius, 1e-8)\n                    local_success = True\n                    succ_local += 1\n                    # update adaptation towards exploitation\n                    self.F_mean = 0.9 * self.F_mean + 0.1 * 0.4\n                    self.CR_mean = 0.9 * self.CR_mean + 0.1 * 0.9\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful -> loosen a bit\n                    trust_radius *= 1.05\n                    trust_radius = min(trust_radius, 0.5 * np.mean(range_vec))\n\n            # adjust p_levy depending on recent progress\n            if stagnation_counter > (3 * self.pop_size):\n                # struggling => increase long jumps\n                self.p_levy = min(0.6, self.p_levy * 1.15)\n                # nudge population diversity by slight randomization of some individuals\n                n_div = max(1, self.pop_size // 8)\n                for _ in range(n_div):\n                    if evals >= self.budget:\n                        break\n                    idx = self.rng.randint(self.pop_size)\n                    pop[idx] = lb + self.rng.rand(self.dim) * range_vec\n                    # evaluate new individual\n                    fvals[idx] = func(pop[idx])\n                    evals += 1\n                    if fvals[idx] < best_f:\n                        best_f = float(fvals[idx])\n                        best_x = pop[idx].copy()\n                        self.f_opt = float(best_f)\n                        self.x_opt = best_x.copy()\n                        stagnation_counter = 0\n            else:\n                # some progress => slightly decay exploration chance\n                self.p_levy = max(0.01, self.p_levy * 0.99)\n\n            # strong stagnation reset: if no improvement for a long time, re-seed half\n            if stagnation_counter > max(200, 10 * self.pop_size):\n                n_reset = max(1, self.pop_size // 2)\n                for i in range(n_reset):\n                    if evals >= self.budget:\n                        break\n                    idx = self.rng.randint(self.pop_size)\n                    pop[idx] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[idx] = func(pop[idx])\n                    evals += 1\n                    if fvals[idx] < best_f:\n                        best_f = float(fvals[idx])\n                        best_x = pop[idx].copy()\n                        self.f_opt = float(best_f)\n                        self.x_opt = best_x.copy()\n                        stagnation_counter = 0\n                # enlarge trust region a bit after reset to encourage exploration\n                trust_radius = min(0.5 * np.mean(range_vec), trust_radius * 1.5)\n                # nudge means\n                self.F_mean = min(0.9, self.F_mean * 1.05)\n                self.CR_mean = max(0.1, self.CR_mean * 0.95)\n                stagnation_counter = 0  # allow fresh exploration\n\n            # small safeguards on means and trust radius\n            self.F_mean = float(np.clip(self.F_mean, 0.05, 0.95))\n            self.CR_mean = float(np.clip(self.CR_mean, 0.0, 0.99))\n            trust_radius = float(np.clip(trust_radius, 1e-8, 0.5 * np.mean(range_vec)))\n\n        # final record\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.212 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09895511840284277, 0.15618426536620866, 0.28648309414012463, 0.21649142181823544, 0.20082805451414942, 0.2727602990803032, 0.24630638051879805, 0.2543910017114209, 0.23746158555935482, 0.1457108059764528]}, "task_prompt": ""}
{"id": "8e80f63e-a325-4eaf-9c5d-0e414da4783a", "fitness": 0.29866561169537303, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style online parameter adaptation, occasional Lévy-flight jumps for long-range escapes, and a trust-region local Gaussian search around the current best (fast exploration + focused exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=42)\n        f_best, x_best = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # bounds are known for BBOB: [-5, 5] per dimension, but allow override in __call__\n        self.lb = -5.0\n        self.ub = 5.0\n        self.range = self.ub - self.lb\n\n        # population size scaled with dimension but limited relative to budget\n        if pop_size is None:\n            # aim for moderately sized population: between 6 and 10*dim, but not more than budget/4\n            default = int(max(6, min(10 * self.dim, 50)))\n            max_allowed = max(4, self.budget // 6)\n            self.pop_size = min(default, max_allowed)\n        else:\n            self.pop_size = max(4, int(pop_size))\n\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_bounds_from_func(self, func):\n        # if func provides bounds, use them\n        if hasattr(func, \"bounds\"):\n            try:\n                lb = np.asarray(func.bounds.lb).astype(float)\n                ub = np.asarray(func.bounds.ub).astype(float)\n                if lb.shape == (self.dim,) and ub.shape == (self.dim,):\n                    self.lb = lb\n                    self.ub = ub\n                    self.range = self.ub - self.lb\n                    return\n            except Exception:\n                pass\n        # else keep default scalar bounds expanded to vector\n        self.lb = np.full(self.dim, float(self.lb))\n        self.ub = np.full(self.dim, float(self.ub))\n        self.range = self.ub - self.lb\n\n    def __call__(self, func):\n        # prepare bounds possibly from function\n        self._ensure_bounds_from_func(func)\n\n        # bookkeeping\n        evals = 0\n        pop_n = self.pop_size\n        dim = self.dim\n        rng = self.rng\n\n        # initialize population uniformly\n        pop = rng.uniform(self.lb, self.ub, size=(pop_n, dim))\n        fitness = np.full(pop_n, np.inf)\n\n        # evaluation of initial population (may be limited by budget)\n        for i in range(pop_n):\n            if evals >= self.budget:\n                break\n            fitness[i] = float(func(pop[i]))\n            evals += 1\n\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = pop[i].copy()\n\n        # If budget was too small to evaluate all individuals, keep remaining un-evaluated (inf)\n        # algorithm hyper-parameters\n        F_mean = 0.6\n        CR_mean = 0.3\n        adapt_rate = 0.1  # adaptation speed for means\n        base_levy_prob = 0.05\n        trust_radius = 0.1 * np.mean(self.range)  # initial trust region radius (absolute)\n        trust_min = 1e-6 * np.mean(self.range)\n        trust_max = 0.5 * np.mean(self.range)\n        stagnation = 0\n        best_since_reset = 0\n        stagnation_threshold = max(30, pop_n * 4)\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step(scale=1.0):\n            # Use standard Cauchy for heavy tails, but clip extreme outliers\n            step = rng.standard_cauchy(size=dim)\n            # clip extremes to avoid numerical blow-up but keep heavy tail\n            step = np.clip(step, -15.0, 15.0)\n            # scale by per-dim range and extra scale\n            return step * (0.1 * scale)\n\n        # Main loop: iterate until budget exhausted\n        gen = 0\n        while evals < self.budget:\n            gen += 1\n\n            # per-generation trackers for adaptation\n            successful_F = []\n            successful_CR = []\n            success_count = 0\n\n            # dynamic levy probability increases with stagnation\n            p_levy = min(0.5, base_levy_prob + 0.4 * (stagnation / max(1, stagnation_threshold)))\n\n            # Process population sequentially (each candidate may consume one evaluation)\n            for i in range(pop_n):\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[i]\n                f_target = fitness[i]\n\n                # sample Fi and CRi (jDE-like but simpler)\n                Fi = rng.normal(F_mean, 0.2)\n                if Fi <= 0:\n                    Fi = 0.05\n                Fi = float(np.clip(Fi, 0.05, 1.0))\n                CRi = rng.normal(CR_mean, 0.2)\n                CRi = float(np.clip(CRi, 0.0, 1.0))\n\n                # decide exploration mode: Levy jump or DE mutation\n                if rng.rand() < p_levy:\n                    # Levy jump centered on best for exploration\n                    if self.x_opt is None:\n                        # no best known (rare), fallback to random\n                        donor = rng.uniform(self.lb, self.ub)\n                    else:\n                        # get normalized levy step and scale by trust radius and global range\n                        step = levy_step(scale=1.0)\n                        donor = self.x_opt + step * (trust_radius / max(trust_min, 1e-12))\n                else:\n                    # DE/rand/1-like mutation with Fi scaling\n                    idxs = [idx for idx in range(pop_n) if idx != i]\n                    a, b, c = rng.choice(idxs, size=3, replace=False)\n                    donor = pop[a] + Fi * (pop[b] - pop[c])\n                    # incorporate slight move towards current best to bias search (soft \"best\" guidance)\n                    if self.x_opt is not None and rng.rand() < 0.2:\n                        donor = donor + 0.1 * Fi * (self.x_opt - x_target)\n\n                # binomial crossover\n                cross = rng.rand(dim) < CRi\n                if not np.any(cross):\n                    cross[rng.randint(dim)] = True\n                trial = np.where(cross, donor, x_target)\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, self.lb), self.ub)\n\n                # ensure we respect budget\n                if evals >= self.budget:\n                    break\n\n                f_trial = float(func(trial))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_trial <= f_target:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    successful_F.append(Fi)\n                    successful_CR.append(CRi)\n                    success_count += 1\n\n                    # update global best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        stagnation = 0\n                        best_since_reset += 1\n                    else:\n                        stagnation = min(stagnation + 1, self.budget)  # minor increment if not improvement\n\n                else:\n                    # unsuccessful -> small chance to replace with mutated but unevaluated to maintain diversity\n                    stagnation += 1\n\n                # Adapt means incrementally after each replacement to keep responsiveness\n                if successful_F:\n                    F_mean = (1 - adapt_rate) * F_mean + adapt_rate * np.mean(successful_F)\n                if successful_CR:\n                    CR_mean = (1 - adapt_rate) * CR_mean + adapt_rate * np.mean(successful_CR)\n\n                # if budget exhausted break\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            # Trust-region local search around best: sample a few candidates with Gaussian noise\n            if evals < self.budget and self.x_opt is not None:\n                # local sample count depends on remaining budget and dimension (keep small)\n                remaining = self.budget - evals\n                local_cap = max(1, min(5 + dim // 4, remaining))\n                local_samples = rng.randint(1, local_cap + 1)\n\n                local_success = 0\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma: base trust_radius scaled by random [0.5,1.5] per-dim\n                    sigma = (trust_radius * (0.5 + rng.rand(dim)))\n                    cand = self.x_opt + rng.randn(dim) * sigma\n                    cand = np.minimum(np.maximum(cand, self.lb), self.ub)\n                    f_cand = float(func(cand))\n                    evals += 1\n                    if f_cand < self.f_opt:\n                        self.f_opt = f_cand\n                        self.x_opt = cand.copy()\n                        local_success += 1\n                        stagnation = 0\n                        best_since_reset += 1\n                    else:\n                        stagnation += 1\n                # adjust trust region: shrink when successful, expand when failing\n                if local_success > 0:\n                    trust_radius = max(trust_min, trust_radius * (0.8 ** local_success))\n                else:\n                    trust_radius = min(trust_max, trust_radius * 1.12)\n\n            # Stagnation handling and population reset\n            if stagnation >= stagnation_threshold:\n                # strong stagnation reset: re-seed half of population\n                replace_n = max(1, pop_n // 2)\n                indices = rng.choice(range(pop_n), replace_n, replace=False)\n                for idx in indices:\n                    if evals >= self.budget:\n                        break\n                    new_x = rng.uniform(self.lb, self.ub)\n                    pop[idx] = new_x\n                    fitness[idx] = float(func(new_x))\n                    evals += 1\n                    if fitness[idx] < self.f_opt:\n                        self.f_opt = fitness[idx]\n                        self.x_opt = new_x.copy()\n                # enlarge trust radius slightly to encourage exploration\n                trust_radius = min(trust_max, trust_radius * 1.8)\n                stagnation = 0\n                best_since_reset = 0\n                # increase levy probability temporarily\n                p_levy = min(0.7, p_levy + 0.2)\n\n            # gentle drift of means to avoid collapse to extremes\n            F_mean = float(np.clip(F_mean, 0.05, 0.95))\n            CR_mean = float(np.clip(CR_mean, 0.0, 0.99))\n\n            # safety: if very few evaluations left, perform greedy random local probing around best\n            if self.budget - evals <= max(1, dim // 2) and evals < self.budget and self.x_opt is not None:\n                while evals < self.budget:\n                    probe = self.x_opt + rng.randn(dim) * (max(trust_min, trust_radius * 0.5))\n                    probe = np.minimum(np.maximum(probe, self.lb), self.ub)\n                    f_probe = float(func(probe))\n                    evals += 1\n                    if f_probe < self.f_opt:\n                        self.f_opt = f_probe\n                        self.x_opt = probe.copy()\n                        stagnation = 0\n                    else:\n                        stagnation += 1\n\n        # final results\n        return float(self.f_opt), (None if self.x_opt is None else np.asarray(self.x_opt))", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.299 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10468282094186987, 0.17592524946912935, 0.5843577877187405, 0.8954992800729575, 0.20719123678566564, 0.23166796788756217, 0.23559556582628105, 0.19912287375245386, 0.18781915198421073, 0.16479418251485956]}, "task_prompt": ""}
{"id": "2b5c1f32-2f9b-4848-a2ba-051a3f80e20c", "fitness": 0.34137561110925113, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and an adaptive trust-region local search with online step-size and operator adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Designed to be robust on a wide range of continuous optimization\n    problems (e.g., BBOB noiseless functions on [-5,5]^d).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension and budget but kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # make sure population not larger than a fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        # if vector provided, try to broadcast or validate length\n        if b.size == self.dim:\n            return b.astype(float)\n        # if shape doesn't match, try flatten then tile/truncate\n        b = b.flatten()\n        if b.size < self.dim:\n            # repeat to match dim\n            reps = int(np.ceil(self.dim / b.size))\n            b = np.tile(b, reps)[:self.dim]\n            return b.astype(float)\n        else:\n            return b[:self.dim].astype(float)\n\n    def __call__(self, func):\n        # bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        # ensure bounds validity\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (up to budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals[valid])\n            best_indices = np.where(valid)[0]\n            best_idx = best_indices[best_idx]\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # no evals possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight\n        CR_mean = 0.9       # crossover probability\n        p_levy = 0.08       # probability of a Lévy jump\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # scale to unit-typical magnitude to combine with range_vec\n            s = s / (np.std(s) + 1e-12)\n            return s\n\n        gen = 0\n        stagnation_counter = 0\n        last_best_f = best_f\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation jitter for means (keeps some inertia)\n            # number of target attempts per generation: up to population size but may be limited by budget\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # ensure we have enough evaluated individuals to pick DE parents\n                active_idx = np.where(np.isfinite(fvals))[0]\n                # Prepare defaults\n                Fi = None\n                CRi = None\n\n                # choose whether to do a Lévy jump centered on the best or a DE step\n                if rng.rand() < p_levy or active_idx.size < 4:\n                    # Lévy jump centered on best\n                    step = levy_step()\n                    # scale by trust radius relative to range vec\n                    step_scale = trust_radius / (np.linalg.norm(range_vec) + 1e-12)\n                    scale_factor = 0.5 + 0.5 * rng.rand()\n                    donor = best_x + scale_factor * step * (range_vec * step_scale)\n                    # occasionally mix with current individual to keep diversity\n                    if rng.rand() < 0.5 and np.isfinite(fvals[i]):\n                        donor = 0.5 * donor + 0.5 * pop[i]\n                else:\n                    # DE/rand/1-like mutation\n                    # pick 3 distinct indices excluding i (and prefer evaluated ones)\n                    candidates = np.setdiff1d(np.arange(self.pop_size), np.array([i], dtype=int))\n                    # prefer indices with finite fvals, fallback to any\n                    if active_idx.size >= 4:\n                        choices_pool = np.setdiff1d(active_idx, np.array([i], dtype=int))\n                        if choices_pool.size >= 3:\n                            pick = rng.choice(choices_pool, 3, replace=False)\n                        else:\n                            pick = rng.choice(candidates, 3, replace=False)\n                    else:\n                        pick = rng.choice(candidates, 3, replace=False)\n                    r1, r2, r3 = pick\n\n                    # adapt F and CR per individual (jDE-like)\n                    Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                    CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n                    donor_vec = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor_vec, pop[i])\n                    donor = trial\n\n                # boundary projection\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                # If current target had no finite fval yet, treat fvals[i] as +inf\n                if f_candidate < fvals[i]:\n                    fvals[i] = f_candidate\n                    pop[i] = candidate.copy()\n                    successes += 1\n                    # adapt means slightly toward the successful Fi/CRi if available\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    else:\n                        # nudge a bit toward exploration if Levy succeeded\n                        F_mean = 0.98 * F_mean + 0.02 * 0.6\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    else:\n                        CR_mean = 0.98 * CR_mean + 0.02 * 0.9\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                # else: we don't necessarily increment stagnation here; we'll check after generation\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # sample count: small handful scaled by dim and remaining budget\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter += 1\n\n            # if many successes, encourage exploitation; if stagnating increase Lévy jumps\n            if successes > 0:\n                # reduce chance of Levy if progress made\n                if successes > max(1, int(0.2 * self.pop_size)):\n                    p_levy = max(0.01, p_levy * 0.95)\n                else:\n                    p_levy = max(0.01, p_levy * 0.98)\n                # move F_mean/CR_mean a bit toward nominal exploitation-friendly values\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many generations/evaluations, re-seed part of population\n            if best_f < last_best_f:\n                last_best_f = best_f\n                # reset stagnation if we improved\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population randomly to diversify and evaluate a subset\n                k = max(1, self.pop_size // 2)\n                chosen = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in chosen:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    # evaluate new individual immediately to populate fvals\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # slightly enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # give algorithm some time to exploit after reset\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.341 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.23809571552096664, 0.20331132261885354, 0.32456000476752667, 0.30924687266781226, 0.465527344454861, 0.3532775067768974, 0.3065523684962418, 0.38755647752825095, 0.6103440773268415, 0.2152844209342598]}, "task_prompt": ""}
{"id": "fb3c4384-ecba-4591-8632-4a3fa73d94aa", "fitness": 0.26737315784386356, "name": "AdaptiveHybridDELevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with per-individual parameter adaptation, occasional Lévy-flight long jumps, and a trust-region local search around the current best with online step-size adjustment.", "code": "import numpy as np\n\nclass AdaptiveHybridDELevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE) with\n    per-individual F/CR adaptation (jDE-like), occasional Lévy-flight jumps\n    for long-range exploration, and a trust-region local search around the\n    current best. Online adaptation of probabilities and trust radius\n    balances exploration and exploitation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size scaled with dimension\n        if pop_size is None:\n            self.pop_size = max(6, min(80, int(10 + 4 * np.sqrt(self.dim))))\n            # don't exceed a fraction of budget\n            self.pop_size = min(self.pop_size, max(2, self.budget // 10))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n        # results will be written by __call__\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # broadcast if possible\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # get bounds from func if available, else fall back to [-5, 5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate as many initial individuals as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # if no evaluations possible, return empty\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # establish current best\n        valid_idx = np.argmin(fvals)\n        best_x = pop[valid_idx].copy()\n        best_f = fvals[valid_idx]\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # mean differential weight\n        CR_mean = 0.9       # mean crossover prob\n        p_levy = 0.08       # probability of applying a Levy jump mutation\n        step_scale = 0.25   # base scale for Levy and trust radius relative to range\n        trust_radius = 0.2 * range_norm\n        min_trust = 1e-8\n        max_trust = range_norm * 4.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: Levy-like heavy-tailed step using Cauchy, clipped to avoid blow-up\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            # clip extreme outliers but keep heavy tail\n            return np.clip(s, -20.0, 20.0)\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # per-generation small jitter to means to encourage exploration\n            F_mean = np.clip(F_mean * (1.0 + rng.normal(0.0, 0.01)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + rng.normal(0.0, 0.01), 0.0, 1.0)\n\n            # process each member sequentially (consumes evaluations as we go)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual parameters (jDE-like)\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 0.99)\n                CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                if rng.rand() < p_levy:\n                    # Levy-centered jump around the current best for global exploration\n                    step = levy_step()\n                    # scale: base step_scale * trust_radius relative to range components\n                    scale = (step_scale * 0.5 + 0.5 * rng.rand()) * (trust_radius / (range_norm + 1e-12))\n                    donor = best_x + scale * step * (range_vec / (np.mean(range_vec) + 1e-12))\n                    # mix with a randomly mutated DE donor for diversity\n                    # select three distinct indices different from i\n                    ids = np.arange(self.pop_size)\n                    ids = ids[ids != i]\n                    if ids.size >= 3:\n                        r = rng.choice(ids, 3, replace=False)\n                        donor_de = pop[r[0]] + Fi * (pop[r[1]] - pop[r[2]])\n                        # blend donor vectors\n                        alpha = rng.rand()\n                        donor = alpha * donor + (1 - alpha) * donor_de\n                else:\n                    # classical DE/rand/1 mutation\n                    ids = np.arange(self.pop_size)\n                    ids = ids[ids != i]\n                    if ids.size >= 3:\n                        r = rng.choice(ids, 3, replace=False)\n                        donor = pop[r[0]] + Fi * (pop[r[1]] - pop[r[2]])\n                    else:\n                        # fallback: Gaussian perturbation of current\n                        donor = pop[i] + Fi * rng.normal(0, 1.0, size=self.dim) * (range_vec / (np.mean(range_vec) + 1e-12))\n\n                # binomial crossover\n                cr_mask = rng.rand(self.dim) < CRi\n                # ensure at least one dimension is from donor\n                if not np.any(cr_mask):\n                    cr_mask[rng.randint(self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # project to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # one evaluation for candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # greedy selection\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt means toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful local improvement -> shrink trust to focus\n                    trust_radius = max(min_trust, trust_radius * 0.9)\n                else:\n                    stagnation_counter += 1\n\n            # End of generation: do a trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # choose number of local samples adaptive to remaining budget and dim\n            local_samples = min(remaining, max(1, int(1 + self.dim // 4)))\n            # anisotropic sigma: scale per-dimension by trust radius relative to overall range\n            sigma = (trust_radius / (range_norm + 1e-12)) * (step_scale * (0.5 + rng.rand()))\n\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # Gaussian local sample around best\n                candidate = best_x + rng.normal(0, sigma, size=self.dim) * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    successes += 1\n                    # successful local step -> contract trust radius\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n                    # unsuccessful local attempt -> slightly expand trust radius to escape local trap\n                    trust_radius = min(max_trust, trust_radius * (1.02 + 0.01 * rng.rand()))\n\n            # Adjust exploration probability and DE params based on success count\n            if successes > 0:\n                # when we find improvements, reduce frequency of Levy jumps gradually\n                if successes > max(1, int(0.2 * self.pop_size)):\n                    p_levy = max(0.01, p_levy * 0.95)\n                else:\n                    p_levy = max(0.01, p_levy * 0.98)\n                # nudge means toward exploitation defaults\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation: increase Levy chance and diversify\n                p_levy = min(0.6, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: re-seed part of the population randomly\n            if stagnation_counter > max(50, self.dim * 6):\n                k = max(1, self.pop_size // 2)\n                indices_to_reset = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices_to_reset:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius to escape basin\n                trust_radius = min(max_trust, trust_radius * 1.8)\n                stagnation_counter = 0\n\n            # small population-level diversity injection occasionally\n            if rng.rand() < 0.03 and evals < self.budget:\n                j = rng.randint(self.pop_size)\n                pop[j] = lb + rng.rand(self.dim) * range_vec\n                # evaluate re-seeded member if budget allows\n                if evals < self.budget:\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveHybridDELevyTrust scored 0.267 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1274832464444099, 0.17198097556375525, 0.3658491002657208, 0.19282537022408053, 0.22532262543830306, 0.6376427219347501, 0.2437055512417825, 0.31226318532006436, 0.2165194032543155, 0.1801393987514538]}, "task_prompt": ""}
{"id": "9ebda531-c220-4596-94e0-05220f1550a2", "fitness": 0.27074435182949863, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a shrinking/growing trust-region local search with online step-size adaptation and restart diversification.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid DE + Lévy + Trust-region optimizer.\n\n    Key ideas:\n    - Population-based Differential Evolution (DE/rand/1 with binomial crossover)\n      for global recombination and steady improvement.\n    - Occasional Lévy-like (Cauchy) jumps centered on the best to escape basins.\n    - Trust-region local sampling around the best (Gaussian perturbations) that\n      adaptively shrinks on success and expands on failure.\n    - Online adaptation of F and CR means (simple jDE-like nudges) and of the\n      Lévy probability. Partial population re-seeding on stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # sensible default population size: grows with dim but limited by budget\n        if pop_size is None:\n            self.pop_size = max(4, min(self.budget // 10, 10 + 2 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n            self.pop_size = max(4, min(self.pop_size, max(4, self.budget)))\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.ndim == 0:\n            return np.full(self.dim, float(b))\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.reshape(self.dim,)\n        # try broadcasting to dim\n        return np.broadcast_to(b.ravel(), (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # bounds from func\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # safety: if bounds degenerate, give small positive range\n        range_vec = np.where(range_vec == 0.0, 1e-6, range_vec)\n\n        rng = self.rng\n\n        # population initialization in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # find initial best among evaluated individuals\n        finite_mask = np.isfinite(fvals)\n        if not np.any(finite_mask):\n            # no evaluations possible -> return infinities\n            self.f_opt = np.inf\n            self.x_opt = pop[0].copy()\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals)\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-parameters\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # initial probability of a Lévy jump\n        step_scale = 0.25   # base scale factor relative to search range for trust radius\n        trust_radius = np.linalg.norm(range_vec) * step_scale\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        # stagnation and adaptation trackers\n        stagnation_counter = 0\n        successes = 0\n        gen = 0\n        stagnation_threshold = max(30 * self.dim, 200)\n\n        # helper: levy-like step generator using Cauchy (heavy-tailed)\n        def levy_step(scale):\n            # sample Cauchy (standard) and clip extreme outliers\n            step = rng.standard_cauchy(size=self.dim)\n            step = np.clip(step, -1e3, 1e3)\n            return step * scale\n\n        # main loop: generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # per-generation adaptation of F_mean/CR_mean jitter\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi around their means (jDE-like)\n                Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 0.95)\n                CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # choose exploration mode\n                if rng.rand() < p_levy:\n                    # Lévy-like jump centered on current best_x\n                    levy_scale = trust_radius / (np.linalg.norm(range_vec) + 1e-12)\n                    step = levy_step(levy_scale)  # relative step\n                    candidate = best_x + step * range_vec\n                    # small local jitter to keep diversity\n                    candidate += rng.normal(0, 0.02 * trust_radius, size=self.dim)\n                else:\n                    # DE/rand/1-like mutation + binomial crossover\n                    idxs = np.arange(self.pop_size)\n                    if self.pop_size <= 3:\n                        r1, r2, r3 = 0, 0, 0\n                    else:\n                        # allow selection from all indices except i\n                        idxs = idxs[idxs != i]\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    candidate = np.where(cr_mask, donor, pop[i])\n\n                # project to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement (target-to-trial)\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # nudge parameter means toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # slight drift away if unsuccessful to encourage diversity\n                    F_mean = np.clip(0.995 * F_mean + 0.0005, 0.05, 0.99)\n                    CR_mean = np.clip(0.995 * CR_mean + 0.0005, 0.0, 1.0)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful local refinement -> shrink trust radius\n                    trust_radius = max(min_trust, trust_radius * 0.88)\n                else:\n                    stagnation_counter += 1\n                    # modest expansion to escape local plateaus\n                    trust_radius = min(max_trust, trust_radius * 1.01)\n\n            # after population loop: trust-region local search around best\n            if evals >= self.budget:\n                break\n\n            # number of local samples is small, adaptive to remaining budget and dim\n            remaining = self.budget - evals\n            n_local = min(max(1, self.dim // 2), remaining, 6)\n            # anisotropic local sampling: scale per-dim by trust_radius and range\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                sigma_vec = (rng.rand(self.dim) * 0.8 + 0.2) * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma_vec * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.80)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slightly enlarge trust radius to explore further\n                    trust_radius = min(max_trust, trust_radius * 1.03)\n\n            # adaptive adjustment of Lévy probability and means based on successes\n            if successes > 0:\n                # reduce probability of long jumps slightly if many successes (exploitation)\n                decay = 0.95 if successes > (self.pop_size * 0.2) else 0.98\n                p_levy = max(0.01, p_levy * decay)\n                # keep F_mean/CR_mean in reasonable ranges\n                F_mean = np.clip(F_mean, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean, 0.0, 1.0)\n            else:\n                # no success: slowly increase chance for long jumps\n                p_levy = min(0.6, p_levy * 1.08)\n\n            # stagnation handling: if no improvement for long time, partial re-seed\n            if stagnation_counter >= stagnation_threshold:\n                stagnation_counter = 0\n                # reinitialize half of population (or less if budget limited)\n                k = max(1, self.pop_size // 2)\n                indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # immediate improvement handling\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # after reseed, enlarge trust radius to encourage global moves\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # increase Lévy chance to attempt escapes\n                p_levy = min(0.5, p_levy * 1.5)\n                # slightly randomize means to avoid lock-in\n                F_mean = np.clip(F_mean * (0.9 + 0.2 * rng.rand()), 0.05, 0.95)\n                CR_mean = np.clip(CR_mean * (0.9 + 0.2 * rng.rand()), 0.0, 1.0)\n\n        # done: set outputs\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.271 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11365163381151344, 0.17192050063460118, 0.3503376874323534, 0.2932405308768917, 0.24385431801832358, 0.5294380522071864, 0.2519095086073523, 0.35787845819140585, 0.24084040205683965, 0.154372426458519]}, "task_prompt": ""}
{"id": "a7228d21-1162-41b1-a7f8-fe2752738de4", "fitness": 0.2865325171912334, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size and parameter adaptation for robust global-to-local search.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, seed=42)\n        fopt, xopt = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size relative to dimension\n        if pop_size is None:\n            self.pop_size = max(6, min(60, int(6 + 2 * np.sqrt(self.dim))))\n            # never larger than fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # outputs\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like bounds and broadcast to dimension\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds must be scalar or of length dim\")\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box `func` using at most self.budget function evaluations.\n        `func` is expected to accept a 1D numpy array of length dim and expose\n        .bounds.lb and .bounds.ub (scalars or arrays) with the search box.\n        \"\"\"\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # clamp to [-inf, inf] and ensure ub>lb\n        if np.any(ub <= lb):\n            raise ValueError(\"Upper bounds must be strictly greater than lower bounds\")\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # evaluate initial population until budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            fvals[i] = f\n            evals += 1\n\n        # determine initial best\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.nanargmin(fvals)\n            best_f = fvals[best_idx]\n            best_x = pop[best_idx].copy()\n        else:\n            # no evaluations possible (budget == 0)\n            self.f_opt = np.inf\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        min_trust = 1e-8\n        max_trust = range_norm * 2.0\n\n        # Lévy jump probability (chance to propose a long-jump centered on best)\n        p_levy = 0.05\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step():\n            s = self.rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # clipping helper (reflecting boundary might be better but we use clip)\n        def project(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # main loop: iterate generations until budget exhausted\n        idxs = np.arange(self.pop_size)\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            improved_this_gen = False\n\n            # adapt per-generation randomization of F and CR around their means\n            # jDE-like: per individual sample Fi and CRi\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi around means with small noise\n                Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 0.99)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # decide Levy jump or DE\n                if self.rng.rand() < p_levy:\n                    # Levy jump centered at best (long-range)\n                    step = levy_step()\n                    # scale: combine step_scale, trust radius and problem range\n                    candidate = best_x + (step_scale * trust_radius / (range_norm + 1e-12)) * (step * range_vec)\n                    # add a small Gaussian perturb to avoid staying identical\n                    candidate += self.rng.normal(scale=1e-3 * range_vec, size=self.dim)\n                else:\n                    # DE/rand/1 mutation\n                    # choose r1,r2,r3 distinct and not equal to i\n                    choices = np.setdiff1d(idxs, [i])\n                    if choices.size < 3:\n                        # fallback random\n                        donor = pop[i] + Fi * self.rng.randn(self.dim)\n                        candidate = donor\n                    else:\n                        r1, r2, r3 = self.rng.choice(choices, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = self.rng.rand(self.dim) < CRi\n                        # ensure at least one dimension from donor\n                        if not np.any(cr_mask):\n                            cr_mask[self.rng.randint(0, self.dim)] = True\n                        trial = np.where(cr_mask, donor, pop[i])\n                        candidate = trial\n\n                # project to bounds\n                candidate = project(candidate)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # nudge means slightly toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    improved_this_gen = True\n                    stagnation_counter = 0\n                # else do not change stagnation counter here\n\n                # modestly diversify Fi/CRi usage in subsequent steps\n                F_mean = np.clip(F_mean, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean, 0.0, 1.0)\n\n            # End of generation\n            if not improved_this_gen:\n                stagnation_counter += 1\n\n            # trust-region local search around best: sample a few local candidates\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample count: small handful scaled by dim and remaining budget\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 8)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic Gaussian noise scaled by trust_radius and range_vec\n                sigma = (0.5 + self.rng.rand(self.dim) * 0.5) * (trust_radius / (range_norm + 1e-12))\n                candidate = best_x + self.rng.normal(scale=sigma) * range_vec\n                candidate = project(candidate)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.8\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                    improved_this_gen = True\n                else:\n                    # unsuccessful => slightly expand to encourage escape\n                    trust_radius *= 1.05\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n\n            # adjust Levy probability and global params based on successes/stagnation\n            if successes > 0:\n                # reward exploitation: slightly reduce long-jump probability\n                p_levy = max(0.005, p_levy * (0.95 if successes > max(1, 0.15 * self.pop_size) else 0.98))\n                # encourage slightly smaller F and CR drift toward exploitation\n                F_mean = np.clip(F_mean * 0.99, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n            else:\n                # no successes: boost exploration\n                p_levy = min(0.5, p_levy * 1.08)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 1.01, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(40, self.dim * 4):\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                replace_idx = self.rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in replace_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    # evaluate new individual\n                    try:\n                        f_new = float(func(pop[j]))\n                    except Exception:\n                        f_new = np.inf\n                    fvals[j] = f_new\n                    evals += 1\n                    # update best if any\n                    if f_new < best_f:\n                        best_f = f_new\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # slightly increase levy chance to escape basin\n                p_levy = min(0.5, p_levy * 1.2)\n                # reduce stagnation counter to avoid immediate repeated resets\n                stagnation_counter = stagnation_counter // 2\n\n            # safety: keep trust radius bounded\n            trust_radius = max(min(trust_radius, max_trust), min_trust)\n\n            # optional: small random immigrant injection each generation to maintain diversity\n            if self.rng.rand() < 0.02 and evals < self.budget:\n                j = self.rng.randint(0, self.pop_size)\n                pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                try:\n                    f_new = float(func(pop[j]))\n                except Exception:\n                    f_new = np.inf\n                fvals[j] = f_new\n                evals += 1\n                if f_new < best_f:\n                    best_f = f_new\n                    best_x = pop[j].copy()\n                    stagnation_counter = 0\n\n            # small per-generation relaxation to ensure parameters remain stable\n            F_mean = np.clip(0.97 * F_mean + 0.03 * 0.6, 0.05, 0.99)\n            CR_mean = np.clip(0.97 * CR_mean + 0.03 * 0.9, 0.0, 1.0)\n\n        # finalize\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.287 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.23949866253374175, 0.1663770673274947, 0.32466599171948674, 0.3908279827558768, 0.34516664242448347, 0.38395822156393744, 0.29039792745772963, 0.25508361142338587, 0.2858925401205782, 0.18345652458561934]}, "task_prompt": ""}
{"id": "0b3ad62b-3443-455f-92fb-4a08d87d662d", "fitness": 0.22040491049086394, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DE + Lévy jumps + trust-region local search — global exploration via adaptive DE and occasional heavy-tailed Lévy jumps, focused refinement with an online-adaptive trust-region around the best.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history (simple jDE-like adaptation).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # Choose a reasonable default pop size if not provided\n        if pop_size is None:\n            # scale population with dimension, keeping bounds\n            self.pop_size = max(8, min(80, int(8 + 2.0 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n        # results placeholders\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalars, lists, or arrays. Broadcast to self.dim\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.copy()\n        # try to broadcast (e.g., shape (dim,1) or (1,dim))\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be interpreted for dimension %d\" % self.dim)\n\n    def __call__(self, func):\n        # get bounds if provided by func, otherwise default [-5,5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure lb < ub\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n\n        # evaluate as many as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i].copy()))\n            except Exception:\n                # if evaluation fails, keep inf\n                fvals[i] = np.inf\n            evals += 1\n\n        # decide best among evaluated\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals)\n            best_f = float(fvals[best_idx])\n            best_x = pop[best_idx].copy()\n        else:\n            # no evaluations possible (budget == 0)\n            self.f_opt = float(\"inf\")\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # mean differential weight\n        CR_mean = 0.9       # mean crossover prob\n        p_levy = 0.08       # initial probability of Lévy jump\n        trust_radius = 0.5  # relative fraction of range used for trust region\n        max_trust = 2.0\n        min_trust = 1e-6\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step(scale=1.0):\n            # Cauchy-distributed steps; clip extremes for numerical safety\n            step = rng.standard_cauchy(self.dim) * scale\n            # cap extreme outliers to avoid overflow\n            max_abs = 1e3\n            step = np.clip(step, -max_abs, max_abs)\n            return step\n\n        # main loop over generations until budget exhausted\n        # Note: we treat each candidate evaluation as consuming 1 eval\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # adapt per-generation small perturbation to means\n            # (keeps algorithm dynamic)\n            F_mean = np.clip(F_mean * (1.0 + rng.normal(0, 0.01)), 0.1, 1.0)\n            CR_mean = np.clip(CR_mean * (1.0 + rng.normal(0, 0.01)), 0.0, 1.0)\n\n            # shuffle order to avoid bias\n            order = rng.permutation(self.pop_size)\n\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[idx].copy()\n                f_target = fvals[idx]\n\n                # Sample individual-specific parameters (jDE-like)\n                Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # decide mutation strategy: mostly rand/1, sometimes best/1\n                if rng.rand() < 0.85:\n                    # DE/rand/1\n                    ids = rng.choice([j for j in range(self.pop_size) if j != idx], 3, replace=False)\n                    a, b, c = pop[ids[0]], pop[ids[1]], pop[ids[2]]\n                    donor = a + Fi * (b - c)\n                else:\n                    # DE/best/1 anchored at current best\n                    ids = rng.choice([j for j in range(self.pop_size) if j != idx], 2, replace=False)\n                    a, b = pop[ids[0]], pop[ids[1]]\n                    donor = best_x + Fi * (a - b)\n\n                # Occasional Lévy jump: centered on best to explore distant basins\n                if rng.rand() < p_levy:\n                    # scale Levy by trust_radius and problem range\n                    jump = levy_step(scale=0.2) * trust_radius\n                    donor = best_x + jump * range_vec\n\n                # binomial crossover\n                cr_mask = rng.rand(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[rng.randint(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, x_target)\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate\n                try:\n                    f_candidate = float(func(candidate.copy()))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < f_target:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n\n                    # adapt means slightly toward successful Fi/CRi\n                    F_mean = 0.95 * F_mean + 0.05 * Fi\n                    CR_mean = 0.92 * CR_mean + 0.08 * CRi\n                else:\n                    # failed trial: small random perturbation to maintain diversity\n                    if rng.rand() < 0.02:\n                        pop[idx] = np.clip(pop[idx] + rng.normal(0, 0.01, size=self.dim) * range_vec, lb, ub)\n\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # early stop if reached perfect solution (optional)\n                # Many BBOB functions have global minimum near zero; no hard check here.\n\n            # End of population loop -> generation finished\n\n            # Trust-region local search around best: small Gaussian sampling\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: small handful scaled by dim and remaining budget\n            local_samples = min(remaining, max(1, int(np.ceil(self.dim * 0.8))))\n            # anisotropic sigma: base trust_radius scaled by rand per-dimension\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                sigma = trust_radius * (0.5 + rng.rand(self.dim) * 0.5)  # per-dim factor in [0.5,1.0]\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * (sigma * range_vec)\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate.copy()))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # shrink trust region to focus search\n                    trust_radius *= 0.85\n                    trust_radius = np.clip(trust_radius, min_trust, max_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful -> small expansion to escape\n                    trust_radius *= 1.05\n                    trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n            # Adapt Lévy probability and global parameters based on success rate\n            if successes > max(1, int(0.15 * self.pop_size)):\n                # successful generation -> reduce exploration a bit\n                p_levy = max(0.005, p_levy * 0.95)\n                # encourage exploitation by slightly moving means toward conservative values\n                F_mean = np.clip(0.98 * F_mean + 0.02 * 0.5, 0.05, 1.0)\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.9, 0.0, 1.0)\n            else:\n                # less success -> slightly increase exploration\n                p_levy = min(0.5, p_levy * 1.02)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # stagnation handling: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 4):\n                k = max(1, self.pop_size // 2)\n                # choose indices to reinitialize (avoid overwriting best if possible)\n                candidates_idx = [i for i in range(self.pop_size) if not np.allclose(pop[i], best_x)]\n                if len(candidates_idx) == 0:\n                    candidates_idx = list(range(self.pop_size))\n                reinit_idx = rng.choice(candidates_idx, min(k, len(candidates_idx)), replace=False)\n                for j in reinit_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j].copy()))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    # if replacement improved global best\n                    if fvals[j] < best_f:\n                        best_f = float(fvals[j])\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # after reseeding, increase trust radius to promote exploration\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # slightly boost chance of Levy jumps\n                p_levy = min(0.6, p_levy * 1.2)\n                # reset stagnation counter modestly\n                stagnation_counter = stagnation_counter // 2\n\n            # safety tiny adjustments\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n            F_mean = np.clip(F_mean, 0.05, 1.0)\n            CR_mean = np.clip(CR_mean, 0.0, 1.0)\n\n            # Optional break if best_f is extremely small (practical stop)\n            # Not enforced to make sure budget used when needed.\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.220 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09673735478473933, 0.1638850192021667, 0.32113925231083496, 0.23565225378935162, 0.28720670232655354, 0.26353631626739704, 0.22209810899822202, 0.20450982944729157, 0.25962224974152415, 0.14966201804055856]}, "task_prompt": ""}
{"id": "dad5d0f1-bca6-4432-a6b3-17a7202fe61c", "fitness": 0.2424252417125893, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid heuristic combining Differential Evolution with jDE-like online parameter adaptation, occasional Lévy (Cauchy) long jumps for global exploration, and a trust-region Gaussian local search around the current best to focus exploitation — all controlled by simple adaptive rules and stagnation resets.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (mutation factor F and crossover CR) are adapted online.\n    Designed for continuous optimization in box bounds (default [-5,5]).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: scale with dim but never larger than a fraction of budget\n        if pop_size is None:\n            # typical small-to-medium DE population heuristic but bounded by budget\n            default = max(6, int(6 * np.sqrt(self.dim)))\n            self.pop_size = int(min(default, max(4, self.budget // 6)))\n        else:\n            self.pop_size = int(max(4, pop_size))\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        rng = self.rng\n\n        # determine bounds: try to use func.bounds if available, otherwise default [-5, 5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float).reshape(self.dim)\n            ub = np.asarray(func.bounds.ub, dtype=float).reshape(self.dim)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure correct shapes\n        lb = lb.ravel()\n        ub = ub.ravel()\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n        # evaluate initial population (until budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                fvals[i] = np.inf\n            evals += 1\n\n        # if nothing evaluated (budget==0), return trivial\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # if some individuals were not evaluated due to tiny budget, leave them with inf so they won't be used\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6  # mean mutation factor\n        CR_mean = 0.9  # mean crossover rate\n        p_levy = 0.07  # initial probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius (relative to range_norm)\n        trust_radius = step_scale * range_norm\n        min_trust = 1e-6\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        def levy_step():\n            # simple heavy-tailed step via Cauchy; clipped to avoid numerical blow-up\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -15.0, 15.0)\n            # normalize scale to moderate size (unit-ish)\n            scale = np.maximum(1e-12, np.mean(np.abs(s)))\n            return (s / scale)\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            idxs = np.arange(self.pop_size)\n\n            # per-generation adaptative jitter on parameter generation: small Gaussian around means\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi around means (jDE-like simplicity)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 0.99))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # decide exploration mode: Lévy jump centered on best OR classical DE mutation\n                if rng.random() < p_levy:\n                    # Levy jump around best_x: produces a long-range exploratory candidate\n                    step = levy_step()\n                    donor = best_x + step * (trust_radius / max(1e-12, range_norm)) * range_vec\n                else:\n                    # DE/rand/1 mutation variant\n                    # ensure indices exclude i\n                    pool = idxs[idxs != i]\n                    if pool.size < 3:\n                        # fallback: random perturbation if population too small\n                        donor = pop[i] + Fi * rng.normal(0, 1.0, size=self.dim) * (range_vec / max(1e-12, range_norm))\n                    else:\n                        r1, r2, r3 = rng.choice(pool, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cr_mask = rng.random(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[rng.integers(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate (only if budget allows)\n                try:\n                    f_candidate = float(func(candidate)) if evals < self.budget else np.inf\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # nudge parameter means toward successful Fi/CRi (if used)\n                    F_mean = float(np.clip(0.9 * F_mean + 0.1 * Fi, 0.05, 0.99))\n                    CR_mean = float(np.clip(0.9 * CR_mean + 0.1 * CRi, 0.0, 1.0))\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # end per-individual\n\n            # End of generation adjustments / trust-region local search around best\n            # sample a few candidates locally using anisotropic Gaussian noise scaled by trust_radius\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: small handful depending on dim but limited by remaining budget\n            num_local = int(min(max(1, self.dim // 3), remaining, 4 + self.dim // 4))\n            local_success = 0\n            for _ in range(num_local):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dimension to allow directional steps\n                sigma = (0.4 + rng.random(self.dim) * 0.6) * (trust_radius / max(1e-12, range_norm))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step -> tighten trust radius a bit to focus search\n                    trust_radius = max(trust_radius * 0.7, min_trust)\n                    local_success += 1\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful -> expand slightly to try to escape\n                    trust_radius = min(trust_radius * 1.05, max_trust)\n                    stagnation_counter += 1\n\n            # adapt exploration probability and DE means based on recent successes\n            if successes + local_success > 0:\n                # more successes => favor exploitation (reduce Levy chance slightly)\n                p_levy = max(0.01, p_levy * (0.95))\n                # nudge global means slightly toward exploitation defaults when successful\n                F_mean = float(np.clip(0.97 * F_mean + 0.03 * 0.55, 0.05, 0.99))\n                CR_mean = float(np.clip(0.97 * CR_mean + 0.03 * 0.9, 0.0, 1.0))\n            else:\n                # stagnation handling: increase chance of long jumps and diversity\n                p_levy = min(0.6, p_levy * 1.08)\n                F_mean = float(np.clip(0.98 * F_mean + 0.02 * 0.8, 0.05, 0.99))\n                CR_mean = float(np.clip(0.98 * CR_mean + 0.02 * 0.2, 0.0, 1.0))\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                # choose individuals to reinitialize\n                choose_idxs = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in choose_idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 1.8)\n                stagnation_counter = 0\n\n            # small stabilization of trust radius to keep it within bounds\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n        # final results\n        self.x_opt = best_x.copy()\n        self.f_opt = float(best_f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.242 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13035852116665536, 0.20059920250604046, 0.29803446668152944, 0.36213360898540703, 0.23147943042979324, 0.2938205888433021, 0.23590479141363274, 0.25336183127485734, 0.2380075679616812, 0.18055240786299387]}, "task_prompt": ""}
{"id": "96dabe68-c9c2-4c33-b835-a427b02afe48", "fitness": 0.19503750991176982, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid DE with occasional Lévy flights and a trust-region local search — combines population-based differential evolution, sporadic heavy-tailed jumps for long-range escape, and focused trust-region exploitation with online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (jDE-like per-individual F and CR adaptation),\n      - Occasional Lévy-like (Cauchy) jumps centered on best for long-range exploration,\n      - Trust-region local search around current best for focused exploitation.\n\n    __init__(self, budget, dim, pop_size=None, seed=None)\n    __call__(self, func) runs the optimizer using up to self.budget evaluations.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # sensible default population size depending on dimension (but not larger than budget)\n        if pop_size is None:\n            ps = int(min(60, max(8, 8 + int(2 * np.sqrt(self.dim)))))\n        else:\n            ps = int(pop_size)\n        self.pop_size = max(2, min(ps, self.budget))  # at least 2, not more than budget\n\n        # Results filled after run\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # broadcast lb/ub scalars to array of dimension self.dim\n        a = np.asarray(b)\n        if a.size == 1:\n            return np.full(self.dim, float(a))\n        if a.size == self.dim:\n            return a.astype(float)\n        # try to broadcast otherwise\n        return np.broadcast_to(a, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # get bounds as arrays\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_vec = np.where(range_vec == 0, 1.0, range_vec)  # avoid zero-range dims\n\n        # internal variables\n        pop_size = self.pop_size\n        dim = self.dim\n        rng = np.random.default_rng(self.seed)  # use local rng to allow reproducibility\n\n        # Initialize population uniformly inside bounds\n        X = lb + rng.random((pop_size, dim)) * range_vec\n\n        # Evaluate initial population subject to budget\n        fvals = np.full(pop_size, np.inf, dtype=float)\n        evals = 0\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(X[i]))\n            evals += 1\n\n        # If we could not fill population evaluations (tiny budget), keep the rest as inf\n        best_idx = int(np.argmin(fvals))\n        best_x = X[best_idx].copy()\n        best_f = fvals[best_idx] if np.isfinite(fvals[best_idx]) else np.inf\n\n        # Algorithm hyper-parameters (adapted online)\n        F_mean = 0.5\n        CR_mean = 0.5\n        p_levy = 0.08              # base probability to perform a Levy jump (heavy-tailed)\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # initial trust radius (scalar)\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        total_improvements = 0\n        gen = 0\n\n        # helper: Levy-like heavy-tailed step using Cauchy distribution, clipped\n        def levy_step(size):\n            # use standard Cauchy to get heavy tails, but clip out extreme outliers\n            s = rng.standard_cauchy(size=size)\n            # clip extremes for numerical stability\n            s = np.clip(s, -1e2, 1e2)\n            # normalize scale to typical magnitude ~1\n            median_abs = np.median(np.abs(s)) + 1e-12\n            return s / median_abs\n\n        # Main loop\n        # We'll iterate through generations until budget exhausted.\n        while evals < self.budget:\n            gen += 1\n            # per-generation bookkeeping\n            gen_improvements = 0\n            # Process population members sequentially (one evaluation usually per candidate)\n            for i in range(pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Decide exploration move: Levy jump or DE\n                if rng.random() < p_levy:\n                    # Levy jump centered on best_x for long-range exploration\n                    step = levy_step(dim)\n                    # scale step by trust radius and relative ranges\n                    candidate = best_x + step * (trust_radius / (np.linalg.norm(step) + 1e-12)) * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    # slight random mixing with the current individual to maintain diversity\n                    mix = rng.random()\n                    candidate = np.clip((1.0 - mix) * X[i] + mix * candidate, lb, ub)\n                    # record that no Fi/CR were used\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1 mutation with jDE-like adaptation (sample Fi and CRi)\n                    Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.2))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n                    # pick 3 distinct indices different from i\n                    idxs = list(range(pop_size))\n                    idxs.remove(i)\n                    a, b, c = rng.choice(idxs, size=3, replace=False)\n                    donor = X[a] + Fi * (X[b] - X[c])\n                    # binomial crossover\n                    mask = rng.random(dim) < CRi\n                    # ensure at least one dimension is taken from donor\n                    if not mask.any():\n                        mask[rng.integers(0, dim)] = True\n                    candidate = np.where(mask, donor, X[i])\n                    # small random perturbation to maintain exploration\n                    candidate = candidate + rng.normal(0, 1e-6, size=dim)\n\n                    # project to bounds\n                    candidate = np.clip(candidate, lb, ub)\n\n                # One evaluation (ensure not to exceed budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement (replace the target if better)\n                replaced = False\n                if f_candidate < fvals[i]:\n                    X[i] = candidate\n                    fvals[i] = f_candidate\n                    replaced = True\n                    gen_improvements += 1\n                    # move F_mean and CR_mean slightly toward used Fi/CRi if they exist\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # Update global best if needed\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful local contraction of trust radius\n                    trust_radius = max(min_trust, 0.85 * trust_radius)\n                    total_improvements += 1\n                else:\n                    # if this evaluation did not improve the global best, slightly expand trust to encourage escape\n                    trust_radius = min(max_trust, 1.01 * trust_radius)\n                    stagnation_counter += 1\n\n            # After processing a generation, do a focused trust-region local search around best_x\n            # Number of local samples depends on remaining budget and dimension\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = int(min(5, max(1, 3 + self.dim // 10), remaining))\n            # anisotropic sigma: use trust_radius scaled by per-dimension range\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # gaussian local step scaled by trust_radius and normalized by range\n                # use smaller magnitude in small-range dims\n                sigma = trust_radius / (np.linalg.norm(range_vec) + 1e-12)\n                candidate = best_x + rng.normal(0.0, sigma, size=dim) * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # shrink trust radius to focus further\n                    trust_radius = max(min_trust, 0.7 * trust_radius)\n                    stagnation_counter = 0\n                    total_improvements += 1\n                else:\n                    # unsuccessful local step -> slightly increase radius to try to escape\n                    trust_radius = min(max_trust, 1.05 * trust_radius)\n                    stagnation_counter += 1\n\n            # Online adaptation of p_levy based on stagnation and recent improvements\n            # If many improvements, reduce long jumps; if stagnating, increase them\n            if gen_improvements > 0:\n                # slight decrease in exploration probability\n                p_levy = max(0.02, p_levy * (0.99 if gen_improvements > 1 else 0.995))\n            else:\n                # no improvements, escalate probability of long jumps\n                p_levy = min(0.6, p_levy * 1.03)\n\n            # Stagnation handling: if no improvement for a while, re-seed part of the population\n            # Criteria: stagnation_counter large relative to dimension\n            stagnation_threshold = max(20, 8 * self.dim // max(1, pop_size))\n            if stagnation_counter > stagnation_threshold:\n                # reinitialize half of the population randomly (but keep best)\n                k = max(1, pop_size // 2)\n                idxs = np.arange(pop_size)\n                # avoid replacing the best index if present\n                # find index of current best in population (if present)\n                try:\n                    cur_best_idx = int(np.argmin(fvals))\n                except:\n                    cur_best_idx = None\n                to_replace = rng.choice([i for i in idxs if i != cur_best_idx], size=k, replace=False)\n                for j in to_replace:\n                    if evals >= self.budget:\n                        break\n                    X[j] = lb + rng.random(dim) * range_vec\n                    fvals[j] = float(func(X[j]))\n                    evals += 1\n                    # update global best if needed\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = X[j].copy()\n                        stagnation_counter = 0\n                # slightly enlarge trust radius after reset to allow exploration\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # reduce F_mean/CR_mean a bit to reset strategies\n                F_mean = 0.5 * F_mean + 0.25\n                CR_mean = 0.5 * CR_mean + 0.25\n                stagnation_counter = 0\n\n            # safety adjustments to keep sensible ranges\n            F_mean = np.clip(F_mean, 0.05, 1.2)\n            CR_mean = np.clip(CR_mean, 0.0, 1.0)\n            p_levy = float(np.clip(p_levy, 0.01, 0.6))\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # Small random restart if budget remains but population collapsed\n            if np.all(np.isclose(X, X[0], atol=1e-12)):\n                # diversify half population\n                k = pop_size // 2\n                for j in rng.choice(pop_size, size=k, replace=False):\n                    if evals >= self.budget:\n                        break\n                    X[j] = lb + rng.random(dim) * range_vec\n                    fvals[j] = float(func(X[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = X[j].copy()\n\n        # Finalize results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.195 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09304824773608333, 0.16156706054476566, 0.24790334523087487, 0.19813727647988866, 0.21054422705203935, 0.2342489036564669, 0.22837799108813273, 0.22196148891118994, 0.19096829247890657, 0.16361826593935014]}, "task_prompt": ""}
{"id": "8d0eedbc-7074-46f6-81da-87aa06c3059c", "fitness": 0.18142160283065506, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like online parameter adaptation, occasional Lévy (Cauchy) long jumps from the best, and a trust-region Gaussian local search around the best solution — balances fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy/Cauchy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history (jDE-like). Designed for\n    continuous black-box optimization in box [-5,5]^dim (or func.bounds).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n\n        # sensible default pop_size: proportional to dimension but bounded by budget\n        if pop_size is None:\n            pop_size = max(8, min(50, 6 * self.dim))\n        pop_size = int(pop_size)\n        # ensure not larger than budget/3 to avoid wasting evals on initialization\n        pop_size = min(pop_size, max(4, self.budget // 3))\n        self.pop_size = pop_size\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds: try to get from func, else use [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast bounds to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        # tiny safeguard\n        range_vec[range_vec <= 0] = 1.0\n\n        # internal state\n        pop = self.rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # jDE-like per-individual parameters\n        F = np.clip(self.rng.normal(0.6, 0.15, size=self.pop_size), 0.2, 1.0)\n        CR = np.clip(self.rng.uniform(0.2, 0.9, size=self.pop_size), 0.0, 1.0)\n        # population-wide means for smoothing\n        F_mean = 0.6\n        CR_mean = 0.5\n        tau_F = 0.1  # probability to change F\n        tau_CR = 0.1  # probability to change CR\n\n        # trust-region parameters (relative to range)\n        trust_radius = 0.25  # relative initial radius (fraction of range)\n        min_trust = 1e-3\n        max_trust = 2.0\n        # interpreted multiplicatively on range_vec\n        trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n        # levy/Cauchy jump probability baseline\n        levy_prob = 0.05\n\n        # stagnation\n        stagnation_counter = 0\n        best_idx = 0\n\n        # Evaluate initial population (or as much as budget allows)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n                best_idx = i\n                stagnation_counter = 0\n\n        # If no evaluations possible, return trivial\n        if evals == 0:\n            return self.f_opt, self.x_opt\n\n        # If some individuals remain unevaluated (because budget small), fill them with copies of best\n        if np.isinf(fvals).any():\n            # replace unevaluated individuals by random samples without evaluation\n            uneval = np.where(np.isinf(fvals))[0]\n            for i in uneval:\n                pop[i] = self.x_opt + self.rng.normal(0, 1e-6, size=self.dim)\n                fvals[i] = self.f_opt\n\n        generation = 0\n\n        # helper: levy-like step using Cauchy (heavy-tailed)\n        def levy_cauchy_step(scale=0.2):\n            # scale is relative to range_vec\n            step = self.rng.standard_cauchy(self.dim)\n            # clip extreme outliers to avoid numerical blow-ups but keep heavy tails\n            step = np.clip(step, -1e2, 1e2)\n            return step * scale\n\n        # main loop\n        while evals < self.budget:\n            generation += 1\n            # prepare for collecting successful Fi/CRi for adaptation (Lehmer-like)\n            succ_F = []\n            succ_CR = []\n            gen_improvements = 0\n\n            # process population sequentially (one DE trial per individual)\n            order = np.arange(self.pop_size)\n            self.rng.shuffle(order)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[idx].copy()\n                f_target = fvals[idx]\n\n                # adapt individual's F and CR occasionally (jDE)\n                if self.rng.random() < tau_F:\n                    Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.1, 1.0)\n                else:\n                    Fi = F[idx]\n                if self.rng.random() < tau_CR:\n                    CRi = np.clip(self.rng.normal(CR_mean, 0.2), 0.0, 1.0)\n                else:\n                    CRi = CR[idx]\n\n                # choose whether to attempt a Levy jump (global exploration) or DE mutation (exploitation)\n                use_levy = (self.rng.random() < levy_prob)\n\n                if use_levy:\n                    # jump centered at best for long-range exploration\n                    best_x = self.x_opt if self.x_opt is not None else pop[best_idx]\n                    # generate cauchy heavy-tail step, scaled by range and trust radius\n                    scale = (0.6 + self.rng.random() * 1.4)  # randomize scale factor\n                    step = levy_cauchy_step(scale=scale * trust_radius)\n                    trial = best_x + step * range_vec\n                    # minor local perturbation to mix\n                    trial += 0.05 * self.rng.normal(0, 1.0, size=self.dim) * range_vec\n                else:\n                    # DE/rand/1 or current-to-best/1 mixture\n                    # pick three distinct indices not equal to idx\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != idx]\n                    r = self.rng.choice(idxs, size=3, replace=False)\n                    r1, r2, r3 = r\n                    # mix rand/1 and current-to-best/1\n                    if self.rng.random() < 0.5:\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    else:\n                        best_x = self.x_opt if self.x_opt is not None else pop[best_idx]\n                        donor = x_target + Fi * (best_x - x_target) + Fi * (pop[r1] - pop[r2])\n\n                    # binomial crossover\n                    cross = self.rng.random(self.dim) < CRi\n                    # ensure at least one gene from donor\n                    if not np.any(cross):\n                        cross[self.rng.integers(0, self.dim)] = True\n                    trial = np.where(cross, donor, x_target)\n\n                # project to bounds (simple clipping)\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_trial = float(func(trial))\n                evals += 1\n\n                # Selection: greedy\n                if f_trial < f_target:\n                    pop[idx] = trial\n                    fvals[idx] = f_trial\n                    succ_F.append(Fi)\n                    succ_CR.append(CRi)\n                    gen_improvements += 1\n                    # move individual's F and CR toward successful used values\n                    F[idx] = Fi\n                    CR[idx] = CRi\n                    # update global best if improved\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        stagnation_counter = 0\n                        best_idx = idx\n                    else:\n                        stagnation_counter += 1\n                else:\n                    # unsuccessful, maybe slightly damp parameter\n                    F[idx] = 0.98 * F[idx] + 0.02 * (0.5)\n                    CR[idx] = 0.98 * CR[idx] + 0.02 * (0.5)\n                    stagnation_counter += 1\n\n                # occasional small local exploitation after a success: tiny local gaussian try\n                if gen_improvements > 0 and self.rng.random() < 0.05 and evals < self.budget:\n                    local_step = self.rng.normal(0, trust_radius * 0.3, size=self.dim) * range_vec\n                    local_candidate = self.x_opt + local_step\n                    local_candidate = np.minimum(np.maximum(local_candidate, lb), ub)\n                    f_local = float(func(local_candidate))\n                    evals += 1\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = local_candidate.copy()\n                        stagnation_counter = 0\n\n            # End of population loop (one generation)\n            # Adapt F_mean and CR_mean using successes\n            if len(succ_F) > 0:\n                # Lehmer-like mean encourages larger effective F\n                succ_F = np.array(succ_F)\n                F_mean = 0.9 * F_mean + 0.1 * (np.sum(succ_F**2) / np.sum(succ_F))\n            else:\n                # decay slightly to encourage exploration if no successes\n                F_mean = max(0.15, 0.99 * F_mean)\n\n            if len(succ_CR) > 0:\n                CR_mean = 0.9 * CR_mean + 0.1 * np.mean(succ_CR)\n            else:\n                CR_mean = 0.99 * CR_mean\n\n            # Trust-region local search around best: sample a few Gaussian candidates\n            remaining = self.budget - evals\n            # local samples scaled by dimension and remaining budget but small\n            local_samples = min(max(2, self.dim // 2), max(0, remaining))\n            local_samples = min(local_samples, 8)  # keep small to not exhaust budget\n            local_successes = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                sigma = trust_radius * (0.5 + self.rng.random() * 0.5)\n                candidate = self.x_opt + self.rng.normal(0, sigma, size=self.dim) * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    local_successes += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # Adjust trust radius based on local search success\n            if local_successes > 0 or gen_improvements > 0:\n                # focus: shrink trust region a bit when making progress\n                trust_radius *= 0.85\n            else:\n                # expand trust region a bit when failing to improve\n                trust_radius *= 1.05\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # Adjust levy probability based on stagnation\n            if stagnation_counter > max(20, self.dim * 2):\n                levy_prob = min(0.5, levy_prob + 0.05)\n            else:\n                levy_prob = max(0.01, levy_prob * 0.995)\n\n            # Strong stagnation reset: re-seed part of population\n            if stagnation_counter > max(80, self.dim * 8):\n                # reinitialize half of the population randomly\n                k = max(1, self.pop_size // 2)\n                idxs = self.rng.choice(np.arange(self.pop_size), size=k, replace=False)\n                for j in idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = self.rng.uniform(lb, ub)\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < self.f_opt:\n                        self.f_opt = fvals[j]\n                        self.x_opt = pop[j].copy()\n                        stagnation_counter = 0\n                # after reset, encourage exploration more\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                levy_prob = min(0.4, levy_prob + 0.1)\n                stagnation_counter = 0\n\n            # small diversification: if population collapsed, perturb some individuals\n            diversity = np.mean(np.std(pop, axis=0) / (range_vec + 1e-12))\n            if diversity < 1e-3:\n                # inject noise in population\n                for j in range(self.pop_size):\n                    pop[j] = pop[j] + self.rng.normal(0, 1e-3, size=self.dim) * range_vec\n                pop = np.minimum(np.maximum(pop, lb), ub)\n\n            # safety: re-compute best index\n            best_idx = int(np.argmin(fvals))\n            if fvals[best_idx] < self.f_opt:\n                self.f_opt = float(fvals[best_idx])\n                self.x_opt = pop[best_idx].copy()\n                stagnation_counter = 0\n\n            # small chance to replace worst individual by a perturbation of best\n            if self.rng.random() < 0.05 and evals < self.budget:\n                worst = int(np.argmax(fvals))\n                new_candidate = self.x_opt + self.rng.normal(0, 0.1 * trust_radius, size=self.dim) * range_vec\n                new_candidate = np.minimum(np.maximum(new_candidate, lb), ub)\n                fnew = float(func(new_candidate))\n                evals += 1\n                if fnew < fvals[worst]:\n                    pop[worst] = new_candidate\n                    fvals[worst] = fnew\n                    if fnew < self.f_opt:\n                        self.f_opt = fnew\n                        self.x_opt = new_candidate.copy()\n                        stagnation_counter = 0\n\n        # final return\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.181 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09170076158753004, 0.15405390344508096, 0.26020956464959755, 0.16453667101773006, 0.18833772070468113, 0.2719717757329736, 0.19654268537035502, 0.17678987924098444, 0.17128876610245602, 0.13878430045516155]}, "task_prompt": ""}
{"id": "b8f9673e-44a1-436f-ab88-5147691cf3c8", "fitness": 0.2754227139197688, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy-driven global jumps and a trust-region local search; online adapts step-sizes and jump probability to balance global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (mutation factor F, crossover CR, trust radius, Levy probability)\n    are adapted online using simple success-based rules.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=1)\n        f_opt, x_opt = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population size heuristic: between 6 and 30, scaled by dim but bounded by budget\n        if pop_size is None:\n            p = int(4 + 3 * np.log1p(self.dim))\n            p = max(p, min(10, 6 + self.dim // 5))\n            pop_size = int(min(max(6, p * 2), 60))\n        # ensure population is not larger than a sensible fraction of budget\n        pop_size = int(min(pop_size, max(2, self.budget // max(5, self.dim))))\n        self.pop_size = max(2, pop_size)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like, return array of length dim\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.copy()\n        # try flattening and broadcasting if possible\n        try:\n            return np.broadcast_to(b.reshape(1, -1), (self.dim,)).copy()\n        except Exception:\n            raise ValueError(\"Bounds must be scalar or length equal to dim\")\n\n    def __call__(self, func):\n        # Bounds from func\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        # Internal state\n        evals = 0\n        dim = self.dim\n        pop_size = self.pop_size\n\n        # initial hyper-parameters\n        F_mean = 0.6\n        CR_mean = 0.9\n        tau_F = 0.12   # probability to mutate Fi\n        tau_CR = 0.12  # probability to mutate CRi\n        p_levy = 0.05  # initial probability of Levy jump for each trial\n        trust_radius = 0.2 * np.linalg.norm(ub - lb) / np.sqrt(dim)  # initial trust radius (scalar)\n        min_trust = 1e-6 * np.linalg.norm(ub - lb)\n        max_trust = 2.0 * np.linalg.norm(ub - lb)\n\n        # initialize population uniformly\n        pop = rng.uniform(lb, ub, size=(pop_size, dim))\n        fvals = np.full(pop_size, np.inf, dtype=float)\n\n        # Evaluate initial population as budget allows\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = func(x)\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If budget very small and some individuals not evaluated, we keep them unevaluated (inf) and they won't be used\n        # main loop\n        stagnation_counter = 0\n        no_improve_evals = 0\n        consecutive_no_improve_limit = max(50, self.budget // 20)\n\n        # helper: Levy-like step via Cauchy (heavy tail), clipped to avoid blow-up\n        def levy_step():\n            # Cauchy samples with scale 1, clip to avoid extreme outliers, then scale by trust radius\n            s = rng.standard_cauchy(size=dim)\n            s = np.clip(s, -10.0, 10.0)\n            # scale relative to domain range and trust radius\n            scale = trust_radius / max(1e-12, np.linalg.norm(ub - lb))\n            return s * scale\n\n        # keep track of best individual index\n        if self.x_opt is None:\n            # if no evaluations happened (budget=0), return immediately\n            return self.f_opt, self.x_opt\n        best_idx = int(np.argmin(fvals))\n        best_x = self.x_opt.copy()\n        best_f = self.f_opt\n\n        # Main generational loop until budget exhausted\n        while evals < self.budget:\n            successes = 0\n            gen_improvements = 0\n\n            # adapt per-generation exploration intensity slightly\n            CR_mean = np.clip(CR_mean * (1.0 + 0.01 * (rng.rand() - 0.5)), 0.01, 0.99)\n\n            # For each individual sequentially\n            for i in range(pop_size):\n                if evals >= self.budget:\n                    break\n\n                # prepare per-individual parameters (jDE style)\n                if rng.rand() < tau_F:\n                    Fi = rng.uniform(0.1, 0.9)\n                else:\n                    Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 0.95)\n                if rng.rand() < tau_CR:\n                    CRi = rng.rand()\n                else:\n                    CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # Choose mutation strategy: either DE/rand/1 or best-guided or Levy jump\n                use_levy = (rng.rand() < p_levy)\n                if use_levy:\n                    # Levy jump centered near current best, heavy-tailed\n                    jump = levy_step()\n                    donor = best_x + jump * (ub - lb)\n                    # occasionally add a scaled differential for diversity\n                    r1, r2 = rng.choice(pop_size, size=2, replace=False)\n                    donor = donor + Fi * (pop[r1] - pop[r2])\n                else:\n                    # classical DE/rand/1 + occasional best guidance with small probability\n                    idxs = [idx for idx in range(pop_size) if idx != i]\n                    r1, r2, r3 = rng.choice(idxs, size=3, replace=False)\n                    # blend rand/1 and best/1\n                    if rng.rand() < 0.2:\n                        donor = pop[r1] + Fi * (best_x - pop[r2])\n                    else:\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                trial = pop[i].copy()\n                jrand = rng.randint(dim)\n                mask = rng.rand(dim) < CRi\n                mask[jrand] = True\n                trial[mask] = donor[mask]\n\n                # Projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # Evaluate candidate\n                if evals >= self.budget:\n                    break\n                f_trial = func(trial)\n                evals += 1\n\n                # Greedy selection\n                if f_trial <= fvals[i]:\n                    # success: replace\n                    pop[i] = trial\n                    fvals[i] = f_trial\n                    successes += 1\n                    gen_improvements += 1\n                    # move global means slightly toward successful params\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # If Levy used and successful, reduce Levy probability (we found good long jump)\n                    if use_levy:\n                        p_levy = max(0.005, p_levy * 0.7)\n                        # shrink trust region to focus exploitation\n                        trust_radius = max(min_trust, trust_radius * 0.8)\n                    else:\n                        # modest shrink on ordinary success\n                        trust_radius = max(min_trust, trust_radius * 0.95)\n                    no_improve_evals = 0\n                else:\n                    # failure: slightly promote exploration\n                    trust_radius = min(max_trust, trust_radius * 1.01)\n                    no_improve_evals += 1\n                    # slight nudge to encourage more Levy jumps if failing often\n                    p_levy = min(0.6, p_levy * (1.0 + 0.002))\n\n                # Update global best\n                if f_trial < best_f:\n                    best_f = f_trial\n                    best_x = trial.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    best_idx = i\n                    # when best improves drastically, slightly reduce p_levy and trust radius\n                    p_levy = max(0.002, p_levy * 0.85)\n                    trust_radius = max(min_trust, trust_radius * 0.9)\n\n                # occasional local random restart of single individual if stuck & budget available\n                if no_improve_evals > consecutive_no_improve_limit and rng.rand() < 0.02 and evals < self.budget:\n                    pop[i] = rng.uniform(lb, ub)\n                    # evaluate restarted individual\n                    fnew = func(pop[i].copy())\n                    evals += 1\n                    fvals[i] = fnew\n                    if fnew < best_f:\n                        best_f = fnew\n                        best_x = pop[i].copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                    no_improve_evals = 0\n\n            # End of generation adjustments\n\n            # Trust-region local search around best: sample a few localized candidates\n            if evals < self.budget:\n                # number of local samples depends on remaining budget and dimension\n                remaining = self.budget - evals\n                n_local = int(min(max(1, dim // 2), remaining, max(1, pop_size // 3)))\n                improved_local = False\n                for k in range(n_local):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic gaussian noise scaled by trust_radius\n                    noise = rng.normal(0.0, 1.0, size=dim)\n                    # per-dimension scaling to allow anisotropic steps\n                    scales = 1.0 + 0.5 * rng.rand(dim)\n                    step = noise * scales * (trust_radius / (np.sqrt(dim) + 1e-12))\n                    cand = np.minimum(np.maximum(best_x + step, lb), ub)\n                    f_cand = func(cand)\n                    evals += 1\n                    if f_cand < best_f:\n                        best_f = f_cand\n                        best_x = cand.copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        improved_local = True\n                        # also inject candidate into population replacing worst individual\n                        worst_idx = int(np.argmax(fvals))\n                        pop[worst_idx] = cand.copy()\n                        fvals[worst_idx] = f_cand\n                # adjust trust radius based on local search results\n                if improved_local:\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    p_levy = max(0.001, p_levy * 0.9)\n                    no_improve_evals = 0\n                else:\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n                    no_improve_evals += n_local\n\n            # Stagnation handling: if many evaluations without improvement, re-seed part of population\n            if no_improve_evals > max(200, self.budget // 20):\n                # reinitialize half of population (worst half) to diversify\n                worst_idxs = np.argsort(fvals)[- (pop_size // 2):]\n                for idx in worst_idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[idx] = rng.uniform(lb, ub)\n                    # evaluate new individual if we can\n                    fnew = func(pop[idx].copy())\n                    evals += 1\n                    fvals[idx] = fnew\n                    if fnew < best_f:\n                        best_f = fnew\n                        best_x = pop[idx].copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                # expand trust radius to encourage escape\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                p_levy = min(0.8, p_levy * 1.3)\n                no_improve_evals = 0\n\n            # minor damping to keep parameters bounded\n            F_mean = np.clip(F_mean, 0.05, 0.95)\n            CR_mean = np.clip(CR_mean, 0.0, 1.0)\n            p_levy = np.clip(p_levy, 0.0, 0.9)\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n            # quick exit if optimum is already (near zero) and budget nearly spent\n            # (can't know absolute optimum so we just continue until budget used)\n\n        # end while\n        # record final bests\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.275 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11997906320399354, 0.21610840227090622, 0.3922655267200268, 0.16208927987976962, 0.25791871038086955, 0.7409763502382847, 0.20129253239001133, 0.2752568444360137, 0.2170577714154911, 0.1712826582623218]}, "task_prompt": ""}
{"id": "0770dda9-6257-4d27-ab6d-5cf8ce6b27c4", "fitness": 0.17559944975086772, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style parameter adaptation, occasional Lévy-flight long jumps for exploration, and an adaptive trust-region local search around the current best solution.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line: Adaptive DE + occasional Lévy flights + trust-region local search\n    for robust global exploration and focused local exploitation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        rng = np.random.RandomState(seed)\n        self.rng = rng\n\n        # sensible default population size: scale with dimension but not exceed a fraction of budget\n        if pop_size is None:\n            p = max(4, int(8 + 2 * self.dim))  # simple scaling\n            # ensure population leaves room for local search and restarts\n            p = min(p, max(4, int(self.budget // (4 if self.dim<20 else 8))))\n            pop_size = p\n        self.pop_size = max(4, int(pop_size))\n\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_bounds(self, x, lb, ub):\n        # broadcast-clamp to bounds\n        x = np.asarray(x, dtype=float)\n        if x.ndim == 1:\n            return np.minimum(np.maximum(x, lb), ub)\n        else:\n            return np.minimum(np.maximum(x, lb[None, :]), ub[None, :])\n\n    def __call__(self, func):\n        # bounds\n        if hasattr(func, \"bounds\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        rng = self.rng\n        budget = int(self.budget)\n        remaining = budget\n\n        # initialize population uniformly in bounds\n        pop = rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        pop_f = np.full(self.pop_size, np.inf)\n\n        # evaluate as many initial individuals as budget allows\n        to_eval = min(self.pop_size, remaining)\n        for i in range(to_eval):\n            x = pop[i]\n            f = func(x)\n            remaining -= 1\n            pop_f[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n        # if we couldn't evaluate all individuals, leave them as random but unevaluated (f=inf)\n        # they still participate structurally.\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.35\n        tau_F = 0.1  # probability to re-sample F in jDE-like manner\n        tau_CR = 0.1\n        p_levy = 0.03  # base probability to perform a Levy-centered long jump\n        levy_increase_factor = 1.5\n        levy_decrease_factor = 0.9\n\n        trust_radius = 0.08 * np.linalg.norm(ub - lb)  # absolute scale\n        trust_min = 1e-6 * np.linalg.norm(ub - lb)\n        trust_max = np.linalg.norm(ub - lb) * 1.5\n\n        evals_since_impr = 0\n        stagn_threshold = max(50, 10 * self.dim)  # restart/rescue threshold\n\n        gen = 0\n        # small helper: generate Levy-like heavy-tailed step using Cauchy, clipped\n        def levy_step(scale=1.0):\n            # Cauchy provides heavy tails; clip extremes for numerical stability\n            step = rng.standard_cauchy(size=self.dim)\n            # scale per-dimension and clip\n            step = np.clip(step, -1e2, 1e2)  # avoid enormous jumps\n            return step * (scale / np.sqrt(self.dim))\n\n        # main loop: iterate generations until budget exhausted\n        while remaining > 0:\n            gen += 1\n            # adapt per-generation F and CR means might be nudged slowly\n            # We'll process population members sequentially; each evaluated trial consumes one eval.\n\n            # shuffle order to avoid bias\n            order = np.arange(self.pop_size)\n            rng.shuffle(order)\n            success_F = []\n            success_CR = []\n\n            for idx in order:\n                # generate individual-specific control parameters (jDE-like)\n                if rng.rand() < tau_F:\n                    Fi = rng.rand() * 0.9 + 0.05  # sample fresh F in (0.05, 0.95)\n                else:\n                    Fi = np.clip(rng.normal(F_mean, 0.1), 0.01, 1.2)\n\n                if rng.rand() < tau_CR:\n                    CRi = rng.rand()\n                else:\n                    CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                target = pop[idx].copy()\n\n                # choose three distinct indices for DE/rand/1 mutation\n                ids = np.arange(self.pop_size)\n                ids = ids[ids != idx]\n                if ids.size < 3:\n                    # fallback: random perturbation\n                    a = rng.randint(self.pop_size)\n                    b = rng.randint(self.pop_size)\n                    c = rng.randint(self.pop_size)\n                else:\n                    a, b, c = rng.choice(ids, 3, replace=False)\n\n                # compute base donor: DE/rand/1\n                donor = pop[a] + Fi * (pop[b] - pop[c])\n\n                # occasionally center a Levy jump on the best for exploration\n                if rng.rand() < p_levy:\n                    # center on global best (if exists) else on donor\n                    center = self.x_opt if self.x_opt is not None else donor\n                    # levy step scaled by trust radius combined with domain range\n                    scale = trust_radius / max(1e-12, np.linalg.norm(ub - lb))\n                    levy = levy_step(scale=scale)\n                    donor = center + levy * (ub - lb)\n                    # nudge Fi and CR to encourage exploration\n                    Fi = np.clip(Fi * (1.0 + 0.5 * rng.rand()), 0.01, 1.2)\n                    CRi = np.clip(CRi * (0.2 + 0.8 * rng.rand()), 0.0, 1.0)\n\n                # recombination: binomial crossover\n                jrand = rng.randint(self.dim)\n                cross = rng.rand(self.dim) < CRi\n                cross[jrand] = True  # ensure at least one gene from donor\n                trial = np.where(cross, donor, target)\n\n                # projection to bounds\n                trial = self._ensure_bounds(trial, lb, ub)\n\n                # evaluate trial if budget allows\n                if remaining <= 0:\n                    break\n                f_trial = func(trial)\n                remaining -= 1\n\n                # selection: greedy replacement\n                # if target was never evaluated (pop_f[idx] == inf), treat as replaceable\n                replaced = False\n                if f_trial < pop_f[idx]:\n                    pop[idx] = trial\n                    pop_f[idx] = f_trial\n                    replaced = True\n                    # adapt means toward successful parameters (exponential smoothing)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n\n                # update global best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                    evals_since_impr = 0\n                    # shrinking trust radius on improvement to focus search\n                    trust_radius = max(trust_min, trust_radius * 0.92)\n                else:\n                    evals_since_impr += 1\n\n                # subtle adaptation of p_levy: if many trial successes, reduce long jumps\n                if len(success_F) > max(3, 0.2 * self.pop_size):\n                    p_levy = max(0.005, p_levy * levy_decrease_factor)\n                else:\n                    # if few successes, slightly increase chance of long jumps\n                    p_levy = min(0.5, p_levy * levy_increase_factor)\n\n                if remaining <= 0:\n                    break\n\n            # End of generation adjustments\n\n            # Trust-region local search around best: sample a few candidates with Gaussian noise\n            if remaining > 0 and self.x_opt is not None:\n                # sample count: small handful scaled by dim and remaining budget\n                max_local = min(8 + self.dim // 2, remaining)\n                n_local = max(1, int(min(max_local, max(1, self.dim // 2))))\n                # reduce n_local if budget low\n                n_local = min(n_local, remaining)\n\n                local_success = False\n                for _ in range(n_local):\n                    # anisotropic sigma: base trust_radius scaled per-dimension\n                    sigma = trust_radius * (0.5 + rng.rand(self.dim) * 1.5)\n                    cand = self.x_opt + rng.normal(0, sigma)\n                    cand = self._ensure_bounds(cand, lb, ub)\n\n                    if remaining <= 0:\n                        break\n                    f_cand = func(cand)\n                    remaining -= 1\n\n                    if f_cand < self.f_opt:\n                        self.f_opt = f_cand\n                        self.x_opt = cand.copy()\n                        local_success = True\n                        evals_since_impr = 0\n                        # shrink trust radius to intensify local search\n                        trust_radius = max(trust_min, trust_radius * 0.85)\n                    else:\n                        # small expansion on failure to escape local basins\n                        trust_radius = min(trust_max, trust_radius * 1.02)\n\n                    # incorporate good local candidate into population (replace worst)\n                    worst_idx = np.argmax(pop_f)\n                    if f_cand < pop_f[worst_idx]:\n                        pop[worst_idx] = cand\n                        pop_f[worst_idx] = f_cand\n\n                # if many local successes, encourage exploitation (slightly decrease F_mean)\n                if local_success:\n                    F_mean = max(0.05, F_mean * 0.97)\n                    CR_mean = min(0.99, CR_mean * 1.02)\n                else:\n                    # if local attempts failed, broaden trust radius a bit\n                    trust_radius = min(trust_max, trust_radius * 1.05)\n\n            # stagnation handling: if no improvement for many evals, increase exploration\n            if evals_since_impr > stagn_threshold:\n                # re-seed half of population to diversify (do not evaluate them to save budget)\n                n_reseed = max(1, self.pop_size // 2)\n                re_idx = rng.choice(np.arange(self.pop_size), n_reseed, replace=False)\n                pop[re_idx] = rng.uniform(lb, ub, size=(n_reseed, self.dim))\n                pop_f[re_idx] = np.inf  # unevaluated\n                # slightly enlarge trust radius to escape current basin\n                trust_radius = min(trust_max, trust_radius * 2.0)\n                # increase levy probability temporarily\n                p_levy = min(0.5, p_levy * 2.0)\n                evals_since_impr = 0  # give time to work\n\n            # safety break: if remaining evaluations are too small to make progress break after updating best\n            if remaining <= 0:\n                break\n\n            # small generation counter limit to avoid infinite loops in pathological wrappers\n            if gen > 20000:\n                break\n\n        # final results\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.176 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09315299341238803, 0.15234095954251203, 0.23065381843482446, 0.17873959918529891, 0.1925843455375431, 0.20162517452321083, 0.20363604189404783, 0.1856458998929741, 0.18550531201026155, 0.13211035307561614]}, "task_prompt": ""}
{"id": "78809598-84b4-4e04-94eb-98909ea4f8ff", "fitness": 0.21621444572979537, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online parameter adaptation for robust exploration-exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but bounded by budget fraction\n        if pop_size is None:\n            ps = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            ps = min(ps, max(4, self.budget // 20))\n            self.pop_size = int(ps)\n        else:\n            self.pop_size = int(pop_size)\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # quick exit if no budget\n        if self.budget <= 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # safety: if the user gave degenerate bounds, provide tiny range\n        range_vec = np.where(range_vec == 0.0, 1e-12, range_vec)\n\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                # if evaluation fails, keep inf\n                fvals[i] = np.inf\n            evals += 1\n\n        # find best among evaluated individuals\n        finite_mask = np.isfinite(fvals)\n        if not np.any(finite_mask):\n            # no evaluations succeeded / budget too small\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals[finite_mask])\n        best_indices = np.where(finite_mask)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius fraction\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius (in absolute units)\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        gen = 0\n        stagnation_counter = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # normalize so that typical scale ~1\n            s = s / (np.median(np.abs(s)) + 1e-12)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # per-generation variation of F and CR around their means\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                Fi = None\n                CRi = None\n\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    s = levy_step()\n                    # scale relative to range and trust radius\n                    scale = step_scale * (0.5 + rng.rand() * 1.5) * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                    donor = best_x + s * scale * range_vec\n                    # occasionally add a random direction offset\n                    if rng.rand() < 0.2:\n                        donor += (rng.rand(self.dim) - 0.5) * 0.1 * range_vec\n                    trial = donor\n                else:\n                    # DE/rand/1 mutation + binomial crossover\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback random\n                        r1 = r2 = r3 = idxs[0] if idxs.size > 0 else i\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n                    mutant = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, mutant, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget remains\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt means toward successful Fi/CRi if available\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    else:\n                        # small nudge toward exploration default if Levy produced success\n                        F_mean = 0.98 * F_mean + 0.02 * 0.7\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    else:\n                        CR_mean = 0.98 * CR_mean + 0.02 * 0.8\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / (np.maximum(np.linalg.norm(range_vec), 1e-12)))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand slightly to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter += 1\n\n            # adapt probability of Levy jumps based on success rate\n            if successes > 0:\n                # more successes -> slightly reduce exploration probability\n                p_levy = max(0.01, p_levy * (0.95 if successes > max(1, self.pop_size * 0.2) else 0.98))\n            else:\n                # no successes -> increase exploration chance a bit\n                p_levy = min(0.5, p_levy * 1.05 + 0.005)\n\n            # gently nudge F_mean/CR_mean toward sensible defaults to avoid collapse\n            F_mean = float(np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99))\n            CR_mean = float(np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0))\n\n            # stagnation handling: re-seed part of population if no improvement for a while\n            if stagnation_counter > max(50, self.dim * 5) and evals < self.budget:\n                k = max(1, self.pop_size // 2)\n                indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.216 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10926341441689391, 0.16515589468449465, 0.2745281412702374, 0.22319262797375006, 0.2195717777607401, 0.3189758050197211, 0.23850066413477067, 0.22160029611125165, 0.2410125965484553, 0.15034323937763916]}, "task_prompt": ""}
{"id": "c9b11692-29e6-4096-ac6d-da8299c15146", "fitness": 0.31679241565580873, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like parameter adaptation, occasional Lévy-flight long jumps for escape, and a trust-region local search around the current best — all controlled online by success history and a strict evaluation budget.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history. Works with a fixed\n    function evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # Determine a reasonable population size if not provided\n        if pop_size is None:\n            # scale with dimension but keep limited relative to budget\n            p = max(8, int(8 + 2 * np.sqrt(self.dim)))\n            p = min(p, max(4, self.budget // 20))\n            self.pop_size = int(p)\n        else:\n            self.pop_size = int(pop_size)\n            self.pop_size = max(4, self.pop_size)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Ensure a length-dim numpy array for bounds; allow scalar or array-like input.\n        if b is None:\n            return None\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.copy().astype(float)\n        # try broadcasting to dim if possible\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            # fallback: use first element filled\n            return np.full(self.dim, float(b.ravel()[0]), dtype=float)\n\n    def __call__(self, func):\n        # Get bounds from func if available; otherwise default [-5, 5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        else:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure sensible bounds\n        range_vec = ub - lb\n        # avoid degenerate zero ranges\n        eps = 1e-12\n        range_mean = max(range_vec.mean(), eps)\n        range_norm = max(np.linalg.norm(range_vec), eps)\n\n        rng = self.rng\n\n        # initialize population\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population while respecting budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if no evaluations were possible, return immediately\n        finite_mask = np.isfinite(fvals)\n        if not np.any(finite_mask):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals[finite_mask])\n        best_indices = np.where(finite_mask)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * range_norm\n        min_trust = 1e-6\n        max_trust = max(1e-6, 0.75 * range_norm)\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # precompute index pool for sampling\n            idxs_all = np.arange(self.pop_size)\n\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                Fi = None\n                CRi = None\n\n                # decide between Levy-centered exploration or DE mutation\n                if rng.rand() < p_levy:\n                    # Lévy jump centered at the best with scaled step\n                    step = levy_step()\n                    scale = step_scale * (0.5 + 0.5 * rng.rand())\n                    donor = best_x + scale * step * (range_vec / range_mean)\n                    # trial vector is donor projected to bounds\n                    candidate = np.clip(donor, lb, ub)\n                else:\n                    # DE/rand/1 mutation with jDE-like per-individual parameter sampling\n                    choices = idxs_all[idxs_all != i]\n                    if choices.size < 3:\n                        # degenerate small population, fallback to random sample in bounds\n                        candidate = lb + rng.rand(self.dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(choices, 3, replace=False)\n                        Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n                        mutant = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover to form trial\n                        cr_mask = rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[rng.randint(0, self.dim)] = True\n                        candidate = np.where(cr_mask, mutant, pop[i])\n                        candidate = np.clip(candidate, lb, ub)\n\n                # evaluate trial (if budget available)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement into population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # nudge parameter means toward the used Fi/CRi if they exist (DE branch)\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # trust-region local search around the current best (small focused sampling)\n            remaining = self.budget - evals\n            local_samples = int(max(1, min(5, remaining, 3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: per-dimension fraction of trust_radius\n                sigma = (0.3 + rng.rand(self.dim) * 0.7) * (trust_radius / max(range_norm, eps))\n                # gaussian perturbation scaled by dimension ranges\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus search\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    trust_radius = min(trust_radius, max_trust)\n                    stagnation_counter = 0\n                    successes += 1\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    trust_radius = max(trust_radius, min_trust)\n\n            # online adaptation of global strategy parameters\n            if successes > 0:\n                # less need for Levy jumps if successful\n                p_levy = max(0.01, p_levy * (0.95 if successes > self.pop_size * 0.2 else 0.98))\n                # gently reinforce CR toward exploitation\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                # small decay of F_mean to avoid runaway\n                F_mean = np.clip(0.98 * F_mean + 0.02 * 0.6, 0.05, 1.0)\n            else:\n                # stagnation handling: increase chance of long jumps and lower CR to encourage exploration\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n                F_mean = np.clip(min(1.0, F_mean * 1.01), 0.05, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                reseed_idxs = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reseed_idxs:\n                    if evals >= self.budget:\n                        break\n                    # reinitialize this individual\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # encourage wider search afterwards\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # slightly increase Levy chance to escape valleys\n                p_levy = min(0.5, p_levy + 0.05)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.317 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16530477178217418, 0.23772756786639293, 0.3491431156166417, 0.436934243971164, 0.2941972751902585, 0.597208474885442, 0.2638967659996213, 0.24149163070916035, 0.2910384075705935, 0.2909819029666383]}, "task_prompt": ""}
{"id": "c7f9e0d1-4dfb-42cb-85c9-37b29951ddaa", "fitness": 0.2273297942873226, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range escapes, and a trust-region local search with online step-size adaptation and simple success-history updates.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent successes.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size scaled with dimension\n        default_pop = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n        self.pop_size = int(pop_size) if pop_size is not None else default_pop\n        # ensure population is not absurdly large compared to budget\n        self.pop_size = max(2, min(self.pop_size, max(2, self.budget // 5), self.budget))\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # ensure bounds are arrays of length dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (as budget allows)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # find initial best\n        finite_mask = np.isfinite(fvals)\n        if finite_mask.any():\n            best_idx = np.argmin(fvals)\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # no initial evaluations possible (budget == 0), return defaults\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.15       # probability of performing a Levy jump\n        step_scale = 0.35   # base scale for Lévy step relative to trust_radius\n        trust_radius = 0.25 # initial trust radius (fraction of full range)\n        max_trust = 2.0\n        min_trust = 1e-8\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy-like sampling\n        def levy_step():\n            # produce a heavy-tailed normalized direction\n            # use component-wise Cauchy (tan(pi*(u-0.5))) but stabilize extremes\n            u = rng.rand(self.dim)\n            s = np.tan(np.pi * (u - 0.5))\n            # clamp extremes to avoid numerical blow-up\n            max_abs = 1e6\n            s = np.clip(s, -max_abs, max_abs)\n            # random radial scale with heavy tail (inverse power)\n            radial = (rng.rand() ** (-1.0 / (1.0 + self.dim))) * (0.5 + rng.rand())\n            s = s / (np.linalg.norm(s) + 1e-12) * radial\n            return s\n\n        # main loop: generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            successful_Fs = []\n            successful_CRs = []\n\n            idxs_all = np.arange(self.pop_size)\n\n            # process targets sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual control parameters (jDE-like)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # choose exploration move: Levy jump vs DE mutation\n                if rng.rand() < p_levy:\n                    # Levy-centered jump around best_x\n                    s = levy_step()\n                    donor = best_x + s * (step_scale * trust_radius) * range_vec\n                    # binomial crossover with target\n                    mask = rng.rand(self.dim) < CRi\n                    # ensure at least one component from donor\n                    jrand = rng.randint(self.dim)\n                    mask[jrand] = True\n                    trial = np.where(mask, donor, pop[i])\n                else:\n                    # DE/rand/1 mutation + binomial crossover\n                    candidates = idxs_all[idxs_all != i]\n                    if candidates.size < 3:\n                        # fallback to random perturbation\n                        donor = pop[i] + Fi * rng.randn(self.dim) * trust_radius * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(candidates, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    mask = rng.rand(self.dim) < CRi\n                    jrand = rng.randint(self.dim)\n                    mask[jrand] = True\n                    trial = np.where(mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # one evaluation (only if budget allows)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    successful_Fs.append(Fi)\n                    successful_CRs.append(CRi)\n\n                # update global best and stagnation\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # generation-level parameter adaptation using success-history\n            if successful_Fs:\n                # Lehmer mean for F (gives higher weight to larger successful F)\n                fs = np.array(successful_Fs)\n                lehmer = (np.sum(fs * fs) / (np.sum(fs) + 1e-12))\n                F_mean = 0.9 * F_mean + 0.1 * float(np.clip(lehmer, 0.05, 1.0))\n                # arithmetic mean for CR\n                CR_mean = 0.9 * CR_mean + 0.1 * float(np.clip(np.mean(successful_CRs), 0.0, 1.0))\n\n            # adapt levy probability depending on success: increase if stagnating\n            if successes == 0:\n                p_levy = min(0.9, p_levy * 1.15)\n            else:\n                p_levy = max(0.01, p_levy * (0.97 if successes > self.pop_size * 0.25 else 0.985))\n\n            # trust-region local search around best: a few anisotropic Gaussian samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            local_improved = False\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma scaled by trust_radius and normalized by sqrt(dim)\n                sigma = trust_radius * (0.4 + rng.rand() * 0.6) / np.sqrt(max(1, self.dim))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_improved = True\n                    # shrink trust radius to focus search\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    stagnation_counter = 0\n                else:\n                    # mild expansion to attempt escape\n                    trust_radius = min(max_trust, trust_radius * 1.04)\n                    stagnation_counter += 1\n\n            # additional heuristic: if local_improved moderately often, slightly reduce exploration\n            if local_improved:\n                p_levy = max(0.01, p_levy * 0.9)\n                F_mean = np.clip(F_mean * 0.99, 0.05, 0.99)\n\n            # stagnation handling: re-seed half population if stuck\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                reinit_idx = rng.choice(self.pop_size, k, replace=False)\n                for j in reinit_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius to escape\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.227 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11653543372936048, 0.1529157704691414, 0.33091363045702704, 0.3744889971714871, 0.2072663104826582, 0.24476820091864093, 0.21178025147442803, 0.19743833251661214, 0.28675515774770155, 0.1504358579061691]}, "task_prompt": ""}
{"id": "4f3591e8-a3f3-4b3d-a8a8-0630487d699d", "fitness": 0.2048011421407412, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like parameter adaptation, occasional Lévy-flight long jumps, and a shrinking/expanding trust-region local search around the current best — balances fast global exploration with focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n      - Population-based Differential Evolution with per-individual F/CR adaptation (jDE-style).\n      - Occasional Lévy/Cauchy long jumps centered on best to escape basins.\n      - Trust-region local Gaussian search around the best candidate; trust radius adapts.\n      - Stagnation-driven partial re-seeding to diversify.\n    Intended for continuous bounded optimization; respects exact evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # population size default adapted to dimensionality but capped\n        if pop_size is None:\n            pop_size = int(max(8, min(60, 8 + 2 * np.sqrt(self.dim))))\n        self.pop_size = int(min(pop_size, max(4, self.budget // 10)))\n\n        # internal best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.shape[0] == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def _levy_like(self):\n        # Simple heavy-tailed step: clipped Cauchy (practical and robust)\n        step = self.rng.standard_cauchy(self.dim)\n        # clip extreme outliers but keep heavy tails\n        np.clip(step, -1e3, 1e3, out=step)\n        return step\n\n    def __call__(self, func):\n        # Extract bounds and make arrays of correct shape\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            # trivial case: zero range\n            x0 = lb.copy()\n            f0 = float(func(x0))\n            self.f_opt = f0\n            self.x_opt = x0\n            return self.f_opt, self.x_opt\n\n        # Initialize population uniformly in bounds\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # If we got no evaluations, return inf\n        if not np.isfinite(fvals).any():\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # set current best\n        best_idx = int(np.nanargmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n\n        # Algorithm hyper-parameters (adapted online)\n        p_levy = 0.08\n        trust_radius = 0.2 * range_norm  # absolute scalar radius\n        min_trust = 1e-8\n        max_trust = range_norm * 2.0\n\n        # jDE-like means for F and CR\n        F_mean = 0.6\n        CR_mean = 0.5\n        tau_F = 0.1\n        tau_CR = 0.1\n\n        stagnation_counter = 0\n        gen = 0\n\n        # bookkeeping for adaptive memory of successful Fi/CRi\n        success_F = []\n        success_CR = []\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # For each target vector in the population\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual control parameters (jDE idea)\n                if self.rng.random() < tau_F:\n                    Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 0.99)\n                else:\n                    Fi = F_mean\n                if self.rng.random() < tau_CR:\n                    CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                else:\n                    CRi = CR_mean\n\n                # choose strategy: levy-centered long jump with prob p_levy, otherwise DE/current-to-best/1\n                if self.rng.random() < p_levy:\n                    # Lévy-like jump centered on best with magnitude scaled by trust_radius and range\n                    levy_step = self._levy_like()\n                    # scale: combine absolute trust radius and a fraction of range_vec\n                    scale = (0.5 * trust_radius / (range_norm + 1e-12)) + 0.1 * self.rng.random()\n                    candidate = best_x + levy_step * scale * range_vec\n                else:\n                    # DE/current-to-best/1 mutation: v = x_i + F*(best - x_i) + F*(xr1 - xr2)\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r1, r2 = self.rng.choice(idxs, size=2, replace=False)\n                    donor = pop[i] + Fi * (best_x - pop[i]) + Fi * (pop[r1] - pop[r2])\n\n                    # binomial crossover\n                    cross_mask = self.rng.random(self.dim) < CRi\n                    # ensure at least one dimension is from donor\n                    if not cross_mask.any():\n                        cross_mask[self.rng.integers(0, self.dim)] = True\n                    candidate = np.where(cross_mask, donor, pop[i])\n\n                # Project candidate to bounds (simple clipping)\n                candidate = np.clip(candidate, lb, ub)\n\n                # Evaluate candidate\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection (greedy)\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # record success parameters to update means later\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n\n                    # update best if needed\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                else:\n                    stagnation_counter += 1\n\n                # early break if out of budget\n                if evals >= self.budget:\n                    break\n\n            # End of generation: adapt F_mean and CR_mean using successes (Lehmer mean style)\n            if success_F:\n                # Lehmer mean (gives more weight to larger F)\n                num = sum((f**2 for f in success_F))\n                den = sum(success_F)\n                if den > 0:\n                    F_mean = np.clip(num / den, 0.05, 0.99)\n                success_F = []\n            else:\n                # slightly decay to encourage exploration\n                F_mean = np.clip(0.99 * F_mean + 0.01 * 0.6, 0.05, 0.99)\n\n            if success_CR:\n                CR_mean = np.clip(np.mean(success_CR), 0.0, 1.0)\n                success_CR = []\n            else:\n                CR_mean = np.clip(0.99 * CR_mean + 0.01 * 0.5, 0.0, 1.0)\n\n            # Trust-region local search around best: use remaining budget sensibly\n            remaining = self.budget - evals\n            # sample a few local candidates (at most a small multiple of dim)\n            local_budget = min(remaining, max(0, min(self.dim * 2, max(1, remaining // 10))))\n            # we will try up to local_budget samples but stop if budget runs out\n            local_success = False\n            for _ in range(local_budget):\n                if evals >= self.budget:\n                    break\n                # anisotropic per-dimension sigma scaled by trust radius fraction\n                sigma_rel = (0.3 + 0.7 * self.rng.random(self.dim)) * (trust_radius / (range_norm + 1e-12))\n                candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * sigma_rel * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    local_success = True\n                    # shrink trust region to refine\n                    trust_radius = max(min_trust, trust_radius * 0.6)\n                    # adjust means slightly towards exploitation-friendly values\n                    F_mean = np.clip(0.95 * F_mean + 0.05 * 0.4, 0.05, 0.99)\n                    CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful local sample -> small expansion\n                    trust_radius = min(max_trust, trust_radius * 1.02)\n\n            # Adjust p_levy based on recent success/stagnation\n            if successes + (1 if local_success else 0) > max(1, 0.15 * self.pop_size):\n                # many successes: favor exploitation\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                # few successes: increase chance of long jumps\n                p_levy = min(0.5, p_levy * 1.05 + 0.01)\n\n            # On long stagnation, perform partial re-seed to diversify\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population (except best)\n                k = max(1, self.pop_size // 2)\n                candidates_idx = [idx for idx in range(self.pop_size) if idx != best_idx]\n                if len(candidates_idx) > 0:\n                    reinit_idx = self.rng.choice(candidates_idx, size=min(k, len(candidates_idx)), replace=False)\n                    for j in reinit_idx:\n                        if evals >= self.budget:\n                            break\n                        pop[j] = lb + self.rng.random(self.dim) * range_vec\n                        # evaluate new individuals if budget allows\n                        f_new = float(func(pop[j]))\n                        evals += 1\n                        fvals[j] = f_new\n                        if f_new < best_f:\n                            best_f = f_new\n                            best_x = pop[j].copy()\n                            self.f_opt = best_f\n                            self.x_opt = best_x.copy()\n                # enlarge trust to escape basin\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n            # update global best index after generation (in case best moved)\n            best_idx = int(np.nanargmin(fvals))\n            # safety: ensure best_x and best_f are consistent\n            if fvals[best_idx] < best_f:\n                best_f = float(fvals[best_idx])\n                best_x = pop[best_idx].copy()\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n        # final results\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.205 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10919412190828803, 0.16400042878751464, 0.2706030094191043, 0.15018535915380193, 0.23754611735753361, 0.29416984268711976, 0.24381753441005316, 0.2091714130053336, 0.21275170384041053, 0.15657189083825218]}, "task_prompt": ""}
{"id": "0e3a0568-e5e0-4a26-bebf-7a1a415eb82d", "fitness": 0.3442808122786363, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DE + Lévy jumps + trust-region local search — combines jDE-style adaptive DE, occasional heavy-tailed Lévy-like jumps, and an adaptive trust-region local refinement around the best solution.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (jDE-like per-individual parameter adaptation),\n      - occasional heavy-tailed Lévy-like (Cauchy) jumps for long-range exploration,\n      - trust-region Gaussian local search around the current best for exploitation.\n\n    Designed for continuous bounded optimization problems (works with BBOB-like functions).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # default population size scales with dimension but kept reasonable\n            self.pop_size = int(np.clip(8 + 2 * np.sqrt(self.dim), 8, 60))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        # determine bounds: try func.bounds, else default [-5, 5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n\n        # ensure shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        # small safety\n        ub = np.maximum(ub, lb + 1e-12)\n\n        range_vec = ub - lb\n        norm_range = np.linalg.norm(range_vec) + 1e-12\n\n        # initialize population\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate as many as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if nothing evaluated, return\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # track current best\n        valid_idx = np.argmin(fvals)\n        best_f = fvals[valid_idx]\n        best_x = pop[valid_idx].copy()\n\n        # jDE-like per-individual parameters\n        F = np.clip(0.5 + 0.1 * rng.standard_normal(self.pop_size), 0.1, 0.9)\n        CR = np.clip(0.9 + 0.05 * rng.standard_normal(self.pop_size), 0.0, 1.0)\n\n        # adaptation hyper-parameters\n        tau1 = 0.1\n        tau2 = 0.1\n        p_best = 0.2            # probability to use current-to-best style\n        p_levy = 0.08           # initial probability of performing a Lévy-like jump\n        step_scale = 0.25\n        trust_radius = 0.2 * norm_range\n        min_trust = 1e-6\n        max_trust = norm_range\n\n        stagnation_counter = 0\n        gen = 0\n\n        # main loop: generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # iterate through population (sequential evaluation)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # jDE-like control parameter updates (per-individual proposal)\n                if rng.random() < tau1:\n                    Fi = 0.1 + 0.8 * rng.random()  # in [0.1,0.9]\n                else:\n                    Fi = F[i]\n                if rng.random() < tau2:\n                    CRi = rng.random()\n                else:\n                    CRi = CR[i]\n\n                # decide whether to do a Lévy-like jump\n                do_levy = (rng.random() < p_levy)\n\n                if do_levy:\n                    # Lévy-like step using truncated Cauchy (heavy-tailed)\n                    raw_step = rng.standard_cauchy(self.dim)\n                    raw_step = np.clip(raw_step, -1e3, 1e3)\n                    # scale relative to trust radius and global range\n                    scale = step_scale * (trust_radius / (norm_range))\n                    candidate = best_x + scale * raw_step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                else:\n                    # Differential Evolution style mutation\n                    idxs = np.delete(np.arange(self.pop_size), i)\n                    if idxs.size >= 3:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    else:\n                        # degenerate: pick random points in bounds if not enough individuals\n                        r1 = r2 = r3 = i\n\n                    if rng.random() < p_best:\n                        # current-to-best variant\n                        donor = pop[i] + Fi * (best_x - pop[i]) + Fi * (pop[r1] - pop[r2])\n                    else:\n                        # rand/1 variant\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    jrand = rng.integers(0, self.dim)\n                    mask = rng.random(self.dim) < CRi\n                    mask[jrand] = True\n                    candidate = np.where(mask, donor, pop[i])\n\n                # project to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # one evaluation (if budget allows)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    F[i] = Fi\n                    CR[i] = CRi\n                    # update global best\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # small safety: avoid extremely small trust radius collapse\n                trust_radius = max(trust_radius, min_trust)\n\n            # generation-level adaptation\n            if successes > 0:\n                # encourage exploitation: slightly reduce chance of Lévy jumps and shrink trust region\n                p_levy = max(0.01, p_levy * (0.95 if successes > max(1, self.pop_size * 0.15) else 0.98))\n                trust_radius = max(min_trust, trust_radius * 0.92)\n            else:\n                # stagnation: increase exploration probability and expand trust region a bit\n                p_levy = min(0.5, p_levy * 1.07)\n                trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # Trust-region local search around current best: a few Gaussian perturbations\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dimension but roughly scaled by trust_radius\n                sigma = (0.4 + 0.6 * rng.random(self.dim)) * (trust_radius / (np.sqrt(self.dim) + 1e-12))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    trust_radius = max(min_trust, trust_radius * 0.85)  # focus\n                    stagnation_counter = 0\n                else:\n                    trust_radius = min(max_trust, trust_radius * 1.03)  # explore a little\n\n            # strong stagnation reset: re-seed some individuals if no improvement for a while\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                re_indices = rng.choice(self.pop_size, k, replace=False)\n                for j in re_indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n            # conservative per-generation dampening of parameters to avoid extremes\n            F = np.clip(F, 0.05, 1.0)\n            CR = np.clip(CR, 0.0, 1.0)\n            p_levy = np.clip(p_levy, 0.01, 0.5)\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n        # finished budget or loop exit\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.344 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11802027359018841, 0.18536084274690845, 0.40487621918668226, 0.46842702392473834, 0.2562656571078, 0.8613712210813503, 0.26474042728850156, 0.2177199035666192, 0.5042539272324565, 0.16177262706111784]}, "task_prompt": ""}
{"id": "d7410b04-2743-4e76-a8f4-fd48855b4d2f", "fitness": 0.23694776293129177, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range escapes, and an adaptive trust-region local search with online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size scaled with dimension and budget\n        if pop_size is None:\n            # at least 10, scale with dim, but not exceed a small fraction of budget\n            self.pop_size = int(min(max(10, 4 * self.dim), max(10, self.budget // 8)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # store result placeholders\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        a = np.asarray(b, dtype=float)\n        if a.shape == ():\n            return np.full(self.dim, float(a))\n        if a.ndim == 1 and a.size == self.dim:\n            return a.astype(float)\n        # try broadcasting\n        return np.broadcast_to(a, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # get bounds from func (BBOB style)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_mean = max(range_vec.mean(), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population as far as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # if we couldn't evaluate any individual, return a safe fallback\n        if np.all(np.isinf(fvals)):\n            self.f_opt = np.inf\n            self.x_opt = lb.copy()\n            return self.f_opt, self.x_opt\n\n        # set initial best\n        best_idx = int(np.nanargmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover mean\n        min_trust = 1e-8\n        trust_radius = 0.2 * np.linalg.norm(range_vec) / max(1.0, np.sqrt(self.dim))\n        max_trust = max(1e-8, np.linalg.norm(range_vec))\n\n        stagnation_counter = 0\n\n        # Main loop: generations until budget exhausted\n        while evals < self.budget:\n            successes = 0\n            F_success = []\n            CR_success = []\n\n            # probability of doing a Lévy jump increases with stagnation\n            p_levy = float(min(0.35, 0.02 + 0.02 * stagnation_counter))\n\n            # Process every member (sequentially consume evaluations)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual strategy parameters (jDE-like)\n                Fi = float(np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                use_levy = (self.rng.random() < p_levy)\n                if use_levy:\n                    # Lévy-like via Cauchy (heavy tails); clipped to avoid numerical blow-up\n                    step = self.rng.standard_cauchy(self.dim)\n                    step = np.clip(step, -1e3, 1e3)\n                    scale = (0.5 + 0.5 * self.rng.random()) * trust_radius\n                    donor = best_x + scale * step * (range_vec / range_mean)\n                else:\n                    # DE/rand/1 or DE/best/1 hybrid\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # not enough individuals -> random perturbation\n                        donor = pop[i] + Fi * (self.rng.random(self.dim) - 0.5)\n                    else:\n                        r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                        if self.rng.random() < 0.2:\n                            # biased towards best occasionally (DE/best/1)\n                            donor = pop[r1] + Fi * (best_x - pop[r1])\n                        else:\n                            donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cr_mask = self.rng.random(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[self.rng.integers(0, self.dim)] = True\n                trial = pop[i].copy()\n                trial[cr_mask] = donor[cr_mask]\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    F_success.append(Fi)\n                    CR_success.append(CRi)\n\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        stagnation_counter = 0  # reset stagnation on global improvement\n\n            # End of generation: adapt DE parameter means toward successful ones\n            if len(F_success) > 0:\n                F_mean = 0.9 * F_mean + 0.1 * float(np.mean(F_success))\n            if len(CR_success) > 0:\n                CR_mean = 0.9 * CR_mean + 0.1 * float(np.mean(CR_success))\n\n            # Trust-region local search around best: sample a few local candidates\n            remaining = max(0, self.budget - evals)\n            if remaining <= 0:\n                break\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            any_local_success = False\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                anisotropic = (0.5 + self.rng.random(self.dim) * 0.5)\n                sigma = anisotropic * (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + self.rng.normal(0.0, 1.0, self.dim) * sigma * (range_vec / range_mean)\n                candidate = np.clip(candidate, lb, ub)\n\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    any_local_success = True\n                    stagnation_counter = 0\n\n            # Adjust trust radius: shrink on success to focus, expand on failure to escape\n            if any_local_success or successes > 0:\n                trust_radius = max(min_trust, trust_radius * 0.7)\n            else:\n                trust_radius = min(max_trust, trust_radius * 1.15)\n\n            # Stagnation bookkeeping\n            if (successes == 0) and (not any_local_success):\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n\n            # Strong stagnation reset: diversify by reinitializing half the population\n            if stagnation_counter > max(20, self.dim * 2):\n                n_reset = max(1, self.pop_size // 2)\n                slots = self.rng.choice(self.pop_size, n_reset, replace=False)\n                for j in slots:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        stagnation_counter = 0\n                # after reseeding, relax trust region and slightly increase exploration\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                F_mean = max(0.1, F_mean * 0.9)\n                CR_mean = min(0.95, CR_mean * 1.05)\n                stagnation_counter = 0\n\n            # safety: ensure we always have best recorded\n            self.f_opt = float(best_f)\n            self.x_opt = best_x.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.237 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11640706643325638, 0.17399757742198763, 0.3195003268157649, 0.1721214941485667, 0.2199228559359261, 0.3568202019665958, 0.3129423619782066, 0.2981800876648264, 0.20808862340189693, 0.1914970335458902]}, "task_prompt": ""}
{"id": "81c1f958-649e-433f-bf6c-2993e3c98685", "fitness": 0.46092597583711353, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy-led long jumps and a trust-region local search that adapts step sizes and control parameters online for fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy-like) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (F and CR and jump probability) are adapted online from success history.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # Search bounds (problem statement: every function in [-5, 5])\n        self.lb = -5.0\n        self.ub = 5.0\n        self.range = self.ub - self.lb\n\n        # population size: scale with dim but never exceed a fraction of budget\n        if pop_size is None:\n            # baseline around 8-12 per dimension but limited by budget\n            guess = max(4, int(min(12 * self.dim, max(4, self.budget // max(3, self.dim)))))\n            self.pop_size = min(guess, max(4, self.budget))\n        else:\n            self.pop_size = int(pop_size)\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _levy_step(self):\n        # Use Cauchy-like heavy-tailed step (standard Cauchy): allows long jumps\n        # Truncate extreme outliers to avoid numerical blow-up.\n        step = self.rng.standard_cauchy(self.dim)\n        # limit extremely large values but keep heavy tail behaviour\n        thresh = 50.0  # in scaled units\n        step = np.clip(step, -thresh, thresh)\n        return step\n\n    def __call__(self, func):\n        # quick exit for zero budget\n        if self.budget <= 0:\n            return self.f_opt, self.x_opt\n\n        lb = np.full(self.dim, self.lb)\n        ub = np.full(self.dim, self.ub)\n        range_vec = ub - lb\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n\n        # Evaluate initial population until budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = func(x)\n            fvals[i] = float(f)\n            evals += 1\n\n        # If we couldn't evaluate anyone due to tiny budget, return best known (inf)\n        if evals == 0:\n            return self.f_opt, self.x_opt\n\n        # best so far\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n\n        # Algorithm hyperparameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.3       # crossover prob mean\n        p_levy = 0.08       # probability of a Levy jump per individual\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        # trust radius expressed in fraction of range (0..~1)\n        trust_radius = 0.1 * np.linalg.norm(range_vec)  # initial trust radius (absolute)\n        max_stagn = max(50, self.dim * 5)\n\n        stagnation_counter = 0\n        best_since_reset = 0\n\n        # adaptation rates\n        tau_F = 0.1\n        tau_CR = 0.1\n\n        # generation loop (process until budget exhausted)\n        while evals < self.budget:\n            gen_improvements = 0\n\n            # adapt per-generation randomness around means\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi in jDE style\n                if self.rng.random() < tau_F:\n                    Fi = 0.1 + 0.9 * self.rng.random()\n                else:\n                    Fi = F_mean\n                if self.rng.random() < tau_CR:\n                    CRi = self.rng.random()\n                else:\n                    CRi = CR_mean\n\n                # choose donor\n                if self.rng.random() < p_levy:\n                    # Lévy-style jump centered on best (long range exploration)\n                    step = self._levy_step()\n                    # scale step to the problem range, modulated by trust_radius\n                    step_scale = (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                    donor = best_x + (0.5 + 0.5 * self.rng.random()) * step_scale * step * (range_vec / np.maximum(np.mean(range_vec), 1e-12))\n                    # occasionally mix with a random individual to add diversity\n                    other = pop[self.rng.integers(0, self.pop_size)]\n                    if self.rng.random() < 0.5:\n                        donor = 0.6 * donor + 0.4 * other\n                    # donor may be far, but will be clipped later\n                else:\n                    # DE/rand/1 mutation (classic)\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    a, b, c = self.rng.choice(idxs, size=3, replace=False)\n                    donor = pop[a] + Fi * (pop[b] - pop[c])\n\n                # binomial crossover\n                crossover = self.rng.random(self.dim) < CRi\n                # ensure at least one dimension crosses\n                if not np.any(crossover):\n                    crossover[self.rng.integers(0, self.dim)] = True\n                trial = np.where(crossover, donor, pop[i])\n\n                # project to bounds\n                trial = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                f_candidate = None\n                if evals < self.budget:\n                    f_candidate = float(func(trial))\n                    evals += 1\n                else:\n                    break\n\n                # selection\n                if f_candidate < fvals[i]:\n                    pop[i] = trial\n                    fvals[i] = f_candidate\n                    gen_improvements += 1\n                    # nudge parameter means toward values that worked\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = trial.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    best_since_reset += 1\n                    stagnation_counter = 0\n                # else increment stagnation later\n\n            # End of population loop (one generation)\n            if gen_improvements == 0:\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n\n            # trust-region local search around best:\n            # use budget-aware number of local samples\n            remaining = self.budget - evals\n            # take a small handful, scaled by dim but not exceed remaining\n            local_samples = int(min(max(1, self.dim // 2), remaining, 10))\n            local_successes = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian noise scaled by trust radius\n                sigma = (0.2 + 0.8 * self.rng.random(self.dim)) * (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + sigma * self.rng.standard_normal(self.dim)\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    local_successes += 1\n                    # successful local step -> shrink trust region to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                else:\n                    # unsuccessful local step -> slightly expand trust region to escape\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # after local search adjust overall behavior\n            if local_successes > 0:\n                # favor exploitation a bit: reduce p_levy, tighten F_mean toward moderate\n                p_levy = max(0.01, p_levy * (0.9 ** local_successes))\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.5, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.5, 0.0, 0.99)\n            else:\n                # stagnation handling: gradually increase chance of long jumps and diversity\n                p_levy = min(0.5, p_levy * 1.03)\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max_stagn:\n                # reinitialize half of the population randomly to diversify\n                half = max(1, self.pop_size // 2)\n                to_replace = self.rng.choice(self.pop_size, size=half, replace=False)\n                for j in to_replace:\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    if evals < self.budget:\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                        if fvals[j] < best_f:\n                            best_f = fvals[j]\n                            best_x = pop[j].copy()\n                            self.f_opt = best_f\n                            self.x_opt = best_x.copy()\n                # slightly enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                best_since_reset = 0\n                # also increase p_levy for a while\n                p_levy = min(0.5, p_levy * 1.5)\n\n            # safety clamps\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n            F_mean = np.clip(F_mean, 0.05, 0.99)\n            CR_mean = np.clip(CR_mean, 0.0, 0.99)\n            p_levy = np.clip(p_levy, 0.01, 0.6)\n\n        # final outputs\n        return float(self.f_opt), self.x_opt.copy() if self.x_opt is not None else None", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.461 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1402991294632382, 0.1811307871385367, 0.5031834809969695, 0.8305375566725587, 0.5852627183986959, 0.7401074857167412, 0.2881161990786999, 0.5732271429837895, 0.5123995068936936, 0.2549957510282118]}, "task_prompt": ""}
{"id": "eca20181-a6b8-4b9c-bb0d-f38016bddbb2", "fitness": 0.6193987342099243, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid: Differential Evolution with occasional Lévy jumps and an online trust-region local search (adaptive per-individual F/CR, stagnation resets) — balances fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line idea: Combine adaptive jDE-style DE for global search with rare heavy-tailed Lévy jumps\n    centered on the best and an adaptive trust-region local search around the best; adapt F/CR\n    online and re-seed on stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # default population is moderate, scaled with dim, but never exceed budget\n        default_pop = max(6, min(40, 6 + 2 * self.dim))\n        if pop_size is None:\n            pop_size = default_pop\n        self.pop_size = int(max(4, min(pop_size, self.budget)))  # at least 4, no larger than budget\n        self.seed = seed\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # Determine bounds from func if available, otherwise use [-5, 5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb))\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub))\n        except Exception:\n            lb = -5.0 * np.ones(self.dim, dtype=float)\n            ub = 5.0 * np.ones(self.dim, dtype=float)\n\n        # ensure correct shapes\n        if lb.shape != (self.dim,):\n            lb = np.resize(lb, (self.dim,))\n        if ub.shape != (self.dim,):\n            ub = np.resize(ub, (self.dim,))\n\n        range_vec = ub - lb\n        avg_range = np.maximum(np.mean(range_vec), 1e-12)\n\n        # initialize population uniformly\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate initial population sequentially (without exceeding budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].astype(float)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = float(np.inf)\n            fvals[i] = f\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If budget was too small to evaluate all, reduce effective population\n        if np.isinf(fvals).all():\n            # we couldn't evaluate anybody; fallback to random probing until budget exhausted\n            while evals < self.budget:\n                x = lb + rng.rand(self.dim) * range_vec\n                f = float(func(x))\n                evals += 1\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n            return self.f_opt, self.x_opt\n\n        # If some members are unevaluated, replace them randomly with evaluated ones to keep indices valid\n        for i in range(self.pop_size):\n            if np.isinf(fvals[i]):\n                # pick a random evaluated individual\n                j = rng.choice(np.where(np.isfinite(fvals))[0])\n                pop[i] = pop[j].copy()\n                fvals[i] = fvals[j]\n\n        # adaptation hyper-parameters\n        F_mean = 0.5\n        CR_mean = 0.5\n        p_levy = 0.06  # base probability of a Lévy jump each offspring\n        trust_radius = 0.2 * avg_range  # absolute scale for trust-region sigma\n        min_trust = 1e-6\n        stagnation_counter = 0\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # statistics for adaptation (Lehmer-like)\n        success_Fs = []\n        success_CRs = []\n\n        gen = 0\n        max_stagnation = max(20 * self.dim, 100)\n\n        def levy_step():\n            # Simple heavy-tailed step using scaled Cauchy (approximate Lévy behavior).\n            # Clip extremes to avoid numeric overflow. Return vector of size dim.\n            s = rng.standard_cauchy(size=self.dim)\n            s = np.clip(s, -1e2, 1e2)\n            # scale down by some quantile to keep steps reasonable yet heavy-tailed\n            med = np.median(np.abs(s)) + 1e-12\n            s = s / med\n            # small random scaling\n            s = s * (0.5 + rng.rand(self.dim) * 2.0)\n            return s\n\n        # main loop: produce offspring sequentially until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            success_in_gen = 0\n            # shuffle order each generation for fairness\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[idx].copy()\n                # sample individual Fi and CRi using jDE-like adaptation (Cauchy for F, normal for CR)\n                # Fi drawn from Cauchy centered at F_mean, clipped to reasonable range\n                Fi = F_mean + 0.1 * rng.standard_cauchy()\n                Fi = float(np.clip(Fi, 0.05, 0.99))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # Decide whether to use Lévy jump for this offspring\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best to encourage global exploration from best so far\n                    step = levy_step()\n                    # scale by dynamic trust_radius and range_vec\n                    scaled = step * ((0.5 + 0.5 * rng.rand()) * (trust_radius / np.maximum(avg_range, 1e-12)))\n                    donor = best_x + scaled * (range_vec / np.maximum(avg_range, 1e-12))\n                    # small perturbation by DE-style difference to keep variety\n                    # pick two random distinct indices\n                    a, b = rng.choice([j for j in range(self.pop_size) if j != idx], size=2, replace=False)\n                    donor = donor + Fi * (pop[a] - pop[b])\n                else:\n                    # Standard DE/rand/1 mutation: pick r1,r2,r3 distinct from idx\n                    choices = [j for j in range(self.pop_size) if j != idx]\n                    r1, r2, r3 = rng.choice(choices, size=3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cross_mask = rng.rand(self.dim) < CRi\n                # ensure at least one dimension chosen\n                if not np.any(cross_mask):\n                    cross_mask[rng.randint(self.dim)] = True\n                candidate = np.where(cross_mask, donor, x_target)\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # Evaluate candidate\n                try:\n                    f_cand = float(func(candidate))\n                except Exception:\n                    f_cand = float(np.inf)\n                evals += 1\n\n                # Selection (greedy)\n                if f_cand <= fvals[idx]:\n                    # success\n                    pop[idx] = candidate\n                    fvals[idx] = f_cand\n                    success_in_gen += 1\n                    stagnation_counter = 0\n                    success_Fs.append(Fi)\n                    success_CRs.append(CRi)\n                    # nudge adaptation means towards successful Fi/CRi (soft update)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # shrink trust radius slightly on success to focus\n                    trust_radius *= 0.98\n                    trust_radius = max(trust_radius, min_trust)\n                else:\n                    # failure: small expansion to encourage escape\n                    trust_radius *= 1.001\n\n                # update global best\n                if f_cand < best_f:\n                    best_f = f_cand\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                # end offspring\n\n            # Generation-level adaptation\n            # If there were successes, move means toward successful values using Lehmer-like average\n            if success_Fs:\n                # Lehmer mean favors larger F that succeeded\n                F_num = sum((f ** 2 for f in success_Fs))\n                F_den = sum(success_Fs) + 1e-12\n                F_lehmer = F_num / F_den\n                F_mean = float(np.clip(0.85 * F_mean + 0.15 * F_lehmer, 0.05, 0.99))\n\n            if success_CRs:\n                CR_mean = float(np.clip(0.85 * CR_mean + 0.15 * (sum(success_CRs) / len(success_CRs)), 0.0, 1.0))\n\n            # adjust probability of Lévy jumps based on stagnation: more jumps when stagnating\n            if success_in_gen == 0:\n                stagnation_counter += 1\n                p_levy = min(0.5, p_levy * 1.05 + 0.001)\n            else:\n                p_levy = max(0.02, p_levy * 0.98)\n\n            # trust-region local search around best: sample a small batch, using remaining budget\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # choose number of local samples adaptively\n            num_local = int(min(max(1, self.dim // 2), remaining, 8))\n            # sigma anisotropic per-dim\n            for k in range(num_local):\n                if evals >= self.budget:\n                    break\n                sigma_vec = (0.4 + rng.rand(self.dim) * 0.6) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                try:\n                    f_cand = float(func(candidate))\n                except Exception:\n                    f_cand = float(np.inf)\n                evals += 1\n                if f_cand < best_f:\n                    best_f = f_cand\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    # successful local step: shrink trust region to focus\n                    trust_radius *= 0.7\n                    trust_radius = max(trust_radius, min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful local step => slightly expand to escape\n                    trust_radius *= 1.05\n                # end local sample\n\n            # If stagnation prolonged, perform partial re-seed\n            if stagnation_counter > max_stagnation:\n                # re-seed half the population randomly (but evaluate them as budget allows)\n                n_reseed = max(1, self.pop_size // 2)\n                indices = rng.choice(self.pop_size, size=n_reseed, replace=False)\n                for ii in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[ii] = lb + rng.rand(self.dim) * range_vec\n                    try:\n                        f_new = float(func(pop[ii]))\n                    except Exception:\n                        f_new = float(np.inf)\n                    fvals[ii] = f_new\n                    evals += 1\n                    if f_new < best_f:\n                        best_f = f_new\n                        best_x = pop[ii].copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                # after reseed, encourage exploration\n                p_levy = min(0.5, p_levy * 1.3)\n                trust_radius = min(np.max(range_vec) * 0.5, trust_radius * 1.5)\n                stagnation_counter = 0\n\n            # lightweight bounds on adaptation to avoid runaway\n            F_mean = float(np.clip(F_mean, 0.05, 0.99))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n            trust_radius = float(np.clip(trust_radius, min_trust, np.max(range_vec) * 2.0))\n\n            # occasional gentle diversification each few generations if too many duplicates\n            if gen % 10 == 0:\n                # measure diversity (pairwise distances from best)\n                dists = np.linalg.norm(pop - best_x[None, :], axis=1)\n                if np.median(dists) < 1e-6 * np.maximum(np.linalg.norm(range_vec), 1.0):\n                    # diversify lightly\n                    for i in rng.choice(self.pop_size, size=min(self.pop_size, 3), replace=False):\n                        if evals >= self.budget:\n                            break\n                        pop[i] = lb + rng.rand(self.dim) * range_vec\n                        try:\n                            fvals[i] = float(func(pop[i]))\n                        except Exception:\n                            fvals[i] = float(np.inf)\n                        evals += 1\n                        if fvals[i] < best_f:\n                            best_f = fvals[i]\n                            best_x = pop[i].copy()\n                            self.f_opt = best_f\n                            self.x_opt = best_x.copy()\n\n            # reset per-gen success lists\n            success_Fs.clear()\n            success_CRs.clear()\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.619 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15935060262646816, 0.15551093966723883, 0.7346011660058009, 0.9621178304924566, 0.9234536965934821, 0.9425945282131828, 0.31929162333883765, 0.7565359301474284, 0.8731885567392281, 0.3673424682751196]}, "task_prompt": ""}
{"id": "94371c7f-825b-4aaa-a0f8-53bade19947e", "fitness": 0.3412064689649462, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like self-adaptation, occasional Lévy-flight global jumps (Cauchy-based) and an online trust-region Gaussian local search around the current best; balances exploration and exploitation with stagnation-driven resets.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # default population size heuristic (kept reasonable relative to budget)\n        if pop_size is None:\n            # small dims => small pop; larger dims => larger pop but capped\n            pop_size_guess = int(min(max(6, 6 + int(2 * np.sqrt(self.dim))), 60))\n            # also ensure it's not more than a reasonable fraction of budget\n            pop_size = int(min(pop_size_guess, max(4, self.budget // 10)))\n        self.pop_size = max(4, int(pop_size))\n        # internal final results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # try to use function provided bounds if available, otherwise default [-5, 5]\n        if hasattr(func, \"bounds\"):\n            b = func.bounds\n            if hasattr(b, \"lb\") and hasattr(b, \"ub\"):\n                lb = np.asarray(b.lb, dtype=float)\n                ub = np.asarray(b.ub, dtype=float)\n            elif isinstance(b, (list, tuple)) and len(b) == 2:\n                lb, ub = np.asarray(b[0], float), np.asarray(b[1], float)\n            else:\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # ensure shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def _clip(self, x, lb, ub):\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def __call__(self, func):\n        # Get bounds and range\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # Evaluate as many as budget permits initially\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fv = float(func(pop[i].copy()))\n            except Exception:\n                fv = np.inf\n            fvals[i] = fv\n            evals += 1\n\n        # If no evaluations possible, return trivial\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            self.f_opt = np.inf\n            self.x_opt = pop[0].copy()\n            return self.f_opt, self.x_opt\n\n        best_idx = int(np.nanargmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n        self.f_opt, self.x_opt = best_f, best_x.copy()\n\n        # jDE-like parameter arrays per individual\n        Fi = np.clip(0.5 + 0.1 * self.rng.randn(self.pop_size), 0.1, 0.9)   # scaling factors\n        CRi = np.clip(0.2 + 0.3 * self.rng.rand(self.pop_size), 0.0, 1.0)    # crossover probabilities\n\n        # global means for gentle nudging\n        F_mean = 0.5\n        CR_mean = 0.2\n\n        # levy probability and trust-region radius\n        p_levy = 0.08\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # initial trust radius scaled to problem\n        trust_min = 1e-6 * np.linalg.norm(range_vec)\n        trust_max = np.linalg.norm(range_vec) * 2.0\n\n        gen = 0\n        stagnation_counter = 0\n        no_improve_evals = 0\n\n        def levy_step(scale=0.2):\n            # Cauchy-like heavy tail (Levy-like simple approximation)\n            # returns vector in R^dim\n            # scale relative to range; clip extreme outliers\n            raw = self.rng.standard_cauchy(size=self.dim)\n            # trim extremely large values to avoid blow-ups\n            raw = np.clip(raw, -1e2, 1e2)\n            step = raw * (scale * range_vec)\n            return step\n\n        # main loop: we iterate until eval budget exhausted\n        # we'll proceed generation-wise but still count exact evals\n        while evals < self.budget:\n            gen += 1\n            # adapt per-generation randomization of F and CR around their means (weak)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n                target = pop[i].copy()\n\n                # jDE style: occasionally resample Fi, CRi\n                if self.rng.rand() < 0.1:\n                    Fi[i] = np.clip(F_mean * (0.7 + 0.6 * self.rng.rand()), 0.05, 0.95)\n                if self.rng.rand() < 0.1:\n                    CRi[i] = np.clip(CR_mean + 0.3 * (self.rng.rand() - 0.5), 0.0, 1.0)\n\n                Fi_i = Fi[i]\n                CR_i = CRi[i]\n\n                # With some probability perform a Lévy jump candidate centered on best\n                if self.rng.rand() < p_levy:\n                    # global exploratory jump\n                    cand = best_x + levy_step(scale=0.4 * (1.0 + gen / 50.0))\n                    # small mutation around candidate for variability\n                    cand += self.rng.randn(self.dim) * (0.05 * trust_radius)\n                else:\n                    # DE/rand/1-like mutation with occasional best guidance\n                    # pick 3 distinct indices different from i\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = self.rng.choice(idxs, 3, replace=False)\n                    # mutation: rand/1 + a small pull toward best with weight depending on trust radius\n                    pull = (best_x - pop[i]) * (0.05 * np.clip(np.linalg.norm(trust_radius) / (1e-12 + np.linalg.norm(range_vec)), 0.0, 1.0))\n                    donor = pop[a] + Fi_i * (pop[b] - pop[c]) + pull\n                    # crossover (binomial)\n                    jrand = self.rng.randint(self.dim)\n                    mask = (self.rng.rand(self.dim) < CR_i)\n                    mask[jrand] = True\n                    cand = np.where(mask, donor, target)\n\n                # Bound handling: simple reflection and clip\n                # reflect overshoot once, then clip\n                over_low = cand < lb\n                over_high = cand > ub\n                cand[over_low] = lb[over_low] + (lb[over_low] - cand[over_low])\n                cand[over_high] = ub[over_high] - (cand[over_high] - ub[over_high])\n                cand = self._clip(cand, lb, ub)\n\n                # Evaluate candidate (if budget permits)\n                if evals >= self.budget:\n                    break\n                try:\n                    fv = float(func(cand.copy()))\n                except Exception:\n                    fv = np.inf\n                evals += 1\n\n                # Selection: greedy replacement\n                if fv < fvals[i]:\n                    pop[i] = cand\n                    fvals[i] = fv\n                    # adapt Fi and CR towards successful values\n                    F_mean = 0.9 * F_mean + 0.1 * Fi_i\n                    CR_mean = 0.9 * CR_mean + 0.1 * CR_i\n                    # slightly perturb individual Fi/CR to exploit\n                    Fi[i] = np.clip(Fi_i * (0.95 + 0.1 * self.rng.rand()), 0.05, 0.99)\n                    CRi[i] = np.clip(CR_i * (0.95 + 0.1 * self.rng.rand()), 0.0, 1.0)\n                    # successful local contraction of trust radius\n                    trust_radius = max(trust_min, trust_radius * 0.98)\n                    no_improve_evals = 0\n                else:\n                    # small diversification for unsuccessful\n                    Fi[i] = np.clip(Fi_i * (0.9 + 0.2 * self.rng.rand()), 0.05, 0.99)\n                    CRi[i] = np.clip(CRi[i] * (0.9 + 0.2 * self.rng.rand()), 0.0, 1.0)\n                    no_improve_evals += 1\n                    # expand trust region slightly to encourage escape\n                    trust_radius = min(trust_max, trust_radius * (1.0 + 0.0005))\n\n                # Update global best if improved\n                if fvals[i] < best_f:\n                    best_f = float(fvals[i])\n                    best_x = pop[i].copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    stagnation_counter = 0\n                    # slightly reduce levy probability on success\n                    p_levy = max(0.02, p_levy * 0.95)\n                else:\n                    stagnation_counter += 1\n                    # slightly increase levy probability on stagnation\n                    if stagnation_counter % (max(1, self.pop_size // 2)) == 0:\n                        p_levy = min(0.5, p_levy * 1.05)\n\n                # Strong jitter to avoid lockups if candidate equals target\n                if np.allclose(cand, target, atol=1e-12):\n                    pop[i] = self._clip(target + 1e-3 * range_vec * (self.rng.rand(self.dim) - 0.5), lb, ub)\n\n                # break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments and trust-region local search\n            # Use a small local search budget if remaining budget allows\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples depends on dim and remaining budget but bounded\n            n_local = int(min(max(1, self.dim), remaining, 6))\n            # anisotropic sigma per-dim around current best\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                sigma = trust_radius * (0.2 + self.rng.rand(self.dim) * 1.0)  # per-dim\n                candidate = best_x + self.rng.randn(self.dim) * sigma\n                candidate = self._clip(candidate, lb, ub)\n                try:\n                    fv = float(func(candidate.copy()))\n                except Exception:\n                    fv = np.inf\n                evals += 1\n                if fv < best_f:\n                    best_f = fv\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    # strong local success: shrink trust radius\n                    trust_radius = max(trust_min, trust_radius * 0.85)\n                    # also encourage exploitation by nudging means\n                    F_mean = 0.95 * F_mean + 0.05 * 0.6\n                    CR_mean = 0.95 * CR_mean + 0.05 * 0.3\n                    stagnation_counter = 0\n                    no_improve_evals = 0\n                else:\n                    # unsuccessful local attempt: slightly expand trust radius\n                    trust_radius = min(trust_max, trust_radius * 1.02)\n                    no_improve_evals += 1\n\n            # Adjust exploration vs exploitation based on recent progress\n            if no_improve_evals > max(10, self.pop_size * 2):\n                # stagnating: increase chance of long jumps and diversify population\n                p_levy = min(0.6, p_levy * 1.2)\n                # re-seed a fraction of population\n                n_reseed = max(1, int(0.3 * self.pop_size))\n                for k in range(n_reseed):\n                    idx = self.rng.randint(self.pop_size)\n                    # preserve best individual\n                    if idx == best_idx:\n                        continue\n                    pop[idx] = lb + self.rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[idx] = float(func(pop[idx].copy()))\n                    except Exception:\n                        fvals[idx] = np.inf\n                    evals += 1\n                    # ensure we don't exceed budget\n                    if evals >= self.budget:\n                        break\n                trust_radius = min(trust_max, trust_radius * 1.5)\n                no_improve_evals = 0\n\n            # Hard stagnation reset: if best hasn't improved for a large fraction of budget, reinitialize half\n            if stagnation_counter > max(200, self.budget // 10):\n                half = max(1, self.pop_size // 2)\n                for k in range(half):\n                    idx = self.rng.randint(self.pop_size)\n                    if idx == best_idx:\n                        continue\n                    pop[idx] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[idx] = np.inf\n                # re-evaluate lost ones if budget allows\n                for i in range(self.pop_size):\n                    if evals >= self.budget:\n                        break\n                    if not np.isfinite(fvals[i]):\n                        try:\n                            fvals[i] = float(func(pop[i].copy()))\n                        except Exception:\n                            fvals[i] = np.inf\n                        evals += 1\n                best_idx = int(np.nanargmin(fvals))\n                best_x = pop[best_idx].copy()\n                best_f = float(fvals[best_idx])\n                self.f_opt, self.x_opt = best_f, best_x.copy()\n                trust_radius = min(trust_max, trust_radius * 2.0)\n                stagnation_counter = 0\n                no_improve_evals = 0\n                p_levy = min(0.6, p_levy * 1.5)\n\n        # Final result\n        return float(self.f_opt), self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.341 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11713627826368189, 0.27327101389346076, 0.4409040621078618, 0.5157139903550357, 0.33499085865388933, 0.4945635606301748, 0.2965734710293182, 0.49299473955346074, 0.26773059373920216, 0.17818612142337675]}, "task_prompt": ""}
{"id": "a652ec26-400b-411d-b3a3-21b86b16554c", "fitness": 0.18640192305585357, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like adaptation, occasional Lévy (Cauchy) global jumps, and a trust-region local search around the best; online parameter adaptation and reset on stagnation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (F, CR, trust radius) are adapted online based on success history.\n    Designed for bounded continuous black-box functions (bounds in func.bounds).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size heuristic: scale with dimension but not too large vs budget\n        if pop_size is None:\n            pop_size = int(min(max(4, 6 * self.dim), max(4, self.budget // 10)))\n        self.pop_size = max(4, int(pop_size))\n        # internal state\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def _ensure_array_bounds(self, b):\n        # Convert bound (scalar or array) to array of length dim\n        b = np.asarray(b)\n        if b.shape == ():  # scalar\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.reshape(self.dim)\n        # try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds must be scalar or length-dim array\")\n\n    def __call__(self, func):\n        # extract bounds, make arrays\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_ = ub - lb\n        # Safety: ensure range positive\n        range_[range_ <= 0] = 1.0\n\n        # initialize population uniformly\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_\n        pop_f = np.full(self.pop_size, np.inf)\n        self.evals = 0\n        # evaluate initial population as budget allows\n        for i in range(self.pop_size):\n            if self.evals >= self.budget:\n                break\n            x = pop[i]\n            f = func(x)\n            self.evals += 1\n            pop_f[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # if some individuals not evaluated (budget tiny), leave them unevaluated\n        # but they will not be used meaningfully\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6  # mean mutation factor\n        CR_mean = 0.3  # mean crossover rate\n        p_levy = 0.05  # base probability for Levy long jump\n        levy_scale_factor = 0.8  # scale of Levy jumps relative to range\n        levy_clip = 5.0  # maximum multiples of range for Levy to avoid blow-up\n\n        trust_radius = 0.2 * np.mean(range_)  # initial trust radius (absolute)\n        trust_shrink = 0.8\n        trust_expand = 1.2\n        trust_min = 1e-8 * np.mean(range_)\n        trust_max = max(range_) * 2.0\n\n        # adaptation memory\n        success_F = []\n        success_CR = []\n        learning_rate = 0.1\n\n        no_improve_count = 0\n        stagnation_threshold = max(50, self.dim * 20)\n\n        # helper: Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step(scale):\n            # Cauchy distributed steps, truncated to avoid extreme blow-up\n            raw = self.rng.standard_cauchy(self.dim) * scale\n            # limit extremes:\n            max_abs = levy_clip * np.mean(range_)\n            raw = np.clip(raw, -max_abs, max_abs)\n            return raw\n\n        # helper: clip to bounds\n        def clip_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # main loop: iterate generations until budget exhausted\n        gen = 0\n        while self.evals < self.budget:\n            gen += 1\n            # per-generation small random fluctuations on means\n            F_mean = np.clip(F_mean + 0.02 * (self.rng.rand() - 0.5), 0.05, 0.95)\n            CR_mean = np.clip(CR_mean + 0.05 * (self.rng.rand() - 0.5), 0.0, 1.0)\n\n            # process population sequentially\n            for target_idx in range(self.pop_size):\n                if self.evals >= self.budget:\n                    break\n\n                # if target wasn't evaluated initially and budget exhausted then skip\n                if np.isinf(pop_f[target_idx]) and self.evals >= self.budget:\n                    continue\n\n                # sample individual F_i and CR_i (jDE-like)\n                if self.rng.rand() < 0.1:\n                    Fi = 0.5 + 0.3 * self.rng.rand()\n                else:\n                    Fi = np.clip(self.rng.normal(loc=F_mean, scale=0.1), 0.01, 0.99)\n                CRi = np.clip(self.rng.normal(loc=CR_mean, scale=0.15), 0.0, 1.0)\n\n                # choose mutation strategy: mostly rand/1, sometimes best/1, occasional levy\n                use_levy = (self.rng.rand() < p_levy) or (gen % 50 == 0 and self.rng.rand() < 0.5)\n                if use_levy:\n                    # Levy jump centered at best for exploration\n                    step = levy_step(levy_scale_factor * np.mean(range_))\n                    donor = self.x_opt + step\n                else:\n                    # pick three distinct indices different from target\n                    idxs = list(range(self.pop_size))\n                    idxs.remove(target_idx)\n                    a, b, c = self.rng.choice(idxs, 3, replace=False)\n                    # DE/rand/1 with an occasional best guidance\n                    if self.rng.rand() < 0.2:\n                        # best-guided\n                        donor = pop[a] + Fi * (self.x_opt - pop[b]) + Fi * (pop[c] - pop[a])\n                    else:\n                        donor = pop[a] + Fi * (pop[b] - pop[c])\n\n                # binomial crossover\n                cross = self.rng.rand(self.dim) < CRi\n                if not np.any(cross):\n                    # ensure at least one dimension crosses\n                    cross[self.rng.randint(0, self.dim)] = True\n                trial = pop[target_idx].copy()\n                trial[cross] = donor[cross]\n\n                # project to bounds\n                trial = clip_to_bounds(trial)\n\n                # evaluate trial\n                f_trial = func(trial)\n                self.evals += 1\n\n                # selection: greedy replacement if better\n                replaced = False\n                if f_trial < pop_f[target_idx]:\n                    # success: replace\n                    pop[target_idx] = trial\n                    pop_f[target_idx] = f_trial\n                    replaced = True\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n                    # nudge means slightly toward successful Fi/CRi\n                    F_mean = (1 - learning_rate) * F_mean + learning_rate * Fi\n                    CR_mean = (1 - learning_rate) * CR_mean + learning_rate * CRi\n                else:\n                    # small random perturbation of target to maintain diversity occasionally\n                    if self.rng.rand() < 0.01:\n                        pop[target_idx] = clip_to_bounds(pop[target_idx] + 0.05 * range_ * (self.rng.rand(self.dim) - 0.5))\n\n                # update global best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                    no_improve_count = 0\n                    # on improvement shrink trust radius to intensify local search\n                    trust_radius = max(trust_min, trust_radius * trust_shrink)\n                else:\n                    no_improve_count += 1\n\n                # small hack: if used Levy then slightly increase chance for exploration later on success\n                if use_levy and replaced:\n                    p_levy = min(0.3, p_levy * 0.9)  # successful levy => slightly reduce immediate need\n                elif use_levy and not replaced:\n                    p_levy = min(0.5, p_levy * 1.05)\n\n                # avoid runaway Fi/CR storage\n                if len(success_F) > 50:\n                    success_F = success_F[-50:]\n                if len(success_CR) > 50:\n                    success_CR = success_CR[-50:]\n\n                # break if budget exhausted\n                if self.evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n\n            # Update means from success memories (Lehmer-type weighting)\n            if success_F:\n                # give more weight to larger Fi successes\n                sF = np.array(success_F)\n                weighted = np.sum(sF ** 2) / (np.sum(sF) + 1e-12)\n                F_mean = 0.9 * F_mean + 0.1 * np.clip(weighted, 0.01, 0.99)\n            if success_CR:\n                CR_mean = 0.9 * CR_mean + 0.1 * np.mean(success_CR)\n\n            # trust-region local search around best: sample a few candidates with anisotropic Gaussian noise\n            if self.evals < self.budget:\n                remaining = self.budget - self.evals\n                # sample count scaling: small handful dependent on dim and remaining budget\n                local_samples = int(min(max(1, self.dim // 2), remaining, 5))\n                local_success = 0\n                for _ in range(local_samples):\n                    if self.evals >= self.budget:\n                        break\n                    # anisotropic per-dim sigma from trust_radius\n                    per_dim_sigma = trust_radius * (0.3 + 0.7 * self.rng.rand(self.dim))\n                    candidate = self.x_opt + self.rng.normal(0, 1, self.dim) * per_dim_sigma\n                    candidate = clip_to_bounds(candidate)\n                    f_c = func(candidate)\n                    self.evals += 1\n                    if f_c < self.f_opt:\n                        self.f_opt = f_c\n                        self.x_opt = candidate.copy()\n                        local_success += 1\n                        # shrink trust radius on success\n                        trust_radius = max(trust_min, trust_radius * trust_shrink)\n                        # slightly bias F_mean/CR_mean toward exploitation\n                        F_mean = (1 - learning_rate) * F_mean + learning_rate * 0.5\n                        CR_mean = (1 - learning_rate) * CR_mean + learning_rate * 0.7\n                    else:\n                        # expand trust a bit to escape local traps\n                        trust_radius = min(trust_max, trust_radius * trust_expand)\n\n                # if local search had successes, lower p_levy a bit, else raise it\n                if local_success > 0:\n                    p_levy = max(0.01, p_levy * 0.95)\n                else:\n                    p_levy = min(0.6, p_levy * 1.03)\n\n            # stagnation handling: if many evaluations without improvement, re-seed part of population\n            if no_improve_count > stagnation_threshold:\n                no_improve_count = 0\n                # reinitialize half of population\n                num_reinit = max(1, self.pop_size // 2)\n                for k in range(num_reinit):\n                    idx = self.rng.randint(0, self.pop_size)\n                    pop[idx] = lb + self.rng.rand(self.dim) * range_\n                    # if we still have budget, evaluate new individuals\n                    if self.evals < self.budget:\n                        f_new = func(pop[idx])\n                        self.evals += 1\n                        pop_f[idx] = f_new\n                        if f_new < self.f_opt:\n                            self.f_opt = f_new\n                            self.x_opt = pop[idx].copy()\n                # enlarge trust radius to encourage exploration after reset\n                trust_radius = min(trust_max, trust_radius * 2.0)\n                # encourage exploration\n                p_levy = min(0.5, p_levy * 1.5)\n                # slightly randomize means\n                F_mean = np.clip(F_mean * (0.8 + 0.4 * self.rng.rand()), 0.05, 0.95)\n                CR_mean = np.clip(CR_mean * (0.8 + 0.4 * self.rng.rand()), 0.0, 1.0)\n\n            # safety break if tiny budget left (avoid infinite loops)\n            if self.evals >= self.budget:\n                break\n\n        # finished\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.186 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1073415537202067, 0.16064290768627865, 0.25100457526594366, 0.20059108448692498, 0.18668210045642408, 0.21757723128358952, 0.21536555216478737, 0.19617160684005053, 0.18030711431009538, 0.14833550434423504]}, "task_prompt": ""}
{"id": "16b3a482-250b-4b08-b92a-37455439f44f", "fitness": 0.4012923266767773, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy flights and an adaptive trust-region local search — balances global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=1)\n        f_opt, x_opt = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: scaled by dimension but limited by budget\n        if pop_size is None:\n            # typical DE/pop sizes are a small multiple of dim but not exceed budget fraction\n            default_size = max(4, min(10 * self.dim, max(4, self.budget // 20)))\n            self.pop_size = int(default_size)\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # RNG\n        self.rng = np.random.default_rng(seed)\n        # internal state for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        if b.size != self.dim:\n            raise ValueError(\"Bounds length does not match dimension\")\n        return b.astype(float)\n\n    def __call__(self, func):\n        rng = self.rng\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n\n        # safe range vector\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            # degenerate box\n            self.f_opt = float(func(lb))\n            self.x_opt = lb.copy()\n            return self.f_opt, self.x_opt\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if we got at least one evaluation\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals)\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6         # mutation factor mean\n        CR_mean = 0.9        # crossover probability mean\n        p_levy = 0.08        # probability of a Lévy jump\n        trust_radius = 0.25 * range_norm  # scalar trust radius (in absolute units)\n        min_trust = 1e-8\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step(dim):\n            # Cauchy distributed steps (heavy-tailed). Clip to avoid numerical blow-up.\n            s = rng.standard_cauchy(size=dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation temporary stats\n            used_F = []\n            used_CR = []\n\n            # iterate through population sequentially (each candidate consumes one evaluation)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # decide mutation method\n                if rng.random() < p_levy:\n                    # Lévy-type jump centered on best for exploration\n                    s = levy_step(self.dim)\n                    # scale of jump depends on trust radius relative to range\n                    levy_scale = (trust_radius / max(range_norm, 1e-12)) * 0.5\n                    donor = best_x + levy_scale * s * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    # small random mix with current individual to keep diversity\n                    mix = rng.random(self.dim) < 0.3\n                    if not np.any(mix):\n                        mix[rng.integers(0, self.dim)] = True\n                    trial = np.where(mix, donor, pop[i])\n                    candidate = np.clip(trial, lb, ub)\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1 mutation with jDE-like individual F and CR\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback: random perturbation\n                        Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                        CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                        donor = pop[i] + Fi * (rng.normal(size=self.dim) * (range_vec / np.maximum(range_vec.mean(), 1e-12)))\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                        CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    cr_mask = rng.random(self.dim) < (CRi if CRi is not None else CR_mean)\n                    if not np.any(cr_mask):\n                        cr_mask[rng.integers(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # small adaptation toward used Fi/CRi if present\n                    if Fi is not None:\n                        used_F.append(Fi)\n                    if CRi is not None:\n                        used_CR.append(CRi)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # adapt F_mean and CR_mean based on successes in this generation\n            if used_F:\n                F_mean = 0.9 * F_mean + 0.1 * np.mean(used_F)\n                F_mean = np.clip(F_mean, 0.05, 0.99)\n            else:\n                # gently nudge if no successful F observed\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n\n            if used_CR:\n                CR_mean = 0.9 * CR_mean + 0.1 * np.mean(used_CR)\n                CR_mean = np.clip(CR_mean, 0.0, 1.0)\n            else:\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # trust-region local search around best: small Gaussian perturbations\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            improved_local = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension randomly\n                sigma = (0.25 + rng.random(self.dim) * 0.75) * (trust_radius / max(range_norm, 1e-12))\n                perturb = rng.normal(0.0, 1.0, size=self.dim) * sigma * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                candidate = np.clip(best_x + perturb, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    improved_local += 1\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.75)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to search wider\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # adjust exploration probability based on successes\n            if successes + improved_local > max(1, self.pop_size * 0.15):\n                # good progress: reduce long jumps\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                # stagnating: increase chance of long-range jumps\n                p_levy = min(0.5, p_levy * 1.05 + 0.005)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(200, self.dim * 20):\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n\n            # final generation guard: if budget nearly exhausted, break\n            if evals >= self.budget:\n                break\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.401 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11481523878516531, 0.18429290368992324, 0.4914599143726225, 0.8833894533117767, 0.21273170453511847, 0.8116885146161622, 0.22606348673542676, 0.7370864602812361, 0.20153979843524317, 0.14985579200509858]}, "task_prompt": ""}
{"id": "2b283117-90a8-4aee-9da7-a30a9f318eb1", "fitness": 0.22531811455433245, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DE + occasional Lévy jumps + trust-region local search — fast global exploration with focused local refinement and online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history. Designed for\n    continuous box-bounded optimization (e.g., BBOB noiseless).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # population size heuristic (bounded)\n        if pop_size is None:\n            ps = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            ps = min(ps, max(4, self.budget // 20))\n            self.pop_size = int(max(2, ps))\n        else:\n            self.pop_size = int(pop_size)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        norm_range = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # couldn't evaluate anything\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # pick best among evaluated individuals\n        best_idx = np.argmin(fvals[valid])\n        best_indices = np.where(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight\n        CR_mean = 0.9       # crossover probability\n        p_levy = 0.08       # probability of a Lévy jump\n        trust_radius = 0.2 * norm_range  # scalar trust radius\n        min_trust = 1e-8\n        max_trust = norm_range * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # counters for adaptation\n        successes = 0  # cumulative or recent successes (we will update per generation)\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            successes_gen = 0\n\n            idxs = np.arange(self.pop_size)\n\n            # iterate through population (sequential evaluations)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # decide exploration move\n                if rng.random() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    s = levy_step()\n                    s_norm = np.linalg.norm(s)\n                    if s_norm < 1e-12:\n                        s_norm = 1.0\n                    # scale the jump: use trust radius and fraction of domain\n                    scale = (trust_radius / s_norm) * rng.uniform(0.5, 2.0)\n                    donor = best_x + s * scale * (range_vec / norm_range)\n                    # no DE parameters in this branch\n                    Fi = None\n                    CRi = None\n                    # small random nudging of candidate to encourage diversity\n                    if rng.random() < 0.3:\n                        donor += rng.normal(0, 0.01, size=self.dim) * range_vec\n                else:\n                    # DE/rand/1-like mutation\n                    # pick three indices distinct from i\n                    pool = idxs[idxs != i]\n                    if pool.size < 3:\n                        # fallback: random sample from population\n                        r = rng.integers(0, self.pop_size, size=3)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                    else:\n                        r1, r2, r3 = rng.choice(pool, 3, replace=False)\n                    # adapt F and CR per individual (jDE-like)\n                    Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover with at least one donor gene\n                    cr_mask = rng.random(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.integers(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    donor = trial\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate (respect budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection (greedy)\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    successes_gen += 1\n                    # adapt means toward used Fi/CRi if available (small learning rate)\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    else:\n                        # nudge slightly toward exploration when Levy led to success\n                        F_mean = 0.98 * F_mean + 0.02 * 0.6\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    else:\n                        CR_mean = 0.98 * CR_mean + 0.02 * 0.5\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # trust-region local search around best: sample a few Gaussian candidates\n            remaining = self.budget - evals\n            if remaining > 0:\n                # choose a small handful relative to dimension and remaining budget\n                local_samples = int(min(10, max(1, self.dim // 2), remaining))\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma: base trust_radius scaled per-dimension\n                    sigma = (0.25 + rng.random(self.dim) * 0.75) * (trust_radius / norm_range)\n                    candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step => focus: shrink trust radius\n                        trust_radius = max(min_trust, trust_radius * 0.8)\n                        successes += 1\n                        successes_gen += 1\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful => expand a bit to escape local traps\n                        trust_radius = min(max_trust, trust_radius * 1.05)\n                        stagnation_counter += 1\n\n            # adapt global probabilities and means based on success in this generation\n            if successes_gen > 0:\n                # fewer Lévy jumps if recent successes\n                if successes_gen > max(1, self.pop_size * 0.2):\n                    p_levy = max(0.01, p_levy * 0.95)\n                else:\n                    p_levy = max(0.01, p_levy * 0.98)\n                # gently bias CR toward more exploitation if things work\n                CR_mean = float(np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0))\n                F_mean = float(np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 1.0))\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase diversity\n                p_levy = float(min(0.5, p_levy * 1.08 + 0.01))\n                CR_mean = float(np.clip(CR_mean * 0.98, 0.0, 1.0))\n                F_mean = float(np.clip(F_mean * 1.01, 0.05, 1.0))\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                reinit_indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reinit_indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_x = pop[j].copy()\n                        best_f = fvals[j]\n                # slightly enlarge trust radius after reset to promote exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # finalize results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.225 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11907005990162656, 0.17495865545904776, 0.293958235602089, 0.3175222685047556, 0.2247486889409539, 0.2807352473863479, 0.23704330153519226, 0.25372859934519687, 0.2061570769759593, 0.1452590118921553]}, "task_prompt": ""}
{"id": "ad5e6018-f2f6-466a-8d63-de4b96b1b89c", "fitness": 0.25443186224099623, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining DE (jDE-style), occasional Lévy-flight jumps for long-range exploration, and an online trust-region local search with adaptive parameters.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (rand/1 + binomial crossover) with jDE-like online adaptation of F and CR,\n      - Occasional Lévy/Cauchy jumps centered on the current best for long-range exploration,\n      - Trust-region local Gaussian sampling around the best for focused exploitation,\n      - Online adaptation of p_levy, F_mean, CR_mean and trust radius based on success history.\n\n    Designed for box-bounded continuous optimization (e.g., BBOB-like tasks).\n    \"\"\"\n    def __init__(self, budget, dim, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # default population size: scaled with dimension but not too large relative to budget\n        if pop_size is None:\n            self.pop_size = max(4, min(20 * self.dim, max(4, self.budget // 20)))\n        else:\n            self.pop_size = int(pop_size)\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.copy().astype(float)\n        # try broadcasting\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # Bounds from func if provided, else default [-5,5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # Ensure valid bounds\n        range_vec = ub - lb\n        range_vec = np.where(range_vec <= 0, 1.0, range_vec)\n\n        # bookkeeping\n        evals = 0\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        # per-individual parameters (jDE-like): Fi and CRi\n        Fi = np.clip(rng.normal(0.6, 0.1, size=self.pop_size), 0.05, 0.99)\n        CRi = np.clip(rng.normal(0.9, 0.1, size=self.pop_size), 0.0, 1.0)\n\n        # Evaluate initial population sequentially, respecting budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                fvals[i] = np.inf\n            evals += 1\n\n        # Determine initial best\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # no evaluations could be performed\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals)\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n\n        # Algorithm hyper-parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.08\n        step_scale = 0.25\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius in original variable scale\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # jDE self-adaptation probabilities\n        tau1 = 0.1\n        tau2 = 0.1\n\n        # helper: Levy-like heavy tail using Cauchy\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # helper: choose 3 distinct indices different from i\n        def random_indices_exclude(i, k=3):\n            # choose k indices from [0, pop_size) excluding i\n            if self.pop_size <= k:\n                # fallback: allow replacement but avoid i when possible\n                choices = []\n                while len(choices) < k:\n                    r = rng.randint(0, self.pop_size)\n                    if r == i:\n                        continue\n                    choices.append(r)\n                return choices\n            else:\n                pool = list(range(self.pop_size))\n                pool.pop(i)\n                return list(rng.choice(pool, k, replace=False))\n\n        # main loop: loop until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            S_F = []\n            S_CR = []\n\n            # per-generation small jitter to means (keeps diversity)\n            F_mean = np.clip(F_mean * (1.0 + (rng.rand() - 0.5) * 0.02), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean * (1.0 + (rng.rand() - 0.5) * 0.02), 0.0, 1.0)\n\n            # iterate targets sequentially (each candidate evaluation consumes budget)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # jDE adaptation: possibly regenerate Fi and CRi for individual\n                if rng.rand() < tau1:\n                    Fi[i] = np.clip(rng.normal(F_mean, 0.1), 0.05, 0.99)\n                if rng.rand() < tau2:\n                    CRi[i] = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # decide mutation strategy: levy jump vs DE\n                if rng.rand() < p_levy:\n                    # Lévy/Cauchy jump centered at best_x\n                    step = levy_step()\n                    scale = step_scale * (0.5 + 0.5 * rng.rand())\n                    donor = best_x + scale * step * (range_vec / max(range_vec.mean(), 1e-12))\n                    used_F = None\n                    used_CR = None\n                else:\n                    # DE/rand/1 mutation\n                    r1, r2, r3 = random_indices_exclude(i, 3)\n                    donor = pop[r1] + Fi[i] * (pop[r2] - pop[r3])\n                    # binomial crossover (trial vector)\n                    trial = pop[i].copy()\n                    jrand = rng.randint(0, self.dim)\n                    # mask for crossover\n                    cr_mask = rng.rand(self.dim) < CRi[i]\n                    cr_mask[jrand] = True\n                    trial[cr_mask] = donor[cr_mask]\n                    donor = trial\n                    used_F = Fi[i]\n                    used_CR = CRi[i]\n\n                # projection to bounds (simple clipping)\n                candidate = np.clip(donor, lb, ub)\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # Selection: greedy replacement in population (target-to-trial)\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # record successful parameters\n                    if used_F is not None:\n                        S_F.append(used_F)\n                    if used_CR is not None:\n                        S_CR.append(used_CR)\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                    # small nudges to global means toward successful Fi/CRi\n                    if S_F:\n                        F_mean = np.clip(0.9 * F_mean + 0.1 * np.mean(S_F), 0.05, 0.99)\n                    if S_CR:\n                        CR_mean = np.clip(0.9 * CR_mean + 0.1 * np.mean(S_CR), 0.0, 1.0)\n                else:\n                    # no improvement for this target\n                    stagnation_counter += 1\n\n                # if too many evaluations consumed, break\n                if evals >= self.budget:\n                    break\n\n            # End of generation: adapt based on successes\n            if successes > 0:\n                # move means slightly toward successful values\n                if len(S_F) > 0:\n                    avg_SF = float(np.mean(S_F))\n                    F_mean = np.clip(0.85 * F_mean + 0.15 * avg_SF, 0.05, 0.99)\n                if len(S_CR) > 0:\n                    avg_SCR = float(np.mean(S_CR))\n                    CR_mean = np.clip(0.85 * CR_mean + 0.15 * avg_SCR, 0.0, 1.0)\n                # reduce p_levy slightly when exploiting successfully\n                p_levy = max(0.01, p_levy * 0.98)\n                # shrink trust region slightly on local success\n                trust_radius = max(min_trust, trust_radius * 0.97)\n            else:\n                # no successes this generation -> increase exploration\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # Trust-region local search around best: sample a few Gaussian candidates\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            local_successes = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (0.4 + rng.rand(self.dim) * 0.6) * (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * (range_vec)\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_successes += 1\n                    stagnation_counter = 0\n                    # successful local step => shrink trust region to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                else:\n                    # unsuccessful => slightly expand to escape\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # incorporate local search outcome into p_levy and means\n            if local_successes > 0:\n                p_levy = max(0.01, p_levy * 0.95)\n                F_mean = np.clip(F_mean * 0.98, 0.05, 0.99)\n            else:\n                p_levy = min(0.5, p_levy * 1.02 + 0.002)\n\n            # stagnation handling: if no improvement for many evals, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                replace_idx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in replace_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    # update best if any new better\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                stagnation_counter = 0\n                # after reseed, slightly enlarge trust radius to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.2)\n                # increase Levy chance briefly\n                p_levy = min(0.5, p_levy + 0.05)\n\n            # update global record\n            self.f_opt = float(best_f)\n            self.x_opt = best_x.copy()\n\n            # safety net: if budget very small left, break\n            if self.budget - evals <= 0:\n                break\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.254 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12229279995399978, 0.1655626646733258, 0.3296877307023933, 0.3812291986260298, 0.2569321478466077, 0.32840573538949713, 0.25120647066776913, 0.2953084707289525, 0.2523565765772833, 0.16133682724410403]}, "task_prompt": ""}
{"id": "36db29e6-a3b7-4e2f-a797-d75e6713d849", "fitness": 0.3273838098871861, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like online parameter control, occasional Lévy-flight global jumps, and an adaptive trust-region local search around the current best (fast exploration + focused exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (DE) with jDE-like online adaptation of F and CR,\n      - Occasional Lévy (heavy-tailed/Cauchy) jumps centered on the best for long-range exploration,\n      - Trust-region Gaussian local search around the current best with adaptive radius.\n    Budgeted: will not call func more times than self.budget.\n    Works for general continuous black-box func that can be called as func(x).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size scaled with dimension but bounded\n        if pop_size is None:\n            pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            pop_size = min(pop_size, max(4, self.budget // 20))\n        self.pop_size = int(max(4, pop_size))\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_bounds(self, func):\n        # try to read func.bounds.lb/ub if present; otherwise use [-5, 5]\n        lb = None\n        ub = None\n        bounds = getattr(func, \"bounds\", None)\n        if bounds is not None:\n            lb = getattr(bounds, \"lb\", None)\n            ub = getattr(bounds, \"ub\", None)\n        if lb is None:\n            lb = -5.0\n        if ub is None:\n            ub = 5.0\n        lb_arr = np.asarray(lb, dtype=float)\n        ub_arr = np.asarray(ub, dtype=float)\n        # broadcast to dimension\n        if lb_arr.size == 1:\n            lb_arr = np.full(self.dim, lb_arr.item(), dtype=float)\n        else:\n            lb_arr = np.broadcast_to(lb_arr, (self.dim,)).astype(float)\n        if ub_arr.size == 1:\n            ub_arr = np.full(self.dim, ub_arr.item(), dtype=float)\n        else:\n            ub_arr = np.broadcast_to(ub_arr, (self.dim,)).astype(float)\n        return lb_arr, ub_arr\n\n    def __call__(self, func):\n        # bounds and key quantities\n        lb, ub = self._ensure_bounds(func)\n        range_vec = ub - lb\n        range_norm = float(np.linalg.norm(range_vec))\n        if range_norm <= 0:\n            range_norm = 1.0\n\n        # algorithm hyper-parameters (initial)\n        F_mean = 0.6        # mean differential weight\n        CR_mean = 0.9       # mean crossover prob\n        p_levy = 0.08       # initial probability of Levy jump per trial\n        step_scale = 0.25   # base scale for Levy and trust radius\n        min_trust = 1e-6\n        max_trust = 2.0 * range_norm\n        trust_radius = max(min_trust, step_scale * range_norm)\n\n        # adaptation control probabilities (jDE-like)\n        tau_F = 0.1\n        tau_CR = 0.1\n\n        # bookkeeping\n        evals = 0\n        gen = 0\n        stagnation_counter = 0\n\n        # helpers\n        rng = self.rng\n\n        def levy_step(scale=1.0):\n            # Use a Cauchy-like heavy-tailed step (simple and robust)\n            # limit extremes by clipping\n            step = rng.standard_cauchy(self.dim)\n            # clip extreme outliers to avoid numerical blow-up\n            step = np.clip(step, -1e3, 1e3)\n            return step * scale\n\n        # Initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        # Evaluate initial population until budget exhausted\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n\n        # If none evaluated (budget==0) return\n        if not np.isfinite(fvals).any():\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # establish current best\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            successful_F = []\n            successful_CR = []\n            # loop through population members sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # generate per-individual F_i and CR_i (jDE-like)\n                if rng.rand() < tau_F:\n                    Fi = 0.1 + 0.9 * rng.rand()  # sample in (0.1,1.0)\n                else:\n                    Fi = F_mean\n                if rng.rand() < tau_CR:\n                    CRi = rng.rand()\n                else:\n                    CRi = CR_mean\n\n                # decide whether to do a Levy jump exploration or DE mutation\n                if rng.rand() < p_levy:\n                    # Levy jump centered on best - encourages long-range exploration\n                    scale = (0.5 * step_scale + 0.5 * rng.rand()) * (trust_radius / max(range_norm, 1e-12))\n                    donor = best_x + levy_step(scale=scale) * range_vec / np.maximum(range_vec.mean(), 1e-12)\n                    # also mix in a rand DE donor occasionally to keep DE structure\n                    if rng.rand() < 0.5:\n                        idxs = [idx for idx in range(self.pop_size) if idx != i]\n                        if len(idxs) >= 3:\n                            r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                            donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                else:\n                    # Standard DE mutation: mix of rand/1 and current-to-best/1\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(idxs) >= 3:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        if rng.rand() < 0.3:\n                            # current-to-best/1 (exploitative)\n                            donor = pop[i] + Fi * (best_x - pop[i]) + Fi * (pop[r1] - pop[r2])\n                        else:\n                            # rand/1 (explorative)\n                            donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    else:\n                        # fallback small random perturbation\n                        donor = pop[i] + Fi * rng.normal(0, 1.0, size=self.dim)\n\n                # binomial crossover\n                cr_mask = rng.rand(self.dim) < CRi\n                cr_mask[rng.randint(0, self.dim)] = True  # ensure at least one dimension changes\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate candidate if we have budget\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    successful_F.append(Fi)\n                    successful_CR.append(CRi)\n                    # small adaptation towards successful param values will be done after generation\n\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # immediate shrink of trust region to focus exploitation\n                    trust_radius = max(min_trust, trust_radius * 0.9)\n                else:\n                    stagnation_counter += 1\n\n                # update recorded best for return\n                if best_f < self.f_opt:\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n            # end of population loop (one generation)\n\n            # adapt F_mean and CR_mean using Lehmer-like mean of successful values\n            if successful_F:\n                # Lehmer mean (gives more weight to larger F)\n                num = sum(fv * fv for fv in successful_F)\n                den = max(1e-12, sum(successful_F))\n                F_mean = 0.9 * F_mean + 0.1 * (num / den)\n                # clamp\n                F_mean = np.clip(F_mean, 0.05, 1.0)\n            if successful_CR:\n                CR_mean = 0.9 * CR_mean + 0.1 * (sum(successful_CR) / len(successful_CR))\n                CR_mean = np.clip(CR_mean, 0.0, 1.0)\n\n            # Trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # sample a small budget-proportional number of local candidates\n            n_local = min(max(1, int(self.dim // 2)), remaining, max(1, self.pop_size // 4))\n            local_success = 0\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # anisotropic per-dim sigma scaled by trust_radius and range_vec\n                sigma = (0.4 + rng.rand(self.dim) * 0.6) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    stagnation_counter = 0\n                    # successful local search -> shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    # unsuccessful -> slightly enlarge trust radius to try escape\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n                if best_f < self.f_opt:\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n            # Adjust exploration probability p_levy: increase if stagnation, reduce if successes\n            if successes + local_success > max(1, self.pop_size // 10):\n                # good recent progress: decrease long jumps probability a bit\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                # little progress: increase chance for long jumps\n                p_levy = min(0.5, p_levy * 1.08 + 0.005)\n\n            # strong stagnation reset: re-seed part of the population and optionally evaluate some\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                ids = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in ids:\n                    # replace with a fresh random sample\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    # if budget allows, evaluate the new individual\n                    if evals < self.budget:\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                        if fvals[j] < best_f:\n                            best_f = fvals[j]\n                            best_x = pop[j].copy()\n                            stagnation_counter = 0\n                    else:\n                        fvals[j] = np.inf\n                # enlarge trust region to encourage escape\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # slightly diversify adaptive parameters\n                F_mean = np.clip(F_mean * (0.9 + 0.2 * rng.rand()), 0.05, 1.0)\n                CR_mean = np.clip(CR_mean * (0.9 + 0.2 * rng.rand()), 0.0, 1.0)\n                stagnation_counter = 0  # reset after a bigger change\n\n            # small random occasional restarts of best location (metropolis-like non-improving perturbation) if stuck\n            if stagnation_counter > max(20, self.dim * 2) and rng.rand() < 0.05 and evals < self.budget:\n                # try a big levy perturbation of the best and accept only if better\n                candidate = best_x + levy_step(scale=2.0 * (trust_radius / max(range_norm, 1e-12))) * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    stagnation_counter = 0\n                    if best_f < self.f_opt:\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                else:\n                    # else increase exploration chance\n                    p_levy = min(0.5, p_levy * 1.2 + 0.01)\n\n            # safety clamps\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n            p_levy = float(np.clip(p_levy, 0.0, 0.5))\n\n        # finalize\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.327 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1314903361326495, 0.19314814949960668, 0.5698946431850525, 0.32719041867925946, 0.21945730106859407, 0.9592741913737337, 0.22959182504439568, 0.23471585039584564, 0.21027873952247378, 0.1987966439702501]}, "task_prompt": ""}
{"id": "85889533-26d9-4cb3-9075-d6ec9617bce4", "fitness": 0.6093553818455093, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid-DE with occasional Lévy jumps and an online trust-region — combines jDE-like parameter adaptation, Cauchy/Lévy long jumps for global exploration and a shrinking/expanding trust-region local search around the best for focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent successes (jDE-like).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # default population size scales with dim but not larger than budget/5\n        if pop_size is None:\n            default = max(8, 4 * self.dim)\n            self.pop_size = int(min(default, max(4, self.budget // 5)))\n        else:\n            self.pop_size = int(pop_size)\n            # ensure reasonable bounds\n            self.pop_size = max(4, min(self.pop_size, max(4, self.budget)))\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Attempt to read bounds from func; otherwise assume [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # normalize bounds to correct shapes\n        def _ensure_array_bounds(b):\n            b = np.asarray(b, dtype=float)\n            if b.size == 1:\n                return np.full(self.dim, b.item(), dtype=float)\n            if b.size == self.dim:\n                return b.astype(float)\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n\n        lb = _ensure_array_bounds(lb)\n        ub = _ensure_array_bounds(ub)\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        rng = self.rng\n\n        # initialize population within bounds\n        pop_size = self.pop_size\n        pop = rng.rand(pop_size, self.dim) * (ub - lb) + lb\n        fvals = np.full(pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate as many initial individuals as budget allows\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            try:\n                f = float(func(x))\n            except Exception:\n                # if evaluation fails, set to inf\n                f = np.inf\n            fvals[i] = f\n            evals += 1\n\n        # if we couldn't evaluate any candidate, return infinite\n        if evals == 0 or not np.isfinite(fvals).any():\n            self.f_opt = float(np.min(fvals) if fvals.size else np.inf)\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # determine current best\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6       # mutation factor mean\n        CR_mean = 0.9      # crossover probability mean\n        p_levy = 0.06      # base probability of a Lévy jump (adapted by stagnation)\n        step_scale = 0.15  # base amplitude for Levy steps (fraction of range)\n        trust_radius = 0.2 # initial trust radius (fraction of normalized range)\n        max_trust = 2.0\n        min_trust = 1e-3\n\n        stagnation_counter = 0\n        stagnation_limit = max(20, 5 * self.dim)  # when to trigger stronger diversification\n\n        generation = 0\n        # keep a small success buffer for parameter adaptation\n        success_F = []\n        success_CR = []\n\n        # helper: limited Cauchy-based \"Levy-like\" step\n        def levy_step_vector(scale_fraction=1.0):\n            # draw from standard Cauchy, limit extreme values\n            s = rng.standard_cauchy(size=self.dim)\n            # limit outliers to a finite range and squash extremes\n            s = np.clip(s, -10.0, 10.0)\n            # squash heavy tails smoothly and scale\n            l = s / (1.0 + np.abs(s))\n            return l * step_scale * scale_fraction\n\n        # Main loop: iterate until budget exhausted\n        while evals < self.budget:\n            generation += 1\n            successes = 0\n            success_F.clear()\n            success_CR.clear()\n\n            # adapt per-generation randomization around F_mean and CR_mean\n            for i in range(pop_size):\n                if evals >= self.budget:\n                    break\n\n                # generate per-individual Fi and CRi (jDE-like randomness)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # decide whether to perform a Levy-global jump (centered on best)\n                if rng.rand() < p_levy:\n                    # Levy jump around best (global exploration)\n                    lv = levy_step_vector(scale_fraction=1.0 + trust_radius)\n                    candidate = best_x + lv * range_vec\n                    # small extra local perturbation to keep diversity\n                    if rng.rand() < 0.2:\n                        candidate += rng.normal(0, 0.02, size=self.dim) * range_vec\n                else:\n                    # Differential Evolution current-to-best/1 mutation (diverse and directed)\n                    idxs = np.arange(pop_size)\n                    # ensure r1,r2,r3 distinct and not i\n                    choices = idxs[idxs != i]\n                    if choices.size < 3:\n                        r1 = r2 = r3 = choices[0]\n                    else:\n                        r1, r2, r3 = rng.choice(choices, 3, replace=False)\n                    mutant = pop[r1].copy() + Fi * (pop[r2] - pop[r3]) + 0.2 * Fi * (best_x - pop[i])\n                    # binomial crossover\n                    cross = rng.rand(self.dim) < CRi\n                    # ensure at least one component from mutant\n                    if not cross.any():\n                        cross[rng.randint(0, self.dim)] = True\n                    candidate = np.where(cross, mutant, pop[i])\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # one evaluation (respect budget)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n                    # update best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                    else:\n                        # small improvement in population but not global\n                        stagnation_counter += 0\n                else:\n                    stagnation_counter += 1 if successes == 0 else 0\n\n                # small on-the-fly adaptation nudges (jDE style - moving means slightly)\n                # implemented after the inner loop for stability\n\n                # If budget exhausted, break\n                if evals >= self.budget:\n                    break\n\n            # End of generation: adapt F_mean and CR_mean based on successes\n            if success_F:\n                # Lehmer-like emphasis on larger successful F\n                lm_num = np.sum(np.square(success_F))\n                lm_den = np.sum(success_F) + 1e-12\n                F_lehmer = lm_num / lm_den\n                F_mean = 0.85 * F_mean + 0.15 * np.clip(F_lehmer, 0.05, 1.0)\n                CR_mean = 0.85 * CR_mean + 0.15 * np.clip(np.mean(success_CR), 0.0, 1.0)\n                # Slightly reduce probability of Levy jumps on success (focus on exploitation)\n                p_levy = max(0.01, p_levy * (0.98))\n            else:\n                # no successes this generation => increase exploration\n                p_levy = min(0.35, p_levy * 1.08)\n                # nudge F_mean and CR_mean to encourage larger steps\n                F_mean = np.clip(F_mean * 1.02, 0.05, 1.0)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n\n            # trust-region local search occasionally, more often when few successes\n            do_local = (generation % 3 == 0) or (successes == 0)\n            if do_local and evals < self.budget:\n                remaining = self.budget - evals\n                # sample a small handful scaled by dim and remaining budget\n                local_samples = int(min(6, remaining, max(1, 2 + self.dim // 8)))\n                local_success = False\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                    sigma = (0.4 + rng.rand(self.dim) * 0.6) * trust_radius\n                    candidate = best_x + rng.normal(0.0, sigma, size=self.dim) * range_vec\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    try:\n                        f_candidate = float(func(candidate))\n                    except Exception:\n                        f_candidate = np.inf\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        local_success = True\n                        stagnation_counter = 0\n                        # shrink trust region for focused search\n                        trust_radius = max(min_trust, trust_radius * 0.6)\n                        # add to population by replacing worst\n                        worst_idx = int(np.argmax(fvals))\n                        pop[worst_idx] = candidate\n                        fvals[worst_idx] = f_candidate\n                        break\n                if not local_success:\n                    # expand trust a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # If long stagnation, perform partial re-seeding and stronger Lévy exploration\n            if stagnation_counter >= stagnation_limit and evals < self.budget:\n                stagnation_counter = 0\n                # perform a partial reinitialization: replace half of population with new samples\n                n_replace = max(1, pop_size // 2)\n                indices = rng.choice(np.arange(pop_size), n_replace, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    # combine Levy jump around best and random uniform to diversify\n                    if rng.rand() < 0.6:\n                        lv = levy_step_vector(scale_fraction=1.2 + rng.rand())\n                        candidate = best_x + lv * range_vec\n                    else:\n                        candidate = rng.rand(self.dim) * range_vec + lb\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    try:\n                        f_new = float(func(candidate))\n                    except Exception:\n                        f_new = np.inf\n                    evals += 1\n                    pop[j] = candidate\n                    fvals[j] = f_new\n                    if f_new < best_f:\n                        best_f = f_new\n                        best_x = candidate.copy()\n                # after reseed, relax trust radius a bit\n                trust_radius = min(max_trust, trust_radius * 1.3)\n                # encourage exploration for a while\n                p_levy = min(0.5, p_levy * 1.2)\n\n            # keep trust radius within bounds\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # quick sanity: update best from population in case some replacements happened\n            valid_idx = np.where(np.isfinite(fvals))[0]\n            if valid_idx.size:\n                local_best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n                if fvals[local_best_idx] < best_f:\n                    best_f = float(fvals[local_best_idx])\n                    best_x = pop[local_best_idx].copy()\n\n            # break if budget exhausted\n            if evals >= self.budget:\n                break\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.609 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14013027981800596, 0.39800014056781396, 0.8465749261781008, 0.9521685092849616, 0.8933103186514342, 0.9194093707382118, 0.2934667223920865, 0.5889879751635905, 0.8958593021913251, 0.16564627346956196]}, "task_prompt": ""}
{"id": "de38c3d8-09d8-47b0-8dc4-d0ca64d94d60", "fitness": 0.26941035159403015, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy jumps for long-range escapes, and an adaptive trust-region local search with online step-size and operator adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and an adaptive trust-region local search around the current best.\n    Parameters are adapted online based on recent success history.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: scale with dim, but never larger than budget/4\n        if pop_size is None:\n            default = max(6, 6 * self.dim)\n            self.pop_size = min(default, max(4, self.budget // 4))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # handle bounds: func.bounds.lb / ub may be scalars or arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # broadcast to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 1.0\n\n        # bookkeeping\n        evals = 0\n        pop_size = max(4, self.pop_size)\n        pop = lb + rng.rand(pop_size, self.dim) * range_vec\n        fvals = np.full(pop_size, np.inf)\n        # initial evaluation of population until budget exhausted\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                fvals[i] = np.inf\n            evals += 1\n\n        # if no evaluations possible, return infinite\n        if evals == 0:\n            return np.inf, None\n\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapt online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump (Cauchy)\n        step_scale = 0.25   # base scale for Lévy and trust radius relative to range\n        trust_radius = 0.2 * range_norm\n        max_trust = 10.0 * range_norm\n\n        stagnation_counter = 0\n        global_success_window = []  # record recent successes to adapt means\n\n        # helper: Levy-like heavy-tailed step using Cauchy\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # scale by trust radius and by a randomized factor\n            factor = (step_scale * (0.25 + rng.rand() * 1.75)) * (trust_radius / max(1e-12, range_norm))\n            return s * factor * range_vec\n\n        # Main loop: asynchronous DE-like iterations (one eval per candidate)\n        generation = 0\n        while evals < self.budget:\n            generation += 1\n            successes = 0\n            gen_improvements = 0\n            # process each population member (target-to-trial)\n            for i in range(pop_size):\n                if evals >= self.budget:\n                    break\n\n                # pick three distinct indices != i\n                idxs = np.arange(pop_size)\n                idxs = idxs[idxs != i]\n                if idxs.size < 3:\n                    # Not enough individuals, fallback: random perturb of best\n                    r1 = r2 = r3 = best_idx\n                else:\n                    r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n\n                used_levy = False\n                Fi = None\n                CRi = None\n\n                if rng.rand() < p_levy:\n                    # Lévy centered around best for exploration\n                    used_levy = True\n                    step = levy_step()\n                    # center around best and add a scaled random factor\n                    donor = best_x + step * (0.5 + rng.rand() * 1.0)\n                    trial = donor\n                else:\n                    # DE/rand/1 mutation with jDE-like per-individual Fi/CRi\n                    Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    # ensure at least one dimension crosses over\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate candidate if budget remains\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                replaced = False\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    replaced = True\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    gen_improvements += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # adapt means slightly toward successful Fi/CRi if DE branch\n                if not used_levy and Fi is not None and CRi is not None:\n                    # update running means: small learning rate\n                    alpha = 0.08\n                    if replaced:\n                        F_mean = (1 - alpha) * F_mean + alpha * Fi\n                        CR_mean = (1 - alpha) * CR_mean + alpha * CRi\n                    else:\n                        # small drift to encourage exploration when unsuccessful\n                        F_mean = (1 - 0.01) * F_mean + 0.01 * Fi\n                        CR_mean = (1 - 0.01) * CR_mean + 0.01 * CRi\n\n                # if we used a Levy step and it improved, reward decreasing p_levy a bit\n                if used_levy:\n                    if replaced:\n                        p_levy = max(0.01, p_levy * 0.9)\n                    else:\n                        # slight increase if not helpful (encourage more escapes)\n                        p_levy = min(0.6, p_levy * 1.02 + 0.005)\n\n                # record for generation-wide adaptation\n                global_success_window.append(1 if replaced else 0)\n                # cap window size\n                if len(global_success_window) > 200:\n                    global_success_window.pop(0)\n\n                # break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation: trust-region local search around best\n            remaining = self.budget - evals\n            # choose a small number of local samples based on dim and remaining budget\n            n_local = min(max(1, self.dim // 2), max(0, remaining))\n            if n_local > 0:\n                # anisotropic sigma scaled by trust radius and range\n                sigma_base = (0.3 + rng.rand(self.dim) * 0.9) * (trust_radius / max(1e-12, range_norm))\n                for _ in range(n_local):\n                    if evals >= self.budget:\n                        break\n                    # gaussian perturbation in normalized space times range_vec\n                    cand = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma_base * range_vec\n                    cand = np.minimum(np.maximum(cand, lb), ub)\n                    try:\n                        f_cand = float(func(cand))\n                    except Exception:\n                        f_cand = np.inf\n                    evals += 1\n                    # accept if better than worst or better than current best (greedy)\n                    worst_idx = int(np.argmax(fvals))\n                    if f_cand < fvals[worst_idx]:\n                        pop[worst_idx] = cand\n                        fvals[worst_idx] = f_cand\n                    if f_cand < best_f:\n                        best_f = f_cand\n                        best_x = cand.copy()\n                        # successful local exploitation -> shrink trust region\n                        trust_radius = max(1e-12, trust_radius * 0.7)\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful local search tends to expand to try escape\n                        trust_radius = min(max_trust, trust_radius * 1.06)\n\n            # generation-level adaptation based on success ratio\n            recent = np.array(global_success_window[-min(len(global_success_window), 50):], dtype=float)\n            success_rate = recent.mean() if recent.size > 0 else 0.0\n            if success_rate > 0.15:\n                # good progress -> exploit more\n                p_levy = max(0.01, p_levy * 0.95)\n                F_mean = np.clip(F_mean * 0.98, 0.05, 0.99)\n            else:\n                # little progress -> explore more\n                p_levy = min(0.6, p_levy * 1.04 + 0.005)\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n\n            # stagnation handling: if no improvement for long, perform partial re-seed\n            if stagnation_counter > max(50, 10 * self.dim):\n                k = max(1, pop_size // 2)\n                # reinitialize k individuals randomly (prefer those with poor fitness)\n                sorted_idxs = np.argsort(fvals)  # asc\n                # choose among the worst half\n                worst_half = sorted_idxs[pop_size // 2 :]\n                # ensure we pick k distinct indices\n                if worst_half.size >= k:\n                    pick = rng.choice(worst_half, k, replace=False)\n                else:\n                    pick = rng.choice(np.arange(pop_size), k, replace=False)\n                for j in pick:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # increase trust radius to escape basin\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # slightly increase exploration chance\n                p_levy = min(0.7, p_levy * 1.2 + 0.02)\n\n            # small safeguard to keep trust radius in bounds\n            trust_radius = np.clip(trust_radius, 1e-12, max_trust)\n\n            # If remaining budget is tiny, break\n            if self.budget - evals <= 0:\n                break\n\n        # final return\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.269 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12980890819522306, 0.1621103672326023, 0.3231856854784033, 0.3727422660114589, 0.31105751704436746, 0.36565467348154734, 0.27398063063829137, 0.29597170521340577, 0.24468952980534098, 0.2149022328396607]}, "task_prompt": ""}
{"id": "462f4969-d45f-4d8a-8ed4-93716c7abbaa", "fitness": 0.18709285348350432, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a shrinking/expanding trust-region local search with online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history. Designed for continuous\n    box-constrained problems (e.g. BBOB-like functions).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: scale with dim but limited by budget\n        if pop_size is None:\n            self.pop_size = int(max(4, min(50, 6 * self.dim, max(4, self.budget // 20))))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.default_rng(seed)\n        # best-so-far\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like bounds and broadcast to dim vector\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full((self.dim,), float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # safety: if bounds are None or invalid, fallback to [-5,5]\n        if np.any(~np.isfinite(lb)) or np.any(~np.isfinite(ub)) or np.any(ub <= lb):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # initialize population\n        pop = self.rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Evaluate initial population sequentially as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                f = float(func(pop[i]))\n            except Exception:\n                # if evaluation fails, keep it as inf and continue\n                f = np.inf\n            fvals[i] = f\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = pop[i].copy()\n\n        # If no evaluation was possible, return\n        if evals == 0:\n            return self.f_opt, self.x_opt\n\n        # Hyperparameters and adaptation state\n        F_mean = 0.6\n        CR_mean = 0.5\n        p_levy = 0.05\n        stagnation_counter = 0\n\n        # trust-region radius expressed in absolute space (same scale as x)\n        bound_range = (ub - lb)\n        avg_range = float(np.mean(bound_range))\n        trust_radius = max(1e-6, 0.2 * avg_range)  # start moderately large\n        min_trust = 1e-6 * avg_range\n        max_trust = avg_range * 2.0\n\n        # Levy step helper using Cauchy (heavy-tailed) scaled by levy_scale\n        def levy_step():\n            # generate heavy-tailed step per-dimension, clipped to avoid extremes\n            levy_scale = 0.5 * trust_radius  # scale with trust radius for dynamic behavior\n            step = self.rng.standard_cauchy(self.dim) * levy_scale\n            step = np.clip(step, -1e3, 1e3)\n            return step\n\n        # Main generation loop\n        while evals < self.budget:\n            successful_F = []\n            successful_CR = []\n            successes = 0\n\n            # iterate over population members (one-by-one to count evaluations)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample individual control parameters (jDE-like Gaussian perturbation)\n                Fi = float(np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # decide between global Lévy jump and DE mutation\n                if self.rng.random() < p_levy:\n                    # Lévy jump around current best (exploration)\n                    if self.x_opt is None:\n                        center = pop[i]\n                    else:\n                        center = self.x_opt\n                    step = levy_step()\n                    candidate = center + step\n                else:\n                    # DE/rand/1/bin-like mutation + binomial crossover\n                    idxs = np.arange(self.pop_size)\n                    # ensure at least 3 other indices exist\n                    if self.pop_size >= 4:\n                        # pick r1,r2,r3 distinct and not i\n                        choices = np.delete(idxs, i)\n                        r1, r2, r3 = self.rng.choice(choices, 3, replace=False)\n                    else:\n                        # fallback small-pop strategy: use random perturbation\n                        r1 = r2 = r3 = i\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = self.rng.random(self.dim) < CRi\n                    jrand = self.rng.integers(0, self.dim)\n                    cr_mask[jrand] = True\n                    candidate = np.where(cr_mask, donor, pop[i])\n\n                # project to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    successful_F.append(Fi)\n                    successful_CR.append(CRi)\n                    # move means a bit toward successful params (online adaptation)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # update global best\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = candidate.copy()\n                        stagnation_counter = 0\n                else:\n                    # unsuccessful\n                    stagnation_counter += 1\n\n                # early exit if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # Trust-region local search around current best (focused exploitation)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            local_successes = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                if self.x_opt is None:\n                    center = self.rng.uniform(lb, ub)\n                else:\n                    center = self.x_opt\n                # anisotropic sigma: base trust_radius scaled randomly per-dimension\n                sigma = trust_radius * (0.5 + self.rng.random(self.dim))\n                candidate = center + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    local_successes += 1\n                    stagnation_counter = 0\n                    # successful local step -> shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                else:\n                    # unsuccessful local attempt -> slightly expand to escape\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # If exploitation was very successful in generation, nudge parameters toward exploitation\n            if local_successes + successes > max(1, 0.15 * self.pop_size):\n                p_levy = max(0.005, p_levy * 0.92)\n                F_mean = max(0.05, 0.95 * F_mean)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # few successes => encourage exploration\n                p_levy = min(0.5, p_levy * 1.05)\n\n            # If too much stagnation, increase exploration aggressively\n            if stagnation_counter > max(20, self.dim * 2):\n                p_levy = min(0.8, p_levy * 1.2)\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # slightly randomize means to attempt escapes\n                F_mean = np.clip(F_mean + self.rng.normal(0.0, 0.05), 0.05, 1.0)\n                CR_mean = np.clip(CR_mean + self.rng.normal(0.0, 0.05), 0.0, 1.0)\n\n            # Strong stagnation reset: reseed part of population\n            if stagnation_counter > max(50, self.dim * 5) and evals < self.budget:\n                n_reseed = max(1, self.pop_size // 2)\n                reseed_idx = self.rng.choice(self.pop_size, n_reseed, replace=False)\n                for idx in reseed_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[idx] = self.rng.uniform(lb, ub)\n                    try:\n                        fvals[idx] = float(func(pop[idx]))\n                    except Exception:\n                        fvals[idx] = np.inf\n                    evals += 1\n                    if fvals[idx] < self.f_opt:\n                        self.f_opt = fvals[idx]\n                        self.x_opt = pop[idx].copy()\n                        stagnation_counter = 0\n                # enlarge trust radius a bit after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # cool down stagnation counter to avoid immediate repeat\n                stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.187 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09320448779270851, 0.15564255404885596, 0.301521388768379, 0.17287246808116596, 0.22408516391587607, 0.1787208570717569, 0.19780384090168546, 0.19061369696740416, 0.21385409575471526, 0.14260998153249616]}, "task_prompt": ""}
{"id": "0462c06d-4bcc-4d41-8646-321d91924f88", "fitness": 0.5509971291845145, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy-flight global jumps and an online-adaptive trust-region local search to balance fast exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population size scales with dim but bounded not to exceed budget/4\n            self.pop_size = max(6, min(40, int(6 + 2 * self.dim)))\n            self.pop_size = min(self.pop_size, max(2, self.budget // 4))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # broadcast scalar or array to array of length dim\n        b_arr = np.asarray(b)\n        if b_arr.size == 1:\n            return np.repeat(float(b_arr), self.dim)\n        if b_arr.size == self.dim:\n            return b_arr.astype(float)\n        # try to broadcast any shape to (dim,)\n        return np.broadcast_to(b_arr.flat[0], self.dim).astype(float)\n\n    def __call__(self, func):\n        # RNG\n        rng = np.random.RandomState(self.seed)\n\n        # bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # BBOB tasks are between -5 and 5 normally, but honor func bounds\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        range_vec = ub - lb\n        # avoid zero ranges\n        range_vec[range_vec == 0.0] = 1.0\n\n        # initialize population uniformly\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # evaluate initial population (careful with budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = func(x)\n            evals += 1\n            pop_f[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # if we didn't manage to evaluate full population due to tiny budget,\n        # set remaining individuals randomly without evaluation (they won't be used)\n        if evals < self.budget:\n            # fill rest with random but unevaluated; they'll be used only if budget permits further evals\n            start = np.where(np.isfinite(pop_f) == False)[0]\n            if start.size > 0:\n                # nothing to do but ensure they are valid individuals\n                for idx in start:\n                    pop[idx] = lb + rng.rand(self.dim) * range_vec\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6    # mutation factor mean\n        CR_mean = 0.3   # crossover probability mean\n        p_levy = 0.05   # base chance of performing Levy jump\n        trust_radius = 0.2  # relative to range (0..1)\n        max_trust = 2.0\n        min_trust = 1e-6\n\n        stagnation_counter = 0\n        best_idx = int(np.argmin(pop_f))\n        best_x = pop[best_idx].copy()\n        best_f = pop_f[best_idx]\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # standard Cauchy (heavy tails). Clip extreme outliers to avoid blow-up.\n            step = rng.standard_cauchy(self.dim)\n            # clip so each dimension step not exceed 10 (absolute units)\n            max_abs = 10.0\n            step = np.clip(step, -max_abs, max_abs)\n            return step\n\n        # main loop: iterate generations until budget exhausted\n        generation = 0\n        # simple history for adaptation\n        success_F = []\n        success_CR = []\n\n        while evals < self.budget:\n            generation += 1\n            successes = 0\n            # adapt per-generation randomization of F and CR around their means\n            # We'll process a full pass trying to improve each individual (subject to budget)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # jDE-like: sample individual F and CR\n                Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0)\n\n                # choose mutation or levy\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    donor = best_x + step * (trust_radius * range_vec)\n                    # small random perturbation to donor using DE/rand/1 style\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size >= 3:\n                        a, b, c = rng.choice(idxs, size=3, replace=False)\n                        donor = pop[a] + Fi * (pop[b] - pop[c])\n                    # ensure donor is valid\n                    donor = np.minimum(np.maximum(donor, lb), ub)\n                    # crossover: biased toward donor\n                    cr_mask = rng.rand(self.dim) < CRi\n                    # ensure at least one dimension from donor\n                    cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                else:\n                    # Differential Evolution mutation: rand/1/bin\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback: random local perturbation\n                        donor = pop[i] + Fi * rng.normal(0, 1.0, size=self.dim) * (trust_radius * range_vec)\n                    else:\n                        a, b, c = rng.choice(idxs, size=3, replace=False)\n                        donor = pop[a] + Fi * (pop[b] - pop[c])\n\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # one evaluation\n                f_trial = func(trial)\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_trial < pop_f[i] or np.isnan(pop_f[i]):\n                    # successful replacement\n                    pop[i] = trial\n                    pop_f[i] = f_trial\n                    successes += 1\n                    stagnation_counter = 0\n                    # move means slightly toward successful parameters\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n                    # update global best\n                    if f_trial < best_f:\n                        best_f = f_trial\n                        best_x = trial.copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                else:\n                    stagnation_counter += 1\n\n                # small safeguard: stop if out of budget\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            # Update F_mean and CR_mean based on successes (simple Lehmer-like update)\n            if len(success_F) > 0:\n                # weighted toward larger Fi if they succeeded\n                mean_F = np.mean(success_F)\n                F_mean = np.clip(0.9 * F_mean + 0.1 * mean_F, 0.05, 0.99)\n                CR_mean = np.clip(0.9 * CR_mean + 0.1 * np.mean(success_CR), 0.0, 0.99)\n            else:\n                # if no successes, slightly diversify\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98 + 0.01, 0.0, 0.99)\n\n            success_F.clear()\n            success_CR.clear()\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample count: small handful scaled by dim and remaining budget\n            local_samples = min(max(1, self.dim // 2), remaining, 5)\n            # anisotropic sigma: base trust_radius scaled by rand per-dimension\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                sigma = trust_radius * (0.5 + rng.rand(self.dim))  # per-dim multiplier\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_c = func(candidate)\n                evals += 1\n                if f_c < best_f:\n                    # successful local step => shrink trust radius to focus\n                    best_f = f_c\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    # nudge global DE parameters toward exploitation\n                    F_mean = np.clip(0.95 * F_mean + 0.05 * 0.5, 0.05, 0.99)\n                    CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.8, 0.0, 0.99)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slightly expand trust region to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # adapt probability of Levy jumps: if many successes, reduce exploration; if stagnating, increase\n            if successes > (0.2 * self.pop_size):\n                p_levy = max(0.005, p_levy * 0.9)\n                # encourage exploitation\n                F_mean = np.clip(0.98 * F_mean + 0.02 * 0.5, 0.05, 0.99)\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population randomly to diversify\n                num_reinit = max(1, self.pop_size // 2)\n                to_reinit = rng.choice(self.pop_size, size=num_reinit, replace=False)\n                for idx in to_reinit:\n                    pop[idx] = lb + rng.rand(self.dim) * range_vec\n                    pop_f[idx] = np.inf  # will be evaluated next generation if budget allows\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # reset some DE parameters mildly\n                F_mean = np.clip(0.6 * F_mean + 0.4 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.6 * CR_mean + 0.4 * 0.3, 0.0, 0.99)\n\n            # safety: keep trust radius in sensible range\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n        # final results\n        self.x_opt = best_x.copy()\n        self.f_opt = float(best_f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.551 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11976964940422519, 0.1831230108576123, 0.8004336634307263, 0.8102478333526841, 0.9202371168081304, 0.9228823025192253, 0.30388626672706354, 0.5306427866992879, 0.7280366742527987, 0.19071198779339127]}, "task_prompt": ""}
{"id": "b2b4d899-f200-41ed-9f6d-45302956d385", "fitness": 0.37244267527865976, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: Differential Evolution with occasional Lévy-flight jumps and an online-adapting trust-region local search for fast global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (population-based global search),\n      - occasional Lévy-flight (Cauchy-based) jumps centered on the best for long-range exploration,\n      - trust-region Gaussian local search around the current best for focused exploitation,\n      - simple online adaptation of F and CR (jDE-like) and strategy mixing based on stagnation.\n\n    Usage:\n      optimizer = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=1)\n      f_best, x_best = optimizer(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size scaled with dimension but limited by budget\n        if pop_size is None:\n            pop_guess = max(8, 6 * self.dim)\n            pop_size = min(pop_guess, max(4, self.budget // 10))\n        self.pop_size = int(max(4, pop_size))\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"Broadcast bounds to proper shape (dim,)\"\"\"\n        b = np.asarray(b, dtype=float)\n        if b.shape == ():\n            return np.full(self.dim, float(b))\n        if b.ndim == 1 and b.size == self.dim:\n            return b.copy()\n        # try broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float).copy()\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to problem dimension.\")\n\n    def __call__(self, func):\n        evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # get bounds from func if available, else default [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        # avoid zero range\n        range_vec[range_vec == 0.0] = 1.0\n        # initialize population\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fitness = np.full(self.pop_size, np.inf)\n\n        # evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            try:\n                f = func(x)\n            except Exception:\n                # fallback: try passing copy\n                f = func(np.array(x))\n            fitness[i] = f\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If we couldn't evaluate entire population due to tiny budget,\n        # remaining individuals remain random and unevaluated (won't be used).\n        # hyper-parameters (adapted online)\n        F_mean = 0.6        # mutation scale mean\n        CR_mean = 0.2       # crossover rate mean\n        p_levy = 0.05       # base probability of doing a Levy jump\n        levy_scale = 0.6    # base scale for Levy jumps (relative to range)\n        trust_radius = 0.12 * np.mean(range_vec)  # initial trust region radius (absolute)\n        trust_shrink = 0.8\n        trust_expand = 1.12\n        stagnation_counter = 0\n        best_f_history = []\n        max_stagnation_for_reset = max(50, 10 * self.dim)\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step(scale=1.0):\n            # use independent Cauchy components (heavy-tailed)\n            # sample standard Cauchy via tan(pi*(u-0.5))\n            u = self.rng.random(self.dim)\n            s = np.tan(np.pi * (u - 0.5))\n            # clip extreme outliers to avoid numerical blow-up but keep heavy tails\n            clip = 1e3\n            s = np.clip(s, -clip, clip)\n            return scale * s\n\n        # main loop: run until budget exhausted\n        gen = 0\n        while evals < self.budget:\n            gen += 1\n            # for simple diversity tracking\n            prev_best_f = self.f_opt\n\n            # per-generation: loop over individuals sequentially (each candidate consumes 1 eval)\n            indices = np.arange(self.pop_size)\n            self.rng.shuffle(indices)\n            for idx in indices:\n                if evals >= self.budget:\n                    break\n\n                target = pop[idx].copy()\n\n                # adapt per-individual parameters (jDE-like)\n                # sample Fi ~ N(F_mean, 0.1) but ensure positive and clipped\n                Fi = np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.2)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                # decide strategy: Levy exploration or DE mutation/crossover\n                do_levy = (self.rng.random() < p_levy)\n                if do_levy:\n                    # Lévy jump centered at best\n                    center = self.x_opt.copy() if self.x_opt is not None else target\n                    step = levy_step(scale=levy_scale)\n                    # scale per-dimension by range_vec and a random factor to avoid deterministic directions\n                    candidate = center + step * (0.5 + self.rng.random(self.dim)) * range_vec\n                    # cap extreme deviations relative to range\n                    max_dev = 8.0 * range_vec\n                    candidate = np.where(candidate > ub, ub, candidate)\n                    candidate = np.where(candidate < lb, lb, candidate)\n                    used_Fi = None  # not used for adaptation\n                else:\n                    # DE/rand/1-like mutation variant using three distinct indices\n                    all_idx = list(range(self.pop_size))\n                    all_idx.remove(idx)\n                    a, b, c = self.rng.choice(all_idx, size=3, replace=False)\n                    donor = pop[a] + Fi * (pop[b] - pop[c])\n                    # binomial crossover\n                    cross = self.rng.random(self.dim) < CRi\n                    # ensure at least one dimension from donor\n                    if not np.any(cross):\n                        cross[self.rng.integers(0, self.dim)] = True\n                    candidate = np.where(cross, donor, target)\n                    # projection to bounds\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    used_Fi = Fi\n\n                # evaluate candidate\n                try:\n                    f_candidate = func(candidate)\n                except Exception:\n                    f_candidate = func(np.array(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fitness[idx]:\n                    # accept\n                    pop[idx] = candidate\n                    fitness[idx] = f_candidate\n                    # small online adaptation of F_mean and CR_mean toward successful Fi/CRi\n                    if used_Fi is not None:\n                        F_mean = 0.95 * F_mean + 0.05 * used_Fi\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    else:\n                        # Levy successful: nudge to encourage more exploration for a while\n                        p_levy = min(0.4, p_levy + 0.01)\n                        # nudge means slightly to encourage diversity\n                        F_mean = min(1.2, F_mean * 1.02)\n                        CR_mean = max(0.05, CR_mean * 0.98)\n                else:\n                    # unsuccessful: slight decay toward baseline\n                    F_mean = 0.995 * F_mean + 0.005 * 0.5\n                    CR_mean = 0.995 * CR_mean + 0.005 * 0.2\n                    # reduce chance of Levy if it didn't help\n                    if do_levy:\n                        p_levy = max(0.01, p_levy * 0.98)\n\n                # update global best\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 0\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation: trust-region local search around current best\n            # number of local samples depends on remaining budget and dimension\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            n_local = min(max(1, self.dim // 2), remaining)\n            local_success = 0\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # anisotropic Gaussian noise scaled by trust_radius and range_vec\n                sigma = trust_radius * (0.5 + self.rng.random(self.dim))\n                candidate = self.x_opt + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                try:\n                    f_candidate = func(candidate)\n                except Exception:\n                    f_candidate = func(np.array(candidate))\n                evals += 1\n                if f_candidate < self.f_opt:\n                    # successful local exploitation: shrink trust region\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    trust_radius = max(1e-12, trust_radius * trust_shrink)\n                    local_success += 1\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful: expand a bit to escape local traps\n                    trust_radius = min(np.max(range_vec) * 2.0, trust_radius * trust_expand)\n\n            # monitor progress and adjust exploration probability\n            best_f_history.append(self.f_opt)\n            if len(best_f_history) > 20:\n                # consider last 20 improvements\n                if min(best_f_history[-20:]) >= prev_best_f - 1e-12:\n                    # no improvement recently\n                    stagnation_counter += 1\n                else:\n                    stagnation_counter = 0\n\n            # if many generations without improvement, increase Levy chance and diversity\n            if stagnation_counter > max(5, self.dim):\n                p_levy = min(0.5, p_levy + 0.02)\n                # slightly nudge population diversity by reinitializing a few individuals\n                n_reseed = max(1, int(0.05 * self.pop_size))\n                for _ in range(n_reseed):\n                    if self.rng.random() < 0.5:\n                        j = self.rng.integers(0, self.pop_size)\n                        pop[j] = lb + self.rng.random(self.dim) * range_vec\n                        fitness[j] = np.inf  # will be replaced/evaluated in next loops\n\n            # strong stagnation reset: re-seed half of population\n            if stagnation_counter > max_stagnation_for_reset:\n                half = self.pop_size // 2\n                for j in range(half):\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    fitness[j] = np.inf\n                trust_radius = min(np.max(range_vec), trust_radius * 1.5)\n                p_levy = min(0.6, p_levy * 1.2)\n                stagnation_counter = 0\n\n            # small safeguard: keep CR_mean and F_mean within sensible bounds\n            F_mean = float(np.clip(F_mean, 0.05, 1.2))\n            CR_mean = float(np.clip(CR_mean, 0.0, 0.95))\n            # If budget extremely small ensure we won't loop too long\n            if evals >= self.budget:\n                break\n\n        # final results\n        # ensure x_opt not None\n        if self.x_opt is None:\n            # fallback: return best in evaluated population\n            if np.isfinite(fitness).any():\n                best_idx = np.nanargmin(fitness)\n                self.f_opt = float(fitness[best_idx])\n                self.x_opt = pop[best_idx].copy()\n            else:\n                # random guess\n                self.x_opt = lb + self.rng.random(self.dim) * range_vec\n                self.f_opt = func(self.x_opt)\n\n        return float(self.f_opt), self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.372 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12115664539541249, 0.1662985474599532, 0.4469682908729533, 0.35764022775377646, 0.7272702520477752, 0.7832187380852551, 0.28367037899140646, 0.37439301043268347, 0.29601465327388254, 0.16779600847349962]}, "task_prompt": ""}
{"id": "1f5452a9-ce3c-451c-8236-8ae8a36b630f", "fitness": 0.2369289240305982, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for global exploration, and a trust-region local search with online step-size and parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population sizing: keep reasonable relative to budget and dim\n        if pop_size is None:\n            # typical DE population size scaled with dimension but limited by budget\n            ps = min(max(4, 4 * self.dim), int(max(4, self.budget // 10)))\n        else:\n            ps = int(pop_size)\n        # Ensure we can evaluate at least a few generations\n        ps = max(4, min(ps, max(4, self.budget // 3)))\n        self.pop_size = ps\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept either scalars or arrays and return array of shape (dim,)\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.repeat(b.item(), self.dim)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting last dimension\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # read bounds from func (Many BBOB style)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_ = ub - lb\n        # safety: if bounds degenerate, set small range\n        range_[range_ == 0] = 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_\n        pop_f = np.full(self.pop_size, np.inf)\n\n        evals = 0\n\n        # Evaluate as many initial individuals as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = func(x)\n            evals += 1\n            pop_f[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If budget exhausted early, return best found\n        if evals >= self.budget:\n            return self.f_opt, self.x_opt\n\n        # Algorithm hyper-parameters (adapted online)\n        F_mean = 0.6       # differential weight mean\n        CR_mean = 0.3      # crossover rate mean\n        tau_F = 0.1        # jDE-like self-adap prob for F\n        tau_CR = 0.1       # jDE-like self-adap prob for CR\n        c_adapt = 0.1      # learning rate for means\n        trust_radius = 0.25  # initial trust region radius in [0,2] relative to range\n        trust_min = 1e-6\n        trust_max = 2.0\n\n        no_improve = 0     # count of evaluations since last global improvement\n        best_since_restart = 0\n\n        base_p_levy = 0.05  # base probability of using a Levy jump\n        p_levy = base_p_levy\n\n        stagnation_reset_threshold = max(100, 20 * self.dim)\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (approximation)\n        def levy_step(scale=1.0):\n            # standard Cauchy (ratio of normals) gives heavy tails; limit extreme outliers\n            s = self.rng.standard_cauchy(self.dim) * scale\n            # clip extreme values to avoid numerical blow-up\n            clip = 10.0\n            s = np.clip(s, -clip, clip)\n            return s\n\n        # main loop: iterate until budget exhausted\n        # we'll perform \"generations\" but count evaluations explicitly\n        gen = 0\n        while evals < self.budget:\n            gen += 1\n\n            # adapt p_levy from stagnation measure:\n            p_levy = min(0.6, base_p_levy + 0.001 * no_improve)\n\n            # per-generation bookkeeping of successful parameter values\n            succ_F = []\n            succ_CR = []\n\n            # iterate through population sequentially (consumes one eval each candidate trial)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                xi = pop[i].copy()\n                fi = pop_f[i]\n\n                # sample Fi and CRi using jDE-like adaptation\n                if self.rng.rand() < tau_F:\n                    Fi = 0.1 + 0.9 * self.rng.rand()\n                else:\n                    Fi = np.clip(F_mean + 0.1 * self.rng.randn(), 0.05, 1.2)\n\n                if self.rng.rand() < tau_CR:\n                    CRi = self.rng.rand()\n                else:\n                    CRi = np.clip(CR_mean + 0.1 * self.rng.randn(), 0.0, 1.0)\n\n                # decide branch: Levy jump (centered on global best) or DE mutation\n                if self.rng.rand() < p_levy:\n                    # Lévy-centered exploration around current best (global)\n                    if self.x_opt is None:\n                        base = lb + self.rng.rand(self.dim) * range_\n                    else:\n                        base = self.x_opt.copy()\n\n                    # scale with dynamic trust radius and global range\n                    scale = trust_radius * range_\n                    # small mixing with current individual\n                    levy = levy_step(scale=1.0) * scale\n                    donor = base + levy * Fi * 0.5\n                    used_branch = \"levy\"\n                else:\n                    # DE/rand/1-like with slight best-guided intensification\n                    idxs = [j for j in range(self.pop_size) if j != i]\n                    r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                    xr1 = pop[r1]\n                    xr2 = pop[r2]\n                    xr3 = pop[r3]\n                    donor = xr1 + Fi * (xr2 - xr3)\n                    # add a small pull toward the best to intensify exploitation occasionally\n                    if self.x_opt is not None and self.rng.rand() < 0.4:\n                        donor = donor + 0.3 * Fi * (self.x_opt - xi)\n                    used_branch = \"de\"\n\n                # binomial crossover\n                jrand = self.rng.randint(self.dim)\n                trial = xi.copy()\n                for d in range(self.dim):\n                    if self.rng.rand() < CRi or d == jrand:\n                        trial[d] = donor[d]\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # one evaluation\n                f_trial = func(trial)\n                evals += 1\n\n                # selection: greedy replacement\n                if f_trial <= fi:\n                    pop[i] = trial\n                    pop_f[i] = f_trial\n                    succ_F.append(Fi)\n                    succ_CR.append(CRi)\n                    # update global best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        no_improve = 0\n                        best_since_restart += 1\n                    else:\n                        no_improve += 1\n                else:\n                    # unsuccessful\n                    no_improve += 1\n\n                # adapt the means toward successful parameters (simple moving average)\n                if len(succ_F) > 0:\n                    F_mean = (1 - c_adapt) * F_mean + c_adapt * np.mean(succ_F)\n                if len(succ_CR) > 0:\n                    CR_mean = (1 - c_adapt) * CR_mean + c_adapt * np.mean(succ_CR)\n\n                # clear temporary lists if they get large for stability\n                if len(succ_F) > 100:\n                    succ_F = succ_F[-20:]\n                if len(succ_CR) > 100:\n                    succ_CR = succ_CR[-20:]\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: a small handful scaled by dim and remaining budget\n            n_local = int(min(max(1, self.dim // 2), remaining, 5 + self.dim // 3))\n            # anisotropic sigma: per-dimension scaling from trust radius\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # generate anisotropic Gaussian noise: multiply by range and trust radius\n                per_dim_scale = trust_radius * range_ * np.exp(self.rng.randn(self.dim) * 0.2)\n                cand = self.x_opt + self.rng.randn(self.dim) * per_dim_scale\n                cand = np.minimum(np.maximum(cand, lb), ub)\n                f_cand = func(cand)\n                evals += 1\n                if f_cand < self.f_opt:\n                    self.f_opt = f_cand\n                    self.x_opt = cand.copy()\n                    # successful local step => tighten trust region\n                    trust_radius = max(trust_min, trust_radius * 0.8)\n                    no_improve = 0\n                    best_since_restart += 1\n                else:\n                    # unsuccessful local step => expand trust radius moderately\n                    trust_radius = min(trust_max, trust_radius * 1.05)\n\n            # If the generation produced many successes, slightly focus (shrink trust radius),\n            # otherwise if stagnating, encourage exploration (increase probability of Levy jumps)\n            if best_since_restart > max(3, self.dim // 2):\n                trust_radius = max(trust_min, trust_radius * 0.9)\n                best_since_restart = 0\n            else:\n                # mild expansion if stagnation observed\n                if no_improve > stagnation_reset_threshold // 4:\n                    trust_radius = min(trust_max, trust_radius * 1.08)\n\n            # strong stagnation reset: re-seed part of population\n            if no_improve >= stagnation_reset_threshold:\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                for idx in self.rng.choice(range(self.pop_size), k, replace=False):\n                    if evals >= self.budget:\n                        break\n                    pop[idx] = lb + self.rng.rand(self.dim) * range_\n                    pop_f[idx] = func(pop[idx])\n                    evals += 1\n                    if pop_f[idx] < self.f_opt:\n                        self.f_opt = pop_f[idx]\n                        self.x_opt = pop[idx].copy()\n                        no_improve = 0\n                # slightly enlarge trust radius after reset\n                trust_radius = min(trust_max, trust_radius * 1.5)\n                # encourage more Levy jumps briefly\n                base_p_levy = min(0.5, base_p_levy + 0.02)\n                no_improve = 0\n                best_since_restart = 0\n\n        # final results\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.237 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11921767498772329, 0.1802406403996597, 0.28294286747454955, 0.35588115155315614, 0.21511805080006974, 0.31035829787986835, 0.254634970893135, 0.24719011282450887, 0.25292108838494143, 0.15078438510837]}, "task_prompt": ""}
{"id": "a9edd0bc-4757-4d33-8aa9-dc52840be53a", "fitness": 0.2695138737311318, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and trust-region local search with online parameter adaptation for robust global-to-local search.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but kept reasonable\n        if pop_size is None:\n            default = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # make sure population not larger than a fraction of budget\n            default = min(default, max(4, self.budget // 20))\n            self.pop_size = int(default)\n        else:\n            self.pop_size = max(2, int(pop_size))\n        # internal state for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # read bounds if provided, else assume [-5, 5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        rng = self.rng\n        range_vec = ub - lb\n        # avoid degenerate ranges\n        range_vec = np.where(range_vec <= 0.0, 1.0, range_vec)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            xi = pop[i].copy()\n            fvals[i] = float(func(xi))\n            evals += 1\n\n        # if no evaluations possible\n        if not np.isfinite(fvals).any():\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # determine initial best among evaluated\n        best_idx = int(np.nanargmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight\n        CR_mean = 0.9       # crossover probability\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius\n        min_trust = 1e-8\n        max_trust = max(1e-8, np.linalg.norm(range_vec) * 2.0)\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # adapt per-generation randomization of F and CR around their means\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi like jDE\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    # scale by trust_radius and range_vec\n                    scale = (step_scale * 0.5 + 0.5 * rng.rand()) * (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                    donor = best_x + scale * step * (range_vec / max(range_vec.mean(), 1e-12))\n                    trial = donor.copy()\n                else:\n                    # DE/rand/1-like mutation\n                    if self.pop_size >= 4:\n                        idxs = np.arange(self.pop_size)\n                        idxs = idxs[idxs != i]\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    else:\n                        # fallback to random perturbation if small population\n                        donor = pop[i] + Fi * (rng.randn(self.dim) * (range_vec))\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # one evaluation (respect budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # move means slightly toward successful parameters\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation adjustments\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining > 0:\n                local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma: base trust_radius scaled per-dimension\n                    sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n                    candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step => shrink trust radius to focus\n                        trust_radius *= 0.85\n                        trust_radius = min(max_trust, max(min_trust, trust_radius))\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful => expand a bit to escape local traps\n                        trust_radius *= 1.05\n                        trust_radius = min(max_trust, max(min_trust, trust_radius))\n                        stagnation_counter += 1\n\n            # if many successes, encourage exploitation; if stagnating increase Lévy jumps\n            if successes > 0:\n                p_levy = max(0.01, p_levy * (0.95 if successes > self.pop_size * 0.2 else 0.98))\n                # move F_mean/CR_mean a bit toward conservative exploitation\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                reinit_idxs = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reinit_idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                # update best after reseed\n                valid = np.isfinite(fvals)\n                if np.any(valid):\n                    best_idx = int(np.nanargmin(fvals))\n                    best_f = float(fvals[best_idx])\n                    best_x = pop[best_idx].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.270 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14327772676507866, 0.18513095553760994, 0.33706491640956315, 0.37294353388524315, 0.24900299624448174, 0.3454825517615854, 0.2861522573889773, 0.31997737651592995, 0.27406876076716113, 0.18203766203568772]}, "task_prompt": ""}
{"id": "4e0be516-0f65-4f7c-88d2-778725a9a5cb", "fitness": 0.2805434812050232, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy jumps for long-range exploration, and an online-adapting trust-region local search for focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension and budget but kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # make sure population not larger than a fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = max(4, int(pop_size))\n        # internal state for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full((self.dim,), float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n\n        evals = 0\n        # evaluate initial population (careful with budget)\n        fvals = np.full(self.pop_size, np.inf)\n        best_f = np.inf\n        best_x = None\n\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n            if f < best_f:\n                best_f = f\n                best_x = x.copy()\n\n        # if we didn't manage to evaluate any candidate, return\n        if not np.isfinite(best_f):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight\n        CR_mean = 0.9       # crossover probability\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # limit extreme outliers to avoid numerical blow-up\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # process population members sequentially; each evaluated candidate consumes one eval.\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # adapt per-individual randomization of F and CR around their means\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n                Fi = float(np.clip(rng.normal(F_mean, 0.15), 0.05, 1.0))\n\n                # create donor vector\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    # scale by range and dynamic trust_radius\n                    scale = (step_scale * 0.5 + 0.5 * rng.rand())\n                    donor = best_x + scale * step * (range_vec / max(range_vec.mean(), 1e-12))\n                    # mix a bit with current individual to keep diversity\n                    mix_mask = rng.rand(self.dim) < CRi\n                    if not np.any(mix_mask):\n                        mix_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(mix_mask, donor, pop[i])\n                else:\n                    # DE/rand/1-like mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    # safe draw of three distinct indices\n                    if idxs.size >= 3:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    else:\n                        # fallback to random picks with replacement if small pop\n                        choices = rng.randint(0, self.pop_size, 3)\n                        r1, r2, r3 = choices\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # move means slightly toward successful parameters\n                    # simple Lehmer-like adaptation for F and CR means\n                    F_mean = np.clip(0.9 * F_mean + 0.1 * Fi, 0.05, 0.99)\n                    CR_mean = np.clip(0.9 * CR_mean + 0.1 * CRi, 0.0, 1.0)\n                    # small successful-local focus: shrink trust slightly\n                    trust_radius = max(min_trust, trust_radius * 0.98)\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            # anisotropic sigma: base trust_radius scaled by rand per-dimension\n            sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / max(np.linalg.norm(range_vec), 1e-12))\n\n            for _ in range(int(local_samples)):\n                if evals >= self.budget:\n                    break\n                candidate = best_x + rng.normal(0.0, 1.0, self.dim) * sigma\n                candidate = np.clip(candidate, lb, ub)\n\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # if many successes, slightly reduce exploration, else increase Lévy chance\n            if successes > 0:\n                # reduce p_levy slowly when making steady progress, but not below floor\n                p_levy = max(0.01, p_levy * (0.98 if successes > (self.pop_size * 0.2) else 0.995))\n                # move F_mean/CR_mean a bit toward exploitation-friendly values\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnating: increase chance of long jumps and diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                # choose indices to reinitialize\n                reidx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reidx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.281 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19762949588069667, 0.23927350237653633, 0.30859980102083884, 0.3510104574235, 0.3640662561792587, 0.35243957519089997, 0.30743325854797743, 0.24493333163475617, 0.2392428800849481, 0.20080625371082006]}, "task_prompt": ""}
{"id": "e7e3ad55-5d56-4fa7-80ce-eb9281c5430e", "fitness": "-inf", "name": "AdaptiveDeLevyTrust", "description": "Adaptive Hybrid DE + Lévy + Trust-Region — jDE-style self-adapting DE with occasional Lévy jumps from the current best and a shrinking/expanding trust-region local search for focused exploitation.", "code": "import numpy as np\nimport math\n\nclass AdaptiveDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (jDE-style),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # population size default heuristic\n        if pop_size is None:\n            ps = int(8 + 2 * np.sqrt(self.dim))\n            ps = max(6, min(80, ps))\n            # do not exceed some fraction of budget\n            ps = min(ps, max(3, self.budget // 10))\n            self.pop_size = max(3, ps)\n        else:\n            self.pop_size = int(pop_size)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        a = np.array(b, dtype=float)\n        if a.size == 1:\n            return np.full(self.dim, a.item(), dtype=float)\n        if a.shape == (self.dim,):\n            return a.astype(float)\n        # try broadcast\n        return np.broadcast_to(a, (self.dim,)).astype(float)\n\n    def _levy_step(self, alpha=1.5, size=1):\n        \"\"\"\n        Mantegna's algorithm for Lévy (alpha-stable) steps (symmetric).\n        Returns array of steps (size, dim) or (dim,) if size==1.\n        \"\"\"\n        if size == 1:\n            u = self.rng.normal(0, 1.0, size=self.dim)\n            v = self.rng.normal(0, 1.0, size=self.dim)\n            # compute sigma_u\n            num = math.gamma(1 + alpha) * math.sin(math.pi * alpha / 2.0)\n            den = math.gamma((1 + alpha) / 2.0) * alpha * 2 ** ((alpha - 1) / 2.0)\n            sigma_u = (num / den) ** (1.0 / alpha)\n            u = u * sigma_u\n            step = u / (np.abs(v) ** (1.0 / alpha))\n            # clip to avoid infinities\n            step = np.clip(step, -1e3, 1e3)\n            return step\n        else:\n            u = self.rng.normal(0, 1.0, size=(size, self.dim))\n            v = self.rng.normal(0, 1.0, size=(size, self.dim))\n            num = math.gamma(1 + alpha) * math.sin(math.pi * alpha / 2.0)\n            den = math.gamma((1 + alpha) / 2.0) * alpha * 2 ** ((alpha - 1) / 2.0)\n            sigma_u = (num / den) ** (1.0 / alpha)\n            u *= sigma_u\n            step = u / (np.abs(v) ** (1.0 / alpha))\n            step = np.clip(step, -1e3, 1e3)\n            return step\n\n    def __call__(self, func):\n        # bounds handling\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # In BBOB the typical bounds may be -5..5 but code is generic\n        range_vec = ub - lb\n        # fallback if range_vec has zeros\n        range_vec = np.where(range_vec == 0.0, 1.0, range_vec)\n\n        rng = self.rng\n        pop_size = self.pop_size\n        dim = self.dim\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(pop_size, dim) * range_vec\n        fvals = np.full(pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population until budget exhausted or all evaluated\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # If we couldn't evaluate any individual (tiny budget) fall back to random search\n        if not np.isfinite(fvals).any():\n            # budget might be 0 - nothing to do\n            if self.budget <= 0:\n                self.f_opt = np.inf\n                self.x_opt = None\n                return self.f_opt, self.x_opt\n            # do simple random sampling using up remaining budget\n            best_f = np.inf\n            best_x = None\n            while evals < self.budget:\n                x = lb + rng.rand(dim) * range_vec\n                f = float(func(x))\n                evals += 1\n                if f < best_f:\n                    best_f = f\n                    best_x = x.copy()\n            self.f_opt = float(best_f)\n            self.x_opt = best_x\n            return self.f_opt, self.x_opt\n\n        # determine initial best\n        best_idx = np.nanargmin(fvals)\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # jDE-like per-individual F and CR arrays\n        F_mean = 0.6\n        CR_mean = 0.9\n        tau1 = 0.1  # prob to change F\n        tau2 = 0.1  # prob to change CR\n        Fi_array = np.full(pop_size, F_mean, dtype=float)\n        CRi_array = np.full(pop_size, CR_mean, dtype=float)\n\n        # Levy parameters\n        p_levy = 0.06  # initial probability of performing Lévy jump for an individual\n        step_scale = 0.8  # scaling factor of levy steps relative to range_vec\n\n        # trust-region parameters\n        trust_radius = 0.25 * np.linalg.norm(range_vec)  # absolute scale\n        max_trust = 2.0 * np.linalg.norm(range_vec)\n        min_trust = 1e-6 * np.linalg.norm(range_vec)\n\n        stagnation_counter = 0\n        max_stagn = max(50, dim * 8)\n\n        generation = 0\n        # bookkeeping for adaptation\n        success_history_F = []\n        success_history_CR = []\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            generation += 1\n            successes = 0\n            gen_success_F = []\n            gen_success_CR = []\n\n            # iterate over population sequentially (target-to-target)\n            for i in range(pop_size):\n                if evals >= self.budget:\n                    break\n\n                # adapt Fi, CRi per jDE rules\n                if rng.rand() < tau1:\n                    Fi = np.clip(rng.normal(F_mean, 0.2), 0.05, 1.0)\n                    Fi_array[i] = Fi\n                else:\n                    Fi = Fi_array[i]\n                if rng.rand() < tau2:\n                    CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                    CRi_array[i] = CRi\n                else:\n                    CRi = CRi_array[i]\n\n                # choose between DE mutation or Levy jump\n                if rng.rand() < p_levy:\n                    # Levy jump centered on best: heavy-tail step then small random mixing\n                    step = self._levy_step(alpha=1.5, size=1)\n                    # combine with Gaussian to avoid pure outliers and scale to range\n                    scale = step_scale * (0.5 + 0.5 * rng.rand())\n                    donor = best_x + scale * step * (range_vec / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                    # also mix with current vector slightly\n                    donor = 0.3 * pop[i] + 0.7 * donor\n                else:\n                    # DE/rand/1 mutation excluding target i\n                    idxs = np.arange(pop_size)\n                    # ensure we have enough indices to pick distinct ones\n                    if pop_size >= 4:\n                        choices = rng.choice(idxs[idxs != i], 3, replace=False)\n                    else:\n                        # small population fallback: allow replacement but avoid i\n                        choices = rng.choice(idxs[idxs != i], 3, replace=True)\n                    r1, r2, r3 = choices\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover to produce trial\n                    cr_mask = rng.rand(dim) < CRi\n                    # ensure at least one component\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    donor = trial\n\n                # enforce bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate (ensure not exceeding budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy (replace parent if better)\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    gen_success_F.append(Fi)\n                    gen_success_CR.append(CRi)\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                else:\n                    # nothing replaced; small stagnation increment\n                    stagnation_counter += 1\n\n                # occasional replacement of worst by a promising Levy from best to keep diversity\n                if rng.rand() < 0.01 and evals < self.budget:\n                    candidate2 = best_x + 0.5 * self._levy_step(alpha=1.2, size=1) * (range_vec / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                    candidate2 = np.clip(candidate2, lb, ub)\n                    f_candidate2 = float(func(candidate2))\n                    evals += 1\n                    # replace worst if better\n                    worst_idx = np.argmax(fvals)\n                    if f_candidate2 < fvals[worst_idx]:\n                        pop[worst_idx] = candidate2\n                        fvals[worst_idx] = f_candidate2\n                        if f_candidate2 < best_f:\n                            best_f = f_candidate2\n                            best_x = candidate2.copy()\n                            stagnation_counter = 0\n\n                # stop if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # end of population loop: adaptation of F_mean and CR_mean from successful parameters (Lehmer-like)\n            if len(gen_success_F) > 0:\n                # Lehmer mean favored: sum(F^2)/sum(F)\n                num = sum([f * f for f in gen_success_F])\n                den = sum(gen_success_F)\n                if den > 0:\n                    F_new = num / den\n                    # move mean slightly toward the new value\n                    F_mean = 0.85 * F_mean + 0.15 * np.clip(F_new, 0.05, 1.0)\n                # CR mean simple average\n                CR_mean = 0.85 * CR_mean + 0.15 * (sum(gen_success_CR) / len(gen_success_CR))\n            else:\n                # no success: slightly perturb means to increase exploration\n                F_mean = np.clip(F_mean * (1.0 + 0.01 * rng.randn()), 0.05, 1.0)\n                CR_mean = np.clip(CR_mean * (1.0 + 0.01 * rng.randn()), 0.0, 1.0)\n\n            # Trust-region local search: sample a few Gaussian neighbors around best_x\n            # number of local samples scaled by dimension and remaining budget\n            remaining = self.budget - evals\n            local_samples = min(max(1, dim // 2), remaining, 1 + dim // 4)\n            improved_local = False\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma using trust_radius relative to range\n                sigma = (rng.rand(dim) * 0.6 + 0.4) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    improved_local = True\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # adjust trust radius: shrink on improvement, expand slightly on failures\n            if improved_local:\n                trust_radius = max(min_trust, trust_radius * 0.7)\n            else:\n                trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # adjust levy probability according to recent success/stagnation\n            if successes > max(1, pop_size // 6):\n                # many successes -> favor exploitation\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                # few successes -> increase exploration\n                p_levy = min(0.5, p_levy * 1.06 + 0.005)\n\n            # strong stagnation counter: re-seed part of population\n            if stagnation_counter > max_stagn:\n                k = max(1, pop_size // 2)\n                for j in rng.choice(np.arange(pop_size), k, replace=False):\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust region to escape\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n            # generation end - safety: if any budget remains but best_f is np.inf (shouldn't happen) do a few random tries\n            if np.isinf(best_f) and evals < self.budget:\n                x = lb + rng.rand(dim) * range_vec\n                f = float(func(x))\n                evals += 1\n                if f < best_f:\n                    best_f = f\n                    best_x = x.copy()\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 48, in _levy_step, the following error occurred:\nNameError: name 'math' is not defined\nOn line: num = math.gamma(1 + alpha) * math.sin(math.pi * alpha / 2.0)", "error": "In the code, line 48, in _levy_step, the following error occurred:\nNameError: name 'math' is not defined\nOn line: num = math.gamma(1 + alpha) * math.sin(math.pi * alpha / 2.0)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4aab999a-bae5-4eb9-a98f-e862abfa3b44", "fitness": 0.2124944944131137, "name": "AdaptiveHybridDELevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution (self-adaptive F/CR), occasional Lévy-flight long jumps, and a trust-region local search around the best solution with online adaptation of parameters.", "code": "import numpy as np\n\nclass AdaptiveHybridDELevyTrust:\n    \"\"\"\n    Adaptive hybrid optimizer:\n      - Differential Evolution with per-individual adaptive F and CR (jDE-like)\n      - Occasional Lévy-like Cauchy jumps from the current best for long-range exploration\n      - Trust-region Gaussian local search around the best for focused exploitation\n      - Online adaptation of F_mean, CR_mean, Lévy probability and trust radius\n    Interface:\n      obj = AdaptiveHybridDELevyTrust(budget=10000, dim=10, pop_size=None, seed=None)\n      f_opt, x_opt = obj(func)   # func must expose func.bounds.lb and func.bounds.ub\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size rule if not provided\n        if pop_size is None:\n            self.pop_size = max(6, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting\n        return np.broadcast_to(b.flatten(), (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # safety: if bounds are degenerate, set default [-5,5]\n        if np.any(ub <= lb):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        norm_range = np.linalg.norm(range_vec)\n        if norm_range == 0:\n            range_vec = np.full(self.dim, 10.0)\n            norm_range = np.linalg.norm(range_vec)\n\n        # initialization\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # evaluate initial population as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(np.clip(x, lb, ub)))\n            fvals[i] = f\n            evals += 1\n\n        # if we couldn't evaluate any, evaluate a single random point\n        if evals == 0:\n            x0 = lb + self.rng.rand(self.dim) * range_vec\n            f0 = float(func(x0))\n            evals = 1\n            self.f_opt = f0\n            self.x_opt = x0.copy()\n            return self.f_opt, self.x_opt\n\n        # best initialization\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n\n        # algorithm hyperparameters and adaptive state\n        p_levy = 0.08\n        trust_radius = 0.2 * norm_range\n        min_trust = 1e-8\n        max_trust = norm_range * 2.0\n\n        # DE adaptation defaults\n        F_mean = 0.6\n        CR_mean = 0.9\n        F_min, F_max = 0.05, 0.99\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: Levy-like heavy-tailed step using standard Cauchy\n        def levy_step(scale=1.0):\n            # standard cauchy provides heavy tails; clip extremes\n            step = self.rng.standard_cauchy(self.dim)\n            # prevent extremely large outliers that blow up evaluations\n            step = np.clip(step, -50.0, 50.0)\n            # scale with per-dimension range and provided scale\n            return step * (range_vec / (norm_range + 1e-12)) * scale\n\n        # main loop: iterate until budget exhausted\n        # We'll perform per-individual DE updates in a generational-like pass\n        while evals < self.budget:\n            gen += 1\n            successful_F = []\n            successful_CR = []\n            gen_improved = False\n\n            # shuffle order to reduce bias\n            order = self.rng.permutation(self.pop_size)\n\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi in a jDE-like manner\n                if self.rng.rand() < 0.1:\n                    Fi = self.rng.uniform(0.4, 0.95)\n                else:\n                    Fi = np.clip(F_mean + 0.1 * self.rng.randn(), F_min, F_max)\n                if self.rng.rand() < 0.1:\n                    CRi = self.rng.rand()\n                else:\n                    CRi = np.clip(CR_mean + 0.1 * self.rng.randn(), 0.0, 1.0)\n\n                # choose whether to do a Lévy jump (global exploration) or DE operator\n                if self.rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    scale = 0.8 + 1.2 * self.rng.rand()  # scale multiplier\n                    candidate = best_x + levy_step(scale=scale)\n                    # sometimes combine with a random direction nudging\n                    if self.rng.rand() < 0.3:\n                        candidate += (self.rng.rand(self.dim) - 0.5) * 0.05 * range_vec\n                    # clip to bounds\n                    candidate = np.clip(candidate, lb, ub)\n                else:\n                    # classical DE/rand/1/bin\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != idx]\n                    if idxs.size < 3:\n                        # not enough others -> random perturbation\n                        candidate = pop[idx] + Fi * (self.rng.rand(self.dim) - 0.5) * 0.1 * range_vec\n                        candidate = np.clip(candidate, lb, ub)\n                    else:\n                        r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = self.rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[self.rng.randint(0, self.dim)] = True\n                        candidate = np.where(cr_mask, donor, pop[idx])\n                        candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if allowance\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[idx]:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    # record Fi/CRi for adaptation\n                    successful_F.append(Fi)\n                    successful_CR.append(CRi)\n\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    stagnation_counter = 0\n                    gen_improved = True\n                else:\n                    stagnation_counter += 1\n\n                # safe break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of DE pass: adapt F_mean and CR_mean using successful parameters\n            if len(successful_F) > 0:\n                # Lehmer-like / weighted mean gives more weight to larger Fi (helpful often)\n                mean_F = np.sum(np.array(successful_F)**2) / (np.sum(successful_F) + 1e-12)\n                F_mean = 0.9 * F_mean + 0.1 * np.clip(mean_F, F_min, F_max)\n                CR_mean = 0.9 * CR_mean + 0.1 * (np.mean(successful_CR) if len(successful_CR) > 0 else CR_mean)\n            else:\n                # no success this generation: slightly diversify means\n                F_mean = np.clip(0.98 * F_mean + 0.02 * self.rng.uniform(0.05, 0.9), F_min, F_max)\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * self.rng.rand(), 0.0, 1.0)\n\n            # Trust-region local search around best: small Gaussian perturbations\n            if evals < self.budget:\n                remaining = self.budget - evals\n                # choose number of local samples (small handful)\n                n_local = min(max(1, self.dim // 2), max(1, remaining // 10))\n                n_local = min(n_local, remaining)\n                local_success = 0\n                for _ in range(n_local):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma per-dimension relative to trust radius and range\n                    sigma_base = (trust_radius / (norm_range + 1e-12))\n                    per_dim_scale = 0.5 + self.rng.rand(self.dim) * 0.5\n                    sigma = sigma_base * per_dim_scale\n                    candidate = best_x + self.rng.randn(self.dim) * sigma * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        local_success += 1\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n\n                    # adapt population slot with local candidate if better than worst\n                    worst_idx = int(np.argmax(fvals))\n                    if f_candidate < fvals[worst_idx]:\n                        pop[worst_idx] = candidate\n                        fvals[worst_idx] = f_candidate\n\n                # adjust trust radius based on local successes\n                if local_success > 0:\n                    # successful local exploitation -> shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * (0.7 ** local_success))\n                else:\n                    # expand a bit to escape if no improvement locally\n                    trust_radius = min(max_trust, trust_radius * 1.2)\n\n            # adapt probability of large Lévy jumps based on stagnation\n            if not gen_improved:\n                p_levy = min(0.6, p_levy * 1.05 + 0.005)\n            else:\n                p_levy = max(0.02, p_levy * 0.95)\n\n            # if very stagnant, perform partial population reset\n            if stagnation_counter > max(50, self.dim * 6):\n                # replace half population with random samples\n                k = max(1, self.pop_size // 2)\n                sel = self.rng.choice(self.pop_size, k, replace=False)\n                for j in sel:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        stagnation_counter = 0\n                # enlarge trust region to explore\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # slightly increase diversity parameters\n                F_mean = np.clip(F_mean * 0.9 + 0.1 * self.rng.uniform(0.05, 0.9), F_min, F_max)\n                CR_mean = np.clip(CR_mean * 0.9 + 0.1 * self.rng.rand(), 0.0, 1.0)\n\n        # final best values are stored in self.f_opt, self.x_opt\n        return float(self.f_opt), (self.x_opt.copy() if self.x_opt is not None else None)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveHybridDELevyTrust scored 0.212 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11124953767609158, 0.16302307381114212, 0.31032046874856867, 0.16354406102773633, 0.23705752999199614, 0.2805961746572939, 0.2382595924718509, 0.24439093178278581, 0.23026209240457862, 0.1462414815590931]}, "task_prompt": ""}
{"id": "3c3bc72f-bd7b-41b3-9321-1417a468abfb", "fitness": 0.4568817677114919, "name": "HybridDeLevyTrust", "description": "Hybrid DeLevyTrust — Differential Evolution with occasional Lévy-flight global jumps and an adaptive trust-region local search; parameters adapt online (jDE-style) to balance fast exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (jDE-like per-individual F and CR adaptation),\n      - Occasional heavy-tailed (Lévy-like via Cauchy) long jumps centered on the best,\n      - Trust-region local Gaussian search around the current best (adaptive radius).\n    Designed for continuous box-bounded problems (bounds provided by func.bounds).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: around 5*dim but constrained by budget\n        if pop_size is None:\n            default = max(4, min(6 * self.dim, max(10, self.budget // 20)))\n            self.pop_size = int(default)\n        else:\n            self.pop_size = max(4, int(pop_size))\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.ndim == 0:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be interpreted for dimension {}\".format(self.dim))\n\n    def __call__(self, func):\n        # read bounds (BBOB functions have -5, 5 usually, but we respect func.bounds)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # ensure positive ranges\n        range_vec = np.where(range_vec <= 0, 1.0, range_vec)\n\n        rng = self.rng\n\n        # initialize DE hyper-parameter means (jDE-style adaptation anchors)\n        F_mean = 0.6\n        CR_mean = 0.5\n\n        # Levy jump base probability\n        p_levy = 0.08\n\n        # trust region: controlled as fraction of max range (scalar)\n        trust_fraction = 0.15\n        max_range = np.max(range_vec)\n        trust_radius = trust_fraction * max_range\n        min_trust = 1e-6 * max_range\n        max_trust = max_range * 2.0\n\n        # initialize population\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population until budget or all individuals evaluated\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # If budget too small to evaluate entire pop, keep others un-evaluated (inf)\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = int(np.argmin(np.where(valid, fvals, np.inf)))\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = lb + 0.5 * range_vec\n            return self.f_opt, self.x_opt\n\n        stagnation_counter = 0\n        generation = 0\n\n        # helper: Levy-like heavy-tailed step using Cauchy distribution (per-dim)\n        def levy_step(scale_fraction=0.25):\n            # per-dim Cauchy with moderate scale relative to range_vec and trust_radius\n            # scale_fraction controls typical size; we clip extreme outliers\n            c = rng.standard_cauchy(self.dim)\n            # dynamic scale: combine global range and current trust radius\n            scale = scale_fraction * (range_vec * 0.5 * (trust_radius / (max_range + 1e-12)))\n            # ensure scale not zero\n            scale = np.maximum(scale, 1e-8)\n            s = c * scale\n            # clip to avoid numerical blow-up: allow at most 6*range_vec\n            clip_max = 6.0 * range_vec\n            s = np.clip(s, -clip_max, clip_max)\n            return s\n\n        # main loop: generations until budget exhausted\n        while evals < self.budget:\n            generation += 1\n            successes = 0\n            gen_started_evals = evals\n\n            # for reproducible per-generation indexing, shuffle target order\n            order = rng.permutation(self.pop_size)\n\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # if this individual hasn't been evaluated before, treat it as candidate baseline\n                target = pop[idx].copy()\n                target_f = fvals[idx]\n\n                # decide branch: DE mutation/crossover or Levy jump (centered on best)\n                do_levy = (rng.rand() < p_levy) or (np.linalg.norm(range_vec) < 1e-12)\n\n                if do_levy:\n                    # create trial by jumping from the best\n                    step = levy_step(scale_fraction=0.8 if stagnation_counter > max(10, self.dim) else 0.25)\n                    trial = best_x + step\n                    # small chance to mix in a DE-style difference for diversity\n                    if rng.rand() < 0.2 and self.pop_size >= 3:\n                        r = rng.choice(self.pop_size, 3, replace=False)\n                        donor = pop[r[0]] + 0.8 * (pop[r[1]] - pop[r[2]])\n                        trial = 0.6 * trial + 0.4 * donor\n                    # ensure bounds\n                    trial = np.clip(trial, lb, ub)\n\n                    # evaluate\n                    f_trial = float(func(trial))\n                    evals += 1\n\n                    # greedy selection into the population: replace worst individual if improvement\n                    # we will replace target only if trial is better than target\n                    if f_trial < target_f:\n                        pop[idx] = trial\n                        fvals[idx] = f_trial\n                        target_f = f_trial\n                        successes += 1\n                        # nudge F_mean/CR_mean toward exploration (slightly)\n                        F_mean = np.clip(0.98 * F_mean + 0.02 * 0.8, 0.1, 0.99)\n                        CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.5, 0.0, 1.0)\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n\n                    # update global best if needed\n                    if f_trial < best_f:\n                        best_f = f_trial\n                        best_x = trial.copy()\n                        stagnation_counter = 0\n\n                    continue  # next individual\n\n                # DE/rand/1 mutation with per-individual Fi and CRi (jDE idea)\n                # sample Fi and CRi around the means\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 0.99)\n                CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n                # pick three distinct indices not equal to idx\n                perm = rng.choice([j for j in range(self.pop_size) if j != idx], 3, replace=False)\n                r1, r2, r3 = perm[0], perm[1], perm[2]\n                donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # sometimes mix with best (current-to-best) to increase exploitation\n                if rng.rand() < 0.15:\n                    donor = 0.7 * donor + 0.3 * best_x\n\n                # binomial crossover\n                cr_mask = rng.rand(self.dim) < CRi\n                # ensure at least one gene from donor\n                cr_mask[rng.randint(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, target)\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < target_f:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    # adapt means toward successful Fi/CRi (small step)\n                    F_mean = np.clip(0.9 * F_mean + 0.1 * Fi, 0.05, 0.99)\n                    CR_mean = np.clip(0.9 * CR_mean + 0.1 * CRi, 0.0, 1.0)\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n                    # slight drift of means to encourage exploration if failing\n                    F_mean = np.clip(F_mean * 1.001, 0.05, 0.99)\n                    CR_mean = np.clip(CR_mean * 0.999, 0.0, 1.0)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n\n            # End of generation: trust-region local sampling around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # pick a small number of local samples proportional to dim but limited\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 8)))\n            local_success = 0\n            # anisotropic sigma: base trust_radius scaled by random per-dimension factor\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # sigma scales with trust_radius and also shrinks as trust_radius decreases\n                sigma = max(1e-8, trust_radius) / (max_range + 1e-12)\n                # sample gaussian around best with anisotropic scaling by range_vec\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * (sigma * range_vec)\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    stagnation_counter = 0\n                else:\n                    # expand slightly to help escape\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n                    stagnation_counter += 1\n\n            # adjust p_levy and parameter means based on successes\n            if successes > max(1, int(0.15 * self.pop_size)):\n                # successful generation -> encourage exploitation (reduce p_levy)\n                p_levy = max(0.01, p_levy * 0.9)\n                # nudge F_mean slightly down to be more conservative\n                F_mean = np.clip(F_mean * 0.99, 0.05, 0.99)\n            else:\n                # encourage exploration\n                p_levy = min(0.5, p_levy * 1.06 + 0.005)\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n\n            # small random population refresh if stagnation grows large\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                # re-seed k worst individuals (but ensure we have budget to evaluate them)\n                worst_idx = np.argsort(fvals)[-k:]\n                for j in worst_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = float(fvals[j])\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # enlarge trust radius a bit to promote exploration after reset\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n\n            # safety: if trust radius becomes tiny, slowly re-inflate to avoid numerical stagnation\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n            # If we have fewer than pop_size valid fvals because of small budget, ensure best tracked\n            if not np.isfinite(best_f) and np.any(np.isfinite(fvals)):\n                # fallback to any evaluated individual\n                best_idx = int(np.argmin(np.where(np.isfinite(fvals), fvals, np.inf)))\n                best_f = float(fvals[best_idx])\n                best_x = pop[best_idx].copy()\n\n        # Finalize outputs\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.457 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11544803597042419, 0.1789370741210663, 0.4307606292090379, 0.7505169038718917, 0.5572721512387666, 0.9008312952935589, 0.2854238954036207, 0.5193715637349452, 0.6479251351520003, 0.1823309931196061]}, "task_prompt": ""}
{"id": "f0e19eb6-fbd9-4c9c-be28-9501beccc34a", "fitness": 0.3427910606396347, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style adaptation, occasional Lévy-flight global jumps, and a trust-region local search that shrinks/expands online — designed for robust exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=1)\n        fopt, xopt = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # sensible default population size relative to dim and budget if not provided\n        if pop_size is None:\n            p_guess = int(max(6, min(60, 8 + int(2 * np.sqrt(self.dim)))))\n            # ensure we don't use more than a fraction of budget\n            p_guess = max(4, min(p_guess, max(4, self.budget // 10)))\n            self.pop_size = p_guess\n        else:\n            self.pop_size = int(max(4, pop_size))\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_bounds(self, b, name):\n        # Accept scalar or array-like, broadcast to dim\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size != self.dim:\n            raise ValueError(f\"{name} length {b.size} doesn't match dim {self.dim}\")\n        return b.astype(float)\n\n    def __call__(self, func):\n        # get bounds from func if available, otherwise default [-5, 5]\n        try:\n            lb = self._ensure_bounds(func.bounds.lb, \"lb\")\n            ub = self._ensure_bounds(func.bounds.ub, \"ub\")\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # enforce valid bounds\n        range_vec = ub - lb\n        eps = 1e-12\n        range_norm = max(np.linalg.norm(range_vec), eps)\n\n        # initialize population\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n\n        # evaluate as many initial individuals as budget allows\n        init_to_eval = min(self.pop_size, max(1, self.budget))\n        for i in range(init_to_eval):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # if no evaluation possible return\n        if evals == 0 and self.budget <= 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # if some individuals were not evaluated (budget tiny), fill them with random high values\n        for i in range(init_to_eval, self.pop_size):\n            pop[i] = lb + self.rng.rand(self.dim) * range_vec\n            fvals[i] = np.inf\n\n        # find current best\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # mean differential weight (jDE-style adaptation)\n        CR_mean = 0.3      # mean crossover rate\n        p_levy = 0.06      # chance of performing a Lévy jump for a candidate\n        step_scale = 0.18  # base fraction for trust radius relative to range\n        trust_radius = max(eps, step_scale * range_norm)  # absolute scale for local Gaussian\n        max_trust = range_norm * 2.0\n\n        stagnation_counter = 0\n        best_f_history = best_f\n\n        # helper: generate Levy-like heavy-tailed step using truncated Cauchy\n        def levy_step(scale=1.0):\n            # Cauchy distributed steps, truncated to avoid numerical blow-up\n            # generate per-dimension independent Cauchy draws\n            raw = self.rng.standard_cauchy(self.dim)\n            # clip extremes and scale\n            raw = np.clip(raw, -10.0, 10.0)\n            return (raw / 4.0) * scale  # dividing to reduce extremeness\n\n        # Main loop: iterate until budget exhausted\n        generation = 0\n        while evals < self.budget:\n            generation += 1\n            successes = 0\n            gen_start_best = best_f\n\n            # iterate through population sequentially (each candidate evaluation costs one eval)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # jDE-like sampling: with small prob mutate F_i/CR_i from means\n                # Sample Fi and CRi based on means, keep within [0.1,1] and [0,1]\n                Fi = np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.0)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.15), 0.0, 1.0)\n\n                # choose operation: Levy jump or DE mutation\n                if self.rng.rand() < p_levy:\n                    # Lévy jump centered on best_x (global exploration)\n                    # scale by range_vec and trust_radius fraction\n                    scale = (trust_radius / max(range_norm, eps))\n                    step = levy_step(scale=scale) * range_vec\n                    candidate = best_x + step\n                else:\n                    # DE/rand/1/bin mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    # pick three distinct indices\n                    a, b, c = self.rng.choice(idxs, size=3, replace=False)\n                    donor = pop[a] + Fi * (pop[b] - pop[c])\n                    # binomial crossover\n                    cr_mask = self.rng.rand(self.dim) < CRi\n                    # ensure at least one component from donor\n                    if not cr_mask.any():\n                        cr_mask[self.rng.randint(0, self.dim)] = True\n                    candidate = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate (if budget allows)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                replaced = False\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    replaced = True\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    successes += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # Adapt parameter means toward successful Fi/CRi (approx jDE)\n                if replaced:\n                    # move means slowly toward used Fi/CRi\n                    F_mean = 0.92 * F_mean + 0.08 * Fi\n                    CR_mean = 0.92 * CR_mean + 0.08 * CRi\n\n                # slightly nudge p_levy based on whether candidate was Levy type and succeeded\n                # If Levy used (we didn't record explicit flag here), infer via magnitude test: if candidate was far from pop[i]\n                # For simplicity, if f improved and candidate was distant from current population mean, reduce p_levy.\n                # Compute distance heuristic:\n                # (This is a mild heuristic to adapt exploration/exploitation)\n                # Skip heavy computations if no improvement\n                if replaced and self.rng.rand() < 0.2:\n                    p_levy = max(0.005, p_levy * 0.97)\n\n                # if budget exhausted break out\n                if evals >= self.budget:\n                    break\n\n            # End of generation: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: per-dim sigma proportional to trust radius and range\n                per_dim_sigma = (0.5 + self.rng.rand(self.dim) * 0.5) * (trust_radius / max(range_norm, eps)) * range_vec\n                candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * per_dim_sigma\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # adjust trust radius: shrink on success, expand slightly on failure\n            if local_success > 0 or successes > max(1, self.pop_size // 10):\n                # focus search\n                trust_radius = max(1e-8, trust_radius * (0.7 if local_success > 0 else 0.9))\n                # encourage exploitation by nudging CR_mean up a bit\n                CR_mean = min(0.95, CR_mean * 0.98 + 0.01)\n                # slightly reduce levy chance\n                p_levy = max(0.005, p_levy * 0.96)\n            else:\n                # expand trust region to escape stagnation\n                trust_radius = min(max_trust, trust_radius * 1.08)\n                p_levy = min(0.5, p_levy * 1.04 + 0.005)\n\n            # stagnation handling: if no improvement for many evaluations, re-seed half population\n            if stagnation_counter > max(40, self.dim * 4):\n                # reinitialize half of population randomly\n                half = max(1, self.pop_size // 2)\n                indices = self.rng.choice(self.pop_size, size=half, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius a bit after reset\n                trust_radius = min(max_trust, trust_radius * 1.6)\n                stagnation_counter = 0\n                # raise exploration chance temporarily\n                p_levy = min(0.3, p_levy * 1.15 + 0.02)\n\n            # small safeguard: keep means in sensible ranges\n            F_mean = np.clip(F_mean, 0.05, 1.0)\n            CR_mean = np.clip(CR_mean, 0.0, 1.0)\n            p_levy = np.clip(p_levy, 0.001, 0.5)\n\n            # quick termination check if best hasn't changed for long and is very small (optional)\n            # (not mandatory, but safe)\n            if evals >= self.budget:\n                break\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.343 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10535018277968355, 0.16705505352187666, 0.2581759125124363, 0.9237144846864198, 0.20185822655656627, 0.7871643882689303, 0.21405010420128845, 0.4195066673374235, 0.2084806840461919, 0.1425549024855305]}, "task_prompt": ""}
{"id": "99fedd4f-f8bf-4d0b-a531-bd3b5942a7c7", "fitness": 0.19749243390711949, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid: Differential Evolution with occasional Lévy-flight jumps and an online trust-region local search — balances fast global exploration and focused local exploitation with online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy-like) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (mutation factor F, crossover CR, Lévy probability p_levy, trust radius)\n    are adapted online based on recent successes. Designed for continuous\n    box-bounded problems (e.g., BBOB).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(self.seed)\n\n        # sensible default population size: scales with dim but limited by budget\n        if pop_size is None:\n            default = max(8, 8 * self.dim)\n            # ensure we leave budget for some local search steps\n            self.pop_size = int(min(default, max(4, self.budget // 8)))\n        else:\n            self.pop_size = int(max(4, pop_size))\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like bounds and broadcast to dim\n        arr = np.atleast_1d(np.array(b, dtype=float))\n        if arr.size == 1:\n            return np.full(self.dim, arr.item(), dtype=float)\n        if arr.size == self.dim:\n            return arr.astype(float)\n        # allow broadcasting if length divides dim otherwise repeat/truncate\n        try:\n            return np.broadcast_to(arr, (self.dim,)).astype(float)\n        except Exception:\n            # fallback: tile or trim\n            if arr.size < self.dim:\n                reps = int(np.ceil(self.dim / arr.size))\n                return np.tile(arr, reps)[:self.dim].astype(float)\n            else:\n                return arr[:self.dim].astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n        range_vec = ub - lb\n\n        # initialization\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # Evaluate as many initial individuals as budget allows\n        init_evals = min(self.pop_size, self.budget)\n        for i in range(init_evals):\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # If we couldn't evaluate all individuals, leave rest at +inf\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx]) if np.isfinite(fvals[best_idx]) else np.inf\n\n        # algorithm hyperparameters with online adaptation\n        F_mean = 0.6\n        CR_mean = 0.5\n        p_levy = 0.05  # probability of long Lévy jump\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        stagnation_limit = max(50, 10 * self.dim)\n\n        # helper: Lévy-like heavy-tailed step using Cauchy (stable heavy-tail)\n        def levy_step():\n            # Draw Cauchy-like steps and scale to trust radius; limit extreme outliers.\n            s = rng.standard_cauchy(self.dim).astype(float)\n            # avoid NaNs / extreme values by clamping in a lenient way\n            s = np.clip(s, -1e6, 1e6)\n            # scale to roughly trust_radius magnitude\n            norm = np.sqrt(np.mean(s ** 2)) + 1e-12\n            scale = trust_radius / (np.sqrt(self.dim) + 1e-12)\n            step = s / norm * scale * rng.uniform(0.5, 2.0)\n            # final clamp to avoid numerical blow-up relative to search range\n            max_step = max(1e-12, 10.0 * trust_radius)\n            step = np.clip(step, -max_step, max_step)\n            return step\n\n        # main loop: iterate until budget exhausted\n        # We'll treat evaluations as the scarce resource and consume carefully.\n        while evals < self.budget:\n            # Generation-level bookkeeping\n            gen_successes = 0\n            gen_trials = 0\n\n            # iterate over population members as targets\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Decide exploration mode: DE or Lévy\n                if rng.random() < p_levy:\n                    # Lévy exploration centered on best_x\n                    candidate = best_x + levy_step()\n                    # occasionally add a uniform perturbation scaled by range\n                    if rng.random() < 0.1:\n                        candidate += (rng.random(self.dim) - 0.5) * 0.1 * range_vec\n                    mode = 'levy'\n                    # Small \"virtual\" Fi/CRi for adaptation nudges\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1 mutation with binomial crossover (jDE-style adaptation)\n                    # sample Fi and CRi around their means\n                    Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.2))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                    # pick three distinct indices != i\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(idxs) < 3:\n                        # fallback: generate random candidate if not enough individuals\n                        donor = lb + rng.random(self.dim) * range_vec\n                    else:\n                        a, b, c = rng.choice(idxs, size=3, replace=False)\n                        donor = pop[a] + Fi * (pop[b] - pop[c])\n                    # binomial crossover\n                    jrand = rng.integers(0, self.dim)\n                    cross_mask = rng.random(self.dim) < CRi\n                    cross_mask[jrand] = True  # ensure at least one element from donor\n                    candidate = np.where(cross_mask, donor, pop[i])\n                    mode = 'de'\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # Evaluate candidate if budget allows\n                f_candidate = float(func(candidate))\n                evals += 1\n                gen_trials += 1\n\n                # Selection: greedy\n                if f_candidate < fvals[i]:\n                    # accept candidate into population\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    gen_successes += 1\n\n                    # adapt mutation and crossover means slightly toward successful params\n                    if mode == 'de' and Fi is not None:\n                        # simple exponential moving average\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    elif mode == 'levy':\n                        # successful Levy => reduce p_levy\n                        p_levy = max(0.005, p_levy * 0.92)\n\n                # update best global\n                if np.isfinite(f_candidate) and f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # Early exit if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n\n            # Trust-region local search around best: sample a small number of Gaussian candidates\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 8)))\n            local_successes = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian using per-dim scaling\n                sigma = trust_radius * (0.5 + rng.random(self.dim))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_successes += 1\n                    # successful local step -> shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    stagnation_counter = 0\n                else:\n                    # slightly expand to encourage escape when unsuccessful\n                    trust_radius = min(max_trust, trust_radius * 1.03)\n\n            # Generation-level adaptation of Lévy probability\n            if gen_trials > 0:\n                success_rate = gen_successes / gen_trials\n                # if many successes, reduce exploration; if few, increase\n                if success_rate > 0.2:\n                    p_levy = max(0.005, p_levy * 0.96)\n                else:\n                    p_levy = min(0.5, p_levy * 1.04 + 0.001)\n\n            # If local search had quite some successes, nudge F/CR toward exploitation\n            if local_successes > 0:\n                CR_mean = 0.95 * CR_mean + 0.05 * 0.5\n                F_mean = 0.95 * F_mean + 0.05 * 0.5\n\n            # stagnation handling: if no improvement for a while, re-seed part of population\n            if stagnation_counter >= stagnation_limit:\n                # reinitialize half of the population randomly (but keep best)\n                k = max(1, self.pop_size // 2)\n                indices = list(range(self.pop_size))\n                # choose indices to replace, avoid choosing the best index if in population\n                replace_idxs = rng.choice(indices, size=k, replace=False)\n                for idx in replace_idxs:\n                    # do not throw away best solution if it sits in pop; keep it by replacing others\n                    pop[idx] = lb + rng.random(self.dim) * range_vec\n                    fvals[idx] = float(func(pop[idx])) if evals < self.budget - 1 else np.inf\n                    evals += 1 if np.isfinite(fvals[idx]) else 0\n                    # update best if necessary\n                    if np.isfinite(fvals[idx]) and fvals[idx] < best_f:\n                        best_f = fvals[idx]\n                        best_x = pop[idx].copy()\n                # enlarge trust radius to encourage exploration after reset\n                trust_radius = min(max_trust, max(trust_radius * 1.8, 0.1 * np.linalg.norm(range_vec)))\n                # increase Lévy chance\n                p_levy = min(0.5, p_levy * 1.2 + 0.02)\n                stagnation_counter = 0\n\n        # finished (budget exhausted or loop ended)\n        self.f_opt = best_f\n        self.x_opt = best_x\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.197 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11205601747115546, 0.16151673957188106, 0.25940509786485333, 0.2078291735991482, 0.21148830627409498, 0.2637059238074333, 0.20178840810766252, 0.21363856803276748, 0.18517577190930024, 0.15832033243289845]}, "task_prompt": ""}
{"id": "5d955587-cf31-427f-bbda-1fa19063338e", "fitness": 0.31121289167413424, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style online parameter adaptation, occasional Lévy (Cauchy) long jumps centered on the best for global exploration, and an adaptive trust-region Gaussian local search around the best for focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n    - Population-based Differential Evolution (DE) with per-individual F and CR sampled\n      around evolving means (jDE-like adaptation).\n    - Occasional Lévy-like long jumps (implemented using clipped Cauchy samples)\n      centered on the current best solution to escape basins and promote long-range exploration.\n    - Trust-region local search around current best (Gaussian perturbations) with adaptive\n      trust radius that shrinks on success and expands on failures.\n    - Online simple adaptation of F_mean, CR_mean and probability of Lévy jumps based on\n      recent successes; population reseeding on prolonged stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size scaled by dimension but not larger than budget fraction\n        if pop_size is None:\n            default = max(4, min(50, 8 * self.dim))\n            self.pop_size = int(min(default, max(4, self.budget // 5)))\n        else:\n            self.pop_size = int(pop_size)\n            # ensure reasonable size\n            self.pop_size = max(2, min(self.pop_size, max(4, self.budget)))\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _clip_bounds(self, x, lb, ub):\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def __call__(self, func):\n        # Get bounds and ensure array shapes\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # Fallback to global -5..5 if func has no bounds attribute\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        range_mean = float(np.maximum(range_vec.mean(), 1e-12))\n\n        # Initialize population\n        pop_size = self.pop_size\n        X = np.zeros((pop_size, self.dim), dtype=float)\n        fvals = np.full(pop_size, np.inf, dtype=float)\n\n        # initialize uniformly\n        for i in range(pop_size):\n            X[i] = self.rng.uniform(lb, ub)\n\n        evals = 0\n        # Evaluate initial population sequentially until budget exhausted\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            x = X[i].copy()\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If no evaluations could be performed, return trivial result\n        if evals == 0:\n            return float(self.f_opt), None\n\n        # Identify best individual\n        best_idx = int(np.nanargmin(fvals))\n        best_x = X[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # Hyper-parameters (adaptive)\n        F_mean = 0.6\n        CR_mean = 0.3\n        levy_prob = 0.07  # initial chance of a Lévy jump per trial\n        trust_radius = 0.25 * range_mean  # initial trust radius (scalar)\n        max_trust = 2.0 * np.linalg.norm(range_vec)\n        stagnation_counter = 0\n        success_history = []\n\n        # helper: Lévy-like step using clipped Cauchy\n        def levy_step(scale_factor=1.0):\n            # standard Cauchy per-dimension, clipped to avoid huge outliers\n            step = self.rng.standard_cauchy(size=self.dim)\n            # clip extreme outliers reasonably (still heavy-tailed)\n            step = np.clip(step, -50.0, 50.0)\n            # scale by trust radius and a random anisotropic factor\n            anis = 0.5 + self.rng.rand(self.dim)\n            return step * (trust_radius * scale_factor) * anis / (np.maximum(np.mean(np.abs(step)), 1e-12))\n\n        # main loop: iterate until budget exhausted\n        # We will iterate individuals in random order each generation\n        while evals < self.budget:\n            order = np.arange(pop_size)\n            self.rng.shuffle(order)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # Decide exploration operator: Lévy jump vs DE\n                do_levy = (self.rng.rand() < levy_prob)\n                candidate = None\n                used_F = None\n                used_CR = None\n\n                if do_levy:\n                    # generate a long jump centered on best_x\n                    scale = 0.5 + self.rng.rand()  # random scale 0.5..1.5\n                    step = levy_step(scale_factor=scale)\n                    candidate = best_x + step * (range_vec / np.maximum(range_mean, 1e-12))\n                    # small local mixing with current idx to keep diversity\n                    mix = self.rng.rand(self.dim) < 0.1\n                    if np.any(mix):\n                        candidate[mix] = X[idx][mix]\n                    # ensure inside bounds\n                    candidate = self._clip_bounds(candidate, lb, ub)\n                else:\n                    # DE/rand/1 mutation with jDE-like sampling of F and CR\n                    # sample Fi and CRi around means (normal jitter)\n                    Fi = float(np.clip(self.rng.normal(F_mean, 0.15), 0.05, 1.2))\n                    CRi = float(np.clip(self.rng.normal(CR_mean, 0.2), 0.0, 1.0))\n                    used_F = Fi\n                    used_CR = CRi\n\n                    # pick three distinct indices different from idx\n                    ids = list(range(pop_size))\n                    ids.remove(idx)\n                    a, b, c = self.rng.choice(ids, size=3, replace=False)\n                    donor = X[a] + Fi * (X[b] - X[c])\n\n                    # binomial crossover\n                    cross = self.rng.rand(self.dim) < CRi\n                    # ensure at least one dimension is from donor\n                    if not np.any(cross):\n                        cross[self.rng.randint(0, self.dim)] = True\n                    trial = X[idx].copy()\n                    trial[cross] = donor[cross]\n\n                    # sometimes bias toward best (current-to-best) with small prob to exploit\n                    if self.rng.rand() < 0.15:\n                        trial = trial + 0.2 * Fi * (best_x - trial)\n\n                    candidate = self._clip_bounds(trial, lb, ub)\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement\n                replaced = False\n                if f_candidate < fvals[idx]:\n                    X[idx] = candidate.copy()\n                    fvals[idx] = f_candidate\n                    replaced = True\n                    # adapt F_mean and CR_mean slightly toward used values (if set)\n                    if used_F is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * used_F\n                    if used_CR is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * used_CR\n                    # reward success for levy as well\n                    success_history.append(True)\n                else:\n                    success_history.append(False)\n\n                # Update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # shrink trust region to focus search\n                    trust_radius = max(1e-8, trust_radius * 0.85)\n                    # decrease levy probability slightly on successful improvement\n                    levy_prob = max(0.005, levy_prob * 0.92)\n                    stagnation_counter = 0\n                    # update record of best\n                    if best_f < self.f_opt:\n                        self.f_opt = float(best_f)\n                        self.x_opt = best_x.copy()\n                else:\n                    stagnation_counter += 1\n                    # unsuccessful: slightly increase levy_prob to encourage escapes\n                    levy_prob = min(0.5, levy_prob * 1.005)\n\n                # maintain success_history window\n                if len(success_history) > 100:\n                    success_history.pop(0)\n\n                # quick adaptation: if many recent successes, decrease exploration (reduce levy_prob)\n                if len(success_history) >= 20:\n                    recent = np.mean(success_history[-20:])\n                    if recent > 0.3:\n                        levy_prob = max(0.01, levy_prob * 0.97)\n                        F_mean = np.clip(F_mean * 0.98, 0.05, 1.2)\n                        CR_mean = np.clip(CR_mean * 1.02, 0.0, 1.0)\n                    elif recent < 0.05:\n                        # stagnating -> encourage jumps and larger F diversity\n                        levy_prob = min(0.5, levy_prob * 1.08)\n                        F_mean = np.clip(F_mean * 1.02, 0.05, 1.2)\n\n                # Break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation / population sweep: perform trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples: small handful scaled by dimension but limited by remaining budget\n            local_samples = int(min(max(2, self.dim // 2), remaining, 2 + self.dim // 2))\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic Gaussian noise around best with per-dim scaling\n                sigma = trust_radius * (0.2 + 0.8 * self.rng.rand(self.dim))\n                candidate = best_x + self.rng.normal(scale=sigma)\n                candidate = self._clip_bounds(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # strong local improvement: shrink trust radius\n                    trust_radius = max(1e-8, trust_radius * 0.75)\n                    # update global best\n                    if best_f < self.f_opt:\n                        self.f_opt = float(best_f)\n                        self.x_opt = best_x.copy()\n                    # reward adaptation: slightly move CR_mean/F_mean toward exploitation\n                    CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.5, 0.0, 1.0)\n                    F_mean = np.clip(0.98 * F_mean + 0.02 * 0.4, 0.05, 1.2)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful local sample -> expand trust region a little to escape local trap\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # adjust levy_prob based on local search success\n            if local_success == 0:\n                levy_prob = min(0.6, levy_prob * 1.03)\n            else:\n                levy_prob = max(0.005, levy_prob * 0.9)\n\n            # strong stagnation reset: re-seed half of population if no improvement for long time\n            if stagnation_counter > max(50, self.dim * 6):\n                n_reseed = max(1, pop_size // 2)\n                for j in range(n_reseed):\n                    idx = self.rng.randint(0, pop_size)\n                    X[idx] = self.rng.uniform(lb, ub)\n                    # evaluate if budget available (prefer to evaluate few to diversify)\n                    if evals < self.budget:\n                        f_new = float(func(X[idx]))\n                        evals += 1\n                        fvals[idx] = f_new\n                        if f_new < best_f:\n                            best_f = f_new\n                            best_x = X[idx].copy()\n                            if best_f < self.f_opt:\n                                self.f_opt = float(best_f)\n                                self.x_opt = best_x.copy()\n                # enlarge trust radius slightly after reseed\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # reset stagnation counter and promote exploration\n                stagnation_counter = 0\n                levy_prob = min(0.5, levy_prob * 1.2)\n                # make parameter means more exploratory\n                F_mean = min(1.2, F_mean * 1.05)\n                CR_mean = np.clip(CR_mean * 0.95 + 0.05, 0.0, 1.0)\n\n        # final results\n        # make sure x_opt consistent\n        if self.x_opt is None:\n            # pick best from fvals (if any)\n            if np.isfinite(fvals).any():\n                bi = int(np.nanargmin(fvals))\n                self.x_opt = X[bi].copy()\n                self.f_opt = float(fvals[bi])\n            else:\n                self.x_opt = best_x\n                self.f_opt = best_f\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.311 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19910890204669307, 0.25430247960085517, 0.21296430683658163, 0.49279352342864313, 0.2967468133413511, 0.48410396472774975, 0.2734250431978875, 0.3868501585321129, 0.3311983450458822, 0.18063537998358614]}, "task_prompt": ""}
{"id": "490a236b-cf88-4189-9770-93921caf9e37", "fitness": 0.5419136680865992, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining DE-style population search, occasional Lévy-flight long jumps, and a trust-region local search with online step-size adaptation and stagnation-triggered reseeding.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line idea: Combine robust population-based DE search with rare\n    heavy-tailed Lévy perturbations and a local trust-region search,\n    adapting operator parameters and exploration frequency online.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # sensible default population size: scale with dim but keep small vs budget\n        default_pop = max(6, min(20 * self.dim, self.budget // 4 if self.budget >= 8 else 6))\n        if pop_size is None:\n            self.pop_size = int(default_pop)\n        else:\n            self.pop_size = int(max(4, pop_size))\n\n        # ensure population does not exceed budget in tiny-budget scenarios\n        self.pop_size = min(self.pop_size, max(4, self.budget))\n\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _broadcast_bounds(self, b, name):\n        \"\"\"Ensure bounds are arrays of length dim, broadcast scalars/length-1 arrays.\"\"\"\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        raise ValueError(f\"Bounds {name} must be scalar or length dim ({self.dim}).\")\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function func with at most self.budget calls.\n        Function object may optionally expose func.bounds.lb and func.bounds.ub;\n        otherwise default bounds [-5, 5] are used.\n        \"\"\"\n\n        # obtain bounds if provided, else default [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            try:\n                lb = self._broadcast_bounds(func.bounds.lb, \"lb\")\n                ub = self._broadcast_bounds(func.bounds.ub, \"ub\")\n            except Exception:\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        # avoid zero ranges\n        range_vec[range_vec == 0.0] = 1.0\n\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                fvals[i] = np.inf\n            evals += 1\n\n        # if we couldn't evaluate full pop due to tiny budget, leave remaining individuals unevaluated (inf)\n        # ensure at least one evaluated\n        if not np.isfinite(fvals).any():\n            # evaluate at least one random point if none were evaluated (budget may be zero though)\n            if self.budget > 0:\n                x0 = lb + rng.rand(self.dim) * range_vec\n                f0 = float(func(x0))\n                evals += 1\n                pop[0] = x0\n                fvals[0] = f0\n\n        # initial best\n        best_idx = int(np.nanargmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # DE/adaptation hyper-parameters\n        F_mean = 0.6\n        CR_mean = 0.5\n        # trust-region radius as fraction of box range\n        trust_radius = 0.15  # relative (applied per-dimension)\n        # counters for adaptation and stagnation\n        last_improve_eval = evals\n        no_improve_count = 0\n\n        # frequency of Levy jumps (base)\n        base_levy_p = 0.05\n        # keep success history for parameter adaptation (Lehmer-like)\n        success_Fs = []\n        success_CRs = []\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy clipped\n        def levy_step(scale=1.0):\n            # standard Cauchy has heavy tails; clip extremes to avoid blow-up\n            step = rng.standard_cauchy(size=self.dim)\n            step = np.clip(step, -10.0, 10.0)\n            return step * scale\n\n        # Ensure minimum pop size for selection\n        assert self.pop_size >= 4\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            # per-generation success trackers\n            gen_success_F = []\n            gen_success_CR = []\n            gen_success_count = 0\n\n            # dynamic probability of Levy increases with stagnation\n            stagnation_fraction = (evals - last_improve_eval) / max(1.0, self.budget)\n            levy_p = min(0.5, base_levy_p + 0.5 * stagnation_fraction)\n\n            # for each individual (target), produce a candidate and evaluate\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample control parameters per-individual (jDE-style-ish)\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.2)\n                CRi = np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0)\n\n                # choose mutation indices distinct from i\n                idxs = list(range(self.pop_size))\n                idxs.remove(i)\n                r = rng.choice(idxs, size=3, replace=False)\n                r1, r2, r3 = r[0], r[1], r[2]\n\n                # decide whether to do Levy jump\n                do_levy = rng.rand() < levy_p\n\n                if do_levy:\n                    # Levy-centered around best (global exploration), scaled by trust radius and box range\n                    lstep = levy_step(scale=0.25)  # base scale\n                    donor = best_x + (trust_radius * range_vec) * lstep\n                    # mix occasionally with DE/rand/1 to keep diversity\n                    if rng.rand() < 0.5:\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                else:\n                    # standard DE/rand/1 mutation with some best influence scaled by trust radius\n                    # mix between rand and best-guided\n                    if rng.rand() < 0.2:\n                        donor = best_x + Fi * (pop[r1] - pop[r2])\n                    else:\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # small local trust-region contraction occasionally\n                    if rng.rand() < 0.15:\n                        donor = donor * (1.0 - trust_radius * rng.rand()) + best_x * (trust_radius * rng.rand())\n\n                # binomial crossover\n                mask = rng.rand(self.dim) < CRi\n                # ensure at least one dimension is from donor\n                jrand = rng.randint(0, self.dim)\n                mask[jrand] = True\n                candidate = np.where(mask, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    # accept\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    gen_success_count += 1\n                    gen_success_F.append(Fi)\n                    gen_success_CR.append(CRi)\n                    # nudge means toward successful Fi/CRi (small step)\n                    F_mean = 0.92 * F_mean + 0.08 * Fi\n                    CR_mean = 0.92 * CR_mean + 0.08 * CRi\n                else:\n                    # small decay to avoid convergence to zero exploration\n                    F_mean = min(1.2, 0.995 * F_mean + 0.0005)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    last_improve_eval = evals\n                    no_improve_count = 0\n                else:\n                    no_improve_count += 1\n\n                # break if budget used\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            # update means using Lehmer-like weighting if successes occurred\n            if len(gen_success_F) > 0:\n                # Lehmer mean emphasizes larger Fs\n                Fs = np.array(gen_success_F)\n                if Fs.sum() > 0:\n                    F_mean = 0.85 * F_mean + 0.15 * (np.sum(Fs**2) / np.sum(Fs))\n                CR_mean = 0.85 * CR_mean + 0.15 * np.mean(gen_success_CR)\n\n            # trust-region local search around best: sample a few candidates with anisotropic gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # determine number of local samples (small handful scaled by dim and remaining budget)\n            n_local = int(min(max(1, self.dim // 3), 5 + self.dim // 5, remaining))\n            local_success = 0\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dimension\n                sigma = trust_radius * (0.5 + rng.rand(self.dim)) * range_vec\n                local = best_x + rng.normal(0, 1.0, size=self.dim) * sigma\n                local = np.minimum(np.maximum(local, lb), ub)\n                try:\n                    f_local = float(func(local))\n                except Exception:\n                    f_local = np.inf\n                evals += 1\n                if f_local < best_f:\n                    best_f = f_local\n                    best_x = local.copy()\n                    local_success += 1\n                    last_improve_eval = evals\n                    no_improve_count = 0\n                # optionally inject local successes into population replacing worsts\n                if f_local < fvals.max():\n                    worst_idx = int(np.argmax(fvals))\n                    pop[worst_idx] = local.copy()\n                    fvals[worst_idx] = f_local\n\n            # adapt trust radius: shrink on success, expand on failure (bounded)\n            if local_success > 0:\n                trust_radius = max(0.01, trust_radius * (0.85 ** local_success))\n                # encourage exploitation by nudging F/CR\n                F_mean = 0.95 * F_mean + 0.05 * 0.4\n                CR_mean = 0.95 * CR_mean + 0.05 * 0.9\n            else:\n                trust_radius = min(1.0, trust_radius * 1.06)\n                # if stagnating, increase levy probability by nudging F_mean slightly\n                if (evals - last_improve_eval) > max(20, 0.02 * self.budget):\n                    F_mean = min(1.2, F_mean * 1.02)\n\n            # stagnation handling: if no improvement for long, re-seed half of population\n            if (evals - last_improve_eval) > max(200, 0.12 * self.budget):\n                # reinitialize half of population to diversify\n                n_reseed = max(1, self.pop_size // 2)\n                for j in rng.choice(range(self.pop_size), size=n_reseed, replace=False):\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        last_improve_eval = evals\n                # enlarge trust radius slightly after reset to encourage exploration\n                trust_radius = min(1.0, trust_radius * 1.5)\n                # reset F/CR a bit\n                F_mean = 0.6\n                CR_mean = 0.5\n\n            # small maintenance: ensure best is reflected in population occasionally\n            if rng.rand() < 0.15 and evals < self.budget:\n                # replace worst with slight perturbation of best\n                worst_idx = int(np.argmax(fvals))\n                perturb = best_x + rng.normal(0, 0.5 * trust_radius, size=self.dim) * range_vec\n                perturb = np.minimum(np.maximum(perturb, lb), ub)\n                try:\n                    f_pert = float(func(perturb))\n                except Exception:\n                    f_pert = np.inf\n                evals += 1\n                if f_pert < fvals[worst_idx]:\n                    pop[worst_idx] = perturb\n                    fvals[worst_idx] = f_pert\n                if f_pert < best_f:\n                    best_f = f_pert\n                    best_x = perturb.copy()\n                    last_improve_eval = evals\n\n        # finished; set outputs\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.542 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18381631220113315, 0.17318947935789186, 0.5645973480276663, 0.9466532218867009, 0.8677501303158299, 0.9004549479421011, 0.32507366650344616, 0.5760269252852757, 0.2573668368071127, 0.6242078125388356]}, "task_prompt": ""}
{"id": "877d37b4-e2aa-439b-96ba-459791c46848", "fitness": 0.4762666108167225, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy jumps and a trust-region local search; online adapts F/CR, trust radius, and reseeds on stagnation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # Population size heuristic: scale with dimension but limited by budget\n        if pop_size is None:\n            ps = int(6 + 2 * self.dim)  # baseline\n            ps = min(ps, max(4, self.budget // 10))  # not larger than budget/10\n            self.pop_size = max(4, ps)\n        else:\n            self.pop_size = int(pop_size)\n\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"Ensure bounds are arrays of length dim.\"\"\"\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.ones(self.dim) * float(b)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.resize(b.astype(float), self.dim)\n\n    def __call__(self, func):\n        # Prepare bounds from func; assume properties func.bounds.lb/ub or defaults -5,5\n        if hasattr(func, \"bounds\"):\n            lb = getattr(func.bounds, \"lb\", -5.0)\n            ub = getattr(func.bounds, \"ub\", 5.0)\n        else:\n            lb = -5.0\n            ub = 5.0\n        lb = self._ensure_array_bounds(lb)\n        ub = self._ensure_array_bounds(ub)\n\n        # internal counters\n        eval_count = 0\n        budget = self.budget\n\n        # initialize population uniformly in bounds\n        pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.full(self.pop_size, np.inf)\n\n        # evaluate initial population (but don't exceed budget)\n        for i in range(self.pop_size):\n            if eval_count >= budget:\n                break\n            x = pop[i]\n            fitness[i] = float(func(x))\n            eval_count += 1\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = x.copy()\n\n        # If budget exhausted before any evals\n        if eval_count == 0:\n            # not able to evaluate anything\n            return self.f_opt, self.x_opt\n\n        # If some individuals un-evaluated (budget tiny), mark them as inf and they will rarely be used\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.5\n        CR_mean = 0.9\n        p_levy_base = 0.02  # base probability to attempt Lévy centered jumps\n        p_levy = p_levy_base\n        trust_radius = 0.5 * np.linalg.norm(ub - lb) / np.sqrt(self.dim)  # initial trust radius (global-scale)\n        trust_shrink = 0.85\n        trust_expand = 1.15\n        stagnation_counter = 0\n        stagnation_threshold = max(50, int(0.2 * budget / max(1, self.dim)))  # evals with no improvement\n        levy_scale = 1.0  # scale for Lévy steps\n        success_history_F = []\n        success_history_CR = []\n        max_no_improve_restarts = 3\n        restarts_done = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step(scale=1.0):\n            # sample Cauchy (Lévy-like heavy tails); limit extreme outliers\n            # We'll generate a vector of length dim\n            u = np.random.rand(self.dim)\n            c = np.tan(np.pi * (u - 0.5))  # standard Cauchy\n            # limit extremes\n            cap = 1e6\n            c = np.clip(c, -cap, cap)\n            # scale by scale and median absolute value to normalize a bit\n            med = np.median(np.abs(c)) + 1e-12\n            step = (c / med) * scale\n            return step\n\n        # main loop: iterate until budget exhausted\n        gen = 0\n        while eval_count < budget:\n            gen += 1\n            # if population has inf fitness (not evaluated) and still budget, evaluate them first\n            for i in range(self.pop_size):\n                if eval_count >= budget:\n                    break\n                if np.isinf(fitness[i]):\n                    fitness[i] = float(func(pop[i]))\n                    eval_count += 1\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = pop[i].copy()\n\n            if eval_count >= budget:\n                break\n\n            # per-generation adaptation: jitter around means\n            F_is = np.clip(np.random.normal(F_mean, 0.15, size=self.pop_size), 0.05, 1.0)\n            CR_is = np.clip(np.random.normal(CR_mean, 0.1, size=self.pop_size), 0.0, 1.0)\n\n            # shuffle indices for mutation ordering\n            indices = np.arange(self.pop_size)\n            np.random.shuffle(indices)\n\n            # store successful Fi/CR for adaptation\n            success_F = []\n            success_CR = []\n\n            # compute best index\n            best_idx = np.argmin(fitness)\n            best_vec = pop[best_idx].copy()\n            best_val = fitness[best_idx]\n\n            # dynamic chance of Lévy jumps based on stagnation\n            p_levy = p_levy_base * (1.0 + stagnation_counter / (stagnation_threshold + 1.0))\n\n            # For each target vector sequentially\n            for idx in indices:\n                if eval_count >= budget:\n                    break\n\n                xi = pop[idx].copy()\n                fi = fitness[idx]\n\n                Fi = F_is[idx]\n                CRi = CR_is[idx]\n\n                # decide mutation style: with some chance use a Lévy-biased donor, else standard DE/rand/1 augmented with best guidance\n                if np.random.rand() < p_levy:\n                    # Lévy jump centered on best + differential perturbation\n                    levy = levy_step(scale=levy_scale * trust_radius)\n                    donor = best_vec + levy\n                    # mix some DE/rand/1 directional info\n                    # pick three distinct indices\n                    idxs = np.random.choice([j for j in range(self.pop_size) if j != idx], 3, replace=False)\n                    a, b, c = pop[idxs]\n                    donor = donor + Fi * (a - xi) + Fi * (b - c)\n                else:\n                    # DE/rand/1-like with occasional guidance by best (DE/current-to-best/1)\n                    # sample three distinct indices\n                    avail = [j for j in range(self.pop_size) if j != idx]\n                    if len(avail) >= 3:\n                        a_idx, b_idx, c_idx = np.random.choice(avail, 3, replace=False)\n                        a = pop[a_idx]\n                        b = pop[b_idx]\n                        c = pop[c_idx]\n                        # Use a mix between rand/1 and current-to-best/1 depending on trust radius\n                        mix = np.clip((trust_radius / (np.linalg.norm(ub - lb) + 1e-12)) * 2.0, 0.0, 1.0)\n                        donor_rand = a + Fi * (b - c)\n                        donor_ctb = xi + Fi * (best_vec - xi) + Fi * (a - b)\n                        donor = (1.0 - mix) * donor_rand + mix * donor_ctb\n\n                # binomial crossover\n                cross_mask = np.random.rand(self.dim) < CRi\n                # ensure at least one dimension crosses\n                if not np.any(cross_mask):\n                    cross_mask[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_mask, donor, xi)\n\n                # projection to bounds\n                trial = np.maximum(np.minimum(trial, ub), lb)\n\n                # one evaluation\n                f_trial = float(func(trial))\n                eval_count += 1\n\n                # selection: greedy replacement\n                if f_trial <= fi:\n                    pop[idx] = trial\n                    fitness[idx] = f_trial\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n                    # update global best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        stagnation_counter = 0\n                    else:\n                        # even if not global best, it's a local success -> reset little stagnation\n                        stagnation_counter = max(0, stagnation_counter - 1)\n                else:\n                    # failure nudges stagnation forward\n                    stagnation_counter += 1\n\n            # End of generation adjustments: update F_mean and CR_mean using successful parameters (simple Lehmer-like mean)\n            if success_F:\n                # Lehmer mean gives more weight to larger Fi\n                numer = np.sum(np.array(success_F)**2)\n                denom = np.sum(np.array(success_F)) + 1e-12\n                F_lehmer = numer / denom\n                F_mean = 0.9 * F_mean + 0.1 * np.clip(F_lehmer, 0.05, 1.0)\n            else:\n                # slowly decay to encourage exploration\n                F_mean = max(0.05, 0.995 * F_mean)\n\n            if success_CR:\n                CR_mean = 0.9 * CR_mean + 0.1 * (np.mean(success_CR))\n            else:\n                CR_mean = 0.99 * CR_mean\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            # sample count depends on remaining budget and dimension\n            remaining = budget - eval_count\n            if remaining <= 0:\n                break\n            n_local = int(min(max(1, self.dim // 3), remaining, 8))\n            local_success = 0\n            for _ in range(n_local):\n                if eval_count >= budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by random factor per-dimension\n                sigma = trust_radius * (0.5 + np.random.rand(self.dim) * 0.5)\n                candidate = best_vec + np.random.normal(0, sigma, size=self.dim)\n                candidate = np.maximum(np.minimum(candidate, ub), lb)\n                f_cand = float(func(candidate))\n                eval_count += 1\n                if f_cand < best_val:\n                    best_val = f_cand\n                    best_vec = candidate.copy()\n                    local_success += 1\n                    # accept into population in worst slot\n                    worst_idx = np.argmax(fitness)\n                    pop[worst_idx] = candidate.copy()\n                    fitness[worst_idx] = f_cand\n                    if f_cand < self.f_opt:\n                        self.f_opt = f_cand\n                        self.x_opt = candidate.copy()\n                        stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # adjust trust radius based on local performance\n            if local_success > 0:\n                trust_radius = max(1e-6, trust_radius * (trust_shrink ** local_success))\n            else:\n                trust_radius = min(np.linalg.norm(ub - lb), trust_radius * trust_expand)\n\n            # if many successes globally, slightly reduce exploration pressure\n            if len(success_F) + local_success > max(2, self.pop_size // 4):\n                p_levy = max(0.001, p_levy * 0.7)\n            else:\n                # stagnation increases levy chance\n                p_levy = min(0.5, p_levy * (1.0 + stagnation_counter / (stagnation_threshold + 1.0)))\n\n            # stagnation handling: if no improvement for many evaluations -> partial reseed\n            if stagnation_counter > stagnation_threshold and restarts_done < max_no_improve_restarts:\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                reinit_idx = np.random.choice(self.pop_size, k, replace=False)\n                for j in reinit_idx:\n                    if eval_count >= budget:\n                        break\n                    pop[j] = np.random.uniform(lb, ub)\n                    fitness[j] = float(func(pop[j]))\n                    eval_count += 1\n                    if fitness[j] < self.f_opt:\n                        self.f_opt = fitness[j]\n                        self.x_opt = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(np.linalg.norm(ub - lb), trust_radius * 2.0)\n                stagnation_counter = 0\n                restarts_done += 1\n\n            # Strong stagnation extreme reset: if still no improvement and few evals left, inject random restarts\n            if stagnation_counter > 3 * stagnation_threshold and restarts_done >= max_no_improve_restarts:\n                # reseed entire population except best\n                for j in range(self.pop_size):\n                    if eval_count >= budget:\n                        break\n                    if j == best_idx:\n                        continue\n                    pop[j] = np.random.uniform(lb, ub)\n                    fitness[j] = float(func(pop[j]))\n                    eval_count += 1\n                    if fitness[j] < self.f_opt:\n                        self.f_opt = fitness[j]\n                        self.x_opt = pop[j].copy()\n                trust_radius = np.linalg.norm(ub - lb) * 0.5\n                stagnation_counter = 0\n\n            # safety: keep best in sync\n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx].copy()\n\n            # break if budget exhausted\n            if eval_count >= budget:\n                break\n\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.476 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13500478180400866, 0.19473882016229827, 0.9223086393463317, 0.8911918528796667, 0.8119906537545224, 0.4732369510972476, 0.34847520012890876, 0.3485743228898196, 0.46784671778631925, 0.1692981683181023]}, "task_prompt": ""}
{"id": "93146eb1-8d14-48ee-99d2-adade069dee8", "fitness": 0.24535796975066432, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with per-individual parameter adaptation, occasional Lévy-flight long jumps, and a trust-region local search around the best solution (fast global exploration + concentrated local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but bounded by budget\n        if pop_size is None:\n            default = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            self.pop_size = min(default, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # best results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # attempt to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to dimension %d\" % self.dim)\n\n    def __call__(self, func):\n        # read bounds from func (BBOB style)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        # range vector and scale helpers\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population until budget or complete\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            f = float(func(pop[i]))\n            fvals[i] = f\n            evals += 1\n\n        # if nothing evaluated, return immediate\n        if np.all(np.isinf(fvals)):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # determine current global best\n        valid = ~np.isinf(fvals)\n        best_idx = np.argmin(fvals[valid])\n        best_indices = np.where(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius multiplier\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        min_trust = 1e-6\n        max_trust = range_norm * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # normalize moderate steps to avoid huge leaps, preserve heavy tail\n            s_norm = np.linalg.norm(s)\n            if s_norm > 1e-12:\n                s = s / (s_norm + 1e-12)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # process each member as target vector\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual Fi and CRi (jDE-like noisy sampling)\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                # decide between DE-like mutation or Lévy jump\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best_x for exploration,\n                    # scale by trust_radius and global range\n                    s = levy_step()\n                    scale = step_scale * (0.2 + rng.rand() * 1.0) * (trust_radius / (range_norm + 1e-12))\n                    donor = best_x + s * scale * range_vec\n                    # keep diversity: sometimes add small random offset\n                    if rng.rand() < 0.2:\n                        donor += (rng.rand(self.dim) - 0.5) * 0.01 * range_vec\n                    candidate = np.clip(donor, lb, ub)\n                else:\n                    # DE/rand/1 mutation with crossover\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback to random vector in bounds\n                        candidate = lb + rng.rand(self.dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[rng.randint(0, self.dim)] = True\n                        trial = np.where(cr_mask, donor, pop[i])\n                        candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt the means slightly toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # small drift away from unsuccessful parameters\n                    F_mean = np.clip(0.995 * F_mean + 0.0005, 0.05, 0.99)\n                    CR_mean = np.clip(0.995 * CR_mean + 0.0005, 0.0, 1.0)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # after finishing a generation, perform trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            improved_local = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic per-dim sigma but scaled by trust_radius and global range\n                sigma = (0.4 + rng.rand(self.dim) * 0.6) * (trust_radius / (range_norm + 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    improved_local += 1\n                    # shrink trust region to focus search\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # modest expansion to try escaping local minima\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # adapt exploration probability and means based on success\n            if successes + improved_local > 0:\n                # success: slightly reduce levy probability and slowly push means toward exploitation-friendly values\n                p_levy = max(0.01, p_levy * (0.96 if (successes + improved_local) > (0.2 * self.pop_size) else 0.98))\n                F_mean = np.clip(0.96 * F_mean + 0.04 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.96 * CR_mean + 0.04 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation: increase long jumps and diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: re-seed part of population\n            if stagnation_counter > max(50, self.dim * 6):\n                k = max(1, self.pop_size // 2)\n                candidates_idx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in candidates_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 1.8)\n                stagnation_counter = 0\n\n        # finalize\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.245 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.157786201449028, 0.18613080404738025, 0.3232662683607528, 0.3055184472189937, 0.2305379480366141, 0.30140785513300017, 0.23803264822306203, 0.3399463461476494, 0.21606434391577856, 0.15488883497438433]}, "task_prompt": ""}
{"id": "e3f5d7c6-afa1-4466-a2e8-c4cd81c08a8b", "fitness": 0.29205700982199495, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with per-individual parameter adaptation, occasional Lévy-flight long jumps, and a trust-region Gaussian local search around the current best — balancing fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n      - Differential Evolution backbone with per-individual Fi/CRi sampling (jDE-style).\n      - Occasional Lévy (Cauchy-like) jumps centered at the best for long-range exploration.\n      - Trust-region Gaussian local search around the current best, with adaptive trust radius.\n      - Online adaptation of F_mean/CR_mean and p_levy based on success history.\n\n    Usage: create with budget and dim (optional additional args), call with a black-box `func`\n    that exposes bounds via func.bounds.lb and func.bounds.ub.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # sensible default population size scaled with dimension\n        if pop_size is None:\n            pop_size = int(max(6, min(80, 8 + 2 * np.sqrt(self.dim))))\n            pop_size = min(pop_size, max(2, self.budget // 10))\n        self.pop_size = int(max(2, pop_size))\n\n        # final results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast (e.g., shape (dim,1) or similar)\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # Bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_norm = max(1e-12, np.linalg.norm(range_vec))\n\n        # If budget is zero or negative, nothing to do\n        if self.budget <= 0:\n            return float(self.f_opt), self.x_opt\n\n        rng = self.rng\n\n        # Initialize population\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate as many initial individuals as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # If none evaluated (budget very small), do random probing until budget exhausted\n        if np.all(np.isinf(fvals)):\n            # do random single evaluations until budget runs out\n            while evals < self.budget:\n                x = lb + rng.rand(self.dim) * range_vec\n                f = float(func(x))\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n                evals += 1\n            return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()\n\n        # initialize best\n        finite_idx = np.where(np.isfinite(fvals))[0]\n        best_idx = finite_idx[np.argmin(fvals[finite_idx])]\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # Hyper-parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.08\n        step_scale = 0.25\n        trust_radius = 0.2 * range_norm\n        min_trust = 1e-8\n        max_trust = max(range_norm * 2.0, 1e-6)\n\n        stagnation_counter = 0\n        gen = 0\n\n        # Levy-like step generator (Cauchy-based but clipped)\n        def levy_step_vector():\n            s = rng.standard_cauchy(self.dim)\n            # clip outliers to avoid numerical blow-ups\n            s = np.clip(s, -50.0, 50.0)\n            return s\n\n        # Main loop until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # Per-generation small adaptation probabilities (jDE-like)\n            tau_F = 0.1\n            tau_CR = 0.1\n\n            # Shuffle order to avoid bias\n            idx_order = rng.permutation(self.pop_size)\n            for k in idx_order:\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi per individual\n                if rng.rand() < tau_F:\n                    Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 0.95)\n                else:\n                    Fi = np.clip(rng.normal(F_mean, 0.05), 0.05, 0.99)\n\n                if rng.rand() < tau_CR:\n                    CRi = np.clip(rng.normal(CR_mean, 0.2), 0.0, 1.0)\n                else:\n                    CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                target = pop[k]\n\n                # Choose strategy: Levy jump centered on best, or DE mutation\n                if rng.rand() < p_levy:\n                    # Long jump around best using heavy-tailed distribution\n                    step = levy_step_vector()\n                    donor = best_x + step_scale * step * (range_vec / np.maximum(range_vec, 1e-12))\n                    # small local mixing with target to keep diversity\n                    if rng.rand() < 0.5:\n                        donor = 0.5 * donor + 0.5 * target\n                else:\n                    # DE/current-to-best/1 with rand differential\n                    # pick r1,r2 distinct from k\n                    idxs = [i for i in range(self.pop_size) if i != k]\n                    if len(idxs) < 2:\n                        # fallback random perturbation\n                        donor = target + Fi * rng.randn(self.dim) * (range_vec + 1e-6)\n                    else:\n                        r1, r2 = rng.choice(idxs, size=2, replace=False)\n                        # combine guidance from best and difference vector\n                        donor = target + Fi * (best_x - target) + Fi * (pop[r1] - pop[r2])\n\n                # binomial crossover\n                cr_mask = rng.rand(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[rng.randint(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, target)\n\n                # enforce bounds by clipping\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection\n                if f_candidate < fvals[k]:\n                    pop[k] = candidate\n                    fvals[k] = f_candidate\n                    successes += 1\n                    # adapt F_mean/CR_mean mildly toward successful Fi/CRi (moving average)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # Update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # Break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation: trust-region local search around best\n            if evals < self.budget:\n                remaining = self.budget - evals\n                # sample a small number of local samples depending on remaining budget\n                local_samples = int(min(5, remaining, max(1, 2 + self.dim // 8)))\n                improved_local = False\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma per-dimension scaled by trust radius relative to range\n                    sigma = (0.3 + rng.rand(self.dim) * 0.7) * (trust_radius / (range_norm + 1e-12))\n                    candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        improved_local = True\n                        # shrink trust radius to focus search\n                        trust_radius *= 0.85\n                        trust_radius = min(max_trust, max(min_trust, trust_radius))\n                        stagnation_counter = 0\n                    else:\n                        # no improvement => slight expansion to help escape\n                        trust_radius *= 1.05\n                        trust_radius = min(max_trust, max(min_trust, trust_radius))\n                        stagnation_counter += 1\n\n                # if local search found improvements, slightly reduce p_levy\n                if improved_local:\n                    p_levy = max(0.01, p_levy * 0.95)\n\n            # Adaptive adjustments based on success count this generation\n            if successes > 0:\n                # reward exploitation by slightly decaying p_levy\n                p_levy = max(0.01, p_levy * (0.97 if successes > (0.2 * self.pop_size) else 0.99))\n                # tighten trust radius a bit if many successes\n                if successes > 2:\n                    trust_radius *= 0.95\n                stagnation_counter = max(0, stagnation_counter - successes)\n            else:\n                # stagnation: increase exploration\n                p_levy = min(0.5, p_levy * 1.05)\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # Strong stagnation reset: re-seed a fraction of population\n            if stagnation_counter > max(50, self.dim * 4):\n                # reinitialize half of population (or as budget allows)\n                k = max(1, self.pop_size // 2)\n                idxs = rng.choice(self.pop_size, size=k, replace=False)\n                for j in idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius to encourage exploration after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # finalize\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.292 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13685592572830474, 0.16918942985428076, 0.3730813396721021, 0.36893115347414884, 0.277182436001132, 0.34632546581457346, 0.25209184122738326, 0.3339871848143344, 0.4936473397572786, 0.16927798187641152]}, "task_prompt": ""}
{"id": "8d116f32-45f8-4180-a321-ec59cf2c0d05", "fitness": 0.6116716299794333, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy (Cauchy) long jumps and a shrinking/expanding trust-region local search; parameters (F, CR, jump probability, trust radius) adapt online from success history.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n      - Population-based Differential Evolution (DE/rand/1/bin) with per-individual Fi/CRi adaptive (jDE-like).\n      - Occasional heavy-tailed Lévy-like (Cauchy) jumps centered on current best to escape basins.\n      - Trust-region focused local Gaussian search around the best with adaptive radius (shrink on success, expand on failure).\n      - Online adaptation of F_mean, CR_mean and p_levy based on successes.\n    Usage: instantiate with budget, dim, optional pop_size and seed, then call with a black-box `func`.\n    The func must expose .bounds.lb and .bounds.ub (scalars or arrays).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population size heuristic: grows with dim but bounded and also can't exceed half the budget trivially\n        default_pop = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n        if pop_size is None:\n            # ensure we have at least some evaluations for selection and local search\n            max_by_budget = max(4, min(default_pop, max(4, self.budget // 10)))\n            self.pop_size = max_by_budget\n        else:\n            self.pop_size = int(pop_size)\n\n        # best-known values\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # fallback: try broadcasting\n        return np.broadcast_to(b.ravel(), (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate as many as budget allows (prefer all)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # if no evaluations possible, return empty result\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # find current best\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6       # mean mutation factor\n        CR_mean = 0.9      # mean crossover prob\n        p_levy = 0.05      # probability of performing a Levy-like jump instead of DE mutation\n        step_scale = 0.55  # base scale for Levy steps in fraction of range\n        trust_radius = 0.2 * range_norm  # initial trust radius (in absolute space)\n        max_trust = 1.5 * range_norm\n        min_trust = 1e-6 * range_norm\n\n        stagnation_counter = 0\n        no_improve_gen = 0\n        gen = 0\n\n        # helper: levy-like heavy-tailed step via clipped Cauchy (component-wise)\n        def levy_step_vector():\n            # component-wise cauchy, clipped to avoid extreme numerics\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            succ_Fs = []\n            succ_CRs = []\n            prev_best_f = best_f\n\n            # Ensure we have distinct indices helper\n            idxs = np.arange(self.pop_size)\n\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual parameters (jDE-like)\n                Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0))\n\n                if rng.rand() < p_levy:\n                    # Lévy/Cauchy long jump centered on current best_x\n                    s = levy_step_vector()\n                    # scaling: combine global step_scale and current trust radius fraction\n                    scale = step_scale * (0.5 + rng.rand() * 0.8)\n                    # scale per-dimension by range_vec (so step is in problem units)\n                    donor = best_x + scale * (s / (np.std(s) + 1e-12)) * (range_vec / (range_vec.mean() + 1e-12))\n                else:\n                    # DE/rand/1 mutation (ensure r1,r2,r3 distinct from each other and i)\n                    choices = np.delete(idxs, i)\n                    r1, r2, r3 = rng.choice(choices, 3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover -> trial vector\n                jrand = rng.randint(self.dim)\n                cross = rng.rand(self.dim) < CRi\n                cross[jrand] = True  # ensure at least one component from donor\n                trial = np.where(cross, donor, pop[i])\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate (consume budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    succ_Fs.append(Fi)\n                    succ_CRs.append(CRi)\n\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 0  # small improvement but not global\n\n                else:\n                    stagnation_counter += 1\n\n            # generation-level adaptation from successful Fi/CRi\n            if len(succ_Fs) > 0:\n                # Lehmer-like weighted mean tends to favor larger successful Fs\n                succ_Fs = np.array(succ_Fs)\n                if succ_Fs.sum() > 0:\n                    F_mean = 0.85 * F_mean + 0.15 * (np.sum(succ_Fs**2) / (np.sum(succ_Fs) + 1e-12))\n                else:\n                    F_mean = 0.95 * F_mean\n                CR_mean = 0.85 * CR_mean + 0.15 * (np.mean(succ_CRs))\n                no_improve_gen = 0\n                # reduce probability of Levy jumps if we are making progress\n                p_levy = max(0.01, p_levy * (0.95 if successes > max(1, self.pop_size // 10) else 0.98))\n            else:\n                # stagnation handling: increase exploration\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n                p_levy = min(0.5, p_levy * 1.06 + 0.005)\n                no_improve_gen += 1\n\n            # Trust-region local search around best (small number to respect budget)\n            remaining = self.budget - evals\n            if remaining > 0:\n                local_samples = min(3 + max(0, self.dim // 12), remaining, 6)\n                # anisotropic sigma: trust_radius scaled per-dim proportionally to range_vec\n                for _ in range(int(local_samples)):\n                    if evals >= self.budget:\n                        break\n                    # gaussian step scaled by trust_radius and range_vec\n                    noise = rng.randn(self.dim) * (trust_radius / (range_norm + 1e-12)) * (0.5 + rng.rand(self.dim) * 0.5) * range_vec\n                    candidate = np.clip(best_x + noise, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # on success, tighten trust radius to focus\n                        trust_radius = max(min_trust, trust_radius * 0.85)\n                        # encourage exploitation by slightly decreasing p_levy\n                        p_levy = max(0.005, p_levy * 0.9)\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful: expand trust region modestly\n                        trust_radius = min(max_trust, trust_radius * 1.08)\n                        stagnation_counter += 1\n\n            # occasional population rejuvenation if long stagnation\n            if no_improve_gen > max(20, self.dim // 2):\n                # re-seed a fraction of population (but do not exceed budget when evaluating)\n                k = max(1, self.pop_size // 3)\n                indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # update best if necessary\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust region moderately to help exploration\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # slightly increase jump probability\n                p_levy = min(0.6, p_levy * 1.2 + 0.02)\n                no_improve_gen = 0\n\n            # small safeguard: ensure trust radius stays sensible\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # if global best has not improved for a long time, hard reset some individuals\n            if stagnation_counter > max(200, self.dim * 10):\n                k = max(1, self.pop_size // 2)\n                idxs_reset = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in idxs_reset:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                stagnation_counter = 0\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                p_levy = min(0.7, p_levy * 1.3 + 0.01)\n\n            # break if budget exhausted\n            if evals >= self.budget:\n                break\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.612 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14849416142203264, 0.19037529415024368, 0.6562334783855087, 0.9821344735675207, 0.9432807422062014, 0.9468766191601976, 0.29619900483875283, 0.4959067497491655, 0.690060946538763, 0.767154829775947]}, "task_prompt": ""}
{"id": "6e700935-c786-4863-afbc-89e004871fe9", "fitness": 0.27605126510145467, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DE with occasional Lévy jumps and a trust-region local search — combines jDE-style adaptive DE, heavy-tailed long jumps for global escape, and an online-shrinking trust-region around the incumbent for focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight/Cauchy jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-call interface: optimizer(func) where func(x) returns a scalar\n    and has func.bounds.lb / func.bounds.ub (scalars or arrays).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # population size default scaled with dimension but limited\n        if pop_size is None:\n            self.pop_size = max(6, min(50, int(8 + 2 * np.sqrt(self.dim))))\n            # keep population no larger than a reasonable part of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 10))\n        else:\n            self.pop_size = int(pop_size)\n\n        # results placeholders\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # Read bounds from the function; default to [-5,5] if not provided\n        try:\n            lb_raw = func.bounds.lb\n            ub_raw = func.bounds.ub\n        except Exception:\n            lb_raw = -5.0\n            ub_raw = 5.0\n\n        lb = self._ensure_array_bounds(lb_raw)\n        ub = self._ensure_array_bounds(ub_raw)\n        range_vec = ub - lb\n        # protect against zero ranges\n        range_vec[range_vec == 0.0] = 1.0\n\n        rng = self.rng\n        pop_size = self.pop_size\n        dim = self.dim\n\n        # INITIALIZE\n        pop = lb + rng.rand(pop_size, dim) * range_vec\n        fvals = np.full(pop_size, np.inf, dtype=float)\n        evals = 0\n\n        # Initial population evaluation (sequential to obey budget)\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            fvals[i] = f\n            evals += 1\n\n        # If no evaluated individuals (very tiny budget) initialize best to inf\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            idx_valid = np.where(valid)[0]\n            best_idx = idx_valid[np.argmin(fvals[idx_valid])]\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # Nothing evaluated, return default\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # Hyperparameters & adaptive state\n        CR_mean = 0.9\n        F_mean = 0.6\n        p_levy = 0.08  # base probability of performing a long Lévy jump instead of DE mutation\n        trust_radius = 0.2  # relative to the full search range (0..1 of range_vec)\n        max_trust = min(2.0, np.linalg.norm(range_vec))  # absolute cap\n        min_trust = 1e-5\n        sigma_local = 0.2  # local anisotropic noise scale (relative)\n        stagnation_counter = 0\n        success_history = []\n        gen = 0\n\n        # Levy-like heavy-tailed step generator (Cauchy-based for simplicity)\n        def levy_step(scale=1.0):\n            # Cauchy distributed steps; clip extreme outliers to avoid numerical blow-up\n            step = rng.standard_cauchy(size=dim)\n            step = np.clip(step, -1e3, 1e3)\n            # normalize by median absolute deviation to get consistent scale\n            mad = np.median(np.abs(step - np.median(step)))\n            if mad <= 0:\n                mad = 1.0\n            step = step / mad\n            return scale * step\n\n        # Main loop: generations until budget exhausted\n        # We'll iterate generations, each generation tries to update the population\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # Prepare index pool for mutation choices\n            idxs = np.arange(pop_size)\n\n            # per-generation randomization of F and CR means (small jitter)\n            for i in range(pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Sample adaptive Fi and CRi: jDE-inspired small randomization\n                Fi = np.clip(rng.normal(F_mean, 0.2), 0.05, 1.2)\n                CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # Choose mutation strategy: with p_levy perform a Lévy-centered jump around best\n                if rng.rand() < p_levy:\n                    # centered on best with heavy-tailed offset scaled by trust_radius and global range\n                    step = levy_step(scale=1.0)\n                    donor = best_x + (trust_radius * step) * range_vec\n                else:\n                    # Standard DE/rand/1 or DE/best/1 hybrid: pick three distinct indices\n                    choices = idxs[idxs != i]\n                    if choices.size < 3:\n                        # fallback to random gaussian neighbor\n                        donor = pop[i] + Fi * rng.randn(dim) * range_vec\n                    else:\n                        r = rng.choice(choices, 3, replace=False)\n                        r1, r2, r3 = r\n                        # choose between rand/1 and best/1 half-half to mix exploration/exploitation\n                        if rng.rand() < 0.5:\n                            donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        else:\n                            donor = best_x + Fi * (pop[r1] - pop[r2])\n\n                # Binomial crossover to produce trial vector\n                jrand = rng.randint(dim)\n                mask = rng.rand(dim) < CRi\n                mask[jrand] = True  # ensure at least one component from donor\n                trial = np.where(mask, donor, pop[i])\n\n                # Bound handling: projection (clip)\n                candidate = np.clip(trial, lb, ub)\n\n                # Evaluate candidate (if budget allows)\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    stagnation_counter = 0\n                    # nudge parameter means towards successful parameters (small step)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    stagnation_counter += 1\n                    # mild random walk adjustment for unsuccessful individuals occasionally\n                    if rng.rand() < 0.02:\n                        pop[i] = np.clip(pop[i] + 0.1 * rng.randn(dim) * range_vec, lb, ub)\n\n                # Update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n\n                # enforce budget safety at each step (already checked)\n            # End per-generation population loop\n\n            # Trust-region local search around the current best: sample a few focused candidates\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples scaled with dimension but small\n            local_samples = int(np.clip(3 + dim // 8, 1, min(8, remaining)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic Gaussian around best_x scaled by trust_radius and per-dimension range\n                local_scale = trust_radius * sigma_local\n                candidate = best_x + rng.normal(0.0, 1.0, size=dim) * (local_scale * range_vec)\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    successes += 1\n                    stagnation_counter = 0\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    # also slightly reduce sigma_local to intensify local search\n                    sigma_local = max(1e-3, sigma_local * 0.95)\n                else:\n                    # unsuccessful => expand trust radius slightly to encourage escape\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n                    sigma_local = min(1.0, sigma_local * 1.02)\n                    stagnation_counter += 1\n\n            # Adjust p_levy and adaptation rates based on success count\n            if successes > max(1, pop_size * 0.15):\n                # good progress: focus more on exploitation\n                p_levy = max(0.01, p_levy * 0.9)\n                # nudge means to be a bit more exploitative\n                F_mean = np.clip(F_mean * 0.98, 0.05, 1.2)\n                CR_mean = np.clip(CR_mean * 1.01, 0.0, 1.0)\n            elif successes == 0:\n                # stagnation tendency: increase chance of long jumps and diversify population\n                p_levy = min(0.6, p_levy * 1.08 + 0.01)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n            # keep p_levy reasonable\n            p_levy = np.clip(p_levy, 0.01, 0.6)\n\n            # Occasional partial re-seeding if long stagnation\n            if stagnation_counter > max(50, pop_size * 5):\n                # reinitialize a fraction of the population to diversify\n                k = max(1, pop_size // 2)\n                idx_reseed = rng.choice(pop_size, k, replace=False)\n                for j in idx_reseed:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(dim) * range_vec\n                    # evaluate new individual to give it a chance immediately\n                    try:\n                        f_new = float(func(pop[j]))\n                    except Exception:\n                        f_new = np.inf\n                    fvals[j] = f_new\n                    evals += 1\n                    if f_new < best_f:\n                        best_f = f_new\n                        best_x = pop[j].copy()\n                # after reseed, slightly increase trust radius to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.3)\n                stagnation_counter = 0  # reset\n\n            # Soft termination check (if best is already very low or budget reached)\n            if evals >= self.budget:\n                break\n\n        # final result\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.276 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11531633082952986, 0.18192072590803054, 0.29807465057119364, 0.4565257880377436, 0.2592975511734682, 0.4608835010544471, 0.2369192677047971, 0.3192824308491574, 0.22251503913503334, 0.2097773657511458]}, "task_prompt": ""}
{"id": "1a81f52a-83b3-417d-9801-7341997077c3", "fitness": 0.29933324106551085, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DE with occasional Lévy jumps and trust-region local refinement — fast global exploration via DE and Lévy, focused exploitation via adaptive trust-region sampling and online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # population size heuristic: scale with dim but keep reasonable and not bigger than budget fraction\n        if pop_size is None:\n            p = int(8 + 2 * np.sqrt(self.dim))\n            p = max(8, p)\n            p = min(60, p)\n            p = min(p, max(4, self.budget // 20))\n            self.pop_size = p\n        else:\n            self.pop_size = int(pop_size)\n\n        # output placeholders\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Accept scalar or array-like bounds and return a float ndarray of shape (dim,)\n        \"\"\"\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast (e.g. length 2 -> fill)\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(f\"Bounds must be scalar or length {self.dim} arrays.\")\n\n    def _levy_step(self):\n        \"\"\"\n        Simple heavy-tailed step using truncated Cauchy (standard_cauchy) and\n        soft truncation to avoid numerical infinities.\n        \"\"\"\n        # draw Cauchy and clip extremes to avoid overflow\n        s = self.rng.standard_cauchy(self.dim)\n        # soft clip using tanh-like scaling for large values\n        s = np.tanh(s / 10.0) * 10.0\n        return s\n\n    def __call__(self, func):\n        # fetch and normalize bounds\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            # fallback to typical BBOB bounds if func does not provide bounds\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n\n        # ensure bounds are sane\n        if np.any(ub <= lb):\n            raise ValueError(\"Upper bounds must be strictly greater than lower bounds.\")\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population sequentially (stop if budget exhausted)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception as e:\n                # if evaluation fails, mark as inf and continue\n                fvals[i] = np.inf\n            evals += 1\n\n        # determine initial best among evaluated\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals)\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # no evaluations possible (budget==0)\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.5       # crossover rate mean\n        p_levy = 0.08       # probability of a Lévy jump per candidate\n        min_trust = 1e-8\n        max_trust = 2.0\n        trust_radius = 0.2  # relative trust radius (fraction of range)\n        stagnation_counter = 0\n        success_history = 0\n        gen = 0\n\n        # Loop until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # Process each individual sequentially to count evaluations precisely\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual control parameters (jDE-style small randomization)\n                # Fi: sample from truncated Cauchy around F_mean to allow heavy tails but keep bounds\n                Fi = float(self.rng.standard_cauchy() * 0.1 + F_mean)\n                Fi = np.clip(Fi, 0.05, 0.95)\n                CRi = float(self.rng.normal(CR_mean, 0.1))\n                CRi = float(np.clip(CRi, 0.0, 1.0))\n\n                # decide exploration via Levy jump\n                if self.rng.random() < p_levy:\n                    # Lévy-centered jump around best_x scaled by trust_radius and global range\n                    step = self._levy_step()\n                    # normalize step to control magnitude then scale by trust_radius\n                    step = step / (np.linalg.norm(step) + 1e-12)\n                    mag = (0.5 + 0.5 * self.rng.random()) * trust_radius\n                    donor = best_x + mag * step * range_norm * (range_vec / (range_vec.mean() + 1e-12))\n                else:\n                    # DE/rand/1 mutation: pick 3 distinct indices different from i\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback: random directional step\n                        j = self.rng.integers(0, self.pop_size)\n                        donor = pop[j] + Fi * (best_x - pop[j])\n                    else:\n                        r = self.rng.choice(idxs, size=3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cr_mask = self.rng.random(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[self.rng.integers(0, self.dim)] = True  # ensure at least one gene from donor\n\n                candidate = np.where(cr_mask, donor, pop[i])\n                # projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate (respect budget)\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                replace = (not np.isfinite(fvals[i])) or (f_candidate <= fvals[i])\n                if replace:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt means slightly toward successful parameters\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # small drift away from unsuccessful Fi/CRi\n                    F_mean = 0.995 * F_mean + 0.005 * 0.5\n                    CR_mean = 0.995 * CR_mean + 0.005 * 0.5\n\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # prevent local variable leakage (not necessary in Python, but keep conceptually)\n                # end individual\n\n            # End of generation adjustments\n\n            # Trust-region local search around best (budget permitting)\n            remaining = self.budget - evals\n            if remaining > 0:\n                # number of local tries: small, scaled with dim and remaining budget\n                n_local = min(max(1, self.dim // 3), remaining, 8)\n                for _ in range(n_local):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic Gaussian noise scaled by trust_radius and problem range\n                    sigma = trust_radius * (0.5 + self.rng.random(self.dim) * 0.5)\n                    candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n\n                    try:\n                        f_candidate = float(func(candidate))\n                    except Exception:\n                        f_candidate = np.inf\n                    evals += 1\n\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local exploitation: shrink trust region to focus\n                        trust_radius *= 0.85\n                        trust_radius = max(trust_radius, min_trust)\n                        stagnation_counter = 0\n                        success_history += 1\n                    else:\n                        # unsuccessful local step: expand slightly to escape\n                        trust_radius *= 1.05\n                        trust_radius = min(trust_radius, max_trust)\n                        stagnation_counter += 1\n\n            # adapt exploration probability based on recent progress\n            if successes > max(1, 0.1 * self.pop_size):\n                # many successes => favor exploitation (fewer Levy jumps)\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                # few successes => increase chance of long jumps\n                p_levy = min(0.6, p_levy * 1.05 + 0.005)\n\n            # slight cooling of adaptation means to avoid runaway\n            F_mean = np.clip(F_mean, 0.05, 0.95)\n            CR_mean = np.clip(CR_mean, 0.0, 1.0)\n\n            # stagnation handling: if no improvement for long, partially re-seed population\n            if stagnation_counter > max(50, 5 * self.dim):\n                # reinitialize half of the population (or as many as budget allows)\n                k = max(1, self.pop_size // 2)\n                k = min(k, max(1, (self.budget - evals)))\n                # replace worst individuals\n                worst_idxs = np.argsort(-fvals)[:k]\n                for j in worst_idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # encourage Lévy jumps after reset\n                p_levy = min(0.5, p_levy + 0.08)\n\n            # small safeguard: do not let trust radius collapse to zero or explode\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # generation end; loop continues until budget exhausted\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.299 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14407200756171068, 0.16529389032128472, 0.363648682548104, 0.4324952307044435, 0.2783019056925006, 0.43030867196510825, 0.27083670822228045, 0.32398400791697024, 0.34299451731661723, 0.24139678840608947]}, "task_prompt": ""}
{"id": "fd6b0d46-6a23-46d9-a8fc-a6f7bbc106ce", "fitness": 0.23051044296089693, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and an online-adaptive trust-region local search around the current best (fast global exploration + focused local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # default population size scaled with dimension but limited by budget\n        if pop_size is None:\n            p = max(8, int(8 + 2 * np.sqrt(self.dim)))\n            p = min(p, max(4, self.budget // 20))\n            self.pop_size = int(np.clip(p, 4, 200))\n        else:\n            self.pop_size = int(pop_size)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        max_trust = np.linalg.norm(range_vec) * 2.0 + 1e-12\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        best_idx = None\n        best_f = np.inf\n        best_x = None\n\n        # Evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                f = float(func(pop[i]))\n            except Exception:\n                f = np.inf\n            fvals[i] = f\n            evals += 1\n            if f < best_f:\n                best_f = f\n                best_x = pop[i].copy()\n                best_idx = i\n\n        # If we couldn't evaluate anything, return immediately\n        if evals == 0:\n            return self.f_opt, self.x_opt\n\n        # Algorithm hyper-parameters (adapted online)\n        F_mean = 0.6       # scale factor mean\n        CR_mean = 0.9      # crossover probability mean\n        p_levy = 0.08      # probability of using Lévy jump\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius (in absolute units)\n        stagnation_counter = 0\n        stagnation_limit = max(20, self.budget // 50)\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step (simple Cauchy-based, normalized)\n        def levy_step():\n            s = self.rng.standard_cauchy(self.dim)\n            # limit extreme outliers\n            s = np.clip(s, -1e3, 1e3)\n            norm = np.linalg.norm(s) + 1e-12\n            return s / norm  # unit vector with heavy-tailed sampling\n\n        # projection helper (clip)\n        def project(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # Main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # For each target vector in population do one DE-style update (sequential)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual parameters (jDE-like)\n                Fi = np.clip(self.rng.normal(F_mean, 0.15), 0.05, 0.99)\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # choose three distinct indices different from i\n                idxs = np.delete(np.arange(self.pop_size), i)\n                if idxs.size < 3:\n                    # degenerate small population: perturb around best\n                    r1 = r2 = r3 = best_idx if best_idx is not None else i\n                else:\n                    r1, r2, r3 = self.rng.choice(idxs, size=3, replace=False)\n\n                # build donor vector: with some probability perform Levy jump around best\n                if self.rng.random() < p_levy and best_x is not None:\n                    step = levy_step() * (trust_radius * 2.0) * (range_vec / (np.linalg.norm(range_vec) + 1e-12))\n                    donor = best_x + step\n                else:\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover\n                cr_mask = self.rng.random(self.dim) < CRi\n                # ensure at least one dim taken from donor\n                if not np.any(cr_mask):\n                    cr_mask[self.rng.integers(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n                trial = project(trial)\n\n                # Evaluate candidate\n                try:\n                    f_trial = float(func(trial))\n                except Exception:\n                    f_trial = np.inf\n                evals += 1\n\n                # Selection: greedy\n                if f_trial < fvals[i]:\n                    pop[i] = trial\n                    fvals[i] = f_trial\n                    successes += 1\n                    # move means slightly toward successful parameters\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                    # update global best\n                    if f_trial < best_f:\n                        best_f = f_trial\n                        best_x = trial.copy()\n                        best_idx = i\n                        stagnation_counter = 0\n                        # successful local improvement => shrink trust radius slightly\n                        trust_radius = max(1e-12, trust_radius * 0.92)\n                    else:\n                        # small exploitation push\n                        trust_radius = max(1e-12, trust_radius * 0.995)\n                else:\n                    # unsuccessful -> encourage exploration slightly\n                    trust_radius = min(max_trust, trust_radius * 1.002)\n                    stagnation_counter += 1\n\n                # If budget is exhausted, break early\n                if evals >= self.budget:\n                    break\n\n            # Trust-region local search around best: a few focused samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, 3 + self.dim // 10))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian noise scaled by trust_radius and per-dim range\n                anisotropy = 0.2 + self.rng.random(self.dim)  # between 0.2 and 1.2\n                step = self.rng.normal(0, 1, size=self.dim) * (trust_radius * anisotropy) * (range_vec / (np.max(range_vec) + 1e-12))\n                candidate = project(best_x + step)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius\n                    trust_radius = max(1e-12, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local trap\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # Adaptive adjustments per generation\n            # Adjust p_levy: increase if few successes, decrease if many successes\n            if successes < max(1, self.pop_size // 10):\n                p_levy = min(0.4, p_levy * 1.06)\n            else:\n                p_levy = max(0.005, p_levy * 0.96)\n\n            # Slightly nudge CR_mean/F_mean toward exploitation when many successes\n            if successes > self.pop_size * 0.2:\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                F_mean = np.clip(F_mean * 0.98, 0.05, 0.99)\n            else:\n                # encourage larger steps if stagnating\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n\n            # stagnation handling: if no improvement for a while, diversify (re-seed half population)\n            if stagnation_counter > stagnation_limit:\n                n_reseed = max(1, self.pop_size // 2)\n                # replace worst n_reseed individuals\n                worst_idx = np.argsort(fvals)[-n_reseed:]\n                for j in worst_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius after reset (but cap)\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # increase exploration chance briefly\n                p_levy = min(0.5, p_levy * 1.5)\n\n        # Finalize outputs\n        # If we have better than stored\n        if best_f < self.f_opt:\n            self.f_opt = best_f\n            self.x_opt = best_x\n\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.231 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18496104155578486, 0.1646419899922551, 0.2914415576591183, 0.29969813440101756, 0.23770432113271744, 0.2822194574626614, 0.2408963876940189, 0.23999244199167802, 0.19756635030298453, 0.16598274741673313]}, "task_prompt": ""}
{"id": "c1fe72ba-d566-4013-bcae-87c0ff407ac9", "fitness": 0.32704691965520866, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and trust-region local searches with online step-size adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on simple success-history statistics.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size depends on dim and budget\n        if pop_size is None:\n            # scale with dimension, but never exceed a fraction of budget\n            self.pop_size = int(min(max(10, 6 * self.dim), max(10, self.budget // 8)))\n        else:\n            self.pop_size = int(pop_size)\n            self.pop_size = max(4, self.pop_size)  # at least 4 for DE\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _proj_bounds(self, x, lb, ub):\n        # simple projection to bounds\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def __call__(self, func):\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        range_vec = ub - lb\n        range_norm = float(np.linalg.norm(range_vec))\n        if range_norm == 0:\n            range_norm = 1.0\n\n        # algorithm hyper-parameters (adaptive)\n        p_levy = 0.08                     # base probability of a Lévy jump\n        step_scale = 0.25                 # base scale for Levy and trust radius\n        trust_radius = step_scale * range_norm * 0.5\n        min_trust = 1e-8\n        max_trust = range_norm * 2.0\n\n        # DE parameter means for jDE-like adaptation\n        F_mean = 0.6\n        CR_mean = 0.9\n\n        # initialize population uniformly\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate as many individuals as budget allows\n        to_eval = min(self.pop_size, self.budget)\n        order = np.arange(self.pop_size)\n        self.rng.shuffle(order)\n        for k in order[:to_eval]:\n            fvals[k] = float(func(pop[k]))\n            evals += 1\n\n        # if some individuals couldn't be evaluated due to tiny budget, leave them with inf\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals)\n            best_f = float(fvals[best_idx])\n            best_x = pop[best_idx].copy()\n        else:\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = self.rng.uniform(lb, ub)\n            return self.f_opt, self.x_opt\n\n        # small helper: Levy-like heavy-tailed step using Cauchy per-dimension\n        def levy_step(sigma):\n            # Cauchy (alpha=1) heavy tail; clamp extremes\n            step = self.rng.standard_cauchy(size=self.dim)\n            step = np.clip(step, -20.0, 20.0)  # avoid numerical blowups\n            # scale anisotropically by range_vec and sigma\n            return step * (sigma * (range_vec / (np.maximum(range_vec, 1e-12))))\n\n        stagnation_counter = 0\n        success_Fs = []\n        success_CRs = []\n        gen = 0\n\n        # main loop: generate candidates until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            # per-generation jitter on means to encourage diversity\n            F_mean = np.clip(F_mean * (1.0 + 0.02 * (self.rng.randn())), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + 0.05 * (self.rng.randn()), 0.0, 1.0)\n\n            # process each population member sequentially (target-to-trial)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                xi = pop[i].copy()\n                fi = fvals[i]\n\n                # sample parameters jDE-like\n                Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 0.99)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                # decide mutation style: either rand/1 or current-to-best with some bias\n                if self.rng.rand() < 0.7 and self.pop_size >= 4:\n                    # rand/1\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r = self.rng.choice(idxs, 3, replace=False)\n                    xr1, xr2, xr3 = pop[r[0]], pop[r[1]], pop[r[2]]\n                    donor = xr1 + Fi * (xr2 - xr3)\n                else:\n                    # current-to-best/1 (biased toward best)\n                    # pick two random others\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r = self.rng.choice(idxs, 2, replace=False)\n                    xr1, xr2 = pop[r[0]], pop[r[1]]\n                    donor = xi + Fi * (best_x - xi) + Fi * (xr1 - xr2)\n\n                # occasionally do a Levy jump centered on best for long-range exploration\n                if self.rng.rand() < p_levy:\n                    sigma = step_scale * (trust_radius / max(1e-9, range_norm))\n                    donor = best_x + levy_step(sigma)\n                    # small chance to mix in random individual to keep diversity\n                    if self.rng.rand() < 0.15:\n                        donor = 0.7 * donor + 0.3 * pop[self.rng.randint(self.pop_size)]\n\n                # binomial crossover\n                candidate = xi.copy()\n                mask = self.rng.rand(self.dim) < CRi\n                # ensure at least one dimension from donor\n                if not np.any(mask):\n                    mask[self.rng.randint(self.dim)] = True\n                candidate[mask] = donor[mask]\n\n                # project to bounds\n                candidate = self._proj_bounds(candidate, lb, ub)\n\n                # evaluate candidate (consumes one eval)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy\n                if f_candidate <= fi:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    success_Fs.append(Fi)\n                    success_CRs.append(CRi)\n\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = float(f_candidate)\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                        # shrink trust region a bit on success\n                        trust_radius = max(min_trust, trust_radius * 0.88)\n                    else:\n                        # mild improvement within population but not global best\n                        stagnation_counter += 1\n                else:\n                    stagnation_counter += 1\n\n                # occasional small random local mutation on unsuccessful individuals\n                if evals >= self.budget:\n                    break\n\n            # end of generation\n\n            # adapt F_mean and CR_mean from successful parameters (Lehmer mean-like)\n            if len(success_Fs) > 0:\n                # Lehmer mean promotes larger successful F\n                sf = np.array(success_Fs)\n                F_mean = np.clip((np.sum(sf * sf) / np.sum(sf)), 0.05, 0.99)\n                CR_mean = np.clip(np.mean(success_CRs), 0.0, 1.0)\n                success_Fs = []\n                success_CRs = []\n            else:\n                # if no successes, nudge F_mean to encourage exploration\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean, 0.0, 1.0)\n\n            # trust-region local search: sample a few Gaussian candidates around best\n            if evals < self.budget:\n                # number of local samples depends on dim and remaining budget\n                rem = self.budget - evals\n                local_samples = min(max(1, self.dim // 4), rem, 6)\n                # anisotropic sigma per-dimension scaled by trust_radius and range_vec\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # sigma: smaller when trust radius small\n                    sigma = (trust_radius / max(1e-12, range_norm)) * (0.5 + 0.5 * self.rng.rand())\n                    perturb = self.rng.randn(self.dim) * (sigma * (range_vec * 0.5))\n                    candidate = best_x + perturb\n                    candidate = self._proj_bounds(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = float(f_candidate)\n                        best_x = candidate.copy()\n                        # successful local step => shrink trust region\n                        trust_radius = max(min_trust, trust_radius * 0.7)\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful => expand a bit to escape local traps\n                        trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # stagnation handling: if no improvement for long, increase chance of Levy jumps and re-seed half pop\n            if stagnation_counter > max(10, self.pop_size):\n                p_levy = min(0.5, p_levy * 1.4)\n                # re-seed half of the population (keep the best)\n                k = max(1, self.pop_size // 2)\n                idxs = np.arange(self.pop_size)\n                # exclude best index to keep elite\n                # find current best index\n                cur_best_idx = np.argmin(fvals)\n                if np.isfinite(fvals[cur_best_idx]):\n                    keep = set([cur_best_idx])\n                else:\n                    keep = set()\n                cand_idxs = [j for j in idxs if j not in keep]\n                to_replace = self.rng.choice(cand_idxs, k, replace=False)\n                for j in to_replace:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                # slightly enlarge trust radius after reset to promote exploration\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n                # also nudge parameter means toward exploration\n                F_mean = np.clip(F_mean * 1.08, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.95, 0.0, 1.0)\n\n            # gentle decay of levy probability back to base\n            p_levy = max(0.01, 0.95 * p_levy)\n\n        # finalize best\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.327 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10208482235962157, 0.1638855460998344, 0.30063807178420754, 0.8130741149238958, 0.233491624538414, 0.756872496497871, 0.2386143881359276, 0.2634179986366121, 0.24094044802372871, 0.1574496855519737]}, "task_prompt": ""}
{"id": "431ae243-1860-4660-917c-0d0bb3b031f4", "fitness": 0.31043491022797837, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online step-size and operator adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # sensible default population size depending on dimension and budget\n        if pop_size is None:\n            p = int(10 + 2 * np.sqrt(max(1, self.dim)))\n            p = max(8, min(80, p))\n            # don't make population absurdly large relative to budget\n            p = min(p, max(4, self.budget // 8))\n            self.pop_size = p\n        else:\n            self.pop_size = int(pop_size)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Allow scalar or array-like, return numpy array length dim\n        b_arr = np.asarray(b, dtype=float)\n        if b_arr.ndim == 0:\n            return np.full(self.dim, float(b_arr))\n        if b_arr.size == self.dim:\n            return b_arr.astype(float)\n        # broadcast if possible\n        try:\n            return np.broadcast_to(b_arr, (self.dim,)).astype(float)\n        except Exception:\n            # fallback to default [-5,5]\n            return np.full(self.dim, float(b_arr.flat[0]))\n\n    def __call__(self, func):\n        # get bounds if provided, otherwise use [-5,5] (BBOB default)\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        # Avoid zero ranges\n        range_vec = np.where(range_vec <= 0, 1.0, range_vec)\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # initial evaluations (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # if not all individuals evaluated, set remaining to random but keep fvals=inf\n        # allowed - they won't be selected until evaluated\n\n        # Algorithm hyper-parameters (adapted online)\n        F_mean = 0.5\n        CR_mean = 0.9\n        p_levy = 0.08       # base probability of a Lévy jump\n        trust_radius = 0.15  # relative to range_vec\n        min_trust, max_trust = 1e-4, 2.0\n        stagnation_counter = 0\n        last_improvement_eval = evals\n        max_stagnation = max(20, self.budget // 40)\n\n        # helper: Levy-like heavy-tailed step using clipped Cauchy\n        def levy_step(scale=1.0):\n            # Use multidimensional Cauchy as a heavy-tailed step\n            step = self.rng.standard_cauchy(self.dim)\n            # clip extreme outliers to avoid numerical blow-up\n            step = np.clip(step, -1e2, 1e2)\n            # normalize magnitude to avoid huge steps and scale\n            med = np.median(np.abs(step)) if np.median(np.abs(step)) > 0 else 1.0\n            step = step / med\n            return step * float(scale)\n\n        # main loop: iterate until budget exhausted\n        # We'll process population members sequentially; each evaluated candidate consumes one eval.\n        # We treat \"generation-like\" passes over population for adaptation.\n        while evals < self.budget:\n            successful_F = []\n            successful_CR = []\n            successes = 0\n\n            # Shuffle order for fairness\n            order = np.arange(self.pop_size)\n            self.rng.shuffle(order)\n\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # ensure current target is evaluated; if not (fvals[idx]=inf) and budget allows, evaluate it\n                if not np.isfinite(fvals[idx]) and evals < self.budget:\n                    fvals[idx] = float(func(pop[idx]))\n                    evals += 1\n                    if fvals[idx] < self.f_opt:\n                        self.f_opt = fvals[idx]\n                        self.x_opt = pop[idx].copy()\n                        last_improvement_eval = evals\n\n                # Choose Fi and CRi adaptively (jDE-like)\n                # Fi: sample from Cauchy centered at F_mean but clipped\n                Fi = F_mean + 0.1 * self.rng.standard_cauchy()\n                if np.isnan(Fi) or not np.isfinite(Fi):\n                    Fi = F_mean\n                Fi = float(np.clip(Fi, 0.05, 1.0))\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0))\n\n                # Decide whether to perform Levy jump or DE variation\n                if self.rng.random() < p_levy:\n                    # Levy jump around current global best for exploration\n                    if self.x_opt is None:\n                        center = lb + self.rng.random(self.dim) * range_vec\n                    else:\n                        center = self.x_opt\n                    # step scaled by trust radius and global range\n                    step_scale = self.rng.uniform(0.5, 1.5) * trust_radius\n                    step = levy_step(scale=1.0) * step_scale * range_vec\n                    candidate = center + step\n                    # small local jitter as well\n                    candidate += np.random.normal(0, 0.01, self.dim) * range_vec\n                    op_type = \"levy\"\n                else:\n                    # Differential Evolution rand/1/bin style\n                    # pick three distinct indices different from idx\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != idx]\n                    if idxs.size < 3:\n                        # fallback: random sample in bounds\n                        candidate = lb + self.rng.random(self.dim) * range_vec\n                        op_type = \"randfallback\"\n                    else:\n                        r = self.rng.choice(idxs, size=3, replace=False)\n                        r1, r2, r3 = r\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = self.rng.random(self.dim) < CRi\n                        # ensure at least one dimension crosses\n                        if not np.any(cr_mask):\n                            cr_mask[self.rng.integers(0, self.dim)] = True\n                        candidate = np.where(cr_mask, donor, pop[idx])\n                        op_type = \"de\"\n\n                # project to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget available\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population (target-to-trial)\n                # if target was unevaluated, treat it as worse\n                if not np.isfinite(fvals[idx]) or f_candidate < fvals[idx]:\n                    # accept candidate\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    successful_F.append(Fi)\n                    successful_CR.append(CRi)\n\n                    # update global best if improved\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = candidate.copy()\n                        last_improvement_eval = evals\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                else:\n                    # rejected\n                    stagnation_counter += 1\n\n                # quick adaptation of means when successes accrue\n                if len(successful_CR) > 0:\n                    CR_mean = 0.9 * CR_mean + 0.1 * np.mean(successful_CR)\n                if len(successful_F) > 0:\n                    # use Lehmer-like weighting to prefer larger Fi that succeeded\n                    F_mean = 0.9 * F_mean + 0.1 * np.mean(successful_F)\n\n                # adjust Levy probability slightly based on recent improvements\n                if (evals - last_improvement_eval) > max_stagnation:\n                    p_levy = min(0.5, p_levy * 1.15 + 0.02)\n                else:\n                    p_levy = max(0.02, p_levy * 0.995)\n\n                # safety: keep trust_radius within bounds\n                trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # End of generation / pass over population\n\n            # Trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            local_success = 0\n            for k in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                anis = 0.5 + self.rng.random(self.dim)\n                sigma = trust_radius * anis\n                candidate = self.x_opt + self.rng.normal(0, 1, self.dim) * (sigma * range_vec)\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    last_improvement_eval = evals\n                    local_success += 1\n                    # shrink trust radius to focus further\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    # slightly expand to escape local poor area\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # adjust F/CR means slightly toward exploitation if many local successes\n            if local_success > 0:\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.7, 0.0, 1.0)\n\n            # stagnation handling and population re-seeding\n            if (evals - last_improvement_eval) > max(10, self.budget // 40):\n                # mild diversification: reinitialize a fraction of worst individuals\n                worst_frac = 0.4\n                k_reset = max(1, int(self.pop_size * worst_frac))\n                # choose worst indices that are not the best\n                order_by_f = np.argsort(fvals)\n                # ensure best (first in sorted) isn't reset\n                to_reset = order_by_f[-k_reset:]\n                # Avoid resetting the global best if present\n                best_idx = np.argmin(fvals) if np.any(np.isfinite(fvals)) else None\n                if best_idx is not None:\n                    to_reset = [i for i in to_reset if i != best_idx]\n                for j in to_reset:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    # evaluate new individual if budget allows\n                    if evals < self.budget:\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                        if fvals[j] < self.f_opt:\n                            self.f_opt = fvals[j]\n                            self.x_opt = pop[j].copy()\n                            last_improvement_eval = evals\n                # enlarge trust radius slightly to encourage exploration after reset\n                trust_radius = min(max_trust, trust_radius * 1.6)\n\n            # if nothing improved for a long time, perform an aggressive global jump for best\n            if (evals - last_improvement_eval) > max(100, self.budget // 10):\n                # try a few big Levy jumps from random population members\n                big_tries = min(3, self.budget - evals)\n                for _ in range(big_tries):\n                    if evals >= self.budget:\n                        break\n                    center = self.x_opt if self.x_opt is not None else (lb + self.rng.random(self.dim) * range_vec)\n                    step = levy_step(scale=5.0) * trust_radius * range_vec\n                    candidate = np.clip(center + step, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = candidate.copy()\n                        last_improvement_eval = evals\n                # increase Levy probability to encourage more long jumps\n                p_levy = min(0.6, p_levy * 1.3)\n\n        # final results\n        # Ensure we return actual numpy array for x_opt\n        if self.x_opt is None:\n            # if we never evaluated anything (budget==0), return random point\n            self.x_opt = lb + self.rng.random(self.dim) * range_vec\n            self.f_opt = float(func(self.x_opt)) if self.budget > 0 else np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.310 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1299190863820192, 0.1827456296718899, 0.3200157201594215, 0.31795759248101385, 0.23972013011588544, 0.5641177197382459, 0.303456382489772, 0.22092342185594815, 0.6562403280925276, 0.16925309129305988]}, "task_prompt": ""}
{"id": "b23ef5bf-7e35-49e8-b7ae-80cccef1769c", "fitness": 0.25635708459936984, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size and parameter adaptation for robust exploration-exploitation on continuous problems.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=42)\n        f_opt, x_opt = opt(func)\n\n    The func object is expected to expose bounds via func.bounds.lb and func.bounds.ub\n    (each can be scalar or array-like).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        # sensible default population size scaling with dimension:\n        if pop_size is None:\n            # small problems: ensure enough diversity, but never more than budget/4\n            p = int(np.clip(6 + 4 * self.dim, 10, max(10, self.budget // 6)))\n        else:\n            p = int(pop_size)\n        self.pop_size = max(4, min(p, self.budget))  # at least 4\n        # placeholders for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # broadcast bounds to shape (dim,)\n        b = np.array(b, dtype=float)\n        if b.shape == ():\n            return np.full(self.dim, float(b))\n        if b.shape[0] != self.dim:\n            # try to broadcast\n            return np.array(b, dtype=float).reshape((self.dim,))\n        return b\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        # bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng_uniform = rng.uniform\n\n        # prepare population\n        pop_size = min(self.pop_size, max(2, self.budget))\n        X = rng_uniform(lb, ub, size=(pop_size, self.dim))\n        fvals = np.full(pop_size, np.inf)\n        evals = 0\n\n        # evaluate initial population as much as budget allows\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            xi = X[i]\n            f = func(xi)\n            fvals[i] = f\n            evals += 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = xi.copy()\n\n        # if budget exhausted during init, return best we have\n        if evals >= self.budget:\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6                # mean mutation factor\n        CR_mean = 0.3               # mean crossover rate\n        p_levy = 0.06               # probability of long Lévy jump\n        trust_radius = 0.12         # relative to search range (fraction)\n        min_F, max_F = 0.1, 0.95\n        max_trust = 1.0\n        min_trust = 1e-6\n        stagnation_counter = 0\n        stagnation_threshold = max(50, 5 * self.dim)\n        success_memory_F = []\n        success_memory_CR = []\n        # we will use moving averages with learning rate\n        adapt_lr = 0.12\n\n        total_range = ub - lb\n        # avoid zero-range\n        total_range[total_range == 0.0] = 1e-8\n\n        # helper: Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step(scale=0.08):\n            # Cauchy heavy tails, limited to avoid blow-up\n            s = rng.standard_cauchy(self.dim) * scale\n            # clip moderately to avoid numerical issues\n            s = np.clip(s, -10.0, 10.0)\n            return s\n\n        # bookkeeping for improvements\n        last_improvement_evals = evals\n\n        # main loop until budget exhausted\n        while evals < self.budget:\n            # one \"generation\": process population sequentially\n            generation_successes = 0\n            # per-generation temporary success lists for update\n            gen_success_F = []\n            gen_success_CR = []\n\n            for i in range(pop_size):\n                if evals >= self.budget:\n                    break\n\n                xi = X[i]\n                fi = fvals[i]\n\n                # sample individual DE parameters (jDE-like)\n                Fi = rng.normal(F_mean, 0.15)\n                Fi = float(np.clip(Fi, min_F, max_F))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # decide exploration: Levy long jump centered on best OR DE mutation\n                if rng.random() < p_levy:\n                    # long-range exploration: jump from current best with heavy tail\n                    # scale step by trust_radius and problem range\n                    best_x = self.x_opt.copy()\n                    cauchy_step = levy_step(scale=0.08 + 0.6 * trust_radius)\n                    candidate = best_x + cauchy_step * total_range\n                    # small random blend with current individual to keep population linkage\n                    mix = rng.random(self.dim) * 0.5\n                    candidate = np.where(rng.random(self.dim) < 0.5, candidate, xi + rng.normal(0, 0.02, self.dim) * total_range)\n                else:\n                    # DE/rand/1 with optional current-to-best influence\n                    # pick three distinct indices different from i\n                    idxs = list(range(pop_size))\n                    idxs.remove(i)\n                    r = rng.choice(idxs, size=3, replace=False)\n                    r1, r2, r3 = r\n                    donor = X[r1] + Fi * (X[r2] - X[r3])\n                    # small current-to-best pull (helps exploitation)\n                    donor = donor + Fi * 0.15 * (self.x_opt - xi)\n                    # binomial crossover\n                    jrand = rng.integers(self.dim)\n                    mask = rng.random(self.dim) < CRi\n                    mask[jrand] = True\n                    candidate = np.where(mask, donor, xi)\n\n                # projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if we have budget\n                if evals >= self.budget:\n                    break\n                f_candidate = func(candidate)\n                evals += 1\n\n                # greedy selection\n                if f_candidate <= fi:\n                    # accept\n                    X[i] = candidate\n                    fvals[i] = f_candidate\n                    generation_successes += 1\n                    gen_success_F.append(Fi)\n                    gen_success_CR.append(CRi)\n                    # update global best\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = candidate.copy()\n                        last_improvement_evals = evals\n                        stagnation_counter = 0\n                    else:\n                        # small decrease indicates exploration success but not global\n                        pass\n                else:\n                    # rejection, keep old\n                    pass\n\n                # adapt means slightly toward successful parameters (done per individual success)\n                # we will aggregate per-generation successes later for a smoother update\n\n            # end of generation: update means based on gen_success lists\n            if gen_success_F:\n                mean_succ_F = float(np.mean(gen_success_F))\n                F_mean = (1.0 - adapt_lr) * F_mean + adapt_lr * mean_succ_F\n                # keep in bounds\n                F_mean = float(np.clip(F_mean, min_F, max_F))\n            if gen_success_CR:\n                mean_succ_CR = float(np.mean(gen_success_CR))\n                CR_mean = (1.0 - adapt_lr) * CR_mean + adapt_lr * mean_succ_CR\n                CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n            # trust-region local search around best: sample a few local candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample a small handful scaled by dim and remaining budget\n            local_samples = min(6, remaining, max(1, int(2 + self.dim // 6)))\n            local_success = 0\n            # anisotropic sigma: base trust_radius scaled by per-dimension random factor\n            for k in range(local_samples):\n                # scale relative to range per-dim\n                sigma = trust_radius * (0.6 + 0.8 * rng.random(self.dim))\n                candidate = self.x_opt + rng.normal(0.0, sigma, size=self.dim) * total_range\n                candidate = np.clip(candidate, lb, ub)\n                if evals >= self.budget:\n                    break\n                f_candidate = func(candidate)\n                evals += 1\n                if f_candidate < self.f_opt:\n                    # shrink trust radius to focus\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    trust_radius = max(min_trust, trust_radius * 0.55)\n                    local_success += 1\n                    stagnation_counter = 0\n                    last_improvement_evals = evals\n                else:\n                    # unsuccessful local try => may expand slightly to escape\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # adapt p_levy depending on success rate\n            if generation_successes + local_success > max(1, pop_size // 6):\n                # many successes: bias toward exploitation\n                p_levy = max(0.01, p_levy * 0.92)\n            else:\n                # stagnating: increase chance of long jumps a bit\n                p_levy = min(0.6, p_levy * 1.06 + 0.005)\n\n            # track stagnation: if no improvement for long, perturb population\n            if evals - last_improvement_evals > stagnation_threshold:\n                stagnation_counter += 1\n                # partial restart: re-seed half of the population randomly\n                num_reinit = max(1, pop_size // 2)\n                reinit_idx = rng.choice(pop_size, size=num_reinit, replace=False)\n                for idx in reinit_idx:\n                    X[idx] = rng_uniform(lb, ub)\n                    # postpone re-evaluation until they are processed in next generation; but we can evaluate a couple now if budget left\n                    if evals < self.budget:\n                        fnew = func(X[idx])\n                        fvals[idx] = fnew\n                        evals += 1\n                        if fnew < self.f_opt:\n                            self.f_opt = fnew\n                            self.x_opt = X[idx].copy()\n                            last_improvement_evals = evals\n                # increase trust radius to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # relax p_levy a bit to explore more\n                p_levy = min(0.9, p_levy + 0.12)\n                # reset counters\n                last_improvement_evals = evals\n                stagnation_counter = 0\n\n            # small diversification: if p_levy became too small, nudge it\n            p_levy = float(np.clip(p_levy, 0.005, 0.9))\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # safety guard: if we somehow used more evals than budget, break\n            if evals >= self.budget:\n                break\n\n        # done\n        # ensure x_opt is a copy\n        if self.x_opt is None:\n            # fallback: pick best in current population\n            idx = int(np.argmin(fvals))\n            self.f_opt = float(fvals[idx])\n            self.x_opt = X[idx].copy()\n        else:\n            self.x_opt = np.array(self.x_opt, dtype=float)\n\n        return float(self.f_opt), self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.256 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08573877554677689, 0.14798715857987743, 0.28720812455126365, 0.1703270070911127, 0.6138927207274825, 0.2769839350208245, 0.19344528331740207, 0.49323169315180104, 0.16122168472982357, 0.13353446327733387]}, "task_prompt": ""}
{"id": "4836f0f8-3000-4549-89aa-083370a883a9", "fitness": 0.6717112195794384, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps, and a trust-region local search with online step-size adaptation for robust global-local search.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line: Adaptive DE with intermittent Lévy jumps and trust-region\n    local search that tunes step-sizes from observed successes.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: scale with dim but limited by budget\n        if pop_size is None:\n            guessed = max(6, int(8 * self.dim))\n            pop_size = min(guessed, max(4, self.budget // 10))\n        self.pop_size = max(4, int(pop_size))\n        self.rng = np.random.RandomState(seed)\n\n        # internal state\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n        # adaptation state\n        self.F_mean = 0.6\n        self.CR_mean = 0.3\n        self.p_levy = 0.05  # base probability for a long Lévy jump\n        self.trust_radius = 0.25  # relative to (ub-lb), converted to absolute later\n        self.no_improve = 0\n        self.success_history = []\n\n    def _ensure_array_bounds(self, b, dim):\n        b_arr = np.asarray(b)\n        if b_arr.size == 1:\n            return np.full(dim, float(b_arr))\n        if b_arr.shape[0] != dim:\n            raise ValueError(\"Bounds size mismatch with dimension\")\n        return b_arr.astype(float)\n\n    def _levy_step(self, scale):\n        # simple heavy-tailed step using Cauchy distribution (alpha=1 stable)\n        # scale controls typical jump size; clip extremes to avoid numerical issues\n        step = self.rng.standard_cauchy(size=self.dim)\n        # clip extreme tail values, but keep heavy-tailedness\n        step = np.clip(step, -20.0, 20.0)\n        return step * scale\n\n    def __call__(self, func):\n        # prepare bounds\n        lb = self._ensure_array_bounds(func.bounds.lb, self.dim)\n        ub = self._ensure_array_bounds(func.bounds.ub, self.dim)\n        range_vec = ub - lb\n        # absolute trust radius in variable units\n        abs_trust = self.trust_radius * np.maximum(range_vec, 1e-9)\n\n        # initialize population uniformly\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * (range_vec)\n        fpop = np.full(self.pop_size, np.inf)\n        evaluated = np.zeros(self.pop_size, dtype=bool)\n\n        # evaluate as many as budget allows\n        initial_to_eval = min(self.pop_size, self.budget - self.evals)\n        for i in range(initial_to_eval):\n            x = pop[i]\n            f = func(x)\n            self.evals += 1\n            fpop[i] = f\n            evaluated[i] = True\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n        # if we couldn't evaluate full population due to tiny budget, remaining individuals stay unevaluated\n        if self.evals >= self.budget:\n            return self.f_opt, self.x_opt\n\n        # if there were unevaluated individuals, set them to random but will be processed later only if evals remain\n        for i in range(initial_to_eval, self.pop_size):\n            pop[i] = lb + self.rng.rand(self.dim) * range_vec\n\n        # ensure there's at least one finite objective\n        if not np.isfinite(self.f_opt):\n            # try to evaluate at least one random point\n            if self.evals < self.budget:\n                x = lb + self.rng.rand(self.dim) * range_vec\n                f = func(x)\n                self.evals += 1\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n            else:\n                return self.f_opt, self.x_opt\n\n        # DE parameters\n        adapt_lr = 0.1\n        min_F, max_F = 0.05, 0.95\n        min_CR, max_CR = 0.0, 1.0\n\n        gen = 0\n        stagnation_limit = max(50, 10 * self.dim)\n        # track recent improvements to adjust p_levy, trust_radius\n        recent_improvements = 0\n        recent_attempts = 0\n\n        # main loop — process population repeatedly until budget exhausted\n        while self.evals < self.budget:\n            gen += 1\n            # jitter global means a bit for diversity\n            self.F_mean = np.clip(self.F_mean + self.rng.normal(0, 0.01), 0.1, 0.9)\n            self.CR_mean = np.clip(self.CR_mean + self.rng.normal(0, 0.02), 0.0, 1.0)\n\n            # per-individual DE operations\n            inds = np.arange(self.pop_size)\n            self.rng.shuffle(inds)\n            success_F = []\n            success_CR = []\n            gen_successes = 0\n\n            for idx in inds:\n                if self.evals >= self.budget:\n                    break\n\n                # ensure there are at least 3 other indices for mutation\n                others = [i for i in range(self.pop_size) if i != idx]\n                if len(others) < 3:\n                    # trivial local sample instead\n                    trial = np.clip(pop[idx] + self.rng.normal(0, 0.1, size=self.dim) * range_vec, lb, ub)\n                    f_trial = func(trial)\n                    self.evals += 1\n                    if f_trial < fpop[idx]:\n                        pop[idx] = trial\n                        fpop[idx] = f_trial\n                        gen_successes += 1\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial.copy()\n                    continue\n\n                # jDE-like adaptation: sample Fi and CRi\n                Fi = np.clip(self.rng.normal(self.F_mean, 0.1), min_F, max_F)\n                # CR sample: with small prob random else around mean\n                if self.rng.rand() < 0.1:\n                    CRi = self.rng.rand()\n                else:\n                    CRi = np.clip(self.rng.normal(self.CR_mean, 0.1), min_CR, max_CR)\n\n                # mutation selection\n                r1, r2, r3 = self.rng.choice(others, 3, replace=False)\n                base = pop[r1]\n                diff = pop[r2] - pop[r3]\n                # occasionally perform a Lévy centered jump from best, to explore far regions\n                do_levy = self.rng.rand() < self.p_levy\n                if do_levy and np.isfinite(self.f_opt):\n                    # levy centered on best plus DE-like differential\n                    levy_scale = 0.5 * np.mean(range_vec)  # typical big jump scale\n                    levy = self._levy_step(levy_scale)\n                    donor = self.x_opt + 0.5 * diff + levy * (self.rng.rand() * 1.0)\n                else:\n                    donor = base + Fi * diff\n                    # small directed nudge from best to encourage exploitation\n                    if np.isfinite(self.f_opt) and self.rng.rand() < 0.15:\n                        donor = 0.7 * donor + 0.3 * (self.x_opt + Fi * (pop[r2] - pop[r1]))\n\n                # binomial crossover\n                cross = self.rng.rand(self.dim) < CRi\n                if not np.any(cross):\n                    cross[self.rng.randint(0, self.dim)] = True\n                trial = np.where(cross, donor, pop[idx])\n                # trust-region restraint: occasionally shrink trial into trust region around best\n                if np.isfinite(self.f_opt) and self.rng.rand() < 0.2:\n                    # pull towards best a bit limited by abs_trust\n                    pull = self.rng.rand(self.dim)\n                    trial = np.where(self.rng.rand(self.dim) < 0.5,\n                                     self.x_opt + (trial - self.x_opt) * self.rng.uniform(0.0, 1.0, size=self.dim),\n                                     trial)\n\n                # projection to bounds\n                trial = np.clip(trial, lb, ub)\n\n                # one evaluation\n                f_trial = func(trial)\n                self.evals += 1\n                recent_attempts += 1\n\n                # selection: greedy\n                # if current individual was never evaluated before, accept if budget allowed\n                replaced = False\n                if (not evaluated[idx]) or (f_trial < fpop[idx]):\n                    pop[idx] = trial\n                    fpop[idx] = f_trial\n                    evaluated[idx] = True\n                    replaced = True\n                    gen_successes += 1\n                    recent_improvements += 1\n                    self.success_history.append((Fi, CRi))\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n                    # move global best if improved\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        self.no_improve = 0\n                    else:\n                        self.no_improve += 1\n                else:\n                    self.no_improve += 1\n\n                # lightly adapt p_levy based on whether Levy occurred and success/failure\n                if do_levy:\n                    if replaced:\n                        self.p_levy = max(0.01, self.p_levy * 0.9)\n                    else:\n                        # if many unsuccessful attempts, increase chance to jump\n                        self.p_levy = min(0.5, self.p_levy * 1.02)\n\n                # break early if budget exhausted\n                if self.evals >= self.budget:\n                    break\n\n            # End of population processing: adjust means towards successful Fi/CRi\n            if success_F:\n                self.F_mean = (1.0 - adapt_lr) * self.F_mean + adapt_lr * np.mean(success_F)\n            if success_CR:\n                self.CR_mean = (1.0 - adapt_lr) * self.CR_mean + adapt_lr * np.mean(success_CR)\n\n            # small population-level adaptation to encourage exploration when stagnating\n            if gen_successes == 0:\n                # expand trust region slightly to escape local trap\n                abs_trust = abs_trust * 1.08\n                # increase levy probability\n                self.p_levy = min(0.6, self.p_levy * 1.08)\n            else:\n                # successful generation: focus inwards a bit\n                abs_trust = abs_trust * (0.98 ** gen_successes)\n                self.p_levy = max(0.005, self.p_levy * (0.995 ** gen_successes))\n\n            # trust-region local search around best: sample a few candidates if budget left\n            if self.evals < self.budget and np.isfinite(self.f_opt):\n                # number of local samples depends on dim and remaining budget\n                remain = self.budget - self.evals\n                local_count = min(max(1, self.dim // 2), remain, 5)\n                local_success = 0\n                for _ in range(local_count):\n                    # anisotropic gaussian: per-dim scaling around abs_trust\n                    scale = abs_trust * (0.3 + 0.7 * self.rng.rand(self.dim))\n                    local_trial = self.x_opt + self.rng.randn(self.dim) * scale\n                    local_trial = np.clip(local_trial, lb, ub)\n                    f_local = func(local_trial)\n                    self.evals += 1\n                    recent_attempts += 1\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = local_trial.copy()\n                        local_success += 1\n                        recent_improvements += 1\n                        # shrink trust region to focus\n                        abs_trust = abs_trust * 0.85\n                        # adapt means to favor exploitation\n                        self.F_mean = max(min(self.F_mean, 0.95), 0.05)\n                        self.CR_mean = max(min(self.CR_mean, 0.98), 0.02)\n                    else:\n                        # small enlargement to allow escapes\n                        abs_trust = abs_trust * 1.03\n                        # encourage slightly more jumps if local search is failing repeatedly\n                        self.p_levy = min(0.9, self.p_levy * 1.005)\n                    if self.evals >= self.budget:\n                        break\n\n                # adjust trust by success rate\n                if local_success == 0:\n                    # no success => increase trust to explore broader area\n                    abs_trust = abs_trust * 1.04\n                else:\n                    # success => focus tighter\n                    abs_trust = abs_trust * (0.9 ** local_success)\n\n            # stagnation management and partial reseed\n            if recent_attempts >= stagnation_limit:\n                if recent_improvements == 0:\n                    # strong stagnation: reseed half of population\n                    half = max(1, self.pop_size // 2)\n                    indices = self.rng.choice(self.pop_size, half, replace=False)\n                    for idx in indices:\n                        if self.evals >= self.budget:\n                            break\n                        pop[idx] = lb + self.rng.rand(self.dim) * range_vec\n                        fval = func(pop[idx])\n                        self.evals += 1\n                        fpop[idx] = fval\n                        evaluated[idx] = True\n                        if fval < self.f_opt:\n                            self.f_opt = fval\n                            self.x_opt = pop[idx].copy()\n                    # slightly enlarge trust radius to escape\n                    abs_trust = abs_trust * 1.5\n                    # encourage more Lévy jumps after reseed\n                    self.p_levy = min(0.8, self.p_levy * 1.5)\n                # reset counters\n                recent_attempts = 0\n                recent_improvements = 0\n\n            # a safety clip to ensure trust remains within reasonable bounds\n            abs_trust = np.clip(abs_trust, 1e-12 * range_vec, 2.0 * range_vec)\n\n            # final break if budget used up\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.672 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15076781432711328, 0.365927657339698, 0.45072701231643597, 0.899621180788888, 0.855358629602604, 0.872759436824325, 0.7306762790959263, 0.8313515486834918, 0.8396210510016152, 0.7203015858142863]}, "task_prompt": ""}
{"id": "4263c2ae-0d70-47a9-afd8-a502984cb9c6", "fitness": 0.22328809003180544, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a shrinking/expanding trust-region local search with online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but constrained\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # ensure population not larger than some fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.copy().astype(float)\n        # try broadcasting\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to dimension %d\" % self.dim)\n\n    def __call__(self, func):\n        # read bounds from func; fallback to [-5,5] if not available\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim,  5.0, dtype=float)\n\n        # ensure sensible bounds\n        range_vec = ub - lb\n        range_vec = np.where(range_vec > 0, range_vec, 1.0)\n\n        rng = self.rng\n\n        # Initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population respecting budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # if we evaluated nobody (budget==0) return\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # initialize best\n        valid_idx = np.arange(self.pop_size)[~np.isinf(fvals)]\n        if valid_idx.size == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # base probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        min_trust = 1e-6\n\n        # trust region initialization (relative to search range)\n        trust_radius = step_scale  # in relative units of range (0..1)\n        max_trust = 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using standard Cauchy (fast)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            # clip extreme outliers to avoid numerical blow-up but keep heavy-tail\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            gen_start_best = best_f\n\n            # per-individual DE operations\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                use_levy = (rng.rand() < p_levy)\n                if use_levy:\n                    # Centered on best with heavy-tail jump\n                    step = levy_step()\n                    # dynamic scaling: combine trust region and global range\n                    scale = (0.5 * trust_radius + 0.5 * rng.rand()) * (range_vec.mean() if range_vec.mean() > 0 else 1.0)\n                    donor = best_x + step * scale\n                    # ensure donor is finite\n                    donor = np.where(np.isfinite(donor), donor, lb + rng.rand(self.dim) * range_vec)\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1 mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback random neighbor\n                        donor = pop[i] + rng.normal(0, 1.0, self.dim) * (range_vec * 0.05)\n                        Fi = None\n                        CRi = None\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover to produce trial\n                        cr_mask = rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[rng.randint(0, self.dim)] = True\n                        trial = pop[i].copy()\n                        trial[cr_mask] = donor[cr_mask]\n                        donor = trial\n\n                # project to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt F_mean and CR_mean toward successful Fi/CRi if provided\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful local improvement: shrink trust region slightly\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    stagnation_counter += 1\n\n            # End of DE generation actions\n            # trust-region local search around best: sample some Gaussian candidates\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            local_samples = min(5, remaining, max(1, int(2 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by per-dim random factor\n                sigma = (0.5 + rng.rand(self.dim) * 0.5) * trust_radius\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # adaptation after generation\n            if successes > 0:\n                # reduce probability of Levy jumps modestly when making progress\n                p_levy = max(0.01, p_levy * (0.95 if successes > (0.2 * self.pop_size) else 0.98))\n                # gently bias toward exploitation-friendly params\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation handling: increase chance of long jumps and diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.pop_size * 3):\n                k = max(1, self.pop_size // 2)\n                reinit_indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reinit_indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius after a reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n            # small random injection of elite-guided individuals if budget allows and stagnation mild\n            if (gen % 10 == 0) and (self.budget - evals > 0) and (p_levy > 0.02):\n                inject = min(2, self.pop_size, self.budget - evals)\n                for _ in range(inject):\n                    j = rng.randint(0, self.pop_size)\n                    new = best_x + rng.normal(0, 1.0, size=self.dim) * (0.1 * trust_radius) * range_vec\n                    new = np.clip(new, lb, ub)\n                    f_new = float(func(new))\n                    evals += 1\n                    if f_new < fvals[j]:\n                        pop[j] = new\n                        fvals[j] = f_new\n                    if f_new < best_f:\n                        best_f = f_new\n                        best_x = new.copy()\n                        stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.223 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11246406724678348, 0.1707852359137242, 0.28619994847125796, 0.35566204010718294, 0.21234400685814758, 0.2469111881390016, 0.2370441533588118, 0.2466679407522362, 0.21304573878362387, 0.15175658068728493]}, "task_prompt": ""}
{"id": "4eb22d59-6c66-4d59-b5b4-ea39801ce688", "fitness": 0.5709825291806735, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    Usage:\n        opt = HybridDeLevyTrust(budget=10000, dim=10, pop_size=None, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size scaled with dimension\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        # if scalar\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # prepare bounds (functions in the evaluation suite provide .bounds.lb/.ub)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # safety: if bounds look like single constant -5/5 fallback\n        if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        range_mean = max(range_vec.mean(), 1e-12)\n\n        rng = self.rng  # local alias\n\n        # initialize counters and defaults\n        evals = 0\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        # evaluate initial population sequentially until budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = np.clip(pop[i], lb, ub)\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n            pop[i] = x\n\n        # find initial best among evaluated individuals\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # no evaluation possible (budget zero)\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals[valid])\n        best_indices = np.where(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # DE control parameters (means for jDE-like adaptation)\n        F_mean = 0.6\n        CR_mean = 0.5\n\n        # exploration/trust-region parameters\n        p_levy = 0.08\n        step_scale = 0.25\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # global scalar radius\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 4.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # Levy-like heavy-tailed step via Cauchy sampler (standard Cauchy)\n        def levy_step(scale=1.0):\n            # standard Cauchy\n            s = rng.standard_cauchy(self.dim)\n            # temper the extreme outliers, keep heavy tails but numeric-stable\n            s = np.clip(s, -1e6, 1e6)\n            return s * float(scale)\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # small per-generation perturbation to encourage diversity\n            F_mean = np.clip(F_mean * (1.0 + rng.normal(0.0, 0.01)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean * (1.0 + rng.normal(0.0, 0.02)), 0.0, 1.0)\n\n            # prepare index pool for sampling (for DE mutation)\n            idxs = np.arange(self.pop_size)\n\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # ensure we have a current individual\n                target = pop[i].copy()\n\n                # sample Fi and CRi (jDE style)\n                Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # create donor vector (either Levy-based jump around best or DE/rand/1)\n                if rng.rand() < p_levy:\n                    # Levy jump around the best: heavy-tailed, scaled to trust radius + global range\n                    scale = step_scale * (trust_radius + 0.5 * range_mean)\n                    donor = best_x + levy_step(scale=scale) * (range_vec / (range_mean + 1e-12))\n                    # biased toward best plus some directional perturbation\n                    donor = target + 0.2 * (donor - target) + Fi * (rng.randn(self.dim) * (trust_radius / (range_mean + 1e-12)))\n                else:\n                    # DE/rand/1 mutation\n                    # choose three distinct indices not equal to i\n                    others = np.delete(idxs, i)\n                    if others.size < 3:\n                        # fallback: random normal perturbation if population too small\n                        donor = target + Fi * rng.randn(self.dim) * range_mean * 0.1\n                    else:\n                        r1, r2, r3 = rng.choice(others, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    # ensure at least one index from donor\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(self.dim)] = True\n                    trial = np.where(cr_mask, donor, target)\n                    donor = trial\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate if we have budget\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                # compare with current population member value (if current was not evaluated set inf)\n                if not np.isfinite(fvals[i]) or f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt means towards successful Fi/CRi (small learning rate)\n                    F_mean = 0.98 * F_mean + 0.02 * Fi\n                    CR_mean = 0.98 * CR_mean + 0.02 * CRi\n\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # if we just ran out of budget break\n                if evals >= self.budget:\n                    break\n\n            # trust-region local search around best: a few small Gaussian perturbations\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # number of local samples: small handful depending on dim and remaining budget\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            # determine anisotropic sigma relative to trust_radius and problem scale\n            sigma = (trust_radius / (range_mean + 1e-12)) * 0.5\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian perturbation scaled by range_vec and sigma\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * (sigma * range_vec)\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter += 1\n\n            # adaptive logic after generation\n            if successes > max(1, self.pop_size // 10):\n                # many successes -> exploitation working: reduce levy chance slightly\n                p_levy = max(0.01, p_levy * 0.95)\n                # modestly tighten trust region\n                trust_radius = max(min_trust, trust_radius * 0.98)\n            elif successes == 0:\n                # no successes: increase exploration\n                p_levy = min(0.6, p_levy * 1.10 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                # if strong stagnation, re-seed part of population\n                if stagnation_counter > max(10, 3 * self.dim):\n                    k = max(1, self.pop_size // 2)\n                    reidx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                    for j in reidx:\n                        if evals >= self.budget:\n                            break\n                        pop[j] = lb + rng.rand(self.dim) * range_vec\n                        # optionally evaluate some reinitialized individuals if budget permits\n                        if evals < self.budget:\n                            x = np.clip(pop[j], lb, ub)\n                            fvals[j] = float(func(x))\n                            evals += 1\n                            if fvals[j] < best_f:\n                                best_f = fvals[j]\n                                best_x = pop[j].copy()\n                    # enlarge trust radius to encourage escapes\n                    trust_radius = min(max_trust, trust_radius * 2.0)\n                    stagnation_counter = 0\n            else:\n                # mild success: gently push means toward exploitation\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n\n            # safety: keep trust_radius bounded\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n        # store results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.571 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14779577527766274, 0.21759063255258138, 0.47258815842023194, 0.9659270792171277, 0.8258513908190074, 0.9487691708769489, 0.5330180208260056, 0.5217720563092045, 0.5051266433666968, 0.5713863641412679]}, "task_prompt": ""}
{"id": "4320fc9d-4aaf-4a1b-a49e-2d20b428d7ae", "fitness": 0.43124263778138394, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like parameter adaptation, occasional Lévy-flight long jumps, and a trust-region local search that adapts radius online for robust global-to-local optimization.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE) with\n    jDE-like parameter self-adaptation, occasional Lévy-flight jumps for\n    long-range exploration, and a trust-region local search around the\n    current best. Parameters (F, CR, trust radius, Levy probability) are\n    adapted on the fly based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # reasonable default population size scaled with dimension\n        if pop_size is None:\n            # basic rule: between 4*dim and 50, but not exceeding budget/2\n            self.pop_size = int(min(max(4 * self.dim, 20), 50, max(4, self.budget // 10)))\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (careful with budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # if we didn't manage to evaluate full population due to tiny budget,\n        # but if no evaluations were possible, return immediately\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # pick current best among evaluated individuals\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # shouldn't happen given evals>0, but guard\n            best_idx = 0\n            best_x = pop[0].copy()\n            best_f = np.inf\n        else:\n            best_idx = np.argmin(np.where(valid, fvals, np.inf))\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        min_trust = 1e-6\n        trust_radius = step_scale\n        max_trust = 2.0\n        p_levy = 0.05       # base chance for a Levy jump\n        stagnation_counter = 0\n\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            # clip extremely large outliers to keep numerics stable\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation small random jitter of means to maintain diversity\n            F_mean = np.clip(F_mean * (1.0 + (rng.rand() - 0.5) * 0.02), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + (rng.rand() - 0.5) * 0.02, 0.0, 1.0)\n\n            # process each target vector\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # local copies to avoid leakage\n                Fi = None\n                CRi = None\n\n                # decide between Levy jump centered on current best or DE mutation\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration (long-range)\n                    step = levy_step()\n                    # scale by trust radius and overall range; use anisotropic scaling\n                    scale = trust_radius * (0.5 + rng.rand(self.dim) * 1.5)\n                    donor = best_x + (step * scale) * range_vec\n                    # ensure donor is finite / bounded\n                    donor = np.clip(donor, lb, ub)\n                else:\n                    # jDE-like self-adaptation of F and CR\n                    # with small probability mutate means per individual\n                    if rng.rand() < 0.1:\n                        Fi = np.clip(F_mean + 0.1 * (rng.randn()), 0.05, 0.99)\n                    else:\n                        Fi = np.clip(rng.randn() * 0.1 + F_mean, 0.05, 0.99)\n                    CRi = np.clip(rng.randn() * 0.1 + CR_mean, 0.0, 1.0)\n\n                    # classical DE/rand/1 mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback: random perturbation if not enough individuals\n                        donor = pop[i] + Fi * (rng.rand(self.dim) - 0.5) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        mutant = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[rng.randint(0, self.dim)] = True\n                        trial = np.where(cr_mask, mutant, pop[i])\n                        donor = trial\n\n                    # ensure donor within bounds\n                    donor = np.clip(donor, lb, ub)\n\n                # evaluate candidate\n                candidate = donor\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection (greedy)\n                replaced = False\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    replaced = True\n                    successes += 1\n                    # move means slightly toward successful Fi/CRi\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # shrink trust radius a bit on success\n                    trust_radius = max(min_trust, trust_radius * 0.95)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # if budget over, break\n                if evals >= self.budget:\n                    break\n\n            # After generation: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # adapt global heuristics based on successes/stagnation\n            if successes > 0:\n                # encourage exploitation: slightly reduce levy probability\n                p_levy = max(0.01, p_levy * 0.95)\n                # move F_mean/CR_mean slightly to exploitation-friendly values\n                F_mean = np.clip(0.98 * F_mean + 0.02 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation: increase chance of long jumps and widen F for exploration\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n                # gently expand trust radius to try wider neighborhood\n                trust_radius = min(max_trust, trust_radius * 1.02)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                for j in rng.choice(np.arange(self.pop_size), k, replace=False):\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # if this new individual is the best so far, update\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius after reset to explore wider areas\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # also increase Levy chance temporarily\n                p_levy = min(0.5, p_levy + 0.05)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.431 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2067959322900108, 0.19740920164154008, 0.3736884532262178, 0.5514298881576254, 0.3552792154093539, 0.9249029798430167, 0.346573205953149, 0.40281625174747426, 0.7653107375885528, 0.1882205119568987]}, "task_prompt": ""}
{"id": "d4aca3d4-383e-4e2c-b435-6c55d55f33ef", "fitness": 0.2459501640939446, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining differential evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size adaptation and simple self-adaptation of DE parameters.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history. Designed for continuous\n    box-constrained problems (e.g., BBOB).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        # sensible default population size: grows with dim but limited by budget\n        if pop_size is None:\n            guessed = max(4, min(50, 8 * self.dim))\n            # don't use more than a fraction of budget for population\n            guessed = min(guessed, max(4, self.budget // 5))\n            self.pop_size = int(guessed)\n        else:\n            self.pop_size = int(pop_size)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full((self.dim,), float(b))\n        if b.size != self.dim:\n            # try to broadcast or raise\n            try:\n                return np.broadcast_to(b, (self.dim,)).astype(float)\n            except Exception:\n                raise ValueError(\"Bounds length mismatch with dimension\")\n        return b\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # fallback to [-5,5] if bounds are degenerate\n        if np.any(np.isinf(lb)) or np.any(np.isinf(ub)):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        range_mean = max(np.mean(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (careful with budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            fvals[i] = float(func(x))\n            evals += 1\n\n        if evals == 0:\n            # no evaluations possible within budget\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # determine best among evaluated\n        valid = np.isfinite(fvals)\n        best_idx = np.argmin(fvals[valid])\n        best_indices = np.where(valid)[0]\n        best_idx = best_indices[best_idx]\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        trust_radius = 0.2 * range_mean  # scalar trust radius (in absolute units)\n        min_trust = 1e-6\n        max_trust = 2.0 * range_mean\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(size=self.dim)\n            # cap extremes to avoid numerical blow up\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            successful_Fs = []\n            successful_CRs = []\n            improved_in_gen = False\n\n            # per-individual operations\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # choose whether to perform Lévy exploration centered on best\n                if rng.random() < p_levy:\n                    # Lévy jump: heavy-tailed step around best\n                    step = levy_step()\n                    # scale by trust radius and global range\n                    step_scale = (trust_radius / (range_mean + 1e-12)) * (0.5 + rng.random())\n                    donor = best_x + step_scale * step * range_vec\n                    # ensure donor not NaN\n                    donor = np.nan_to_num(donor, nan=0.0, posinf=1e3, neginf=-1e3)\n                    # no CR/F adaptation for levy branch, treat as exploratory\n                    CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                    Fi = None\n                else:\n                    # DE/rand/1-like mutation with jDE-style per-individual F and CR\n                    # sample Fi and CRi around the means\n                    Fi = float(rng.normal(F_mean, 0.12))\n                    Fi = float(np.clip(Fi, 0.05, 0.95))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n\n                    # pick three distinct indices not equal to i\n                    idxs = np.delete(np.arange(self.pop_size), i)\n                    if idxs.size < 3:\n                        # fallback: use random points around i\n                        r1 = rng.integers(0, self.pop_size)\n                        r2 = (r1 + 1) % self.pop_size\n                        r3 = (r2 + 1) % self.pop_size\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, size=3, replace=False)\n\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # ensure some diversity: enforce at least one component from donor\n                    cr_mask = rng.random(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.integers(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    donor = trial\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate <= fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    improved_in_gen = True\n                    # record successful Fi/CR if applicable\n                    if Fi is not None:\n                        successful_Fs.append(Fi)\n                    if 'CRi' in locals() and CRi is not None:\n                        successful_CRs.append(CRi)\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # small per-success nudges to means (lightweight)\n                if successes > 0 and len(successful_Fs) > 0:\n                    F_mean = np.clip(0.95 * F_mean + 0.05 * np.mean(successful_Fs), 0.05, 0.99)\n                if successes > 0 and len(successful_CRs) > 0:\n                    CR_mean = np.clip(0.95 * CR_mean + 0.05 * np.mean(successful_CRs), 0.0, 1.0)\n\n            # End of generation adjustments\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining_budget = self.budget - evals\n            # sample count: at most few points, scaled by dim but limited by remaining budget\n            local_samples = min(max(1, self.dim // 4), remaining_budget)\n            # anisotropic sigma: base trust_radius scaled by uniform factor\n            sigma = trust_radius / (range_mean + 1e-12)\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # per-dimension scaling to allow anisotropic moves\n                anisotropy = 0.5 + rng.random(self.dim)\n                candidate = best_x + rng.normal(0.0, sigma, size=self.dim) * anisotropy * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    improved_in_gen = True\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter += 1\n\n            # adapt probability of Lévy jumps based on recent improvements\n            if improved_in_gen:\n                # reduce exploration a bit\n                p_levy = max(0.01, p_levy * 0.95)\n                # encourage slightly smaller trust region to refine\n                trust_radius = max(min(trust_radius * 0.97, max_trust), min_trust)\n            else:\n                # increase exploration\n                p_levy = min(0.5, p_levy * 1.05)\n                trust_radius = max(min(trust_radius * 1.03, max_trust), min_trust)\n\n            # gentle drift of means toward defaults to avoid stagnation at extreme values\n            F_mean = np.clip(0.995 * F_mean + 0.005 * 0.6, 0.05, 0.99)\n            CR_mean = np.clip(0.995 * CR_mean + 0.005 * 0.9, 0.0, 1.0)\n\n            # stagnation handling: re-seed part of population if no improvement for many actions\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                # reinitialize k random individuals (but keep best_x if possible)\n                candidates = rng.choice(np.arange(self.pop_size), size=k, replace=False)\n                for j in candidates:\n                    # avoid replacing the best member if possible\n                    if np.allclose(pop[j], best_x):\n                        continue\n                    pop[j] = lb + rng.random(self.dim) * range_vec\n                    if evals >= self.budget:\n                        break\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.246 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10835190933951755, 0.20475441189437782, 0.2968231546461204, 0.36789851537449936, 0.26672824355949476, 0.31549931804644893, 0.25141194067524597, 0.23394158661036202, 0.23486724758384325, 0.17922531320953594]}, "task_prompt": ""}
{"id": "78f3c2d8-7588-415d-b532-4b1c4ed21e73", "fitness": 0.1955404258730837, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and trust-region local search with online adaptation of DE parameters for robust global-to-local search.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n\n    One-line: Adaptive hybrid DE + Lévy jumps + trust-region local search.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # population size scaled with dimension and budget but kept reasonable\n        if pop_size is None:\n            # base on dimension, but never larger than half the budget (so we can evaluate at least some)\n            base = max(8, 4 * self.dim)\n            if self.budget > 0:\n                self.pop_size = int(min(base, max(4, self.budget // 2)))\n            else:\n                self.pop_size = int(base)\n        else:\n            self.pop_size = max(4, int(pop_size))\n\n        # internal state for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            # try to broadcast\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        if b.size != self.dim:\n            # try to broadcast or truncate/extend if needed\n            if b.size < self.dim:\n                return np.concatenate([b, np.full(self.dim - b.size, b[-1])]).astype(float)\n            else:\n                return b.ravel()[: self.dim].astype(float)\n        return b\n\n    def __call__(self, func):\n        # bounds from func: expected to have lb/ub attributes\n        lb = self._ensure_array_bounds(getattr(func.bounds, \"lb\", -5.0))\n        ub = self._ensure_array_bounds(getattr(func.bounds, \"ub\", 5.0))\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population (careful with budget)\n        n_init = min(self.pop_size, max(0, self.budget))\n        for i in range(n_init):\n            if evals >= self.budget:\n                break\n            try:\n                f = float(func(pop[i]))\n            except Exception:\n                f = np.inf\n            fvals[i] = f\n            evals += 1\n\n        # If we couldn't evaluate any individual (budget==0), return default\n        if evals == 0:\n            self.f_opt = float(np.min(fvals) if fvals.size and np.isfinite(fvals).any() else np.inf)\n            if np.isfinite(self.f_opt):\n                self.x_opt = pop[int(np.nanargmin(fvals))].copy()\n            else:\n                self.x_opt = lb + 0.5 * range_vec\n            return self.f_opt, self.x_opt\n\n        # find current best among evaluated\n        finite_idx = np.where(np.isfinite(fvals))[0]\n        if finite_idx.size > 0:\n            best_idx = int(finite_idx[np.argmin(fvals[finite_idx])])\n            best_f = float(fvals[best_idx])\n            best_x = pop[best_idx].copy()\n        else:\n            best_idx = 0\n            best_f = float(fvals[0])\n            best_x = pop[0].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        trust_radius = 0.2 * range_norm  # scalar trust radius (in absolute units)\n        max_trust = range_norm * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # Cauchy-distributed steps (alpha-stable proxy) with truncation\n            step = rng.standard_cauchy(self.dim)\n            # limit extreme outliers to avoid numerical blow-up\n            step = np.clip(step, -10.0, 10.0)\n            # normalize to unit-ish scale but retain heavy tails\n            s = np.linalg.norm(step)\n            if s > 0:\n                step = step / (s + 1e-12)\n            return step\n\n        # main loop: iterate generations until budget exhausted\n        # We'll process population members sequentially; each evaluated candidate consumes one eval.\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation perturbation of means not needed, we'll sample per-individual around means\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Sample Fi and CRi (jDE-inspired light adaptation)\n                Fi = np.clip(rng.normal(F_mean, 0.15), 0.05, 1.2)\n                CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # decide mutation type: Levy jump centered on best or DE/rand/1\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    # step scale: influenced by trust_radius and by random factor\n                    step_scale = (trust_radius / max(range_norm, 1e-12)) * (0.5 + rng.rand())\n                    donor = best_x + step * step_scale * range_vec\n                else:\n                    # DE/rand/1-like mutation\n                    # pick three distinct indices different from i\n                    idxs = list(range(self.pop_size))\n                    if len(idxs) <= 1:\n                        donor = pop[i].copy()\n                    else:\n                        idxs.remove(i)\n                        r = rng.choice(idxs, size=min(3, max(1, len(idxs))), replace=False)\n                        # ensure at least two differ for vector difference\n                        if r.size == 1:\n                            donor = pop[r[0]].copy()\n                        elif r.size == 2:\n                            donor = pop[r[0]] + Fi * (pop[r[1]] - pop[r[0]])\n                        else:\n                            donor = pop[r[0]] + Fi * (pop[r[1]] - pop[r[2]])\n\n                # binomial crossover to create trial vector from donor and target pop[i]\n                cr_mask = rng.rand(self.dim) < CRi\n                # ensure at least one dimension from donor\n                cr_mask[rng.randint(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                trial = np.clip(trial, lb, ub)\n\n                # one evaluation (if budget allows)\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(trial))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = trial.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # move means slightly toward successful parameters (simple adaptation)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # slight drift of means away from unsuccessful parameters\n                    F_mean = np.clip(0.995 * F_mean + 0.005 * Fi, 0.01, 1.5)\n                    CR_mean = np.clip(0.995 * CR_mean + 0.005 * CRi, 0.0, 1.0)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = trial.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # stop early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n\n            remaining = max(0, self.budget - evals)\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            if remaining > 0:\n                local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n                improved_local = 0\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                    sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / max(range_norm, 1e-12))\n                    candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                    try:\n                        f_candidate = float(func(candidate))\n                    except Exception:\n                        f_candidate = np.inf\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        improved_local += 1\n                        stagnation_counter = 0\n                        # successful local step => shrink trust radius to focus\n                        trust_radius = max(1e-6, trust_radius * 0.8)\n                    else:\n                        # unsuccessful => expand a bit to escape local traps\n                        trust_radius = min(max_trust, trust_radius * 1.05)\n                        stagnation_counter += 1\n\n                # small encouragement: if we had local improvements, slightly reduce p_levy\n                if improved_local > 0:\n                    p_levy = max(0.01, p_levy * (0.95 ** improved_local))\n\n            # if many successes overall, encourage exploitation less (reduce levy), else increase\n            if successes > 0:\n                if successes > max(1, self.pop_size * 0.2):\n                    p_levy = max(0.01, p_levy * 0.95)\n                else:\n                    p_levy = max(0.01, p_levy * 0.98)\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                CR_mean = np.clip(CR_mean * 0.98 + 0.01, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                replace_idx = rng.choice(self.pop_size, size=k, replace=False)\n                for j in replace_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0  # reset stagnation\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.196 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12206353959971195, 0.17862593118664793, 0.2500025591432675, 0.19642999948730255, 0.21876897402646966, 0.22661759255485303, 0.2248451652919885, 0.2012916391618027, 0.19714231051453723, 0.13961654776425614]}, "task_prompt": ""}
{"id": "5c571e70-ace1-4e33-8d35-df2b9750d11e", "fitness": 0.20791777993864957, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid: Differential Evolution with occasional Lévy jumps and trust-region local search, with online parameter adaptation for robust global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but bounded\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        a = np.atleast_1d(b).astype(float)\n        if a.size == 1:\n            return np.full(self.dim, a.item(), dtype=float)\n        if a.size != self.dim:\n            # try broadcasting\n            return np.broadcast_to(a, (self.dim,)).astype(float)\n        return a\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # safety: if bounds degenerate, give small range\n        range_vec = np.where(range_vec <= 0, 1e-6, range_vec)\n\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # initial evaluation (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fvals[i] = float(func(pop[i]))\n            except Exception:\n                # if evaluation fails, keep inf and continue (count as eval)\n                fvals[i] = np.inf\n            evals += 1\n\n        # If nothing was evaluated, return trivial result\n        finite_idx = np.isfinite(fvals)\n        if not np.any(finite_idx):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals)\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        p_levy = 0.08\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        trust_radius = max(1e-3, np.linalg.norm(range_vec) / 8.0)\n\n        # DE parameter means (jDE-like adaptation)\n        F_mean = 0.6\n        CR_mean = 0.5\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step(scale):\n            # standard Cauchy heavy-tailed; scale controls magnitude\n            s = self.rng.standard_cauchy(self.dim)\n            # limit extreme outliers\n            s = np.clip(s, -1e6, 1e6)\n            nrm = np.linalg.norm(s) + 1e-12\n            step = s / nrm * (scale * (1.0 + self.rng.rand()))\n            # dimension-wise scale by range to respect problem size\n            return step * (range_vec / (np.linalg.norm(range_vec) + 1e-12))\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            gen_successes = 0\n            gen_F_success_sum = 0.0\n            gen_CR_success_sum = 0.0\n\n            # adapt per-generation randomization of F and CR\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # choose distinct indices for mutation (r1,r2,r3 != i)\n                idxs = list(range(self.pop_size))\n                idxs.remove(i)\n                # if population small, allow some reuse but prefer distinct\n                if len(idxs) >= 3:\n                    r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                else:\n                    # fallback\n                    r1 = idxs[self.rng.randint(len(idxs))]\n                    r2 = idxs[self.rng.randint(len(idxs))]\n                    r3 = idxs[self.rng.randint(len(idxs))]\n\n                # sample Fi and CRi around means (small perturbation)\n                Fi = float(np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # decide exploration via Levy or DE mutation\n                if self.rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    donor = best_x + levy_step(trust_radius * 1.5)\n                else:\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover -> trial vector\n                trial = pop[i].copy()\n                jrand = self.rng.randint(self.dim)\n                for j in range(self.dim):\n                    if self.rng.rand() < CRi or j == jrand:\n                        trial[j] = donor[j]\n\n                # projection to bounds\n                trial = np.clip(trial, lb, ub)\n\n                # one evaluation (if budget allows)\n                try:\n                    f_candidate = float(func(trial))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = trial\n                    fvals[i] = f_candidate\n                    gen_successes += 1\n                    gen_F_success_sum += Fi\n                    gen_CR_success_sum += CRi\n                    # adjust global best\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = trial.copy()\n                        stagnation_counter = 0\n                    else:\n                        # slight progress counts\n                        stagnation_counter += 0\n                else:\n                    stagnation_counter += 1\n\n                # maintain overall best too\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = trial.copy()\n\n                # safety: stop if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of population loop (one generation)\n\n            # Update F_mean and CR_mean if there were successes in this generation\n            if gen_successes > 0:\n                F_mean = 0.85 * F_mean + 0.15 * (gen_F_success_sum / gen_successes)\n                CR_mean = 0.85 * CR_mean + 0.15 * (gen_CR_success_sum / gen_successes)\n                # when succeeding, slightly reduce trust radius to focus\n                trust_radius = max(min_trust, trust_radius * (0.95))\n            else:\n                # stagnation: widen search scope\n                trust_radius = min(max_trust, trust_radius * 1.15)\n                # increase chance of long jumps slowly\n                p_levy = min(0.4, p_levy * 1.08)\n                # and nudge CR_mean down to encourage diversity\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # trust-region local search around best: sample a few candidates with anisotropic Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # pick a small number of local samples adaptively\n            local_samples = min( max(1, self.dim // 2), remaining )\n            successes = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by per-dim random factor\n                sigma = trust_radius * (0.5 + self.rng.rand(self.dim))\n                candidate = best_x + self.rng.normal(0.0, sigma, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    successes += 1\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slight expansion to encourage escape\n                    trust_radius = min(max_trust, trust_radius * 1.03)\n\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n\n            # If many local successes, reduce p_levy to focus exploitation; else slowly increase\n            if successes > 0:\n                p_levy = max(0.02, p_levy * 0.9)\n            else:\n                p_levy = min(0.6, p_levy * 1.03)\n\n            # strong stagnation reset: if no improvement for a while, re-seed part of population\n            if stagnation_counter > max(50, 5 * self.dim):\n                # reinitialize half of the population randomly to diversify\n                half = max(1, self.pop_size // 2)\n                idxs = self.rng.choice(self.pop_size, half, replace=False)\n                for j in idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # encourage exploration after reset\n                p_levy = min(0.5, p_levy * 1.2)\n\n            # update overall best tracking\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n            gen += 0  # placeholder for clarity; loop repeats until budget exhausted\n\n        # final results\n        # ensure x_opt consistent\n        if self.x_opt is None:\n            # pick best known from population\n            finite_idx = np.isfinite(fvals)\n            if np.any(finite_idx):\n                idx = np.argmin(fvals[finite_idx])\n                # map back to absolute index\n                all_idx = np.where(finite_idx)[0]\n                real_idx = all_idx[idx]\n                self.x_opt = pop[real_idx].copy()\n                self.f_opt = fvals[real_idx]\n            else:\n                self.x_opt = None\n                self.f_opt = np.inf\n\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.208 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10654414324400463, 0.15956836687751974, 0.270399398712042, 0.29016493320310355, 0.21095798283823008, 0.2523896953935517, 0.2365118242909946, 0.2062152789177325, 0.19897227630048064, 0.14745389960883626]}, "task_prompt": ""}
{"id": "614dd3f1-7915-43bf-9d64-7f8d812b1a0c", "fitness": 0.2908510507370311, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy (Cauchy) long jumps and a shrinking/expanding trust-region local search; parameters (F, CR, jump rate, trust radius) adapt online from success history to balance global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (mutation scale F, crossover CR, Lévy probability, trust radius)\n    are adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # sensible default: scale with dimension but not exceed budget\n            self.pop_size = min(max(20, int(6 * np.log(max(2, self.dim)) + 4 * self.dim // 3)), max(4, self.budget))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"Broadcast a scalar or vector bound to array of length dim.\"\"\"\n        b_arr = np.array(b, dtype=float)\n        if b_arr.size == 1:\n            return np.full(self.dim, float(b_arr))\n        if b_arr.size == self.dim:\n            return b_arr.astype(float)\n        # try to broadcast (for shapes like (1,) or (dim,1))\n        try:\n            return np.broadcast_to(b_arr, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds must be scalar or length equal to dim\")\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # parse bounds (func.bounds.lb / ub expected)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 1.0\n\n        # initialize population\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # evaluate as many as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # if not all evaluated (tiny budget), leave rest as inf (won't be used)\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx]) if fvals[best_idx] != np.inf else np.inf\n        best_x = pop[best_idx].copy() if fvals[best_idx] != np.inf else pop[0].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.3\n        p_levy = 0.08\n        # trust radius in absolute scale, start with fraction of domain norm\n        trust_radius = 0.2 * range_norm\n        min_trust = 1e-9 * range_norm\n        max_trust = 2.0 * range_norm\n\n        no_improve_count = 0\n        stagnation_threshold = max(20, 5 * self.dim)\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (tan-based)\n        def levy_step(scale=1.0, max_abs=10.0):\n            # draw Cauchy-like by tan(pi*(u-0.5)), then limit extremes\n            u = rng.rand(self.dim)\n            step = np.tan(np.pi * (u - 0.5))\n            # limit extreme tails\n            step = np.clip(step, -max_abs, max_abs)\n            return scale * step\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            gen_successes = 0\n            successful_Fs = []\n            successful_CRs = []\n\n            # iterate individuals sequentially\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # If this individual hasn't been evaluated (inf), treat it as low-quality\n                target = pop[i].copy()\n\n                # adapt per-individual F and CR (jDE-like sampling around means)\n                Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.2))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0))\n\n                # decide exploration mode\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on current best (global exploration)\n                    # scale by trust radius and domain\n                    scale = (trust_radius / (range_norm + 1e-12)) * (0.5 + rng.rand())\n                    candidate = best_x + levy_step(scale=scale) * range_vec\n                    # small chance to sample totally random to diversify\n                    if rng.rand() < 0.05:\n                        candidate = lb + rng.rand(self.dim) * range_vec\n                    mode = 'levy'\n                else:\n                    # DE/rand/1-like mutation and binomial crossover\n                    idxs = np.arange(self.pop_size)\n                    # ensure r1,r2,r3 distinct from each other and i\n                    choices = idxs[idxs != i]\n                    r = rng.choice(choices, size=3, replace=False)\n                    r1, r2, r3 = r[0], r[1], r[2]\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    # ensure at least one gene from donor\n                    cr_mask[rng.randint(0, self.dim)] = True\n                    candidate = target.copy()\n                    candidate[cr_mask] = donor[cr_mask]\n                    mode = 'de'\n\n                # projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate (if budget left)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                replaced = False\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    replaced = True\n                    gen_successes += 1\n                    # adapt means based on success\n                    successful_Fs.append(Fi)\n                    successful_CRs.append(CRi)\n                    # slight nudging when success in levy mode to prefer exploration parameters\n                    if mode == 'levy':\n                        p_levy = max(1e-4, p_levy * 0.95)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # on improvement, shrink trust-region slightly (focus search)\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    no_improve_count = 0\n                else:\n                    no_improve_count += 1\n\n                # update means incrementally when successes occur\n                # We'll update after iteration with averages\n\n            # end of population loop; update means using collected successes\n            if len(successful_Fs) > 0:\n                F_mean = 0.9 * F_mean + 0.1 * float(np.mean(successful_Fs))\n            else:\n                # small drift toward exploration when no success\n                F_mean = 0.99 * F_mean + 0.01 * (0.5 + 0.5 * rng.rand())\n\n            if len(successful_CRs) > 0:\n                CR_mean = 0.9 * CR_mean + 0.1 * float(np.mean(successful_CRs))\n            else:\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * rng.rand(), 0.0, 1.0)\n\n            # adjust p_levy based on stagnation\n            if no_improve_count > stagnation_threshold:\n                p_levy = min(0.5, p_levy + 0.03)  # encourage more jumps when stagnating\n                trust_radius = min(max_trust, trust_radius * 1.15)  # expand trust to escape\n            else:\n                # slowly decay jump probability if doing well\n                p_levy = max(1e-5, p_levy * 0.995)\n\n            # trust-region local search around best if we have budget left\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample a few local candidates (small handful scaled by dim, but never exceed budget)\n            local_samples = min(max(1, int(2 + self.dim // 2)), remaining)\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian around best scaled by trust_radius\n                # produce a step vector whose expected norm ~ trust_radius\n                r_scale = (0.5 + rng.rand())  # random scalar to vary steps\n                step = rng.normal(0, 1, size=self.dim)\n                step /= (np.linalg.norm(step) + 1e-12)\n                step *= trust_radius * r_scale\n                candidate = best_x + step\n                candidate = np.clip(candidate, lb, ub)\n\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    trust_radius = max(min_trust, trust_radius * 0.8)  # focus more\n                    local_success += 1\n                    no_improve_count = 0\n                else:\n                    # slightly expand to try to escape\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # encourage exploitation if many local successes\n            if local_success > 0:\n                p_levy = max(1e-6, p_levy * 0.9)\n                # nudge CR_mean a bit toward higher exploitation\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.6, 0.0, 1.0)\n\n            # occasional partial reseed if long stagnation (diversify population)\n            if no_improve_count > 10 * stagnation_threshold and evals < self.budget:\n                # reinitialize half of population\n                n_reseed = max(1, self.pop_size // 2)\n                ids = rng.choice(np.arange(self.pop_size), size=n_reseed, replace=False)\n                for j in ids:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # after reseed slightly enlarge trust radius and increase jumps\n                trust_radius = min(max_trust, trust_radius * 1.4)\n                p_levy = min(0.5, p_levy + 0.05)\n                no_improve_count = 0\n\n            # small cooling of CR to encourage exploitation gradually\n            CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # safety cap on trust radius\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n            # update reported best\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n            # If we've reached a very small trust radius (converged), increase jumps a bit\n            if trust_radius < 1e-6 * range_norm:\n                p_levy = min(0.5, p_levy + 0.02)\n\n            # loop again if budget remains\n\n        # final results\n        # ensure we return the best seen\n        if self.x_opt is None:\n            # if nothing evaluated (budget 0), return a random point\n            x0 = lb + rng.rand(self.dim) * range_vec\n            self.x_opt = x0\n            self.f_opt = float(func(x0)) if self.budget > 0 else np.inf\n\n        return float(self.f_opt), self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.291 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11310703932590671, 0.16038641603224402, 0.30489720405208154, 0.2828347283674202, 0.33920323750722237, 0.7194316713222797, 0.24125525539389459, 0.3209008143059998, 0.23740867884697303, 0.1890854622162892]}, "task_prompt": ""}
{"id": "a17d3994-2934-454c-8b79-02ee2b6abe96", "fitness": 0.6519099838798843, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: differential evolution with online F/CR adaptation, occasional Lévy (Cauchy) long jumps for exploration, and a trust-region Gaussian local search around the best; adapts jump frequency and trust radius from stagnation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight-like (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters are\n    adapted online based on success history and stagnation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: scaled with dim but bounded by budget\n        if pop_size is None:\n            self.pop_size = max(8, min(50, 6 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        # ensure population doesn't exceed a fraction of budget (must leave room for evolution)\n        self.pop_size = min(self.pop_size, max(2, self.budget // 6))\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # result holders\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"Broadcast lower/upper bounds into vectors of length dim.\"\"\"\n        b = np.asarray(b, dtype=float)\n        if b.ndim == 0:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        if np.any(range_vec <= 0):\n            raise ValueError(\"Upper bounds must be greater than lower bounds per-dimension.\")\n        # Initialization\n        pop = self.rng.rand(self.pop_size, self.dim) * range_vec + lb\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # Evaluate initial population while budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                fi = float(func(pop[i]))\n            except Exception:\n                fi = float(\"inf\")\n            fvals[i] = fi\n            evals += 1\n\n        # If none evaluated (very small budget), return a random point evaluated if possible\n        if not np.any(np.isfinite(fvals)):\n            # try one evaluation if budget allowed\n            if evals < self.budget:\n                x = self.rng.rand(self.dim) * range_vec + lb\n                f = float(func(x))\n                evals += 1\n                self.x_opt = x.copy()\n                self.f_opt = f\n                return self.f_opt, self.x_opt\n            else:\n                # No evals possible\n                self.x_opt = pop[0].copy()\n                self.f_opt = float(\"inf\")\n                return self.f_opt, self.x_opt\n\n        # Establish current best\n        best_idx = int(np.nanargmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n        self.x_opt = best_x.copy()\n        self.f_opt = best_f\n\n        # Hyper-parameters and adaptation state\n        F_mean = 0.6\n        CR_mean = 0.9\n        tau_F = 0.1  # prob to sample new F for an individual\n        tau_CR = 0.1\n        max_trust = np.linalg.norm(range_vec) * 1.5\n        min_trust = np.linalg.norm(range_vec) * 1e-6\n        trust_radius = np.linalg.norm(range_vec) * 0.2  # initial trust radius (absolute scale)\n        stagnation_counter = 0\n        recent_successes = 0\n        levy_base_prob = 0.08  # base probability for Lévy long jump\n        levy_max_prob = 0.5\n        levy_scale = 0.6  # relative to range\n        cauchy_clip = 10.0  # avoid numerical blow up\n        generation = 0\n\n        def levy_step():\n            # simple heavy-tailed step via standard Cauchy; limit extremes\n            # generate per-dimension Cauchy and scale\n            step = self.rng.standard_cauchy(self.dim)\n            # clip extreme outliers and scale by levy_scale\n            step = np.clip(step, -cauchy_clip, cauchy_clip) * levy_scale\n            return step\n\n        # Main loop: generation-wise but counting individual evaluations\n        while evals < self.budget:\n            generation += 1\n            gen_successes = 0\n            # shuffle order to avoid bias\n            order = self.rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # decide individual's Fi and CRi (simple jDE-style adaptation)\n                if self.rng.rand() < tau_F:\n                    Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 1.0)\n                else:\n                    Fi = F_mean\n                if self.rng.rand() < tau_CR:\n                    CRi = np.clip(self.rng.normal(CR_mean, 0.15), 0.0, 1.0)\n                else:\n                    CRi = CR_mean\n\n                # choose whether to perform a long Lévy jump (prob increases with stagnation)\n                levy_prob = min(levy_max_prob, levy_base_prob + 0.008 * stagnation_counter)\n                do_levy = (self.rng.rand() < levy_prob)\n\n                if do_levy:\n                    # long jump centered on best, with some directional bias from current\n                    step = levy_step() * range_vec * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                    donor = best_x + step * (0.5 + self.rng.rand() * 0.5)\n                else:\n                    # DE/rand/1 mutation: select three distinct indices\n                    idxs = [j for j in range(self.pop_size) if j != idx]\n                    a, b, c = self.rng.choice(idxs, 3, replace=False)\n                    donor = pop[a] + Fi * (pop[b] - pop[c])\n                    # small bias towards best to improve convergence when near optimum\n                    if self.rng.rand() < 0.2:\n                        donor += 0.05 * Fi * (best_x - pop[idx])\n\n                # binomial crossover\n                cr_mask = self.rng.rand(self.dim) < CRi\n                # ensure at least one dimension from donor\n                jrand = self.rng.randint(self.dim)\n                cr_mask[jrand] = True\n                trial = np.where(cr_mask, donor, pop[idx])\n                # boundary projection (reflective)\n                below = trial < lb\n                above = trial > ub\n                trial = np.where(below, lb + (lb - trial) * 0.5, trial)\n                trial = np.where(above, ub - (trial - ub) * 0.5, trial)\n                # final clip\n                trial = np.clip(trial, lb, ub)\n\n                # Evaluate trial (if budget)\n                if evals >= self.budget:\n                    break\n                try:\n                    f_trial = float(func(trial))\n                except Exception:\n                    f_trial = float(\"inf\")\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_trial <= fvals[idx]:\n                    pop[idx] = trial\n                    fvals[idx] = f_trial\n                    gen_successes += 1\n                    # update DE means toward successful Fi/CRi (small step)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    recent_successes += 1\n                    stagnation_counter = 0\n                else:\n                    # slight drift of means away if many failures (introduce variance)\n                    F_mean = np.clip(0.995 * F_mean + 0.005 * Fi, 0.05, 1.0)\n                    CR_mean = np.clip(0.995 * CR_mean + 0.005 * CRi, 0.0, 1.0)\n                    stagnation_counter += 1\n\n                # update global best\n                if f_trial < best_f:\n                    best_f = f_trial\n                    best_x = trial.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n                # occasional opportunistic trust-region local attempt after improvements\n                if (gen_successes > 0 and self.rng.rand() < 0.03 and evals < self.budget):\n                    # small local Gaussian refinement\n                    local_step = self.rng.normal(0, 0.5 * trust_radius, self.dim) * (range_vec / (np.linalg.norm(range_vec) + 1e-12))\n                    cand = np.clip(best_x + local_step, lb, ub)\n                    try:\n                        fc = float(func(cand))\n                    except Exception:\n                        fc = float(\"inf\")\n                    evals += 1\n                    if fc < best_f:\n                        best_f = fc\n                        best_x = cand.copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        trust_radius = max(min_trust, trust_radius * 0.85)\n                        recent_successes += 1\n                        stagnation_counter = 0\n\n                # safety check: never reuse Fi/CRi beyond their scope (they're local variables)\n\n            # End of generation: trust-region local search (small batch)\n            if evals < self.budget:\n                remaining = self.budget - evals\n                local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n                local_improved = False\n                for _ in range(local_samples):\n                    # anisotropic gaussian: dimension-wise sigma proportional to trust_radius and range\n                    sigma = (trust_radius / (np.linalg.norm(range_vec) + 1e-12)) * (0.5 + self.rng.rand(self.dim))\n                    step = self.rng.normal(0, 1.0, self.dim) * sigma * range_vec\n                    cand = np.clip(best_x + step, lb, ub)\n                    try:\n                        fc = float(func(cand))\n                    except Exception:\n                        fc = float(\"inf\")\n                    evals += 1\n                    if fc < best_f:\n                        best_f = fc\n                        best_x = cand.copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        local_improved = True\n                        recent_successes += 1\n                        stagnation_counter = 0\n                    if evals >= self.budget:\n                        break\n                # trust radius adaptation\n                if local_improved:\n                    trust_radius = max(min_trust, trust_radius * 0.75)\n                else:\n                    # expand slightly to encourage escape if no local improvement\n                    trust_radius = min(max_trust, trust_radius * 1.10)\n\n            # adjust global heuristics after generation\n            if gen_successes == 0:\n                stagnation_counter += 1\n            # if many successes, reduce chance of Levy jumps slightly and shrink trust to exploit\n            if recent_successes > max(3, self.pop_size // 6):\n                levy_base_prob = max(0.01, levy_base_prob * 0.95)\n                trust_radius = max(min_trust, trust_radius * 0.9)\n                recent_successes = 0\n            # if stagnating for long, increase levy probability and diversify\n            if stagnation_counter > max(20, self.pop_size * 2):\n                levy_base_prob = min(levy_max_prob, levy_base_prob * 1.35)\n                # re-seed half of the population to diversify\n                n_reseed = max(1, self.pop_size // 2)\n                worst_idx = np.argsort(fvals)[-n_reseed:]\n                for j in worst_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = self.rng.rand(self.dim) * range_vec + lb\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = float(\"inf\")\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                # expand trust radius to escape potential basin\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n            # safety clamp for means\n            F_mean = np.clip(F_mean, 0.05, 1.0)\n            CR_mean = np.clip(CR_mean, 0.0, 1.0)\n\n        # End of budget\n        # final best is stored in self.f_opt, self.x_opt\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.652 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.24058106159999637, 0.4336147464411836, 0.515222169233214, 0.9445200885473394, 0.9098031373816733, 0.9217078482430787, 0.35744991568987083, 0.7499104250876638, 0.8987292387163387, 0.5475612078584837]}, "task_prompt": ""}
{"id": "bff9380c-afb0-4049-893f-1ca494e54a82", "fitness": 0.6898859764743567, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and an online-adapting trust-region local search for robust global exploration and focused local refinement.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid DE + Lévy jumps + trust-region local search with simple online adaptation.\n    - budget: total number of allowed function evaluations\n    - dim: problem dimensionality\n    - seed: optional RNG seed\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(self.seed)\n\n        # adaptive population sizing: keep reasonable relative to dim and budget\n        # ensure pop_size <= budget so we can at least initialize population\n        base_pop = max(6, 6 * self.dim)\n        self.pop_size = int(min(base_pop, max(2, self.budget)))\n        # initial trust-region radius as fraction of search range\n        self.trust_radius = 0.2  # fraction of range (will be adapted)\n        # Levy jump base probability\n        self.levy_prob = 0.05\n        # adaptation memory (moving means for F and CR)\n        self.F_mean = 0.6\n        self.CR_mean = 0.9\n\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # broadcast to dimension, cast to float\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def _levy_step(self):\n        # heavy-tailed Cauchy-based step, truncated to avoid numerical blow-up\n        step = self.rng.standard_cauchy(self.dim)\n        # truncate extreme tails to keep things numerical\n        step = np.clip(step, -10.0, 10.0)\n        # small scaling to balance exploration\n        scale = 0.5 + self.rng.rand()  # in (0.5,1.5)\n        return step * scale\n\n    def __call__(self, func):\n        # determine bounds from func if present, else use [-5,5]\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        lb = self._ensure_array_bounds(lb)\n        ub = self._ensure_array_bounds(ub)\n        span = ub - lb\n        # ensure no zero spans\n        span[span == 0] = 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * span\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate initial population (but not exceeding budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            f = float(func(pop[i]))\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = pop[i].copy()\n\n        # If there are unevaluated individuals due to tiny budget, shrink population view\n        if evals < self.pop_size:\n            # Only keep evaluated individuals to avoid using unevaluated ones\n            pop = pop[:evals].copy()\n            fvals = fvals[:evals].copy()\n            self.pop_size = evals\n            if self.pop_size == 0:\n                # no evaluations possible\n                return self.f_opt, self.x_opt\n\n        gen = 0\n        stagnation_counter = 0\n        best_since_reset = 0\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            # per-generation counters\n            successes = 0\n            successes_F = []\n            successes_CR = []\n\n            indices = np.arange(self.pop_size)\n            self.rng.shuffle(indices)\n\n            # per-individual processing\n            for idx in indices:\n                if evals >= self.budget:\n                    break\n\n                target = pop[idx].copy()\n\n                # sample Fi and CRi (jDE-like): Fi from Cauchy around F_mean, CRi from normal around CR_mean\n                # but ensure bounds [0.05, 1.0] for F, CR in [0,1]\n                Fi = self.F_mean + 0.1 * self.rng.standard_cauchy()\n                Fi = float(np.clip(Fi, 0.05, 1.0))\n                CRi = float(np.clip(self.CR_mean + 0.1 * self.rng.randn(), 0.0, 1.0))\n\n                # decide whether to perform Levy jump or DE mutation\n                if self.rng.rand() < self.levy_prob:\n                    # Levy-centered jump around current best\n                    step = self._levy_step()\n                    candidate = self.x_opt + step * self.trust_radius * span\n                    # also add small random mixing with target to keep diversity\n                    mix = self.rng.rand(self.dim) * 0.2\n                    candidate = candidate * (1 - mix) + target * mix\n                else:\n                    # DE/rand/1 with optional best guiding\n                    # pick three distinct indices different from idx\n                    if self.pop_size >= 4:\n                        choices = list(range(self.pop_size))\n                        choices.remove(idx)\n                        r = self.rng.choice(choices, size=3, replace=False)\n                        r1, r2, r3 = r\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    else:\n                        # fallback: simple Gaussian perturbation\n                        donor = target + Fi * (self.rng.randn(self.dim) * span * 0.1)\n\n                    # occasionally nudge donor toward best to accelerate convergence\n                    if self.rng.rand() < 0.15:\n                        donor = donor * 0.7 + self.x_opt * 0.3\n\n                    # binomial crossover\n                    cross = self.rng.rand(self.dim) < CRi\n                    # ensure at least one dimension crosses\n                    if not np.any(cross):\n                        cross[self.rng.randint(0, self.dim)] = True\n                    candidate = np.where(cross, donor, target)\n\n                # project to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate if budget remains\n                if evals >= self.budget:\n                    break\n                f_cand = float(func(candidate))\n                evals += 1\n\n                # selection: greedy\n                if f_cand < fvals[idx]:\n                    pop[idx] = candidate\n                    fvals[idx] = f_cand\n                    successes += 1\n                    successes_F.append(Fi)\n                    successes_CR.append(CRi)\n                    # update best if improved\n                    if f_cand < self.f_opt:\n                        self.f_opt = f_cand\n                        self.x_opt = candidate.copy()\n                        best_since_reset += 1\n                        stagnation_counter = 0\n                else:\n                    # slight occasional acceptance with tiny prob to keep diversity (metropolis-like)\n                    if self.rng.rand() < 0.0005:\n                        pop[idx] = candidate\n                        fvals[idx] = f_cand\n\n            # End of generation: adapt F_mean and CR_mean using successes (Lehmer-type update)\n            if len(successes_F) > 0:\n                # weighted update: put more weight on larger Fi for F_mean\n                sf = np.array(successes_F, dtype=float)\n                # Lehmer-like mean encourages larger successful Fi\n                lehmer = (np.sum(sf * sf) / (np.sum(sf) + 1e-12))\n                self.F_mean = 0.9 * self.F_mean + 0.1 * float(np.clip(lehmer, 0.05, 1.0))\n                self.CR_mean = 0.9 * self.CR_mean + 0.1 * float(np.mean(successes_CR))\n            else:\n                # no successes: slightly increase exploration\n                self.F_mean = float(np.clip(self.F_mean * 1.02, 0.05, 1.0))\n                self.CR_mean = float(np.clip(self.CR_mean * 0.98, 0.0, 1.0))\n\n            # trust-region local search around current best (small budget-aware sampling)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample a small number of local candidates: scaled by dim but limited\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            improved_local = False\n            # anisotropic sigma per-dim\n            for _ in range(local_samples):\n                sigma = self.trust_radius * (0.2 + 0.8 * self.rng.rand(self.dim))\n                candidate = self.x_opt + self.rng.randn(self.dim) * sigma * span\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                if evals >= self.budget:\n                    break\n                f_cand = float(func(candidate))\n                evals += 1\n                if f_cand < self.f_opt:\n                    self.f_opt = f_cand\n                    self.x_opt = candidate.copy()\n                    improved_local = True\n                    # incorporate into population by replacing worst individual\n                    worst_idx = int(np.argmax(fvals))\n                    pop[worst_idx] = candidate.copy()\n                    fvals[worst_idx] = f_cand\n\n            # adapt trust region radius depending on local success\n            if improved_local:\n                # shrink to focus\n                self.trust_radius *= 0.7\n            else:\n                # expand to escape local traps\n                self.trust_radius *= 1.15\n\n            # bound trust radius to reasonable fraction of range\n            self.trust_radius = float(np.clip(self.trust_radius, 1e-6, 1.0))\n\n            # adjust levy probability with stagnation: increase if no improvements this generation\n            if successes == 0:\n                stagnation_counter += 1\n                self.levy_prob = float(np.clip(self.levy_prob * 1.08 + 0.01, 0.01, 0.5))\n            else:\n                stagnation_counter = 0\n                # reward exploration reduction when making progress\n                self.levy_prob = float(np.clip(self.levy_prob * 0.9, 0.01, 0.5))\n\n            # mild decay of CR_mean to encourage exploitation slowly\n            self.CR_mean = float(np.clip(self.CR_mean * 0.995, 0.0, 1.0))\n\n            # strong stagnation reset: if no improvement for many evals, re-seed half population\n            if stagnation_counter > max(10, int(50 * (1 + self.dim / 50))):\n                # reinitialize half of the population randomly\n                n_reseed = max(1, self.pop_size // 2)\n                replace_idx = self.rng.choice(self.pop_size, size=n_reseed, replace=False)\n                for j in replace_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * span\n                    f_new = float(func(pop[j]))\n                    evals += 1\n                    fvals[j] = f_new\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = pop[j].copy()\n                        stagnation_counter = 0\n                # enlarge trust region slightly to escape\n                self.trust_radius = float(np.clip(self.trust_radius * 1.3, 1e-6, 1.0))\n                # reset levy_prob somewhat\n                self.levy_prob = float(np.clip(self.levy_prob + 0.05, 0.01, 0.5))\n                # reset some adaptation memory lightly\n                self.F_mean = float(np.clip(self.F_mean * 1.05, 0.05, 1.0))\n                self.CR_mean = float(np.clip(self.CR_mean * 1.05, 0.0, 1.0))\n                stagnation_counter = 0\n\n            # small generation-level random injection to maintain diversity if population has converged\n            diversity = np.std(pop, axis=0).mean() / (span.mean() + 1e-12)\n            if diversity < 1e-3 and evals < self.budget:\n                # perturb a random individual (not the best) mildly\n                idx_perturb = int(self.rng.randint(0, self.pop_size))\n                if self.pop_size > 1:\n                    idx_perturb = (idx_perturb + 1) % self.pop_size\n                pop[idx_perturb] = np.minimum(np.maximum(pop[idx_perturb] + 0.05 * self.rng.randn(self.dim) * span, lb), ub)\n\n        # final result\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.690 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.21082971996827282, 0.7732360165794067, 0.803576117327585, 0.9184983926031003, 0.8319480081956834, 0.9213076381813547, 0.41010839777104924, 0.5717782136242087, 0.7732974131972457, 0.6842798472956602]}, "task_prompt": ""}
{"id": "3b795c5a-4e88-4ef2-adc7-9a68c050012a", "fitness": 0.3285261411942533, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-like adaptation, occasional truncated Cauchy (Lévy-like) long jumps, and a shrinking/expanding trust-region local search around the current best to balance global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history. Designed for box bounds [-5,5]\n    or func.bounds if provided.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        if self.seed is not None:\n            np.random.seed(self.seed)\n\n        # population size heuristics: scale with dim but not too large vs budget\n        if pop_size is None:\n            p = max(6, int(8 * self.dim))  # heuristic base\n            p = min(p, max(4, self.budget // 6))\n            self.pop_size = p\n        else:\n            self.pop_size = int(pop_size)\n\n        # state for results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, x, lb, ub):\n        # broadcast and clip an array to bounds\n        x = np.asarray(x, dtype=float)\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def __call__(self, func):\n        # Establish bounds from func if available, else default to [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_ = ub - lb\n        range_[range_ == 0] = 1.0\n\n        budget = int(self.budget)\n        dim = self.dim\n        pop_size = max(4, min(self.pop_size, budget))  # at least 4\n\n        # initialize population uniformly in bounds\n        pop = np.random.rand(pop_size, dim) * range_ + lb\n\n        # evaluate initial population but respect budget\n        fpop = np.full(pop_size, np.inf)\n        evals = 0\n        for i in range(pop_size):\n            if evals >= budget:\n                break\n            try:\n                fpop[i] = float(func(pop[i]))\n            except Exception:\n                fpop[i] = np.inf\n            evals += 1\n\n        # If budget too small to evaluate full pop, leave remaining individuals with inf\n        # find initial best among evaluated\n        evaluated_mask = np.isfinite(fpop)\n        if np.any(evaluated_mask):\n            best_idx = int(np.nanargmin(fpop))\n            best_x = pop[best_idx].copy()\n            best_f = float(fpop[best_idx])\n        else:\n            # nothing evaluated (budget==0)\n            self.f_opt = np.inf\n            self.x_opt = pop[0].copy()\n            return self.f_opt, self.x_opt\n\n        # jDE-like parameter adaptation state\n        F_mean = 0.6\n        CR_mean = 0.5\n        tau_F = 0.1\n        tau_CR = 0.1\n\n        # Levy / Cauchy jump baseline probability, adaptively changed\n        levy_prob = 0.06\n\n        # trust-region radius (relative fraction of range)\n        trust_radius = 0.15  # relative to search range\n        trust_min = 1e-4\n        trust_max = 1.0\n\n        # stagnation and adaptation trackers\n        best_since = 0   # evals since last improvement\n        stagnation_limit = max(40, 8 * dim)\n        recent_success_F = []\n        recent_success_CR = []\n        levy_success = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy, truncated\n        def levy_step(scale=1.0):\n            # generate a vector of independent Cauchy draws, truncated\n            step = np.random.standard_cauchy(size=dim)\n            # scale per-dim and clip extreme outliers to avoid blow-up\n            clip_val = 10.0\n            step = np.clip(step, -clip_val, clip_val)\n            return step * scale\n\n        # Main loop: process generations until budget exhausted\n        while evals < budget:\n            gen += 1\n            # per-generation success trackers\n            succ_F = []\n            succ_CR = []\n            succ_count = 0\n\n            # adapt per-generation randomization of F and CR around their means\n            for i in range(pop_size):\n                if evals >= budget:\n                    break\n\n                target = pop[i].copy()\n                target_f = fpop[i]\n\n                # sample Fi and CRi with small jDE-like randomization\n                if np.random.rand() < tau_F:\n                    Fi = np.random.uniform(0.1, 0.9)\n                else:\n                    Fi = np.clip(np.random.normal(F_mean, 0.05), 0.05, 0.95)\n\n                if np.random.rand() < tau_CR:\n                    CRi = np.random.rand()\n                else:\n                    CRi = np.clip(np.random.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # create donor vector: sometimes use Lévy jump centered on best\n                if np.random.rand() < levy_prob:\n                    # long jump: best + cauchy * trust_radius * range\n                    jump = levy_step(scale=1.0)\n                    donor = best_x + jump * (trust_radius * range_)\n                    # also nudge with a small DE difference to add diversity\n                    # pick two random distinct indices\n                    idxs = [idx for idx in range(pop_size) if idx != i]\n                    if len(idxs) >= 2:\n                        r1, r2 = np.random.choice(idxs, 2, replace=False)\n                        donor += Fi * (pop[r1] - pop[r2])\n                    # record that this was a levy-based candidate (for adaptation)\n                    used_levy = True\n                else:\n                    # standard DE/rand/1 mutation (with Fi)\n                    idxs = [idx for idx in range(pop_size) if idx != i]\n                    if len(idxs) < 3:\n                        donor = pop[i].copy()\n                    else:\n                        r1, r2, r3 = np.random.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    used_levy = False\n\n                # binomial crossover\n                crossover_mask = np.random.rand(dim) < CRi\n                # ensure at least one dimension from donor\n                jrand = np.random.randint(dim)\n                crossover_mask[jrand] = True\n                trial = np.where(crossover_mask, donor, target)\n                # projection to bounds\n                trial = self._ensure_array_bounds(trial, lb, ub)\n\n                # one evaluation (if budget allows)\n                if evals >= budget:\n                    break\n                try:\n                    f_trial = float(func(trial))\n                except Exception:\n                    f_trial = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                if not np.isfinite(target_f) or f_trial <= target_f:\n                    # Replace\n                    pop[i] = trial\n                    fpop[i] = f_trial\n                    succ_count += 1\n                    # record success parameters for adaptation\n                    succ_F.append(Fi)\n                    succ_CR.append(CRi)\n                    if used_levy:\n                        levy_success += 1\n                    # update global best\n                    if f_trial < best_f:\n                        best_f = f_trial\n                        best_x = trial.copy()\n                        best_since = 0\n                    else:\n                        best_since += 0  # no change to reset here\n                else:\n                    best_since += 1\n\n                # update per-individual memory minimal: continue\n\n            # End of generation: adapt F_mean and CR_mean\n            if len(succ_F) > 0:\n                # use Lehmer-like mean for F (gives weight to larger successful F)\n                numer = np.sum(np.array(succ_F) ** 2)\n                denom = np.sum(np.array(succ_F))\n                if denom > 0:\n                    F_new = numer / denom\n                else:\n                    F_new = F_mean\n                F_mean = 0.9 * F_mean + 0.1 * np.clip(F_new, 0.05, 0.95)\n                CR_mean = 0.9 * CR_mean + 0.1 * (np.mean(succ_CR))\n            else:\n                # slight drift if no success\n                F_mean = np.clip(0.98 * F_mean + 0.02 * (0.5), 0.05, 0.95)\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.5, 0.0, 1.0)\n\n            # Adjust levy_prob: if levy successes occur often, reduce; if stagnating, increase\n            if succ_count == 0:\n                levy_prob = min(0.5, levy_prob * 1.05 + 0.002)\n            else:\n                # if many successes but few due to levy, prefer exploitation\n                if levy_success > 0:\n                    levy_prob = max(0.01, levy_prob * 0.95)\n                else:\n                    levy_prob = max(0.01, levy_prob * 0.98)\n\n            # trust-region local search around best: sample a small number of local candidates\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n            # sample count depends on dim and budget\n            n_local = int(min(max(1, dim // 2), max(1, remaining // max(1, pop_size // 4))))\n            n_local = min(n_local, remaining)\n            local_success = 0\n            for _ in range(n_local):\n                # anisotropic sigma: per-dim random factor\n                sigma = trust_radius * (0.5 + np.random.rand(dim))  # [0.5,1.5]*trust_radius\n                local = best_x + np.random.randn(dim) * (sigma * range_)\n                local = self._ensure_array_bounds(local, lb, ub)\n                try:\n                    f_local = float(func(local))\n                except Exception:\n                    f_local = np.inf\n                evals += 1\n                if f_local < best_f:\n                    best_f = f_local\n                    best_x = local.copy()\n                    local_success += 1\n                    best_since = 0\n                else:\n                    best_since += 1\n                # replace worst in population if local better than worst\n                worst_idx = int(np.nanargmax(fpop))\n                if f_local < fpop[worst_idx]:\n                    pop[worst_idx] = local\n                    fpop[worst_idx] = f_local\n\n            # adjust trust radius\n            if local_success > 0:\n                # successful local search: shrink to refine\n                trust_radius = max(trust_min, trust_radius * (0.7 ** local_success))\n                # reward exploitation by nudging means slightly\n                F_mean = np.clip(F_mean * 0.98, 0.05, 0.95)\n                CR_mean = np.clip(0.98 * CR_mean + 0.02 * 0.9, 0.0, 1.0)\n            else:\n                # expand trust region modestly to escape local trap\n                trust_radius = min(trust_max, trust_radius * 1.12)\n                # encourage exploration\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.95)\n                CR_mean = np.clip(0.99 * CR_mean, 0.0, 1.0)\n\n            # stagnation handling: if no improvement for many evaluations, partially re-seed\n            if best_since >= stagnation_limit and evals < budget:\n                # reinitialize half of the population\n                half = max(1, pop_size // 2)\n                for k in range(half):\n                    j = np.random.randint(pop_size)\n                    pop[j] = np.random.rand(dim) * range_ + lb\n                    # evaluate replacement if budget allows\n                    if evals < budget:\n                        try:\n                            fpop[j] = float(func(pop[j]))\n                        except Exception:\n                            fpop[j] = np.inf\n                        evals += 1\n                    else:\n                        fpop[j] = np.inf\n                # slightly enlarge trust radius to escape\n                trust_radius = min(trust_max, trust_radius * 2.0)\n                levy_prob = min(0.4, levy_prob * 2.0)\n                best_since = 0  # reset stagnation measure\n\n            # update global best among population in case local search updated worst replacements\n            curr_best_idx = int(np.nanargmin(fpop))\n            if fpop[curr_best_idx] < best_f:\n                best_f = float(fpop[curr_best_idx])\n                best_x = pop[curr_best_idx].copy()\n                best_since = 0\n\n            # small safeguard: if budget is very low, break\n            if evals >= budget:\n                break\n\n        # finalize\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.329 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0810111833070658, 0.15719305988241572, 0.517829034077041, 0.9108737013839764, 0.26467548624371917, 0.6349061282696455, 0.18698030918446806, 0.17085078963937883, 0.22107191988679764, 0.13986980006802474]}, "task_prompt": ""}
{"id": "d8b56cfa-2005-43eb-a795-095fab3fde37", "fitness": 0.24711994887982325, "name": "HybridDeLevyTrust", "description": "HybridDeLevyTrust — adaptive Differential Evolution with occasional Lévy (Cauchy) long jumps and a trust-region local search, with online step-size and parameter adaptation to balance fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight-like (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters are\n    adapted online based on success history. Designed to respect a strict\n    evaluation budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size depending on dim, but not exceeding budget\n        if pop_size is None:\n            base = int(max(6, min(60, 8 + 2 * np.sqrt(self.dim))))\n            self.pop_size = min(base, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size == self.dim:\n            return b.astype(float)\n        # broadcast to dimension\n        return np.broadcast_to(b.ravel(), (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = np.maximum(ub - lb, 1e-12)\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n        evals = 0\n\n        # evaluate initial population (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals[valid])\n            best_indices = np.where(valid)[0]\n            best_idx = best_indices[best_idx]\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy-style jump\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius (absolute)\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step_cauchy(size):\n            # draw from standard Cauchy and clip extremes to avoid numerical blow-up\n            s = rng.standard_cauchy(size=size)\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # for each individual do mutation/crossover/selection\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample individual-specific parameters (jDE-like)\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                if rng.rand() < p_levy:\n                    # Lévy-like jump centered at best (explorative)\n                    step = levy_step_cauchy(self.dim)\n                    scale = step_scale * (0.3 + 0.7 * rng.rand())  # randomize magnitude\n                    donor = best_x + scale * step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    # small perturbation around donor to create trial\n                    trial = np.clip(donor + rng.normal(0, 0.02, size=self.dim) * range_vec, lb, ub)\n                else:\n                    # DE/rand/1-like mutation and binomial crossover\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback to small gaussian perturbation if population too small\n                        donor = pop[i] + Fi * rng.normal(0, 1.0, size=self.dim) * (range_vec)\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    trial = np.clip(trial, lb, ub)\n\n                # evaluate candidate\n                if evals >= self.budget:\n                    break\n                f_trial = float(func(trial))\n                evals += 1\n\n                # greedy selection\n                if f_trial < fvals[i]:\n                    pop[i] = trial\n                    fvals[i] = f_trial\n                    successes += 1\n                    # adapt means towards successful parameters (small step)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_trial < best_f:\n                    best_f = f_trial\n                    best_x = trial.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # trust-region local search around best: small anisotropic Gaussian samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dimension relative to current trust radius and problem range\n                sigma = (0.3 + rng.rand(self.dim) * 0.7) * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # successful local step => shrink trust radius to focus exploitation\n                    trust_radius *= 0.8\n                    trust_radius = np.clip(trust_radius, min_trust, max_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slightly expand to try to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n            # generation-level adaptation\n            if successes > 0 or local_success > 0:\n                # reward exploitation: slightly reduce chance of Lévy jumps\n                p_levy = max(0.01, p_levy * (0.95 if successes + local_success > max(1, 0.2 * self.pop_size) else 0.98))\n                # gently move F_mean toward a moderate value to keep stability\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n            else:\n                # stagnation: encourage exploration\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                to_replace = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in to_replace:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius a bit after a reset to allow broader search\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.247 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14293704769063476, 0.23523538353203777, 0.3091573093138984, 0.3104444341899265, 0.24217603759630912, 0.3408547131634544, 0.23498317873407137, 0.25230866624403747, 0.21740818315507004, 0.18569453517879275]}, "task_prompt": ""}
{"id": "bb76e778-ac0d-4029-85bd-55c25fe7e9be", "fitness": 0.37822010121150473, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size and operator adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size scaled with dimension\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # ensure not exceeding budget too badly\n            self.pop_size = min(self.pop_size, max(2, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # safety: if bounds are identical, expand slightly to allow search\n        range_vec = ub - lb\n        zero_mask = range_vec == 0.0\n        if np.any(zero_mask):\n            range_vec = np.where(zero_mask, 1.0, range_vec)  # avoid zeros\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (up to budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if none evaluated (tiny budget), return fallback\n        if not np.isfinite(fvals).any():\n            self.f_opt = float(np.inf)\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # global best\n        best_idx = int(np.nanargmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # adaptive hyper-parameters\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # initial probability of Levy jump\n        step_scale = 0.18   # base scale for Levy and trust moves (fraction of range)\n        trust_frac = 0.18   # trust radius as fraction of norm(range_vec)\n        min_trust_frac = 1e-6\n        max_trust_frac = 4.0\n        trust_radius = trust_frac * np.linalg.norm(range_vec)\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            # cap extreme outliers but keep heavy tail\n            s = np.clip(s, -1e3, 1e3)\n            # normalize scale to unit-ish vector to combine with step_scale\n            nrm = np.linalg.norm(s)\n            if nrm == 0:\n                return s\n            return s / nrm\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation small jitter to means to maintain exploration\n            F_mean = np.clip(F_mean * (1.0 + rng.normal(0, 0.005)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + rng.normal(0, 0.01), 0.0, 1.0)\n\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # prepare per-target indices\n                idxs = np.arange(self.pop_size)\n                idxs = idxs[idxs != i]\n\n                Fi = None\n                CRi = None\n                candidate = None\n\n                # decide operation: Levy jump centered on best or DE mutation\n                if rng.rand() < p_levy:\n                    # Levy jump centered on best_x, scaled to problem range and trust radius\n                    step = levy_step()  # unit-ish heavy-tailed direction\n                    scale = step_scale * (0.5 + rng.rand() * 1.0)  # randomize magnitude\n                    # mix trust radius and problem range for robust scaling\n                    problem_scale = np.linalg.norm(range_vec) / max(1.0, self.dim)\n                    mag = scale * (trust_radius + 0.5 * problem_scale)\n                    candidate = best_x + step * mag\n                else:\n                    # DE/rand/1 mutation + binomial crossover\n                    # sample Fi, CRi per-individual (jDE-like)\n                    Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                    CRi = float(np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0))\n                    # select r1,r2,r3 distinct\n                    if idxs.size >= 3:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    else:\n                        # degenerate: fallback sampling with replacement\n                        r1, r2, r3 = rng.randint(0, self.pop_size, 3)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    candidate = trial\n\n                # project to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # move means slightly toward successful parameters (if DE used)\n                    if Fi is not None and CRi is not None:\n                        # small learning rate toward successful Fi/CRi\n                        F_mean = np.clip(0.92 * F_mean + 0.08 * Fi, 0.05, 0.99)\n                        CR_mean = np.clip(0.92 * CR_mean + 0.08 * CRi, 0.0, 1.0)\n                    else:\n                        # success via Levy: reduce p_levy a bit\n                        p_levy = max(0.01, p_levy * 0.98)\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # trust-region local search around current best (small batch)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # number of local samples: a small fraction of population scaled by dim and remaining budget\n            local_samples = min(max(1, self.dim // 2), max(1, remaining // max(1, self.pop_size)))\n            # ensure we don't overspend by picking too many local samples\n            local_samples = min(local_samples, remaining)\n\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic Gaussian noise: per-dim random sigma times trust radius fraction\n                # scale by range_vec to maintain units\n                base = trust_radius / max(1e-12, np.linalg.norm(range_vec))\n                sigma = (0.3 + rng.rand(self.dim) * 0.7) * base\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min(trust_radius * 0.7, max_trust_frac * np.linalg.norm(range_vec)), min_trust_frac * np.linalg.norm(range_vec))\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to encourage escape\n                    trust_radius = min(max_trust_frac * np.linalg.norm(range_vec), max(min_trust_frac * np.linalg.norm(range_vec), trust_radius * 1.06))\n                    stagnation_counter += 1\n\n            # adapt exploration/exploitation rates based on successes and stagnation\n            if successes > 0:\n                # reduce chance of Levy jumps slightly when DE is working\n                p_levy = max(0.01, p_levy * (0.98 if successes > max(1, self.pop_size // 5) else 0.995))\n                # nudge F_mean/CR_mean slightly toward exploitation presets\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # if no successes, increase Levy probability a bit\n                p_levy = min(0.6, p_levy * 1.06)\n\n            # strong stagnation reset: if no improvement for many evals, re-seed part of population\n            if stagnation_counter > max(50, 8 * self.pop_size):\n                k = max(1, self.pop_size // 2)\n                replace_idx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in replace_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius a bit to escape potential basin\n                trust_radius = min(max_trust_frac * np.linalg.norm(range_vec), max(min_trust_frac * np.linalg.norm(range_vec), trust_radius * 2.0))\n                stagnation_counter = 0\n\n            # Occasionally replace worst individuals with Levy jumps to inject long-range novelty\n            if rng.rand() < 0.15 and evals < self.budget:\n                num_replace = max(1, min(3, self.pop_size // 6))\n                worst_idx = np.argsort(fvals)[-num_replace:]\n                for w in worst_idx:\n                    if evals >= self.budget:\n                        break\n                    step = levy_step()\n                    mag = step_scale * (1.0 + rng.rand() * 2.0) * np.linalg.norm(range_vec) / max(1.0, self.dim)\n                    candidate = best_x + step * mag\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < fvals[w]:\n                        pop[w] = candidate\n                        fvals[w] = f_candidate\n                        if f_candidate < best_f:\n                            best_f = f_candidate\n                            best_x = candidate.copy()\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.378 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16892496749580022, 0.23206137040857788, 0.40514543298113215, 0.6162392997228177, 0.3664276265874704, 0.7018626852357882, 0.2860255153666067, 0.3370367795566922, 0.35746109217743727, 0.3110162425827251]}, "task_prompt": ""}
{"id": "c7d87d53-b714-41d5-ac71-e87340714940", "fitness": "-inf", "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online step-size adaptation and simple success-history for jDE-like parameter tuning.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on simple success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # population size default scaled with dimension but kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(80, int(8 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like bounds and broadcast to (dim,)\n        arr = np.asarray(b, dtype=float)\n        if arr.size == 1:\n            return np.full(self.dim, float(arr.item()), dtype=float)\n        if arr.size == self.dim:\n            return arr.astype(float).copy()\n        # try to broadcast other shapes\n        return np.broadcast_to(arr.reshape(-1), (self.dim,)).astype(float).copy()\n\n    def _levy_step(self, alpha=1.4):\n        \"\"\"\n        Mantegna-style Levy step generator for heavy-tailed steps.\n        Returns a scalar step length drawn from a Levy-stable-like distribution.\n        We then multiply by a random direction outside.\n        \"\"\"\n        # Mantegna algorithm: sigma_u depends on alpha\n        sigma_u = (np.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                   (np.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n        u = self.rng.normal(0, sigma_u, size=self.dim)\n        v = self.rng.normal(0, 1.0, size=self.dim)\n        # elementwise step, but we'll normalize to a direction later\n        step = u / (np.abs(v) ** (1.0 / alpha) + 1e-16)\n        # clip extreme outliers\n        step = np.clip(step, -1e3, 1e3)\n        return step\n\n    def __call__(self, func):\n        # read bounds; many BBOB wrappers expose .bounds.lb and .bounds.ub\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # fallback: if bounds are invalid, use -5..5 as requested\n        if np.any(np.isinf(lb)) or np.any(np.isinf(ub)):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # ensure lb < ub\n        swap = lb > ub\n        if np.any(swap):\n            tmp = lb.copy()\n            lb[swap] = ub[swap]\n            ub[swap] = tmp[swap]\n\n        range_vec = ub - lb\n        # prevent zero-range dims\n        range_vec = np.where(range_vec == 0.0, 1.0, range_vec)\n\n        rng = self.rng\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population up to the budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            xi = pop[i].copy()\n            fvals[i] = float(func(xi))\n            evals += 1\n\n        # if no evaluations happened (budget==0)\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # set initial best from evaluated individuals\n        valid_idx = np.isfinite(fvals)\n        if not np.any(valid_idx):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = np.argmin(fvals)\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.06       # initial probability of a Levy jump for a candidate\n\n        # trust region radius in normalized units [0, something], scale with range norm\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n        trust_radius = 0.25 * range_norm\n        min_trust = 1e-6\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        success_global = 0\n        gen = 0\n\n        # Simple memory for successful F/CR (Lehmer mean style)\n        S_F = []\n        S_CR = []\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # iterate over population sequentially (each candidate evaluation counts)\n            idxs_all = np.arange(self.pop_size)\n            rng.shuffle(idxs_all)\n            for i in idxs_all:\n                if evals >= self.budget:\n                    break\n\n                # sample per-individual parameters (jDE-like)\n                Fi = np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                do_levy = (rng.rand() < p_levy)\n\n                if do_levy:\n                    # heavy-tailed jump centered on best with random direction and anisotropy\n                    step_vec = self._levy_step(alpha=1.3)\n                    # normalize direction and scale with trust_radius (in units of range_norm)\n                    if np.linalg.norm(step_vec) == 0:\n                        direction = rng.normal(0, 1.0, size=self.dim)\n                    else:\n                        direction = step_vec / (np.linalg.norm(step_vec) + 1e-16)\n                    # anisotropic scaling: random per-dim multipliers in [0.4,1.6]\n                    anis = 0.4 + rng.rand(self.dim) * 1.2\n                    step = direction * anis * (trust_radius / (range_norm + 1e-16))\n                    donor = best_x + step * range_vec\n                    # small local jitter to keep diversity\n                    donor += rng.normal(0, 0.02, size=self.dim) * range_vec\n                else:\n                    # DE/rand/1/bin\n                    idxs = np.delete(np.arange(self.pop_size), i)\n                    # select 3 distinct indices\n                    r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    donor_mut = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover to generate trial\n                    cross_mask = rng.rand(self.dim) < CRi\n                    if not np.any(cross_mask):\n                        cross_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cross_mask, donor_mut, pop[i])\n                    donor = trial\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # Evaluate candidate (respect budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # record success parameters if DE branch\n                    S_F.append(Fi)\n                    S_CR.append(CRi)\n                    # nudge parameter means toward successful Fi/CRi\n                    # we'll update global means after the generation\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    success_global += 1\n                else:\n                    stagnation_counter += 1\n\n                # safety: keep fvals consistent and finite\n                if not np.isfinite(fvals[i]):\n                    fvals[i] = np.inf\n\n            # End of one \"generation\" through population\n\n            # Adapt F_mean and CR_mean using Lehmer-style mean if we have successes\n            if len(S_F) > 0:\n                # Lehmer mean to bias toward larger successful F\n                Sf = np.array(S_F)\n                F_mean = (np.sum(Sf ** 2) / (np.sum(Sf) + 1e-16)) if np.sum(Sf) > 0 else F_mean\n                # CR mean as simple average\n                CR_mean = float(np.mean(S_CR))\n                # clamp\n                F_mean = np.clip(F_mean, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean, 0.0, 1.0)\n                # clear memory for next generation\n                S_F = []\n                S_CR = []\n\n            # Trust-region local search: sample a few local candidates around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            local_improved = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma scaled by trust_radius (normalized)\n                sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / (range_norm + 1e-12))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_improved += 1\n                    # accept candidate but do not necessarily replace population individuals\n            # adjust trust radius\n            if local_improved > 0:\n                # shrink to intensify\n                trust_radius = max(min_trust, trust_radius * 0.80)\n            else:\n                # expand slightly to escape\n                trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # adapt levy probability based on generation successes\n            if successes > 0:\n                # reduce levy probability slowly when we're making progress\n                p_levy = max(0.005, p_levy * (0.96 if successes > max(1, self.pop_size * 0.15) else 0.98))\n            else:\n                # increase chance of long jumps under stagnation\n                p_levy = min(0.5, p_levy * 1.07)\n\n            # mild diversification if stagnation is high\n            if stagnation_counter > max(50, self.dim * 6):\n                # re-seed a fraction of population randomly and (optionally) evaluate some of them\n                reinit_count = max(1, int(0.25 * self.pop_size))\n                reinit_idx = rng.choice(np.arange(self.pop_size), reinit_count, replace=False)\n                for j in reinit_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius a bit after reset\n                trust_radius = min(max_trust, trust_radius * 1.3)\n                # reset stagnation\n                stagnation_counter = 0\n\n            # a safety break if budget exhausted\n            if evals >= self.budget:\n                break\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'gamma'", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'gamma'", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "7ac43651-7b4d-44b7-9cdf-9874f5aaeb14", "fitness": 0.3774454928135641, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy (Cauchy) long jumps and a trust-region Gaussian local search; online adapts mutation/crossover and exploration rate to balance global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (F, CR, Lévy probability, trust radius) are adapted online.\n    Designed for continuous boxes (here [-5,5] typical in BBOB).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default pop size scaled with dim, but limited by budget\n        if pop_size is None:\n            base = max(8, int(8 + 2 * np.sqrt(self.dim)))\n            self.pop_size = min(base, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds and broadcast to dimensional vectors\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        else:\n            lb = np.asarray(lb, dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        else:\n            ub = np.asarray(ub, dtype=float)\n        # range vector\n        range_vec = ub - lb\n        range_vec[range_vec == 0.0] = 1e-12\n\n        rng = self.rng\n        pop_size = self.pop_size\n        dim = self.dim\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(pop_size, dim) * range_vec\n        fvals = np.full(pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate as many as budget allows\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # find best among evaluated (if none evaluated, sample one point)\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # extremely tiny budget -> do a single random draw and evaluate\n            x0 = lb + rng.rand(dim) * range_vec\n            f0 = float(func(x0))\n            evals += 1\n            self.f_opt = float(f0)\n            self.x_opt = x0.copy()\n            return self.f_opt, self.x_opt\n\n        best_idx = np.nanargmin(fvals)\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.05              # chance to take a Lévy long jump for a candidate\n        trust_radius = 0.2         # fraction of range used as Gaussian sigma factor\n        min_trust = 1e-4\n        max_trust = 1.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: Levy-like step via Cauchy sampling (heavy-tailed); clipped\n        def levy_step():\n            # Cauchy has heavy tails; clip extreme outliers for numerical stability\n            s = rng.standard_cauchy(size=dim)\n            s = np.clip(s, -1e6, 1e6)\n            # scale provide moderate magnitude\n            scale = 0.5 + 1.5 * rng.rand()\n            return s * scale\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            succ_F = []\n            succ_CR = []\n\n            # per-generation random jitter\n            for i in range(pop_size):\n                if evals >= self.budget:\n                    break\n\n                # decide exploration type\n                if rng.rand() < p_levy:\n                    # Levy jump centered on best_x\n                    step = levy_step()\n                    # scale by coordinate-wise range (normalized) and trust radius\n                    norm_factor = np.maximum(range_vec.mean(), 1e-12)\n                    scale = (0.3 * trust_radius + 0.7 * rng.rand()) * (range_vec / norm_factor)\n                    candidate = best_x + step * scale\n                else:\n                    # Differential Evolution rand/1/bin style\n                    # pick three distinct indices different from i\n                    idxs = np.arange(pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # degenerate small population -> random jump\n                        candidate = lb + rng.rand(dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        # per-individual Fi and CRi (jDE-like sampling)\n                        Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0))\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = rng.rand(dim) < CRi\n                        # ensure at least one variable from donor\n                        cr_mask[rng.randint(0, dim)] = True\n                        candidate = np.where(cr_mask, donor, pop[i])\n\n                # project candidate into bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate only if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    # replace\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # record if Fi/CRi present\n                    if 'Fi' in locals():\n                        succ_F.append(Fi)\n                    if 'CRi' in locals():\n                        succ_CR.append(CRi)\n                    # update global best if improved\n                    if f_candidate < best_f - 1e-15:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                # cleanup possible locals to avoid accidental reuse next iteration\n                if 'Fi' in locals():\n                    del Fi\n                if 'CRi' in locals():\n                    del CRi\n\n            # End of generation: adapt F_mean and CR_mean via simple moving towards successful params\n            if len(succ_F) > 0:\n                # prefer larger Fs if they succeeded; use simple exponential smoothing\n                F_mean = 0.9 * F_mean + 0.1 * np.mean(succ_F)\n            else:\n                # small drift to increase exploration a bit if no success\n                F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n\n            if len(succ_CR) > 0:\n                CR_mean = 0.9 * CR_mean + 0.1 * np.mean(succ_CR)\n            else:\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 0.99)\n\n            # adapt Levy probability depending on success / stagnation\n            if successes > max(1, pop_size // 10):\n                # many successes -> reduce long jumps slightly\n                p_levy = max(0.005, p_levy * 0.95)\n                stagnation_counter = max(0, stagnation_counter - 1)\n            elif successes == 0:\n                # no success -> raise chance of long jumps\n                p_levy = min(0.5, p_levy * 1.08)\n                stagnation_counter += 1\n            else:\n                # modest success -> small decay\n                p_levy = np.clip(p_levy * 0.98, 0.005, 0.5)\n\n            # trust-region local search occasionally or when stagnating\n            remaining = self.budget - evals\n            do_local = (gen % 5 == 0) or (stagnation_counter >= max(3, dim // 2))\n            if do_local and remaining > 0:\n                # local samples limited by budget and dimension\n                local_samples = min(remaining, max(1, dim // 2 + 1))\n                local_success = 0\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic Gaussian around best scaled by trust_radius and range_vec\n                    sigma = trust_radius * (0.5 + rng.rand(dim))  # per-dim factor\n                    candidate = best_x + rng.normal(0.0, sigma) * range_vec\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    f_cand = float(func(candidate))\n                    evals += 1\n                    if f_cand < best_f:\n                        best_f = f_cand\n                        best_x = candidate.copy()\n                        local_success += 1\n                        stagnation_counter = 0\n                    # optionally replace worst individuals with good local samples\n                    # replace the worst individual if the candidate is better than worst\n                    worst_idx = int(np.argmax(fvals))\n                    if f_cand < fvals[worst_idx]:\n                        pop[worst_idx] = candidate.copy()\n                        fvals[worst_idx] = f_cand\n\n                # adapt trust radius depending on local success\n                if local_success > 0:\n                    # shrink to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                else:\n                    # expand a bit to encourage escaping\n                    trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # strong stagnation reset: re-seed part of population if no improvement for a while\n            if stagnation_counter >= max(10, 3 * dim):\n                k = max(1, pop_size // 2)\n                # replace k worst individuals with random samples and evaluate them (as budget allows)\n                worst_idx_sorted = np.argsort(-fvals)  # descending => worst first\n                replaced = 0\n                for idx in worst_idx_sorted[:k]:\n                    if evals >= self.budget:\n                        break\n                    new_x = lb + rng.rand(dim) * range_vec\n                    new_f = float(func(new_x))\n                    evals += 1\n                    pop[idx] = new_x.copy()\n                    fvals[idx] = new_f\n                    replaced += 1\n                    if new_f < best_f:\n                        best_f = new_f\n                        best_x = new_x.copy()\n                        stagnation_counter = 0\n                # after reset encourage exploration\n                trust_radius = min(max_trust, trust_radius * 1.2)\n                p_levy = min(0.5, p_levy * 1.5)\n                stagnation_counter = 0 if replaced > 0 else stagnation_counter\n\n            # update stored best\n            # (recompute in case initial best was not the absolute best)\n            cur_best_idx = int(np.nanargmin(fvals))\n            if fvals[cur_best_idx] < best_f:\n                best_f = float(fvals[cur_best_idx])\n                best_x = pop[cur_best_idx].copy()\n                stagnation_counter = 0\n\n            # small safeguard to ensure trust_radius within bounds\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n        # final result\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.377 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17165909254486922, 0.1648535415750607, 0.4721189304677599, 0.3515509636984787, 0.3213834239989055, 0.4806376227920076, 0.7021794511408634, 0.4036734941607677, 0.2999680697989643, 0.4064303379579639]}, "task_prompt": ""}
{"id": "bcf410a7-535e-4910-91a2-eac6ab8744d4", "fitness": 0.17022154115523785, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a shrinking/expanding trust-region local search with online step-size and parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size: between 8 and 20 per dimension but not exceeding budget/2\n        if pop_size is None:\n            default = max(10, min(20 * self.dim, int(max(8, self.budget // max(4, self.dim)))))\n            self.pop_size = int(min(default, max(4, self.budget)))  # at least 4, not > budget\n        else:\n            self.pop_size = int(pop_size)\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalars or arrays, broadcast to dim\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        # if given larger, take first dim elements\n        if b.size > self.dim:\n            return b.flat[:self.dim].astype(float)\n        # otherwise fill with repeats\n        return np.resize(b.astype(float), self.dim)\n\n    def __call__(self, func):\n        # get bounds from func or default to [-5,5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        range_vec = ub - lb\n        if np.any(range_vec <= 0):\n            # degenerate bounds -> treat as small box\n            range_vec = np.maximum(range_vec, 1e-6)\n            ub = lb + range_vec\n\n        rng = self.rng\n        budget = self.budget\n\n        # initialize population uniformly\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # evaluate initial population until budget exhausted or all evaluated\n        for i in range(self.pop_size):\n            if evals >= budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if we couldn't evaluate even one individual, return trivial\n        if evals == 0:\n            self.f_opt = float(np.min(fvals))\n            idx = int(np.argmin(fvals)) if np.isfinite(fvals).any() else None\n            self.x_opt = pop[idx] if idx is not None else None\n            return self.f_opt, self.x_opt\n\n        # find best\n        best_idx = int(np.argmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.3       # crossover mean\n        p_levy = 0.08       # base probability of a Lévy jump\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        min_trust = 1e-6\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # initial trust radius (absolute)\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy\n        def levy_step(scale=1.0):\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            return s * float(scale)\n\n        # main optimization loop\n        while evals < budget:\n            gen += 1\n            successes = 0\n            # per-generation temporary accumulators for parameter adaptation\n            F_success_vals = []\n            CR_success_vals = []\n\n            # shuffle order to avoid bias\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= budget:\n                    break\n\n                # prepare indices for mutation\n                idxs = [j for j in range(self.pop_size) if j != idx]\n                if len(idxs) < 3:\n                    # not enough individuals to do DE; try random restart of this individual\n                    pop[idx] = lb + rng.rand(self.dim) * range_vec\n                    if evals < budget:\n                        fvals[idx] = float(func(pop[idx])); evals += 1\n                    continue\n\n                # sample per-individual F and CR (small jitter around means)\n                Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0))\n\n                # decide exploration via Levy with small probability\n                if rng.rand() < p_levy:\n                    # Levy jump centered on best\n                    lev_scale = 0.3 * max(1.0, trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                    candidate = best_x + levy_step(scale=lev_scale) * range_vec\n                else:\n                    # DE/current-to-best/1 strategy with rand selection\n                    r1, r2 = rng.choice(idxs, 2, replace=False)\n                    # current vector\n                    current = pop[idx]\n                    # base = current + Fi*(best - current)\n                    base = current + Fi * (best_x - current)\n                    donor = base + Fi * (pop[r1] - pop[r2])\n                    # binomial crossover\n                    mask = rng.rand(self.dim) < CRi\n                    # ensure at least one dimension comes from donor\n                    jrand = rng.randint(self.dim)\n                    mask[jrand] = True\n                    candidate = np.where(mask, donor, current)\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate\n                if evals >= budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # greedy selection\n                if f_candidate < fvals[idx]:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    F_success_vals.append(Fi)\n                    CR_success_vals.append(CRi)\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # end of population pass: adapt global parameters\n            if F_success_vals:\n                # Lehmer-like update: weight larger Fi more\n                F_mean = 0.9 * F_mean + 0.1 * (np.sum(np.array(F_success_vals)**2) / (np.sum(F_success_vals) + 1e-12))\n                CR_mean = 0.9 * CR_mean + 0.1 * (np.mean(CR_success_vals))\n            else:\n                # no successes: slightly broaden F_mean to encourage exploration\n                F_mean = np.clip(F_mean * 1.03, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 0.99)\n\n            # trust-region local search around best: allocate a few evaluations if available\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n            # adapt number of local samples based on dimension and remaining budget\n            local_samples = min( max(1, int(3 + self.dim // 10)), remaining, 8)\n            improved_local = False\n            for _ in range(local_samples):\n                if evals >= budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                rel = rng.rand(self.dim) * 0.5 + 0.5  # in [0.5,1.0]\n                sigma = rel * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                candidate = best_x + rng.randn(self.dim) * sigma * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    improved_local = True\n                    # shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    # also slightly nudge DE params toward exploitation\n                    F_mean = np.clip(0.95 * F_mean + 0.05 * 0.5, 0.05, 0.99)\n                    CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 0.99)\n                else:\n                    # small expansion if not improving\n                    trust_radius *= 1.05\n                    trust_radius = min(max_trust, trust_radius)\n\n            # stagnation handling: if many evaluations without improvement, increase long jumps and re-seed\n            if stagnation_counter > max(50, self.dim * 5):\n                # increase chance of Levy jumps\n                p_levy = min(0.5, p_levy * 1.2)\n                # re-seed a fraction of the population to diversify\n                n_reset = max(1, self.pop_size // 3)\n                reset_indices = rng.choice(self.pop_size, n_reset, replace=False)\n                for j in reset_indices:\n                    if evals >= budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius to escape local basin\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0  # reset\n\n            else:\n                # slowly decay p_levy if progress observed\n                p_levy = max(0.01, p_levy * (0.98 if successes > 0 else 1.02))\n\n            # small safeguard: if remaining budget is tiny, try a few random probes\n            if (budget - evals) <= max(2, self.dim // 5) and (budget - evals) > 0:\n                probes = budget - evals\n                for _ in range(probes):\n                    candidate = lb + rng.rand(self.dim) * range_vec\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n\n            # update stored best values from fvals array for consistency\n            # (in case other individuals improved best without updating best_x variable)\n            min_idx = int(np.argmin(fvals))\n            if fvals[min_idx] < best_f:\n                best_f = float(fvals[min_idx])\n                best_x = pop[min_idx].copy()\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.170 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0971212330296205, 0.15461929123928198, 0.23103907499286636, 0.17094592476609327, 0.1869810284919209, 0.19103473315157637, 0.19337731898788635, 0.17829241709974097, 0.15965269905167412, 0.13915169074171774]}, "task_prompt": ""}
{"id": "e5a8ece7-1819-4392-b798-1a45796eb74d", "fitness": 0.3806245269843842, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining:\n      - Differential Evolution (population-based recombination with jDE-like\n        online F/CR adaptation),\n      - occasional Lévy-flight (heavy-tailed / Cauchy) jumps centered on the\n        current best for long-range exploration,\n      - trust-region local Gaussian sampling around the best for focused exploitation.\n\n    Designed for continuous bounded problems (e.g. Many Affine BBOB). The\n    optimizer respects the exact evaluation budget passed via self.budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible defaults for population size: scale with dim but not too large\n        if pop_size is None:\n            # at least 8, typically 6*dim but not exceeding budget/4\n            p = max(8, 6 * self.dim)\n            p = min(p, max(8, self.budget // 4)) if self.budget > 0 else p\n            self.pop_size = int(p)\n        else:\n            self.pop_size = int(pop_size)\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Ensure bounds are numpy arrays of shape (dim,). Accept scalars or arrays.\n        \"\"\"\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()))\n        if b.size == self.dim:\n            return b.copy()\n        # try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).copy()\n        except Exception:\n            raise ValueError(\"Bounds cannot be interpreted for dimension=%d\" % self.dim)\n\n    def __call__(self, func):\n        # Extract bounds if available, otherwise default to [-5, 5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        # safe fallback if some dimension has zero range\n        range_vec = np.where(range_vec <= 0.0, 1.0, range_vec)\n\n        evals = 0\n        rng = self.rng\n\n        # Initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        # Evaluate initial population as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # Set best if we have any evaluations\n        finite_mask = np.isfinite(fvals)\n        if np.any(finite_mask):\n            best_idx = np.argmin(fvals[finite_mask])\n            # map to absolute index\n            idxs = np.nonzero(finite_mask)[0]\n            best_idx = idxs[best_idx]\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # no evals possible (budget == 0)\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # Algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.05       # initial probability of a Lévy jump per candidate\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 5.0 + 1e-12\n\n        # Initialize trust region radius (scalar) relative to problem range\n        trust_radius = max(1e-3, 0.2 * np.linalg.norm(range_vec) / np.sqrt(self.dim))\n\n        stagnation_counter = 0\n        gen = 0\n\n        # Helper to generate Levy-like heavy-tailed steps (use Cauchy as proxy)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            # clip extreme outliers to keep numerics stable\n            s = np.clip(s, -1e3, 1e3)\n            # scale to have median magnitude around 1\n            med = np.median(np.abs(s)) + 1e-12\n            return s / med\n\n        # Main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # jDE-like per-individual adaptation parameters stored temporarily\n            # process indices in random order\n            indices = rng.permutation(self.pop_size)\n\n            for i in indices:\n                if evals >= self.budget:\n                    break\n\n                # pick three distinct indices for DE mutation (exclude i)\n                idxs = np.arange(self.pop_size)\n                if self.pop_size >= 4:\n                    choices = idxs[idxs != i]\n                    r1, r2, r3 = rng.choice(choices, 3, replace=False)\n                else:\n                    # fallback small-pop handling: pick with replacement excluding i\n                    choices = idxs[idxs != i]\n                    r1 = rng.choice(choices)\n                    r2 = rng.choice(choices)\n                    r3 = rng.choice(choices)\n\n                # sample per-individual F and CR around means (with clipping)\n                Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                do_levy = (rng.rand() < p_levy) or (stagnation_counter > max(50, self.dim * 3) and rng.rand() < 0.5)\n\n                if do_levy:\n                    # Lévy jump centered on best: encourages global exploration\n                    step = levy_step()\n                    # scale per-dimension by range and trust radius (normalize by norm(range_vec))\n                    norm_factor = np.maximum(np.linalg.norm(range_vec), 1e-12)\n                    donor = best_x + step * (trust_radius / norm_factor) * range_vec\n                    # small chance to do a purely random reinitialization instead to diversify\n                    if rng.rand() < 0.02:\n                        donor = lb + rng.rand(self.dim) * range_vec\n                else:\n                    # DE/rand/1 mutation and binomial crossover (trial from donor/pop)\n                    donor_base = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = rng.rand(self.dim) < CRi\n                    # ensure at least one dimension taken from donor\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor_base, pop[i])\n                    donor = trial\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # Evaluate candidate\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement (target-to-trial)\n                if f_candidate <= fvals[i]:\n                    # improvement for that slot\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt parameter means slightly toward the successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # if this candidate is better than global best, update\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                        # contracting trust radius to focus search\n                        trust_radius = max(min_trust, trust_radius * 0.85)\n                    else:\n                        # slight local contraction if it improved the population slot but not global best\n                        trust_radius = max(min_trust, trust_radius * 0.98)\n                else:\n                    # unsuccessful replacement: slowly increase diversity parameters\n                    F_mean = np.clip(0.995 * F_mean + 0.0005, 0.01, 1.0)\n                    CR_mean = np.clip(0.995 * CR_mean + 0.0005, 0.0, 1.0)\n                    trust_radius = min(max_trust, trust_radius * 1.002)\n                    stagnation_counter += 1\n\n            # End of a generation: trust-region local search around best\n            if evals < self.budget:\n                # decide how many local samples to spend: small handful scaled by dim and remaining budget\n                remaining = self.budget - evals\n                local_samples = int(min(max(1, self.dim // 2), max(1, remaining // 20), 8))\n                # anisotropic sigma: base trust_radius scaled per-dimension using random multipliers\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # gaussian local perturbation around best, scaled to problem range\n                    per_dim_scale = (0.25 + rng.rand(self.dim) * 0.75)  # in [0.25,1.0]\n                    sigma = per_dim_scale * (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                    noise = rng.randn(self.dim) * sigma * range_vec\n                    candidate = np.clip(best_x + noise, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        successes += 1\n                        stagnation_counter = 0\n                        # success => shrink trust region to exploit\n                        trust_radius = max(min_trust, trust_radius * 0.7)\n                    else:\n                        # failure => expand a little to escape local trap\n                        trust_radius = min(max_trust, trust_radius * 1.06)\n                        stagnation_counter += 1\n\n            # adjust probability of Lévy jumps based on success/stagnation\n            if successes > max(1, self.dim // 8):\n                # exploitation is working => reduce long jumps probability slightly\n                p_levy = max(0.005, p_levy * 0.9)\n            else:\n                # not many successes => increase chance of long jumps\n                p_levy = min(0.75, p_levy * 1.06 + 0.005)\n\n            # occasional strong stagnation reset: re-seed part of population\n            if stagnation_counter > max(100, self.dim * 8):\n                to_reset = max(1, self.pop_size // 2)\n                reset_indices = rng.choice(np.arange(self.pop_size), to_reset, replace=False)\n                for j in reset_indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # if any new candidate is better, update best\n                    if fvals[j] < best_f:\n                        best_f = float(fvals[j])\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # after a reset, enlarge trust radius to explore\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # nudge p_levy up to encourage exploration after reset\n                p_levy = min(0.9, p_levy * 1.2 + 0.05)\n\n            # small safety: keep means in valid ranges\n            F_mean = np.clip(F_mean, 0.01, 1.0)\n            CR_mean = np.clip(CR_mean, 0.0, 1.0)\n            trust_radius = np.clip(trust_radius, min_trust, max_trust)\n\n        # Finalize\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.381 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1131517609408853, 0.15629099445793904, 0.45423766036866375, 0.25455690047948853, 0.24215239020805357, 0.8605839360526746, 0.3157165744516953, 0.38604463606239514, 0.2643706570853833, 0.7591397597366634]}, "task_prompt": ""}
{"id": "af684e54-5fb5-453b-8382-3faae5b60478", "fitness": 0.6503088910370635, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style online parameter control, occasional Lévy (Cauchy) long jumps for global exploration, and a trust-region Gaussian local search around the best; online stagnation handling and occasional partial reseeding.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE with jDE-like\n    online parameter control), occasional Lévy/Cauchy jumps for long-range\n    exploration, and a trust-region local search around the current best.\n    Evaluations are strictly limited by self.budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size: scale with dim but bounded by budget\n        if pop_size is None:\n            # base size proportional to dim but not exceeding a fraction of budget\n            self.pop_size = int(min(max(8, 4 * self.dim), max(4, self.budget // 8)))\n        else:\n            self.pop_size = max(4, int(pop_size))\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = self.rng\n\n        # Determine bounds (BBOB style often provides func.bounds.lb/ub)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # default known search domain for the Many-Affine BBOB noiseless problems\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # broadcast to vectors\n        if lb.shape == (): lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.shape == (): ub = np.full(self.dim, float(ub), dtype=float)\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        # initialize population\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n\n        # If none evaluated (budget==0), return defaults\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = pop[0].copy()\n            return self.f_opt, self.x_opt\n\n        # Establish initial best from evaluated individuals\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        min_trust = 1e-8\n\n        # jDE-like means for F and CR\n        F_mean = 0.6\n        CR_mean = 0.5\n\n        # probability of a Lévy (Cauchy) long jump (adaptive)\n        p_levy = 0.03\n\n        # stagnation tracking\n        no_improve_count = 0\n        gen = 0\n\n        # helper: Levy-like step using Cauchy (heavy tails). returns a vector.\n        def levy_step(scale=1.0):\n            # Cauchy (standard) heavy tail, limited to avoid numerical blow-ups\n            step = rng.standard_cauchy(self.dim)\n            step = np.clip(step, -1e3, 1e3)\n            return scale * step\n\n        # small helper to sample Fi and CRi in jDE style\n        def sample_jde(Fm, CRm):\n            # jDE: with small prob mutate Fi/CRi randomly, else use mean\n            if rng.rand() < 0.1:\n                Fi = 0.1 + 0.9 * rng.rand()\n            else:\n                Fi = Fm\n            if rng.rand() < 0.1:\n                CRi = rng.rand()\n            else:\n                CRi = CRm\n            return float(np.clip(Fi, 0.01, 0.99)), float(np.clip(CRi, 0.0, 1.0))\n\n        # main loop: generations until budget exhausted\n        # We treat each individual once per generation (one trial = one eval).\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            successful_Fs = []\n            successful_CRs = []\n            prev_best_f = best_f\n\n            # shuffle processing order for diversity\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                # if no budget left stop immediately\n                if evals >= self.budget:\n                    break\n\n                # If this individual hasn't been evaluated yet (fvals==inf),\n                # we try to evaluate it directly first (warm-start)\n                if not np.isfinite(fvals[idx]):\n                    candidate = pop[idx].copy()\n                    # ensure within bounds\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    fvals[idx] = f_candidate\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        no_improve_count = 0\n                    continue\n\n                # sample Fi and CRi\n                Fi, CRi = sample_jde(F_mean, CR_mean)\n\n                # choose mutation strategy: Lévy jump or DE/rand/1\n                if rng.rand() < p_levy:\n                    # Levy jump centered on best for exploration.\n                    # scale relative to current trust radius and global range\n                    step_scale = (trust_radius / max(range_norm, 1e-12)) * (0.5 + rng.rand())\n                    step = levy_step(scale=step_scale)\n                    donor = best_x + step * (range_vec / max(range_vec.mean(), 1e-12))\n                    # small random perturbation for diversity\n                    donor += 0.01 * rng.randn(self.dim) * range_vec / max(range_vec.mean(), 1e-12)\n                else:\n                    # DE/rand/1 mutation (ensure distinct indices)\n                    idxs = list(range(self.pop_size))\n                    idxs.remove(idx)\n                    r1, r2, r3 = rng.choice(idxs, size=3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover to produce candidate\n                jrand = rng.randint(self.dim)\n                mask = rng.rand(self.dim) < CRi\n                mask[jrand] = True  # ensure at least one dimension from donor\n                candidate = np.where(mask, donor, pop[idx])\n                # projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[idx]:\n                    pop[idx] = candidate.copy()\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    successful_Fs.append(Fi)\n                    successful_CRs.append(CRi)\n                    # nudge global F_mean and CR_mean slightly toward used values\n                    # (collected for stronger update at end of generation)\n                # update global best if needed\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    no_improve_count = 0\n                    # shrink trust region to focus exploitation\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                else:\n                    # small expansion on failure to help escape\n                    trust_radius = max(min_trust, trust_radius * 1.001)\n\n            # End of generation: adapt parameters and perform local trust-region search\n            # update F_mean and CR_mean based on successes (simple Lehmer-like update)\n            if len(successful_Fs) > 0:\n                # weighted toward larger Fi successes (Lehmer style)\n                nums = np.array(successful_Fs)\n                F_mean = 0.9 * F_mean + 0.1 * (np.sum(nums**2) / max(1e-12, np.sum(nums)))\n                CR_mean = 0.9 * CR_mean + 0.1 * np.mean(successful_CRs)\n                F_mean = float(np.clip(F_mean, 0.01, 0.99))\n                CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n            else:\n                # slight decay to encourage exploration if no successes\n                F_mean = float(np.clip(0.99 * F_mean + 0.01 * 0.5, 0.01, 0.99))\n                CR_mean = float(np.clip(0.98 * CR_mean, 0.0, 1.0))\n\n            # Update Levy probability based on stagnation\n            if best_f < prev_best_f - 1e-12:\n                # improvement observed -> reduce long jumps\n                p_levy = max(0.005, p_levy * 0.9)\n            else:\n                no_improve_count += 1\n                p_levy = min(0.3, p_levy * (1.0 + 0.02 * no_improve_count))\n\n            # trust-region local search around best: small anisotropic Gaussian samples\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dim\n                sigma = (trust_radius / max(range_vec.mean(), 1e-12)) * (0.1 + 0.9 * rng.rand(self.dim))\n                perturb = rng.normal(scale=sigma) * range_vec\n                candidate = best_x + perturb\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # successful local step => shrink trust radius\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    no_improve_count = 0\n                else:\n                    # unsuccessful local step => slight increase to encourage leaving local trap\n                    trust_radius = max(min_trust, trust_radius * 1.05)\n\n            # overall generation feedback: if many successes, focus more (shrink),\n            # if few and stagnation rising, increase probability of long jumps and slightly diversify\n            if successes > max(1, self.pop_size // 10):\n                trust_radius = max(min_trust, trust_radius * 0.95)\n            elif no_improve_count > max(20, 4 * self.dim):\n                # stagnation handling: re-seed half of population to diversify\n                k = max(1, self.pop_size // 2)\n                reinit_idx = rng.choice(self.pop_size, size=k, replace=False)\n                for j in reinit_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    # eagerly evaluate new individuals if budget allows\n                    f_new = float(func(pop[j]))\n                    evals += 1\n                    fvals[j] = f_new\n                    if f_new < best_f:\n                        best_f = f_new\n                        best_x = pop[j].copy()\n                        no_improve_count = 0\n                # enlarge trust radius a bit after reset\n                trust_radius = min(range_norm, trust_radius * 1.5)\n                # slightly increase exploration probability\n                p_levy = min(0.5, p_levy * 1.5)\n                # mild reset of means to diversify parameters\n                F_mean = np.clip(0.6 * F_mean + 0.4 * (0.1 + 0.8 * rng.rand()), 0.01, 0.99)\n                CR_mean = np.clip(0.6 * CR_mean + 0.4 * rng.rand(), 0.0, 1.0)\n                no_improve_count = 0\n\n            # safety clamps\n            trust_radius = max(min_trust, min(range_norm, trust_radius))\n            p_levy = float(np.clip(p_levy, 0.0, 0.6))\n\n        # finalize\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.650 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16249091034750285, 0.20262733054209403, 0.6782331764495878, 0.9026686787366838, 0.8301828834377188, 0.8664063209895374, 0.608766676131225, 0.7040993264239211, 0.8240211638975128, 0.7235924434148507]}, "task_prompt": ""}
{"id": "12253591-1264-42d0-bd5c-08e762a87498", "fitness": 0.18266596446702493, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style parameter adaptation, occasional Lévy-like heavy-tailed jumps for long-range escapes, and a trust-region local search around the current best with online step-size adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy-based) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters F\n    and CR are adapted online (jDE-like) and the trust radius adapts based\n    on local success. Designed to respect a strict evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # sensible default population size scaled with dimension but not too large\n        if pop_size is None:\n            # base on dim but not exceed half the budget initially\n            default = max(8, min(4 * self.dim, max(8, self.budget // 10)))\n            self.pop_size = int(default)\n        else:\n            self.pop_size = int(pop_size)\n        # never exceed budget (we will still run even if small budget)\n        self.pop_size = min(self.pop_size, max(1, self.budget))\n        # internal results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like; return numpy array of length dim\n        b_arr = np.asarray(b, dtype=float)\n        if b_arr.ndim == 0:\n            return np.full(self.dim, float(b_arr))\n        if b_arr.size == 1:\n            return np.full(self.dim, float(b_arr.item()))\n        if b_arr.size != self.dim:\n            # attempt to broadcast if possible, otherwise raise\n            try:\n                return np.broadcast_to(b_arr, (self.dim,)).astype(float)\n            except Exception as e:\n                raise ValueError(f\"Bounds length {b_arr.size} incompatible with dim={self.dim}\") from e\n        return b_arr.copy()\n\n    def __call__(self, func):\n        # Prepare bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = np.maximum(ub - lb, 1e-12)\n\n        # Initialization\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate initial population as budget allows\n        n_init = min(self.pop_size, self.budget)\n        for i in range(n_init):\n            fvals[i] = func(pop[i])\n            evals += 1\n            if fvals[i] < self.f_opt:\n                self.f_opt = fvals[i]\n                self.x_opt = pop[i].copy()\n\n        # unevaluated individuals (if any) keep f=inf; they will be replaced later\n        # Algorithm hyper-parameters (adapted online)\n        F_mean = 0.6      # mean scale factor for DE\n        CR_mean = 0.3     # mean crossover rate for DE\n        adapt_lr = 0.1    # adaptation learning rate toward successful Fi/CRi\n        p_levy = 0.08     # initial probability of a Lévy jump\n        trust_radius = 0.25 * range_vec.mean()  # initial trust radius (absolute)\n        min_trust = 1e-8\n\n        stagnation_counter = 0\n        best_idx = np.argmin(fvals)\n        if fvals[best_idx] < np.inf:\n            best_f = fvals[best_idx]\n            best_x = pop[best_idx].copy()\n        else:\n            # If none evaluated (very tiny budget), sample one point to be best\n            if evals < self.budget:\n                cand = lb + self.rng.random(self.dim) * range_vec\n                best_f = func(cand); evals += 1\n                best_x = cand.copy()\n                self.f_opt = best_f; self.x_opt = best_x.copy()\n            else:\n                # no evaluations possible\n                return self.f_opt, self.x_opt\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step(scale=1.0):\n            # use Cauchy (standard) and limit extremes\n            s = self.rng.standard_cauchy(self.dim) * scale\n            # cap to avoid numerical blow-up\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # Main loop: keep performing candidate creation + selection until budget exhausted\n        generation = 0\n        while evals < self.budget:\n            generation += 1\n            # per-generation small random jitter of means\n            F_mean = np.clip(F_mean + 0.02 * (self.rng.random() - 0.5), 0.05, 0.95)\n            CR_mean = np.clip(CR_mean + 0.05 * (self.rng.random() - 0.5), 0.0, 1.0)\n\n            # iterate over population (sequentially) or until budget exhausted\n            idxs = np.arange(self.pop_size)\n            # shuffle order to avoid bias\n            for i in self.rng.permutation(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # choose whether to do Lévy jump or DE mutation\n                if self.rng.random() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step(scale=1.0)\n                    # scale step by trust radius and global range\n                    scale_vector = (trust_radius + 0.5 * range_vec) / np.maximum(range_vec.mean(), 1e-12)\n                    candidate = best_x + (0.5 + 0.5 * self.rng.random()) * step * scale_vector\n                    # small local polish possibility: occasionally do a small gaussian around candidate\n                    if self.rng.random() < 0.2:\n                        candidate += self.rng.normal(0, 0.1 * trust_radius, size=self.dim)\n                else:\n                    # DE/rand/1 mutation with binomial crossover (jDE-like adaptation for Fi and CRi)\n                    # sample Fi and CRi around means with some variability\n                    Fi = np.clip(self.rng.normal(F_mean, 0.2), 0.01, 0.99)\n                    CRi = np.clip(self.rng.normal(CR_mean, 0.2), 0.0, 1.0)\n\n                    # pick three distinct indices different from i (if available)\n                    if self.pop_size >= 4:\n                        candidates = [x for x in idxs if x != i]\n                        r1, r2, r3 = self.rng.choice(candidates, 3, replace=False)\n                    else:\n                        # fallback when population small\n                        r = self.rng.choice(idxs, 3, replace=True)\n                        r1, r2, r3 = r[0], r[1], r[2]\n\n                    donor = pop[r1].copy() + Fi * (pop[r2] - pop[r3])\n                    # crossover\n                    trial = pop[i].copy()\n                    jrand = self.rng.integers(self.dim)\n                    mask = self.rng.random(self.dim) < CRi\n                    mask[jrand] = True\n                    trial[mask] = donor[mask]\n                    candidate = trial\n\n                # Projection to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # One evaluation (if still budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = func(candidate)\n                evals += 1\n\n                # Selection and adaptation\n                replaced = False\n                # If original f unknown (inf), we treat candidate as replacement if it has finite value\n                if fvals[i] == np.inf or f_candidate < fvals[i]:\n                    # Greedy replacement\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    replaced = True\n\n                # Update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    # improve adaptation: nudge means toward used parameters (if DE branch)\n                    if 'Fi' in locals():\n                        F_mean = (1.0 - adapt_lr) * F_mean + adapt_lr * Fi\n                        CR_mean = (1.0 - adapt_lr) * CR_mean + adapt_lr * CRi\n                    # success resets stagnation\n                    stagnation_counter = 0\n                else:\n                    # increase stagnation\n                    stagnation_counter += 1\n\n                # When successful DE replacement, make the means move slightly\n                if replaced and 'Fi' in locals():\n                    F_mean = (1.0 - 0.5 * adapt_lr) * F_mean + 0.5 * adapt_lr * Fi\n                    CR_mean = (1.0 - 0.5 * adapt_lr) * CR_mean + 0.5 * adapt_lr * CRi\n\n                # nudges for Levy branch to reduce p_levy on success\n                if replaced and self.rng.random() < 0.5:\n                    p_levy = max(0.01, p_levy * 0.95)\n\n                # small automatic diversity control: if many replacements recently, slightly reduce F_mean to focus\n                # else increase F_mean a bit to encourage exploration\n                if generation % 10 == 0:\n                    # gentle global adjustment\n                    if stagnation_counter < 5:\n                        F_mean = max(0.05, F_mean * 0.99)\n                    else:\n                        F_mean = min(0.95, F_mean * 1.01)\n\n                # clear local Fi/CRi variables to avoid misuse in Levy branch checks\n                if 'Fi' in locals():\n                    del Fi\n                if 'CRi' in locals():\n                    del CRi\n\n            # End of generation: do trust-region local search around best (budget permitting)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # decide number of local samples: small handful scaled by dim and remaining budget\n            local_samples = int(min(max(1, self.dim // 2), max(1, remaining // max(1, self.dim))))\n            # ensure some variety but not blow budget\n            local_samples = max(1, min(local_samples, remaining))\n\n            local_successes = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = trust_radius * (0.3 + 0.7 * self.rng.random(self.dim)) * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                candidate = best_x + self.rng.normal(0, sigma)\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = func(candidate)\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n                    local_successes += 1\n                    # shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    # reward exploration reduction: slightly reduce p_levy\n                    p_levy = max(0.01, p_levy * 0.9)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful local step => gently expand trust to try a wider local net\n                    trust_radius = min(range_vec.mean(), trust_radius * 1.08)\n\n            # Adjust p_levy based on local success/stagnation\n            if local_successes == 0:\n                # no local improvement: increase chance for long jumps\n                p_levy = min(0.6, p_levy * 1.06 + 0.005)\n                stagnation_counter += 1\n            else:\n                p_levy = max(0.01, p_levy * (0.95 ** local_successes))\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5) and evals < self.budget:\n                # reinitialize up to half of the population (but limited by remaining budget for evaluations)\n                k = max(1, self.pop_size // 2)\n                # ensure we can evaluate at least some of newly sampled individuals\n                k = min(k, max(1, self.budget - evals))\n                replace_idxs = self.rng.choice(np.arange(self.pop_size), size=k, replace=False)\n                for j in replace_idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    fvals[j] = func(pop[j])\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        stagnation_counter = 0\n                # slightly enlarge trust radius after reset to encourage exploration\n                trust_radius = min(range_vec.mean(), trust_radius * 1.5)\n                # moderately increase Levy chance to utilize resets\n                p_levy = min(0.5, p_levy * 1.2 + 0.02)\n\n        # finalize results\n        # ensure stored best matches current best variables\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n        return float(self.f_opt), self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.183 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09272388713085267, 0.15398360493726493, 0.24850460417685505, 0.21997134014262187, 0.19344646534361654, 0.21326601140268753, 0.20584049968449936, 0.18020337315002743, 0.17852964835188423, 0.14019021034993961]}, "task_prompt": ""}
{"id": "92653da1-d558-4b1b-aea8-acce80952aef", "fitness": 0.42949902657966066, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and an online-adapting trust-region local search for focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    One-line idea: fast global exploration via DE + Lévy jumps, focused local\n    exploitation via a shrinking/expanding trust region; online adaptation\n    for step sizes and crossover to balance exploration/exploitation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # sensible default population size scaled with dimension\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n\n        # internal placeholders filled during run\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # ensure lower/upper bounds are arrays of length dim\n        b_arr = np.atleast_1d(b)\n        if b_arr.size == 1:\n            return np.full(self.dim, float(b_arr[0]))\n        if b_arr.size == self.dim:\n            return b_arr.astype(float)\n        # any other shape: try to broadcast\n        return np.broadcast_to(b_arr, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        # range per-dimension\n        span = ub - lb\n        max_trust = 0.5 * np.max(span)  # don't let trust region exceed this\n        trust_radius = 0.2 * np.max(span)  # initial trust radius (global relative)\n        trust_radius = max(trust_radius, 1e-6)\n\n        # initialize DE parameters (means for jDE-like adaptation)\n        F_mean = 0.6\n        CR_mean = 0.3\n\n        # probability of using Levy jump on a given target\n        levy_prob = 0.05\n\n        # initialize population\n        pop = np.empty((self.pop_size, self.dim), dtype=float)\n        for i in range(self.pop_size):\n            pop[i] = rng.uniform(lb, ub)\n\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n\n        # evaluate initial population as many as budget permits\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If we couldn't evaluate full pop, leave remaining individuals with inf (unused until re-eval)\n        # bookkeeping for adaptation\n        stagnation_counter = 0\n        stagnation_threshold = max(50, int(0.01 * self.budget))\n        levy_success_counter = 0\n        gen = 0\n\n        # helper: levy-like heavy-tailed step using truncated Cauchy\n        def levy_step(scale):\n            # sample Cauchy and truncate extreme outliers\n            # using standard Cauchy via ratio of normals or numpy\n            step = rng.standard_cauchy(size=self.dim)\n            # truncate extreme values to avoid blow-ups\n            lim = 10.0\n            step = np.clip(step, -lim, lim)\n            return scale * step\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            # per-generation success buffers\n            S_F = []\n            S_CR = []\n            any_improvement_in_gen = False\n\n            # iterate through targets sequentially\n            indices = np.arange(self.pop_size)\n            rng.shuffle(indices)\n            for idx in indices:\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[idx].copy()\n                f_target = fvals[idx]\n\n                # sample per-individual parameters (jDE-like)\n                Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n\n                # decide branch: Levy jump (global exploration) or DE mutation (population-based)\n                if rng.rand() < levy_prob:\n                    # Lévy jump centered around best with occasional long tails\n                    scale = 0.5 * np.mean(span)  # base global scaling\n                    # scale further by trust radius to mix exploration/exploitation\n                    levy_scale = scale * (1.0 + trust_radius / (np.max(span) + 1e-12))\n                    candidate = self.x_opt + levy_step(levy_scale)\n                    branch = 'levy'\n                else:\n                    # DE/rand/1 mutation\n                    # pick three distinct indices different from idx\n                    ids = [j for j in range(self.pop_size) if j != idx]\n                    if len(ids) < 3:\n                        # fallback to random perturbation if pop too small\n                        candidate = x_target + Fi * rng.randn(self.dim)\n                    else:\n                        r1, r2, r3 = rng.choice(ids, size=3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        mask = rng.rand(self.dim) < CRi\n                        if not np.any(mask):\n                            mask[rng.randint(0, self.dim)] = True\n                        candidate = np.where(mask, donor, x_target)\n                    branch = 'de'\n\n                # projection to bounds\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n\n                # evaluate candidate if budget remains\n                f_candidate = None\n                if evals < self.budget:\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                else:\n                    break\n\n                # selection\n                replaced = False\n                if f_candidate < f_target or np.isinf(f_target):\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    replaced = True\n\n                # adaptation based on success\n                if replaced:\n                    any_improvement_in_gen = True\n                    S_F.append(Fi)\n                    S_CR.append(CRi)\n                    if branch == 'levy':\n                        # reward Levy success by slightly lowering levy_prob (focus)\n                        levy_success_counter += 1\n                        levy_prob = max(0.01, levy_prob * 0.95)\n                    else:\n                        # nudge means toward successful Fi/CRi\n                        pass\n\n                    # update global best\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = candidate.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n                else:\n                    # no improvement\n                    stagnation_counter += 1\n\n                # slightly move means toward recent successful parameters\n                if len(S_F) > 0:\n                    # small learning rate\n                    lr = 0.1\n                    F_mean = (1 - lr) * F_mean + lr * np.mean(S_F)\n                if len(S_CR) > 0:\n                    lr = 0.1\n                    CR_mean = (1 - lr) * CR_mean + lr * np.mean(S_CR)\n\n                # occasionally nudge Levy probability upward if stagnating\n                if stagnation_counter > 0 and stagnation_counter % (max(1, stagnation_threshold // 5)) == 0:\n                    levy_prob = min(0.5, levy_prob * 1.05 + 0.001)\n\n                # update levy success counter decay\n                if levy_success_counter > 0 and gen % 10 == 0:\n                    levy_success_counter = max(0, levy_success_counter - 1)\n\n                # clean local Fi/CRi not necessary due to local scope\n\n            # End of generation adjustments\n\n            # trust-region local search around current best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            local_improved = False\n            # anisotropic sigma: random per-dim scaling around trust_radius\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                sigma = trust_radius * (0.5 + rng.rand(self.dim))  # per-dim anisotropy\n                candidate = self.x_opt + rng.randn(self.dim) * sigma\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # accept\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    local_improved = True\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # optionally insert into population replacing worst individual to keep diversity\n                worst_idx = int(np.argmax(fvals))\n                if f_candidate < fvals[worst_idx]:\n                    pop[worst_idx] = candidate\n                    fvals[worst_idx] = f_candidate\n\n            # adjust trust radius based on local success\n            if local_improved:\n                trust_radius = max(1e-8, trust_radius * 0.85)  # shrink to refine\n                # nudge DE parameters toward exploitation\n                F_mean = max(0.05, F_mean * 0.98)\n                CR_mean = min(0.9, CR_mean * 1.02)\n            else:\n                trust_radius = min(max_trust, trust_radius * 1.06)  # expand to escape\n                F_mean = min(1.0, F_mean * 1.01)  # encourage larger steps slightly\n\n            # adjust levy probability based on generation performance\n            if not any_improvement_in_gen:\n                levy_prob = min(0.5, levy_prob * 1.08)\n            else:\n                levy_prob = max(0.01, levy_prob * 0.995)\n\n            # Strong stagnation handling: re-seed half of the population\n            if stagnation_counter >= stagnation_threshold and evals < self.budget:\n                # reinitialize half of population\n                half = max(1, self.pop_size // 2)\n                reorder = rng.permutation(self.pop_size)\n                reinit_idx = reorder[:half]\n                for j in reinit_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = rng.uniform(lb, ub)\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # update global best if necessary\n                    if fvals[j] < self.f_opt:\n                        self.f_opt = fvals[j]\n                        self.x_opt = pop[j].copy()\n                        stagnation_counter = 0\n                # after reset, expand trust region to explore new space\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                # reduce levy_prob a bit to let DE exploit new diversity\n                levy_prob = max(0.01, levy_prob * 0.6)\n                # slightly randomize parameter means\n                F_mean = float(np.clip(rng.normal(F_mean, 0.05), 0.05, 1.0))\n                CR_mean = float(np.clip(rng.normal(CR_mean, 0.05), 0.0, 1.0))\n                stagnation_counter = 0\n\n        # final results\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.429 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12591728728449936, 0.19309412526572178, 0.45753257921528934, 0.9565595177350386, 0.32060991446396914, 0.7456647917393469, 0.29479933428383154, 0.6437892380848069, 0.36003292719521696, 0.19699055052888603]}, "task_prompt": ""}
{"id": "bf6728a7-6759-4992-978c-82a21101b198", "fitness": 0.6100545328179654, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy (Cauchy) long jumps and a trust-region local search; online adapts DE parameters and trust radius to balance global exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size: scale with dim but not exceed budget fraction\n        if pop_size is None:\n            # base size grows with dim; cap to budget/5 but at least 6\n            pop = max(6, 8 + int(1.5 * self.dim))\n            pop = min(pop, max(6, self.budget // 5))\n            self.pop_size = int(pop)\n        else:\n            self.pop_size = int(pop_size)\n        # internal state for last run results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.ones(self.dim) * float(b)\n        elif b.size == self.dim:\n            return b.astype(float)\n        else:\n            raise ValueError(\"Bounds must be scalar or length dim arrays\")\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        # Range for scaling\n        space_range = ub - lb\n        if np.any(space_range <= 0.0):\n            raise ValueError(\"Upper bounds must be > lower bounds per-dimension\")\n\n        # Initialize internal variables\n        budget = self.budget\n        dim = self.dim\n        pop_size = min(self.pop_size, max(2, budget))  # cannot have more individuals than eval budget\n        evals = 0\n\n        # jDE-like parameter means\n        F_mean = 0.6\n        CR_mean = 0.3\n        tau_F = 0.1\n        tau_CR = 0.1\n        learn_rate = 0.1\n\n        # trust region around current best\n        trust_radius = 0.25 * np.mean(space_range)  # initial trust radius\n        trust_min = 1e-6\n        trust_max = np.max(space_range)\n\n        # stagnation tracking\n        best_since_last_reset = np.inf\n        stagnation_evals = 0\n        stagnation_threshold = max(50, 10 * dim)  # if no improvement for this many evals -> partial reset\n        long_stagnation_threshold = max(300, 60 * dim)\n\n        # Levy jump parameters\n        p_levy_base = 0.03  # base probability for long jump\n        p_levy = p_levy_base\n\n        # Initialize population uniformly in bounds and evaluate as many as budget allows\n        pop = lb + rng.rand(pop_size, dim) * space_range\n        pop_f = np.full(pop_size, np.inf)\n\n        # Evaluate initial individuals up to budget\n        n_initial = min(pop_size, budget)\n        for i in range(n_initial):\n            x = pop[i]\n            f = func(x)\n            pop_f[i] = f\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n                best_since_last_reset = self.f_opt\n        # If budget exhausted during initialization, return best so far\n        if evals >= budget:\n            return float(self.f_opt), self.x_opt\n\n        # If some individuals not evaluated due to tiny budget, assign them high cost (they won't be used until re-evaluation)\n        # main loop\n        gen = 0\n        # keep running until budget exhausted\n        while evals < budget:\n            gen += 1\n\n            # Per-generation success records for adaptation\n            succ_F = []\n            succ_CR = []\n            n_success = 0\n\n            # shuffle order to avoid positional bias\n            order = rng.permutation(pop_size)\n\n            for idx in order:\n                if evals >= budget:\n                    break\n\n                # Sample individual-specific F_i and CR_i (jDE-like probabilistic mutation of params)\n                if rng.rand() < tau_F:\n                    Fi = 0.4 + 0.5 * rng.rand()  # sample within [0.4, 0.9]\n                else:\n                    Fi = np.clip(F_mean + 0.1 * rng.randn(), 0.1, 0.95)\n\n                if rng.rand() < tau_CR:\n                    CRi = rng.rand()\n                else:\n                    CRi = np.clip(CR_mean + 0.1 * rng.randn(), 0.0, 1.0)\n\n                # Build mutant vector: DE/rand/1-like anchored by three distinct indices\n                # pick three indices different from idx\n                idxs = [i for i in range(pop_size) if i != idx]\n                if len(idxs) < 3:\n                    # fallback: sample within bounds\n                    a = lb + rng.rand(dim) * space_range\n                    b = lb + rng.rand(dim) * space_range\n                    c = lb + rng.rand(dim) * space_range\n                else:\n                    a_idx, b_idx, c_idx = rng.choice(idxs, 3, replace=False)\n                    a = pop[a_idx]\n                    b = pop[b_idx]\n                    c = pop[c_idx]\n\n                # Occasionally apply heavy-tailed Levy (Cauchy) jump centered on best to encourage long-range moves\n                levy_apply = (rng.rand() < p_levy)\n                if levy_apply:\n                    # generate Cauchy-distributed vector (standard Cauchy via tan(pi*(u-0.5)))\n                    u = rng.rand(dim)\n                    # avoid extreme outliers by truncating\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    cauchy = np.clip(cauchy, -50.0, 50.0)\n                    # center on best and scale by trust radius and range\n                    center = np.copy(self.x_opt) if self.x_opt is not None else lb + rng.rand(dim) * space_range\n                    levy_step = center + (cauchy * (0.5 * trust_radius)) * (space_range / (np.mean(space_range) + 1e-12))\n                    # Use difference vector plus levy perturbation\n                    mutant = a + Fi * (b - c) + 0.7 * (levy_step - a)\n                else:\n                    mutant = a + Fi * (b - c)\n\n                # binomial crossover\n                cross = rng.rand(dim) < CRi\n                # ensure at least one dimension from mutant\n                if not np.any(cross):\n                    cross[rng.randint(dim)] = True\n                trial = np.where(cross, mutant, pop[idx])\n\n                # trust-region local bias: nudge trial toward current best within trust radius sometimes\n                if rng.rand() < 0.15:\n                    if self.x_opt is not None:\n                        # isotropic factor\n                        perturb = rng.randn(dim) * (0.5 * trust_radius)\n                        trial = np.clip(self.x_opt + perturb * (space_range / (np.mean(space_range) + 1e-12)), lb, ub)\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate candidate\n                f_trial = func(trial)\n                evals += 1\n\n                # Selection: greedy\n                if f_trial < pop_f[idx]:\n                    # success\n                    pop[idx] = trial\n                    pop_f[idx] = f_trial\n                    n_success += 1\n                    succ_F.append(Fi)\n                    succ_CR.append(CRi)\n\n                    # update global best\n                    if f_trial < self.f_opt:\n                        self.f_opt = float(f_trial)\n                        self.x_opt = trial.copy()\n                        best_since_last_reset = self.f_opt\n                        stagnation_evals = 0\n                        # shrink trust radius on success to focus search\n                        trust_radius = max(trust_min, trust_radius * 0.85)\n                        # when we find improvement, reduce levy probability slightly\n                        p_levy = max(p_levy_base * 0.5, p_levy * 0.9)\n                else:\n                    # unsuccessful: expand trust radius slightly to encourage exploration\n                    trust_radius = min(trust_max, trust_radius * 1.01)\n\n                # adapt means incrementally\n                if n_success > 0:\n                    # update means toward successful Fi and CRi using small learning rate\n                    F_mean = (1 - learn_rate) * F_mean + learn_rate * np.mean(succ_F)\n                    CR_mean = (1 - learn_rate) * CR_mean + learn_rate * np.mean(succ_CR)\n                    # keep in bounds\n                    F_mean = np.clip(F_mean, 0.05, 0.95)\n                    CR_mean = np.clip(CR_mean, 0.0, 1.0)\n\n                # dynamic adaptation of levy probability based on recent successes/failures\n                if n_success == 0:\n                    p_levy = min(0.5, p_levy * 1.02)\n                else:\n                    p_levy = max(p_levy_base * 0.3, p_levy * (0.98 if n_success < 2 else 0.95))\n\n                # break if budget exhausted\n                if evals >= budget:\n                    break\n\n            # End of generation adjustments\n            gen += 0\n\n            # trust-region local search around best: sample a small number of Gaussian perturbs\n            if evals < budget:\n                # number of local samples depends on remaining budget and dimension\n                remaining = budget - evals\n                local_samples = min( max(1, dim // 2), remaining, max(1, pop_size // 4) )\n                improved_local = False\n                for _ in range(local_samples):\n                    if evals >= budget:\n                        break\n                    # anisotropic sigma: base trust_radius with per-dim random scaling\n                    sigma = trust_radius * (0.5 + rng.rand(dim))\n                    cand = np.clip(self.x_opt + rng.randn(dim) * (sigma * (space_range / (np.mean(space_range) + 1e-12))), lb, ub)\n                    f_cand = func(cand)\n                    evals += 1\n                    if f_cand < self.f_opt:\n                        self.f_opt = float(f_cand)\n                        self.x_opt = cand.copy()\n                        improved_local = True\n                        # strong shrink to focus search\n                        trust_radius = max(trust_min, trust_radius * 0.7)\n                        # replace worst in population with this good local solution\n                        worst_idx = np.argmax(pop_f)\n                        pop[worst_idx] = cand\n                        pop_f[worst_idx] = f_cand\n                if not improved_local:\n                    # expand trust radius moderately to escape stagnation\n                    trust_radius = min(trust_max, trust_radius * 1.1)\n\n            # stagnation handling: detect if no significant improvement\n            stagnation_evals += 1\n            if self.f_opt < best_since_last_reset - 1e-12:\n                best_since_last_reset = self.f_opt\n                stagnation_evals = 0\n\n            # partial re-seed if stagnating\n            if stagnation_evals > stagnation_threshold and evals < budget:\n                # re-seed half of population randomly\n                n_reseed = max(1, pop_size // 2)\n                for i in rng.choice(pop_size, n_reseed, replace=False):\n                    pop[i] = lb + rng.rand(dim) * space_range\n                    # evaluate new ones as budget allows\n                    if evals < budget:\n                        f_new = func(pop[i])\n                        pop_f[i] = f_new\n                        evals += 1\n                        if f_new < self.f_opt:\n                            self.f_opt = float(f_new)\n                            self.x_opt = pop[i].copy()\n                    else:\n                        break\n                # increase trust radius to encourage exploration\n                trust_radius = min(trust_max, trust_radius * 1.5)\n                # increase levy probability to promote jumps\n                p_levy = min(0.5, p_levy * 2.0)\n                stagnation_evals = 0\n\n            # strong stagnation full partial restart\n            if stagnation_evals > long_stagnation_threshold and evals < budget:\n                # reinitialize 70% of population\n                n_reseed = max(1, int(pop_size * 0.7))\n                for i in rng.choice(pop_size, n_reseed, replace=False):\n                    pop[i] = lb + rng.rand(dim) * space_range\n                    if evals < budget:\n                        f_new = func(pop[i])\n                        pop_f[i] = f_new\n                        evals += 1\n                        if f_new < self.f_opt:\n                            self.f_opt = float(f_new)\n                            self.x_opt = pop[i].copy()\n                    else:\n                        break\n                trust_radius = min(trust_max, trust_radius * 2.0)\n                p_levy = min(0.8, p_levy * 3.0)\n                # reset adaptation means slightly to encourage new dynamics\n                F_mean = 0.6\n                CR_mean = 0.3\n                stagnation_evals = 0\n\n            # keep pop_f consistent if we haven't evaluated some individuals\n            # (if some pop_f remain inf, that's fine; they will be evaluated when selected)\n            # Next generation\n\n        # All evaluations exhausted\n        return float(self.f_opt), self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.610 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17525686529594886, 0.1801975318635184, 0.8114157458143612, 0.27322871007484373, 0.7165937117464787, 0.6285587178920471, 0.8198194737769539, 0.71756339598817, 0.8964797528049335, 0.881431422922399]}, "task_prompt": ""}
{"id": "bfec17c1-cf3f-406c-86cd-4d0445df64f9", "fitness": 0.20323657506122578, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range escape, and an online-adaptive trust-region local search (fast global exploration + focused local exploitation).", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # rng for reproducibility\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension and budget but kept reasonable\n        if pop_size is None:\n            pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            pop_size = min(pop_size, max(4, self.budget // 20))\n        self.pop_size = int(pop_size)\n        # internal result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b)).astype(float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast to dim\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to dimension {}\".format(self.dim))\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (but respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # determine initial best among evaluated individuals\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_indices = np.where(valid)[0]\n        best_idx_rel = np.argmin(fvals[best_indices])\n        best_idx = best_indices[best_idx_rel]\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        max_trust = range_norm * 2.0\n        trust_radius = max(0.05 * range_norm, 0.1)  # initial trust radius (absolute scale)\n        min_trust = 1e-6\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # normalize to unit scale but keep heavy-tailed direction\n            s_norm = np.linalg.norm(s)\n            if s_norm == 0:\n                return s\n            return s / max(s_norm, 1e-12)\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # per-generation loop over population (sequential evaluations)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # build index pool excluding target i\n                idxs = np.arange(self.pop_size)\n                idxs = idxs[idxs != i]\n\n                # decide branch: Lévy exploration or DE mutation/crossover\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for long-range exploration\n                    step = levy_step()\n                    scale = step_scale * (0.5 + 0.5 * rng.rand())\n                    donor = best_x + scale * trust_radius / max(range_norm, 1e-12) * (step * range_vec)\n                    # simple trial is donor (no crossover)\n                    candidate = np.clip(donor, lb, ub)\n                    # No Fi/CRi in this branch\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1 mutation and binomial crossover\n                    if len(idxs) < 3:\n                        # fallback: random restart if not enough distinct indices (very small pop)\n                        donor = lb + rng.rand(self.dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        # adapt F and CR per individual with gaussian noise around means\n                        Fi = float(np.clip(rng.normal(F_mean, 0.12), 0.05, 1.0))\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.12), 0.0, 1.0))\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover to produce trial\n                    cr_mask = rng.rand(self.dim) < (CRi if CRi is not None else CR_mean)\n                    if not np.any(cr_mask):\n                        cr_mask[rng.randint(0, self.dim)] = True\n                    trial = np.where(cr_mask, donor, pop[i])\n                    candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate (respect budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement if improved\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt F_mean/CR_mean toward successful Fi/CRi (only if defined)\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # small safeguard: keep means in sensible ranges\n                F_mean = float(np.clip(F_mean, 0.05, 0.99))\n                CR_mean = float(np.clip(CR_mean, 0.0, 0.999))\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # number of local samples: a few, depending on dimension and remaining budget\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: random per-dim factor scaled by trust_radius (absolute)\n                sigma = (0.4 + rng.rand(self.dim) * 0.6) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    successes += 1\n                    # successful local step => shrink trust radius to focus exploitation\n                    trust_radius *= 0.85\n                    trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slightly expand trust radius to encourage escape\n                    trust_radius *= 1.05\n                    trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n                    stagnation_counter += 1\n\n            # Adapt exploration probability and mutation means based on success\n            if successes > 0:\n                # if we had notable success, reduce chance of Lévy jumps gradually\n                p_levy = max(0.01, p_levy * (0.95 if successes > max(1, self.pop_size // 5) else 0.98))\n                # nudge F_mean slightly toward exploitation mid-value\n                F_mean = float(np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99))\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase F_mean for exploration\n                p_levy = min(0.5, p_levy * 1.02)\n                F_mean = float(np.clip(F_mean * 1.02, 0.05, 0.99))\n                CR_mean = float(np.clip(CR_mean * 0.99, 0.0, 0.999))\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, self.pop_size // 2)\n                reinit_indices = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in reinit_indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # after reset, slightly enlarge trust radius to encourage exploration\n                trust_radius = float(np.clip(trust_radius * 1.5, min_trust, max_trust))\n                # reduce p_levy temporarily to avoid too many random jumps immediately after reset\n                p_levy = min(0.25, p_levy * 0.9)\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.203 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12481402806386133, 0.16620789414180637, 0.24238733699517323, 0.23509906576139272, 0.19441728038309714, 0.22700812803003323, 0.20759615051874458, 0.26660477633057056, 0.22325013627060963, 0.14498095411696899]}, "task_prompt": ""}
{"id": "8aa81f74-480a-4ca7-8295-94016d5b2c73", "fitness": 0.217422502123887, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy jumps and a trust-region local search; online adaption of operator strengths balances global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on simple success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # population size scaled with dimension but not exceeding a fraction of budget\n        if pop_size is None:\n            default_ps = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            self.pop_size = min(default_ps, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n\n        # results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.copy()\n        # try to broadcast\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float).copy()\n        except Exception:\n            raise ValueError(\"Bounds cannot be interpreted for dimension %d\" % self.dim)\n\n    def __call__(self, func):\n        # bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm <= 0:\n            # degenerate bounds: single point\n            x0 = lb.copy()\n            f0 = float(func(x0))\n            self.x_opt = x0\n            self.f_opt = f0\n            return f0, x0\n\n        # initialize population uniformly in bounds\n        pop = np.empty((self.pop_size, self.dim), dtype=float)\n        for i in range(self.pop_size):\n            pop[i] = lb + self.rng.rand(self.dim) * range_vec\n\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n        evals = 0\n\n        # evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # find initial best\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals[valid])\n            best_indices = np.where(valid)[0]\n            best_idx = best_indices[best_idx]\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # no evaluations possible\n            self.x_opt = None\n            self.f_opt = np.inf\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6       # initial mutation scale mean\n        CR_mean = 0.9      # crossover probability mean\n        p_levy = 0.08      # probability of a Lévy jump\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        max_trust = range_norm * 2.0\n        min_trust = 1e-6\n\n        stagnation_counter = 0\n        gen = 0\n\n        rng = self.rng\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # Cauchy has heavy tails; cap extremes\n            s = rng.standard_cauchy(self.dim)\n            # cap to reasonable magnitude relative to range\n            s = np.clip(s, -1e2, 1e2)\n            return s\n\n        # main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            succ_F = []\n            succ_CR = []\n\n            # per-generation dynamic scales\n            # slightly jitter means to maintain diversity\n            F_mean = np.clip(F_mean + rng.normal(0, 0.02), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + rng.normal(0, 0.02), 0.0, 1.0)\n\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # decide whether to perform a Lévy-centered exploration or DE mutation\n                do_levy = (rng.rand() < p_levy)\n                if do_levy:\n                    # Lévy jump centered on best_x to explore far regions\n                    step = levy_step()\n                    # scale relative to trust radius and problem range\n                    # random amplitude to mix medium and large jumps\n                    amp = (0.5 + 1.5 * rng.rand()) * (trust_radius / max(range_vec.mean(), 1e-12))\n                    donor = best_x + amp * step * (range_vec / np.maximum(range_vec.mean(), 1e-12))\n                    # small probabilistic local smoothing\n                    donor = donor + rng.normal(0, 0.02, size=self.dim) * range_vec\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1 mutation with jDE-like individual adaptation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback to random sample\n                        donor = lb + rng.rand(self.dim) * range_vec\n                        Fi = None\n                        CRi = None\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        Fi = np.clip(rng.normal(F_mean, 0.15), 0.05, 1.0)\n                        CRi = np.clip(rng.normal(CR_mean, 0.15), 0.0, 1.0)\n                        donor_vec = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                        # binomial crossover\n                        cr_mask = rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            cr_mask[rng.randint(0, self.dim)] = True\n                        trial = np.where(cr_mask, donor_vec, pop[i])\n                        donor = trial\n\n                # projection to bounds\n                candidate = np.clip(donor, lb, ub)\n\n                # evaluate candidate (respect budget)\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement within population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    if Fi is not None:\n                        succ_F.append(Fi)\n                    if CRi is not None:\n                        succ_CR.append(CRi)\n                    # nudge F_mean/CR_mean toward successful Fi/CRi\n                    if succ_F:\n                        F_mean = 0.9 * F_mean + 0.1 * np.mean(succ_F)\n                    if succ_CR:\n                        CR_mean = 0.9 * CR_mean + 0.1 * np.mean(succ_CR)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation: remaining budget\n            remaining = self.budget - evals\n            # trust-region local search around best: sample a few candidates with anisotropic Gaussian noise\n            if remaining > 0:\n                local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n                # sigma scaled by trust_radius relative to mean range\n                sigma = (trust_radius / max(range_vec.mean(), 1e-12))\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # random anisotropic Gaussian: mix dimensions differently\n                    anis = rng.rand(self.dim) * 0.5 + 0.5  # between 0.5 and 1.0\n                    candidate = best_x + rng.normal(0, sigma * anis, size=self.dim) * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step => shrink trust radius to focus\n                        trust_radius = max(min_trust, trust_radius * 0.85)\n                        trust_radius = min(trust_radius, max_trust)\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful => expand slightly to escape local traps\n                        trust_radius = min(max_trust, trust_radius * 1.05)\n                        trust_radius = max(trust_radius, min_trust)\n                        stagnation_counter += 1\n\n            # adapt probability of Levy jumps based on recent successes/stagnation\n            if successes > 0:\n                # if many successes, reduce exploration (Lévy) slightly\n                p_levy = max(0.01, p_levy * (0.98 if successes > max(1, self.pop_size * 0.15) else 0.995))\n                # move means toward exploitation-favoring values mildly\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # stagnating: increase chance of long jumps and diversity\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5) and evals < self.budget:\n                k = max(1, self.pop_size // 2)\n                idxs = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in idxs:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_x = pop[j].copy()\n                        best_f = fvals[j]\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.217 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10862023457085956, 0.17669669617530348, 0.27865326400746127, 0.2716027830903437, 0.23142640805304315, 0.2752411897995449, 0.2274705960957718, 0.230581727829138, 0.21262264698215183, 0.16130947463525236]}, "task_prompt": ""}
{"id": "14c82b40-2611-4316-b4c7-8c8d75fff6e7", "fitness": 0.19629554309436298, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy (Cauchy) long jumps, and a trust-region Gaussian local search with online step-size and operator adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy/Cauchy long jumps for exploration, and a\n    trust-region local Gaussian search around the current best.\n    Online adaptation of F, CR and jump probability is used.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population size scaled with dimension but kept reasonable\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.default_rng(seed)\n        # results placeholders\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # fixed search bounds for BBOB-like tasks\n        lb = np.full(self.dim, -5.0, dtype=float)\n        ub = np.full(self.dim, 5.0, dtype=float)\n        range_vec = ub - lb\n\n        # safety: do not attempt more population members than budget (we still keep structure)\n        pop_size = self.pop_size\n\n        # initialize population\n        pop = lb + self.rng.random((pop_size, self.dim)) * range_vec\n        fvals = np.full(pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population up to budget\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # if none evaluated, return empty result\n        finite_mask = np.isfinite(fvals)\n        if not np.any(finite_mask):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # get initial best\n        best_idx = np.nanargmin(np.where(finite_mask, fvals, np.inf))\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy/Cauchy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius relative to range\n        min_trust = 1e-6\n        max_trust = np.linalg.norm(range_vec) * 2.0\n        # initial trust radius (in units of norm(range_vec))\n        trust_radius = 0.25 * np.linalg.norm(range_vec)\n\n        stagnation_counter = 0\n        gen = 0\n\n        # main optimization loop (generations)\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            success_Fs = []\n            success_CRs = []\n\n            # iterate over population (each trial uses one evaluation)\n            for i in range(pop_size):\n                if evals >= self.budget:\n                    break\n\n                old_f = fvals[i]\n\n                # sample per-individual F and CR (jDE-like)\n                Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 1.2)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                # produce donor either by DE/rand/1 or Levy jump centered on best\n                if self.rng.random() < p_levy:\n                    # Lévy-like heavy-tailed step: use Cauchy (stable, simple)\n                    cauchy = np.clip(self.rng.standard_cauchy(self.dim), -1e3, 1e3)\n                    # scale: combine Fi and trust radius and step_scale\n                    scale = step_scale * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                    donor = best_x + Fi * cauchy * scale * range_vec\n                else:\n                    # DE/rand/1 mutation\n                    idxs = np.arange(pop_size)\n                    if pop_size <= 3:\n                        # fallback: sample with replacement if too small\n                        r1, r2, r3 = self.rng.integers(0, pop_size, size=3)\n                    else:\n                        idxs = idxs[idxs != i]\n                        r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # binomial crossover to produce candidate\n                cross = self.rng.random(self.dim) < CRi\n                # ensure at least one dimension crosses\n                if not np.any(cross):\n                    cross[self.rng.integers(0, self.dim)] = True\n                candidate = np.where(cross, donor, pop[i])\n\n                # projection to bounds (reflection + clip to avoid stagnation at border)\n                # reflect values outside bounds once then clip\n                below = candidate < lb\n                above = candidate > ub\n                # reflect those out-of-bounds components\n                candidate[below] = lb[below] + (lb[below] - candidate[below])\n                candidate[above] = ub[above] - (candidate[above] - ub[above])\n                # final clip to ensure within bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection (greedy)\n                if f_candidate < old_f:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    success_Fs.append(Fi)\n                    success_CRs.append(CRi)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # adapt means from successful parameters (simple moving toward successful values)\n            if len(success_Fs) > 0:\n                # Lehmer-like weighting: larger Fi contribute more\n                sFs = np.array(success_Fs)\n                weighted_mean_F = np.sum(sFs**2) / (np.sum(sFs) + 1e-12)\n                F_mean = 0.9 * F_mean + 0.1 * weighted_mean_F\n                CR_mean = 0.9 * CR_mean + 0.1 * (np.mean(success_CRs) if len(success_CRs) > 0 else CR_mean)\n            else:\n                # encourage exploration a bit if no successes\n                F_mean = np.clip(F_mean * 1.02, 0.05, 1.2)\n                CR_mean = np.clip(CR_mean * 0.995, 0.0, 1.0)\n\n            # trust-region local search around best: small Gaussian anisotropic perturbations\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dimension scaled by trust radius\n                sigma = (0.5 + self.rng.random(self.dim) * 0.5) * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                # bound handling\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = min(max_trust, max(min_trust, trust_radius))\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slightly expand to try escape\n                    trust_radius *= 1.05\n                    trust_radius = min(max_trust, max(min_trust, trust_radius))\n                    stagnation_counter += 1\n\n            # adjust Lévy probability based on recent success/stagnation\n            if successes > 0:\n                # decrease jump probability slowly if we find improvements\n                p_levy = max(0.01, p_levy * (0.95 if successes > pop_size * 0.2 else 0.98))\n                # nudge CR_mean toward exploitation-friendly value\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n            else:\n                # increase jumps when stagnating\n                p_levy = min(0.5, p_levy * 1.08 + 0.01)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 1.0)\n\n            # strong stagnation handling: re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                k = max(1, pop_size // 2)\n                idxs = self.rng.choice(pop_size, k, replace=False)\n                for j in idxs:\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    # replace fval if budget allows\n                    if evals >= self.budget:\n                        fvals[j] = np.inf\n                        continue\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # enlarge trust radius to help escape\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final result assignment\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.196 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10040371827348604, 0.1633134986401621, 0.26049716441641035, 0.25522018065146523, 0.19932506134374706, 0.21280070193188194, 0.2342243036754068, 0.19782629573930055, 0.19144556915468625, 0.14789893711708302]}, "task_prompt": ""}
{"id": "f477fdf6-9d28-4061-83c4-024b2e23599c", "fitness": 0.24852388375629234, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online adaptation of DE parameters and jump probability.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # sensible default population size depending on dimensionality\n        if pop_size is None:\n            p = int(8 + 2 * np.sqrt(self.dim))\n            p = max(8, min(60, p))\n            # limit by budget a bit so we don't spend initial budget evaluating huge pop\n            p = min(p, max(4, self.budget // 10))\n            self.pop_size = int(p)\n        else:\n            self.pop_size = max(4, int(pop_size))\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds size incompatible with problem dimension\")\n\n    def __call__(self, func):\n        # bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # if bounds not provided or nonsense, fallback to [-5,5]\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        rng = self.rng\n        range_vec = ub - lb\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate as many of initial population as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # if we evaluated none (extremely small budget) make sure we at least return NaN-like\n        if np.isfinite(fvals).sum() == 0:\n            # nothing evaluated\n            self.f_opt = np.inf\n            self.x_opt = pop[0].copy()\n            return self.f_opt, self.x_opt\n\n        # fill remaining (if any) with inf (already) and set best among evaluated entries\n        valid_mask = np.isfinite(fvals)\n        best_idx = np.argmin(fvals)\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # DE and hyper-parameters\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.4       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        step_scale = 0.25   # base scale for Lévy and trust radius (fraction of range)\n        trust_radius = 0.2 * range_norm\n        max_trust = 2.0 * range_norm\n        min_trust = 1e-8\n\n        stagnation_counter = 0\n        stagnation_threshold = max(20, 2 * self.pop_size)\n        generation = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step():\n            s = rng.standard_cauchy(self.dim)\n            # clip extreme outliers to keep numerics stable\n            return np.clip(s, -1e3, 1e3)\n\n        # main loop: process until budget exhausted\n        while evals < self.budget:\n            generation += 1\n            successes = 0\n            gen_improvements = 0\n            # iterate population as targets (one-at-a-time to count evals)\n            idxs = np.arange(self.pop_size)\n            rng.shuffle(idxs)\n            for i in idxs:\n                if evals >= self.budget:\n                    break\n\n                # sample individual control parameters (jDE style)\n                Fi = np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0)\n                CRi = np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0)\n\n                target = pop[i]\n                # decide branch: Levy jump around best (exploration) or DE mutation (exploitation)\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best\n                    step = levy_step()\n                    scale = step_scale * (0.3 + 0.7 * rng.rand()) * (range_vec / max(range_vec.mean(), 1e-12))\n                    donor = best_x + step * scale\n                    # create trial candidate via crossover between donor and target\n                    jrand = rng.randint(self.dim)\n                    trial = np.empty(self.dim, dtype=float)\n                    for j in range(self.dim):\n                        if rng.rand() < CRi or j == jrand:\n                            trial[j] = donor[j]\n                        else:\n                            trial[j] = target[j]\n                else:\n                    # classic DE/rand/1 mutation\n                    # pick 3 distinct indices different from i\n                    all_idx = np.arange(self.pop_size)\n                    choices = all_idx[all_idx != i]\n                    if choices.size < 3:\n                        # not enough individuals; fallback to random near best\n                        donor = best_x + Fi * (rng.rand(self.dim) - 0.5) * 2.0 * (range_vec)\n                    else:\n                        r = rng.choice(choices, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    jrand = rng.randint(self.dim)\n                    trial = np.empty(self.dim, dtype=float)\n                    for j in range(self.dim):\n                        if rng.rand() < CRi or j == jrand:\n                            trial[j] = donor[j]\n                        else:\n                            trial[j] = target[j]\n\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                replaced = False\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    replaced = True\n                    successes += 1\n                    # update individual means slightly toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # slight drift to encourage exploration if not replacing\n                    F_mean = np.clip(F_mean * 0.999 + 0.0005, 0.05, 0.99)\n                    CR_mean = np.clip(CR_mean * 0.999 + 0.0005, 0.0, 0.99)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    gen_improvements += 1\n                    stagnation_counter = 0\n                    # tighten trust-region on success\n                    trust_radius = max(min_trust, trust_radius * 0.9)\n                else:\n                    stagnation_counter += 1\n                    # slowly relax trust radius on failures\n                    trust_radius = min(max_trust, trust_radius * 1.01)\n\n                # small adaption of p_levy to reflect immediate exploration success\n                if rng.rand() < 0.02:\n                    # small random walk\n                    p_levy = np.clip(p_levy + rng.normal(0, 0.01), 0.01, 0.5)\n\n            # End of generation: do a trust-region local search around best\n            if evals < self.budget:\n                # local_samples scaled by dimension and remaining budget\n                remaining = self.budget - evals\n                local_samples = min(max(2, self.dim // 2), max(1, remaining // max(1, (10 + self.pop_size))))\n                # do a few anisotropic Gaussian perturbations around best\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic sigma per-dim: fraction of trust_radius relative to range\n                    sigma = (0.3 + 0.7 * rng.rand(self.dim)) * (trust_radius / max(range_norm, 1e-12))\n                    candidate = best_x + rng.normal(0.0, 1.0, self.dim) * sigma\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step => shrink trust radius to focus\n                        trust_radius = max(min_trust, trust_radius * 0.85)\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful => expand a bit to escape local traps\n                        trust_radius = min(max_trust, trust_radius * 1.07)\n\n            # adjust global strategy parameters based on generation outcomes\n            if gen_improvements > 0:\n                # if improvements happened, slightly reduce chance of Levy jumps (exploit)\n                p_levy = max(0.01, p_levy * (0.98 if gen_improvements > self.pop_size * 0.1 else 0.995))\n                # nudge F_mean toward moderate exploitation\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.4, 0.0, 0.99)\n            else:\n                # stagnation handling: increase chance of long jumps and diversity\n                p_levy = min(0.5, p_levy * 1.08)\n                F_mean = np.clip(F_mean * 1.03, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.99, 0.0, 0.99)\n\n            # strong stagnation reset: if no improvement for many evals, re-seed part of population\n            if stagnation_counter >= stagnation_threshold:\n                k = max(1, self.pop_size // 2)\n                for j in rng.choice(np.arange(self.pop_size), k, replace=False):\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    # evaluate new individuals if possible\n                    if evals < self.budget:\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                # enlarge trust radius after reset to encourage exploration\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # encourage more Levy jumps after a reset\n                p_levy = min(0.5, p_levy * 1.5)\n\n            # update tracked best from fvals in case some were evaluated in resets/etc.\n            cur_best_idx = np.argmin(fvals)\n            if np.isfinite(fvals[cur_best_idx]) and fvals[cur_best_idx] < best_f:\n                best_f = float(fvals[cur_best_idx])\n                best_x = pop[cur_best_idx].copy()\n\n            # safety clamps\n            F_mean = float(np.clip(F_mean, 0.05, 0.99))\n            CR_mean = float(np.clip(CR_mean, 0.0, 0.99))\n            p_levy = float(np.clip(p_levy, 0.01, 0.5))\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.249 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11404020182908481, 0.18427048968182713, 0.3125916204753234, 0.3778530060747224, 0.2518479420782981, 0.32509167132942474, 0.24795867825125506, 0.2843525597389097, 0.2468417990149241, 0.140390869089154]}, "task_prompt": ""}
{"id": "536363b9-872f-4f4a-a435-fb7392372826", "fitness": 0.212454115006902, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style parameter adaptation, occasional Cauchy (Lévy-like) long jumps, and a trust-region Gaussian local search around the best solution — balancing fast global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (jDE-style),\n    occasional Lévy-like (Cauchy) jumps for long-range exploration, and a\n    shrinking/expanding trust-region Gaussian local search around the current best.\n    Designed to respect a strict evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but limited by budget\n        if pop_size is None:\n            approx = int(max(6, min(80, 8 + 2 * np.sqrt(self.dim))))\n            approx = min(approx, max(4, self.budget // 20))\n            self.pop_size = int(approx)\n        else:\n            self.pop_size = int(max(4, pop_size))\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # Accept scalar or array-like, return 1-d numpy array of length self.dim\n        arr = np.asarray(b, dtype=float)\n        if arr.size == 1:\n            return np.full(self.dim, float(arr.item()))\n        if arr.size == self.dim:\n            return arr.astype(float).copy()\n        # try to broadcast\n        try:\n            return np.broadcast_to(arr, (self.dim,)).astype(float).copy()\n        except Exception:\n            raise ValueError(\"Bounds must be scalar or length dim\")\n\n    def __call__(self, func):\n        # Prepare bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # enforce typical BBOB bounds if given differently\n        # If bounds are infinite or equal, fall back to [-5,5]\n        if not np.all(np.isfinite(lb)) or not np.all(np.isfinite(ub)) or np.any(ub <= lb):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate initial population until budget permits\n        idx_order = np.arange(self.pop_size)\n        for i in idx_order:\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            fvals[i] = f\n            evals += 1\n\n        # if none evaluated (very tiny budget), return trivial best among evaluated (None)\n        valid_mask = np.isfinite(fvals)\n        if not np.any(valid_mask):\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # set initial best\n        best_idx = int(np.nanargmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n        self.f_opt, self.x_opt = best_f, best_x.copy()\n\n        # Hyper-parameters and adaptive means\n        p_levy = 0.08\n        F_mean = 0.6\n        CR_mean = 0.3\n        tau_F = 0.1  # probability to change Fi\n        tau_CR = 0.1  # probability to change CRi\n\n        trust_radius = 0.2 * np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 0.2\n        min_trust = 1e-6\n        max_trust = 2.0 * np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 10.0\n\n        stagnation_counter = 0\n        stagnation_threshold = max(20, self.budget // 25)\n\n        gen = 0\n        # helper: generate Levy-like heavy-tailed step using standard Cauchy\n        def levy_step():\n            # Cauchy (heavy-tailed) scaled to moderate values and clipped\n            s = self.rng.standard_cauchy(size=self.dim)\n            # scale down extreme tails relative to problem scale\n            s = np.tanh(s / 5.0)  # map to (-1,1)\n            return s\n\n        # Main loop: iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # process individuals in random order each generation\n            order = self.rng.permutation(self.pop_size)\n            for idx in order:\n                if evals >= self.budget:\n                    break\n\n                # jDE-like adaptation: sample Fi and CRi per individual\n                if self.rng.rand() < tau_F:\n                    Fi = 0.1 + 0.9 * self.rng.rand()  # (0.1,1.0)\n                else:\n                    Fi = F_mean\n                if self.rng.rand() < tau_CR:\n                    CRi = self.rng.rand()\n                else:\n                    CRi = CR_mean\n\n                # Choose between Lévy jump and DE-style mutation\n                if self.rng.rand() < p_levy:\n                    # Lévy-like jump centered at best_x (global exploration)\n                    step = levy_step()\n                    scale = trust_radius * (1.0 + self.rng.rand())  # allow somewhat larger jumps\n                    candidate = best_x + scale * step * (range_vec / np.maximum(np.linalg.norm(range_vec), 1e-12))\n                    # small Gaussian perturbation added\n                    candidate += 0.05 * trust_radius * self.rng.randn(self.dim)\n                else:\n                    # DE/current-to-best/1 mutation with rand partner difference\n                    # pick r1,r2 distinct from each other and idx\n                    choices = [j for j in range(self.pop_size) if j != idx]\n                    if len(choices) < 2:\n                        # degenerate case: fallback to random uniform\n                        candidate = lb + self.rng.rand(self.dim) * range_vec\n                    else:\n                        r1, r2 = self.rng.choice(choices, size=2, replace=False)\n                        target = pop[idx]\n                        mutant = target + Fi * (best_x - target) + Fi * (pop[r1] - pop[r2])\n                        # binomial crossover\n                        cross = self.rng.rand(self.dim) < CRi\n                        # ensure at least one dimension from mutant\n                        if not np.any(cross):\n                            cross[self.rng.randint(self.dim)] = True\n                        candidate = np.where(cross, mutant, target)\n\n                # projection to bounds (simple clipping)\n                candidate = np.clip(candidate, lb, ub)\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_candidate < fvals[idx]:\n                    pop[idx] = candidate.copy()\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    # nudge means toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                else:\n                    # slight decay to encourage exploration if fail\n                    F_mean = max(0.05, 0.995 * F_mean)\n                    CR_mean = np.clip(0.995 * CR_mean, 0.0, 1.0)\n\n                # Update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful local exploitation: shrink trust radius\n                    trust_radius = max(min_trust, 0.9 * trust_radius)\n                else:\n                    stagnation_counter += 1\n                    # increase trust radius gradually to escape traps\n                    if stagnation_counter % max(5, self.pop_size // 4) == 0:\n                        trust_radius = min(max_trust, 1.05 * trust_radius)\n\n                # record best overall\n                if best_f < self.f_opt:\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n                # break early if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments: trust-region local search\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # Number of local samples: small handful depending on dimension and remaining budget\n            local_samples = int(max(1, min(5 + self.dim // 10, remaining // 3, 10)))\n            # anisotropic sigma relative to trust_radius and problem scale\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # generate gaussian candidate around best_x\n                # scale per-dim by a random factor to encourage anisotropic local search\n                anis = 0.5 + self.rng.rand(self.dim) * 0.5\n                sigma = (trust_radius / np.maximum(np.linalg.norm(range_vec), 1e-12)) * anis\n                # Create step in absolute coordinates\n                step = sigma * range_vec * self.rng.randn(self.dim)\n                candidate = best_x + step\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate))\n                except Exception:\n                    f_candidate = np.inf\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, 0.7 * trust_radius)\n                    # populate into population replacing worst\n                    worst_idx = int(np.nanargmax(fvals))\n                    pop[worst_idx] = candidate.copy()\n                    fvals[worst_idx] = f_candidate\n                else:\n                    # unsuccessful => expand slightly to escape\n                    trust_radius = min(max_trust, 1.08 * trust_radius)\n\n                # update global best record\n                if best_f < self.f_opt:\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n            # Adapt p_levy based on recent success / stagnation\n            if successes > max(1, self.pop_size // 5):\n                # many successes => reduce long jumps slowly\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                # few successes => increase chance of long jumps\n                p_levy = min(0.5, p_levy * 1.05 + 0.005)\n\n            # If strong stagnation, perform partial re-seeding\n            if stagnation_counter >= stagnation_threshold and evals < self.budget:\n                # reinitialize half of population randomly (or at least some)\n                k = max(1, self.pop_size // 2)\n                reinit_idx = self.rng.choice(self.pop_size, size=k, replace=False)\n                for j in reinit_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    try:\n                        fvals[j] = float(func(pop[j]))\n                    except Exception:\n                        fvals[j] = np.inf\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                        stagnation_counter = 0\n                # enlarge trust radius a bit after reset\n                trust_radius = min(max_trust, 1.5 * trust_radius)\n                # nudge means to diversify\n                F_mean = 0.6\n                CR_mean = 0.3\n                p_levy = min(0.5, p_levy * 1.2)\n\n            # small safety: avoid trust radius becoming NaN/inf\n            if not np.isfinite(trust_radius) or trust_radius <= 0:\n                trust_radius = 0.2 * np.linalg.norm(range_vec) if np.linalg.norm(range_vec) > 0 else 0.2\n\n        # final best\n        # If we never evaluated anything, return None, else return best found\n        if np.isfinite(self.f_opt):\n            return float(self.f_opt), self.x_opt.copy()\n        else:\n            return np.inf, None", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.212 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11247473472642011, 0.16087609233136446, 0.3087431957959663, 0.13670328173429502, 0.20714643620850592, 0.317848269307904, 0.257150748601667, 0.1728410719264013, 0.2969476708532556, 0.15380964858324053]}, "task_prompt": ""}
{"id": "900a047d-a261-4c62-89fd-3d32b22dffbe", "fitness": 0.3603034409530908, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a shrinking/expanding trust-region local search with online step-size adaptation and population resets.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default pop size: scale with dim but keep reasonable\n        if pop_size is None:\n            self.pop_size = min(60, max(8, 6 + 4 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        # if scalar, broadcast\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size != self.dim:\n            # try to broadcast\n            try:\n                return np.broadcast_to(b, (self.dim,)).astype(float)\n            except Exception:\n                raise ValueError(\"Bounds size does not match dim\")\n        return b.astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # safety: if bounds are infinite or not provided, use [-5,5] as requested\n        if not np.all(np.isfinite(lb)) or not np.all(np.isfinite(ub)):\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        rng = self.rng\n\n        pop = rng.rand(self.pop_size, self.dim) * range_vec + lb\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate as many initial individuals as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].astype(float)\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # if no evaluations possible\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # initial best from evaluated individuals\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        if valid_idx.size == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # algorithm hyperparameters and adaptive means\n        F_mean = 0.6\n        CR_mean = 0.9\n        p_levy = 0.06  # probability of long Lévy jump mutation\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        min_trust = 1e-6\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        no_improve_since = 0\n\n        gen = 0\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            successful_F = []\n            successful_CR = []\n\n            # per-generation temporary counters\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample individual control parameters Fi, CRi (jDE-like but stochastic)\n                if rng.rand() < 0.1:\n                    Fi = rng.uniform(0.4, 0.95)\n                else:\n                    Fi = np.clip(rng.normal(loc=F_mean, scale=0.1), 0.05, 0.99)\n                CRi = np.clip(rng.normal(loc=CR_mean, scale=0.15), 0.0, 1.0)\n\n                # mutation: DE/rand/1-like\n                idxs = np.arange(self.pop_size)\n                idxs = idxs[idxs != i]\n                if idxs.size < 3:\n                    # trivial perturbation if tiny population\n                    donor = pop[i] + Fi * (rng.randn(self.dim) * (range_vec / np.maximum(range_norm, 1e-12)))\n                else:\n                    r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # occasional Lévy-centered exploration: jump centered on best with heavy tail\n                if rng.rand() < p_levy:\n                    # generate truncated Cauchy (simple Lévy proxy)\n                    lev = rng.standard_cauchy(self.dim)\n                    # limit extremes\n                    lev = np.clip(lev, -10, 10)\n                    # anisotropic scaling: modulate by range_vec and current trust radius\n                    scale = (0.05 + rng.rand(self.dim) * 0.25) * (trust_radius / np.maximum(range_norm, 1e-12))\n                    levy_step = lev * scale * range_vec\n                    # center some jumps on global best for exploration\n                    donor = best_x + levy_step\n\n                # binomial crossover (ensure at least one gene from donor)\n                trial = pop[i].copy()\n                cr_mask = rng.rand(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[rng.randint(0, self.dim)] = True\n                trial[cr_mask] = donor[cr_mask]\n\n                # project to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(trial))\n                evals += 1\n\n                # selection\n                if f_candidate < fvals[i]:\n                    pop[i] = trial\n                    fvals[i] = f_candidate\n                    successes += 1\n                    successful_F.append(Fi)\n                    successful_CR.append(CRi)\n                    # nudge parameter means toward successful values\n                    F_mean = 0.92 * F_mean + 0.08 * Fi\n                    CR_mean = 0.92 * CR_mean + 0.08 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = trial.copy()\n                    stagnation_counter = 0\n                    no_improve_since = 0\n                else:\n                    no_improve_since += 1\n\n            # end of generation adaptation\n            # If there were successful Fi, update F_mean with Lehmer-like preference (squared weighted)\n            if len(successful_F) > 0:\n                sf = np.array(successful_F)\n                # Lehmer mean to favor larger F if they helped\n                num = np.sum(sf**2)\n                den = np.sum(sf) + 1e-12\n                F_mean = np.clip(0.85 * F_mean + 0.15 * (num / den), 0.05, 0.99)\n                CR_mean = np.clip(0.9 * CR_mean + 0.1 * (np.mean(successful_CR)), 0.0, 1.0)\n\n            # trust-region local search around best: small anisotropic Gaussian sampling\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # decide number of local samples: small handful, but not exceed budget\n            local_samples = min(max(1, self.dim // 2), remaining)\n            made_local_improve = False\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma per-dimension\n                sigma = (0.25 + rng.rand(self.dim) * 0.75) * (trust_radius / np.maximum(range_norm, 1e-12))\n                candidate = best_x + rng.randn(self.dim) * sigma\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, 0.7 * trust_radius)\n                    stagnation_counter = 0\n                    no_improve_since = 0\n                    made_local_improve = True\n                else:\n                    # unsuccessful local step => slightly expand trust to escape traps\n                    trust_radius = min(max_trust, 1.05 * trust_radius)\n                    stagnation_counter += 1\n                    no_improve_since += 1\n\n            # adapt exploration intensity based on recent success\n            if successes > 0:\n                # reduce chance of long jumps when successes occur\n                p_levy = max(0.01, p_levy * 0.92)\n                # slightly tighten trust region on success tendency\n                trust_radius = max(min_trust, 0.98 * trust_radius)\n            else:\n                # if no successes, increase chance of Lévy jumps up to a cap\n                p_levy = min(0.5, p_levy * 1.08 + 0.005)\n                trust_radius = min(max_trust, trust_radius * 1.03)\n\n            # stagnation handling: partial reseed if no improvements for long\n            if no_improve_since > max(50, self.dim * 5):\n                # reinitialize half population (except the best) if budget allows\n                k = max(1, self.pop_size // 2)\n                candidates_idx = np.setdiff1d(np.arange(self.pop_size), np.array([np.argmin(fvals)]))\n                if candidates_idx.size > 0:\n                    select_k = min(k, candidates_idx.size)\n                    replace_idx = rng.choice(candidates_idx, select_k, replace=False)\n                    for j in replace_idx:\n                        if evals >= self.budget:\n                            break\n                        pop[j] = rng.rand(self.dim) * range_vec + lb\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                        # update best if needed\n                        if fvals[j] < best_f:\n                            best_f = fvals[j]\n                            best_x = pop[j].copy()\n                    # after reset slightly enlarge trust radius to allow more exploration\n                    trust_radius = min(max_trust, trust_radius * 1.2)\n                no_improve_since = 0\n\n            # safety stop if nothing left\n            if evals >= self.budget:\n                break\n\n            # small random perturbation of F_mean/CR_mean to avoid premature convergence\n            F_mean = np.clip(F_mean * (1.0 + (rng.rand() - 0.5) * 0.02), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + (rng.rand() - 0.5) * 0.02, 0.0, 1.0)\n\n        # final results\n        self.x_opt = best_x.copy()\n        self.f_opt = float(best_f)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.360 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11479736142525099, 0.15815052669544616, 0.3309179245671131, 0.8811111988268722, 0.2116529335154066, 0.6778946361146849, 0.21648074837914244, 0.2126619667757882, 0.6320380095064009, 0.1673291037248027]}, "task_prompt": ""}
{"id": "53f94d86-e375-4b9f-8966-f68ddebb5d2f", "fitness": 0.3557374418926601, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight global jumps, and a trust-region local search with online parameter adaptation and stagnation recovery.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n    - Differential Evolution (rand/1/bin variant) with per-individual Fi/CRi sampled\n      around online-adapted means (jDE-like).\n    - Occasional Lévy-like Cauchy jumps centered on the global best for long-range exploration.\n    - Periodic trust-region Gaussian local search around the best, adapting the trust radius.\n    - Stagnation detection and partial population re-seeding to restore diversity.\n    Designed for continuous bounded optimization; uses exactly self.budget evaluations.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population size heuristic: scale with dimension but not exceed budget/4\n        if pop_size is None:\n            base = max(6, 4 + 2 * self.dim)\n            self.pop_size = int(min(base, max(4, self.budget // 8)))\n        else:\n            self.pop_size = int(pop_size)\n        # RNG for reproducibility\n        self.rng = np.random.RandomState(seed)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # adaptive DE parameter means\n        self.F_mean = 0.6\n        self.CR_mean = 0.3\n\n        # base trust region (fraction of variable range); will be scaled by range\n        self.base_trust = 0.25\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n        var_range = ub - lb\n        # initialize population uniformly in bounds\n        pop = rng.rand(self.pop_size, self.dim) * var_range + lb\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # Evaluate as many initial individuals as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            fvals[i] = float(func(x))\n            evals += 1\n\n        # If no evaluations possible, return defaults\n        valid = np.isfinite(fvals)\n        if not valid.any():\n            self.f_opt = float(np.inf)\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = int(np.nanargmin(np.where(valid, fvals, np.inf)))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # dynamic trust radius (in absolute variable units)\n        trust_radius = self.base_trust * np.mean(var_range)\n\n        # helper: generate Levy-like heavy-tailed step using standard Cauchy\n        def levy_step(scale=1.0):\n            # Cauchy draws from standard; scale and clip extremes for stability\n            step = rng.standard_cauchy(self.dim)\n            # clip extreme outliers\n            step = np.clip(step, -10.0, 10.0)\n            return step * float(scale)\n\n        stagnation_counter = 0\n        global_success_counter = 0\n\n        # main loop: iterate generations until budget exhausted\n        generation = 0\n        # controls\n        max_stagnation = max(50, 5 * self.pop_size)\n        while evals < self.budget:\n            generation += 1\n            gen_successes = 0\n            success_F = []\n            success_CR = []\n\n            # per generation randomization of F and CR: sample per individual later\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # choose mutation parameters for this individual\n                Fi = np.clip(rng.normal(self.F_mean, 0.15), 0.05, 0.99)\n                CRi = np.clip(rng.normal(self.CR_mean, 0.2), 0.0, 1.0)\n\n                # choose indices for DE/rand/1\n                idxs = np.arange(self.pop_size)\n                idxs = idxs[idxs != i]\n                if idxs.size < 3:\n                    # fallback: random new sample\n                    donor = rng.rand(self.dim) * var_range + lb\n                else:\n                    r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # occasional Levy long jump centered on best: probability increases with stagnation\n                p_levy = np.clip(0.04 + (stagnation_counter / (max_stagnation * 4)), 0.04, 0.45)\n                if rng.rand() < p_levy:\n                    # add a Lévy-like perturbation scaled by trust_radius and problem range\n                    lstep = levy_step(scale=2.0) * (trust_radius / (1.0 + np.mean(var_range)))\n                    donor = best_x + lstep * var_range\n                # binomial crossover\n                cr_mask = rng.rand(self.dim) < CRi\n                # ensure at least one gene from donor\n                cr_mask[rng.randint(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate\n                f_trial = float(func(trial))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_trial < fvals[i]:\n                    pop[i] = trial\n                    fvals[i] = f_trial\n                    gen_successes += 1\n                    success_F.append(Fi)\n                    success_CR.append(CRi)\n\n                    # update global best if improved\n                    if f_trial < best_f:\n                        best_f = f_trial\n                        best_x = trial.copy()\n                        # successful local exploitation => shrink trust region to focus\n                        trust_radius *= 0.90\n                        # small floor\n                        trust_radius = max(trust_radius, 1e-9)\n                        stagnation_counter = 0\n                    else:\n                        # modest reduction for successful but not improving global best\n                        trust_radius *= 0.98\n                else:\n                    # unsuccessful => slightly expand to encourage exploration\n                    trust_radius *= 1.005\n\n                # Update individual's evaluation result if we replaced\n                # move parameter means slowly toward successful Fi/CRi later in batch\n\n                # break if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # End of generation adjustments\n            # Update F_mean and CR_mean based on successes (Lehmer-like weighting)\n            if success_F:\n                # weighted average giving more weight to better Fi (simple mean here)\n                newF = float(np.mean(success_F))\n                # smooth update\n                self.F_mean = 0.88 * self.F_mean + 0.12 * newF\n            else:\n                # small drift to increase exploration if no successes\n                self.F_mean = np.clip(self.F_mean * 1.01, 0.05, 0.99)\n\n            if success_CR:\n                newCR = float(np.mean(success_CR))\n                self.CR_mean = 0.88 * self.CR_mean + 0.12 * newCR\n            else:\n                # nudge CR toward 0.5 if no successes\n                self.CR_mean = np.clip(0.98 * self.CR_mean + 0.02 * 0.5, 0.0, 1.0)\n\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # number of local samples: small handful scaled by dim and remaining budget\n            local_budget = min(remaining, max(1, int(min(6 + self.dim // 4, remaining))))\n            local_successes = 0\n            for _ in range(local_budget):\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                sigma = (trust_radius / max(1.0, np.mean(var_range))) * rng.rand(self.dim)\n                candidate = best_x + rng.normal(0.0, sigma, self.dim) * var_range\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_successes += 1\n                    # successful local step => shrink trust radius more\n                    trust_radius *= 0.85\n                    trust_radius = max(trust_radius, 1e-9)\n                    stagnation_counter = 0\n                else:\n                    trust_radius *= 1.01\n\n                if evals >= self.budget:\n                    break\n            # adjust F/CR means slightly toward exploitation if local successes found\n            if local_successes > 0:\n                self.F_mean = np.clip(0.95 * self.F_mean + 0.05 * 0.5, 0.05, 0.99)\n                self.CR_mean = np.clip(0.95 * self.CR_mean + 0.05 * 0.9, 0.0, 1.0)\n\n            # stagnation handling\n            if gen_successes + local_successes == 0:\n                stagnation_counter += 1\n                # increase chance of Lévy jumps by increasing F_mean a bit (more diverse mutations)\n                self.F_mean = np.clip(self.F_mean * 1.02, 0.05, 0.99)\n            else:\n                stagnation_counter = 0\n\n            # strong stagnation reset: if no improvement for many generations, re-seed part of population\n            if stagnation_counter > max_stagnation:\n                k = max(1, self.pop_size // 2)\n                replace_idx = rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in replace_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = rng.rand(self.dim) * var_range + lb\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                # enlarge trust region to escape traps\n                trust_radius *= 1.5\n                trust_radius = min(trust_radius, max(var_range) * 2.0)\n                stagnation_counter = 0\n\n            # global success bookkeeping\n            if best_f < self.f_opt:\n                self.f_opt = float(best_f)\n                self.x_opt = best_x.copy()\n\n            # safety clamp for means\n            self.F_mean = np.clip(self.F_mean, 0.05, 0.99)\n            self.CR_mean = np.clip(self.CR_mean, 0.0, 1.0)\n\n        # final results\n        # ensure final best reflects current fvals if necessary\n        if self.x_opt is None:\n            # pick best known\n            idx = int(np.nanargmin(fvals))\n            self.f_opt = float(fvals[idx])\n            self.x_opt = pop[idx].copy()\n        else:\n            self.f_opt = float(self.f_opt)\n            self.x_opt = self.x_opt.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.356 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15790153703513676, 0.2389882105705644, 0.3817061253540469, 0.42774189751506897, 0.4852897583737623, 0.271858151134325, 0.333135158473, 0.36100318711166424, 0.2974117514198834, 0.6023386419391484]}, "task_prompt": ""}
{"id": "06d2ccaf-8804-447d-89d3-a0df2ab458b9", "fitness": 0.4798030339737006, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range escapes, and a trust-region local search with online step-size adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # sensible default population scaling with dimension\n        if pop_size is None:\n            self.pop_size = int(np.clip(6 * self.dim, 10, max(10, self.budget // 10)))\n        else:\n            self.pop_size = int(pop_size)\n\n        # state to return\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"\n        Ensure bound is an array of length self.dim. Broadcast scalars or\n        single-values to arrays.\n        \"\"\"\n        b_arr = np.asarray(b, dtype=float)\n        if b_arr.size == 1:\n            return np.full(self.dim, float(b_arr))\n        if b_arr.size == self.dim:\n            return b_arr.astype(float)\n        # try to broadcast to (dim,)\n        try:\n            return np.broadcast_to(b_arr, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds could not be interpreted for dimension {}\".format(self.dim))\n\n    def __call__(self, func):\n        # prepare bounds\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # defensive non-zero\n        range_vec = np.where(range_vec == 0.0, 1e-12, range_vec)\n\n        # RNG\n        rng = self.rng\n\n        # initialize population uniformly\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # evaluate initial population carefully with budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            xi = pop[i].copy()\n            fvals[i] = float(func(xi))\n            evals += 1\n\n        # If budget exhausted prematurely, set best from evaluated individuals and return\n        if evals == 0:\n            # no evaluations possible\n            self.f_opt = np.inf\n            self.x_opt = pop[0]\n            return self.f_opt, self.x_opt\n\n        # determine initial best among evaluated\n        valid_idx = np.where(np.isfinite(fvals))[0]\n        if valid_idx.size == 0:\n            # nothing evaluated successfully; return random\n            self.f_opt = np.inf\n            self.x_opt = pop[0]\n            return self.f_opt, self.x_opt\n\n        best_idx = valid_idx[np.argmin(fvals[valid_idx])]\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        # Hyper-parameters and their adaptive summaries\n        F_mean = 0.6\n        CR_mean = 0.3\n        p_levy = 0.08              # initial probability of doing a Lévy jump mutation\n        step_scale = 0.25         # base scale multiplier for levy / trust\n        trust_radius = 0.2 * np.linalg.norm(range_vec)\n        min_trust = 1e-6\n        max_trust = 10.0 * np.linalg.norm(range_vec)\n\n        stagnation_counter = 0\n        stagnation_threshold = max(50, int(2 * self.dim))\n        generation = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple)\n        def levy_step(beta=1.0):\n            # Cauchy-like heavy tail using standard Cauchy scaled to dimension\n            # produces vector of shape (dim,)\n            # We clip extremes to avoid numerical issues\n            s = rng.standard_cauchy(self.dim) * 0.5  # moderate scaling\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop: iterate until budget exhausted\n        # we'll process population members sequentially in \"generations\"\n        while evals < self.budget:\n            generation += 1\n            successes = 0\n\n            # adapt per-generation parameter noise\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample F and CR for this individual (jDE-like sampling around means)\n                Fi = float(np.clip(rng.normal(F_mean, 0.15), 0.05, 0.95))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.2), 0.0, 1.0))\n\n                # randomly decide to perform a Lévy-based exploration mutation\n                do_levy = (rng.random() < p_levy)\n\n                # pick distinct indices for mutation\n                idxs = np.arange(self.pop_size)\n                # ensure we have enough unique indices\n                if self.pop_size < 4:\n                    # fall back to simple Gaussian mutation if tiny pop\n                    r1 = r2 = r3 = rng.integers(0, self.pop_size)\n                else:\n                    # choose r1,r2,r3 distinct from each other and i\n                    choices = np.setdiff1d(idxs, np.array([i], dtype=int))\n                    r1, r2, r3 = rng.choice(choices, 3, replace=False)\n\n                # base DE/rand/1 donor\n                donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                if do_levy:\n                    # center a heavy-tailed jump on the current best for occasional long-range exploration\n                    levy = levy_step()\n                    # scale by dynamic step_scale and range, and by trust radius influence\n                    levy_vec = levy * (step_scale * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))) * range_vec\n                    donor = best_x + levy_vec\n\n                # binomial crossover\n                cr_mask = rng.random(self.dim) < CRi\n                jrand = rng.integers(0, self.dim)\n                cr_mask[jrand] = True  # ensure at least one component from donor\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate trial if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(trial))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    # accept\n                    pop[i] = trial\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # nudge means toward successful Fi/CRi (small learning rate)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # when a Lévy mutation succeeded, reduce its probability a bit\n                    if do_levy:\n                        p_levy = max(0.01, p_levy * 0.9)\n                else:\n                    # unsuccessful -> slightly increase chance of Lévy jumps to escape\n                    p_levy = min(0.6, p_levy * 1.01 + 0.001)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = trial.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # avoid lingering references\n                try:\n                    del CRi\n                except UnboundLocalError:\n                    pass\n\n            # End of generation: trust-region local search around best\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # local_samples: attempt a modest number of local refinements\n            local_samples = int(min(max(2, self.dim // 2), remaining, max(1, self.pop_size // 4)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled per-dimension\n                # normalized by range magnitude to keep units consistent\n                norm_range = max(np.linalg.norm(range_vec), 1e-12)\n                sigma = (0.3 + rng.random(self.dim) * 0.7) * (trust_radius / norm_range)\n                candidate = best_x + rng.normal(0.0, sigma, self.dim) * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => tighten trust region to focus\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    # encourage exploitation by nudging CR_mean upward slightly\n                    CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n\n            # generation-level adaptation\n            if successes > 0:\n                # reward successful generations by slightly reducing p_levy (less exploration)\n                p_levy = max(0.01, p_levy * (0.98 ** successes))\n            else:\n                # if no successes, enlarge trust radius and increase chance of long jumps\n                trust_radius = min(max_trust, trust_radius * 1.08)\n                p_levy = min(0.8, p_levy * 1.08 + 0.01)\n\n                # stagnation handling: if stuck for long, re-seed part of the population\n                if stagnation_counter > stagnation_threshold:\n                    # reinitialize half of the population randomly (diversify)\n                    n_replace = max(1, self.pop_size // 2)\n                    replace_idx = rng.choice(np.arange(self.pop_size), n_replace, replace=False)\n                    for j in replace_idx:\n                        pop[j] = lb + rng.random(self.dim) * range_vec\n                        # evaluate re-seeded individuals if budget allows\n                        if evals >= self.budget:\n                            break\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                    # enlarge trust region to try broad search shortly after reset\n                    trust_radius = min(max_trust, trust_radius * 2.0)\n                    stagnation_counter = 0\n                    # slightly increase exploration probability\n                    p_levy = min(0.9, p_levy * 1.2)\n\n            # also keep the population's worst individuals nudged toward best to transfer knowledge\n            worst_idx = np.argmax(fvals)\n            if np.isfinite(fvals[worst_idx]) and np.isfinite(best_f):\n                # replace worst with slightly perturbed best if budget abundant\n                if evals < self.budget and rng.random() < 0.15:\n                    new_candidate = best_x + rng.normal(0, 0.05, self.dim) * range_vec\n                    new_candidate = np.minimum(np.maximum(new_candidate, lb), ub)\n                    f_new = float(func(new_candidate))\n                    evals += 1\n                    if f_new < fvals[worst_idx]:\n                        pop[worst_idx] = new_candidate\n                        fvals[worst_idx] = f_new\n                        if f_new < best_f:\n                            best_f = f_new\n                            best_x = new_candidate.copy()\n\n            # safeguard: if many evals done and best_x not currently in population, inject it\n            if not any(np.allclose(px, best_x) for px in pop):\n                # replace a random individual with best to keep search focused\n                j = rng.integers(0, self.pop_size)\n                pop[j] = best_x.copy()\n                fvals[j] = best_f\n\n            # quick exit if budget exhausted\n            if evals >= self.budget:\n                break\n\n        # final assignment\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.480 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17696445753897516, 0.208239162825828, 0.3960395103734736, 0.47996624033332, 0.7493640916868229, 0.7921802275320591, 0.3249851316243122, 0.4841504792625305, 0.7043161727271521, 0.48182486583253203]}, "task_prompt": ""}
{"id": "2aa159f2-df23-4e7d-b114-0d9147f3a94f", "fitness": 0.40081435804751225, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy jumps and a trust-region local search; online adapts step sizes and crossover to balance global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population size heuristic: between 6 and 20*dim but also not too large relative to budget\n        if pop_size is None:\n            self.pop_size = max(6, min(20 * self.dim, int(4 + 3 * np.sqrt(self.dim))))\n            # keep it reasonable relative to budget\n            self.pop_size = min(self.pop_size, max(6, self.budget // 5))\n        else:\n            self.pop_size = int(pop_size)\n\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        # prefer func.bounds if provided; otherwise assume [-5,5]^dim\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb))\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub))\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb, ub\n\n    def _levy_step(self):\n        # Simple heavy-tailed step using Cauchy (t distribution with df=1).\n        # Limit extremes to avoid numerical blow-ups.\n        # Returns a vector in R^dim.\n        s = self.rng.standard_cauchy(self.dim)\n        s = np.clip(s, -30.0, 30.0)  # clamp extremes\n        # occasionally allow a bit bigger jumps\n        if self.rng.random() < 0.01:\n            s *= 3.0\n        return s\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        range_vec = ub - lb\n        # ensure positive range\n        range_vec = np.maximum(range_vec, 1e-8)\n\n        evals = 0\n        rng = self.rng\n\n        # algorithm hyper-parameters (adaptive)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.5       # crossover mean\n        step_scale = 0.25   # base scale for Lévy and trust radius\n        trust_radius = 0.25  # relative to range_vec (0..1)\n        p_levy = 0.05       # probability to attempt Levy-centered exploration\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.random((self.pop_size, self.dim)) * range_vec\n        pop_f = np.full(self.pop_size, np.inf)\n\n        # evaluate as many as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            pop_f[i] = f\n            evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If we couldn't evaluate full population due to tiny budget, set remaining vectors but they won't be used\n        # (pop entries left with inf fitness)\n\n        stagnation_counter = 0\n        best_improved_since = 0\n\n        # indices array for selecting distinct individuals\n        idxs = np.arange(self.pop_size)\n\n        # main loop: iterate while budget remains\n        gen = 0\n        while evals < self.budget:\n            gen += 1\n            # Adaptive small perturbations to means\n            F_mean = np.clip(F_mean * (1.0 + 0.02 * (rng.random() - 0.5)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + 0.02 * (rng.random() - 0.5), 0.0, 1.0)\n\n            # Process population sequentially\n            order = rng.permutation(self.pop_size)\n            for ii in order:\n                if evals >= self.budget:\n                    break\n\n                # if this individual wasn't evaluated initially and pop_f is inf, treat it as candidate to evaluate\n                target = pop[ii]\n\n                # sample individual Fi and CRi (jDE-inspired simple adaptation)\n                Fi = np.clip(rng.normal(F_mean, 0.15), 0.05, 0.99)\n                CRi = np.clip(rng.normal(CR_mean, 0.2), 0.0, 1.0)\n\n                # choose mutation strategy probabilistically: DE/rand/1 with occasional best-guided or Levy jump\n                use_levy = (rng.random() < p_levy)\n                if use_levy and self.x_opt is not None:\n                    # Levy-centered exploration: jump relative to best\n                    s = self._levy_step()\n                    donor = self.x_opt + (step_scale * trust_radius) * s * range_vec\n                else:\n                    # DE/rand/1-like mutation but sometimes DE/best/1\n                    # pick r1,r2,r3 distinct from ii\n                    pick = idxs[idxs != ii]\n                    if pick.size < 3:\n                        # fallback: small gaussian mutation around target\n                        donor = target + Fi * rng.normal(0, 1.0, size=self.dim) * range_vec\n                    else:\n                        r = rng.choice(pick, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        if rng.random() < 0.3 and self.x_opt is not None:\n                            # mix best-guided\n                            donor = self.x_opt + Fi * (pop[r1] - pop[r2]) + 0.5 * Fi * (pop[r3] - target)\n                        else:\n                            donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # crossover (binomial)\n                cross_mask = rng.random(self.dim) < CRi\n                if not np.any(cross_mask):\n                    # ensure at least one gene from donor\n                    cross_mask[rng.integers(0, self.dim)] = True\n                trial = np.where(cross_mask, donor, target)\n\n                # Occasionally apply a small trust-region directed nudge toward best\n                if self.x_opt is not None and rng.random() < 0.1:\n                    nudge = (self.x_opt - trial) * (rng.random(self.dim) * 0.5)\n                    trial = trial + nudge\n\n                # Projection to bounds\n                trial = np.minimum(np.maximum(trial, lb), ub)\n\n                # evaluate trial if we still have budget\n                if evals >= self.budget:\n                    break\n                try:\n                    f_trial = func(trial)\n                except Exception:\n                    f_trial = np.inf\n                evals += 1\n\n                # selection: greedy replacement\n                if f_trial <= pop_f[ii]:\n                    # success\n                    pop[ii] = trial\n                    pop_f[ii] = f_trial\n                    # adapt means toward successful parameters\n                    F_mean = np.clip(0.9 * F_mean + 0.1 * Fi, 0.05, 0.99)\n                    CR_mean = np.clip(0.9 * CR_mean + 0.1 * CRi, 0.0, 1.0)\n                    # trust-radius shrink a bit on success (focus exploitation)\n                    trust_radius *= 0.98\n                    trust_radius = np.clip(trust_radius, 1e-6, 2.0)\n                    stagnation_counter = 0\n                    best_improved_since += 1\n                else:\n                    # failure: slightly increase exploration tendency\n                    trust_radius *= 1.005\n                    trust_radius = np.clip(trust_radius, 1e-6, 2.0)\n                    stagnation_counter += 1\n\n                # update global best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                    best_improved_since = 0\n                    stagnation_counter = 0\n\n            # End of generation: perform a small trust-region local search around current best\n            if evals >= self.budget:\n                break\n\n            # Determine number of local samples depending on remaining budget and dimension\n            remaining = self.budget - evals\n            # at least 1, typically small handful\n            n_local = int(np.clip(min(max(1, self.dim // 2), remaining, 6), 1, remaining))\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                if self.x_opt is None:\n                    break\n                # anisotropic sigma per-dimension\n                sigma = trust_radius * (0.3 + rng.random(self.dim) * 0.8)\n                candidate = self.x_opt + rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                try:\n                    f_cand = func(candidate)\n                except Exception:\n                    f_cand = np.inf\n                evals += 1\n\n                if f_cand < self.f_opt:\n                    # successful local step => shrink trust radius more\n                    self.f_opt = f_cand\n                    self.x_opt = candidate.copy()\n                    trust_radius *= 0.85\n                    trust_radius = np.clip(trust_radius, 1e-8, 2.0)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => slightly expand to escape local traps\n                    trust_radius *= 1.02\n                    trust_radius = np.clip(trust_radius, 1e-8, 2.0)\n                    stagnation_counter += 1\n\n            # Adjust the probability of Levy jumps depending on stagnation\n            if stagnation_counter > max(20, self.dim * 2):\n                p_levy = min(0.5, p_levy + 0.05)  # favor more long jumps\n                # slightly increase diversity in DE parameters\n                F_mean = np.clip(F_mean + 0.05, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean + 0.05, 0.0, 1.0)\n            else:\n                # slowly decay p_levy back toward baseline\n                p_levy = p_levy * 0.995 + 0.0005\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population randomly to diversify\n                n_reseed = max(1, self.pop_size // 2)\n                inds = rng.choice(self.pop_size, n_reseed, replace=False)\n                for ii in inds:\n                    if evals >= self.budget:\n                        break\n                    new_x = lb + rng.random(self.dim) * range_vec\n                    try:\n                        new_f = func(new_x)\n                    except Exception:\n                        new_f = np.inf\n                    evals += 1\n                    pop[ii] = new_x\n                    pop_f[ii] = new_f\n                    if new_f < self.f_opt:\n                        self.f_opt = new_f\n                        self.x_opt = new_x.copy()\n                        stagnation_counter = 0\n                # slightly enlarge trust radius after reset\n                trust_radius = min(1.0, trust_radius * 1.5)\n                p_levy = min(0.5, p_levy + 0.05)\n                # reduce stagnation counter a bit to avoid repeated resets\n                stagnation_counter = stagnation_counter // 2\n\n            # small dampening to means to avoid runaway\n            F_mean = np.clip(0.995 * F_mean + 0.005 * 0.6, 0.05, 0.99)\n            CR_mean = np.clip(0.995 * CR_mean + 0.005 * 0.5, 0.0, 1.0)\n\n        # final results\n        # ensure x_opt is set\n        if self.x_opt is None:\n            # try pick best from population\n            idx = np.argmin(pop_f)\n            if np.isfinite(pop_f[idx]):\n                self.x_opt = pop[idx].copy()\n                self.f_opt = float(pop_f[idx])\n            else:\n                # fallback: random point\n                x = lb + rng.random(self.dim) * range_vec\n                try:\n                    f = func(x)\n                except Exception:\n                    f = np.inf\n                self.x_opt = x\n                self.f_opt = f\n\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.401 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14152534735252387, 0.24177970036620666, 0.3958837194307723, 0.8375884079650334, 0.6721055409682053, 0.3348995893394151, 0.2683787406265431, 0.28708461226115756, 0.2658182111837635, 0.5630797109815024]}, "task_prompt": ""}
{"id": "49c0e9a2-ce19-4a6c-aab5-f9cd8fa8e04f", "fitness": "-inf", "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with jDE-style parameter adaptation, occasional Lévy (Cauchy) long jumps for exploration, and an online trust-region local search around the best; it balances fast global exploration with focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (F, CR, trust radius) are adapted online using success history.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None,\n                 levy_prob=0.08, init_F=0.6, init_CR=0.3,\n                 min_F=0.1, max_F=0.95, stagnation_reset=200):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.levy_prob = float(levy_prob)\n        self.init_F = float(init_F)\n        self.init_CR = float(init_CR)\n        self.min_F = float(min_F)\n        self.max_F = float(max_F)\n        # population sizing: reasonably scaled with dim and budget\n        if pop_size is None:\n            # try to keep population not larger than budget/5 and about 4-10*dim\n            guess = max(8, 4 * self.dim)\n            self.pop_size = int(min(guess, max(4, self.budget // 5)))\n        else:\n            self.pop_size = int(pop_size)\n        self.pop_size = max(4, self.pop_size)\n        self.stagnation_reset = int(stagnation_reset)\n\n        # results will be set when running\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_bounds(self, x, lb, ub):\n        # broadcast and clip\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def __call__(self, func):\n        # Obtain bounds (BBOB style uses func.bounds.lb/ub); fallback to [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure correct shapes\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        span[span == 0] = 1.0  # avoid zero range\n\n        # initialize population uniformly; evaluate as budget allows\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * span\n        pop_f = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # Evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = func(x)\n            evals += 1\n            pop_f[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If budget exhausted already, return best we have\n        if evals >= self.budget:\n            return self.f_opt, self.x_opt\n\n        # jDE-like adaptive means\n        F_mean = self.init_F\n        CR_mean = self.init_CR\n\n        # data for adaptation\n        successful_F = []\n        successful_CR = []\n\n        # trust-region radius (relative to span)\n        trust_radius = 0.2 * np.mean(span)  # initial trust radius\n        trust_min = 1e-8 * np.mean(span)\n        trust_max = np.max(span) * 2.0\n\n        # stagnation tracking\n        since_last_improve = 0\n        global_best_idx = int(np.argmin(pop_f))\n        self.f_opt = float(pop_f[global_best_idx])\n        self.x_opt = pop[global_best_idx].copy()\n        last_improve_eval_count = evals\n\n        # helper: levy-like step using Cauchy (heavy-tailed)\n        def levy_step(scale):\n            # Cauchy heavy tail, but cap extremes\n            step = self.rng.standard_cauchy(self.dim)\n            # cap to avoid numeric blow-ups\n            cap = 10.0\n            step = np.clip(step, -cap, cap)\n            return step * scale\n\n        # main loop: process population repeatedly until budget is reached\n        idx_cycle = 0\n        gen = 0\n        # We will adapt per-individual F_i and CR_i drawn around means (jDE style)\n        while evals < self.budget:\n            gen += 1\n            # randomize per-generation small jitter to means to encourage diversity\n            F_mean = np.clip(F_mean * (1.0 + 0.02 * (self.rng.rand() - 0.5)), self.min_F, self.max_F)\n            CR_mean = np.clip(CR_mean + 0.05 * (self.rng.rand() - 0.5), 0.0, 1.0)\n            # iterate through population indices\n            indices = np.arange(self.pop_size)\n            self.rng.shuffle(indices)\n            for i in indices:\n                if evals >= self.budget:\n                    break\n\n                x_target = pop[i].copy()\n\n                # draw Fi and CRi similar to jDE:\n                # Fi = F_mean with some gaussian jitter, clipped\n                Fi = F_mean + 0.1 * (self.rng.randn())\n                if Fi < self.min_F or Fi > self.max_F:\n                    Fi = self.min_F + self.rng.rand() * (self.max_F - self.min_F)\n                CRi = self.rng.rand() if self.rng.rand() < 0.5 else CR_mean  # mix random and mean\n\n                # Decide to try a Lévy-step guided mutation with some probability\n                use_levy_jump = (self.rng.rand() < self.levy_prob) or (since_last_improve > max(50, 5*self.dim))\n                if use_levy_jump:\n                    # target anchored near best to jump far\n                    base = self.x_opt + levy_step(scale=0.5 * np.mean(span))\n                    # small DE/rand/1 added to base\n                    idxs = [j for j in range(self.pop_size) if j != i]\n                    a, b, c = self.rng.choice(idxs, 3, replace=False)\n                    donor = base + Fi * (pop[a] - pop[b]) + 0.5 * Fi * (pop[b] - pop[c])\n                else:\n                    # classical DE/rand/1 or current-to-best guidance\n                    if self.rng.rand() < 0.2:\n                        # current-to-best guidance to intensify around best\n                        idxs = [j for j in range(self.pop_size) if j != i]\n                        a, b = self.rng.choice(idxs, 2, replace=False)\n                        donor = x_target + Fi * (self.x_opt - x_target) + Fi * (pop[a] - pop[b])\n                    else:\n                        idxs = [j for j in range(self.pop_size) if j != i]\n                        a, b, c = self.rng.choice(idxs, 3, replace=False)\n                        donor = pop[a] + Fi * (pop[b] - pop[c])\n\n                # crossover (binomial)\n                cross_mask = self.rng.rand(self.dim) < CRi\n                if not np.any(cross_mask):\n                    # ensure at least one dimension crosses\n                    cross_mask[self.rng.randint(self.dim)] = True\n                trial = np.where(cross_mask, donor, x_target)\n\n                # small trust-region perturbation occasionally to refine\n                if self.rng.rand() < 0.15:\n                    trial += (self.rng.randn(self.dim) * (trust_radius * self.rng.rand()))\n\n                # projection to bounds\n                trial = self._ensure_bounds(trial, lb, ub)\n\n                # evaluate trial if budget allows\n                if evals >= self.budget:\n                    break\n                f_trial = func(trial)\n                evals += 1\n\n                # selection (greedy)\n                if f_trial <= pop_f[i]:\n                    # success: replace\n                    pop[i] = trial\n                    pop_f[i] = f_trial\n                    successful_F.append(Fi)\n                    successful_CR.append(CRi)\n                    # update global best\n                    if f_trial < self.f_opt:\n                        self.f_opt = float(f_trial)\n                        self.x_opt = trial.copy()\n                        since_last_improve = 0\n                        last_improve_eval_count = evals\n                else:\n                    since_last_improve += 1\n\n                # periodic adaptation of means using successes (Aizenberg-like simple update)\n                if len(successful_F) >= max(3, self.pop_size // 6):\n                    # prefer larger F if successful\n                    new_F_mean = (0.9 * F_mean + 0.1 * (0.5 * (np.mean(successful_F) + np.sum(successful_F**2) / (np.sum(successful_F) + 1e-12))))\n                    F_mean = np.clip(new_F_mean, self.min_F, self.max_F)\n                    # CR mean as simple mean\n                    CR_mean = np.clip(0.9 * CR_mean + 0.1 * np.mean(successful_CR), 0.0, 1.0)\n                    successful_F.clear()\n                    successful_CR.clear()\n\n                # adjust trust radius a bit on successes/failures\n                if f_trial <= pop_f[i]:\n                    trust_radius = max(trust_min, trust_radius * 0.95)\n                else:\n                    trust_radius = min(trust_max, trust_radius * 1.02)\n\n                # stagnation handling\n                if since_last_improve > self.stagnation_reset:\n                    # partial re-seed: reinitialize half population (except best)\n                    k = max(1, self.pop_size // 2)\n                    replace_idx = self.rng.choice([j for j in range(self.pop_size) if j != global_best_idx], k, replace=False)\n                    for j in replace_idx:\n                        pop[j] = lb + self.rng.rand(self.dim) * span\n                        pop_f[j] = np.inf\n                    # evaluate some of them if budget allows (but not more than half remaining)\n                    reevals = min(len(replace_idx), max(0, (self.budget - evals) // 2))\n                    for jj in replace_idx[:reevals]:\n                        if evals >= self.budget:\n                            break\n                        pop_f[jj] = func(pop[jj])\n                        evals += 1\n                        if pop_f[jj] < self.f_opt:\n                            self.f_opt = float(pop_f[jj])\n                            self.x_opt = pop[jj].copy()\n                            since_last_improve = 0\n                            last_improve_eval_count = evals\n                    trust_radius = min(trust_max, trust_radius * 1.5)\n                    since_last_improve = 0  # reset stagnation counter after reseed\n                # update global best index for possible reseed exclusion\n                global_best_idx = int(np.argmin(pop_f))\n\n            # End of generation (after processing all indices or budget exhausted)\n            # trust-region local search around best: small Gaussian samples\n            if evals < self.budget:\n                rem = self.budget - evals\n                # decide how many local samples allowed; allow small handful scaled by dim and remaining budget\n                local_budget = min(rem, max(1, min(5 + self.dim // 2, rem)))\n                # anisotropic sigma per dimension\n                for _ in range(local_budget):\n                    # anisotropic gaussian: some dims bigger some smaller\n                    anis = self.rng.rand(self.dim) * 0.8 + 0.2\n                    sigma = (trust_radius / (1.0 + np.sqrt(self.dim))) * anis\n                    candidate = self.x_opt + self.rng.randn(self.dim) * sigma\n                    candidate = self._ensure_bounds(candidate, lb, ub)\n                    f_c = func(candidate)\n                    evals += 1\n                    if f_c < self.f_opt:\n                        # strong local improvement: shrink trust region\n                        self.f_opt = float(f_c)\n                        self.x_opt = candidate.copy()\n                        trust_radius = max(trust_min, trust_radius * 0.6)\n                        since_last_improve = 0\n                        last_improve_eval_count = evals\n                    else:\n                        # little expansion on failures to explore more broadly\n                        trust_radius = min(trust_max, trust_radius * 1.05)\n                        since_last_improve += 1\n                    if evals >= self.budget:\n                        break\n\n            # small adaptation based on generation success: if many improvements recently, lessen levy prob\n            if since_last_improve < max(10, self.dim):\n                self.levy_prob = max(0.01, self.levy_prob * 0.97)\n            else:\n                self.levy_prob = min(0.5, self.levy_prob * 1.03)\n\n            # final safety: if no evaluation improved after long time, enlarge F_mean and CR_mean slightly\n            if since_last_improve > 3 * self.dim:\n                F_mean = np.clip(F_mean * 1.05, self.min_F, self.max_F)\n                CR_mean = np.clip(CR_mean + 0.02, 0.0, 1.0)\n\n            # update global_best_idx again\n            global_best_idx = int(np.argmin(pop_f))\n\n        # Return final best\n        return float(self.f_opt), self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "In the code, line 195, in __call__, the following error occurred:\nTypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'\nOn line: new_F_mean = (0.9 * F_mean + 0.1 * (0.5 * (np.mean(successful_F) + np.sum(successful_F**2) / (np.sum(successful_F) + 1e-12))))", "error": "In the code, line 195, in __call__, the following error occurred:\nTypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'\nOn line: new_F_mean = (0.9 * F_mean + 0.1 * (0.5 * (np.mean(successful_F) + np.sum(successful_F**2) / (np.sum(successful_F) + 1e-12))))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "828f0ef9-448b-45aa-b16c-39aff9f54411", "fitness": 0.24428640067303822, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DE with occasional Lévy jumps and a trust-region local search — fast global exploration with focused local exploitation and online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension and budget but kept reasonable\n        if pop_size is None:\n            self.pop_size = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # make sure population not larger than a fraction of budget\n            self.pop_size = min(self.pop_size, max(4, self.budget // 20))\n        else:\n            self.pop_size = int(pop_size)\n        # internal state for results\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try to broadcast\n        return np.broadcast_to(b, (self.dim,)).astype(float)\n\n    def __call__(self, func):\n        # bounds from func (Many BBOB: typically -5..5)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        rng = self.rng\n\n        eps = 1e-12\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec) + eps\n\n        # initialize population uniformly in bounds\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population (careful with budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # if we didn't manage to evaluate full population due to tiny budget,\n        # set remaining individuals randomly without evaluation (they won't be used)\n        valid = np.isfinite(fvals)\n        if np.any(valid):\n            best_idx = np.argmin(fvals[valid])\n            # map to absolute index\n            best_indices = np.where(valid)[0]\n            best_idx = best_indices[best_idx]\n            best_x = pop[best_idx].copy()\n            best_f = fvals[best_idx]\n        else:\n            # no evals possible\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # initial differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # probability of a Lévy jump\n        trust_radius = 0.2 * range_norm  # scalar trust radius\n        min_trust = 1e-6\n        max_trust = range_norm * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step():\n            # limit extreme outliers to avoid numerical blow-up\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -1e3, 1e3)\n            # normalize typical magnitude to 1\n            s = s / (np.std(s) + 1e-12)\n            return s\n\n        # main loop: iterate generations until budget exhausted\n        # We'll process population members sequentially; each evaluated candidate consumes one eval.\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n\n            # adapt per-generation randomization of F and CR around their means\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # choose between Lévy exploration and DE mutation/exchange\n                if rng.rand() < p_levy:\n                    # Lévy jump centered on best for exploration\n                    step = levy_step()\n                    # scale of the jump relative to trust radius and overall range\n                    levy_scale = (0.5 + 1.5 * rng.rand()) * (trust_radius / (range_norm + eps))\n                    donor = best_x + levy_scale * step * (range_vec / (np.maximum(range_vec.mean(), eps)))\n                    Fi = None\n                    CRi = None\n                    # no crossover for pure Levy steps (already diverse)\n                    candidate = np.clip(donor, lb, ub)\n                else:\n                    # DE/rand/1-like mutation\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # can't do DE properly; fallback to small gaussian around current\n                        Fi = None\n                        CRi = None\n                        candidate = np.clip(pop[i] + 0.01 * rng.randn(self.dim) * range_vec, lb, ub)\n                    else:\n                        r1, r2, r3 = rng.choice(idxs, 3, replace=False)\n                        # adapt F and CR per individual (jDE-like perturbation)\n                        Fi = float(np.clip(rng.normal(F_mean, 0.1), 0.05, 1.0))\n                        CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                        # binomial crossover mask ensuring at least one gene from donor\n                        cr_mask = rng.rand(self.dim) < CRi\n                        cr_mask[rng.randint(0, self.dim)] = True\n                        trial = np.where(cr_mask, donor, pop[i])\n                        candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate (one evaluation)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement in population\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # move means slightly toward successful parameters (if available)\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # End of generation adjustments\n            # trust-region local search around best: sample a few candidates with Gaussian noise\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # sample count: small handful scaled by dim and remaining budget\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by rand per-dimension\n                sigma = (0.5 + rng.rand(self.dim) * 0.5) * (trust_radius / np.maximum(range_norm, eps))\n                candidate = best_x + rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    # successful local step => shrink trust radius to focus\n                    trust_radius *= 0.85\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful => expand a bit to escape local traps\n                    trust_radius *= 1.05\n                    trust_radius = max(min(trust_radius, max_trust), min_trust)\n                    stagnation_counter += 1\n\n            # parameter adaptation by generation performance\n            if successes > 0:\n                # encourage slight exploitation\n                F_mean = np.clip(0.95 * F_mean + 0.05 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.95 * CR_mean + 0.05 * 0.9, 0.0, 1.0)\n                p_levy = max(0.01, p_levy * 0.98)\n            else:\n                # stagnation handling: increase chance of long jumps and slightly increase diversity\n                p_levy = min(0.5, p_levy * 1.06 + 0.01)\n                F_mean = np.clip(F_mean * 1.02, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n\n            # strong stagnation reset: if no improvement for many evaluations, re-seed part of population\n            if stagnation_counter > max(50, self.dim * 5):\n                # reinitialize half of the population randomly to diversify\n                k = max(1, self.pop_size // 2)\n                re_idx = rng.choice(self.pop_size, k, replace=False)\n                for j in re_idx:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # slightly enlarge trust radius after reset\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n\n        # final results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.244 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12224666272218576, 0.16739867531849517, 0.2824827790157428, 0.4147404878251565, 0.22976846221539182, 0.39496976729182554, 0.24446949798455675, 0.22446484526339683, 0.20429699040112126, 0.1580258386925094]}, "task_prompt": ""}
{"id": "8ff57eb0-c572-464b-a008-e3d9d8461983", "fitness": 0.25740779581990364, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining DE (jDE-style), occasional Lévy (Cauchy) long jumps, and an adaptive trust-region local search with online parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE/rand/1-like with jDE-style\n    parameter adaptation), occasional Lévy-flight (approximated with truncated Cauchy)\n    long jumps for global exploration, and a small trust-region local search around the\n    current best. Parameters (F_mean, CR_mean, p_levy, trust_radius) are adapted online.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # sensible default population size if not provided\n        if pop_size is None:\n            # scale with dimension but keep in [4, 120] and limited by budget\n            approx = max(8, min(8 + int(2 * np.sqrt(self.dim)), 120))\n            approx = max(4, min(approx, self.budget // 10))\n            self.pop_size = approx\n        else:\n            self.pop_size = max(2, int(pop_size))\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, b.item(), dtype=float)\n        if b.size == self.dim:\n            return b.copy().astype(float)\n        # try broadcasting (e.g., shape (dim,1) or (1,dim))\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be broadcast to problem dimension\")\n\n    def __call__(self, func):\n        # read bounds from func (Many BBOB uses .bounds.lb/.ub or scalars)\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # avoid zero ranges\n        range_vec = np.where(range_vec <= 0, 1.0, range_vec)\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.random((self.pop_size, self.dim)) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            fvals[i] = float(func(pop[i]))\n            evals += 1\n\n        # determine best among evaluated\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # cannot evaluate anything\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        best_idx = int(np.nanargmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # hyper-parameters (adaptive)\n        F_mean = 0.6        # mean differential weight\n        CR_mean = 0.9       # mean crossover prob\n        p_levy = 0.08       # probability of performing a Lévy jump per candidate\n        step_scale = 0.25   # base scale for Lévy and trust radius multiplier\n        trust_radius = 0.25 * np.linalg.norm(range_vec)  # absolute scale\n        min_trust = 1e-8\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using truncated Cauchy\n        def levy_step(dim):\n            s = self.rng.standard_cauchy(dim)\n            # truncate extreme outliers to avoid numerical issues\n            s = np.clip(s, -1e3, 1e3)\n            return s\n\n        # main loop\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # perform one DE-like generation (sequential, safe for budget)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # decide exploration vs exploitation\n                if self.rng.random() < p_levy:\n                    # Lévy jump centered on current best (global exploration)\n                    s = levy_step(self.dim)\n                    # anisotropic scaling by problem range and trust radius\n                    scale = step_scale * (0.5 + self.rng.random() * 1.5)\n                    step = s * scale * (trust_radius / max(np.mean(range_vec), 1e-12))\n                    candidate = best_x + step\n                    # small gaussian smoothing\n                    candidate += self.rng.normal(0, 0.01 * np.linalg.norm(range_vec), size=self.dim)\n                    # mark that no Fi/CRi were used\n                    Fi = None\n                    CRi = None\n                else:\n                    # DE/rand/1 mutation + binomial crossover (jDE-like Fi/CRi sampling)\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback: random uniform candidate\n                        candidate = lb + self.rng.random(self.dim) * range_vec\n                        Fi = None\n                        CRi = None\n                    else:\n                        r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                        # sample Fi and CRi around means\n                        Fi = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 0.99)\n                        CRi = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # crossover\n                        cr_mask = self.rng.random(self.dim) < CRi\n                        # ensure at least one component from donor\n                        if not np.any(cr_mask):\n                            cr_mask[self.rng.integers(0, self.dim)] = True\n                        candidate = np.where(cr_mask, donor, pop[i])\n\n                # project to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection vs current individual\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # adapt means slightly toward successful Fi/CRi if available\n                    if Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * Fi\n                    if CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * CRi\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # small safeguard: keep means in bounds\n                F_mean = np.clip(F_mean, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean, 0.0, 1.0)\n\n            # trust-region local search around best (small number of samples)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # choose number of local samples conservatively\n            local_samples = min(max(1, self.dim // 2), remaining)\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic gaussian perturbation, smaller than trust_radius\n                sigma = trust_radius * (0.5 + self.rng.random(self.dim) * 0.5)\n                candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # shrink trust radius to focus exploitation\n                    trust_radius = max(min_trust, trust_radius * 0.7)\n                    stagnation_counter = 0\n                else:\n                    # expand slightly if no improvement locally\n                    trust_radius = min(max_trust, trust_radius * 1.02)\n                    stagnation_counter += 1\n\n            # adapt exploration probability and means based on successes\n            if successes + local_success > 0:\n                # more successes => slightly reduce heavy-tail exploration\n                if successes + local_success > max(1, self.pop_size * 0.15):\n                    p_levy = max(0.005, p_levy * 0.94)\n                else:\n                    p_levy = max(0.005, p_levy * 0.98)\n                # gently move means back toward exploitation-friendly defaults\n                F_mean = np.clip(0.96 * F_mean + 0.04 * 0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.96 * CR_mean + 0.04 * 0.9, 0.0, 1.0)\n            else:\n                # stagnation => increase chance of long jumps and increase mutation\n                p_levy = min(0.6, p_levy * 1.06 + 0.005)\n                F_mean = np.clip(F_mean * 1.03, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 0.98, 0.0, 1.0)\n                # enlarge trust radius moderately to escape\n                trust_radius = min(max_trust, trust_radius * 1.2)\n\n            # strong stagnation reset: re-seed half population if stuck for long\n            if stagnation_counter > max(50, self.dim * 6):\n                k = max(1, self.pop_size // 2)\n                indices = self.rng.choice(np.arange(self.pop_size), k, replace=False)\n                for j in indices:\n                    if evals >= self.budget:\n                        break\n                    pop[j] = lb + self.rng.random(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    # update best if new random point is good\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                trust_radius = min(max_trust, trust_radius * 1.8)\n                stagnation_counter = 0\n\n        # finalize results\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.257 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14358934085717334, 0.19208213669669194, 0.30940435911982644, 0.33576741041121994, 0.337666721113402, 0.31093859967934734, 0.24747270815438094, 0.3012506004530484, 0.23201685063956845, 0.1638892310743777]}, "task_prompt": ""}
{"id": "36f91ca9-48ce-40c8-b3f1-159479cc1c32", "fitness": 0.4239620775253353, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy jumps and a trust-region local search; online adapts mutation/crossover and leap frequency to balance global exploration and focused local exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (heavy-tailed) jumps for long-range exploration,\n    and a trust-region Gaussian local search around the current best.\n    Online adaptation of DE parameters and jump probability drives behavior.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # Default population size heuristics: scale with dim but bounded by budget\n        if pop_size is None:\n            # prefer moderate pop sizes; don't exceed a fraction of budget\n            suggested = max(6, 8 + 2 * self.dim)\n            self.pop_size = int(min(suggested, max(4, self.budget // 8)))\n        else:\n            self.pop_size = int(pop_size)\n        self.pop_size = max(4, min(self.pop_size, self.budget))  # ensure sensible bounds\n\n        # outputs\n        self.x_opt = None\n        self.f_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"Return a 1-D numpy array of length self.dim from bound scalar/array.\"\"\"\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting:\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds must be scalar or length-dimension arrays.\")\n\n    def __call__(self, func):\n        # get bounds robustly\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        # fallback if bounds are non-finite: Many BBOB uses [-5,5]\n        if not np.all(np.isfinite(lb)):\n            lb = np.full(self.dim, -5.0)\n        if not np.all(np.isfinite(ub)):\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        range_norm = np.linalg.norm(range_vec)\n        if range_norm == 0:\n            range_norm = 1.0\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population up to budget\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # identify best\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = float(fvals[best_idx])\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.08       # initial probability of a Lévy jump\n        step_scale = 0.25   # base scale multiplier for Levy and trust radius\n        trust_radius = 0.2 * range_norm  # scalar trust radius (in absolute space)\n        min_trust = 1e-6\n        max_trust = 0.5 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy normalized\n        def levy_step():\n            s = self.rng.standard_cauchy(self.dim)\n            # normalize to avoid runaway magnitudes while keeping heavy tails\n            s = np.clip(s, -1e6, 1e6)\n            norm = np.linalg.norm(s)\n            if norm == 0:\n                return s\n            return s / (norm / np.sqrt(self.dim))\n\n        # record small history for adaptation\n        recent_successes = []\n\n        # main loop: iterate generations until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            attempts = 0\n\n            # iterate over each individual (sequential DE style)\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n                attempts += 1\n\n                # decide mutation style\n                if self.rng.rand() < p_levy:\n                    # Lévy jump centered at best for exploration\n                    step = levy_step()\n                    # random scale near step_scale and scale by problem range\n                    scale = step_scale * (0.3 + 0.7 * self.rng.rand())\n                    candidate = best_x + scale * step * (range_vec / max(range_vec.mean(), 1e-12))\n                    # small gaussian around candidate for diversity\n                    candidate = candidate + self.rng.normal(0, 0.05, size=self.dim) * range_vec\n                    # clip to bounds\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    used_Fi = None\n                    used_CRi = None\n                else:\n                    # Differential Evolution /rand/1 + binomial crossover with per-individual Fi/CRi\n                    # pick three distinct indices different from i\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    if idxs.size < 3:\n                        # fallback to random perturbation if population too small\n                        candidate = pop[i] + 0.01 * self.rng.randn(self.dim) * range_vec\n                        candidate = np.minimum(np.maximum(candidate, lb), ub)\n                        used_Fi = None\n                        used_CRi = None\n                    else:\n                        r = self.rng.choice(idxs, 3, replace=False)\n                        r1, r2, r3 = r[0], r[1], r[2]\n                        # sample Fi and CRi around means (simple jDE-like)\n                        Fi = float(np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.0))\n                        CRi = float(np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0))\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # binomial crossover\n                        cr_mask = self.rng.rand(self.dim) < CRi\n                        if not np.any(cr_mask):\n                            # ensure at least one component from donor\n                            cr_mask[self.rng.randint(0, self.dim)] = True\n                        trial = np.where(cr_mask, donor, pop[i])\n                        candidate = np.minimum(np.maximum(trial, lb), ub)\n                        used_Fi = Fi\n                        used_CRi = CRi\n\n                # Evaluate candidate if still budget\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # greedy selection\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes += 1\n                    recent_successes.append((used_Fi, used_CRi))\n                    # adapt F_mean and CR_mean slightly toward successes (if Fi/CRi provided)\n                    if used_Fi is not None:\n                        F_mean = 0.9 * F_mean + 0.1 * used_Fi\n                    if used_CRi is not None:\n                        CR_mean = 0.9 * CR_mean + 0.1 * used_CRi\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # occasional local improvement using current-to-best style\n                if self.rng.rand() < 0.02 and evals < self.budget:\n                    # small local perturbation towards best\n                    direction = best_x - pop[i]\n                    candidate2 = pop[i] + 0.5 * self.rng.rand() * direction + 0.05 * self.rng.randn(self.dim) * range_vec\n                    candidate2 = np.minimum(np.maximum(candidate2, lb), ub)\n                    f_candidate2 = float(func(candidate2))\n                    evals += 1\n                    if f_candidate2 < fvals[i]:\n                        pop[i] = candidate2.copy()\n                        fvals[i] = f_candidate2\n                        successes += 1\n                        if f_candidate2 < best_f:\n                            best_f = f_candidate2\n                            best_x = candidate2.copy()\n                            stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n\n                # stop early if budget used up\n                if evals >= self.budget:\n                    break\n\n            # End of generation: adapt high-level parameters\n            # reduce p_levy if many successes (favor exploitation), increase if stagnating\n            succ_ratio = successes / max(1, attempts)\n            if succ_ratio > 0.15:\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                p_levy = min(0.5, p_levy * 1.02 + 0.005)\n\n            # gently nudge F_mean/CR_mean toward stable values to avoid drift\n            F_mean = float(np.clip(0.98 * F_mean + 0.02 * 0.6, 0.05, 0.99))\n            CR_mean = float(np.clip(0.98 * CR_mean + 0.02 * 0.9, 0.0, 1.0))\n\n            # small population-level diversity injection when stagnating\n            if stagnation_counter > max(20, self.dim * 2):\n                # re-seed a fraction of worst individuals\n                k = max(1, int(0.25 * self.pop_size))\n                worst_idx = np.argsort(-fvals)[:k]\n                for j in worst_idx:\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = np.inf\n                    # optionally evaluate one re-seeded individual if budget allows\n                    if evals < self.budget:\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                        if fvals[j] < best_f:\n                            best_f = fvals[j]\n                            best_x = pop[j].copy()\n                            stagnation_counter = 0\n                # slightly increase trust radius to promote exploration\n                trust_radius = min(max_trust, trust_radius * 1.2)\n                stagnation_counter = 0\n\n            # Trust-region local search around best_x: few focused gaussian probes\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled relative to problem range\n                sigma = (0.3 + 0.7 * self.rng.rand(self.dim)) * (trust_radius / max(range_norm, 1e-12))\n                candidate = best_x + self.rng.normal(0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.minimum(np.maximum(candidate, lb), ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    # on success, shrink trust radius to focus the search\n                    trust_radius = max(min_trust, trust_radius * 0.85)\n                    stagnation_counter = 0\n                else:\n                    # on failure expand slightly to explore a bit further\n                    trust_radius = min(max_trust, trust_radius * 1.06)\n                    stagnation_counter += 1\n\n            # adjust p_levy depending on local success\n            if local_success > 0:\n                p_levy = max(0.01, p_levy * 0.95)\n            else:\n                p_levy = min(0.6, p_levy * 1.03 + 0.002)\n\n            # Clip trust radius into sensible bounds\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # keep some memory: if many recent successes with specific Fi/CRi, bias means\n            if recent_successes:\n                Fi_vals = [s[0] for s in recent_successes if s[0] is not None]\n                CR_vals = [s[1] for s in recent_successes if s[1] is not None]\n                if Fi_vals:\n                    F_mean = float(np.clip(0.9 * F_mean + 0.1 * (np.mean(Fi_vals)), 0.05, 0.99))\n                if CR_vals:\n                    CR_mean = float(np.clip(0.9 * CR_mean + 0.1 * (np.mean(CR_vals)), 0.0, 1.0))\n            # decay history\n            if len(recent_successes) > 200:\n                recent_successes = recent_successes[-200:]\n\n        # final outputs\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.424 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.22960826017173808, 0.20543655396128602, 0.4096984056131028, 0.864976239383328, 0.550971955947552, 0.6100564075192965, 0.37538389624516666, 0.44153532846801946, 0.33813511350315706, 0.2138186144407065]}, "task_prompt": ""}
{"id": "6f42ab13-1bf7-46a4-9534-0a080dc7493a", "fitness": 0.2735379463286377, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight jumps for long-range exploration, and an online trust-region local search with simple self-adaptation of DE parameters.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n\n    Usage: instantiate with budget and dim, then call with a black-box `func`\n    that obeys func.bounds.lb and func.bounds.ub (scalars or arrays).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # default population: scaled with dim but bounded by budget\n        if pop_size is None:\n            self.pop_size = int(min(max(8, 4 * self.dim), max(4, self.budget // 10)))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.RandomState(seed)\n        # final outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b.item()), dtype=float)\n        if b.size == self.dim:\n            return b.astype(float)\n        # try broadcasting to dim\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds cannot be converted to an array of size dim\")\n\n    def __call__(self, func):\n        # Prepare bounds and ranges\n        lb = self._ensure_array_bounds(func.bounds.lb)\n        ub = self._ensure_array_bounds(func.bounds.ub)\n        range_vec = ub - lb\n        # guard against degenerate ranges\n        range_norm = max(np.linalg.norm(range_vec), 1e-12)\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        evals = 0\n        # evaluate initial population sequentially but stop if budget exhausted\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # If no evaluations possible, return\n        if not np.isfinite(fvals).any():\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # best-known\n        best_idx = int(np.nanargmin(fvals))\n        best_f = float(fvals[best_idx])\n        best_x = pop[best_idx].copy()\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6       # mutation factor mean\n        CR_mean = 0.9      # crossover probability mean\n        p_levy = 0.08      # probability of using a Lévy-based long jump\n        trust_radius = 0.2 * range_norm\n        min_trust = 1e-6 * range_norm\n        max_trust = 2.0 * range_norm\n\n        stagnation_counter = 0\n        gen = 0\n\n        # helper: generate Levy-like heavy-tailed step (Cauchy approximation)\n        def levy_step(dim=self.dim):\n            # Cauchy via tan(pi*(u-0.5)) - heavy tails, median-scaled\n            u = self.rng.rand(dim)\n            step = np.tan(np.pi * (u - 0.5))\n            # clamp extreme outliers to avoid numerical blow-up\n            step = np.clip(step, -1e3, 1e3)\n            # normalize scale to unit typical magnitude\n            med = np.median(np.abs(step)) + 1e-12\n            return step / med\n\n        # main loop until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            previous_best_f = best_f\n\n            # per-generation small jitter to means (keeps diversity)\n            F_mean = np.clip(F_mean * (1.0 + 0.02 * (self.rng.rand() - 0.5)), 0.05, 0.99)\n            CR_mean = np.clip(CR_mean + 0.05 * (self.rng.rand() - 0.5), 0.0, 1.0)\n\n            # iterate targets\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # jDE-like self-adaptation (simple)\n                Fi = np.clip(self.rng.normal(F_mean, 0.12), 0.01, 1.0)\n                CRi = np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0)\n\n                # mutation: rand/1\n                idxs = np.arange(self.pop_size)\n                idxs = idxs[idxs != i]\n                if idxs.size < 3:\n                    # fallback: small perturbation around current\n                    r1 = r2 = r3 = i\n                else:\n                    r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n\n                donor_DE = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                # occasionally do a Lévy-centered jump near best (exploration)\n                if self.rng.rand() < p_levy:\n                    lev = levy_step(self.dim)\n                    # combine DE donor and levy step around best to get a diverse donor\n                    levy_scale = 0.5 * (0.5 + self.rng.rand())\n                    donor = best_x + levy_scale * (lev * (range_vec / max(range_vec.mean(), 1e-12))) \\\n                            + 0.5 * (donor_DE - pop[i])\n                else:\n                    donor = donor_DE\n\n                # binomial crossover\n                cr_mask = self.rng.rand(self.dim) < CRi\n                if not np.any(cr_mask):\n                    cr_mask[self.rng.randint(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, pop[i])\n\n                # project to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection and adaptation\n                if f_candidate < fvals[i]:\n                    # successful replacement\n                    pop[i] = candidate\n                    fvals[i] = f_candidate\n                    successes += 1\n                    # nudge means slightly toward successful Fi/CRi\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                # else: keep old individual\n\n                # update global best if improved\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                # early exit if budget exhausted\n                if evals >= self.budget:\n                    break\n\n            # end of generation: adapt exploration probabilities and trust radius\n            remaining = self.budget - evals\n\n            # more successes -> reduce chance of heavy jumps, else increase slowly\n            if successes > 0:\n                p_levy = max(0.01, p_levy * (0.97 if successes > max(1, 0.15 * self.pop_size) else 0.99))\n                # shrink trust region moderately if progress\n                trust_radius = max(min_trust, trust_radius * (0.92 if successes > 0 else 1.02))\n            else:\n                p_levy = min(0.5, p_levy * 1.05)\n                # little expansion to escape stagnation\n                trust_radius = min(max_trust, trust_radius * 1.08)\n\n            # trust-region local search around best: cheap Gaussian sampling when budget allows\n            if remaining > 0:\n                local_samples = int(min(5, remaining, max(1, 3 + self.dim // 10)))\n                # anisotropic scaling factor relative to bounds\n                local_scale = (trust_radius / (range_norm + 1e-12)) * (range_vec + 1e-12)\n                for _ in range(local_samples):\n                    if evals >= self.budget:\n                        break\n                    # Gaussian candidate around best with current trust radius\n                    candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * (local_scale)\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        # successful local step => shrink trust-radius to focus search\n                        trust_radius = max(min_trust, trust_radius * 0.85)\n                        stagnation_counter = 0\n                    else:\n                        # unsuccessful => slight expansion to encourage escape\n                        trust_radius = min(max_trust, trust_radius * 1.02)\n                        stagnation_counter += 1\n\n            # stagnation handling: occasional partial re-seed if no improvement for long\n            if stagnation_counter > max(50, 5 * self.pop_size):\n                # reinitialize roughly half of population (but don't exceed budget when evaluating them)\n                k = max(1, self.pop_size // 2)\n                reinit_idxs = self.rng.choice(np.arange(self.pop_size), k, replace=False)\n                # reinitialize these individuals (un-evaluated => set inf first)\n                for j in reinit_idxs:\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = np.inf\n                # evaluate a small subset immediately to refresh population (bounded by remaining budget)\n                to_eval = min(len(reinit_idxs), self.budget - evals)\n                if to_eval > 0:\n                    chosen = self.rng.choice(reinit_idxs, to_eval, replace=False)\n                    for j in chosen:\n                        if evals >= self.budget:\n                            break\n                        fvals[j] = float(func(pop[j]))\n                        evals += 1\n                        if fvals[j] < best_f:\n                            best_f = fvals[j]\n                            best_x = pop[j].copy()\n                # enlarge trust radius a bit\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n                # slightly increase exploration probability\n                p_levy = min(0.5, p_levy * 1.2)\n                # mild reset of means\n                F_mean = np.clip(0.6, 0.05, 0.99)\n                CR_mean = np.clip(0.9, 0.0, 1.0)\n\n            # early stop if no improvement for a long time and budget low: concentrate local search\n            if remaining <= max(0, int(0.02 * self.budget)) and stagnation_counter > max(100, 10 * self.dim):\n                # use remaining budget for local exploitation\n                while evals < self.budget:\n                    candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * (trust_radius / (range_norm + 1e-12)) * range_vec\n                    candidate = np.clip(candidate, lb, ub)\n                    f_candidate = float(func(candidate))\n                    evals += 1\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        trust_radius = max(min_trust, trust_radius * 0.9)\n                        stagnation_counter = 0\n                    else:\n                        trust_radius = min(max_trust, trust_radius * 1.05)\n                        stagnation_counter += 1\n\n        # finalize\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.274 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11938237999097912, 0.1776361405919149, 0.34885881282703335, 0.37207781811493423, 0.346344347025913, 0.23959110452545762, 0.2908212144077852, 0.30737675199654657, 0.33078672505622086, 0.20250416874959176]}, "task_prompt": ""}
{"id": "60a4bb91-e38d-4f6f-86af-794a9f5bf83b", "fitness": 0.466097772502844, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online parameter adaptation for fast exploration and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on recent success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size: scaled with dimension but limited by budget\n        if pop_size is None:\n            ps = max(4, min(50, 8 * self.dim))\n            # ensure we don't ask for more individuals than budget allows (keep room for iterations)\n            ps = min(ps, max(4, self.budget // 5))\n            self.pop_size = int(ps)\n        else:\n            self.pop_size = int(pop_size)\n            if self.pop_size < 4:\n                self.pop_size = 4\n        self.rng = np.random.RandomState(seed)\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # obtain bounds if provided by the func object, else assume [-5,5]^dim\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item(), dtype=float)\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item(), dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # enforce dimensionality on bounds\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        range_vec = ub - lb\n        mean_range = max(range_vec.mean(), 1e-12)\n        max_trust = np.linalg.norm(range_vec) * 2.0\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate initial population sequentially (respect budget)\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            try:\n                f = float(func(pop[i].copy()))\n            except Exception:\n                f = float(np.inf)\n            fvals[i] = f\n            evals += 1\n\n        # if no evaluations possible, return\n        if evals == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            return self.f_opt, self.x_opt\n\n        # best-so-far\n        finite_mask = np.isfinite(fvals)\n        if finite_mask.any():\n            best_idx = int(np.nanargmin(fvals))\n            best_x = pop[best_idx].copy()\n            best_f = float(fvals[best_idx])\n        else:\n            # if none evaluated, choose random\n            best_x = pop[0].copy()\n            best_f = float(np.inf)\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6        # differential weight mean\n        CR_mean = 0.9       # crossover probability mean\n        p_levy = 0.07       # probability of using a Lévy jump instead of DE\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # initial trust radius (absolute)\n        stagnation_counter = 0\n        gen = 0\n\n        # bookkeeping for adaptation\n        recent_successes = 0\n        recent_trials = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy approximation\n        def levy_step(scale=1.0):\n            # Cauchy distribution provides a heavy tail similar to a Levy step.\n            # Draw per-dimension, but cap extremes to avoid numerical explosion.\n            s = self.rng.standard_cauchy(self.dim) * scale\n            # cap extreme outliers\n            s = np.clip(s, -10.0, 10.0)\n            return s\n\n        # Main loop: generations until budget exhausted\n        # We'll process population members sequentially; each evaluated candidate consumes one eval.\n        while evals < self.budget:\n            gen += 1\n            successes_this_gen = 0\n            trials_this_gen = 0\n\n            # for each target vector in population, produce a trial\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # adapt per-individual F_i and CR_i (jDE-like)\n                Fi = float(np.clip(self.rng.normal(F_mean, 0.12), 0.05, 1.0))\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.12), 0.0, 1.0))\n\n                # decide whether to do Levy exploration centered on best\n                if self.rng.rand() < p_levy:\n                    # Lévy long jump: centered on best, scaled by trust radius and global range\n                    step_scale = 0.5 + self.rng.rand()  # random scale in [0.5,1.5)\n                    s = levy_step(scale=0.5)  # moderate scale\n                    donor = best_x + step_scale * s * (range_vec / mean_range) * (trust_radius / max_trust + 0.1)\n                    # small chance to mix with a randomly chosen individual for diversity\n                    if self.rng.rand() < 0.3:\n                        r = self.rng.randint(0, self.pop_size)\n                        donor = 0.5 * donor + 0.5 * pop[r]\n                    # ensure donor within bounds\n                    candidate = np.clip(donor, lb, ub)\n                    branch = 'levy'\n                else:\n                    # DE/rand/1 mutation with binomial crossover\n                    # pick three distinct indices != i\n                    idxs = list(range(self.pop_size))\n                    if self.pop_size <= 3:\n                        # fallback: use random perturbation of current individual\n                        donor = pop[i] + Fi * (self.rng.randn(self.dim) * range_vec / mean_range)\n                    else:\n                        idxs.remove(i)\n                        r1, r2, r3 = self.rng.choice(idxs, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                    # binomial crossover\n                    cr_mask = self.rng.rand(self.dim) < CRi\n                    # ensure at least one gene from donor\n                    cr_mask[self.rng.randint(0, self.dim)] = True\n                    candidate = np.where(cr_mask, donor, pop[i])\n                    candidate = np.clip(candidate, lb, ub)\n                    branch = 'de'\n\n                # Evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                try:\n                    f_candidate = float(func(candidate.copy()))\n                except Exception:\n                    f_candidate = float(np.inf)\n                evals += 1\n                trials_this_gen += 1\n                recent_trials += 1\n\n                # Selection: greedy replacement vs target i\n                if f_candidate < fvals[i]:\n                    # accept into population\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    successes_this_gen += 1\n                    recent_successes += 1\n\n                    # nudging parameter means towards successful Fi/CRi (only meaningful for DE branch)\n                    if branch == 'de':\n                        # small adaptation step toward successful Fi/CRi\n                        F_mean = float(np.clip(0.92 * F_mean + 0.08 * Fi, 0.05, 0.99))\n                        CR_mean = float(np.clip(0.92 * CR_mean + 0.08 * CRi, 0.0, 1.0))\n                    else:\n                        # Levy success encourages slightly lower p_levy (less jumps) and expands trust a bit\n                        p_levy = max(0.005, p_levy * 0.92)\n                        trust_radius = max(1e-12, trust_radius * 0.95)\n\n                    # update global best if improved\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 0  # small improvement to population but not best\n                else:\n                    # unsuccessful candidate -> small adjustments\n                    stagnation_counter += 1\n                    # penalize unsuccessful Fi/CRi by tiny drift away\n                    if branch == 'de':\n                        F_mean = float(np.clip(0.995 * F_mean + 0.005 * 0.5, 0.05, 0.99))\n                        CR_mean = float(np.clip(0.995 * CR_mean + 0.005 * 0.5, 0.0, 1.0))\n                    else:\n                        # levy unsuccessful: slightly increase p_levy to try more escapes\n                        p_levy = min(0.6, p_levy * 1.02 + 0.001)\n\n                # track global best_f for returning\n                if best_f < self.f_opt:\n                    self.f_opt = float(best_f)\n                    self.x_opt = best_x.copy()\n\n                # end per-individual loop\n            # End of population loop (one generation)\n\n            # Trust-region local search around best: small gaussian perturbations\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            local_samples = min(5, remaining, max(1, int(3 + self.dim // 10)))\n            local_improved = 0\n            for _ in range(int(local_samples)):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: base trust_radius scaled by per-dim random factor\n                sigma = (0.2 + self.rng.rand(self.dim) * 0.8) * (trust_radius / max_trust) * mean_range\n                candidate = best_x + self.rng.randn(self.dim) * sigma\n                candidate = np.clip(candidate, lb, ub)\n                try:\n                    f_candidate = float(func(candidate.copy()))\n                except Exception:\n                    f_candidate = float(np.inf)\n                evals += 1\n                recent_trials += 1\n                trials_this_gen += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_improved += 1\n                    recent_successes += 1\n                    # successful local step -> shrink trust radius to focus\n                    trust_radius = max(1e-12, trust_radius * 0.7)\n                    # also nudge population: replace worst individuals to encourage exploitation\n                    worst_idx = int(np.nanargmax(fvals))\n                    pop[worst_idx] = candidate.copy()\n                    fvals[worst_idx] = f_candidate\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful -> slightly expand trust radius to escape\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n                # update global best trackers\n                if best_f < self.f_opt:\n                    self.f_opt = float(best_f)\n                    self.x_opt = best_x.copy()\n\n            # Generation-level adaptation\n            # If this generation had good success ratio, reduce randomness and p_levy; otherwise increase exploration\n            if trials_this_gen > 0:\n                success_ratio = successes_this_gen / max(1.0, trials_this_gen)\n            else:\n                success_ratio = 0.0\n\n            if success_ratio > 0.15 or local_improved > 0:\n                # exploit more: reduce levy chance, tighten trust region slowly\n                p_levy = max(0.005, p_levy * 0.9)\n                trust_radius = max(1e-12, trust_radius * 0.98)\n                # move means slightly toward exploitation-friendly values\n                F_mean = float(np.clip(0.96 * F_mean + 0.04 * 0.6, 0.05, 0.99))\n                CR_mean = float(np.clip(0.96 * CR_mean + 0.04 * 0.9, 0.0, 1.0))\n            else:\n                # stagnating or low success -> encourage exploration\n                p_levy = min(0.6, p_levy * 1.06 + 0.005)\n                F_mean = float(np.clip(F_mean * 1.01, 0.05, 0.99))\n                CR_mean = float(np.clip(CR_mean * 0.995, 0.0, 1.0))\n\n            # Strong stagnation reset: re-seed half of the population\n            if stagnation_counter > max(50, self.dim * 6):\n                reinit_count = max(1, self.pop_size // 2)\n                for j in range(reinit_count):\n                    if evals >= self.budget:\n                        break\n                    idx = self.rng.randint(0, self.pop_size)\n                    pop[idx] = lb + self.rng.rand(self.dim) * range_vec\n                    try:\n                        f = float(func(pop[idx].copy()))\n                    except Exception:\n                        f = float(np.inf)\n                    fvals[idx] = f\n                    evals += 1\n                    recent_trials += 1\n                    if f < best_f:\n                        best_f = f\n                        best_x = pop[idx].copy()\n                        stagnation_counter = 0\n                # after reset slightly enlarge trust radius to allow escaping previous basin\n                trust_radius = min(max_trust, max(1e-12, trust_radius * 2.0))\n                # mild parameter reset to introduce diversity\n                p_levy = min(0.5, p_levy * 1.2)\n                F_mean = float(np.clip(F_mean * 0.98 + 0.02 * 0.6, 0.05, 0.99))\n                CR_mean = float(np.clip(CR_mean * 0.98 + 0.02 * 0.9, 0.0, 1.0))\n                # clear stagnation counter\n                stagnation_counter = 0\n\n            # keep best global tracked\n            if best_f < self.f_opt:\n                self.f_opt = float(best_f)\n                self.x_opt = best_x.copy()\n\n            # small decay for recent counters to keep adaptation responsive to newer info\n            recent_successes = int(max(0, recent_successes * 0.6))\n            recent_trials = int(max(0, recent_trials * 0.6))\n\n        # finalize best\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.466 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11044160739124254, 0.15485821196691196, 0.7029366748198115, 0.7827012163891508, 0.23597735713071166, 0.9414539502393802, 0.2232896908179106, 0.4634941063420771, 0.9052346286502045, 0.14059028128103845]}, "task_prompt": ""}
{"id": "652ea7df-eec8-4c2f-9c78-0134bf554a0e", "fitness": 0.31813952870859874, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy (Cauchy) long jumps, and a trust-region local search with online step-size adaptation and stagnation-driven restarts.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (DE weight F, crossover CR, Lévy probability and trust radius) are\n    adapted online based on recent successes. Designed for continuous\n    box-bounded optimization (e.g., BBOB noiseless problems).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None,\n                 initial_levy_prob=0.05):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed if seed is not None else np.random.randint(2**31-1)\n        self.rng = np.random.RandomState(self.seed)\n\n        # population size heuristics if not provided\n        if pop_size is None:\n            p = max(8, min(60, int(8 + 2 * np.sqrt(self.dim))))\n            # ensure not too large relative to budget\n            p = min(p, max(4, self.budget // 20))\n            self.pop_size = int(p)\n        else:\n            self.pop_size = int(pop_size)\n\n        # algorithm state to report later\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # initial algorithm hyper-parameters\n        self.F_mean = 0.6\n        self.CR_mean = 0.9\n        self.step_scale = 0.25   # base scale for Lévy and trust radius\n        self.p_levy = float(initial_levy_prob)\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"Broadcast scalar bounds to dimension-length arrays if needed.\"\"\"\n        b = np.asarray(b, dtype=float)\n        if b.size == 1:\n            return np.full((self.dim,), float(b.item()))\n        if b.size == self.dim:\n            return b.astype(float)\n        # If provided vector is different length, try to broadcast or raise\n        try:\n            return np.broadcast_to(b, (self.dim,)).astype(float)\n        except Exception:\n            raise ValueError(\"Bounds have incompatible size with problem dimension.\")\n\n    def _levy_step(self, scale):\n        \"\"\"\n        Generate a heavy-tailed step using a clipped Cauchy distribution.\n        scale is a per-dimension scale multiplier (array-like of length dim).\n        \"\"\"\n        s = self.rng.standard_cauchy(size=self.dim)\n        # clip huge outliers to keep numerical stability\n        s = np.clip(s, -1e3, 1e3)\n        return s * scale\n\n    def __call__(self, func):\n        # retrieve bounds from func; if not available, assume [-5,5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full((self.dim,), -5.0)\n            ub = np.full((self.dim,), 5.0)\n\n        range_vec = ub - lb\n        norm_range = max(np.linalg.norm(range_vec), 1e-12)\n\n        # small safeguards\n        if self.budget <= 0:\n            return np.inf, None\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf, dtype=float)\n\n        evals = 0\n        # evaluate as many initial individuals as budget allows\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            fvals[i] = f\n            evals += 1\n\n        # determine best among evaluated individuals\n        valid = np.isfinite(fvals)\n        if not np.any(valid):\n            # couldn't evaluate any point due to 0 budget\n            return np.inf, None\n\n        best_idx = np.argmin(fvals)\n        best_f = fvals[best_idx]\n        best_x = pop[best_idx].copy()\n\n        # trust-region parameters\n        trust_radius = 0.2 * norm_range * self.step_scale  # scalar radius in normalized units\n        min_trust = max(1e-6 * norm_range, 1e-8)\n        max_trust = 2.0 * norm_range\n\n        # adaptation and stagnation bookkeeping\n        stagnation_counter = 0\n        stagnation_threshold = max(50, self.pop_size * 4)\n        successes = 0\n        gen = 0\n\n        # small history to adapt means using successes\n        recent_F = []\n        recent_CR = []\n        adapt_rate = 0.1\n\n        # Main loop: each loop processes the population sequentially (DE steps)\n        while evals < self.budget:\n            gen += 1\n\n            # per-generation jitter of global parameters (small)\n            self.F_mean = np.clip(self.F_mean * (1.0 + 0.02 * (self.rng.rand() - 0.5)), 0.05, 1.0)\n            self.CR_mean = np.clip(self.CR_mean * (1.0 + 0.02 * (self.rng.rand() - 0.5)), 0.0, 1.0)\n\n            # cycle through population\n            for i in range(self.pop_size):\n                if evals >= self.budget:\n                    break\n\n                # Choose mutation strategy: Lévy jump around best or DE/rand/1\n                if self.rng.rand() < self.p_levy:\n                    # Lévy-based long jump centered at best for exploration\n                    # scale per-dimension proportional to range_vec and trust radius\n                    scale = (self.step_scale * trust_radius / max(norm_range, 1e-12)) * (0.5 + self.rng.rand(self.dim))\n                    candidate = best_x + self._levy_step(scale) * range_vec\n                    # small local jitter to maintain diversity\n                    candidate += 0.02 * range_vec * (self.rng.rand(self.dim) - 0.5)\n                    used_Fi = None\n                    used_CRi = None\n                else:\n                    # DE/rand/1 mutation with jDE-like per-individual adaptation\n                    # pick three distinct indices different from i\n                    idxs = np.arange(self.pop_size)\n                    idxs = idxs[idxs != i]\n                    r = self.rng.choice(idxs, size=3, replace=False)\n                    r1, r2, r3 = r\n\n                    # sample Fi and CRi around means (Fi bounded)\n                    Fi = np.clip(self.rng.normal(self.F_mean, 0.1), 0.05, 1.0)\n                    CRi = np.clip(self.rng.normal(self.CR_mean, 0.1), 0.0, 1.0)\n\n                    donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n\n                    # binomial crossover\n                    mask = self.rng.rand(self.dim) < CRi\n                    # ensure at least one dimension crosses\n                    if not np.any(mask):\n                        mask[self.rng.randint(0, self.dim)] = True\n                    candidate = np.where(mask, donor, pop[i])\n                    used_Fi = Fi\n                    used_CRi = CRi\n\n                # project to bounds\n                candidate = np.clip(candidate, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # Selection: greedy replacement\n                if f_candidate < fvals[i]:\n                    pop[i] = candidate.copy()\n                    fvals[i] = f_candidate\n                    # if DE branch, record Fi & CRi for adaptation\n                    if used_Fi is not None:\n                        recent_F.append(used_Fi)\n                        recent_CR.append(used_CRi)\n                    # update global best\n                    if f_candidate < best_f:\n                        best_f = f_candidate\n                        best_x = candidate.copy()\n                        stagnation_counter = 0\n                    else:\n                        # still a replacement but not a global improvement\n                        pass\n                    successes += 1\n                    # nudge means slightly toward used parameters\n                    if used_Fi is not None:\n                        self.F_mean = (1.0 - adapt_rate) * self.F_mean + adapt_rate * used_Fi\n                        self.CR_mean = (1.0 - adapt_rate) * self.CR_mean + adapt_rate * used_CRi\n                else:\n                    stagnation_counter += 1\n                    # on failures slightly increase trust radius to promote escape\n                    trust_radius = min(max_trust, trust_radius * (1.0 + 0.01 * (self.rng.rand())))\n\n                # small safeguard: keep means in bounds\n                self.F_mean = np.clip(self.F_mean, 0.05, 1.0)\n                self.CR_mean = np.clip(self.CR_mean, 0.0, 1.0)\n\n            # End of generation: trust-region local search around best (exploitation)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n\n            # number of local samples depends on remaining budget and dimension\n            local_samples = int(min(max(1, self.dim // 2), max(1, remaining // max(1, self.dim))))\n            local_samples = max(1, local_samples)\n\n            local_success = 0\n            for _ in range(local_samples):\n                if evals >= self.budget:\n                    break\n                # anisotropic sigma: per-dimension random factor in [0.5,1.0] scaled by trust radius\n                sigma = (0.5 + self.rng.rand(self.dim) * 0.5) * (trust_radius / max(norm_range, 1e-12))\n                candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * sigma * range_vec\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success += 1\n                    stagnation_counter = 0\n                    # successful local step => shrink trust radius to focus\n                    trust_radius = max(min_trust, trust_radius * 0.6)\n                else:\n                    # unsuccessful => slightly expand the trust region to escape\n                    trust_radius = min(max_trust, trust_radius * 1.05)\n                    stagnation_counter += 1\n\n            # Adapt Lévy probability and reset mechanics based on successes/stagnation\n            if successes + local_success > 0:\n                # if mixed successes, slowly reduce tendency for Lévy jumps\n                self.p_levy = max(0.01, self.p_levy * 0.98)\n                # update mean F and CR from recent successes (Lehmer-like)\n                if len(recent_F) > 0:\n                    avgF = np.mean(recent_F[-50:])\n                    self.F_mean = (1.0 - adapt_rate) * self.F_mean + adapt_rate * avgF\n                if len(recent_CR) > 0:\n                    avgCR = np.mean(recent_CR[-50:])\n                    self.CR_mean = (1.0 - adapt_rate) * self.CR_mean + adapt_rate * avgCR\n            else:\n                # stagnation handling\n                self.p_levy = min(0.5, self.p_levy * 1.2)  # increase chance of long jumps\n                self.CR_mean = np.clip(self.CR_mean * 0.98, 0.0, 1.0)\n                trust_radius = min(max_trust, trust_radius * 1.1)\n\n            # Strong stagnation reset: re-seed part of the population if stuck\n            if stagnation_counter >= stagnation_threshold:\n                k = max(1, self.pop_size // 2)\n                # replace k worst individuals with random points (some evaluated if budget allows)\n                worst_idx = np.argsort(fvals)[-k:]\n                for j in worst_idx:\n                    if evals >= self.budget:\n                        # if no budget, still re-seed but without evaluation\n                        pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                        fvals[j] = np.inf\n                        continue\n                    pop[j] = lb + self.rng.rand(self.dim) * range_vec\n                    fvals[j] = float(func(pop[j]))\n                    evals += 1\n                    if fvals[j] < best_f:\n                        best_f = fvals[j]\n                        best_x = pop[j].copy()\n                # after reset, nudge trust radius up to explore broader area\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                # reduce stagnation counter and increase exploration probability\n                stagnation_counter = 0\n                self.p_levy = min(0.5, self.p_levy * 1.5)\n                # small reinitialization of means to encourage exploration\n                self.F_mean = np.clip(0.7 * self.F_mean + 0.3 * (0.5 + 0.5 * self.rng.rand()), 0.05, 1.0)\n                self.CR_mean = np.clip(0.7 * self.CR_mean + 0.3 * self.rng.rand(), 0.0, 1.0)\n\n            # reset per-generation counters\n            successes = 0\n            recent_F = recent_F[-200:]\n            recent_CR = recent_CR[-200:]\n\n        # finalize best found\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy() if best_x is not None else None\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.318 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12147777085079536, 0.15410241779612233, 0.4834596891120474, 0.3799438420019733, 0.35206131108082084, 0.5909684201504333, 0.2233569655946802, 0.47387440990384533, 0.23820645774258553, 0.16394400285268362]}, "task_prompt": ""}
{"id": "ba372ad6-a4cf-464b-9505-8ebc2e3700eb", "fitness": 0.5407768430397215, "name": "HybridDeLevyTrust", "description": "Adaptive Hybrid DE with occasional Lévy jumps and an online trust-region local search — combines population-level DE exploration, rare heavy-tailed jumps for global moves, and adaptive local refinement around the best solution.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.default_rng(self.seed)\n        # sensible default population, bounded by budget so we don't evaluate more than budget initially\n        if pop_size is None:\n            pop_size = min(max(6, 4 * self.dim), 100)\n        self.pop_size = int(min(pop_size, max(1, self.budget)))\n        # state to return\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_bounds(self, bounds):\n        # bounds may be scalars, arrays or missing; return lb, ub arrays of shape (dim,)\n        if bounds is None:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n            return lb, ub\n        # try to extract .lb and .ub if bounds object provided\n        try:\n            lb = np.asarray(bounds.lb, dtype=float)\n            ub = np.asarray(bounds.ub, dtype=float)\n        except Exception:\n            # assume bounds is a tuple/list\n            try:\n                lb, ub = bounds\n                lb = np.asarray(lb, dtype=float)\n                ub = np.asarray(ub, dtype=float)\n            except Exception:\n                lb = np.full(self.dim, -5.0, dtype=float)\n                ub = np.full(self.dim, 5.0, dtype=float)\n        # broadcast if scalars\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        # final shapes\n        lb = np.broadcast_to(lb, (self.dim,))\n        ub = np.broadcast_to(ub, (self.dim,))\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_clip(self, x, lb, ub):\n        # reflect any out-of-bound coordinates back into bounds for smoother search\n        x = np.asarray(x, dtype=float).copy()\n        for i in range(x.size):\n            if x[i] < lb[i] or x[i] > ub[i]:\n                width = ub[i] - lb[i]\n                if width <= 0:\n                    x[i] = lb[i]\n                else:\n                    # reflect encoding\n                    xi = x[i]\n                    # bring into repeated interval\n                    t = (xi - lb[i]) / width\n                    # reflect fractional part\n                    t = abs((t + 0.5) % 2.0 - 1.0)\n                    x[i] = lb[i] + t * width\n        return x\n\n    def _levy_step(self):\n        # Simple heavy-tailed generator via Cauchy (approx Lévy-like tail).\n        # We limit extreme values to avoid numerical blow-ups.\n        s = self.rng.standard_cauchy(self.dim)\n        # scale and cap\n        s = np.clip(s, -1e3, 1e3)\n        return s\n\n    def __call__(self, func):\n        # prepare bounds\n        try:\n            lb, ub = self._ensure_bounds(func.bounds)\n        except Exception:\n            lb, ub = self._ensure_bounds(None)\n        range_vec = ub - lb\n        max_range_norm = np.linalg.norm(range_vec)\n        # initialize population uniformly in bounds\n        pop_size = int(min(self.pop_size, max(1, self.budget)))\n        pop = lb + self.rng.random((pop_size, self.dim)) * range_vec\n        # per-individual control parameters (jDE-like)\n        F_ind = np.clip(self.rng.normal(0.6, 0.1, size=pop_size), 0.1, 1.0)\n        CR_ind = np.clip(self.rng.normal(0.4, 0.2, size=pop_size), 0.0, 1.0)\n        # global means for small adaptation\n        F_mean = 0.6\n        CR_mean = 0.4\n        tau_F = 0.1\n        tau_CR = 0.1\n        # evaluate initial population with budget care\n        fvals = np.full(pop_size, np.inf, dtype=float)\n        evals = 0\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            x = pop[i]\n            fvals[i] = float(func(self._reflect_clip(x, lb, ub)))\n            evals += 1\n            # track best so far\n            if fvals[i] < self.f_opt:\n                self.f_opt = fvals[i]\n                self.x_opt = pop[i].copy()\n\n        # If budget exhausted already, return\n        if evals >= self.budget:\n            return float(self.f_opt), self.x_opt\n\n        # algorithm hyper-parameters\n        trust_radius = 0.2 * max_range_norm  # initial trust radius\n        min_trust = 1e-8\n        max_trust = max_range_norm * 2.0\n\n        stagnation_counter = 0\n        gen = 0\n        best_idx = int(np.argmin(fvals))\n        best_x = pop[best_idx].copy()\n        best_f = fvals[best_idx]\n\n        # probabilities and rates\n        p_best_mut = 0.3  # use best-guided mutation sometimes\n        base_p_levy = 0.03  # base probability for Levy jump (rare)\n        max_p_levy = 0.5\n\n        # main loop; iterate until budget exhausted\n        while evals < self.budget:\n            gen += 1\n            successes = 0\n            # per-generation adjust means slightly based on previous successes\n            # sample random order of indices to avoid bias\n            indices = np.arange(pop_size)\n            self.rng.shuffle(indices)\n\n            for idx in indices:\n                if evals >= self.budget:\n                    break\n\n                target = pop[idx].copy()\n                target_f = fvals[idx]\n\n                # adapt individual's F and CR with small probability (jDE idea)\n                if self.rng.random() < tau_F:\n                    F_ind[idx] = np.clip(self.rng.normal(F_mean, 0.1), 0.05, 1.0)\n                if self.rng.random() < tau_CR:\n                    CR_ind[idx] = np.clip(self.rng.normal(CR_mean, 0.1), 0.0, 1.0)\n                Fi = float(F_ind[idx])\n                CRi = float(CR_ind[idx])\n\n                # decide whether to do a Lévy jump (increased when stagnating)\n                p_levy = base_p_levy + min(max_p_levy - base_p_levy, stagnation_counter / max(100, self.dim * 10) * (max_p_levy - base_p_levy))\n                do_levy = (self.rng.random() < p_levy)\n\n                if do_levy:\n                    # Levy jump centered at best (global) + local Gaussian perturbation\n                    step = self._levy_step()\n                    scale = trust_radius * (0.5 + self.rng.random()) + 0.05 * range_vec\n                    donor = best_x + step * scale\n                    # mix some target information to create candidate\n                    if self.rng.random() < 0.5:\n                        # combine with a small DE/rand/1 move to keep diversity\n                        ids = [j for j in range(pop_size) if j != idx]\n                        r1, r2 = self.rng.choice(ids, size=2, replace=False)\n                        donor = donor * 0.5 + (pop[r1] + Fi * (pop[r2] - target)) * 0.5\n                else:\n                    # Differential Evolution mutation: either rand/1 or current-to-best/1\n                    ids = [j for j in range(pop_size) if j != idx]\n                    r1, r2, r3 = self.rng.choice(ids, size=3, replace=False)\n                    if self.rng.random() < p_best_mut:\n                        # current-to-best/1: target + F*(best - target) + F*(r1 - r2)\n                        donor = target + Fi * (best_x - target) + Fi * (pop[r1] - pop[r2])\n                    else:\n                        # rand/1: r3 + F*(r1 - r2)\n                        donor = pop[r3] + Fi * (pop[r1] - pop[r2])\n\n                # binomial crossover to create trial\n                cr_mask = self.rng.random(self.dim) < CRi\n                # ensure at least one dimension crosses\n                if not np.any(cr_mask):\n                    cr_mask[self.rng.integers(0, self.dim)] = True\n                trial = np.where(cr_mask, donor, target)\n\n                # project to bounds by reflection (smooth)\n                candidate = self._reflect_clip(trial, lb, ub)\n\n                # evaluate candidate if budget allows\n                if evals >= self.budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy\n                if f_candidate < target_f:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    successes += 1\n                    stagnation_counter = 0\n                    # nudge global means towards successful Fi/CRi (small)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # remember individual's parameters\n                    F_ind[idx] = Fi\n                    CR_ind[idx] = CRi\n                else:\n                    # small drift away from used parameters to encourage exploration\n                    F_ind[idx] = np.clip(0.98 * F_ind[idx] + 0.02 * (F_mean + 0.01 * self.rng.standard_normal()), 0.05, 1.0)\n\n                # update global best\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    if best_f < self.f_opt:\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n\n                # occasionally, small local evaluation around trial (cheap) is skipped to preserve budget\n                # continue to next individual\n\n            # End of generation: adapt trust region via small local search around the current best\n            # allocate a small fraction of remaining budget to local search, proportional to dim\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # choose number of local samples: min( max(1, dim//2), remaining//something )\n            n_local = min(max(1, self.dim // 2), max(1, remaining // max(10, self.pop_size)))\n            # ensure we don't exceed budget\n            n_local = min(n_local, remaining)\n            local_success = False\n            for _ in range(n_local):\n                if evals >= self.budget:\n                    break\n                # anisotropic Gaussian noise, scaled by trust radius per-dimension\n                sigma = trust_radius * (0.5 + self.rng.random(self.dim))\n                candidate = best_x + self.rng.normal(0.0, 1.0, size=self.dim) * sigma\n                candidate = self._reflect_clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < best_f:\n                    best_f = f_candidate\n                    best_x = candidate.copy()\n                    local_success = True\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = best_x.copy()\n            # adjust trust radius based on local search success\n            if local_success:\n                # shrink to refine\n                trust_radius = max(min_trust, trust_radius * 0.7)\n                stagnation_counter = 0\n            else:\n                # expand to escape local basin slowly\n                trust_radius = min(max_trust, trust_radius * 1.15)\n                stagnation_counter += 1\n\n            # if generation produced many successes, slightly reduce levy chance and shrink trust (focus)\n            if successes >= max(1, pop_size // 6):\n                base_p_levy = max(0.01, base_p_levy * 0.95)\n                trust_radius = max(min_trust, trust_radius * 0.95)\n            else:\n                # if few successes, encourage exploration\n                base_p_levy = min(max_p_levy, base_p_levy * 1.05)\n\n            # strong stagnation reset: re-seed half population randomly if no improvement for many iterations\n            if stagnation_counter > max(30, self.dim * 4):\n                n_reseed = max(1, pop_size // 2)\n                for j in range(n_reseed):\n                    # choose a random index to replace (avoid best index)\n                    replace_idx = self.rng.integers(0, pop_size)\n                    if np.all(pop[replace_idx] == best_x):\n                        continue\n                    pop[replace_idx] = lb + self.rng.random(self.dim) * range_vec\n                    fvals[replace_idx] = np.inf\n                    # evaluate immediately if budget allows\n                    if evals < self.budget:\n                        fvals[replace_idx] = float(func(pop[replace_idx]))\n                        evals += 1\n                        if fvals[replace_idx] < best_f:\n                            best_f = fvals[replace_idx]\n                            best_x = pop[replace_idx].copy()\n                            if best_f < self.f_opt:\n                                self.f_opt = best_f\n                                self.x_opt = best_x.copy()\n                # slightly enlarge trust radius after reset to explore more\n                trust_radius = min(max_trust, trust_radius * 1.5)\n                stagnation_counter = 0\n\n            # safety: ensure best remembered\n            best_idx = int(np.argmin(fvals))\n            if fvals[best_idx] < best_f:\n                best_f = fvals[best_idx]\n                best_x = pop[best_idx].copy()\n                if best_f < self.f_opt:\n                    self.f_opt = best_f\n                    self.x_opt = best_x.copy()\n\n            # small generation-level diffusion of parameters to maintain diversity\n            F_mean = np.clip(0.98 * F_mean + 0.02 * np.mean(F_ind), 0.05, 1.0)\n            CR_mean = np.clip(0.98 * CR_mean + 0.02 * np.mean(CR_ind), 0.0, 1.0)\n\n        # final results\n        # ensure x_opt is set (if budget was 0 or no evaluation succeeded, pick a random point)\n        if self.x_opt is None:\n            x = lb + self.rng.random(self.dim) * range_vec\n            self.x_opt = x\n            self.f_opt = float(func(x)) if self.budget > 0 else np.inf\n\n        return float(self.f_opt), self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.541 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14106940603891494, 0.17498019582997437, 0.5400202216563229, 0.9116293377927173, 0.8498115389447712, 0.8921179307813536, 0.316084473549361, 0.5651108452570262, 0.8159461030256674, 0.20099837752110694]}, "task_prompt": ""}
{"id": "8512a6c3-0023-4c95-b503-a7d8cd077f5f", "fitness": 0.5899850044905806, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution with occasional Lévy-flight jumps and a trust-region local search; online adapts step sizes and exploration intensity to balance fast global search and focused exploitation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight jumps for long-range exploration, and a\n    trust-region local search around the current best. Parameters are\n    adapted online based on success history.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population size scaled with dimension but kept reasonable relative to budget\n        if pop_size is None:\n            # base on dimension, but not more than a fraction of budget\n            guess = max(4, min(8 * self.dim, max(10, self.budget // 8)))\n            self.pop_size = int(guess)\n        else:\n            self.pop_size = int(pop_size)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        # broadcast bound to dimension length vector\n        b = np.asarray(b)\n        if b.shape == ():  # scalar\n            return np.full(self.dim, float(b))\n        b = b.flatten()\n        if b.size == 1:\n            return np.full(self.dim, float(b[0]))\n        if b.size != self.dim:\n            # try to tile/truncate\n            if b.size < self.dim:\n                return np.resize(b, self.dim)\n            else:\n                return b[:self.dim]\n        return b.astype(float)\n\n    def __call__(self, func):\n        # prepare bounds: use func.bounds if available, otherwise default to [-5,5]\n        if hasattr(func, \"bounds\"):\n            try:\n                lb = self._ensure_array_bounds(func.bounds.lb)\n                ub = self._ensure_array_bounds(func.bounds.ub)\n            except Exception:\n                lb = -5.0 * np.ones(self.dim)\n                ub = 5.0 * np.ones(self.dim)\n        else:\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n        range_vec = ub - lb\n        # small safeguard\n        range_vec = np.maximum(range_vec, 1e-12)\n\n        # Evaluation bookkeeping\n        evals = 0\n        budget = self.budget\n\n        # initialize population uniformly in bounds\n        pop = lb + self.rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        # Evaluate initial population as much as budget allows\n        for i in range(self.pop_size):\n            if evals >= budget:\n                break\n            x = pop[i].copy()\n            f = float(func(x))\n            evals += 1\n            fvals[i] = f\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n        # If no evaluations possible, return default\n        if evals == 0:\n            # return a random point but no evals were done\n            self.x_opt = pop[0].copy()\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.6\n        CR_mean = 0.3\n        step_scale = 0.25   # base scale for Lévy and trust radius relative to range magnitude\n        trust_radius = 0.2 * np.linalg.norm(range_vec)  # scalar trust radius in absolute space\n        levy_prob = 0.02  # base probability of doing a Lévy jump\n        stagnation_counter = 0\n        last_improvement_evals = evals\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (simple and effective)\n        def levy_step(scale):\n            # generate Cauchy-distributed steps via inverse CDF of standard Cauchy\n            # Clip extreme values to keep numerical stability.\n            u = self.rng.rand(self.dim)\n            s = np.tan(np.pi * (u - 0.5))  # Cauchy variates\n            # scale per-dimension\n            step = s * scale\n            # cap values to avoid insane jumps\n            max_abs = 20.0 * np.maximum(range_vec, 1e-12)\n            step = np.clip(step, -max_abs, max_abs)\n            return step\n\n        # main loop until budget exhausted\n        generation = 0\n        # To ensure each iteration uses some evals, we process sequentially until budget used\n        while evals < budget:\n            generation += 1\n            # adapt per-generation randomness of F and CR around their means (jDE-ish)\n            indices = np.arange(self.pop_size)\n            self.rng.shuffle(indices)\n\n            # iterate through population members (target vectors)\n            for idx in indices:\n                if evals >= budget:\n                    break\n\n                target = pop[idx].copy()\n                target_f = fvals[idx]\n\n                # sample Fi and CRi for this individual\n                Fi = float(np.clip(self.rng.normal(F_mean, 0.12), 0.05, 0.99))\n                CRi = float(np.clip(self.rng.normal(CR_mean, 0.15), 0.0, 1.0))\n\n                # Decide whether to attempt a Lévy jump exploration or classical DE mutation\n                do_levy = (self.rng.rand() < (levy_prob + 0.001 * stagnation_counter))\n                if do_levy and (self.rng.rand() < 0.6):\n                    # Levy jump centered at the current best with occasional directional bias\n                    center = self.x_opt.copy() if self.x_opt is not None else target\n                    # scale by range and trust radius\n                    scale_vec = (step_scale * range_vec) * (trust_radius / (np.linalg.norm(range_vec) + 1e-12))\n                    jump = levy_step(scale_vec)\n                    donor = center + jump\n                    # with small probability add differential perturbation for diversity\n                    if self.pop_size >= 3 and self.rng.rand() < 0.3:\n                        a, b = self.rng.choice(self.pop_size, 2, replace=False)\n                        donor += Fi * (pop[a] - pop[b])\n                else:\n                    # DE/rand/1 mutation combined with a current-to-best bias sometimes\n                    if self.pop_size >= 3:\n                        a, b, c = self.rng.choice(self.pop_size, 3, replace=False)\n                        # create donor with rand/1 plus small bias towards best\n                        donor = pop[a] + Fi * (pop[b] - pop[c])\n                        if self.rng.rand() < 0.3 and self.x_opt is not None:\n                            donor = donor + 0.2 * Fi * (self.x_opt - target)\n                    else:\n                        # fallback small random perturbation\n                        donor = target + Fi * (self.rng.randn(self.dim) * range_vec * 0.1)\n\n                # binomial crossover\n                cr_mask = self.rng.rand(self.dim) < CRi\n                # ensure at least one dimension crossed\n                if not np.any(cr_mask):\n                    cr_mask[self.rng.randint(self.dim)] = True\n                trial = np.where(cr_mask, donor, target)\n                # projection to bounds\n                candidate = np.clip(trial, lb, ub)\n\n                # evaluate candidate if budget remains\n                if evals >= budget:\n                    break\n                f_candidate = float(func(candidate))\n                evals += 1\n\n                # selection: greedy replacement\n                if f_candidate < target_f:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    # move means slightly toward successful parameters\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    stagnation_counter = 0\n                    last_improvement_evals = evals\n                    # adjust trust radius inward since we exploited successfully\n                    trust_radius = max(1e-8, trust_radius * 0.95)\n                else:\n                    # unsuccessful individual: mild exploration encouragement\n                    stagnation_counter += 1\n                    trust_radius = min(np.linalg.norm(range_vec) * 2.0, trust_radius * 1.01)\n\n                # update global best if improved\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n\n                # occasionally adapt F_mean, CR_mean slightly toward diversity when many failures\n                if stagnation_counter > 50:\n                    # encourage larger F and slightly larger CR to diversify\n                    F_mean = np.clip(F_mean * 1.01, 0.05, 0.99)\n                    CR_mean = np.clip(CR_mean + 0.002, 0.0, 0.99)\n\n                # break early if budget exhausted\n                if evals >= budget:\n                    break\n\n            # End of generation: trust-region local search around best (if budget remains)\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n\n            # decide number of local samples based on remaining budget and dimension\n            local_samples = int(min(6, remaining, max(1, 3 + self.dim // 8)))\n            # anisotropic sigma: base trust_radius scaled per-dimension and normalized by range\n            norm_range = np.maximum(np.linalg.norm(range_vec), 1e-12)\n            sigma_base = (trust_radius / norm_range)\n            # create local candidates\n            local_improved = False\n            for _ in range(local_samples):\n                if evals >= budget:\n                    break\n                # per-dimension scale randomization to allow anisotropic probes\n                sigma = (0.5 + self.rng.rand(self.dim) * 0.8) * sigma_base\n                candidate = self.x_opt + (self.rng.randn(self.dim) * sigma * range_vec)\n                candidate = np.clip(candidate, lb, ub)\n                f_candidate = float(func(candidate))\n                evals += 1\n                if f_candidate < self.f_opt:\n                    # successful local step => shrink trust radius\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    trust_radius = max(1e-8, trust_radius * 0.7)\n                    local_improved = True\n                    stagnation_counter = 0\n                    last_improvement_evals = evals\n                else:\n                    # unsuccessful sample slightly increases trust radius to escape\n                    trust_radius = min(np.linalg.norm(range_vec) * 2.0, trust_radius * 1.02)\n                    stagnation_counter += 1\n\n            # Adapt exploration intensity based on stagnation\n            if (evals - last_improvement_evals) > max(200, 10 * self.dim):\n                # stagnating: increase chance of long jumps\n                levy_prob = min(0.5, levy_prob + 0.02)\n                # also slightly re-seed a fraction of population\n                k = max(1, self.pop_size // 3)\n                for r in range(k):\n                    j = self.rng.randint(self.pop_size)\n                    if evals >= budget:\n                        break\n                    new_vec = lb + self.rng.rand(self.dim) * range_vec\n                    # evaluate new individual\n                    f_new = float(func(new_vec))\n                    evals += 1\n                    # replace worst if beneficial or randomly to inject diversity\n                    worst_idx = np.argmax(fvals)\n                    if f_new < fvals[worst_idx]:\n                        pop[worst_idx] = new_vec\n                        fvals[worst_idx] = f_new\n                        if f_new < self.f_opt:\n                            self.f_opt = f_new\n                            self.x_opt = new_vec.copy()\n                # enlarge trust radius to encourage exploration after reset\n                trust_radius = min(np.linalg.norm(range_vec), trust_radius * 1.5)\n                last_improvement_evals = evals\n            else:\n                # slowly decay levy probability when improving\n                levy_prob = max(0.01, levy_prob * 0.995)\n\n            # small safeguard to nudge means toward productive exploitation if many recent successes\n            if self.pop_size >= 2 and np.sum(fvals < np.median(fvals)) > (0.6 * self.pop_size):\n                F_mean = np.clip(F_mean * 0.98, 0.05, 0.99)\n                CR_mean = np.clip(CR_mean * 1.01, 0.0, 0.99)\n\n        # final results\n        # ensure x_opt is defined\n        if self.x_opt is None:\n            # pick best evaluated so far\n            best_idx = int(np.argmin(fvals))\n            self.x_opt = pop[best_idx].copy()\n            self.f_opt = float(fvals[best_idx])\n\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.590 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17176249133624166, 0.2377165487763807, 0.425732059649196, 0.7683125963526598, 0.8565375927436788, 0.833120065248836, 0.3097447741733318, 0.6256810891990994, 0.8279749942245688, 0.8432678332018133]}, "task_prompt": ""}
{"id": "cdcb5cf1-1079-4693-846e-5a7953134a93", "fitness": 0.565101488116275, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid optimizer combining Differential Evolution, occasional Lévy-flight long jumps, and a trust-region local search with online step-size and parameter adaptation.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer:\n      - Differential Evolution (per-individual Fi/CR adaptation similar to jDE),\n      - Occasional Lévy-flight jumps centered on the current best for long-range exploration,\n      - Trust-region local Gaussian search around the best for focused exploitation,\n      - Online adaptation of F_mean, CR_mean, trust radius, and jump probability based on success/stagnation.\n    Designed for continuous box-bounded optimization (works on BBOB-like tests).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # sensible default population size scaled with dim but limited by budget\n        if pop_size is None:\n            # prefer at least 8*dim but keep it small relative to budget\n            self.pop_size = int(min(max(8 * self.dim, 20), max(4, self.budget // 8)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, b):\n        \"\"\"Ensure bounds vector of length dim from scalar or array-like.\"\"\"\n        b = np.asarray(b)\n        if b.size == 1:\n            return np.full(self.dim, float(b))\n        if b.size != self.dim:\n            raise ValueError(\"Bounds size does not match dimension.\")\n        return b.astype(float)\n\n    def __call__(self, func):\n        rng = self.rng\n\n        # obtain bounds from func if available, otherwise assume [-5, 5]\n        try:\n            lb = self._ensure_array_bounds(func.bounds.lb)\n            ub = self._ensure_array_bounds(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # guard against invalid bounds\n        range_vec = ub - lb\n        range_vec[range_vec <= 0] = 1.0  # fallback\n        range_norm = np.linalg.norm(range_vec)\n        max_trust = max(range_norm * 2.0, 1e-8)\n        min_trust = max(range_norm * 1e-6, 1e-12)\n\n        # initialize population\n        pop = lb + rng.rand(self.pop_size, self.dim) * range_vec\n        fvals = np.full(self.pop_size, np.inf)\n\n        # evaluate initial population (as many as budget allows)\n        evals_used = 0\n        initial_eval = min(self.pop_size, self.budget)\n        for i in range(initial_eval):\n            x = pop[i].copy()\n            fvals[i] = func(x)\n            evals_used += 1\n            # track global best\n            if fvals[i] < self.f_opt:\n                self.f_opt = fvals[i]\n                self.x_opt = x.copy()\n        # remaining individuals are initialized but unevaluated (fvals = inf)\n\n        # algorithm hyper-parameters (adapted online)\n        F_mean = 0.5\n        CR_mean = 0.9\n        trust_radius = max(range_norm * 0.1, 1e-6)  # initial trust scale (in absolute units)\n        p_levy = 0.05  # probability of performing a Lévy jump for a candidate\n        stagnation_counter = 0\n        success_in_cycle = 0\n        cycles_since_improve = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy (clipped)\n        def levy_step():\n            # draw per-dimension Cauchy, clip extreme outliers for numerical stability\n            s = rng.standard_cauchy(self.dim)\n            s = np.clip(s, -25.0, 25.0)\n            # normalize to unit vector but preserve relative component signs\n            norm = np.linalg.norm(s)\n            if norm == 0:\n                s = rng.randn(self.dim)\n                norm = np.linalg.norm(s)\n            return s / (norm + 1e-12)\n\n        # helper: project into bounds\n        def project_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # main loop: process until budget exhausted\n        generation = 0\n        while evals_used < self.budget:\n            generation += 1\n            success_in_cycle = 0\n\n            # randomize traversal order\n            order = rng.permutation(self.pop_size)\n            for idx in order:\n                if evals_used >= self.budget:\n                    break\n\n                target = pop[idx].copy()\n                f_target = fvals[idx]\n                # sample Fi and CRi around the means (small gaussian jitter) with sensible clipping\n                Fi = float(np.clip(rng.normal(F_mean, 0.15), 0.05, 1.0))\n                CRi = float(np.clip(rng.normal(CR_mean, 0.1), 0.0, 1.0))\n\n                # choose between Lévy jump and DE mutation\n                if rng.rand() < p_levy:\n                    # Lévy-centered exploration around current best\n                    if self.x_opt is None:\n                        center = target\n                    else:\n                        center = self.x_opt\n                    step = levy_step()  # unit vector heavy-tailed\n                    # scale: combine trust_radius and global range to allow both local and long jumps\n                    scale = trust_radius * (1.0 + rng.rand() * 5.0)  # sometimes large multiple\n                    candidate = center + step * scale\n                    candidate = project_to_bounds(candidate)\n                else:\n                    # DE/rand/1-like mutation with an extra bias toward the best for exploitation\n                    # select three distinct indices different from idx\n                    choices = [i for i in range(self.pop_size) if i != idx]\n                    if len(choices) < 3:\n                        # fallback random candidate if population too small\n                        candidate = lb + rng.rand(self.dim) * range_vec\n                    else:\n                        r1, r2, r3 = rng.choice(choices, 3, replace=False)\n                        donor = pop[r1] + Fi * (pop[r2] - pop[r3])\n                        # small \"best-guided\" term to encourage exploitation:\n                        if self.x_opt is not None:\n                            donor = donor + Fi * 0.1 * (self.x_opt - target)\n                        # binomial crossover\n                        crossover_mask = rng.rand(self.dim) < CRi\n                        if not np.any(crossover_mask):\n                            crossover_mask[rng.randint(self.dim)] = True\n                        candidate = target.copy()\n                        candidate[crossover_mask] = donor[crossover_mask]\n                        candidate = project_to_bounds(candidate)\n\n                # evaluate candidate if budget remains\n                if evals_used >= self.budget:\n                    break\n                f_candidate = func(candidate)\n                evals_used += 1\n\n                # selection (greedy): replace target if improved\n                improved = False\n                if f_candidate < f_target:\n                    pop[idx] = candidate\n                    fvals[idx] = f_candidate\n                    improved = True\n                    success_in_cycle += 1\n                    # move means slightly toward successful Fi/CRi (simple exponential moving average)\n                    F_mean = 0.9 * F_mean + 0.1 * Fi\n                    CR_mean = 0.9 * CR_mean + 0.1 * CRi\n                    # shrink trust radius a bit on success to focus exploitation\n                    trust_radius = max(trust_radius * 0.92, min_trust)\n                    stagnation_counter = 0\n                else:\n                    # unsuccessful -> small expansion to encourage escape\n                    trust_radius = min(trust_radius * 1.01 + 1e-12, max_trust)\n                    stagnation_counter += 1\n\n                # update global best if improved\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    # additional trust shrinking when we find a new best\n                    trust_radius = max(trust_radius * 0.8, min_trust)\n                    cycles_since_improve = 0\n                else:\n                    cycles_since_improve += 1\n\n                # adapt Lévy probability based on stagnation\n                if stagnation_counter > max(5, self.pop_size // 2):\n                    p_levy = min(0.5, p_levy + 0.02)  # encourage more long jumps\n                else:\n                    p_levy = max(0.02, p_levy * 0.995)\n\n                # small safeguard to keep means in valid ranges\n                F_mean = float(np.clip(F_mean, 0.05, 0.99))\n                CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n                trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n\n            # End of generation: trust-region local search around best (if budget remains)\n            # number of local samples proportional to dim but bounded by remaining budget\n            remaining = self.budget - evals_used\n            if remaining <= 0:\n                break\n            local_samples = min(remaining, max(1, self.dim // 2))\n            # anisotropic sigma scaling per-dimension\n            for _ in range(local_samples):\n                if evals_used >= self.budget:\n                    break\n                # sigma scales with trust_radius and per-dimension randomization\n                sigma = trust_radius * (0.3 + 0.7 * rng.rand(self.dim))\n                step = rng.randn(self.dim) * sigma\n                candidate = self.x_opt + step\n                candidate = project_to_bounds(candidate)\n                f_candidate = func(candidate)\n                evals_used += 1\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate.copy()\n                    # success: shrink trust radius to focus\n                    trust_radius = max(trust_radius * 0.6, min_trust)\n                    stagnation_counter = 0\n                    # nudge parameter means toward exploitation\n                    F_mean = 0.95 * F_mean + 0.05 * 0.5\n                    CR_mean = 0.95 * CR_mean + 0.05 * 0.9\n                else:\n                    # failure: open up trust to allow escape\n                    trust_radius = min(trust_radius * 1.08 + 1e-12, max_trust)\n                    stagnation_counter += 1\n\n            # handle strong stagnation: re-seed part of the population\n            if stagnation_counter > max(50, self.pop_size * 5):\n                # reinitialize half the population randomly (but don't evaluate them all immediately)\n                half = self.pop_size // 2\n                for k in range(half):\n                    i = rng.randint(self.pop_size)\n                    pop[i] = lb + rng.rand(self.dim) * range_vec\n                    fvals[i] = np.inf\n                # enlarge trust radius moderately to escape deep traps\n                trust_radius = min(max_trust, trust_radius * 2.0)\n                stagnation_counter = 0\n                # encourage Lévy jumps temporarily\n                p_levy = min(0.4, p_levy + 0.15)\n\n            # mild cooling/encouragement of exploitation when many successes in a generation\n            if success_in_cycle > (0.2 * self.pop_size):\n                # reduce exploration\n                p_levy = max(0.01, p_levy * 0.7)\n                trust_radius = max(min_trust, trust_radius * 0.9)\n            else:\n                # if few successes, slightly increase exploration chance\n                p_levy = min(0.5, p_levy * 1.02 + 0.001)\n\n            # safety clamp\n            trust_radius = float(np.clip(trust_radius, min_trust, max_trust))\n            F_mean = float(np.clip(F_mean, 0.05, 0.99))\n            CR_mean = float(np.clip(CR_mean, 0.0, 1.0))\n\n        # final best\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.565 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18032622703430456, 0.16661225320380113, 0.3109389778474484, 0.9201577119941742, 0.8886101347310252, 0.8964577244871049, 0.3047216862461992, 0.6125024333090708, 0.8326409671934498, 0.5380467651161717]}, "task_prompt": ""}
{"id": "194b1a4a-e5eb-4b8e-b614-712cec2b0269", "fitness": 0.6850783329292962, "name": "HybridDeLevyTrust", "description": "Adaptive hybrid: Differential Evolution with jDE-like online parameter adaptation, occasional Lévy/Cauchy long jumps for exploration, and a trust-region Gaussian local search around the best with adaptive trust radius and stagnation-driven partial re-seeding.", "code": "import numpy as np\n\nclass HybridDeLevyTrust:\n    \"\"\"\n    Hybrid heuristic optimizer combining Differential Evolution (DE),\n    occasional Lévy-flight (Cauchy) jumps for long-range exploration,\n    and a trust-region local search around the current best. Parameters\n    (F, CR, jump prob, trust radius) are adapted online based on success.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(int(seed))\n        # sensible default population size: grows with dim, but limited by budget\n        if pop_size is None:\n            default = max(6, min(40, 8 + 2 * self.dim))\n            # limit to a small fraction of the budget so evaluations remain available\n            self.pop_size = min(default, max(4, int(max(4, 0.1 * self.budget))))\n        else:\n            self.pop_size = int(pop_size)\n        # bounds are known to be [-5, 5] for BBOB; but use func.bounds if present in __call__\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _ensure_array_bounds(self, arr, lb, ub):\n        arr = np.asarray(arr, dtype=float)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        return np.minimum(np.maximum(arr, lb), ub)\n\n    def __call__(self, func):\n        # try to discover bounds from func, else default to [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item())\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure correct dimension\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, lb.flat[0] if lb.size else -5.0)\n            ub = np.full(self.dim, ub.flat[0] if ub.size else 5.0)\n\n        rng = np.random.default_rng()  # use new style RNG for local operations\n\n        range_vec = ub - lb\n        # initialize population uniformly in bounds\n        pop_size = self.pop_size\n        X = rng.uniform(lb, ub, size=(pop_size, self.dim))\n        f_vals = np.full(pop_size, np.inf)\n\n        evals = 0\n        # evaluate initial population as budget allows\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            xi = X[i]\n            try:\n                f = func(xi)\n            except Exception:\n                f = np.inf\n            f_vals[i] = f\n            evals += 1\n\n        # initialize best\n        best_idx = int(np.nanargmin(f_vals))\n        best_f = float(f_vals[best_idx]) if np.isfinite(f_vals[best_idx]) else np.inf\n        best_x = X[best_idx].copy() if np.isfinite(best_f) else rng.uniform(lb, ub, size=self.dim)\n\n        self.f_opt = best_f\n        self.x_opt = best_x.copy()\n\n        # algorithm hyper-parameters\n        F_mean = 0.6      # mean mutation factor\n        CR_mean = 0.3     # mean crossover rate\n        p_jump = 0.06     # base probability for Lévy/Cauchy jump\n        trust_radius = 0.15 * np.mean(range_vec)  # initial trust radius (absolute)\n        trust_min = 1e-6 * np.mean(range_vec)\n        trust_max = 2.0 * np.mean(range_vec)\n\n        # adaptation parameters\n        c_mean = 0.1  # learning rate for means\n        stagnation_evals = 0\n        best_since_last_reset = best_f\n        no_improve_counter = 0\n        max_stagnate = max(50, int(0.05 * self.budget))\n\n        generation = 0\n\n        # helper: generate Levy-like heavy-tailed step using Cauchy but clipped\n        def levy_step(scale):\n            # standard Cauchy in each dimension, scaled by scale\n            step = rng.standard_cauchy(self.dim) * scale\n            # clip extreme outliers to prevent numerical blow-up\n            max_clip = 10.0 * np.mean(range_vec)\n            step = np.clip(step, -max_clip, max_clip)\n            return step\n\n        # main loop\n        while evals < self.budget:\n            generation += 1\n            succ_F = []\n            succ_CR = []\n            succ_weights = []  # improvement magnitude as weight\n\n            # per-individual DE operations\n            for i in range(pop_size):\n                if evals >= self.budget:\n                    break\n\n                # sample Fi and CRi around their means (jDE-like)\n                # Fi from a Cauchy around F_mean (promotes heavy tails), clipped\n                Fi = F_mean + 0.1 * rng.standard_cauchy()\n                Fi = float(np.clip(Fi, 0.05, 0.95))\n                CRi = float(np.clip(rng.normal(loc=CR_mean, scale=0.15), 0.0, 1.0))\n\n                # decide whether to do a long Lévy jump centered on the best (exploration)\n                if rng.random() < p_jump:\n                    # heavy-tailed jump around best\n                    scale = 0.5 * trust_radius  # scale relative to trust radius\n                    donor = best_x + levy_step(scale)\n                    # small random bias by an individual's vector to keep diversity\n                    donor = donor + Fi * (X[rng.integers(pop_size)] - X[rng.integers(pop_size)])\n                else:\n                    # DE/rand/1 with an occasional best-influence\n                    idxs = np.arange(pop_size)\n                    idxs = idxs[idxs != i]\n                    a, b, c = rng.choice(idxs, size=3, replace=False)\n                    donor = X[a] + Fi * (X[b] - X[c])\n                    # sometimes pull towards best for exploitation (controlled by trust radius)\n                    if rng.random() < 0.2:\n                        donor = donor + Fi * (best_x - X[i]) * (trust_radius / (np.mean(range_vec) + 1e-12))\n\n                # binomial crossover\n                trial = X[i].copy()\n                jrand = rng.integers(self.dim)\n                mask = rng.random(self.dim) < CRi\n                mask[jrand] = True\n                trial[mask] = donor[mask]\n\n                # trust-region inspired small perturbation: occasionally perturb trial slightly\n                if rng.random() < 0.1:\n                    trial += rng.normal(0, 0.05 * trust_radius, size=self.dim)\n\n                # project to bounds\n                trial = self._ensure_array_bounds(trial, lb, ub)\n\n                # evaluate trial\n                try:\n                    f_trial = func(trial)\n                except Exception:\n                    f_trial = np.inf\n                evals += 1\n\n                # selection\n                if f_trial < f_vals[i]:\n                    # successful\n                    improvement = max(1e-12, f_vals[i] - f_trial if np.isfinite(f_vals[i]) else 0.0)\n                    succ_F.append(Fi)\n                    succ_CR.append(CRi)\n                    succ_weights.append(improvement)\n                    X[i] = trial\n                    f_vals[i] = f_trial\n\n                    # update global best\n                    if f_trial < best_f:\n                        best_f = float(f_trial)\n                        best_x = trial.copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        no_improve_counter = 0\n                    else:\n                        no_improve_counter += 1\n                else:\n                    no_improve_counter += 1\n\n                # safety: if budget exhausted break\n                if evals >= self.budget:\n                    break\n\n            # end of generation: adapt mean parameters if successes occurred\n            if succ_F:\n                weights = np.array(succ_weights, dtype=float)\n                weights /= (weights.sum() + 1e-12)\n                # weighted Lehmer-like mean: gives preference to larger F that succeeded\n                succ_F = np.array(succ_F)\n                lehmer = (weights * succ_F**2).sum() / (weights * succ_F).sum()\n                new_F_mean = (1 - c_mean) * F_mean + c_mean * float(lehmer)\n                F_mean = float(np.clip(new_F_mean, 0.05, 0.95))\n\n                succ_CR = np.array(succ_CR)\n                new_CR_mean = (1 - c_mean) * CR_mean + c_mean * float((weights * succ_CR).sum())\n                CR_mean = float(np.clip(new_CR_mean, 0.0, 1.0))\n\n            else:\n                # small drift to encourage exploration if no success\n                F_mean = float(np.clip(F_mean + 0.01 * (rng.random() - 0.5), 0.05, 0.95))\n                CR_mean = float(np.clip(CR_mean + 0.02 * (rng.random() - 0.5), 0.0, 1.0))\n\n            # trust-region local search around best: sample a handful of gaussian candidates\n            remaining = self.budget - evals\n            if remaining > 0:\n                # make number of local samples small and adaptive\n                n_local = min(max(1, self.dim // 2), remaining)\n                local_success = False\n                for _ in range(n_local):\n                    if evals >= self.budget:\n                        break\n                    # anisotropic noise: per-dim scaling around trust_radius\n                    noise = rng.normal(0, 1.0, size=self.dim) * (trust_radius * (0.5 + rng.random(self.dim)))\n                    cand = best_x + noise\n                    cand = self._ensure_array_bounds(cand, lb, ub)\n                    try:\n                        f_cand = func(cand)\n                    except Exception:\n                        f_cand = np.inf\n                    evals += 1\n                    if f_cand < best_f:\n                        best_f = float(f_cand)\n                        best_x = cand.copy()\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n                        local_success = True\n                        # reward: shrink trust region to intensify local search\n                        trust_radius = max(trust_min, 0.85 * trust_radius)\n                        # adjust means toward exploitation\n                        F_mean = float(np.clip(0.9 * F_mean + 0.1 * 0.4, 0.05, 0.95))\n                        CR_mean = float(np.clip(0.9 * CR_mean + 0.1 * 0.8, 0.0, 1.0))\n                        break  # accept first improving local sample (greedy)\n                if not local_success:\n                    # expand trust radius to encourage escape if local attempts fail\n                    trust_radius = min(trust_max, 1.08 * trust_radius)\n\n            # stagnation handling\n            if best_f < best_since_last_reset - 1e-12:\n                best_since_last_reset = best_f\n                stagnation_evals = 0\n            else:\n                stagnation_evals += (sum(1 for _ in succ_F) if succ_F is not None else 1)\n\n            # if long stagnation: increase jump probability and diversity slightly\n            if no_improve_counter > max_stagnate:\n                p_jump = min(0.5, p_jump * 1.5)\n                # re-seed half the population (conservative)\n                num_reseed = max(1, pop_size // 2)\n                for ri in rng.choice(np.arange(pop_size), size=num_reseed, replace=False):\n                    if evals >= self.budget:\n                        break\n                    X[ri] = rng.uniform(lb, ub)\n                    try:\n                        f_vals[ri] = func(X[ri])\n                    except Exception:\n                        f_vals[ri] = np.inf\n                    evals += 1\n                # slightly enlarge trust radius to escape\n                trust_radius = min(trust_max, trust_radius * 1.5)\n                no_improve_counter = 0  # reset counter after aggressive action\n\n            else:\n                # slowly decay p_jump toward baseline if progress is steady\n                p_jump = max(0.02, p_jump * 0.995)\n\n            # small safety: ensure best is synced with population if possible\n            # occasionally replace a worst individual with the best to spread best info\n            if rng.random() < 0.05 and evals < self.budget:\n                worst_idx = int(np.argmax(f_vals))\n                X[worst_idx] = best_x.copy()\n                # do not evaluate — we already know best_f\n\n            # stop early if reached a very good solution (optional threshold)\n            if best_f <= 1e-12:\n                break\n\n        # final store and return\n        self.f_opt = float(best_f)\n        self.x_opt = best_x.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDeLevyTrust scored 0.685 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.22775686294737407, 0.19980188080104944, 0.7376302329447695, 0.9588712963191248, 0.9223486130166806, 0.9108422377386235, 0.32554947460796724, 0.7549734485703083, 0.9279978867468213, 0.8850113956002439]}, "task_prompt": ""}

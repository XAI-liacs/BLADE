{"id": "c30759dc-e6cd-4e79-b18c-70f689ea5675", "fitness": 0.4581463064337804, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that uses randomized directional local searches, orthogonal refinements and occasional Lévy jumps to explore and exploit the continuous search space.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        # ensure lb/ub are full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        remaining = self.budget\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure x is numpy array of correct shape\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.458 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13898661099180287, 0.16798988399364267, 0.5542045673674655, 0.95854611327996, 0.23700198397254146, 0.9527000268023871, 0.24265382283680903, 0.41100070527939814, 0.7475516863499366, 0.1708276634638597]}, "task_prompt": ""}
{"id": "d74b00d0-21ab-48f5-a72b-95e21217592b", "fitness": 0.4613095700513473, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional search with orthogonal refinement, recombination and occasional Cauchy (Lévy-like) escapes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional tuning:\n    - pop_size: population size (default scales modestly with dim)\n    - seed: rng seed\n    - init_pop: if True will evaluate an initial population; else starts with random sampling on the fly\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None, init_pop=True):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows slowly with dimension\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.init_pop = bool(init_pop)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds and ensure arrays of correct shape\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x.copy()\n\n        # trivial fallback\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # base scale per-dimension and a scalar base sigma for step-size\n        base_scale = 0.2 * (ub - lb)           # typical exploratory step per-dim\n        base_sigma_scalar = max(1e-12, float(np.mean(base_scale)))  # used to cap/inflate sigma\n\n        # initialize population (positions, fitnesses, and scalar sigmas)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        n_init = min(self.pop_size, remaining) if self.init_pop else 0\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0_clipped = callf(x0)\n            pop.append(x0_clipped)\n            pop_f.append(f0)\n            # initial sigma randomized around base_sigma_scalar\n            pop_sigma.append(base_sigma_scalar * (0.6 + 0.8 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If no initial population (either init_pop=False or extremely small budget), create at least one evaluated sample\n        if len(pop) == 0 and remaining > 0:\n            x0 = np.random.uniform(lb, ub)\n            f0, x0_clipped = callf(x0)\n            pop.append(x0_clipped)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma_scalar * (0.6 + 0.8 * np.random.rand()))\n\n        # Keep population sizes consistent (if we have more budget we can grow to pop_size gradually)\n        # We will add random evaluated individuals occasionally when budget and population allow.\n        # Main loop\n        while remaining > 0:\n            # ensure arrays are available\n            n_pop = len(pop)\n            if n_pop == 0:\n                # should not happen, but guard\n                x = np.random.uniform(lb, ub)\n                callf(x)\n                continue\n\n            # selection: small tournament (k=2 or 3)\n            k = min(3, n_pop)\n            if n_pop >= k:\n                inds = np.random.choice(n_pop, k, replace=False)\n            else:\n                inds = np.random.choice(n_pop, k, replace=True)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            improved = False\n\n            # sample a random direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length (alpha can be negative; take magnitude but keep direction sign)\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            if abs(alpha) < 1e-16:\n                alpha = sigma * (0.5 + 0.5 * np.random.rand())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # accept and increase sigma moderately\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.25, np.mean(ub - lb))\n                improved = True\n\n            if improved:\n                # small chance to try a quick orthogonal refine after success\n                if remaining > 0 and np.random.rand() < 0.3:\n                    r = np.random.randn(self.dim)\n                    r = r - np.dot(r, d) * d\n                    nr = np.linalg.norm(r)\n                    if nr > 1e-12:\n                        r = r / nr\n                        x_try2 = np.clip(pop[parent_i] + 0.4 * pop_sigma[parent_i] * r, lb, ub)\n                        if remaining > 0:\n                            f_try2, x_try2 = callf(x_try2)\n                            if f_try2 < pop_f[parent_i]:\n                                pop[parent_i] = x_try2\n                                pop_f[parent_i] = f_try2\n                                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.15, np.mean(ub - lb))\n                continue  # go to next main iteration\n\n            # local backtracking / multi-scale refinement along the same direction (few tries)\n            for frac in (0.5, 0.25, -0.25, -0.5):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal diversification: perturb orthogonally to direction d\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    step_mag = 0.6 * sigma\n                    x_try = np.clip(x_parent + step_mag * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                        improved = True\n            if improved:\n                continue\n\n            # occasional Lévy-like jump (Cauchy) to escape local minima\n            if np.random.rand() < 0.08 and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                # reduce effect of extreme outliers by scaling with a robust percentile\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.25 * (ub - lb)  # bigger jump\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # replace worst if better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if n_pop >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = 0.6 + 0.3 * np.random.rand()  # bias toward best\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.02 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        improved = True\n                    else:\n                        # try to inject into population by replacing worst if it's better\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma_scalar * 0.6\n\n            # adapt parent sigma on failure (if still not improved)\n            if not improved:\n                pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation: add/evaluate a random sample and replace worst\n            if remaining > 0 and np.random.rand() < 0.03:\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                worst_i = int(np.argmax(pop_f))\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma_scalar * (0.7 + 0.6 * np.random.rand())\n\n            # if population smaller than target and budget allows, grow by adding evaluated random individuals\n            if remaining > 0 and len(pop) < self.pop_size and np.random.rand() < 0.2:\n                x_add = np.random.uniform(lb, ub)\n                f_add, x_add = callf(x_add)\n                pop.append(x_add)\n                pop_f.append(f_add)\n                pop_sigma.append(base_sigma_scalar * (0.6 + 0.8 * np.random.rand()))\n\n            # enforce population arrays correct length invariants\n            # (this can happen if we replaced worst via instantaneous calculations earlier)\n            # Ensure pop_f and pop_sigma lengths match pop length\n            if len(pop_f) != len(pop):\n                # re-evaluate or fill small mismatch conservatively (shouldn't happen often)\n                # fill missing fitnesses by evaluating positions (only if budget available)\n                for i in range(len(pop_f), len(pop)):\n                    if remaining <= 0:\n                        break\n                    f_i, _ = callf(pop[i])\n                    pop_f.append(f_i)\n                    pop_sigma.append(base_sigma_scalar * (0.6 + 0.8 * np.random.rand()))\n                # if there are extra fitnesses, truncate\n                if len(pop_f) > len(pop):\n                    pop_f = pop_f[:len(pop)]\n                    pop_sigma = pop_sigma[:len(pop)]\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.461 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08104384996740033, 0.16524590165289121, 0.4923222140072989, 0.963407700074005, 0.37240329055118826, 0.8127856726002093, 0.24056667455953418, 0.3851236244310402, 0.9423856686894739, 0.15781110398043163]}, "task_prompt": ""}
{"id": "d105470d-6c23-436a-89ea-6ddb15df1fb5", "fitness": 0.5441261749787886, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining directional local probes, orthogonal refinements, recombination and occasional Lévy jumps to robustly explore and exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size; by default scales modestly with dim\n    - seed: optional random seed for reproducibility\n\n    The solver respects func.bounds.lb and func.bounds.ub. It will not call\n    func more times than self.budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows slowly with dimension\n            pop_size = int(max(4, min(30, 4 + 2 * np.sqrt(self.dim))))\n        self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds as arrays of length dim\n        lb_raw = np.atleast_1d(func.bounds.lb)\n        ub_raw = np.atleast_1d(func.bounds.ub)\n        # Expand scalar bounds if necessary\n        if lb_raw.size == 1:\n            lb = np.full(self.dim, float(lb_raw.item()))\n        else:\n            lb = lb_raw.astype(float)\n        if ub_raw.size == 1:\n            ub = np.full(self.dim, float(ub_raw.item()))\n        else:\n            ub = ub_raw.astype(float)\n\n        if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n            # try to broadcast or truncate/pad if necessary\n            lb = np.resize(lb, self.dim).astype(float)\n            ub = np.resize(ub, self.dim).astype(float)\n\n        # helper evaluation wrapper that tracks remaining budget and best\n        remaining = int(self.budget)\n\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(-1)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # quick fallback: if budget is extremely small, do pure random search\n        if self.budget <= 2:\n            # do as many random evaluations as budget allows\n            for _ in range(self.budget):\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (may be smaller if budget limits)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n\n        # try to create up to pop_size members but don't exceed budget - keep reserves\n        target_pop = min(self.pop_size, max(1, remaining // 2))\n        for _ in range(target_pop):\n            if remaining <= 0:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n\n        # if population couldn't be created (very small budget), do random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop: directional local searches, orthogonal tries, Lévy jumps, recombination\n        while remaining > 0:\n            n_pop = len(pop)\n            # small tournament selection for parent\n            k = min(3, n_pop)\n            inds = self.rng.choice(n_pop, size=k, replace=False)\n            # pick best among sampled\n            parent_i = inds[int(np.argmin([pop_f[i] for i in inds]))]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample random direction (unit)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and increase sigma slightly\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation\n            if remaining > 0:\n                r = self.rng.randn(self.dim)\n                # remove component along d\n                proj = np.dot(r, d)\n                r = r - proj * d\n                rn = np.linalg.norm(r)\n                if rn < 1e-12:\n                    # fallback small gaussian\n                    r = self.rng.randn(self.dim)\n                    rn = np.linalg.norm(r) + 1e-12\n                r = r / rn\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    continue\n\n            # occasional Lévy-like jump to escape local basins\n            if self.rng.rand() < 0.08 and remaining > 0:\n                step = self.rng.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                # replace worst if improved\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if remaining > 0 and n_pop >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = 0.08 * sigma * self.rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n            # adapt parent sigma on failure (if not improved so far)\n            # if the parent wasn't updated in this iteration, shrink its sigma\n            if pop_f[parent_i] > self.f_opt:  # crude indicator that parent not improved relative to global\n                pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n            # Occasionally add an extra candidate around best to intensify search\n            if remaining > 0 and self.rng.rand() < 0.03:\n                best_i = int(np.argmin(pop_f))\n                local_sigma = pop_sigma[best_i] * 0.5\n                x_new = np.clip(pop[best_i] + local_sigma * self.rng.randn(self.dim), lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[best_i]:\n                    pop[best_i] = x_new\n                    pop_f[best_i] = f_new\n                    pop_sigma[best_i] = max(pop_sigma[best_i] * 1.05, 1e-12)\n\n            # maintain population size if budget still allows (rare)\n            if len(pop) < self.pop_size and remaining > 0 and self.rng.rand() < 0.1:\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n\n        # finished or exhausted budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.544 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14589801097636945, 0.15990056463419666, 0.8524819884947257, 0.9663741190546884, 0.6795887305194026, 0.9518117362594078, 0.3318262611071042, 0.4928943052462159, 0.7397593646158988, 0.12072666887987671]}, "task_prompt": ""}
{"id": "7025995c-8f97-492b-94ce-6409bd37a46b", "fitness": 0.3763821321310344, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that combine randomized directional local searches, orthogonal refinements and occasional heavy‑tailed Lévy-like jumps to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size; by default scales modestly with dim\n    - seed: optional RNG seed for reproducibility\n\n    Notes:\n    - The algorithm always clips candidate points to the provided bounds before evaluating.\n    - It never calls func more than `budget` times.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        self.pop_size = int(pop_size)\n        # RNG: use RandomState for reproducibility\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        # read bounds and ensure arrays of correct shape\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # safety: ensure lb/ub shapes\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        # initialize best trackers\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        remaining = int(self.budget)\n\n        # evaluation wrapper: clips, evaluates (if budget left), updates trackers\n        def callf(x):\n            nonlocal remaining\n            # do not call func when budget exhausted\n            if remaining <= 0:\n                return float(np.inf), None\n            x = np.asarray(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                # store a copy\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget extremely small, fallback to simple randomized search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (bounded by remaining)\n        n_init = min(self.pop_size, remaining)\n        pop = []\n        pop_f = []\n        pop_sigma = []  # per-individual step scale (scalar)\n        # base_sigma: relative to bounds width\n        base_sigma_vec = 0.08 * (ub - lb)  # vector scale used to initialize sigma\n        base_sigma = max(1e-12, np.mean(base_sigma_vec))\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x = self.rng.uniform(lb, ub)\n            f, x_clipped = callf(x)\n            pop.append(x_clipped)\n            pop_f.append(f)\n            # initialize sigma as fraction of bounds with some diversity\n            s = base_sigma * (0.5 + self.rng.rand())\n            pop_sigma.append(float(s))\n\n        # If budget too small and no population created, do random search until exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                f, x = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # If partially filled population (due to small budget), pad with copies of best\n        pop = np.array(pop, dtype=float)\n        pop_f = np.array(pop_f, dtype=float)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n        if len(pop) < self.pop_size:\n            # duplicate best individuals to reach pop_size (no extra evaluations)\n            needed = self.pop_size - len(pop)\n            best_idx = int(np.argmin(pop_f))\n            to_add = np.tile(pop[best_idx], (needed, 1))\n            pop = np.vstack([pop, to_add])\n            pop_f = np.concatenate([pop_f, np.full(needed, pop_f[best_idx])])\n            pop_sigma = np.concatenate([pop_sigma, np.full(needed, pop_sigma[best_idx])])\n\n        # main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        # parameters\n        tournament_k = min(3, max(2, int(np.sqrt(self.pop_size))))\n        frac_list = [0.5, 0.25, 0.1]  # backtracking fractions\n        levy_prob = 0.08\n        rejuvenate_prob = 0.02\n        orthogonal_scale = 0.6\n        recomb_noise_scale = 0.01 * (ub - lb)\n        max_sigma = np.mean(ub - lb)\n        min_sigma = 1e-12\n\n        # main iterative search until budget exhausted\n        while remaining > 0:\n            # small tournament to pick parent index\n            inds = self.rng.choice(self.pop_size, size=tournament_k, replace=False)\n            values = pop_f[inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    # fallback to unit basis random\n                    d = np.zeros(self.dim); d[self.rng.randint(self.dim)] = 1.0\n                    nd = 1.0\n            d = d / nd\n\n            # stochasticized step length scaled by sigma and a small random multiplier\n            alpha = sigma * max(1e-12, 1.0 + 0.2 * self.rng.randn())\n\n            # primary directional trial\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.12)\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in frac_list:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.05)\n                    improved = True\n                    break\n                else:\n                    # shrink sigma slightly on failing smaller steps\n                    pop_sigma[parent_i] = max(min_sigma, pop_sigma[parent_i] * 0.95)\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            r = r - np.dot(r, d) * d  # make r orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + orthogonal_scale * sigma * r, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.1)\n                    continue\n                else:\n                    # slightly expand sigma to encourage exploration orthogonally if nothing improved\n                    pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.08)\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < levy_prob and remaining > 0:\n                step = self.rng.standard_cauchy(self.dim)\n                denom = (np.median(np.abs(step)) + 1e-6)\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(min_sigma, pop_sigma[worst_i] * 0.6)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best with small noise (no extra eval yet)\n            best2_idx = np.argsort(pop_f)[:2]\n            a, b = best2_idx[0], best2_idx[1]\n            mix = 0.5 + 0.1 * (self.rng.rand() - 0.5)\n            x_recomb = np.clip(mix * pop[a] + (1.0 - mix) * pop[b] + self.rng.randn(self.dim) * recomb_noise_scale, lb, ub)\n            if remaining <= 0:\n                break\n            f_recomb, x_recomb = callf(x_recomb)\n            # try to inject recombined candidate: replace worst if better\n            worst_i = int(np.argmax(pop_f))\n            if f_recomb < pop_f[worst_i]:\n                pop[worst_i] = x_recomb\n                pop_f[worst_i] = f_recomb\n                pop_sigma[worst_i] = max(min_sigma, base_sigma * 0.5)\n\n            # adapt parent sigma on failure (encourage exploration if parent hasn't improved)\n            # if parent's f still equals previous (no improvement), slightly increase sigma occasionally\n            if self.rng.rand() < 0.25:\n                pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.03)\n            else:\n                pop_sigma[parent_i] = max(min_sigma, pop_sigma[parent_i] * 0.97)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < rejuvenate_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if x_new is not None:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.5 + self.rng.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.376 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14328951512980148, 0.16420667626655883, 0.4944123380087584, 0.955561243708195, 0.1607633780878539, 0.9148061573185144, 0.22120053725844002, 0.3842552329376514, 0.18162015764375505, 0.14370608495081538]}, "task_prompt": ""}
{"id": "aa6fcc47-ef45-4519-bcca-6a4c4e2afa07", "fitness": 0.5320988764201007, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step search combining randomized directional local probes, orthogonal refinements, recombination and occasional Lévy (Cauchy) jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None, chosen based on dim)\n    - seed: optional random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows slowly with dimension\n            self.pop_size = max(4, min(24, int(4 + 1.5 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds as arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():  # scalar bounds -> expand\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.shape == ():\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        assert lb.size == self.dim and ub.size == self.dim\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                # budget exhausted: return a sentinel large value and copy\n                return float(np.inf), np.asarray(x, dtype=float).copy()\n            x = np.asarray(x, dtype=float)\n            x = np.clip(x, lb, ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If no budget, return immediately\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population by sampling up to pop_size or until budget runs out\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        span = ub - lb\n        base_sigma = max(1e-8, 0.25 * np.mean(span))  # initial scale per individual\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # small randomization in sigma to encourage diversity\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * np.random.rand()))\n\n        # if no population could be created due to tiny budget, do pure random search until budget exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return self.f_opt, self.x_opt\n\n        # main loop\n        while remaining > 0:\n            # ensure pop lists are consistent\n            n_pop = len(pop)\n            if n_pop == 0:\n                break\n\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, n_pop)\n            inds = np.random.choice(n_pop, k, replace=False)\n            # choose the best among tournament as parent\n            parent_i = inds[int(np.argmin([pop_f[i] for i in inds]))]\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                continue\n            d = d / nd\n\n            # stochastic step-length (alpha) scaled by sigma, keep positive\n            alpha = max(1e-12, sigma * (1.0 + 0.5 * np.random.randn()))\n\n            # 1) Primary directional trial\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n            success = False\n            if f_try < f_parent:\n                # accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(np.mean(span), sigma * 1.15)\n                success = True\n                # small local exploitation by trying a bit further in same direction occasionally\n                if remaining > 0 and np.random.rand() < 0.25:\n                    extra_step = 0.5 * alpha\n                    x_try2 = np.clip(pop[parent_i] + extra_step * d, lb, ub)\n                    f_try2, x_try2 = callf(x_try2)\n                    if f_try2 < pop_f[parent_i]:\n                        pop[parent_i] = x_try2\n                        pop_f[parent_i] = f_try2\n                        pop_sigma[parent_i] = min(np.mean(span), pop_sigma[parent_i] * 1.1)\n                continue  # go to next iteration\n\n            # 2) Local backtracking / small-step refinement along direction (few tries)\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    success = True\n                    break\n            if success:\n                continue\n\n            # 3) orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # project out component parallel to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 0:\n                    r = r / nr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(np.mean(span), sigma * 1.1)\n                        continue\n\n            # 4) occasional Lévy-like jump to escape local basins\n            if remaining > 0 and np.random.rand() < 0.08:\n                # Cauchy-like heavy-tailed vector (using inverse CDF via tan)\n                u = np.random.rand(self.dim)\n                step = np.tan(np.pi * (u - 0.5))\n                denom = np.median(np.abs(step)) + 1e-12\n                step = step / denom\n                scale_vec = 0.18 * span  # scale relative to search span\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace worst if better\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                # if jump improved parent, accept\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(np.mean(span), sigma * 1.2)\n                # continue to next main loop iteration\n                continue\n\n            # 5) recombination exploitation: mix two best and small noise\n            if remaining > 0 and len(pop) >= 2 and np.random.rand() < 0.25:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = 0.4 + 0.2 * np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = 0.05 * np.mean(span) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace parent if improved, otherwise maybe replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(np.mean(span), sigma * 1.05)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                continue\n\n            # 6) If none of the above improved, do a small Gaussian probe and adapt sigma downwards\n            if remaining > 0:\n                x_try = np.clip(x_parent + 0.6 * sigma * np.random.randn(self.dim), lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(np.mean(span), sigma * 1.08)\n                else:\n                    # failure: slightly reduce sigma to encourage finer search\n                    pop_sigma[parent_i] = max(sigma * 0.97, 1e-12)\n\n            # 7) occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # keep population size bounded and remove extremely bad individuals if budget is low\n            if remaining < 0.02 * self.budget and len(pop) > max(2, int(self.pop_size / 2)):\n                # drop the worst one occasionally to focus budget\n                worst_i = int(np.argmax(pop_f))\n                del pop[worst_i], pop_f[worst_i], pop_sigma[worst_i]\n\n        # finished budget or loop exit\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.532 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14843776003239462, 0.16199459842975805, 0.8543926761764058, 0.9630146654825507, 0.583230018815704, 0.9572954778619854, 0.27475799835427994, 0.5040585090700125, 0.713151696941468, 0.16065536303644856]}, "task_prompt": ""}
{"id": "ebf944fc-1943-45e6-84f1-1eb8e2698099", "fitness": 0.5213177910160833, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based directional local searches with adaptive step-sizes, orthogonal refinements and occasional Lévy-like jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest scaling with dimension, keep population small relative to budget\n            self.pop_size = int(min(40, max(4, 4 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # best seen\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds: func.bounds.lb / ub may be scalar or array-like\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        # safety: ensure shapes\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.resize(lb, self.dim)\n            ub = np.resize(ub, self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            x = np.clip(x, lb, ub)\n            if remaining <= 0:\n                # budget exhausted, do not call function\n                return np.inf, x\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget extremely small, do uniform random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initial population: ensure we don't request more evaluations than budget\n        pop_size = min(self.pop_size, remaining)\n        pop = np.random.uniform(lb, ub, size=(pop_size, self.dim))\n        pop_f = np.full(pop_size, np.inf, dtype=float)\n        # initial sigmas (step sizes) scaled to problem span\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n        pop_sigma = np.full(pop_size, base_sigma, dtype=float)\n\n        # evaluate initial population (one eval each)\n        for i in range(pop_size):\n            if remaining <= 0:\n                break\n            pop_f[i], pop[i] = callf(pop[i])\n\n        # if no population could be evaluated due to tiny budget, fallback to random sampling\n        if pop.size == 0 or remaining <= 0:\n            # maybe we evaluated some initial points; just return best\n            return self.f_opt, self.x_opt\n\n        # algorithm hyper-parameters\n        success_inc = 1.20  # multiply sigma on success\n        fail_dec = 0.85     # multiply sigma on failure\n        sigma_min = 1e-10\n        sigma_max = 1e2 * base_sigma\n\n        stagnation = 0\n        iter_count = 0\n\n        # main loop\n        while remaining > 0:\n            iter_count += 1\n\n            # tournament parent selection (small tournament)\n            k = min(3, pop.shape[0])\n            inds = np.random.choice(pop.shape[0], k, replace=False)\n            # choose best among tournament by current value\n            best_tourn = inds[int(np.argmin(pop_f[inds]))]\n            parent_i = best_tourn\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction and normalize\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # rare degenerate, skip this iteration\n                continue\n            d = d / nd\n\n            # primary directional trial with stochastic step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())  # stochasticized length\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # accept improvement, increase sigma modestly\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * success_inc, sigma_max)\n                stagnation = 0\n                continue\n            else:\n                # directional failure: prepare for backtracking and orthogonal tries\n                pop_sigma[parent_i] = max(sigma * fail_dec, sigma_min)\n\n            # local backtracking / small-step refinement along the direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * success_inc, sigma_max)\n                    improved = True\n                    stagnation = 0\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d: r_orth = r - (r·d) d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                orth_step = 0.6 * sigma * (1.0 + 0.3 * np.random.randn())\n                x_try = np.clip(x_parent + orth_step * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * success_inc, sigma_max)\n                        stagnation = 0\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # sample Cauchy-ish scalar to get heavy tails\n                # use standard Cauchy: tan(pi*(u-0.5))\n                u = np.random.rand()\n                cauchy_scalar = np.tan(np.pi * (u - 0.5))\n                # robust denom to avoid extremely huge jumps, but keep heavy-tail\n                mag = sigma * (1.0 + 2.0 * np.abs(cauchy_scalar))\n                v = np.random.randn(self.dim)\n                nv = np.linalg.norm(v)\n                if nv > 1e-12:\n                    v = v / nv\n                    step = mag * v\n                    step = np.clip(step, - (ub - lb) * 2.0, (ub - lb) * 2.0)\n                    x_try = np.clip(x_parent + step, lb, ub)\n                    if remaining > 0:\n                        f_try, x_try = callf(x_try)\n                        # If jump is good, replace worst; else maybe keep as candidate\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = max(min(mag, sigma_max), sigma_min)\n                            stagnation = 0\n                            continue\n                        elif f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5\n                            # no improvement to parent; continue main loop\n                            stagnation += 1\n                            continue\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0 and pop.shape[0] >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.uniform(0.2, 0.8)\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # small gaussian mutation proportional to parent's sigma\n                noise = 0.1 * np.random.randn(self.dim) * (pop_sigma[parent_i] if pop_sigma[parent_i] > 0 else base_sigma)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if mixed child improves parent, replace parent, else maybe replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * success_inc, sigma_max)\n                        stagnation = 0\n                        continue\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5\n                            stagnation += 1\n\n            # adapt parent sigma on failure (mild shrink already applied earlier)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * fail_dec, sigma_min)\n\n            # occasional population rejuvenation if stagnation\n            if stagnation >= 20 and remaining > 0:\n                # replace worst with a random sample to reintroduce diversity\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * 0.5\n                # reset stagnation measure a little\n                stagnation = 0\n\n            # small chance to inject a bit of Gaussian noise into some individuals (exploration)\n            if np.random.rand() < 0.03 and remaining > 0:\n                idx = np.random.randint(0, pop.shape[0])\n                sigma_i = pop_sigma[idx]\n                perturb = np.random.randn(self.dim) * (0.3 * sigma_i)\n                x_new = np.clip(pop[idx] + perturb, lb, ub)\n                if remaining > 0:\n                    f_new, x_new = callf(x_new)\n                    if f_new < pop_f[idx]:\n                        pop[idx] = x_new\n                        pop_f[idx] = f_new\n                        pop_sigma[idx] = min(sigma_i * success_inc, sigma_max)\n\n            # increment stagnation if no global improvement happened this iteration\n            # (self.f_opt was updated by callf if any improvement occurred)\n            # We track local improvement by comparing parent fitness to global best is already handled.\n            stagnation += 1\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.521 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1413354243057845, 0.16402220623942576, 0.8217300066039275, 0.9509913028254607, 0.5757028119561929, 0.9017689945409786, 0.25759143882865543, 0.4862026222082517, 0.7382729758812006, 0.17556012677095667]}, "task_prompt": ""}
{"id": "90ce3fff-96e3-48a9-8eae-d8f2b06f41b8", "fitness": 0.5190887477492165, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based search combining adaptive directional local searches, orthogonal refinements, occasional Lévy jumps and light recombination to explore and exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None, chosen based on dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size if pop_size is not None else max(6, min(20, 4 * self.dim))\n        self.rng = np.random.RandomState(seed)\n\n        # Will be populated at run-time\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds handling: support scalar or array bounds\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        # Enforce expected bounds [-5,5] typically, but use provided ones\n        lb = lb.copy()\n        ub = ub.copy()\n\n        # Budget tracking\n        remaining = int(self.budget)\n\n        # local helper to call func, clip to bounds, and track best; ensures we never exceed budget\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = float(func(x))\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, do simple uniform random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n        if remaining < 2 * self.dim:\n            # quick random sampling\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population (may be smaller than desired if budget limits)\n        pop_size = min(self.pop_size, max(1, remaining))  # ensure at least 1 if budget allows\n        pop_x = np.zeros((pop_size, self.dim), dtype=float)\n        pop_f = np.full(pop_size, np.inf, dtype=float)\n        pop_sigma = np.zeros(pop_size, dtype=float)\n\n        # base_sigma based on search range scale\n        base_sigma = 0.1 * np.mean(ub - lb)\n        for i in range(pop_size):\n            if remaining <= 0:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop_x[i] = x0\n            pop_f[i] = f0\n            # per-individual adaptive step s.d.\n            pop_sigma[i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n\n        # If no population was created (should be rare), fallback to random search\n        if np.isfinite(pop_f).sum() == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: directional local searches, orthogonal tries, Lévy jumps, recombination & rejuvenation\n        stagnation_counter = 0\n        best_since_rejuv = self.f_opt\n\n        # tuning knobs\n        p_levy = 0.06  # baseline probability of attempting a Lévy jump per iteration\n        p_recomb = 0.25\n        max_backtracks = 3\n        orthogonal_factor = 0.6\n        levy_scale_fraction = 0.2\n\n        while remaining > 0:\n            # pick a parent via small tournament\n            tour_size = min(3, pop_size)\n            tour_idx = self.rng.choice(pop_size, size=tour_size, replace=False)\n            parent_i = tour_idx[np.argmin(pop_f[tour_idx])]\n            x_parent = pop_x[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d /= nd\n\n            # stochasticized step-length (log-normal-like noise)\n            step_len = sigma * max(1e-12, np.exp(0.1 * self.rng.randn()))\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n\n            # primary trial\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # accept and slightly increase sigma\n                pop_x[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, np.mean(ub - lb))\n                improved = True\n                stagnation_counter = 0\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                back_factor = 0.5\n                for bt in range(max_backtracks):\n                    if remaining <= 0:\n                        break\n                    small_step = step_len * (back_factor ** (bt + 1))\n                    x_bt = np.clip(x_parent + small_step * d, lb, ub)\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt < f_try:\n                        f_try = f_bt\n                        x_try = x_bt\n                    if f_bt < pop_f[parent_i]:\n                        pop_x[parent_i] = x_bt\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.08, np.mean(ub - lb))\n                        improved = True\n                        stagnation_counter = 0\n                        break\n\n            # orthogonal perturbation for local diversification\n            if remaining > 0 and not improved:\n                r = self.rng.randn(self.dim)\n                # remove component along d to make orthogonal\n                r -= np.dot(r, d) * d\n                rn = np.linalg.norm(r)\n                if rn > 1e-12:\n                    r /= rn\n                    ort_step = orthogonal_factor * sigma * (0.5 + self.rng.rand())\n                    x_o = np.clip(x_parent + ort_step * r, lb, ub)\n                    f_o, x_o = callf(x_o)\n                    if f_o < pop_f[parent_i]:\n                        pop_x[parent_i] = x_o\n                        pop_f[parent_i] = f_o\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, np.mean(ub - lb))\n                        improved = True\n                        stagnation_counter = 0\n                    else:\n                        # small increase for exploration occasionally\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.02, np.mean(ub - lb))\n\n            # occasional Lévy-like jump to escape local basins\n            if remaining > 0 and self.rng.rand() < p_levy:\n                # Cauchy-like heavy-tailed vector\n                step = self.rng.standard_cauchy(self.dim)\n                # robust normalization to avoid infinite scale\n                med = np.median(np.abs(step)) + 1e-12\n                step = step / med\n                scale_vec = levy_scale_fraction * (ub - lb)\n                x_levy = np.clip(x_parent + step * scale_vec * (0.8 + 0.4 * self.rng.rand()), lb, ub)\n                f_levy, x_levy = callf(x_levy)\n                # replace worst if improved\n                worst_i = int(np.argmax(pop_f))\n                if f_levy < pop_f[worst_i]:\n                    pop_x[worst_i] = x_levy\n                    pop_f[worst_i] = f_levy\n                    pop_sigma[worst_i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n                    improved = True\n                    stagnation_counter = 0\n\n            # recombination exploitation between two best (small probability)\n            if remaining > 0 and self.rng.rand() < p_recomb and pop_size >= 2:\n                best_two = np.argsort(pop_f)[:2]\n                b1, b2 = best_two[0], best_two[1]\n                beta = self.rng.rand()\n                noise = (0.01 * (ub - lb)) * self.rng.randn(self.dim)\n                x_rec = np.clip(beta * pop_x[b1] + (1 - beta) * pop_x[b2] + noise, lb, ub)\n                f_rec, x_rec = callf(x_rec)\n                worst_i = int(np.argmax(pop_f))\n                if f_rec < pop_f[worst_i]:\n                    pop_x[worst_i] = x_rec\n                    pop_f[worst_i] = f_rec\n                    pop_sigma[worst_i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n                    improved = True\n                    stagnation_counter = 0\n\n            # if the directional attempts produced a candidate that is better than worst, try to inject\n            if remaining > 0 and (f_try < np.inf):\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop_x[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.9 + 0.3 * self.rng.rand())\n\n            # adapt parent sigma on failure\n            if not improved:\n                # small shrink on failure\n                pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.92)\n                stagnation_counter += 1\n            else:\n                # reward parent with slight increase already done above; just reset counter\n                stagnation_counter = 0\n\n            # occasional population rejuvenation if stagnation is high\n            if remaining > 0 and stagnation_counter > 12:\n                # replace worst with a fresh random sample\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop_x[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n                stagnation_counter = 0\n                # slightly increase exploration probability temporarily\n                p_levy = min(0.2, p_levy * 1.2)\n\n            # occasional light mutation to maintain diversity\n            if remaining > 0 and self.rng.rand() < 0.03:\n                idx = self.rng.randint(pop_size)\n                perturb = 0.05 * (ub - lb) * self.rng.randn(self.dim)\n                x_mut = np.clip(pop_x[idx] + perturb, lb, ub)\n                f_mut, x_mut = callf(x_mut)\n                if f_mut < pop_f[idx]:\n                    pop_x[idx] = x_mut\n                    pop_f[idx] = f_mut\n                    pop_sigma[idx] = min(pop_sigma[idx] * 1.1, np.mean(ub - lb))\n\n            # periodic check: if global best hasn't improved, slightly cool sigmas\n            if remaining > 0 and self.f_opt >= best_since_rejuv - 1e-15:\n                # no improvement since last checkpoint\n                for i in range(pop_size):\n                    pop_sigma[i] *= 0.995\n            else:\n                best_since_rejuv = self.f_opt\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.519 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12143238547557045, 0.16427601498183297, 0.5121385909226417, 0.9588800158382174, 0.8677353877369648, 0.8845263794604935, 0.2735154253047237, 0.5140937877865273, 0.7764483125565244, 0.11784117742866818]}, "task_prompt": ""}
{"id": "7df03bf7-fa4a-4534-b2d5-fe07dbeb8908", "fitness": 0.41577791116860546, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — combines adaptive directional local searches, orthogonal refinements, recombination and occasional Lévy-like escapes for robust continuous black-box optimization.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a modest function of dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            # small DIM -> small pop; larger DIM -> slightly larger pop\n            self.pop_size = int(min(10 + 2 * self.dim, 40))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget function evaluations.\n        `func(x)` must accept a 1D numpy array of length dim and return a scalar.\n        The function's search bounds are determined from func.bounds if present, otherwise [-5, 5] each dimension.\n        Returns (f_opt, x_opt).\n        \"\"\"\n        # Determine bounds (Many BBOB uses -5..5 but handle if func provides bounds)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Ensure full-dimensional bounds\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.ravel()[:self.dim]\n        ub = ub.ravel()[:self.dim]\n\n        # State\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        remaining = self.budget\n\n        # Helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float).ravel()[:self.dim]\n            # Clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            self.evals += 1\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        if remaining < 10:\n            # very small budget: pure random search\n            for _ in range(remaining):\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        n_init = min(self.pop_size, remaining)\n        pop = np.zeros((n_init, self.dim), dtype=float)\n        pop_f = np.full(n_init, np.inf, dtype=float)\n        pop_sigma = np.zeros(n_init, dtype=float)\n\n        for i in range(n_init):\n            x0 = lb + self.rng.rand(self.dim) * (ub - lb)\n            f0, x0 = callf(x0)\n            pop[i] = x0\n            pop_f[i] = f0\n            # initial sigma per individual proportional to domain size but randomized\n            pop_sigma[i] = (0.1 + 0.4 * self.rng.rand()) * np.mean(ub - lb)\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Main loop: directional local searches, orthogonal tries, Lévy jumps, recombination, rejuvenation\n        stagnation = 0\n        best_history = 0\n        max_no_improve_for_rejuvenation = max(10, 5 * self.dim)\n        p_levy = 0.07  # probability of heavy-tailed jump attempt\n        p_recomb = 0.18\n        p_rejuvenate = 0.03\n\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # step-length sampled from a right-skewed distribution (favor small steps but allow larger)\n            frac = (self.rng.rand() ** 1.5)  # in [0,1], skews to small\n            alpha = sigma * (0.6 + 1.4 * self.rng.rand())  # base multiplier\n            step = alpha * frac * d\n            x_try = np.clip(x_parent + step, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                improved = True\n                stagnation = 0\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                for back in range(2):\n                    if remaining <= 0:\n                        break\n                    frac2 = frac * (0.5 ** (back + 1))\n                    x_bt = np.clip(x_parent + alpha * frac2 * d, lb, ub)\n                    try:\n                        f_bt, x_bt = callf(x_bt)\n                    except RuntimeError:\n                        break\n                    if f_bt < pop_f[parent_i]:\n                        pop[parent_i] = x_bt\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                        improved = True\n                        stagnation = 0\n                        break\n\n            if improved:\n                # small orthogonal refinement with some probability\n                if remaining > 0 and self.rng.rand() < 0.6:\n                    # create an orthogonal vector to d\n                    r = self.rng.randn(self.dim)\n                    proj = np.dot(r, d) * d\n                    r_orth = r - proj\n                    nr = np.linalg.norm(r_orth)\n                    if nr > 1e-12:\n                        r_orth = r_orth / nr\n                        x_o = np.clip(pop[parent_i] + 0.4 * pop_sigma[parent_i] * r_orth, lb, ub)\n                        try:\n                            f_o, x_o = callf(x_o)\n                        except RuntimeError:\n                            break\n                        if f_o < pop_f[parent_i]:\n                            pop[parent_i] = x_o\n                            pop_f[parent_i] = f_o\n                            pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, np.mean(ub - lb))\n            else:\n                # try an orthogonal perturbation for local diversification\n                r = self.rng.randn(self.dim)\n                proj = np.dot(r, d) * d\n                r_orth = r - proj\n                nr = np.linalg.norm(r_orth)\n                if nr > 1e-12:\n                    r_orth = r_orth / nr\n                    x_try2 = np.clip(x_parent + 0.6 * sigma * r_orth, lb, ub)\n                    try:\n                        f_try2, x_try2 = callf(x_try2)\n                    except RuntimeError:\n                        break\n                    if f_try2 < pop_f[parent_i]:\n                        pop[parent_i] = x_try2\n                        pop_f[parent_i] = f_try2\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        improved = True\n                        stagnation = 0\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and self.rng.rand() < p_levy:\n                # Cauchy-like heavy-tailed vector\n                step_raw = np.tan(np.pi * (self.rng.rand(self.dim) - 0.5))\n                # scale by local sigma with additional random multiplier\n                step = step_raw * (sigma * (1.0 + 3.0 * self.rng.rand()))\n                # normalize deltas to avoid extreme scale but keep heavy-tail properties\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom * (0.8 * np.mean(ub - lb))\n                x_jump = np.clip(x_parent + step, lb, ub)\n                try:\n                    f_jump, x_jump = callf(x_jump)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(0.01 * np.mean(ub - lb), sigma * 0.7)\n                    stagnation = 0\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0 and self.rng.rand() < p_recomb and len(pop) >= 2:\n                best2_inds = np.argsort(pop_f)[:2]\n                b1 = pop[best2_inds[0]]\n                b2 = pop[best2_inds[1]]\n                beta = self.rng.rand()\n                noise = (0.01 * (ub - lb)) * self.rng.randn(self.dim)\n                child = np.clip(beta * b1 + (1 - beta) * b2 + noise, lb, ub)\n                try:\n                    f_child, child = callf(child)\n                except RuntimeError:\n                    break\n                # replace worse of the two parents if improved, else possibly replace worst\n                if f_child < pop_f[parent_i]:\n                    pop[parent_i] = child\n                    pop_f[parent_i] = f_child\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    stagnation = 0\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_child < pop_f[worst_i]:\n                        pop[worst_i] = child\n                        pop_f[worst_i] = f_child\n                        pop_sigma[worst_i] = max(0.01 * np.mean(ub - lb), sigma * 0.5)\n                        stagnation = 0\n\n            # adapt parent sigma on failure\n            if not improved:\n                pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n                stagnation += 1\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (self.rng.rand() < p_rejuvenate or stagnation > max_no_improve_for_rejuvenation):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.clip(lb + self.rng.rand(self.dim) * (ub - lb), lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = (0.05 + 0.2 * self.rng.rand()) * np.mean(ub - lb)\n                stagnation = 0\n\n            # small housekeeping: ensure sigma not too small or huge\n            pop_sigma = np.clip(pop_sigma, 1e-12, np.mean(ub - lb) * 2.0)\n\n            # safety break if out of evals\n            if remaining <= 0:\n                break\n\n            # occasionally try to intensify around global best by small directed local search\n            if remaining > 0 and self.rng.rand() < 0.08:\n                # pick a center: current global best or a good individual\n                if self.x_opt is not None and self.rng.rand() < 0.6:\n                    center = self.x_opt.copy()\n                    local_sigma = 0.08 * np.mean(ub - lb)\n                else:\n                    idx = int(np.argmin(pop_f))\n                    center = pop[idx].copy()\n                    local_sigma = 0.12 * np.mean(ub - lb)\n                # a few tiny explorations\n                tries = min(3, remaining)\n                for _t in range(tries):\n                    x_small = np.clip(center + local_sigma * self.rng.randn(self.dim), lb, ub)\n                    try:\n                        f_small, x_small = callf(x_small)\n                    except RuntimeError:\n                        break\n                    if f_small < self.f_opt:\n                        # inject into population replacing worst\n                        worst_i = int(np.argmax(pop_f))\n                        pop[worst_i] = x_small\n                        pop_f[worst_i] = f_small\n                        pop_sigma[worst_i] = local_sigma\n                        stagnation = 0\n\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.416 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11872229092928888, 0.16446442061829225, 0.6928564702951981, 0.9267537042941623, 0.22560636126213418, 0.8779570603638862, 0.23974643784319327, 0.48517834305274177, 0.2592966822833417, 0.1671973407438162]}, "task_prompt": ""}
{"id": "dfdb4adc-4c27-4654-ae44-e97b9b5cfec0", "fitness": 0.4627854364314669, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population of points with individual adaptive step-sizes that performs randomized directional local searches, orthogonal refinements, recombination and occasional Lévy-like jumps to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest scaling: keep population modest but growing with dim\n            self.pop_size = int(min(40, max(6, 4 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        # Bounds handling (Many BBOB uses func.bounds with lb/ub)\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\"):\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        else:\n            # fallback to common evaluation bounds\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # Ensure bounds are full-dimensional arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # internal state\n        self.calls = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while respecting budget and tracking best\n        def callf(x):\n            # x -> numpy array, clipped to bounds\n            if self.calls >= self.budget:\n                return None, None\n            x = np.array(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            # evaluate\n            f = func(x)\n            self.calls += 1\n            # track global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is extremely small, fallback to pure random sampling\n        if self.budget <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population with evaluated points (respect budget)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base sigma relative to box size (a fraction of median range)\n        base_sigma = max(1e-6, 0.1 * np.median(ub - lb))\n        for i in range(self.pop_size):\n            if self.calls >= self.budget:\n                break\n            x0 = lb + self.rng.rand(self.dim) * (ub - lb)\n            f0, x0 = callf(x0)\n            if f0 is None:\n                break\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # randomized initial sigma per individual to encourage diversity\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.rand()))\n        pop = np.array(pop) if len(pop) > 0 else np.empty((0, self.dim))\n        pop_f = np.array(pop_f) if len(pop_f) > 0 else np.array([])\n        pop_sigma = np.array(pop_sigma) if len(pop_sigma) > 0 else np.array([])\n\n        # if no population could be created (very small budget), do random search\n        if pop.shape[0] == 0:\n            while self.calls < self.budget:\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main optimization loop: directional local searches, orthogonal tries, Lévy jumps, recombination\n        while self.calls < self.budget:\n            remaining = self.budget - self.calls\n            n_pop = pop.shape[0]\n\n            # small tournament selection for parent (balance exploration/exploitation)\n            k = min(3, n_pop)\n            inds = self.rng.choice(n_pop, k, replace=False)\n            # choose index with best fitness among the sampled ones (exploitation)\n            parent_idx = inds[np.argmin(pop_f[inds])]\n            x_parent = pop[parent_idx].copy()\n            f_parent = pop_f[parent_idx]\n            sigma = pop_sigma[parent_idx]\n\n            # sample a random search direction\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                continue\n            dir_unit = d / nd\n\n            # primary directional trial with stochasticized step-length\n            step = sigma * (1.0 + 0.25 * self.rng.randn())\n            x_try = x_parent + dir_unit * step\n            f_try, x_try = callf(x_try)\n            if f_try is None:\n                break\n\n            if f_try < f_parent:\n                # success: accept and slightly increase sigma\n                pop[parent_idx] = x_try.copy()\n                pop_f[parent_idx] = f_try\n                pop_sigma[parent_idx] = min((ub - lb).max(), sigma * 1.12)\n                continue\n\n            # local backtracking / small-step refinement along direction\n            backtrack_factors = [0.5, 0.25, 0.125]\n            improved = False\n            for bf in backtrack_factors:\n                if self.calls >= self.budget:\n                    break\n                x_try = x_parent + dir_unit * (step * bf)\n                f_try, x_try = callf(x_try)\n                if f_try is None:\n                    break\n                if f_try < f_parent:\n                    pop[parent_idx] = x_try.copy()\n                    pop_f[parent_idx] = f_try\n                    pop_sigma[parent_idx] = min((ub - lb).max(), sigma * (1.05 + 0.05 * self.rng.rand()))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            # make orthogonal to dir_unit\n            r = r - np.dot(r, dir_unit) * dir_unit\n            rn = np.linalg.norm(r)\n            if rn > 0:\n                r = r / rn\n                x_try = x_parent + r * (0.6 * sigma * (0.6 + 0.8 * self.rng.rand()))\n                f_try, x_try = callf(x_try)\n                if f_try is None:\n                    break\n                if f_try < f_parent:\n                    pop[parent_idx] = x_try.copy()\n                    pop_f[parent_idx] = f_try\n                    pop_sigma[parent_idx] = sigma * 1.08\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.06 and self.calls < self.budget:\n                # Cauchy-like heavy-tailed sampling\n                # sample independent Cauchy (standard) for each dim, scale by robust scale\n                # create heavy-tailed vector but normalize to avoid too extreme overall scale\n                cauchy = self.rng.standard_cauchy(size=self.dim)\n                # robust scale from pop sigmas or base_sigma\n                robust_scale = max(1e-8, np.median(pop_sigma) if pop_sigma.size > 0 else base_sigma)\n                # create candidate\n                lev = cauchy\n                lev = lev / (np.median(np.abs(lev)) + 1e-9)  # normalize to robust scale ~1\n                step_lev = lev * robust_scale * (2.0 + 4.0 * self.rng.rand())  # variable magnitude\n                x_try = x_parent + step_lev\n                f_try, x_try = callf(x_try)\n                if f_try is None:\n                    break\n                # if it's good replace worst in population, else maybe keep it as candidate\n                if f_try < pop_f[parent_idx]:\n                    pop[parent_idx] = x_try.copy()\n                    pop_f[parent_idx] = f_try\n                    pop_sigma[parent_idx] = max(1e-8, robust_scale * (1.0 + 0.5 * self.rng.rand()))\n                    continue\n                # else try to inject into population by replacing the worst if better\n                worst_idx = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_idx]:\n                    pop[worst_idx] = x_try.copy()\n                    pop_f[worst_idx] = f_try\n                    pop_sigma[worst_idx] = max(1e-8, robust_scale * (0.8 + 0.4 * self.rng.rand()))\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if n_pop >= 2:\n                best_idx = int(np.argmin(pop_f))\n                # pick second best distinct\n                idxs = list(range(n_pop))\n                idxs.remove(best_idx)\n                second_idx = self.rng.choice(idxs)\n                mix = 0.5 * (pop[best_idx] + pop[second_idx])\n                noise = self.rng.randn(self.dim) * (0.06 * np.mean(pop_sigma))\n                x_try = mix + noise\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                f_try, x_try = callf(x_try)\n                if f_try is None:\n                    break\n                # replace parent if improved, else try to inject into population by replacing worst\n                if f_try < pop_f[parent_idx]:\n                    pop[parent_idx] = x_try.copy()\n                    pop_f[parent_idx] = f_try\n                    pop_sigma[parent_idx] = max(1e-8, np.mean(pop_sigma) * (0.9 + 0.2 * self.rng.rand()))\n                    continue\n                else:\n                    worst_idx = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_idx]:\n                        pop[worst_idx] = x_try.copy()\n                        pop_f[worst_idx] = f_try\n                        pop_sigma[worst_idx] = max(1e-8, np.mean(pop_sigma) * (0.8 + 0.4 * self.rng.rand()))\n                        continue\n\n            # adapt parent's sigma on failure (reduce to focus search)\n            pop_sigma[parent_idx] = max(1e-8, pop_sigma[parent_idx] * 0.94)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.calls < self.budget and self.rng.rand() < 0.02:\n                worst_idx = int(np.argmax(pop_f))\n                x_new = lb + self.rng.rand(self.dim) * (ub - lb)\n                f_new, x_new = callf(x_new)\n                if f_new is None:\n                    break\n                pop[worst_idx] = x_new.copy()\n                pop_f[worst_idx] = f_new\n                pop_sigma[worst_idx] = base_sigma * (0.6 + 0.8 * self.rng.rand())\n\n            # safety: if budget is dwindling, spend remaining calls on local random sampling around best\n            if remaining <= max(3, int(0.05 * self.budget)):\n                # small local sampling around global best to refine\n                if self.x_opt is not None:\n                    while self.calls < self.budget:\n                        # tiny Gaussian around best\n                        local_step = np.mean(pop_sigma) * 0.15\n                        x = self.x_opt + self.rng.randn(self.dim) * local_step\n                        callf(x)\n                break\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.463 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16167436642662814, 0.17401585710161893, 0.570932513022643, 0.8554393088880783, 0.6144086029830693, 0.6951291151565095, 0.2822237455342722, 0.5116564914516191, 0.5793601861490765, 0.18301417760115435]}, "task_prompt": ""}
{"id": "ca90a45e-9b2c-4eef-925f-bada72fc8d04", "fitness": 0.32848591430471463, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — directional local searches with per-individual adaptive step-sizes, orthogonal refinements and occasional Cauchy (Lévy-like) jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults adaptively)\n    - seed: optional RNG seed\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # State to be filled during run\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # ensure bounds are full-dim arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.repeat(lb.item(), self.dim)\n        if ub.size == 1:\n            ub = np.repeat(ub.item(), self.dim)\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        range_scale = (ub - lb)\n        range_norm = np.linalg.norm(range_scale) / np.sqrt(max(1, self.dim))  # typical scale\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            if self.evals >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is extremely small, fallback to random search\n        if self.budget <= 5:\n            self.f_opt = np.inf\n            self.x_opt = None\n            for _ in range(self.budget):\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        # Heuristic pop size: scale with dim but respect budget\n        if self.pop_size is None:\n            pop_guess = max(6, min(2 * self.dim, max(6, self.budget // 25)))\n        else:\n            pop_guess = int(self.pop_size)\n        pop_size = max(2, min(pop_guess, max(2, self.budget // 6)))\n        self.pop_size = pop_size\n\n        pop_x = np.zeros((pop_size, self.dim), dtype=float)\n        pop_f = np.full(pop_size, np.inf, dtype=float)\n        pop_sigma = np.zeros(pop_size, dtype=float)\n\n        # initial sigma scale: fraction of domain\n        init_sigma = 0.12 * range_norm  # typical step size\n        min_sigma = 1e-8 * range_norm\n        max_sigma = 1.5 * range_norm\n\n        # Create initial population (random samples)\n        try:\n            for i in range(pop_size):\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                f, x = callf(x)\n                pop_x[i] = x\n                pop_f[i] = f\n                pop_sigma[i] = init_sigma * (0.8 + 0.4 * self.rng.rand())\n        except StopIteration:\n            # budget exhausted during initialization\n            return self.f_opt, self.x_opt\n\n        # If no population could be created (very small budget), do pure random search\n        if self.evals >= self.budget:\n            return self.f_opt, self.x_opt\n\n        # Main loop\n        # parameters\n        p_jump = 0.06  # probability to attempt a Lévy-like jump\n        p_rejuvenate = 0.02\n        tournament_k = min(3, pop_size)\n        backtrack_tries = 3\n        orthogonal_tries = 1\n        max_iter = 10**9  # bounded by budget via callf\n\n        while self.evals < self.budget:\n            # pick a parent via small tournament to balance exploration/exploitation\n            idxs = self.rng.choice(pop_size, size=tournament_k, replace=False)\n            parent_idx = idxs[np.argmin(pop_f[idxs])]\n            parent_x = pop_x[parent_idx].copy()\n            parent_f = float(pop_f[parent_idx])\n            sigma = pop_sigma[parent_idx]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            dn = np.linalg.norm(d) + 1e-12\n            d = d / dn\n\n            # primary directional trial with stochasticized step-length\n            # step size is sigma times a log-normalish multiplier\n            step_mult = np.exp(self.rng.normal(loc=0, scale=0.25))\n            step = sigma * step_mult\n            try:\n                x_try = parent_x + d * step\n                f_try, x_try = callf(x_try)\n            except StopIteration:\n                break\n\n            if f_try < parent_f:\n                # success: accept and slightly increase sigma\n                pop_x[parent_idx] = x_try\n                pop_f[parent_idx] = f_try\n                pop_sigma[parent_idx] = min(max_sigma, sigma * 1.12)\n                parent_f = f_try\n                parent_x = x_try\n            else:\n                # failure: local backtracking / small-step refinement along direction (few tries)\n                improved = False\n                step_bt = step\n                for bt in range(backtrack_tries):\n                    step_bt *= 0.5\n                    try:\n                        x_bt = parent_x + d * step_bt\n                        f_bt, x_bt = callf(x_bt)\n                    except StopIteration:\n                        improved = False\n                        break\n                    if f_bt < parent_f:\n                        pop_x[parent_idx] = x_bt\n                        pop_f[parent_idx] = f_bt\n                        pop_sigma[parent_idx] = min(max_sigma, sigma * 1.08)\n                        parent_f = f_bt\n                        parent_x = x_bt\n                        improved = True\n                        break\n                if not improved:\n                    # adapt parent sigma on failure (slightly decrease)\n                    pop_sigma[parent_idx] = max(min_sigma, sigma * 0.88)\n\n            # try an orthogonal perturbation for local diversification\n            if orthogonal_tries > 0 and self.evals < self.budget:\n                v = self.rng.randn(self.dim)\n                # make orthogonal to d\n                v = v - d * (v.dot(d))\n                vn = np.linalg.norm(v) + 1e-12\n                v = v / vn\n                orth_step = 0.6 * pop_sigma[parent_idx]\n                try:\n                    x_o = parent_x + v * orth_step\n                    f_o, x_o = callf(x_o)\n                except StopIteration:\n                    break\n                if f_o < parent_f:\n                    pop_x[parent_idx] = x_o\n                    pop_f[parent_idx] = f_o\n                    pop_sigma[parent_idx] = min(max_sigma, pop_sigma[parent_idx] * 1.07)\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if (self.rng.rand() < p_jump) and (self.evals < self.budget):\n                # scalar Cauchy as heavy-tailed magnitude\n                cauchy_scalar = np.tan(np.pi * (self.rng.rand() - 0.5))\n                # direction random\n                v = self.rng.randn(self.dim)\n                v = v / (np.linalg.norm(v) + 1e-12)\n                jump_scale = 0.7 * max(1.0, np.median(pop_sigma))\n                delta = v * cauchy_scalar * jump_scale\n                # normalize deltas to avoid crazy numerical overflow but keep heavy-tail\n                # clamp magnitude to a safe multiple of domain\n                mag = np.linalg.norm(delta)\n                max_mag = 6.0 * range_norm + 1e-12\n                if mag > max_mag:\n                    delta = delta * (max_mag / mag)\n                try:\n                    x_jump = parent_x + delta\n                    f_jump, x_jump = callf(x_jump)\n                except StopIteration:\n                    break\n                worst_idx = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_idx]:\n                    pop_x[worst_idx] = x_jump\n                    pop_f[worst_idx] = f_jump\n                    pop_sigma[worst_idx] = max(min_sigma, 0.9 * pop_sigma[parent_idx])\n                else:\n                    # keep as candidate in a simple crowding-like replacement: replace parent if slightly better\n                    if f_jump < parent_f:\n                        pop_x[parent_idx] = x_jump\n                        pop_f[parent_idx] = f_jump\n                        pop_sigma[parent_idx] = max(min_sigma, pop_sigma[parent_idx] * 1.02)\n\n            # recombination exploitation: mix two best and small noise\n            if self.evals < self.budget:\n                best_idxs = np.argsort(pop_f)[:2]\n                b0, b1 = best_idxs[0], best_idxs[1]\n                mix_noise = self.rng.normal(scale=0.08 * max(1.0, np.median(pop_sigma)), size=self.dim)\n                child = pop_x[b0] + 0.5 * (pop_x[b1] - pop_x[b0]) + mix_noise\n                try:\n                    f_child, child = callf(child)\n                except StopIteration:\n                    break\n                if f_child < pop_f[parent_idx]:\n                    pop_x[parent_idx] = child\n                    pop_f[parent_idx] = f_child\n                    pop_sigma[parent_idx] = min(max_sigma, pop_sigma[parent_idx] * 1.06)\n                else:\n                    # possibly replace worst if the child is good\n                    worst_idx = int(np.argmax(pop_f))\n                    if f_child < pop_f[worst_idx]:\n                        pop_x[worst_idx] = child\n                        pop_f[worst_idx] = f_child\n                        pop_sigma[worst_idx] = max(min_sigma, 0.8 * np.median(pop_sigma))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if (self.rng.rand() < p_rejuvenate) and (self.evals < self.budget):\n                worst_idx = int(np.argmax(pop_f))\n                try:\n                    x_rand = lb + self.rng.rand(self.dim) * (ub - lb)\n                    f_rand, x_rand = callf(x_rand)\n                except StopIteration:\n                    break\n                pop_x[worst_idx] = x_rand\n                pop_f[worst_idx] = f_rand\n                pop_sigma[worst_idx] = init_sigma * (0.6 + 0.8 * self.rng.rand())\n\n            # safety clamp for sigma values and ensure diversity\n            pop_sigma = np.clip(pop_sigma, min_sigma, max_sigma)\n\n            # if budget extremely low left, break to avoid overshooting in logic\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.328 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10650036287611098, 0.16638663767465656, 0.7994596833009354, 0.9722530854855498, 0.18013673727008772, 0.19816775301272382, 0.22781519899760616, 0.25467870829864125, 0.24469914866487452, 0.13476182746596077]}, "task_prompt": ""}
{"id": "da77efae-75e6-4a35-b185-06c08db6fa65", "fitness": 0.3533173548576041, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional searches with orthogonal refinements, recombination and occasional Cauchy (Lévy-like) jumps to balance exploitation and heavy-tailed exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by dim if None)\n    - seed: optional RNG seed for reproducibility\n\n    The algorithm maintains a small population of points, each with an adaptive\n    step-size sigma. It performs randomized directional local searches,\n    small-step backtracking along the direction, orthogonal perturbations,\n    occasional heavy-tailed Cauchy jumps to escape basins, and simple\n    recombination of the two best individuals. Sigma values are adapted\n    on success/failure to encourage self-adaptation of step lengths.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows slowly with dimension\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds as full-dim arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # Ensure bounds length matches dim\n        if lb.size != self.dim or ub.size != self.dim:\n            raise ValueError(\"Bounds size must match dimensionality\")\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # base scale for sigmas (a fraction of search range)\n        global_scale = np.maximum(1e-12, np.mean(ub - lb))\n        base_sigma = 0.12 * global_scale\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is zero, return immediately\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = int(inds[int(np.argmin(values))])\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            # keep alpha reasonable (can be negative to try opposite direction occasionally)\n            alpha = np.clip(alpha, -2.0 * global_scale, 2.0 * global_scale)\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15, global_scale)\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, global_scale)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalize to keep scale moderate but heavy-tailed\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = 0.06 * base_sigma * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (reduce slightly)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.353 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08931564546743931, 0.1609406994829622, 0.5119762990517988, 0.9015909242473095, 0.16141344923177525, 0.14935850905436454, 0.2218462425656006, 0.43112449020591226, 0.7919801638565226, 0.11362712541235642]}, "task_prompt": ""}
{"id": "ba93cc2f-d52d-4141-b999-e106c2d99b78", "fitness": 0.3928847431617798, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search mixing randomized directional local searches, orthogonal refinements, occasional Lévy jumps and population rejuvenation to balance exploitation and heavy-tailed exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None it's set adaptively)\n    - seed: optional random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # use a numpy Generator for reproducibility\n        self.rng = np.random.default_rng(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds handling: func.bounds.lb / ub might be scalar or array-like\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # sanity: ensure shapes\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search (or zero evaluations)\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialization\n        pop = []           # list of solution vectors\n        pop_f = []         # function values\n        pop_sigma = []     # adaptive step sizes\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale (fraction of range)\n\n        # Seed the population (until budget or population full)\n        for _ in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # initialize sigma with some variation\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * self.rng.random()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop\n        while remaining > 0:\n            n_pop = len(pop)\n            # small tournament selection of parent indices\n            tour_k = min(3, n_pop)\n            inds = self.rng.choice(n_pop, size=tour_k, replace=False)\n            # pick best among tournament\n            best_ind_in_tour = inds[int(np.argmin([pop_f[i] for i in inds]))]\n            parent_i = int(best_ind_in_tour)\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = self.rng.normal(size=self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.normal(size=self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * self.rng.normal()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n\n            # evaluate primary try if budget allows\n            improved = False\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                    improved = True\n\n            if improved:\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            # make r orthogonal to d\n            r = self.rng.normal(size=self.dim)\n            r = r - (r @ d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                scale_r = sigma * (0.4 + 0.4 * self.rng.random())\n                x_try = np.clip(x_parent + scale_r * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.random() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = self.rng.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                # scale vector by parent's sigma but keep heavy-tail character\n                step = (step / denom) * max(0.5 * sigma, 1e-12)\n                x_try = np.clip(x_parent + step, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                # find two best indices\n                sorted_inds = sorted(range(len(pop_f)), key=lambda i: pop_f[i])\n                a, b = sorted_inds[0], sorted_inds[1]\n                beta = self.rng.random()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * self.rng.normal(size=self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (shrinking)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.random() < 0.02:\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                worst_i = int(np.argmax(pop_f))\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.random())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.393 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13058994769885268, 0.14945203643196248, 0.4451103121820127, 0.8913176514769232, 0.2562428365533931, 0.9462682240011766, 0.2741898241768226, 0.3564356020674594, 0.34439342624509095, 0.13484757078410436]}, "task_prompt": ""}
{"id": "916f9551-373a-44a1-acd3-dd088f98e673", "fitness": 0.5793884697349168, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive directional local searches with orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size if pop_size is not None else min(max(6, self.dim), max(6, self.budget // 10))\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # obtain and normalize bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # allow scalar or 1-element bounds\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # budget bookkeeping\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to call function while tracking remaining and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return None, None\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If no budget, return immediately\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (as many as budget allows)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            # if no remaining or no evaluation returned break\n            if f0 is None:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n\n        # if no population could be created, do pure random search with remaining budget\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # auxiliary trackers\n        iters_since_improve = 0\n        total_evals_used_initial = self.budget - remaining\n        # main loop\n        while remaining > 0:\n            # small tournament selection\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), replace=False, size=k)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # fallback to random unit vector\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try is None:\n                break\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                iters_since_improve = 0\n                continue\n            # backtracking / refinement along direction\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try is None:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    improved = True\n                    iters_since_improve = 0\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation\n            r = np.random.randn(self.dim)\n            # remove component along d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try is None:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    iters_since_improve = 0\n                    continue\n\n            # occasional Lévy-like heavy-tailed jump (probability decays as budget used up)\n            frac_budget_used = (self.budget - remaining) / max(1, self.budget)\n            levy_prob = 0.15 * (1.0 - frac_budget_used) + 0.02  # more likely early\n            if np.random.rand() < levy_prob and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization to avoid runaway\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try is None:\n                    break\n                # replace worst if better\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    iters_since_improve = 0\n                    continue\n\n            # recombination exploitation: mix two best and small gaussian noise\n            best2 = np.argsort(pop_f)[:2]\n            if best2.size >= 2:\n                a, b = best2[0], best2[1]\n            else:\n                a = b = best2[0]\n            beta = np.random.rand()\n            mix = beta * pop[a] + (1.0 - beta) * pop[b]\n            noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n            x_try = np.clip(mix + noise, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try is None:\n                break\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.05, np.mean(ub - lb))\n                iters_since_improve = 0\n                continue\n            else:\n                # try inject into population by replacing worst if better\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                    iters_since_improve = 0\n                    continue\n\n            # adapt sigma for parent on failure (shrink)\n            pop_sigma[parent_i] = max(1e-12, sigma * 0.94)\n            iters_since_improve += 1\n\n            # occasional population rejuvenation if stagnation detected\n            if iters_since_improve > max(20, 5 * self.dim) and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new is None:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                iters_since_improve = 0\n\n            # occasionally shrink population (remove worst) or expand (add random) depending on budget\n            # keep population size adaptive to remaining budget\n            if remaining < len(pop) // 2 and len(pop) > 4:\n                # drop the worst to save maintenance overhead\n                worst_i = int(np.argmax(pop_f))\n                pop = np.delete(pop, worst_i, axis=0)\n                pop_f = np.delete(pop_f, worst_i)\n                pop_sigma = np.delete(pop_sigma, worst_i)\n            elif remaining > 5 * len(pop) and len(pop) < self.pop_size:\n                # we can afford to add a new random candidate\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new is None:\n                    break\n                pop = np.vstack([pop, x_new])\n                pop_f = np.append(pop_f, f_new)\n                pop_sigma = np.append(pop_sigma, base_sigma * (0.7 + 0.6 * np.random.rand()))\n\n        # end while\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.579 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09229854493542744, 0.15828559691057442, 0.9242167308844175, 0.9346109309006185, 0.9154091139338377, 0.9553917907778362, 0.2334673010806827, 0.4888305444556824, 0.9387262930343286, 0.15264785043576268]}, "task_prompt": ""}
{"id": "cdf551f1-9169-41de-9931-d32b59b3c0ab", "fitness": 0.4526940958766506, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based directional local searches with adaptive step sizes, orthogonal refinements and occasional heavy-tailed jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional args:\n    - pop_size: number of population members (auto-scaled by dim if None)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # scale population with dimension but keep modest\n        if pop_size is None:\n            self.pop_size = max(4, min(40, int(3 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds (allow scalar or single-value arrays)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # safety: if bounds give different size than dim, try to broadcast or clip\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.broadcast_to(lb.ravel()[0], (self.dim,))\n            ub = np.broadcast_to(ub.ravel()[0], (self.dim,))\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).ravel()\n            # enforce dimension\n            if x.size != self.dim:\n                x = np.resize(x, (self.dim,))\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # initialize best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        n_init = min(self.pop_size, remaining)\n        pop = []\n        pop_f = []\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            if remaining <= 0:\n                break\n\n        pop = np.array(pop) if len(pop) > 0 else np.empty((0, self.dim))\n        pop_f = np.array(pop_f) if len(pop_f) > 0 else np.array([])\n\n        # If no population could be created (very small budget), fallback to pure random search\n        if pop.shape[0] == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # base sigma scale based on problem size\n        base_sigma = max(1e-12, 0.2 * np.mean(ub - lb))\n        # per-individual adaptive sigma\n        pop_sigma = np.full(pop.shape[0], base_sigma)\n\n        # main loop\n        while remaining > 0:\n            n = pop.shape[0]\n            # pick a parent via small tournament (prefer better ones sometimes)\n            k = min(3, n)\n            inds = np.random.choice(n, k, replace=False)\n            # choose best among sampled inds\n            best_ind_local = inds[int(np.argmin(pop_f[inds]))]\n            parent_i = best_ind_local\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d /= nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            success = False\n            if f_try < pop_f[parent_i]:\n                # accept and enlarge sigma slightly\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.25, np.mean(ub - lb))\n                success = True\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                fracs = [0.5, 0.25, 0.125]\n                for frac in fracs:\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        success = True\n                        break\n\n            if success:\n                # small orthogonal refinement around improved point to exploit local basin\n                if remaining > 0:\n                    r = np.random.randn(self.dim)\n                    # make r orthogonal to d\n                    r -= (r.dot(d)) * d\n                    nr = np.linalg.norm(r)\n                    if nr > 1e-12:\n                        r /= nr\n                        orth_step = 0.5 * pop_sigma[parent_i] * (0.5 + 0.5 * np.random.rand())\n                        x_try = np.clip(pop[parent_i] + orth_step * r, lb, ub)\n                        if remaining > 0:\n                            try:\n                                f_try, x_try = callf(x_try)\n                            except RuntimeError:\n                                f_try = np.inf\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.15, np.mean(ub - lb))\n            else:\n                # failure -> shrink sigma a bit\n                pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n                # try one orthogonal perturbation to escape plateaus\n                if remaining > 0:\n                    r = np.random.randn(self.dim)\n                    r -= (r.dot(d)) * d\n                    nr = np.linalg.norm(r)\n                    if nr > 1e-12:\n                        r /= nr\n                        orth_step = pop_sigma[parent_i] * (0.8 * np.random.rand())\n                        x_try = np.clip(x_parent + orth_step * r, lb, ub)\n                        try:\n                            f_try, x_try = callf(x_try)\n                        except RuntimeError:\n                            f_try = np.inf\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                            success = True\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and np.random.rand() < 0.08:\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(size=self.dim)\n                # scale by robust local scale (median absolute sigma)\n                denom = np.median(np.abs(step)) + 1e-12\n                step = step / denom\n                # scale by problem scale but keep moderate overall amplitude\n                scale = 0.5 * np.mean(pop_sigma)\n                if scale <= 0:\n                    scale = base_sigma\n                x_jump = np.clip(x_parent + scale * step, lb, ub)\n                try:\n                    f_jump, x_jump = callf(x_jump)\n                except RuntimeError:\n                    f_jump = np.inf\n                # if jump is good, replace worst, else maybe replace worst if not too bad\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(1e-12, scale * 0.7)\n                elif f_jump < pop_f[parent_i]:\n                    pop[parent_i] = x_jump\n                    pop_f[parent_i] = f_jump\n                    pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.7)\n\n            # recombination exploitation: mix two good individuals occasionally\n            if remaining > 0 and np.random.rand() < 0.12 and pop.shape[0] >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.02 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    f_try = np.inf\n                # accept mixed child if it's better than the worst\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * 0.5\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # If population is small but budget remains, try to expand population gradually\n            if remaining > 0 and pop.shape[0] < self.pop_size and np.random.rand() < 0.2:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop = np.vstack([pop, x_new])\n                pop_f = np.concatenate([pop_f, [f_new]])\n                pop_sigma = np.concatenate([pop_sigma, [base_sigma * (0.5 + np.random.rand())]])\n\n            # defensive break if exhausted\n            if remaining <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.453 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1408606806451279, 0.16276280691601697, 0.5107525644814297, 0.8898057959548145, 0.7882075888854121, 0.6003482162515594, 0.292921026893495, 0.34177349686553704, 0.6701509106792618, 0.12935787119385234]}, "task_prompt": ""}
{"id": "6ba419f8-9834-4f78-8b71-6a258ae8be20", "fitness": 0.5058288366814571, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer that alternates randomized directional local searches, orthogonal refinements, recombination, and rare heavy-tailed (Lévy/Cauchy) jumps with per-individual adaptive step-size.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None it is set relative to dim and budget)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # scale modestly with dimension but also constrained by budget\n            self.pop_size = max(4, min(40, int(6 + 1.5 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # best so far\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds (support scalar or vector bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # ensure shapes\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        # remaining evals\n        remaining = int(self.budget)\n\n        # helper evaluator that updates best and respects budget\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).ravel()\n            if x.size == 1:\n                # broadcast scalar to full-dim\n                x = np.full(self.dim, float(x.item()))\n            if x.shape != (self.dim,):\n                # try to reshape or take first dim entries\n                x = x.flatten()[:self.dim].copy()\n                if x.size < self.dim:\n                    x = np.pad(x, (0, self.dim - x.size), 'edge')\n            # clip into bounds\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # quick bailout: if budget is extremely small do random sampling\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # adapt population size if budget is small\n        max_initial = max(1, remaining // 3)\n        pop_size = min(self.pop_size, max_initial)\n\n        # base sigma scale (scalar) tied to mean range\n        mean_range = float(np.mean(ub - lb))\n        base_sigma = max(1e-9, 0.12 * mean_range)  # initial typical step length (absolute)\n        max_sigma = max(1e-9, 1.0 * mean_range)\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        for i in range(pop_size):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # initialize per-individual sigma with some spread\n            pop_sigma.append(base_sigma * (0.5 + np.random.rand()))\n        pop = list(pop)\n        pop_f = np.array(pop_f, dtype=float) if pop_f else np.empty(0)\n        pop_sigma = list(pop_sigma)\n\n        # If no population could be created, fallback to random search for remaining budget\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return self.f_opt, self.x_opt\n\n        # main loop\n        stagnation_counter = 0\n        best_history = [self.f_opt]\n        while remaining > 0:\n            n = len(pop)\n            # pick a parent via small tournament (k) to balance exploration/exploitation\n            k = min(3, n)\n            inds = np.random.choice(n, k, replace=False)\n            best_idx_in_inds = inds[int(np.argmin(pop_f[inds]))]\n            parent_i = int(best_idx_in_inds)\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n            parent_f = float(pop_f[parent_i])\n\n            # sample a random normalized direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.ones(self.dim)\n                nd = np.linalg.norm(d)\n            d = d / nd\n\n            # primary directional trial: use a stochastic step length (sometimes heavy-tailed)\n            # mix Gaussian and occasional Cauchy for heavier tails\n            if np.random.rand() < 0.12:\n                step_scalar = np.random.standard_cauchy()\n            else:\n                step_scalar = np.random.randn()\n            # normalize step magnitude and cap extremes\n            step_scalar = np.clip(step_scalar, -6.0, 6.0)\n            step_len = step_scalar * sigma  # absolute distance in search space units\n            x_try = np.clip(x_parent + d * step_len, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            improved = False\n            if f_try < parent_f:\n                # accept into parent slot\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                # slightly increase sigma for this parent (success-based)\n                pop_sigma[parent_i] = min(sigma * 1.2, max_sigma)\n                improved = True\n                stagnation_counter = 0\n            else:\n                # small decrease on failure\n                pop_sigma[parent_i] = max(sigma * 0.88, 1e-12)\n                stagnation_counter += 1\n\n            # local backtracking / small-step refinement along direction (few tries)\n            if remaining > 0:\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    if remaining <= 0:\n                        break\n                    small_step = frac * step_len\n                    x_bt = np.clip(x_parent + d * small_step, lb, ub)\n                    try:\n                        f_bt, x_bt = callf(x_bt)\n                    except RuntimeError:\n                        break\n                    if f_bt < pop_f[parent_i]:\n                        pop[parent_i] = x_bt.copy()\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, max_sigma)\n                        improved = True\n                        stagnation_counter = 0\n                        break\n                else:\n                    # if no break (no improvement) then slightly shrink sigma\n                    if not improved:\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n\n            # orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    ortho_step = 0.6 * pop_sigma[parent_i] * np.random.randn()\n                    x_ortho = np.clip(pop[parent_i] + r * ortho_step, lb, ub)\n                    try:\n                        f_ortho, x_ortho = callf(x_ortho)\n                    except RuntimeError:\n                        break\n                    if f_ortho < pop_f[parent_i]:\n                        pop[parent_i] = x_ortho.copy()\n                        pop_f[parent_i] = f_ortho\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.08, max_sigma)\n                        improved = True\n                        stagnation_counter = 0\n\n            # occasional Lévy-like jump to escape local basins\n            # triggered with small probability or when stagnating\n            p_jump = 0.04 + 0.002 * min(200, stagnation_counter)\n            if remaining > 0 and (np.random.rand() < p_jump):\n                # create Cauchy-like heavy-tailed vector, zero-center and normalize\n                z = np.random.standard_cauchy(size=self.dim)\n                # robust normalization: divide by robust scale (median absolute deviation)\n                mad = np.median(np.abs(z - np.median(z))) + 1e-9\n                z = z / (mad + 1e-12)\n                # scale relative to bounds but keep step moderate\n                scale_vec = 0.25 * (ub - lb) * (0.5 + np.random.rand())\n                x_jump = np.clip(x_parent + z * scale_vec, lb, ub)\n                try:\n                    f_jump, x_jump = callf(x_jump)\n                except RuntimeError:\n                    break\n                # if it's good, inject by replacing worst; else keep candidate occasionally\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump.copy()\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(0.8 * np.mean(pop_sigma), 1e-12)\n                    stagnation_counter = 0\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0 and len(pop) >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.015 * (ub - lb)) * np.random.randn(self.dim)\n                x_mix = np.clip(mix + noise, lb, ub)\n                try:\n                    f_mix, x_mix = callf(x_mix)\n                except RuntimeError:\n                    break\n                # replace worst if better\n                worst_i = int(np.argmax(pop_f))\n                if f_mix < pop_f[worst_i]:\n                    pop[worst_i] = x_mix.copy()\n                    pop_f[worst_i] = f_mix\n                    pop_sigma[worst_i] = base_sigma * 0.5\n                    stagnation_counter = 0\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (np.random.rand() < 0.02):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n                stagnation_counter = 0\n\n            # keep arrays consistent\n            pop_f = np.array(pop_f, dtype=float)\n\n            # occasional population size adjustments if budget dwindles\n            if remaining < max(3, self.dim // 2) and len(pop) > 1:\n                # shrink population conservatively by removing the worst\n                if len(pop) > 1:\n                    worst_i = int(np.argmax(pop_f))\n                    del pop[worst_i]\n                    pop_f = np.delete(pop_f, worst_i)\n                    del pop_sigma[worst_i]\n\n            # record best history and small restart if heavily stuck\n            best_history.append(self.f_opt)\n            if len(best_history) > 30:\n                if best_history[-1] >= best_history[-15] and stagnation_counter > 80:\n                    # mild rejuvenation: replace half of population\n                    num_replace = max(1, len(pop) // 3)\n                    for _ in range(num_replace):\n                        if remaining <= 0:\n                            break\n                        worst_i = int(np.argmax(pop_f))\n                        x_new = np.random.uniform(lb, ub)\n                        try:\n                            f_new, x_new = callf(x_new)\n                        except RuntimeError:\n                            break\n                        pop[worst_i] = x_new.copy()\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = base_sigma * (0.4 + np.random.rand())\n                    stagnation_counter = 0\n                    best_history = [self.f_opt]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.506 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1120432526411751, 0.15928455621752147, 0.8021841867878152, 0.9176818798232758, 0.474806587606191, 0.927883801145547, 0.257096616122898, 0.4197279412089233, 0.8505837572952075, 0.1369957879660164]}, "task_prompt": ""}
{"id": "677f8597-fad3-481a-a8fe-04884f517021", "fitness": 0.36334659752356113, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — maintains a modest population with adaptive step-sizes, performs randomized directional local searches with orthogonal refinements and occasional heavy-tailed Lévy jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a modest scale with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # modest population that scales with dimension but stays small\n        if pop_size is None:\n            self.pop_size = max(4, min(24, 4 + self.dim // 2))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds (support scalar or array-like)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # safety: ensure dimension matches\n        if lb.size != self.dim or ub.size != self.dim:\n            raise ValueError(\"func.bounds.lb/ub must be scalar or length dim arrays\")\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            x = np.array(x, dtype=float).copy()\n            # clip to bounds before evaluating\n            x = np.clip(x, lb, ub)\n            if remaining <= 0:\n                return np.inf, x\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to simple random search\n        if remaining <= 8:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base sigma relative to domain size\n        base_sigma = 0.2 * np.mean(ub - lb)\n        # create initial population from random samples (use at least one)\n        while len(pop) < self.pop_size and remaining > 0:\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # slightly randomized sigma per individual\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # bookkeeping\n        stagnation = 0\n        iter_count = 0\n\n        # Main loop\n        while remaining > 0:\n            iter_count += 1\n            # tournament selection (small) to pick a parent index\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            x_parent = pop[parent_i].copy()\n            parent_f = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                continue\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = abs(sigma * (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n            improved = False\n            if f_try < parent_f:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.12, np.max(ub - lb))\n                improved = True\n                stagnation = 0\n                continue  # good, go to next iteration\n\n            # local backtracking / line refinement along direction (few tries, decreasing length)\n            for sf in (0.6, 0.3, 0.15, 0.07):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + sf * alpha * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < parent_f:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    stagnation = 0\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # project r to be orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < parent_f:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        improved = True\n                        stagnation = 0\n            if improved:\n                continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            p_levy = 0.06\n            if remaining > 0 and np.random.rand() < p_levy:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                jump_scale = sigma * (2.0 + 1.5 * np.random.rand())\n                x_try = np.clip(x_parent + jump_scale * step, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < parent_f:\n                    # replace parent\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.7)\n                    improved = True\n                    stagnation = 0\n                else:\n                    # if promising, replace the worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                if improved:\n                    continue\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0 and np.random.rand() < 0.25 and len(pop) >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                # weighted average + small Gaussian noise relative to domain\n                w = np.random.rand()\n                child = w * pop[a] + (1.0 - w) * pop[b]\n                noise = (0.02 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(child + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace worst if better, else maybe replace parent\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n                    improved = True\n                    stagnation = 0\n                elif f_try < parent_f:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.9)\n                    improved = True\n                    stagnation = 0\n                if improved:\n                    continue\n\n            # adapt parent sigma on failure\n            if not improved:\n                pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.94)\n                stagnation += 1\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (stagnation > 12 or (iter_count % 50 == 0 and np.random.rand() < 0.2)):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n                    stagnation = 0\n\n            # small safeguard: if the population has very similar fitness, inject randomness\n            if remaining > 0 and np.ptp(pop_f) < 1e-9 and np.random.rand() < 0.1:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.363 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07025566982944653, 0.1591278460984903, 0.4998517667977236, 0.946655552222403, 0.33509259314856155, 0.5248899304756209, 0.2786343442827335, 0.33811694255355496, 0.31210337299119983, 0.16873795683587745]}, "task_prompt": ""}
{"id": "be6abaee-8d5c-4534-8a12-e072478253f6", "fitness": 0.4315620483942178, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional local searches with orthogonal refinements, recombination and occasional Lévy jumps to efficiently explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size; if None it is set relative to dim and budget\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            self._rng = np.random.RandomState(seed)\n        else:\n            self._rng = np.random.RandomState()\n        # default population scaled with problem size but modest\n        if pop_size is None:\n            # small population for efficiency, but at least 2\n            self.pop_size = max(2, min(20, int(4 * np.sqrt(self.dim))))\n            # if budget is very small, lower the pop to allow initialization\n            self.pop_size = min(self.pop_size, max(2, self.budget // 10))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds as full-dim arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # ensure budgets and outputs\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # safe evaluation wrapper to clip, count budget, and update best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x.copy()\n\n        # If budget extremely small, do random sampling until exhausted\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # If we cannot initialize a full population due to budget, fallback to random search\n        if remaining < self.pop_size:\n            while remaining > 0:\n                x = self._rng.uniform(lb, ub)\n                f, x = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population uniformly in bounds\n        pop = np.zeros((self.pop_size, self.dim), dtype=float)\n        pop_f = np.zeros(self.pop_size, dtype=float)\n        for i in range(self.pop_size):\n            x0 = self._rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop[i, :] = x0\n            pop_f[i] = f0\n\n        # adaptive per-individual step sizes (sigma) initialized relative to range\n        rng_range = np.maximum(ub - lb, 1e-12)\n        base_scale = np.mean(rng_range)\n        pop_sigma = np.full(self.pop_size, max(1e-8, 0.2 * base_scale), dtype=float)\n\n        # Main loop: use remaining evaluations for localized directional search, orthogonal tries, Lévy jumps, recombination\n        while remaining > 0:\n            # small tournament to pick a parent (balance exploration/exploitation)\n            k = min(3, self.pop_size)\n            inds = self._rng.choice(self.pop_size, size=k, replace=False)\n            vals = pop_f[inds]\n            parent_i = inds[int(np.argmin(vals))]\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self._rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-16:\n                d = self._rng.randn(self.dim)\n                nd = np.linalg.norm(d)\n                if nd <= 1e-16:\n                    d = np.ones(self.dim) / np.sqrt(self.dim)\n                    nd = 1.0\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # multiplicative noise to allow both exploration and conservative moves\n            step_mult = max(1e-12, 1.0 + 0.25 * self._rng.randn())\n            alpha = sigma * step_mult\n            x_try = x_parent + alpha * d\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < f_parent:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.18, np.mean(rng_range) * 2.0)\n                continue  # next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            backtries = 3\n            for j in range(backtries):\n                factor = 0.5 ** (j + 1)\n                x_try = x_parent + alpha * factor * d\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, np.mean(rng_range) * 2.0)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            # generate r orthogonal to d (by subtracting projection)\n            r = self._rng.randn(self.dim)\n            proj = np.dot(r, d) * d\n            r = r - proj\n            nr = np.linalg.norm(r)\n            if nr <= 1e-16:\n                # fallback to random small perturbation\n                r = self._rng.randn(self.dim)\n                nr = np.linalg.norm(r)\n                if nr <= 1e-16:\n                    r = np.ones(self.dim)\n                    nr = np.linalg.norm(r)\n            r = r / nr\n            x_try = x_parent + 0.6 * sigma * r\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, np.mean(rng_range) * 2.0)\n                continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            p_jump = 0.06  # jump probability\n            if self._rng.rand() < p_jump and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step_vec = self._rng.standard_cauchy(self.dim)\n                # robust scale: median absolute value (avoid extreme infinite scale)\n                med = np.median(np.abs(step_vec)) + 1e-12\n                step_vec = step_vec / med\n                # scale relative to problem range and individual's sigma\n                scale = max(0.5 * sigma, 0.6 * base_scale)\n                x_try = x_parent + 0.8 * scale * step_vec\n                # clip to bounds (and shrink if massive)\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < np.max(pop_f):\n                    # replace worst in population\n                    worst_i = int(np.argmax(pop_f))\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(scale * 0.8, 1e-12)\n                else:\n                    # keep as candidate by maybe replacing parent if slightly better\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if self.pop_size >= 2 and remaining > 0 and self._rng.rand() < 0.2:\n                # pick two best\n                best_inds = np.argsort(pop_f)[:2]\n                a, b = pop[best_inds[0]], pop[best_inds[1]]\n                w = 0.6 + 0.2 * self._rng.rand()\n                child = w * a + (1.0 - w) * b\n                # small Gaussian perturbation scaled by average sigma\n                avg_sigma = max(1e-12, np.mean(pop_sigma))\n                child = child + 0.05 * avg_sigma * self._rng.randn(self.dim)\n                child = np.minimum(np.maximum(child, lb), ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, child = callf(child)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst, else maybe replace the parent's sigma\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = child\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(avg_sigma * 0.5, 1e-12)\n                else:\n                    # slightly reduce parent's sigma due to failure to improve\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n                continue\n\n            # adapt parent sigma on failure (no improvements): gently decrease to focus search\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a fresh random sample\n            if self._rng.rand() < 0.03 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self._rng.uniform(lb, ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = max(0.1 * base_scale, 1e-12)\n\n        # return best found\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.432 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16472233215271936, 0.16013854271056016, 0.9342687879871601, 0.9691767369316292, 0.2727460399221665, 0.956632641237763, 0.24249189180010422, 0.2054806969976728, 0.2635432863231517, 0.1464195278792515]}, "task_prompt": ""}
{"id": "72ab64e5-98bb-472c-b9b4-b7e09ebed035", "fitness": 0.22803986155974793, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-sizes that alternates randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy jumps to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None it's chosen from dim and budget)\n    - seed: RNG seed for reproducibility\n    Notes:\n      - func must expose bounds via func.bounds.lb and func.bounds.ub (scalars or arrays)\n      - this implementation will never call the function more times than `budget`\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # default population: scale with dimension but keep modest\n        if pop_size is None:\n            # aim for between 4*dim and 10*dim but limited by budget\n            suggested = max(4 * self.dim, 8)\n            suggested = min(suggested, max(2, self.budget // 20))\n            self.pop_size = max(2, int(suggested))\n        else:\n            self.pop_size = max(2, int(pop_size))\n\n    def __call__(self, func):\n        # prepare bounds (allow scalar bounds or per-dim arrays)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        assert lb.shape == (self.dim,) and ub.shape == (self.dim,), \"Bounds must match dimensionality\"\n\n        remaining = int(self.budget)\n        # best-so-far\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.clip(x, lb, ub)\n            remaining -= 1\n            f = float(func(x))\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to pure random search\n        if remaining <= 10:\n            while remaining > 0:\n                x = self.rng.rand(self.dim) * (ub - lb) + lb\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population (may reduce pop_size if budget too small)\n        # we need to reserve at least one evaluation per member\n        max_init = min(self.pop_size, max(2, remaining // 6))\n        pop_size = max_init\n        pop = np.empty((pop_size, self.dim), dtype=float)\n        pop_f = np.empty(pop_size, dtype=float)\n        # initial sigma per member: relative scale (fraction of search range)\n        global_scale = 0.2 * np.mean(ub - lb)\n        pop_sigma = np.empty(pop_size, dtype=float)\n\n        for i in range(pop_size):\n            x = self.rng.rand(self.dim) * (ub - lb) + lb\n            f, x = callf(x)\n            pop[i] = x\n            pop_f[i] = f\n            # initialize sigma from scaled random fraction of range\n            pop_sigma[i] = global_scale * (0.5 + self.rng.rand())\n\n        # If after initialization there is no budget left, return\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # helper to pick small tournament parent index\n        def pick_parent():\n            k = min(3, pop_size)\n            inds = self.rng.randint(0, pop_size, size=k)\n            best = inds[0]\n            for ii in inds:\n                if pop_f[ii] < pop_f[best]:\n                    best = ii\n            return best\n\n        # Main optimization loop\n        # Parameters controlling behavior\n        p_levy = 0.05            # probability of a Lévy jump per iteration\n        p_rejuv = 0.03           # probability to rejuvenate worst\n        p_recomb = 0.12          # probability to attempt recombination\n        max_backtrack = 3        # backtrack steps along direction\n        orth_scale = 0.6         # relative scale for orthogonal perturbations\n\n        # bookkeeping to avoid infinite loops when remaining small\n        while remaining > 0:\n            # pick a parent using small tournament\n            parent_i = pick_parent()\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d)\n            d = d / (nd + 1e-16)\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = pop_sigma[parent_i] * (1.1 + 0.05 * self.rng.rand())\n                improved = True\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                alpha_bt = alpha * 0.5\n                for bt in range(max_backtrack):\n                    if remaining <= 0:\n                        break\n                    x_bt = np.clip(x_parent + alpha_bt * d, lb, ub)\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt < pop_f[parent_i]:\n                        pop[parent_i] = x_bt\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.05, 1e-12)\n                        improved = True\n                        break\n                    alpha_bt *= 0.5\n                if not improved:\n                    # failure: slightly shrink sigma for that parent\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = self.rng.randn(self.dim)\n                # make r orthogonal to d\n                proj = np.dot(r, d)\n                r = r - proj * d\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    r = self.rng.randn(self.dim)\n                    nr = np.linalg.norm(r)\n                r = r / nr\n                ortho_step = orth_scale * sigma * (0.8 + 0.4 * self.rng.rand())\n                x_ort = np.clip(pop[parent_i] + ortho_step * r, lb, ub)\n                f_ort, x_ort = callf(x_ort)\n                if f_ort < pop_f[parent_i]:\n                    pop[parent_i] = x_ort\n                    pop_f[parent_i] = f_ort\n                    pop_sigma[parent_i] = pop_sigma[parent_i] * 1.07\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and self.rng.rand() < p_levy:\n                # standard Cauchy (heavy tail) scaled by range and parent sigma\n                cauchy = self.rng.standard_cauchy(self.dim)\n                c_norm = np.linalg.norm(cauchy)\n                if c_norm < 1e-12:\n                    cauchy = self.rng.randn(self.dim)\n                    c_norm = np.linalg.norm(cauchy)\n                cvec = cauchy / (c_norm + 1e-16)\n                # heavy magnitude drawn from another Cauchy to keep heavy tail\n                mag = np.abs(self.rng.standard_cauchy()) + 0.5\n                jump_scale = mag * (np.mean(ub - lb) * 0.6 + sigma * 0.4)\n                x_jump = np.clip(x_parent + cvec * jump_scale, lb, ub)\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < pop_f[parent_i]:\n                    # accept as new parent (escape)\n                    pop[parent_i] = x_jump\n                    pop_f[parent_i] = f_jump\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * (1.2 + 0.2 * self.rng.rand()), 1e-12)\n                else:\n                    # maybe replace the worst if jump is good\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(pop_sigma[worst_i] * 0.9, 1e-12)\n\n            # recombination exploitation: mix two best with small noise\n            if remaining > 0 and self.rng.rand() < p_recomb:\n                # pick top 2\n                best_inds = np.argsort(pop_f)[:2]\n                i1, i2 = best_inds[0], best_inds[1]\n                w = self.rng.rand()\n                x_rec = w * pop[i1] + (1 - w) * pop[i2]\n                # add small normal noise proportional to their sigmas\n                noise_scale = 0.05 * (pop_sigma[i1] + pop_sigma[i2]) / 2.0\n                x_rec = np.clip(x_rec + noise_scale * self.rng.randn(self.dim), lb, ub)\n                f_rec, x_rec = callf(x_rec)\n                # inject into population: replace worst if improved\n                worst_i = int(np.argmax(pop_f))\n                if f_rec < pop_f[worst_i]:\n                    pop[worst_i] = x_rec\n                    pop_f[worst_i] = f_rec\n                    pop_sigma[worst_i] = max(noise_scale * 2.0, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < p_rejuv:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.rand(self.dim) * (ub - lb) + lb\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = global_scale * (0.5 + self.rng.rand())\n\n            # ensure at least one small exploitation attempt if no improvement for a while:\n            # adapt global sigma a little based on population spread\n            # (small self-adaptation)\n            if remaining > 0:\n                # compute simple diversity measure\n                spread = np.mean(np.std(pop, axis=0))\n                target = 0.2 * np.mean(ub - lb)\n                factor = 1.0\n                if spread < 0.5 * target:\n                    factor = 1.05\n                elif spread > 2.0 * target:\n                    factor = 0.95\n                pop_sigma = np.clip(pop_sigma * factor, 1e-12, np.mean(ub - lb) * 10.0)\n\n            # stop if we've exhausted the budget\n            if remaining <= 0:\n                break\n\n        # final best\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.228 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11562678019971706, 0.16024295802671684, 0.26677290759156935, 0.25034179805021706, 0.22107308873451115, 0.4972882979688684, 0.23320956105153978, 0.20112639839014623, 0.19951548381773, 0.13520134176646303]}, "task_prompt": ""}
{"id": "67bd6f88-ee5d-46f0-befd-866ddc255834", "fitness": 0.3844654282608082, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional search with orthogonal refinements and occasional heavy-tailed Lévy jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # scale population with dimension but keep it modest\n            self.pop_size = int(min(max(8, 3 * self.dim), 40))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n\n        # Outputs (set during run)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # rng\n        if self.seed is not None:\n            np.random.seed(self.seed)\n\n        # bounds handling (BBOB passes func.bounds.lb/ub — sometimes scalars)\n        lb = np.asarray(func.bounds.lb, dtype=float).ravel()\n        ub = np.asarray(func.bounds.ub, dtype=float).ravel()\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb[0]))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub[0]))\n        lb = lb.copy()\n        ub = ub.copy()\n        assert lb.shape == (self.dim,) and ub.shape == (self.dim,)\n\n        # remaining budget\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                # budget exhausted - signal to caller\n                raise StopIteration()\n            x = np.asarray(x, dtype=float).ravel()\n            if x.size != self.dim:\n                x = x.reshape(self.dim)\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        n_init = min(self.pop_size, remaining)\n        pop_x = np.zeros((n_init, self.dim), dtype=float)\n        pop_f = np.full(n_init, np.inf, dtype=float)\n        # initial sigma per individual (adaptive step-size)\n        scale_init = np.mean(ub - lb)\n        pop_sigma = np.full(n_init, max(scale_init * 0.25, 1e-3), dtype=float)\n\n        for i in range(n_init):\n            x = np.random.uniform(lb, ub)\n            try:\n                f = callf(x)\n            except StopIteration:\n                break\n            pop_x[i, :] = x\n            pop_f[i] = f\n            # jitter sigma a bit\n            pop_sigma[i] = max(pop_sigma[i] * (1.0 + 0.1 * np.random.randn()), 1e-8)\n\n        # If no population could be created (very small budget), do pure random search\n        if pop_x.shape[0] == 0 or remaining <= 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                try:\n                    callf(x)\n                except StopIteration:\n                    break\n            return self.f_opt, self.x_opt\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        try:\n            while remaining > 0:\n                # pick a parent via small tournament to balance exploration/exploitation\n                k = min(3, pop_x.shape[0])\n                cand = np.random.choice(pop_x.shape[0], size=k, replace=False)\n                # choose best among candidates with slight stochasticity\n                cand_f = pop_f[cand]\n                best_idx = cand[np.argmin(cand_f)]\n                if np.random.rand() < 0.15:\n                    # occasionally pick a random one for exploration\n                    parent_i = np.random.randint(0, pop_x.shape[0])\n                else:\n                    parent_i = best_idx\n\n                x_parent = pop_x[parent_i].copy()\n                sigma = pop_sigma[parent_i]\n\n                # sample a random search direction (normalized)\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d)\n                if nd == 0:\n                    d = np.ones(self.dim) / np.sqrt(self.dim)\n                else:\n                    d = d / nd\n\n                # primary directional trial with stochasticized step-length\n                # step length uses adaptive sigma with a log-uniform jitter\n                alpha = sigma * (10 ** np.random.uniform(-0.6, 0.6)) * (0.8 + 0.4 * np.random.rand())\n                x_try = np.clip(x_parent + alpha * d, lb, ub)\n                f_try = callf(x_try)\n\n                # success: accept and slightly increase sigma\n                if f_try < pop_f[parent_i]:\n                    pop_x[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.20, scale_init)\n                    continue\n\n                # local backtracking / small-step refinement along direction (few tries)\n                improved = False\n                for tau in (0.5, 0.25, 0.125):\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + alpha * tau * d, lb, ub)\n                    f_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop_x[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = sigma * 1.10\n                        improved = True\n                        break\n                if improved:\n                    continue\n\n                # try an orthogonal perturbation for local diversification\n                r = np.random.randn(self.dim)\n                r = r - r.dot(d) * d  # make orthogonal to d\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    r = np.random.randn(self.dim)\n                    nr = np.linalg.norm(r)\n                r = r / nr\n                beta = 0.6 * sigma * (0.8 + 0.4 * np.random.rand())\n                x_try = np.clip(x_parent + beta * r, lb, ub)\n                f_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop_x[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.12, scale_init)\n                    continue\n\n                # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n                if np.random.rand() < 0.08 and remaining > 0:\n                    # Cauchy-like heavy-tailed vector normalized by a robust scale\n                    v = np.random.standard_cauchy(size=self.dim)\n                    med = np.median(np.abs(v)) + 1e-12\n                    v = v / med\n                    # scale chosen to often be big but bounded by problem size\n                    scale = scale_init * (0.2 + np.random.rand() ** 2.5)\n                    # mix with parent to avoid complete random replacement\n                    mix = np.clip(x_parent + scale * v, lb, ub)\n                    f_jump = callf(mix)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop_x[worst_i] = mix\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(scale_init * 0.02, sigma * 0.6)\n                    else:\n                        # small chance to accept as new parent (diversify)\n                        if np.random.rand() < 0.03:\n                            pop_x[worst_i] = mix\n                            pop_f[worst_i] = f_jump\n                            pop_sigma[worst_i] = scale_init * 0.05\n                    continue\n\n                # recombination exploitation: mix two best and small noise\n                best_two = np.argsort(pop_f)[:2]\n                a, b = best_two[0], best_two[1]\n                w = 0.6 + 0.2 * np.random.rand()  # weight for best\n                x_recomb = w * pop_x[a] + (1 - w) * pop_x[b] + 0.08 * sigma * np.random.randn(self.dim)\n                x_recomb = np.clip(x_recomb, lb, ub)\n                f_recomb = callf(x_recomb)\n                if f_recomb < pop_f[parent_i]:\n                    pop_x[parent_i] = x_recomb\n                    pop_f[parent_i] = f_recomb\n                    pop_sigma[parent_i] = max(sigma * 0.85, 1e-8)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_recomb < pop_f[worst_i]:\n                        pop_x[worst_i] = x_recomb\n                        pop_f[worst_i] = f_recomb\n                        pop_sigma[worst_i] = max(sigma * 0.7, 1e-8)\n\n                # adapt parent sigma on failure: slightly contract\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.94, 1e-12)\n\n                # occasional population rejuvenation by replacing the worst with a random sample\n                if remaining > 0 and np.random.rand() < 0.02:\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = np.random.uniform(lb, ub)\n                    f_new = callf(x_new)\n                    pop_x[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = max(scale_init * 0.03, pop_sigma[worst_i] * 0.7)\n\n        except StopIteration:\n            # budget exhausted\n            pass\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.384 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16266911231566683, 0.17176058779255388, 0.25305699575645, 0.9517250439934518, 0.3035428438048612, 0.9571916296051788, 0.22756698970506695, 0.37960622701832925, 0.2800212114240509, 0.1575136411924727]}, "task_prompt": ""}
{"id": "b0553abb-f26a-410d-9be7-2e522bf43bd5", "fitness": "-inf", "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements and occasional Lévy-like jumps to balance exploitation and global escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (adaptive default)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        assert budget > 0 and dim > 0\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # default population scales with dimension but bounded and also tied to budget\n        if pop_size is None:\n            # baseline 4 * dim but not too big and limited by budget\n            pop_size = max(4 * self.dim, 8)\n            pop_size = min(pop_size, 40)\n            pop_size = min(pop_size, max(2, self.budget // 6))\n        self.pop_size = max(2, int(pop_size))\n\n    def __call__(self, func):\n        # Determine bounds (BBOB style usually provides func.bounds.lb / ub)\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback to [-5,5]^dim\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Ensure full-dimension bounds\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        assert np.all(span > 0)\n\n        remaining = self.budget\n        evals = 0\n\n        best_f = np.inf\n        best_x = None\n\n        def callf(x):\n            nonlocal remaining, evals, best_f, best_x\n            if remaining <= 0:\n                raise RuntimeError(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            evals += 1\n            if f < best_f:\n                best_f = float(f)\n                best_x = x.copy()\n            return float(f), x\n\n        # If budget extremely small, do pure random sampling\n        if remaining < 2:\n            # simple random search\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return best_f, best_x\n\n        # Initialize population (as many as budget allows but at most pop_size)\n        pop = []\n        pop_f = []\n        # initial per-individual sigma (scalar) roughly proportional to domain\n        sigma = []\n\n        target_init = min(self.pop_size, max(2, remaining // 3))\n        for i in range(target_init):\n            if remaining <= 0:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # initial sigma: randomize around domain scale\n            sigma.append(0.2 * np.linalg.norm(span) * (0.5 + self.rng.rand()))\n        pop = np.array(pop) if pop else np.empty((0, self.dim))\n        pop_f = np.array(pop_f) if pop_f else np.empty((0,))\n        sigma = np.array(sigma) if sigma else np.empty((0,))\n\n        # If no population was created (very small budget), do random search until budget exhausted\n        if pop.shape[0] == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return best_f, best_x\n\n        pop_size = pop.shape[0]\n\n        # Precompute some constants\n        orth_attempts = 2\n        backtrack_tries = 3\n        levy_prob = 0.06\n        rejuvenation_prob = 0.01\n        tournament_k = min(3, pop_size)\n\n        # Helper: replace worst if candidate better, else optionally do nothing\n        def replace_worst_if_better(x_candidate, f_candidate):\n            nonlocal pop, pop_f, sigma\n            worst_idx = np.argmax(pop_f)\n            if f_candidate < pop_f[worst_idx]:\n                pop[worst_idx] = x_candidate.copy()\n                pop_f[worst_idx] = f_candidate\n                # adopt a moderate sigma for replaced individual\n                sigma[worst_idx] = max(1e-8, np.mean(sigma) if sigma.size else 1.0)\n                return True\n            return False\n\n        # Main optimization loop\n        while remaining > 0:\n            # choose a parent by small tournament\n            idxs = self.rng.choice(pop_size, size=tournament_k, replace=False)\n            parent_idx = idxs[np.argmin(pop_f[idxs])]\n            parent_x = pop[parent_idx].copy()\n            parent_f = pop_f[parent_idx]\n            parent_sigma = sigma[parent_idx]\n\n            # sample a random direction (normalized)\n            dir_raw = self.rng.normal(size=self.dim)\n            norm_dir = np.linalg.norm(dir_raw)\n            if norm_dir == 0:\n                dir_vec = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                dir_vec = dir_raw / norm_dir\n\n            # primary directional trial with stochasticized step-length\n            # step length uses log-normal like jitter to adapt exploration\n            step_multiplier = np.exp(self.rng.normal(scale=0.35))\n            step_len = parent_sigma * step_multiplier\n            x_trial = parent_x + step_len * dir_vec\n            # clip & evaluate if budget allows\n            if remaining <= 0:\n                break\n            f_trial, x_trial = callf(x_trial)\n\n            if f_trial < parent_f:\n                # accept and slightly increase sigma for this individual\n                pop[parent_idx] = x_trial\n                pop_f[parent_idx] = f_trial\n                sigma[parent_idx] = parent_sigma * (1.08 + 0.02 * self.rng.rand())\n                parent_x = x_trial.copy()\n                parent_f = f_trial\n            else:\n                # local backtracking / small-step refinement along direction\n                s = step_len\n                accepted = False\n                for bt in range(backtrack_tries):\n                    s *= 0.5\n                    x_bt = parent_x + s * dir_vec\n                    if remaining <= 0:\n                        break\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt < parent_f:\n                        pop[parent_idx] = x_bt\n                        pop_f[parent_idx] = f_bt\n                        sigma[parent_idx] = max(1e-12, parent_sigma * (0.95 + 0.02 * self.rng.rand()))\n                        parent_x = x_bt.copy()\n                        parent_f = f_bt\n                        accepted = True\n                        break\n                if not accepted:\n                    # adapt sigma downward on failure\n                    sigma[parent_idx] = max(1e-12, parent_sigma * 0.92)\n\n            # orthogonal perturbation for local diversification\n            for _ in range(orth_attempts):\n                v = self.rng.normal(size=self.dim)\n                # make orthogonal to dir_vec\n                v = v - np.dot(v, dir_vec) * dir_vec\n                nv = np.linalg.norm(v)\n                if nv == 0:\n                    continue\n                v = v / nv\n                step = 0.6 * sigma[parent_idx]\n                x_orth = parent_x + step * v\n                if remaining <= 0:\n                    break\n                f_orth, x_orth = callf(x_orth)\n                if f_orth < parent_f:\n                    pop[parent_idx] = x_orth\n                    pop_f[parent_idx] = f_orth\n                    sigma[parent_idx] = sigma[parent_idx] * 1.03\n                    parent_x = x_orth.copy()\n                    parent_f = f_orth\n\n            # occasional Lévy-like jump to escape local basins\n            if self.rng.rand() < levy_prob and remaining > 0:\n                # sample heavy-tailed vector (Cauchy-like) but robustly normalized\n                levy_raw = self.rng.standard_cauchy(size=self.dim)\n                # clip extreme outliers to avoid infinities but keep heavy tail behavior\n                levy_raw = np.clip(levy_raw, -1e6, 1e6)\n                # robust scale: median absolute deviation\n                mad = np.median(np.abs(levy_raw - np.median(levy_raw))) + 1e-9\n                levy_norm = levy_raw / (mad)\n                # combine with exponential scale to vary jump length\n                scale = sigma[parent_idx] * (1.0 + self.rng.exponential(scale=1.0))\n                levy_step = levy_norm\n                ln = np.linalg.norm(levy_step)\n                if ln > 0:\n                    levy_step = levy_step / ln * scale * (0.5 + self.rng.rand() * 5.0)\n                else:\n                    levy_step = (self.rng.normal(size=self.dim) / np.sqrt(self.dim)) * scale\n                x_jump = parent_x + levy_step\n                # clamp to bounds and evaluate\n                if remaining <= 0:\n                    break\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < parent_f:\n                    # successful escape: replace worst with jump (and also make it new parent)\n                    replace_worst_if_better(x_jump, f_jump)\n                    # set parent to jump if it's an improvement\n                    pop[parent_idx] = x_jump\n                    pop_f[parent_idx] = f_jump\n                    sigma[parent_idx] = max(1e-12, scale)\n                else:\n                    # maybe keep as candidate: replace worst with some small probability if it's competitive\n                    worst_idx = np.argmax(pop_f)\n                    if f_jump < pop_f[worst_idx] or self.rng.rand() < 0.02:\n                        pop[worst_idx] = x_jump\n                        pop_f[worst_idx] = f_jump\n                        sigma[worst_idx] = max(1e-12, scale * 0.8)\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0:\n                best_idxs = np.argsort(pop_f)[:2]\n                if best_idxs.size >= 2:\n                    p1, p2 = best_idxs[0], best_idxs[1]\n                    child = 0.5 * (pop[p1] + pop[p2]) + self.rng.normal(scale=0.08 * np.mean(sigma + 1e-12), size=self.dim)\n                    # small random bias toward parent\n                    child = np.minimum(np.maximum(child, lb), ub)\n                    if remaining <= 0:\n                        break\n                    f_child, child = callf(child)\n                    if f_child < pop[parent_idx].tolist() and f_child < pop_f[parent_idx]:\n                        pop[parent_idx] = child\n                        pop_f[parent_idx] = f_child\n                        sigma[parent_idx] = max(1e-12, 0.9 * (sigma[p1] + sigma[p2]).mean())\n                    else:\n                        # try to inject into population by replacing worst if better\n                        replace_worst_if_better(child, f_child)\n\n            # adapt parent sigma on recent performance (slight cooling if stagnating)\n            sigma[parent_idx] = max(1e-12, sigma[parent_idx] * (0.995 + 0.01 * self.rng.rand()))\n\n            # occasional population rejuvenation by random replacement\n            if remaining > 0 and self.rng.rand() < rejuvenation_prob:\n                worst_idx = np.argmax(pop_f)\n                x_new = self.rng.uniform(lb, ub)\n                if remaining <= 0:\n                    break\n                f_new, x_new = callf(x_new)\n                pop[worst_idx] = x_new\n                pop_f[worst_idx] = f_new\n                sigma[worst_idx] = 0.2 * np.linalg.norm(span) * (0.3 + 0.7 * self.rng.rand())\n\n            # defensive check: keep sigma sizes reasonable relative to domain\n            sigma = np.clip(sigma, 1e-12, 5.0 * np.linalg.norm(span))\n\n            # ensure arrays consistent (in rare numerical cases)\n            pop_size = pop.shape[0]\n            if pop_f.size != pop_size:\n                pop_f = np.resize(pop_f, pop_size)\n\n        # finished budget\n        return best_f, best_x", "configspace": "", "generation": 0, "feedback": "In the code, line 247, in __call__, the following error occurred:\nTypeError: '<' not supported between instances of 'float' and 'list'\nOn line: if f_child < pop[parent_idx].tolist() and f_child < pop_f[parent_idx]:", "error": "In the code, line 247, in __call__, the following error occurred:\nTypeError: '<' not supported between instances of 'float' and 'list'\nOn line: if f_child < pop[parent_idx].tolist() and f_child < pop_f[parent_idx]:", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "42f8e50a-959c-472c-8a30-821c60c741a8", "fitness": 0.5485648432474323, "name": "ADLS", "description": "Adaptive Directional Lévy Search — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy-style jumps to explore and exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds and make them full-dimension arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        # ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # internal budget counter\n        remaining = int(self.budget)\n\n        # reset best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # quick fallback if no budget\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []          # list of numpy arrays (solutions)\n        pop_f = []        # objective values\n        pop_sigma = []    # adaptive step-sizes per individual\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial global scale\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = max(np.linalg.norm(d), 1e-12)\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_bt = np.clip(x_parent + frac * sigma * d, lb, ub)\n                try:\n                    f_bt, x_bt = callf(x_bt)\n                except RuntimeError:\n                    break\n                if f_bt < pop_f[parent_i]:\n                    pop[parent_i] = x_bt.copy()\n                    pop_f[parent_i] = f_bt\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d  # make orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_or = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_or, x_or = callf(x_or)\n                    except RuntimeError:\n                        break\n                    if f_or < pop_f[parent_i]:\n                        pop[parent_i] = x_or.copy()\n                        pop_f[parent_i] = f_or\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_lv = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_lv, x_lv = callf(x_lv)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_lv < pop_f[worst_i]:\n                    pop[worst_i] = x_lv.copy()\n                    pop_f[worst_i] = f_lv\n                    pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_re = np.clip(mix + noise, lb, ub)\n                try:\n                    f_re, x_re = callf(x_re)\n                except RuntimeError:\n                    break\n                if f_re < pop_f[parent_i]:\n                    pop[parent_i] = x_re.copy()\n                    pop_f[parent_i] = f_re\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_re < pop_f[worst_i]:\n                        pop[worst_i] = x_re.copy()\n                        pop_f[worst_i] = f_re\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (conservative shrink)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.549 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13611947584219797, 0.2137186357748746, 0.6115717580868787, 0.9689668402060738, 0.9003520787860336, 0.961455522598228, 0.2631000100120917, 0.47387783133678096, 0.7846422091437397, 0.17184407068742336]}, "task_prompt": ""}
{"id": "a1a459d5-d4f7-4b65-ad19-ec35ee3e59a2", "fitness": 0.403548421213635, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive directional searches with orthogonal refinements and occasional heavy-tailed Lévy jumps for robust exploration/exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by default)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # Use a local RNG to avoid altering global state\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # get bounds and cast to arrays of length dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        # safety: if provided bounds have different length, broadcast or truncate\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper: safe evaluator that enforces budget and tracks best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise StopIteration(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is zero, nothing to do\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n        for i in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except StopIteration:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n\n        # If no population could be created, do random single-sample attempts until budget exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                try:\n                    callf(x)\n                except StopIteration:\n                    break\n            return self.f_opt, self.x_opt\n\n        # convert fitness and sigma to numpy arrays for convenience\n        pop_f = np.array(pop_f, dtype=float)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n\n        # main optimization loop\n        while remaining > 0:\n            # choose a parent by small tournament\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction and normalize\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                continue\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * self.rng.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except StopIteration:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # accept improvement and expand sigma slightly\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except StopIteration:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            # remove component along d to get orthogonal vector\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.5 * sigma * r, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except StopIteration:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                step = self.rng.standard_cauchy(size=self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                # heavy-tailed but scaled to problem size\n                scale_vec = 0.2 * (ub - lb)\n                step_scaled = (step / denom) * scale_vec\n                x_try = np.clip(x_parent + step_scaled, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except StopIteration:\n                    break\n                # replace worst in population if better\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and self.rng.rand() < 0.4:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = 0.05 * np.mean(ub - lb) * self.rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except StopIteration:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                # continue main loop after recombination attempt\n                continue\n\n            # if reached here, the primary attempt and refinements failed: penalize sigma\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except StopIteration:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n        # return the best found solution\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.404 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11630764742746746, 0.1606023708943598, 0.45164189859105575, 0.9770837695832715, 0.2927194860234653, 0.9159362969250002, 0.23239991821420014, 0.3984625668923173, 0.3318587531891931, 0.15847150439601876]}, "task_prompt": ""}
{"id": "ace68f11-9b2e-48ca-9b2c-7ca4892de420", "fitness": 0.39004288416926797, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional searches with orthogonal refinements, occasional Lévy escapes and light recombination.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_size: desired population size (default scales with dim)\n    - seed: random seed (optional)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # best-so-far\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds (support scalar or array)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        remaining = int(self.budget)\n\n        # helper evaluation that updates remaining and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip before evaluation to keep feasibility\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            nonlocal_f = f  # local alias for clarity\n            if nonlocal_f < self.f_opt:\n                self.f_opt = float(nonlocal_f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # trivial fallback\n        if remaining <= 0:\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population within budget\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base scale tied to domain size\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n        n_init = min(self.pop_size, remaining)  # can't initialize more than budget\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize sigma with small variability\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * np.random.rand()))\n        # If no members created, do pure random sampling with remaining budget\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # parameters\n        tournament_k = min(3, len(pop))\n        backtrack_fracs = [0.5, 0.25, 0.125]\n        levy_prob = 0.08\n        rejuvenation_prob = 0.03\n        orthogonal_scale = 0.6\n        recomb_noise_scale = 0.01\n        beta_mean = 0.6\n\n        # main loop\n        while remaining > 0:\n            # pick parent by small tournament (choose best among random k)\n            inds = np.random.choice(len(pop), tournament_k, replace=False)\n            parent_i = inds[np.argmin([pop_f[i] for i in inds])]\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # directional primary trial\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # stochasticized step length (adaptive log-normal-like perturbation)\n            step_len = sigma * np.exp(0.3 * np.random.randn()) * (0.5 + 0.5 * np.random.rand())\n            x_try = np.clip(x_parent + d * step_len, lb, ub)\n\n            improved = False\n            if remaining > 0:\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < f_parent:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.12, np.mean(ub - lb))\n                    continue  # go to next iteration\n\n            # backtracking along direction (smaller fractions)\n            for frac in backtrack_fracs:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + d * (sigma * frac), lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation to the direction for local diversification\n            # construct r orthogonal to d\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr < 1e-12:\n                r = np.random.randn(self.dim)\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r) + 1e-12\n            r = r / nr\n            x_try = np.clip(x_parent + orthogonal_scale * sigma * r * (0.5 + 0.5 * np.random.rand()), lb, ub)\n            if remaining > 0:\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    continue\n                else:\n                    # slight increase of sigma for diversification\n                    pop_sigma[parent_i] = min(sigma * 1.07, np.mean(ub - lb))\n\n            # occasional Lévy-like jump (Cauchy) to escape basins\n            if remaining > 0 and np.random.rand() < levy_prob:\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace worst if better\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * 0.5\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two good individuals + small gaussian noise\n            if len(pop) >= 2 and remaining > 0:\n                # pick two distinct parents (prefer better ones)\n                sorted_idx = np.argsort(pop_f)\n                a, b = sorted_idx[0], sorted_idx[min(1, len(sorted_idx)-1)]\n                beta = np.clip(beta_mean + 0.1 * np.random.randn(), 0.1, 0.9)\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = recomb_noise_scale * (ub - lb) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    continue\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                        continue\n\n            # adapt sigma on failure (shrink)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation: replace worst with fresh random\n            if remaining > 0 and np.random.rand() < rejuvenation_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n            # small safeguard: if some individuals are stale and budget remains, perturb them\n            if remaining > 0 and np.random.rand() < 0.02:\n                idx = np.random.randint(len(pop))\n                sigma_i = pop_sigma[idx]\n                x_new = np.clip(pop[idx] + 0.2 * sigma_i * np.random.randn(self.dim), lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                if f_new < pop_f[idx]:\n                    pop[idx] = x_new\n                    pop_f[idx] = f_new\n                    pop_sigma[idx] = min(pop_sigma[idx] * 1.08, np.mean(ub - lb))\n                else:\n                    pop_sigma[idx] = max(pop_sigma[idx] * 0.9, 1e-12)\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.390 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1534943528273104, 0.15047095807315403, 0.4724326051368899, 0.179182016252874, 0.73061249028883, 0.6868576058711643, 0.25541342534389044, 0.40299075367856396, 0.730275369247289, 0.13869926497271345]}, "task_prompt": ""}
{"id": "09dc6b1e-e04c-4274-888d-722a283a41c7", "fitness": 0.5126872706562287, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that mix directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy (Cauchy) jumps to explore and exploit continuous search spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(int(seed))\n\n    def __call__(self, func):\n        # Prepare bounds as full-dim arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # robust ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        base_scale = (ub - lb)\n        # base sigma as a fraction of range per-dim; used for initializing and resets\n        base_sigma = 0.12 * base_scale  # modest initial step (12% of range)\n        # scalar fallback when needed\n        base_sigma_mean = max(1e-12, float(np.mean(base_sigma)))\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If no budget, return trivial\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population (as many as budget allows, up to pop_size)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # per-individual sigma initialized around mean base sigma but randomized\n            pop_sigma.append(max(1e-12, base_sigma_mean * (0.8 + 0.4 * np.random.rand())))\n\n        # If we could not create any population (extremely small budget), do random sampling with remaining budget\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return self.f_opt, self.x_opt\n\n        # Main optimization loop\n        while remaining > 0:\n            # pick a parent via a small tournament (size up to 3)\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            # select the best among the tournament (exploitation bias)\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = max(pop_sigma[parent_i], 1e-12)\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                # degenerate; skip this iteration\n                pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                continue\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(base_scale))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    improved = False\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    # modest sigma increase on successful refinement\n                    pop_sigma[parent_i] = max(sigma * 1.05, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d  # make r orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                # randomize orthogonal step magnitude (smaller than primary)\n                ortho_step = 0.4 * sigma * (0.5 + 0.5 * np.random.rand())\n                x_try = np.clip(x_parent + ortho_step * r, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 1.1, 1e-12)\n                    continue\n\n            # occasional Lévy-like (Cauchy) jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                step = np.random.standard_cauchy(size=self.dim)\n                # normalize deltas to avoid extreme scale while keeping heavy-tail property\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * base_scale\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump attempt (we do not necessarily continue to recombination here)\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = 0.3 + 0.4 * np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * base_scale) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma_mean * 0.5\n\n            # adapt parent sigma on failure (make it a bit smaller)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma_mean * (0.7 + 0.6 * np.random.rand())\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.513 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1001484081803532, 0.15714639038580758, 0.9408737015404661, 0.9603494685390441, 0.2826321997184964, 0.9526399490566214, 0.23560253963185085, 0.4904486938163968, 0.8754478480928793, 0.13158350760037196]}, "task_prompt": ""}
{"id": "6ad1117f-e0ee-4913-b295-90c71bfb9b80", "fitness": 0.5632033961610134, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional searches with orthogonal refinements and occasional heavy-tailed Lévy jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales modestly with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # try to get bounds from func; fallback to [-5,5] if not available\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure lb/ub are full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # clip helper\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            x = clip(x)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if f is not None and (not np.isnan(f)) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n        if remaining < 5:\n            # simple random sampling for all remaining budget\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale\n\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # recompute best/worst indices\n            best_idx = int(np.argmin(pop_f))\n            worst_idx = int(np.argmax(pop_f))\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            contestants = self.rng.choice(len(pop), size=k, replace=False)\n            parent_i = int(contestants[np.argmin([pop_f[c] for c in contestants])])\n            sigma = float(pop_sigma[parent_i])\n            x_parent = pop[parent_i].copy()\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * self.rng.randn()))\n            x_try = clip(x_parent + alpha * d)\n            improved = False\n\n            # attempt primary try\n            if remaining > 0:\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                    improved = True\n                else:\n                    # local backtracking / small-step refinement along direction (few tries)\n                    for frac in (0.5, 0.25, 0.125):\n                        if remaining <= 0:\n                            break\n                        x_try = clip(x_parent + alpha * frac * d)\n                        try:\n                            f_try, x_try = callf(x_try)\n                        except RuntimeError:\n                            break\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                            improved = True\n                            break\n\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = self.rng.randn(self.dim)\n                # make orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    beta = sigma * (0.6 + 0.4 * self.rng.rand())\n                    x_try = clip(x_parent + beta * r)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.12, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # generate Cauchy-like heavy tail and robustly scale it\n                levy = self.rng.standard_cauchy(size=self.dim)\n                # guard extreme outliers: scale robustly by median absolute deviation\n                mad = np.median(np.abs(levy - np.median(levy))) + 1e-12\n                scale = (np.mean(ub - lb) * (0.5 + self.rng.rand()))\n                scale_vec = levy / (mad * np.sqrt(self.dim) + 1e-12) * scale\n                # clip magnitude to avoid numerical blowups\n                max_step = 5.0 * np.mean(ub - lb)\n                norm_sv = np.linalg.norm(scale_vec)\n                if norm_sv > max_step:\n                    scale_vec = scale_vec / (norm_sv + 1e-12) * max_step\n                x_try = clip(x_parent + scale_vec)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    if f_try < pop_f[worst_idx]:\n                        pop[worst_idx] = x_try\n                        pop_f[worst_idx] = f_try\n                        pop_sigma[worst_idx] = max(1e-12, sigma * 0.5)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2:\n                sorted_idx = np.argsort(pop_f)\n                a, b = int(sorted_idx[0]), int(sorted_idx[1])\n                mix = 0.5 * (pop[a] + pop[b])\n                noise = (0.01 * (ub - lb)) * self.rng.randn(self.dim)\n                x_try = clip(mix + noise)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # replace parent if improved, else possibly replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = base_sigma * 0.8\n                        continue\n                    # try to inject into population by replacing the worst if it's better\n                    if f_try < pop_f[worst_idx]:\n                        pop[worst_idx] = x_try\n                        pop_f[worst_idx] = f_try\n                        pop_sigma[worst_idx] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (shrinks to encourage finer search)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < 0.06 and remaining > 0:\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_idx] = x_new\n                pop_f[worst_idx] = f_new\n                pop_sigma[worst_idx] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n            # maintain population size: if we have extra budget create small mutants occasionally\n            if self.rng.rand() < 0.04 and remaining > 0 and len(pop) < self.pop_size:\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.563 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14036679461653545, 0.15867884593556825, 0.8364032142595255, 0.9259704615702413, 0.912487110622049, 0.9294514924927151, 0.22296732444646872, 0.5445501257555336, 0.8070069881912885, 0.15415160372020986]}, "task_prompt": ""}
{"id": "1ec3ebfe-8d32-40bd-a9ca-ca0bb701efb7", "fitness": 0.5255387219934248, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that combine randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest function of dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that scales with dimension but keeps evaluations manageable\n            self.pop_size = max(4, min(20, int(2 + 1.5 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds from the black-box wrapper\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        # ensure full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # safeguard: if provided bounds do not match dim, try to broadcast\n        if lb.shape[0] != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.shape[0] != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        remaining = int(self.budget)\n\n        # helper to call func while tracking remaining evaluations and best seen\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return np.inf, None\n            # ensure numpy array and clipped to bounds\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # trivial case: no budget\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initial population (random uniform) and individual sigmas\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial step scale relative to search box\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            if remaining <= 0:\n                break\n            x = np.random.uniform(lb, ub)\n            f, x = callf(x)\n            pop.append(x.copy())\n            pop_f.append(f)\n            pop_sigma.append(base_sigma * (0.9 + 0.2 * np.random.rand()))\n\n        # If we couldn't form any population (very tiny budget), fallback to greedy random samples\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = np.array(pop)  # shape (N, dim)\n        pop_f = np.array(pop_f, dtype=float)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n        N = pop.shape[0]\n\n        # main loop: iteratively use remaining evaluations\n        # algorithm mixes directional local search, orthogonal tweaks, Lévy jumps, recombination, rejuvenation\n        while remaining > 0:\n            # pick a parent by small tournament selection (prefer low function values)\n            k = min(3, N)\n            inds = np.random.choice(N, k, replace=False)\n            values = pop_f[inds]\n            parent_rel = int(np.argmin(values))\n            parent_i = inds[parent_rel]\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochastic step length\n            alpha = max(1e-12, sigma * (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            if remaining < 0:\n                break\n\n            if f_try < f_parent:\n                # success: accept and moderately increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue  # move to next iteration to exploit improvement\n\n            # backtracking / local zoom: try smaller steps along same direction a few times\n            improved = False\n            alpha_bt = alpha * 0.5\n            for _ in range(3):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha_bt * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < f_parent:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n                alpha_bt *= 0.5\n            if improved:\n                continue\n\n            # orthogonal refinement: small perturbation orthogonal to d for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make orthogonal to d\n                r -= np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < f_parent:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # generate Cauchy-like (standard Cauchy) heavy-tailed steps and scale by box size and sigma\n                # use robust scaling to avoid single giant leaps always (keep occasional big jumps)\n                step = np.random.standard_cauchy(self.dim)\n                # clip extreme tails to avoid numerical issues but preserve heavy-tail behavior\n                step = np.clip(step, -1e6, 1e6)\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + np.sign(step) * np.abs(step) * scale_vec * (0.5 + 0.5 * np.random.rand()), lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < np.max(pop_f):\n                    # replace worst if better\n                    worst_i = int(np.argmax(pop_f))\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best individuals and add small noise\n            if N >= 2 and remaining > 0:\n                best_two = np.argsort(pop_f)[:2]\n                x_mix = 0.5 * (pop[best_two[0]] + pop[best_two[1]])\n                noise = np.random.randn(self.dim) * (0.05 * np.mean(ub - lb)) * np.random.rand()\n                x_try = np.clip(x_mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(0.9 * sigma, 1e-12)\n                else:\n                    # try to inject by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (no improvement)\n            # slight random walk for sigma to encourage exploration/exploitation balance\n            pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * (1.0 + 0.1 * (np.random.rand() - 0.5)))\n\n            # occasional population rejuvenation: replace the worst with a random sample\n            if np.random.rand() < 0.02 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n\n            # keep arrays consistent (in case we mutated arrays)\n            # (no-op but keeps habit of syncing shape)\n            N = pop.shape[0]\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.526 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09511062712469998, 0.30134139868850096, 0.7627706308137416, 0.8890044476091824, 0.6865277251672022, 0.8905171801584401, 0.2927129927434188, 0.5098377802203311, 0.6662074554201993, 0.1613569819885312]}, "task_prompt": ""}
{"id": "d136d586-6c99-4120-94d7-c04df609999a", "fitness": 0.5174103862138022, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive directional local search with orthogonal refinements, occasional Lévy jumps and light recombination to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size; if None it's chosen from dim and budget\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # bounds to arrays of correct shape\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        rng = np.random\n        span = ub - lb\n        avg_span = float(np.mean(span))\n\n        # bookkeeping\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # safe evaluator that clips inputs and enforces budget\n        def callf(x):\n            nonlocal remaining, lb, ub\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            x = np.clip(x, lb, ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                # store a copy so later mutations don't change it\n                self.x_opt = x.copy()\n            return f, x\n\n        # determine population size reasonably sized with respect to dimension and budget\n        if self.pop_size is None:\n            default_pop = max(6, min(self.dim * 3, 40))\n            self.pop_size = min(default_pop, max(1, self.budget // 12))\n        pop_size = max(1, int(self.pop_size))\n\n        # base sigma relative to problem scale\n        base_sigma = max(1e-8, 0.08 * avg_span)\n\n        # initialize population (random samples)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(pop_size, remaining)\n        for _ in range(n_init):\n            x0 = rng.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * rng.rand()))\n\n        # If very small budget made population impossible, fallback to random search until budget exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = rng.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return self.f_opt, self.x_opt\n\n        # main adaptive search loop\n        try:\n            while remaining > 0:\n                # parent selection via small tournament\n                k = min(3, len(pop))\n                inds = rng.choice(len(pop), size=k, replace=False)\n                # pick best among sampled\n                parent_rel = int(np.argmin([pop_f[i] for i in inds]))\n                parent_i = inds[parent_rel]\n                x_parent = pop[parent_i].copy()\n                sigma = pop_sigma[parent_i]\n\n                # sample a random search direction (normalized)\n                d = rng.normal(size=self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    pop_sigma[parent_i] *= 0.98\n                    continue\n                d = d / nd\n\n                # primary directional trial with stochastic step length\n                alpha = sigma * (0.6 + 1.4 * rng.rand())  # randomized step size\n                x_try = np.clip(x_parent + alpha * d, lb, ub)\n                f_try, x_try = callf(x_try)\n\n                # success: accept and slightly increase sigma\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(base_sigma * 10, pop_sigma[parent_i] * (1.06 + 0.04 * rng.rand()))\n                    continue\n\n                # local backtracking / small-step refinement along direction (few tries)\n                improved = False\n                for frac in (0.5, 0.25, 0.125):\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(base_sigma * 10, pop_sigma[parent_i] * (1.03 + 0.03 * rng.rand()))\n                        improved = True\n                        break\n                if improved:\n                    continue\n\n                # try an orthogonal perturbation for local diversification\n                if remaining > 0:\n                    v = rng.normal(size=self.dim)\n                    # make orthogonal to d\n                    v = v - np.dot(v, d) * d\n                    nv = np.linalg.norm(v)\n                    if nv > 1e-12:\n                        v /= nv\n                        x_try = np.clip(x_parent + sigma * 0.5 * v, lb, ub)\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = min(base_sigma * 10, pop_sigma[parent_i] * 1.04)\n                            continue\n\n                # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n                if rng.rand() < 0.08 and remaining > 0:\n                    # standard Cauchy components, robust-normalize to preserve heavy tail but avoid extreme blowups\n                    u = rng.rand(self.dim)\n                    cauchy = np.tan(np.pi * (u - 0.5))\n                    denom = np.median(np.abs(cauchy)) + 1e-9\n                    scale = avg_span * (0.25 + rng.rand() * 1.25)\n                    step = (cauchy / denom) * scale\n                    x_try = np.clip(x_parent + step, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(base_sigma * 0.5, pop_sigma[worst_i] * 0.9)\n                    # continue main loop after jump attempt\n                    continue\n\n                # recombination exploitation: mix two best and small noise\n                if rng.rand() < 0.20 and remaining > 0 and len(pop) > 1:\n                    best2 = np.argsort(pop_f)[:2]\n                    beta = rng.rand()\n                    noise_scale = 0.03 * avg_span\n                    x_mix = beta * pop[best2[0]] + (1.0 - beta) * pop[best2[1]] + rng.normal(scale=noise_scale, size=self.dim)\n                    x_mix = np.clip(x_mix, lb, ub)\n                    f_try, x_mix = callf(x_mix)\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * rng.rand())\n                    else:\n                        # adapt parent sigma on failure (mild shrink)\n                        pop_sigma[parent_i] *= 0.94\n\n                # adapt parent sigma on failure (mild shrink)\n                pop_sigma[parent_i] *= 0.95\n                pop_sigma[parent_i] = np.clip(pop_sigma[parent_i], 1e-10, avg_span * 5.0)\n\n                # occasional population rejuvenation by replacing the worst with a random sample\n                if rng.rand() < 0.05 and remaining > 0:\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = rng.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * rng.rand())\n\n                # if population has vacancies (shouldn't normally), fill them\n                while len(pop) < pop_size and remaining > 0:\n                    x_new = rng.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop.append(x_new)\n                    pop_f.append(f_new)\n                    pop_sigma.append(base_sigma * (0.6 + 0.8 * rng.rand()))\n\n        except RuntimeError:\n            # budget exhausted - safe termination\n            pass\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.517 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12695319470676403, 0.21556514218836542, 0.6598259074262516, 0.8723134994265039, 0.7378634356788818, 0.8086843729105093, 0.2916339300581624, 0.5368484857132294, 0.7839160749500353, 0.14049981907931952]}, "task_prompt": ""}
{"id": "b73d2c91-394d-4e52-a428-90d3887a663b", "fitness": 0.3930662812843521, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — maintains a small population of adaptive-step points that perform randomized directional local searches, orthogonal refinements and occasional Lévy-like jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box function `func` using up to self.budget evaluations.\n        func is called like: f = func(x), and func.bounds.lb / func.bounds.ub provide bounds.\n        Returns (f_best, x_best).\n        \"\"\"\n        # bounds: make sure they are full-dim numpy arrays\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            # handle scalars\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n        except Exception:\n            # fallback to the known [-5,5] if bounds are not present\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # safe-guards on shapes\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        # remaining evaluations\n        remaining = int(self.budget)\n\n        # internal best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking remaining budget and best-so-far\n        def callf(x):\n            nonlocal remaining, lb, ub\n            if remaining <= 0:\n                # signal that we exhausted budget\n                raise StopIteration()\n            x = np.asarray(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)  # one function evaluation\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # initialize population: try to spread across bounds\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        # base sigma as fraction of search range\n        base_sigma = np.linalg.norm(ub - lb) / (2.0 * np.sqrt(max(1, self.dim)))\n\n        # initial population size reduced if extremely small budget\n        n_init = min(self.pop_size, max(1, remaining))\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except StopIteration:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize sigma around base_sigma with some variability\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n        # if no population could be created (very small budget), do random sampling until exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                try:\n                    f, x = callf(x)\n                except StopIteration:\n                    break\n            return self.f_opt, self.x_opt\n\n        pop = np.vstack(pop)\n        pop_f = np.asarray(pop_f, dtype=float)\n        pop_sigma = np.asarray(pop_sigma, dtype=float)\n\n        # main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        try:\n            while remaining > 0:\n                # pick a parent via small tournament to balance exploration/exploitation\n                k = min(3, pop.shape[0])\n                candidates = np.random.choice(pop.shape[0], size=k, replace=False)\n                parent_i = candidates[int(np.argmin(pop_f[candidates]))]\n                x_parent = pop[parent_i].copy()\n                sigma = float(pop_sigma[parent_i])\n\n                # sample a random search direction (normalized)\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d)\n                if nd == 0:\n                    continue\n                d = d / nd\n\n                # primary directional trial with stochasticized step-length\n                # alpha uses sigma scaled and some multiplicative noise\n                alpha = sigma * (1.0 + 0.25 * np.random.randn())\n                x_try = np.clip(x_parent + alpha * d, lb, ub)\n\n                f_try, x_try = callf(x_try)\n\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(base_sigma * 10.0, sigma * 1.12)\n                    # local backtracking / small-step refinement along direction (a few small tries)\n                    n_refine = 2\n                    for _ in range(n_refine):\n                        # smaller steps along same direction\n                        step = pop_sigma[parent_i] * (0.4 + 0.8 * np.random.rand())\n                        x_ref = np.clip(pop[parent_i] + step * d, lb, ub)\n                        f_ref, x_ref = callf(x_ref)\n                        if f_ref < pop_f[parent_i]:\n                            pop[parent_i] = x_ref\n                            pop_f[parent_i] = f_ref\n                            pop_sigma[parent_i] *= 1.06\n                        else:\n                            pop_sigma[parent_i] *= 0.98\n                else:\n                    # failure: reduce sigma moderately\n                    pop_sigma[parent_i] = max(base_sigma * 1e-4, sigma * 0.88)\n\n                    # try an orthogonal perturbation for local diversification\n                    # construct orthogonal vector by removing projection onto d\n                    r = np.random.randn(self.dim)\n                    r = r - np.dot(r, d) * d\n                    nr = np.linalg.norm(r)\n                    if nr > 1e-12:\n                        r = r / nr\n                        # small orthogonal steps: try a couple times\n                        for _ in range(2):\n                            step = pop_sigma[parent_i] * (0.5 + 0.5 * np.random.rand()) * 0.6\n                            x_try = np.clip(x_parent + step * r, lb, ub)\n                            f_try, x_try = callf(x_try)\n                            if f_try < pop_f[parent_i]:\n                                pop[parent_i] = x_try\n                                pop_f[parent_i] = f_try\n                                pop_sigma[parent_i] *= 1.05\n                                break\n                            else:\n                                pop_sigma[parent_i] *= 0.96\n\n                # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n                if remaining > 0 and np.random.rand() < 0.06:\n                    # Cauchy-like heavy-tailed vector: ratio of gaussians to produce heavy tails\n                    levy = np.random.standard_cauchy(size=self.dim)\n                    # robust scale: median sigma of population or base_sigma fallback\n                    robust_scale = max(base_sigma, np.median(pop_sigma))\n                    # normalize to avoid absurdly huge jumps but keep heavy tail behaviour\n                    lv_norm = np.linalg.norm(levy)\n                    if lv_norm > 0:\n                        levy = levy / lv_norm\n                    jump_scale = robust_scale * (4.0 + 6.0 * np.random.rand())\n                    x_jump = np.clip(x_parent + jump_scale * levy, lb, ub)\n                    f_jump, x_jump = callf(x_jump)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(base_sigma * 0.5, pop_sigma[parent_i] * 0.8)\n                    else:\n                        # sometimes keep as new member by replacing second-worst with small prob\n                        if np.random.rand() < 0.08:\n                            sec_worst = np.argsort(pop_f)[-2]\n                            pop[sec_worst] = x_jump\n                            pop_f[sec_worst] = f_jump\n                            pop_sigma[sec_worst] = max(base_sigma * 0.2, pop_sigma[parent_i] * 0.6)\n                    # continue main loop after jump attempt\n                    continue\n\n                # recombination exploitation: mix two best and small noise\n                if remaining > 0 and np.random.rand() < 0.18 and pop.shape[0] >= 2:\n                    idxs = np.argsort(pop_f)\n                    a, b = idxs[0], idxs[1]\n                    beta = np.random.rand()\n                    mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                    # add small gaussian perturbation scaled by local sigma\n                    noise = np.random.randn(self.dim) * (0.5 * (pop_sigma[a] + pop_sigma[b]))\n                    x_try = np.clip(mix + noise, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    # replace the parent if improved, else possibly replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(base_sigma * 1e-4, 0.9 * (pop_sigma[a] + pop_sigma[b]) / 2.0)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = max(base_sigma * 0.1, 0.8 * (pop_sigma[a] + pop_sigma[b]) / 2.0)\n\n                # occasional population rejuvenation by replacing the worst with a random sample\n                if remaining > 0 and np.random.rand() < 0.03:\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n                # small adaptive population trimming/growth: if there is room and budget, add mutated offspring\n                if remaining > 0 and pop.shape[0] < self.pop_size and np.random.rand() < 0.05:\n                    # create new offspring from best with mutation\n                    best_i = int(np.argmin(pop_f))\n                    offspring = pop[best_i] + np.random.randn(self.dim) * pop_sigma[best_i] * 0.5\n                    offspring = np.clip(offspring, lb, ub)\n                    f_off, offspring = callf(offspring)\n                    # append if budget allowed\n                    pop = np.vstack([pop, offspring])\n                    pop_f = np.append(pop_f, f_off)\n                    pop_sigma = np.append(pop_sigma, max(base_sigma * 0.2, pop_sigma[best_i] * 0.7))\n\n                # ensure arrays remain consistent shape-wise\n                if pop.shape[0] != pop_f.shape[0]:\n                    pop_f = pop_f[:pop.shape[0]]\n                if pop.shape[0] != pop_sigma.shape[0]:\n                    pop_sigma = np.resize(pop_sigma, pop.shape[0])\n\n        except StopIteration:\n            # budget exhausted from callf\n            pass\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.393 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08556897253830609, 0.1707937871587354, 0.6997909199324945, 0.9800083943151635, 0.20924141026680465, 0.8373918980054127, 0.24520268652592936, 0.34398793350286927, 0.22639829470026307, 0.13227851589754291]}, "task_prompt": ""}
{"id": "742e1fca-2408-44c5-934f-fdc435b46ac1", "fitness": 0.17485341330379253, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional local search with orthogonal refinements and occasional Lévy-like jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optionally override population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimensionality\n            self.pop_size = min(40, max(6, int(6 + 1.5 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        # Prepare bounds (func may provide scalar or arrays)\n        try:\n            lb = np.atleast_1d(func.bounds.lb)\n            ub = np.atleast_1d(func.bounds.ub)\n        except Exception:\n            # Many BBOB tasks use [-5,5]\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        # clamp bounds\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        # State\n        remaining = self.budget\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return np.inf\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = func(x)\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f\n\n        # If budget is extremely small, fallback to simple random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # initial sigma scale proportional to search range\n        init_sigma = 0.2 * np.mean(ub - lb)  # moderate initial step\n        for i in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = callf(x)\n            pop.append(x)\n            pop_f.append(f)\n            # diversify initial sigmas a bit across population\n            pop_sigma.append(init_sigma * (10 ** self.rng.uniform(-1.0, 1.0)))\n        pop = np.array(pop) if len(pop) > 0 else np.zeros((0, self.dim))\n        pop_f = np.array(pop_f) if len(pop_f) > 0 else np.array([])\n        pop_sigma = np.array(pop_sigma) if len(pop_sigma) > 0 else np.array([])\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Main loop\n        stagnation = 0\n        iter_count = 0\n        while remaining > 0:\n            iter_count += 1\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            tour_idx = self.rng.choice(len(pop), size=k, replace=False)\n            parent_i = tour_idx[np.argmin(pop_f[tour_idx])]\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            normd = np.linalg.norm(d)\n            if normd < 1e-12:\n                d = self.rng.randn(self.dim)\n                normd = np.linalg.norm(d) + 1e-12\n            d = d / normd\n\n            # primary directional trial with stochasticized step-length\n            # step magnitude: sigma times a multiplicative log-uniform noise and an exponential to allow occasional larger steps\n            step_multiplier = (10 ** self.rng.uniform(-0.4, 0.6)) * (1.0 + 0.5 * self.rng.exponential())\n            step = sigma * step_multiplier\n            x_try = x_parent + step * d\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n\n            f_try = callf(x_try)\n            if f_try < f_parent:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(np.max(ub - lb), sigma * (1.15 + 0.05 * self.rng.randn()))\n                stagnation = 0\n                continue  # continue main loop\n            else:\n                # failure: small sigma reduction\n                pop_sigma[parent_i] = max(1e-12, sigma * 0.97)\n                stagnation += 1\n\n            # local backtracking / small-step refinement along direction (few tries)\n            for alpha in (0.5, 0.25, 0.125):\n                if remaining <= 0:\n                    break\n                small_step = sigma * alpha\n                x_local = x_parent + small_step * d\n                x_local = np.minimum(np.maximum(x_local, lb), ub)\n                f_local = callf(x_local)\n                if f_local < pop_f[parent_i]:\n                    pop[parent_i] = x_local\n                    pop_f[parent_i] = f_local\n                    pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 1.08)\n                    stagnation = 0\n                    break\n\n            if remaining <= 0:\n                break\n\n            # try an orthogonal perturbation for local diversification\n            # create vector orthogonal to d\n            v = self.rng.randn(self.dim)\n            v = v - np.dot(v, d) * d\n            nv = np.linalg.norm(v)\n            if nv < 1e-12:\n                v = self.rng.randn(self.dim)\n                nv = np.linalg.norm(v) + 1e-12\n            v = v / nv\n            ortho_step = 0.6 * sigma\n            x_ortho = x_parent + ortho_step * v * (1.0 + 0.5 * self.rng.randn())\n            x_ortho = np.minimum(np.maximum(x_ortho, lb), ub)\n            f_ortho = callf(x_ortho)\n            if f_ortho < pop_f[parent_i]:\n                pop[parent_i] = x_ortho\n                pop_f[parent_i] = f_ortho\n                pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 1.05)\n                stagnation = 0\n\n            if remaining <= 0:\n                break\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            # small probability or triggered by stagnation\n            jump_prob = 0.03 + 0.0005 * stagnation\n            if self.rng.rand() < jump_prob:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                delta = self.rng.standard_cauchy(size=self.dim)\n                # robust scale normalization to avoid infinite/outsize raw values\n                median_abs = np.median(np.abs(delta)) + 1e-12\n                delta = delta / median_abs\n                # scale relative to typical sigma and bounds\n                scale = max(1.0, self.rng.exponential()) * (2.0 + 3.0 * self.rng.rand()) * sigma\n                x_jump = x_parent + scale * delta / (np.linalg.norm(delta) + 1e-12)\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                f_jump = callf(x_jump)\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[parent_i]:\n                    pop[parent_i] = x_jump\n                    pop_f[parent_i] = f_jump\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.8)\n                    stagnation = 0\n                elif f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                else:\n                    # keep attempt as temporary candidate: small sigma decrease for parent\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.5)\n                # continue loop after jump attempt\n                if remaining <= 0:\n                    break\n                else:\n                    continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2:\n                best_idx = np.argsort(pop_f)[:2]\n                x1 = pop[best_idx[0]]\n                x2 = pop[best_idx[1]]\n                beta = self.rng.rand()\n                noise_scale = 0.05 * np.mean(pop_sigma)\n                x_recomb = beta * x1 + (1 - beta) * x2 + noise_scale * self.rng.randn(self.dim)\n                x_recomb = np.minimum(np.maximum(x_recomb, lb), ub)\n                f_recomb = callf(x_recomb)\n                worst_i = int(np.argmax(pop_f))\n                if f_recomb < pop_f[parent_i]:\n                    pop[parent_i] = x_recomb\n                    pop_f[parent_i] = f_recomb\n                    pop_sigma[parent_i] = max(1e-12, np.mean(pop_sigma[best_idx]) * 0.8)\n                    stagnation = 0\n                elif f_recomb < pop_f[worst_i]:\n                    pop[worst_i] = x_recomb\n                    pop_f[worst_i] = f_recomb\n                    pop_sigma[worst_i] = max(1e-12, noise_scale * (1.0 + 0.5 * self.rng.randn()))\n\n            # adapt parent sigma on failure\n            if pop_f[parent_i] >= f_parent:\n                pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.98)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < 0.02 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = init_sigma * (10 ** self.rng.uniform(-1.0, 1.0))\n\n            # safety: if population diversity dwindles, inject randoms\n            if np.max(pop_f) - np.min(pop_f) < 1e-9 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = init_sigma\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.175 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09445126636306533, 0.14600290475701416, 0.31642606572456333, 0.19732605929436087, 0.12739106613221707, 0.20053129017692628, 0.16471790857894664, 0.1883890085020593, 0.1939114367210426, 0.11938712678772978]}, "task_prompt": ""}
{"id": "48a501f2-a776-4db2-99f8-3b351282e570", "fitness": 0.44176860964917664, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size points using randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy-like) jumps to balance exploitation and heavy-tailed exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (adaptive by default)\n    - seed: RNG seed for reproducibility\n    One-line idea: maintain a small population with per-individual adaptive step-sizes,\n    perform randomized directional local searches and orthogonal refinements, and\n    occasionally perform heavy-tailed Lévy-like jumps to escape basins.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # keep population small but scaling with dim\n        if pop_size is None:\n            self.pop_size = max(3, min(6 + self.dim // 2, int(self.budget // 20)))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        # ensure feasible pop_size\n        self.pop_size = min(self.pop_size, max(2, self.budget))\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds: try to read from func, else default [-5,5]\n        lb = None\n        ub = None\n        if hasattr(func, 'bounds'):\n            try:\n                lb = np.array(func.bounds.lb, dtype=float)\n                ub = np.array(func.bounds.ub, dtype=float)\n            except Exception:\n                lb, ub = None, None\n        if lb is None or ub is None:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # ensure full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # budget tracking\n        evals_left = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def clip_x(x):\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal evals_left, self\n            if evals_left <= 0:\n                return np.inf\n            x = clip_x(x)\n            f = func(x)\n            evals_left -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # If budget extremely small, fallback to pure random search\n        if evals_left <= 2:\n            while evals_left > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population uniformly\n        pop_n = min(self.pop_size, evals_left)\n        X = np.empty((pop_n, self.dim), dtype=float)\n        F = np.empty(pop_n, dtype=float)\n        # initialize individual sigmas (initial step lengths)\n        # start with a fraction of the search range\n        base_sigma = 0.2 * np.linalg.norm(ub - lb) / np.sqrt(self.dim)\n        sigma = np.maximum(1e-8, base_sigma * (1.0 + 0.5 * self.rng.rand(pop_n)))\n        for i in range(pop_n):\n            x = self.rng.uniform(lb, ub)\n            f = callf(x)\n            X[i] = x\n            F[i] = f\n            if evals_left <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if pop_n == 0:\n            while evals_left > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # helper to get index of worst/best\n        def idx_best():\n            return int(np.argmin(F))\n\n        def idx_worst():\n            return int(np.argmax(F))\n\n        # main loop\n        # parameters\n        p_jump = 0.06  # probability of heavy-tail jump per iteration\n        p_rejuvenate = 0.03  # probability to replace worst with random sample\n        max_backtracks = 3\n        ortho_scale = 0.5  # orthogonal step relative to sigma\n        levy_scale_mult = 2.5  # multiplier for heavy-tail overall scale\n        iters = 0\n\n        while evals_left > 0:\n            iters += 1\n            # small tournament selection to pick parent\n            k = min(3, pop_n)\n            candidates = self.rng.randint(0, pop_n, size=k)\n            parent_idx = candidates[np.argmin(F[candidates])]\n            x_parent = X[parent_idx].copy()\n            f_parent = F[parent_idx]\n            s_parent = sigma[parent_idx]\n\n            # sample random direction u\n            u = self.rng.randn(self.dim)\n            u_norm = np.linalg.norm(u)\n            if u_norm == 0:\n                u = np.ones(self.dim)\n                u_norm = np.linalg.norm(u)\n            u = u / u_norm\n\n            # primary directional trial with stochasticized step-length\n            s_trial = s_parent * max(0.5, 1.0 + 0.3 * self.rng.randn())\n            x_trial = clip_x(x_parent + s_trial * u)\n            f_trial = callf(x_trial)\n            if f_trial < f_parent:\n                # accept\n                X[parent_idx] = x_trial\n                F[parent_idx] = f_trial\n                sigma[parent_idx] = s_parent * 1.08  # slightly increase\n                continue  # successful move; next iteration\n\n            # local backtracking / small-step refinement along direction\n            back_success = False\n            s_bt = s_trial\n            for bt in range(max_backtracks):\n                if evals_left <= 0:\n                    break\n                s_bt *= 0.5\n                x_bt = clip_x(x_parent + s_bt * u)\n                f_bt = callf(x_bt)\n                if f_bt < f_parent:\n                    X[parent_idx] = x_bt\n                    F[parent_idx] = f_bt\n                    sigma[parent_idx] = s_parent * (1.03 + 0.02 * self.rng.rand())\n                    back_success = True\n                    break\n            if back_success:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if evals_left > 0:\n                # sample v and make orthogonal to u\n                v = self.rng.randn(self.dim)\n                v = v - np.dot(v, u) * u\n                v_norm = np.linalg.norm(v)\n                if v_norm > 0:\n                    v = v / v_norm\n                    s_ortho = ortho_scale * s_parent * (0.5 + self.rng.rand())\n                    x_ortho = clip_x(x_parent + s_ortho * v)\n                    f_ortho = callf(x_ortho)\n                    if f_ortho < f_parent:\n                        X[parent_idx] = x_ortho\n                        F[parent_idx] = f_ortho\n                        sigma[parent_idx] = s_parent * 1.06\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < p_jump and evals_left > 0:\n                # Cauchy-like heavy-tailed vector\n                # generate standard Cauchy and stabilize scale by median absolute\n                cauchy = self.rng.standard_cauchy(size=self.dim)\n                # to avoid overflow, clip extreme large values but keep heavy tail behavior\n                cauchy = np.clip(cauchy, -1e6, 1e6)\n                # robust scale normalization\n                med = np.median(np.abs(cauchy))\n                if med <= 0 or not np.isfinite(med):\n                    med = 1.0\n                v = cauchy / med\n                # set overall step scale relative to population sigmas and search range\n                avg_sigma = np.median(sigma)\n                global_scale = levy_scale_mult * avg_sigma + 0.1 * np.linalg.norm(ub - lb)\n                delta = v / np.linalg.norm(v) * (np.abs(v).mean()) * global_scale\n                # normalize to avoid absurd jumps across boundary\n                norm_delta = np.linalg.norm(delta)\n                max_allowed = np.linalg.norm(ub - lb) * 1.5\n                if norm_delta > max_allowed:\n                    delta = delta / norm_delta * max_allowed\n                x_levy = clip_x(x_parent + delta)\n                f_levy = callf(x_levy)\n                if f_levy < F[idx_worst()]:\n                    # replace worst\n                    w = idx_worst()\n                    X[w] = x_levy\n                    F[w] = f_levy\n                    sigma[w] = avg_sigma * (0.5 + self.rng.rand())\n                else:\n                    # maybe keep as candidate: replace parent if better than parent\n                    if f_levy < f_parent:\n                        X[parent_idx] = x_levy\n                        F[parent_idx] = f_levy\n                        sigma[parent_idx] = max(1e-8, avg_sigma * (0.8 + 0.4 * self.rng.rand()))\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if pop_n >= 2 and evals_left > 0:\n                sorted_idx = np.argsort(F)\n                b1, b2 = sorted_idx[0], sorted_idx[1]\n                wmix = self.rng.rand()\n                child = wmix * X[b1] + (1.0 - wmix) * X[b2]\n                # add small adaptive noise proportional to sigmas of parents\n                noise_scale = 0.3 * (sigma[b1] + sigma[b2]) / 2.0\n                child += noise_scale * self.rng.randn(self.dim)\n                child = clip_x(child)\n                f_child = callf(child)\n                if f_child < F[parent_idx]:\n                    X[parent_idx] = child\n                    F[parent_idx] = f_child\n                    sigma[parent_idx] = max(1e-8, (sigma[b1] + sigma[b2]) / 2.0 * 1.02)\n                    continue\n                else:\n                    # if child is good enough to replace worst\n                    w = idx_worst()\n                    if f_child < F[w]:\n                        X[w] = child\n                        F[w] = f_child\n                        sigma[w] = max(1e-8, noise_scale * (0.5 + self.rng.rand()))\n\n            # adapt parent sigma on failure\n            sigma[parent_idx] = max(1e-8, sigma[parent_idx] * 0.94)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < p_rejuvenate and evals_left > 0:\n                w = idx_worst()\n                x_new = self.rng.uniform(lb, ub)\n                f_new = callf(x_new)\n                X[w] = x_new\n                F[w] = f_new\n                sigma[w] = base_sigma * (0.5 + self.rng.rand())\n\n            # safety break if no more evaluations\n            if evals_left <= 0:\n                break\n\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.442 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.131981049220407, 0.17483696406492255, 0.9178637617864974, 0.963208776877703, 0.1968443882806873, 0.9507599590533973, 0.29223405207603215, 0.37884527405543267, 0.24605103138566542, 0.1650608396910218]}, "task_prompt": ""}
{"id": "0c9882fc-249a-47a2-a498-00bfb3af1f34", "fitness": 0.5223297056248817, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional search combining normalized directional probes, orthogonal refinements, occasional Lévy-like escapes and light recombination.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional initial population size (auto-scaled by default)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales modestly with dimensionality\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # best-so-far\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # obtain bounds (func.bounds.lb/ub can be scalar or arrays)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # robust check: ensure bounds match dim\n        if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n            lb = np.resize(lb, (self.dim,))\n            ub = np.resize(ub, (self.dim,))\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best-so-far\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return None  # no budget left\n            # ensure numpy array and clip to bounds\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # quick fallback: if no budget at all\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []         # list of x arrays\n        pop_f = []       # corresponding function values\n        pop_sigma = []   # adaptive step sizes\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))  # initial scale reference\n\n        for _ in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            res = callf(x0)\n            if res is None:\n                break\n            f0, x0_e = res\n            pop.append(x0_e.copy())\n            pop_f.append(float(f0))\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop\n        while remaining > 0:\n            # pick a parent via small tournament\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            # find best among chosen\n            vals = [pop_f[i] for i in inds]\n            parent_choice = inds[int(np.argmin(vals))]\n            parent_i = int(parent_choice)\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # try again once\n                d = np.random.randn(self.dim)\n                nd = max(1e-12, np.linalg.norm(d))\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            res = callf(x_try)\n            if res is None:\n                break\n            f_try, x_try = res\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = float(f_try)\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                res = callf(x_try)\n                if res is None:\n                    break\n                f_try, x_try = res\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = float(f_try)\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    res = callf(x_try)\n                    if res is not None:\n                        f_try, x_try = res\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try.copy()\n                            pop_f[parent_i] = float(f_try)\n                            pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                            continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization to preserve heavy-tail shape but avoid infinities\n                denom = max(1e-8, np.sqrt(np.mean(step ** 2)))\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    res = callf(x_try)\n                    if res is not None:\n                        f_try, x_try = res\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try.copy()\n                            pop_f[worst_i] = float(f_try)\n                            pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    res = callf(x_try)\n                    if res is not None:\n                        f_try, x_try = res\n                        worst_i = int(np.argmax(pop_f))\n                        # replace parent if improved, else try to inject into population by replacing worst\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try.copy()\n                            pop_f[parent_i] = float(f_try)\n                            pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        else:\n                            if f_try < pop_f[worst_i]:\n                                pop[worst_i] = x_try.copy()\n                                pop_f[worst_i] = float(f_try)\n                                pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (contraction)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                res = callf(x_new)\n                if res is not None:\n                    f_new, x_new = res\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = float(f_new)\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.522 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11811983975733387, 0.15986782067107652, 0.45940379601333214, 0.9085804997187816, 0.9390449957329869, 0.9576435190449689, 0.2593502015328356, 0.4641795446065724, 0.7990773721081796, 0.15802946706274934]}, "task_prompt": ""}
{"id": "7a3f7fb3-3420-4aa8-ae4d-732375317982", "fitness": 0.5237652738860132, "name": "ADLS", "description": "Population-based Adaptive Directional Lévy Search (ADLS) — combines adaptive step-size directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to robustly explore and exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    Parameters\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed for reproducibility\n\n    Usage: instantiate and call with a black-box `func` that supports func(x)\n    and provides func.bounds.lb / func.bounds.ub (scalars or arrays).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales modestly with dimensionality\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = None if seed is None else int(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds as full-dim numpy arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # initial bookkeeping\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        remaining = int(self.budget)\n        # RNG (use RandomState for compatibility with numpy functions below)\n        rng = np.random.RandomState(self.seed)\n\n        # evaluation wrapper that enforces budget and updates best found\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(-1)[:self.dim]\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x.copy()\n\n        # If budget is zero, return immediately\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        n_init = min(self.pop_size, remaining)\n        # initial base scale relative to bounds (avoid zero)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n\n        for i in range(n_init):\n            x0 = rng.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # give some diversity in sigmas\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * rng.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population members were created (extremely small budget), do a tiny random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = rng.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return self.f_opt, self.x_opt\n\n        # main optimization loop\n        # We iterate until budget exhausted. Each iteration uses between 1 and several evals.\n        while remaining > 0:\n            # select a parent through a small tournament (improves selection pressure)\n            k = min(3, len(pop))\n            inds = rng.choice(len(pop), k, replace=False)\n            # choose the best among the sampled indices\n            parent_i = inds[int(np.argmin([pop_f[ii] for ii in inds]))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                # degenerate direction, skip this iteration\n                pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                continue\n            d = d / nd\n\n            # stochasticized step-length (mostly local, occasionally larger)\n            # combine a normal multiplicative noise with a small chance of cauchy multiplier\n            if rng.rand() < 0.08:\n                step_mult = abs(rng.standard_normal() * 0.5 + rng.standard_cauchy() * 0.2) + 0.1\n            else:\n                step_mult = max(0.1, 1.0 + 0.25 * rng.randn())\n            step_len = sigma * step_mult\n\n            # primary directional trial\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma to exploit direction\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.25, np.mean(ub - lb))\n                continue  # move to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + step_len * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = rng.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like heavy-tailed jump to escape basins\n            if rng.rand() < 0.12 and remaining > 0:\n                step = rng.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale relative to bounds (preserves heavy tail but limits extremes)\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # if it's better than worst in population, replace worst\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # after a heavy jump continue to next iteration\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * rng.randn(self.dim)\n                x_new = np.clip(mix + noise, lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_new < pop_f[parent_i]:\n                    pop[parent_i] = x_new\n                    pop_f[parent_i] = f_new\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_new < pop_f[worst_i]:\n                        pop[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (slight decrease)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.4 * rng.rand())\n\n            # small adaptive shrink/expand of global base sigma occasionally\n            if rng.rand() < 0.01:\n                base_sigma = max(1e-12, base_sigma * (0.9 + 0.2 * rng.rand()))\n\n            # if population size is less than allowed and we have budget, try to add diversity\n            if len(pop) < self.pop_size and remaining > 0 and rng.rand() < 0.1:\n                x_new = rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.8 + 0.4 * rng.rand()))\n\n        # finished or budget exhausted\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.524 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12977353019686333, 0.15578978689870404, 0.6499092815863732, 0.8101976935784304, 0.8965851760202903, 0.9580206315130998, 0.26072774417641675, 0.4418147374929806, 0.8065979802467291, 0.12823617715024394]}, "task_prompt": ""}
{"id": "4a510cb4-c8a0-494e-bd7c-dd569ffb131a", "fitness": 0.4987098309284369, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based optimizer that combines randomized directional local searches, orthogonal refinements, adaptive step-size per individual and occasional heavy-tailed (Lévy/Cauchy-like) jumps to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # bounds from func; assume scalar or vector bounds provided\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        else:\n            # ensure they match requested dim\n            if lb.size != self.dim:\n                lb = np.resize(lb, self.dim)\n            if ub.size != self.dim:\n                ub = np.resize(ub, self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure x is numpy array of correct shape\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.clip(x, lb, ub)\n            remaining -= 1\n            f = float(func(x))\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                # store a copy\n                self.x_opt = x.copy()\n            return f, x.copy()\n\n        # If budget is extremely small, fallback to random sampling of what we can\n        if remaining <= 0:\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_scale = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale for sigma\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_scale * (1.0 + 0.5 * np.random.rand()))\n        # if we couldn't initialize any individuals because budget tiny\n        if len(pop) == 0:\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            # choose best among sampled inds\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            # ensure alpha not exactly zero to move\n            if abs(alpha) < 1e-16:\n                alpha = sigma * (0.2 + 0.8 * np.random.rand())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            # Evaluate primary trial if we have budget\n            if remaining > 0:\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(np.mean(ub - lb), sigma * 1.12)\n                    # continue to next iteration to exploit further from improved parent\n                    continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(np.mean(ub - lb), sigma * 1.08)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(np.mean(ub - lb), sigma * 1.1)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and np.random.rand() < 0.18:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale the heavy tail by current sigma and random multiplier\n                mult = np.random.lognormal(mean=0.0, sigma=0.8)\n                jump_scale = sigma * (1.0 + 2.0 * mult)\n                x_jump = np.clip(x_parent + jump_scale * step, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_jump, x_jump = callf(x_jump)\n                    except RuntimeError:\n                        break\n                    # if it's good replace the worst in population\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                        # slight boost to diversity\n                    # if jump significantly improves parent, accept it\n                    if f_jump < pop_f[parent_i]:\n                        pop[parent_i] = x_jump\n                        pop_f[parent_i] = f_jump\n                        pop_sigma[parent_i] = max(1e-12, sigma * 1.2)\n                        continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_scale * 0.5\n\n            # adapt parent sigma on failure (mild shrink)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_scale * (0.5 + np.random.rand() * 0.5)\n\n            # If population has shrunk (shouldn't normally), re-fill a bit\n            if len(pop) < self.pop_size and remaining > 0:\n                add_n = min(self.pop_size - len(pop), remaining)\n                for _ in range(add_n):\n                    x_new = np.random.uniform(lb, ub)\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        break\n                    pop.append(x_new)\n                    pop_f.append(f_new)\n                    pop_sigma.append(base_scale * (0.5 + np.random.rand()))\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.499 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1277502306809537, 0.15765292200302428, 0.9142313472967635, 0.9720962775461633, 0.22908002006484385, 0.9580190123053567, 0.25731504976258435, 0.44589156001091834, 0.7803699520967008, 0.14469193751706]}, "task_prompt": ""}
{"id": "8c770961-673d-41d4-87a5-64995caeadbb", "fitness": 0.5805356215866597, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search using randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy) jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n    def __call__(self, func):\n        # Extract bounds and normalize to arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # Ensure bounds shape\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        # State\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # Helper to evaluate a candidate with budget check\n        def callf(x):\n            nonlocal evals, f_opt, x_opt\n            if evals >= self.budget:\n                return None, None\n            x = np.asarray(x, dtype=float).ravel()[:self.dim].copy()\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            evals += 1\n            if f < f_opt:\n                f_opt = float(f)\n                x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget extremely small, fallback to random sampling\n        if self.budget <= 2:\n            while evals < self.budget:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(f_opt), (None if x_opt is None else x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        remaining = lambda: self.budget - evals\n\n        # base scale is a fraction of search range\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n\n        n_init = min(self.pop_size, max(1, self.budget // 10))  # reserve budget for improvements\n        n_init = min(n_init, self.pop_size, self.budget)\n        for _ in range(n_init):\n            if evals >= self.budget:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            if x0 is None:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n        # If we couldn't create any population (extremely low budget), do random search\n        if len(pop) == 0:\n            while evals < self.budget:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(f_opt), (None if x_opt is None else x_opt.copy())\n\n        # If population smaller than desired, fill a bit with randoms until budget or size reached\n        while len(pop) < self.pop_size and evals < self.budget:\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            if x0 is None:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n        # Main loop\n        while evals < self.budget:\n            # Tournament selection of parent (small tournament)\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sampled search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-16\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if evals >= self.budget:\n                break\n            f_try, x_try = callf(x_try)\n            if x_try is None:\n                break\n\n            success = False\n            if f_try < pop_f[parent_i]:\n                # accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.25, np.mean(ub - lb))\n                success = True\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    if evals >= self.budget:\n                        break\n                    x_tmp = np.clip(x_parent + alpha * frac * d, lb, ub)\n                    f_tmp, x_tmp = callf(x_tmp)\n                    if x_tmp is None:\n                        break\n                    if f_tmp < pop_f[parent_i]:\n                        pop[parent_i] = x_tmp\n                        pop_f[parent_i] = f_tmp\n                        pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                        success = True\n                        break\n\n            if success:\n                # continue to next iteration\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            # build r orthogonal to d via random vector and Gram-Schmidt\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr < 1e-12:\n                r = np.random.randn(self.dim)\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r) + 1e-16\n            r = r / nr\n            x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n            if evals < self.budget:\n                f_try, x_try = callf(x_try)\n            else:\n                f_try = None\n            if x_try is not None and f_try is not None and f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                continue  # found something, go to next iteration\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and evals < self.budget:\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(self.dim)\n                # robust scaling by bounding extreme Cauchy values\n                step = np.clip(step, -50, 50)\n                # scale relative to domain\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                if x_try is not None and f_try is not None:\n                    # replace worst if better, else maybe keep as candidate by replacing parent if slightly better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n                    elif f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                # continue main loop after jump\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and evals < self.budget:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = pop[a] + beta * (pop[b] - pop[a])\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if x_try is None:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n            # adapt parent sigma on failure\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if evals < self.budget and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if x_new is None:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n            # if population stagnates too long, slightly nudge sigmas\n            if np.random.rand() < 0.01:\n                j = np.random.randint(len(pop))\n                pop_sigma[j] = min(np.mean(ub - lb), pop_sigma[j] * (1.0 + 0.2 * (np.random.rand() - 0.5)))\n\n        return float(f_opt), (None if x_opt is None else x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.581 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16639442798341708, 0.16938517404919873, 0.8963380212969205, 0.9689869393994502, 0.8251645372211205, 0.9503701554110121, 0.24966008991846955, 0.5213930183910852, 0.9105463134848154, 0.14711753871110833]}, "task_prompt": ""}
{"id": "4716575d-a37d-4706-8374-960a1a3db182", "fitness": 0.5195034998618397, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points using randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy) jumps to balance robust exploration and fast local exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None a heuristic size is used)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # heuristic: scale with dimension but keep modest\n            self.pop_size = int(max(4, min(20, 2 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n\n    def __call__(self, func):\n        if self.seed is not None:\n            np.random.seed(self.seed)\n\n        # get bounds if provided, else use standard [-5,5]\n        lb = getattr(func.bounds, \"lb\", -5.0)\n        ub = getattr(func.bounds, \"ub\", 5.0)\n        lb = np.asarray(lb, dtype=float)\n        ub = np.asarray(ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # internal evaluator that enforces clipping, budget and best tracking\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise StopIteration(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If no budget, return immediately\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # base sigma scale (global reference)\n        base_sigma = max(1e-12, 0.2 * np.mean(ub - lb))\n\n        # initialize population (as many as budget allows up to pop_size)\n        n_init = min(self.pop_size, remaining)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except StopIteration:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.5 + 0.8 * np.random.rand()))\n        if len(pop) == 0:\n            # budget might have been 0 initially\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # Main loop: operate until budget exhausted\n        while remaining > 0:\n            # ensure we have at least one individual\n            n_pop = len(pop)\n            if n_pop == 0:\n                break\n\n            # parent selection: small tournament\n            k = min(3, n_pop)\n            inds = np.random.choice(n_pop, k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # generate a random normalized search direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # degenerate direction, skip iteration\n                pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                continue\n            d = d / nd\n\n            # primary directional trial (stochastic step-length)\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except StopIteration:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept, enlarge sigma slightly\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except StopIteration:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - r.dot(d) * d  # remove component along d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except StopIteration:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.12, np.mean(ub - lb))\n                    continue\n\n            # occasional Lévy-like (Cauchy) jump to escape basins\n            if (np.random.rand() < 0.08) and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale = 2.5 * sigma\n                x_try = np.clip(x_parent + scale * step, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except StopIteration:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 1.2)\n                else:\n                    # try to inject into population by replacing worst if better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                # after jump attempt continue main loop\n                continue\n\n            # recombination exploitation: mix two best with small noise\n            if (np.random.rand() < 0.12) and n_pop >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                beta = np.random.rand()\n                x_try = beta * pop[best2[0]] + (1.0 - beta) * pop[best2[1]]\n                x_try = x_try + 0.02 * sigma * np.random.randn(self.dim)\n                x_try = np.clip(x_try, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except StopIteration:\n                    break\n                # replace worst if recombined is better\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * 0.5\n                continue\n\n            # adapt parent sigma on failure (shrink)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing worst with random sample\n            if (np.random.rand() < 0.03) and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except StopIteration:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished or budget exhausted\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.520 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1199245740013909, 0.16005723700993257, 0.8234540355120437, 0.9480930115287338, 0.3685734618839145, 0.8892960282826011, 0.246188306990213, 0.6161874759966699, 0.8579810261773771, 0.16527984123552086]}, "task_prompt": ""}
{"id": "27c0036d-282a-40b3-9ace-7fd30a3be5d2", "fitness": 0.5216312544211787, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that combine randomized normalized directional local searches, orthogonal refinements, occasional Lévy-like jumps and light recombination to robustly explore/exploit bounded continuous problems.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional override of internal population size heuristic\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # Get bounds and ensure arrays of correct shape\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # helper to call func while tracking budget and best-so-far\n        remaining = int(self.budget)\n        f_opt = np.inf\n        x_opt = None\n\n        def callf(x):\n            nonlocal remaining, f_opt, x_opt\n            if remaining <= 0:\n                raise StopIteration(\"Budget exhausted\")\n            # ensure correct shape and clip to bounds\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = func(x)\n            if f < f_opt:\n                f_opt = float(f)\n                x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget extremely small: fallback to random search\n        if remaining <= 0:\n            self.f_opt = f_opt\n            self.x_opt = x_opt\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (use up to remaining evaluations)\n        n_init = min(self.pop_size, remaining)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except StopIteration:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize individual sigma with some small randomness\n            pop_sigma.append(max(1e-12, base_sigma * (0.5 + np.random.rand())))\n        # If no population could be created, do pure random search until budget exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                try:\n                    f, x = callf(x)\n                except StopIteration:\n                    break\n            self.f_opt = f_opt\n            self.x_opt = x_opt\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # convert lists to arrays for speed/indices\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # main loop: keep using remaining budget for directional local searches,\n        # orthogonal tries, occasional Lévy jumps, recombination and rejuvenation\n        while remaining > 0:\n            n_pop = len(pop)\n            # pick a parent via a small tournament to balance exploration/exploitation\n            k = min(3, n_pop)\n            inds = np.random.choice(n_pop, k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length (heavy-tailed influence)\n            # use single-sample Cauchy scalar to vary step-length robustly but clip extremes\n            c = np.random.standard_cauchy()\n            c = np.clip(c, -10, 10)\n            step_len = sigma * (1.0 + 0.5 * c)\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except StopIteration:\n                break\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max(sigma * 1.15, 1e-12), max(1e3, base_sigma * 1e3))\n                improved = True\n                continue  # go to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            back_step = step_len\n            for bt in range(3):\n                back_step *= 0.5\n                x_try = np.clip(x_parent + back_step * d, lb, ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, x_try = callf(x_try)\n                except StopIteration:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r -= np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr < 1e-12:\n                r = np.random.randn(self.dim)\n                nr = np.linalg.norm(r) + 1e-12\n            r = r / nr\n            x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except StopIteration:\n                break\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            # with small probability or if parent seems stagnant\n            p_jump = 0.05\n            # increase jump chance if parent's sigma very small relative to base\n            if np.random.rand() < p_jump:\n                # generate cauchy vector, normalize by robust scale (mean absolute)\n                step = np.random.standard_cauchy(self.dim)\n                # avoid extremely large values: robust normalization\n                scale = np.mean(np.abs(step)) + 1e-12\n                step = step / scale\n                # scale jump relative to problem scale and sigma\n                jump_scale = max(base_sigma * 0.5, sigma * 2.0)\n                step_vec = np.clip(step * jump_scale, -5*(ub-lb), 5*(ub-lb))\n                x_try = np.clip(x_parent + step_vec, lb, ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, x_try = callf(x_try)\n                except StopIteration:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                # if jump produces improvement relative to worst, inject it\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if n_pop >= 2 and np.random.rand() < 0.2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.uniform(0.2, 0.8)\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, x_try = callf(x_try)\n                except StopIteration:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, base_sigma * 0.7)\n                # small chance to also replace parent if it's better\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                continue\n\n            # adapt parent sigma on failure (no improvement in this cycle)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.02 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except StopIteration:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = max(1e-12, base_sigma * (0.5 + np.random.rand()))\n\n        # finished budget or exhausted loop\n        self.f_opt = f_opt\n        self.x_opt = x_opt\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.522 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.107323467401511, 0.16697833183190436, 0.5434439086069875, 0.9630810659735494, 0.8888563492840885, 0.9590977421871762, 0.23872824478352983, 0.37767629892224397, 0.8332832032512648, 0.1378439319695317]}, "task_prompt": ""}
{"id": "9da5ff48-2498-4660-8ffb-cd2dd2062171", "fitness": 0.4427753420194841, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based heuristic combining randomized directional local searches, orthogonal refinements and occasional heavy-tailed (Lévy/Cauchy-like) jumps with adaptive per-individual step sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults adaptively to problem dimension)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population default: moderate, scales with dim but bounded by budget\n        if pop_size is None:\n            # keep population modest but growing with dimension\n            self.pop_size = max(6, min(int(4 * np.sqrt(self.dim)), int(max(6, self.budget // 10))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        # try to extract bounds, otherwise default to [-5, 5]\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # if scalar bounds given, expand to full dimension\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # safety: ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = int(self.budget)\n        # global best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            # wrapper to call func, clip, count budget, and update best\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No remaining budget for evaluations.\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.clip(x, lb, ub)\n            f = func(x)\n            # Some black-boxes might return arrays; reduce to scalar\n            try:\n                f = float(f)\n            except Exception:\n                # fallback if function returns array-like\n                f = float(np.asarray(f).ravel()[0])\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # quick fallback if budget exhausted\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initial population sampling\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-12, 0.25 * float(np.mean(ub - lb)))  # initial scale from search range\n\n        for i in range(n_init):\n            x = lb + self.rng.rand(self.dim) * (ub - lb)\n            f, x = callf(x)\n            pop.append(x)\n            pop_f.append(f)\n            # initial adaptive sigma: spread around base_sigma\n            pop_sigma.append(base_sigma * (0.5 + self.rng.rand()))\n\n        # If no population created (very tiny budget), fall back to uniform random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                f, x = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Main loop\n        while remaining > 0:\n            # tournament selection (small tournament to balance exploration/exploitation)\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            # choose best among tournament by fitness (lower is better)\n            tournament_fs = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(tournament_fs))]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            success = False  # track if we improved this iteration\n\n            # sample a random search direction and normalize\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                # fallback to coordinate direction\n                d = np.zeros(self.dim)\n                d[self.rng.randint(0, self.dim)] = 1.0\n            else:\n                d /= nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                success = True\n                # accept and move on\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    success = True\n                    break\n            if success:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            proj = np.dot(r, d) * d\n            ort = r - proj\n            nr = np.linalg.norm(ort)\n            if nr > 1e-12:\n                ort /= nr\n                step = sigma * (0.6 * (0.8 + 0.4 * self.rng.rand()))\n                x_try = np.clip(x_parent + step * ort, lb, ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.05, np.mean(ub - lb))\n                    success = True\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.15 and remaining > 0:\n                # generate Cauchy-like heavy-tailed vector\n                u = self.rng.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                # robust normalization to keep direction heavy-tailed but avoid overflowing steps\n                med = np.median(np.abs(cauchy)) + 1e-12\n                scale_vec = cauchy / med\n                step = sigma * (1.5 + 1.0 * self.rng.rand())\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(sigma * 0.9, 1e-12)\n                    success = True\n                    # continue to next main iteration\n                    continue\n                else:\n                    # occasionally keep as exploration candidate replacing worst if not too bad\n                    if f_try < pop_f[worst_i] * 1.02:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = sigma * 0.8\n\n            # recombination exploitation: mix two best solutions + small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                mix = 0.6 * pop[a] + 0.4 * pop[b]\n                x_try = mix + sigma * 0.05 * self.rng.randn(self.dim)\n                x_try = np.clip(x_try, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 0.95, np.mean(ub - lb))\n                    success = True\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (shrink if not successful)\n            if not success:\n                pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n            else:\n                # ensure some growth upper bound to keep search range\n                pop_sigma[parent_i] = min(pop_sigma[parent_i], np.mean(ub - lb))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = lb + self.rng.rand(self.dim) * (ub - lb)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n            # loop continues until budget exhausted\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.443 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1398659446583146, 0.15707941011196047, 0.8909976882338958, 0.9672363902281558, 0.28234235083357373, 0.78879814889756, 0.29055100066530004, 0.4580854835901088, 0.29700589077142014, 0.15579111220455144]}, "task_prompt": ""}
{"id": "46101668-d93e-49ed-954c-d118a96f3758", "fitness": 0.314369532226892, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points using randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy) jumps to explore and exploit continuous search spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = int(pop_size) if pop_size is not None else min(20, max(4, 2 * self.dim))\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds as full-length numpy arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        # ensure shapes\n        lb = lb.ravel()[:self.dim]\n        ub = ub.ravel()[:self.dim]\n\n        # budget remaining\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper: evaluate while tracking budget and best found\n        def callf(x):\n            nonlocal remaining\n            # Clip before evaluating to respect bounds\n            x = np.clip(np.asarray(x, dtype=float).ravel()[:self.dim], lb, ub)\n            if remaining <= 0:\n                # no budget: return +inf so caller treats it as failure\n                return np.inf, x\n            remaining -= 1\n            f = float(func(x))\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget very small, fallback to simple random sampling\n        if self.budget <= 2:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population (try to create up to pop_size individuals but do not exceed budget)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        max_init = min(self.pop_size, max(1, remaining))\n        for _ in range(max_init):\n            x = np.random.uniform(lb, ub)\n            f, x = callf(x)\n            pop.append(x.copy())\n            pop_f.append(f)\n            # initial sigma proportional to domain with small randomization\n            pop_sigma.append(0.2 * np.mean(ub - lb) * (0.5 + np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If couldn't initialize any population member (extremely small budget) do random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # Keep an iteration-level stagnation counter to decide on rejuvenation / levy jumps\n        stagnation = 0\n        best_seen = self.f_opt\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # small tournament selection to pick parent index\n            k = min(3, len(pop))\n            choices = np.random.choice(len(pop), size=k, replace=False)\n            parent_i = choices[np.argmin(pop_f[choices])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # degenerate, skip this iteration to avoid zero direction\n                # but perform a small random perturbation instead\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length (heavy-tailed multiplicative tweak)\n            # use lognormal-ish multiplicative noise to vary step-size\n            step_len = sigma * (1.0 + 0.3 * np.random.randn()) * (1.0 + 0.2 * np.random.rand())\n            x_try = x_parent + step_len * d\n            x_try = np.clip(x_try, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                stagnation = 0\n            else:\n                # local backtracking / small-step refinement along direction (a few tries with smaller steps)\n                improved = False\n                small_sigma = max(sigma * 0.5, 1e-12)\n                for back in range(3):\n                    step2 = small_sigma * (0.5 ** back)\n                    x_b = x_parent + step2 * d\n                    x_b = np.clip(x_b, lb, ub)\n                    f_b, x_b = callf(x_b)\n                    if f_b < pop_f[parent_i]:\n                        pop[parent_i] = x_b\n                        pop_f[parent_i] = f_b\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        improved = True\n                        stagnation = 0\n                        break\n                if not improved:\n                    # if no improvement, slightly decrease sigma (cautious)\n                    pop_sigma[parent_i] = max(sigma * 0.92, 1e-12)\n                    stagnation += 1\n\n            # try an orthogonal perturbation for local diversification\n            if remaining <= 0:\n                break\n            r = np.random.randn(self.dim)\n            # subtract projection on d to get orthogonal component\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                ortho_step = 0.6 * pop_sigma[parent_i]\n                x_o = pop[parent_i] + ortho_step * r\n                x_o = np.clip(x_o, lb, ub)\n                f_o, x_o = callf(x_o)\n                if f_o < pop_f[parent_i]:\n                    pop[parent_i] = x_o\n                    pop_f[parent_i] = f_o\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n                    stagnation = 0\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            # triggered with small probability or when stagnation high\n            levy_prob = 0.03 + 0.02 * (stagnation > 10)\n            if remaining > 0 and np.random.rand() < levy_prob:\n                # generate Cauchy (heavy-tailed) vector and scale robustly\n                step = np.tan(np.pi * (np.random.rand(self.dim) - 0.5))  # Cauchy-like\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                levy_scale = max(0.5 * np.mean(ub - lb), pop_sigma.mean())\n                x_levy = pop[parent_i] + levy_scale * step\n                x_levy = np.clip(x_levy, lb, ub)\n                f_levy, x_levy = callf(x_levy)\n                if f_levy < np.max(pop_f):\n                    # replace the worst individual with this jump candidate\n                    worst_i = int(np.argmax(pop_f))\n                    pop[worst_i] = x_levy\n                    pop_f[worst_i] = f_levy\n                    pop_sigma[worst_i] = levy_scale * 0.5\n                    stagnation = 0\n                else:\n                    # keep candidate only if it's better than the chosen parent\n                    if f_levy < pop_f[parent_i]:\n                        pop[parent_i] = x_levy\n                        pop_f[parent_i] = f_levy\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.8, 1e-12)\n                        stagnation = 0\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0 and len(pop) >= 2:\n                best_idxs = np.argsort(pop_f)[:2]\n                a, b = pop[best_idxs[0]].copy(), pop[best_idxs[1]].copy()\n                mix = 0.5 * (a + b)\n                noise = (0.02 * (ub - lb)) * np.random.randn(self.dim)\n                x_mix = np.clip(mix + noise, lb, ub)\n                f_mix, x_mix = callf(x_mix)\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                    stagnation = 0\n                else:\n                    # try injecting into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = np.mean(pop_sigma) * 0.5\n                        stagnation = 0\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (stagnation > 25 or np.random.rand() < 0.01):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = 0.2 * np.mean(ub - lb)\n                stagnation = 0\n\n            # keep best_seen tracking to detect long stagnation across whole population\n            if self.f_opt < best_seen - 1e-12:\n                best_seen = self.f_opt\n            # soft shrink population if budget dwindles\n            if remaining < 3 and len(pop) > 1:\n                # try to use remaining evaluations on best individuals only\n                # remove worst ones (they won't be evaluated further)\n                keep = max(1, remaining)\n                keep_idxs = np.argsort(pop_f)[:keep]\n                pop = pop[keep_idxs]\n                pop_f = pop_f[keep_idxs]\n                pop_sigma = pop_sigma[keep_idxs]\n\n        # Finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.314 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12534360016449586, 0.17567968092273656, 0.5070213817931378, 0.5889334403750172, 0.23986988935341402, 0.4677630013448646, 0.2377228439554192, 0.3111562925417135, 0.33926376546392256, 0.15094142635419883]}, "task_prompt": ""}
{"id": "266216b8-2fbd-4203-b2f6-6de5e7a38133", "fitness": 0.5210359144737944, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — adaptive population of step-sizes that mixes directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by default)\n    - seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        self.seed = seed\n\n        # best seen\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # ensure shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # small helper to enforce bounds & evaluate with budget accounting\n        remaining = self.budget\n\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return np.inf\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # If budget too small => pure random search fallback\n        if self.budget <= 8:\n            # simple random search using full budget\n            for _ in range(self.budget):\n                x = rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Determine population size adaptively\n        proposed = 6 + 2 * self.dim\n        if self.pop_size is None:\n            # scale with dim but never huge relative to budget\n            pop_size = int(min(proposed, max(4, self.budget // 20)))\n        else:\n            pop_size = int(max(1, self.pop_size))\n            pop_size = min(pop_size, max(1, self.budget))\n        pop_size = max(1, pop_size)\n\n        # If not enough budget to init population, fallback to random search\n        if remaining < pop_size:\n            for _ in range(remaining):\n                x = rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population uniformly\n        pop_x = np.zeros((pop_size, self.dim), dtype=float)\n        pop_f = np.zeros(pop_size, dtype=float)\n        pop_sigma = np.zeros(pop_size, dtype=float)  # adaptive step sizes\n        # initial sigma scale relative to search range\n        base_sigma = 0.2 * np.maximum(ub - lb, 1e-6)\n        for i in range(pop_size):\n            xi = rng.uniform(lb, ub)\n            fi = callf(xi)\n            pop_x[i] = xi\n            pop_f[i] = fi\n            # initialize per-individual scalar sigma as fraction of diagonal length\n            pop_sigma[i] = max(1e-6, 0.5 * np.linalg.norm(base_sigma) * (0.5 + rng.rand()))\n\n        # if budget exhausted during init\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # convenience functions\n        def best_indices_sorted():\n            return np.argsort(pop_f)\n\n        def replace_index_with(i, x_new, f_new, sigma_new=None):\n            pop_x[i] = x_new\n            pop_f[i] = f_new\n            if sigma_new is not None:\n                pop_sigma[i] = sigma_new\n\n        # main optimization loop\n        iter_no_improve = 0\n        iters = 0\n        while remaining > 0:\n            iters += 1\n\n            # pick parent by small tournament\n            k = min(3, pop_size)\n            tour = rng.choice(pop_size, k, replace=False)\n            parent_i = tour[np.argmin(pop_f[tour])]\n            x_parent = pop_x[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma_parent = pop_sigma[parent_i]\n\n            improved = False\n\n            # random search direction\n            d = rng.normal(size=self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                # rare degenerate -> random direction\n                d = rng.normal(size=self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized length (log-normal jitter)\n            jitter = np.exp(0.05 * rng.randn())\n            step_len = sigma_parent * jitter\n            x_try = np.clip(x_parent + d * step_len, lb, ub)\n            f_try = callf(x_try)\n            if f_try < f_parent:\n                # accept\n                replace_index_with(parent_i, x_try, f_try, sigma_parent * 1.12)\n                improved = True\n            else:\n                # local backtracking: try a few smaller steps along same direction\n                back_sigma = sigma_parent\n                for bt in range(3):\n                    back_sigma *= 0.5\n                    x_bt = np.clip(x_parent + d * back_sigma, lb, ub)\n                    f_bt = callf(x_bt)\n                    if f_bt < f_parent:\n                        replace_index_with(parent_i, x_bt, f_bt, sigma_parent * (1.0 + 0.08))\n                        improved = True\n                        break\n                    # if budget exhausted during backtracking break\n                    if remaining <= 0:\n                        break\n\n            # orthogonal perturbation for local diversification\n            if remaining > 0:\n                # produce a vector orthonormal to d\n                v = rng.normal(size=self.dim)\n                v = v - np.dot(v, d) * d\n                nv = np.linalg.norm(v)\n                if nv > 1e-12:\n                    v = v / nv\n                    ortho_step = 0.6 * sigma_parent\n                    x_o = np.clip((pop_x[parent_i] if not improved else x_try) + v * ortho_step, lb, ub)\n                    f_o = callf(x_o)\n                    if f_o < pop_f[parent_i]:\n                        replace_index_with(parent_i, x_o, f_o, sigma_parent * 1.08)\n                        improved = True\n\n            # occasional Lévy-like heavy-tailed jump to escape basins\n            if remaining > 0 and rng.rand() < 0.06:\n                # Cauchy vector (heavy tail)\n                step = rng.standard_cauchy(self.dim)\n                # robust scale: median absolute of step\n                mad = np.median(np.abs(step)) + 1e-9\n                # scale to a moderate fraction of the search diagonal times individual's sigma\n                diag = np.linalg.norm(ub - lb)\n                scale_vec = (0.6 * diag) * (sigma_parent / (1.0 + np.linalg.norm(sigma_parent)))\n                x_levy = np.clip(x_parent + (step / mad) * scale_vec, lb, ub)\n                f_levy = callf(x_levy)\n                # If jump is good, inject by replacing worst or parent\n                worst_i = np.argmax(pop_f)\n                if f_levy < pop_f[worst_i]:\n                    replace_index_with(worst_i, x_levy, f_levy, max(1e-8, sigma_parent * 0.7))\n                elif f_levy < pop_f[parent_i]:\n                    replace_index_with(parent_i, x_levy, f_levy, max(1e-8, sigma_parent * 0.9))\n                # continue main loop after heavy jump (no further local moves this iteration)\n                # but we still allow recombination below\n\n            # recombination exploitation: mix two best and add small noise\n            if remaining > 0 and pop_size >= 2:\n                bidx = best_indices_sorted()\n                a, b = bidx[0], bidx[1]\n                weight = 0.6 + 0.1 * rng.randn()\n                weight = np.clip(weight, 0.2, 0.8)\n                child = weight * pop_x[a] + (1.0 - weight) * pop_x[b]\n                # small gaussian perturb scaled by average sigma\n                avg_sigma = max(1e-8, 0.5 * (pop_sigma[a] + pop_sigma[b]))\n                child += rng.normal(scale=0.4 * avg_sigma, size=self.dim)\n                child = np.clip(child, lb, ub)\n                f_child = callf(child)\n                worst_i = np.argmax(pop_f)\n                if f_child < pop_f[worst_i]:\n                    replace_index_with(worst_i, child, f_child, avg_sigma * (0.7 + 0.6 * rng.rand()))\n                    # if it's really good replace parent sometimes\n                    if f_child < pop_f[parent_i] and rng.rand() < 0.5:\n                        replace_index_with(parent_i, child, f_child, avg_sigma * 1.0)\n\n            # adapt parent sigma on failure/success\n            if improved:\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.18, 10.0 * np.linalg.norm(ub - lb))\n                iter_no_improve = 0\n            else:\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.86, 1e-8)\n                iter_no_improve += 1\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and rng.rand() < 0.02:\n                worst_i = np.argmax(pop_f)\n                xr = rng.uniform(lb, ub)\n                fr = callf(xr)\n                replace_index_with(worst_i, xr, fr, max(1e-8, 0.5 * np.linalg.norm(base_sigma) * (0.5 + rng.rand())))\n\n            # when stagnation detected, perform a focused restart-like injection\n            if remaining > 0 and iter_no_improve > max(20, 5 * self.dim) and rng.rand() < 0.4:\n                # inject a few new samples near global best if available, else random\n                worst_i = np.argmax(pop_f)\n                if self.x_opt is not None:\n                    perturb = rng.normal(scale=0.5 * np.linalg.norm(base_sigma), size=self.dim)\n                    xr = np.clip(self.x_opt + perturb, lb, ub)\n                else:\n                    xr = rng.uniform(lb, ub)\n                fr = callf(xr)\n                replace_index_with(worst_i, xr, fr, max(1e-8, pop_sigma[worst_i] * 0.8))\n                iter_no_improve = 0\n\n            # bound step sizes to avoid divergence or too small\n            pop_sigma = np.clip(pop_sigma, 1e-9, 10.0 * np.linalg.norm(ub - lb))\n\n            # safety break if loop too long (shouldn't be necessary but guards accidental infinite loops)\n            if iters > 1000000:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.521 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1793251835068822, 0.18153094258923874, 0.7497731952472564, 0.9182853851823406, 0.3371562078887359, 0.844666202952494, 0.2800049398505109, 0.7365150190687404, 0.8060631382320005, 0.1770389302197436]}, "task_prompt": ""}
{"id": "2cb148aa-9672-479d-8804-c31bfc27575d", "fitness": 0.2854211221063886, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that use randomized directional local searches, orthogonal refinements and occasional Lévy jumps to explore and exploit the continuous search space.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest value depending on dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # keep population modest but scale with dimension\n            self.pop_size = int(max(6, min(40, 4 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rs = np.random.RandomState(seed)\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Determine bounds (support typical BBOB interface with func.bounds.lb/ub)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # default search range for Many BBOB problems\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # Ensure bounds are full-dimensional arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        assert np.all(span > 0), \"Upper bounds must be greater than lower bounds\"\n\n        # Internal counters and helper\n        evals = 0\n        budget = self.budget\n        eps = 1e-12\n\n        def clip_x(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def callf(x):\n            nonlocal evals, budget\n            if evals >= budget:\n                raise StopIteration(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = clip_x(x)\n            f = func(x)\n            evals += 1\n            # update global best\n            nonlocal_best_update(f, x)\n            return f, x.copy()\n\n        def nonlocal_best_update(f, x):\n            nonlocal evals\n            # update class-wide best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # If budget extremely small, fallback to pure random search\n        if budget <= 5 or self.pop_size <= 1:\n            # simple random search until budget exhausted\n            self.f_opt = np.inf\n            self.x_opt = None\n            while evals < budget:\n                x = lb + self.rs.rand(self.dim) * span\n                f = func(x)\n                evals += 1\n                if f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n            return float(self.f_opt), self.x_opt\n\n        # Initialize population\n        population = []\n        initial_sigma = 0.25 * np.max(span)  # initial step-size (absolute)\n        for i in range(self.pop_size):\n            if evals >= budget:\n                break\n            x = lb + self.rs.rand(self.dim) * span\n            f = func(x); evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f); self.x_opt = x.copy()\n            individual = {\n                'x': x.copy(),\n                'f': float(f),\n                'sigma': initial_sigma * (0.8 + 0.4 * self.rs.rand()),  # slight diversity\n                'age': 0\n            }\n            population.append(individual)\n\n        # If no population members (very small budget), do pure random search\n        if len(population) == 0:\n            while evals < budget:\n                x = lb + self.rs.rand(self.dim) * span\n                f = func(x); evals += 1\n                if f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n            return float(self.f_opt), self.x_opt\n\n        # Utility functions on population\n        def get_best_index():\n            return int(np.argmin([ind['f'] for ind in population]))\n\n        def get_worst_index():\n            return int(np.argmax([ind['f'] for ind in population]))\n\n        def tournament_select(k=3):\n            idxs = self.rs.randint(0, len(population), size=min(k, len(population)))\n            best = idxs[0]\n            for ii in idxs:\n                if population[ii]['f'] < population[best]['f']:\n                    best = ii\n            return best\n\n        # Main loop\n        stagnation_counter = 0\n        while evals < budget:\n            # pick a parent via small tournament\n            pidx = tournament_select(k=3)\n            parent = population[pidx]\n            x_parent = parent['x']\n            sigma = parent['sigma']\n\n            # sample a random search direction (normalized)\n            d = self.rs.randn(self.dim)\n            dnrm = np.linalg.norm(d) + eps\n            d = d / dnrm\n\n            # primary directional trial with stochasticized step-length\n            # lognormal multiplicative noise provides variability in step length\n            step_mult = float(np.exp(self.rs.normal(loc=0.0, scale=0.5)))\n            step_len = sigma * step_mult\n            x_trial = clip_x(x_parent + d * step_len)\n\n            # Evaluate\n            if evals >= budget: break\n            f_trial, x_trial = callf(x_trial)\n\n            if f_trial < parent['f']:\n                # success: accept, increase sigma slightly\n                population[pidx] = {\n                    'x': x_trial.copy(),\n                    'f': float(f_trial),\n                    'sigma': min(np.max(span), sigma * (1.08 + 0.02 * self.rs.rand())),\n                    'age': 0\n                }\n                stagnation_counter = 0\n                # small local refinement/backtracking along direction (very local)\n                for back in range(2):\n                    if evals >= budget: break\n                    small_step = (0.2 ** (back + 1)) * sigma\n                    x_ref = clip_x(x_trial + d * (-small_step))\n                    f_ref, x_ref = callf(x_ref)\n                    if f_ref < population[pidx]['f']:\n                        population[pidx]['x'] = x_ref.copy()\n                        population[pidx]['f'] = float(f_ref)\n                        population[pidx]['sigma'] *= 1.02\n            else:\n                # failure: try a few small steps along same direction (local search)\n                improved = False\n                for local in range(3):\n                    if evals >= budget: break\n                    small = sigma * (0.2 + 0.3 * self.rs.rand())\n                    x_local = clip_x(x_parent + d * (-small))\n                    f_local, x_local = callf(x_local)\n                    if f_local < parent['f']:\n                        population[pidx] = {\n                            'x': x_local.copy(),\n                            'f': float(f_local),\n                            'sigma': max(1e-8, sigma * 0.9),\n                            'age': 0\n                        }\n                        improved = True\n                        break\n                if not improved:\n                    # orthogonal perturbation for local diversification\n                    r = self.rs.randn(self.dim)\n                    # make orthogonal to d: r - (r.d)d\n                    r = r - np.dot(r, d) * d\n                    rnrm = np.linalg.norm(r) + eps\n                    r = r / rnrm\n                    ortho_step = sigma * (0.2 + 0.6 * self.rs.rand())\n                    x_ortho = clip_x(x_parent + r * ortho_step)\n                    if evals < budget:\n                        f_ortho, x_ortho = callf(x_ortho)\n                        if f_ortho < parent['f']:\n                            population[pidx] = {\n                                'x': x_ortho.copy(),\n                                'f': float(f_ortho),\n                                'sigma': max(1e-8, sigma * 0.95),\n                                'age': 0\n                            }\n                            improved = True\n\n                # adapt parent sigma on failure\n                if not improved:\n                    population[pidx]['sigma'] = max(1e-8, sigma * 0.92)\n                    population[pidx]['age'] += 1\n                    stagnation_counter += 1\n                else:\n                    stagnation_counter = 0\n\n            # occasional Lévy-like jump (heavy-tailed) to escape basins\n            # frequency increases with stagnation\n            levy_prob = 0.015 + min(0.2, 0.001 * stagnation_counter)\n            if self.rs.rand() < levy_prob and evals < budget:\n                # generate Cauchy-like vector and robustly scale it\n                c = self.rs.standard_cauchy(self.dim)\n                # avoid infinities/explosions: clip large values and normalize\n                c = np.clip(c, -1e6, 1e6)\n                c_scale = np.median(np.abs(c)) + eps\n                c = c / (c_scale + eps)\n                # scale relative to search span and current global situation\n                levy_scale = 0.5 * np.max(span) * (1.0 + self.rs.rand())\n                x_levy = clip_x(self.x_opt + c * levy_scale)\n                if evals < budget:\n                    f_levy, x_levy = callf(x_levy)\n                    # if it's good, replace the worst in population, else maybe keep as candidate\n                    widx = get_worst_index()\n                    if f_levy < population[widx]['f']:\n                        population[widx] = {\n                            'x': x_levy.copy(),\n                            'f': float(f_levy),\n                            'sigma': initial_sigma * (0.5 + self.rs.rand()),\n                            'age': 0\n                        }\n                        stagnation_counter = 0\n                    else:\n                        # occasionally keep it as a new candidate replacing the oldest\n                        if self.rs.rand() < 0.05:\n                            # find oldest\n                            ages = [ind['age'] for ind in population]\n                            oldest = int(np.argmax(ages))\n                            population[oldest] = {\n                                'x': x_levy.copy(),\n                                'f': float(f_levy),\n                                'sigma': initial_sigma * (0.5 + self.rs.rand()),\n                                'age': 0\n                            }\n\n            # recombination exploitation: mix two best and small noise\n            if evals < budget:\n                # pick two best\n                idxs_sorted = np.argsort([ind['f'] for ind in population])\n                b1 = int(idxs_sorted[0])\n                b2 = int(idxs_sorted[1]) if len(idxs_sorted) > 1 else b1\n                x1 = population[b1]['x']\n                x2 = population[b2]['x']\n                mix = 0.5 + 0.2 * (self.rs.rand() - 0.5)\n                noise = (self.rs.randn(self.dim) * 0.02 * np.max(span))\n                x_recomb = clip_x(x1 + mix * (x2 - x1) + noise)\n                if evals < budget:\n                    f_recomb, x_recomb = callf(x_recomb)\n                    # replace parent if improved, else possibly replace worst\n                    if f_recomb < population[pidx]['f']:\n                        population[pidx] = {\n                            'x': x_recomb.copy(),\n                            'f': float(f_recomb),\n                            'sigma': max(1e-8, population[pidx]['sigma'] * 1.02),\n                            'age': 0\n                        }\n                    else:\n                        widx = get_worst_index()\n                        if f_recomb < population[widx]['f']:\n                            population[widx] = {\n                                'x': x_recomb.copy(),\n                                'f': float(f_recomb),\n                                'sigma': initial_sigma * (0.6 + 0.4 * self.rs.rand()),\n                                'age': 0\n                            }\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rs.rand() < 0.01 and evals < budget:\n                widx = get_worst_index()\n                x_new = lb + self.rs.rand(self.dim) * span\n                f_new, x_new = callf(x_new)\n                if f_new < population[widx]['f']:\n                    population[widx] = {\n                        'x': x_new.copy(),\n                        'f': float(f_new),\n                        'sigma': initial_sigma * (0.6 + 0.4 * self.rs.rand()),\n                        'age': 0\n                    }\n\n            # occasionally shrink population members' sigma that are old\n            for ind in population:\n                if ind['age'] > 15:\n                    ind['sigma'] = max(1e-9, ind['sigma'] * 0.9)\n\n            # keep population sizes stable, and increment ages for those not replaced\n            for ind in population:\n                ind['age'] += 1\n\n            # If stagnating badly, force a restart-like small injection\n            if stagnation_counter > 80 and evals + 3 < budget:\n                # replace half the worst with randoms\n                n_replace = max(1, len(population) // 3)\n                for _ in range(n_replace):\n                    widx = get_worst_index()\n                    x_new = lb + self.rs.rand(self.dim) * span\n                    f_new, x_new = callf(x_new)\n                    population[widx] = {\n                        'x': x_new.copy(),\n                        'f': float(f_new),\n                        'sigma': initial_sigma * (0.5 + 0.5 * self.rs.rand()),\n                        'age': 0\n                    }\n                stagnation_counter = 0\n\n            # keep a guard to not exceed budget (loop condition handles it)\n\n        # finish: return best found\n        # if no best recorded (shouldn't happen), find best in population\n        if self.x_opt is None and len(population) > 0:\n            bi = get_best_index()\n            self.x_opt = population[bi]['x'].copy()\n            self.f_opt = float(population[bi]['f'])\n\n        return float(self.f_opt), self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.285 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13828574332377652, 0.14704043876197626, 0.3609837522574695, 0.4831839839958675, 0.3059122160822699, 0.38379760551081843, 0.2584898639028701, 0.31799066582210045, 0.30107453271123186, 0.15745241869550564]}, "task_prompt": ""}
{"id": "938cb2fd-6cc6-47a7-bc08-07d11ff719c0", "fitness": 0.47558058932174596, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points using randomized directional local searches, orthogonal refinements and occasional Lévy jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled if None)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            self.pop_size = int(max(4, min(30, int(4 + 2 * np.sqrt(self.dim)))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # get bounds; many benchmarks expose bounds as scalars or arrays\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback to default [-5,5] if bounds missing\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # clip in case bounds are inverted or so\n        lb, ub = np.minimum(lb, ub), np.maximum(lb, ub)\n\n        remaining = self.budget\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # quick fallback\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # population initialization\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        scale_range = ub - lb\n        base_sigma = max(1e-8, 0.25 * np.mean(scale_range))  # initial step scale (global)\n        for i in range(n_init):\n            x0 = lb + np.random.rand(self.dim) * (ub - lb)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = lb + np.random.rand(self.dim) * (ub - lb)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop\n        while remaining > 0:\n            # Small tournament selection\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            vals = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(vals))]\n            x_parent = pop[parent_i].copy()\n            sigma = max(1e-12, pop_sigma[parent_i])\n\n            # choose search direction: prefer difference of two random individuals, else gaussian\n            if len(pop) >= 2 and np.random.rand() < 0.7:\n                a, b = np.random.choice(len(pop), 2, replace=False)\n                d = pop[a] - pop[b]\n                if np.allclose(d, 0):\n                    d = np.random.randn(self.dim)\n            else:\n                d = np.random.randn(self.dim)\n\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            # keep alpha reasonable (allow negative to try reverse direction occasionally)\n            alpha = float(alpha)\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(scale_range))\n                continue\n\n            # local backtracking / small-step refinement along direction\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # subtract projection on d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(scale_range))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed step per coordinate\n                step = np.tan(np.pi * (np.random.rand(self.dim) - 0.5))\n                # robust normalization to avoid astronomically huge jumps\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # replace worst if better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (mild annealing)\n            pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * (0.98 + 0.04 * np.random.rand()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = lb + np.random.rand(self.dim) * (ub - lb)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # ensure population arrays remain consistent (if budget allowed more evaluations, we keep iterating)\n            # if any individual became None due to some unexpected condition, regenerate\n            for i in range(len(pop)):\n                if pop[i] is None or pop_f[i] is None:\n                    pop[i] = lb + np.random.rand(self.dim) * (ub - lb)\n                    try:\n                        f_new, x_new = callf(pop[i])\n                    except RuntimeError:\n                        break\n                    pop[i] = x_new\n                    pop_f[i] = f_new\n                    pop_sigma[i] = base_sigma * (0.8 + 0.4 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.476 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13583940050457433, 0.1610921018812309, 0.7255839930883269, 0.9618368098365675, 0.8702793982625561, 0.1642537865805117, 0.2842660957675234, 0.5249917601706195, 0.792695251832892, 0.13496729529265794]}, "task_prompt": ""}
{"id": "8302d2aa-d09b-411e-b3db-7622637938d5", "fitness": 0.48208447038964686, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining randomized directional local searches, orthogonal refinements, recombination, adaptive step-sizes and occasional Lévy-like jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None, set relative to dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that scales with dimension\n            self.pop_size = max(4, min(20, int(4 + np.sqrt(self.dim) * 2)))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds (support scalar or per-dim)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n            ub = np.full(self.dim, float(ub.item()))\n        else:\n            lb = lb.astype(float)\n            ub = ub.astype(float)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best solution\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds robustly\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = float(func(x))\n            # update best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is zero or negative, nothing to do\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n\n        # If budget left but no population (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return self.f_opt, self.x_opt\n\n        # Main loop\n        while remaining > 0:\n            # small tournament selection\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            # guard alpha\n            if not np.isfinite(alpha) or abs(alpha) < 1e-16:\n                alpha = sigma\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                step_scale = sigma * 0.8\n                x_try = np.clip(x_parent + step_scale * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n                    # if it's better than parent, also replace parent\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.05 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # occasional small expansion of population if budget allows (inject new candidate)\n            if remaining > 0 and len(pop) < self.pop_size and np.random.rand() < 0.02:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop.append(x_new.copy())\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.6 + 0.6 * np.random.rand()))\n\n        # finished budget or loop exit\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.482 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12993168831129853, 0.15711974185726063, 0.48925279556779966, 0.9612598391926197, 0.7778742471898609, 0.7435005261293517, 0.22837613023166525, 0.4393604385579172, 0.7433100386275456, 0.1508592582311491]}, "task_prompt": ""}
{"id": "8b055eb3-d578-4a52-894d-cb57e4925214", "fitness": 0.5756815823741087, "name": "ADLS", "description": "Population-based adaptive directional search combining local directional moves, orthogonal refinements, recombination and occasional Lévy jumps for robust global continuous optimization.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None, chosen based on dim)\n    - seed: optional RNG seed for reproducibility\n\n    The algorithm maintains a small population of solutions with per-individual\n    adaptive sigmas. It performs directional local searches, orthogonal refinements,\n    recombination between good solutions, occasional Lévy-like jumps to escape basins,\n    and rare population rejuvenation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # RNG\n        self.rng = np.random.RandomState(seed)\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds handling: ensure arrays of length dim\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        # Clip bounds just in case\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        remaining = int(self.budget)\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure numpy array and correct shape\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                # store a copy\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, fallback to pure random sampling\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-9, 0.25 * float(np.mean(ub - lb)))  # initial scale\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0c = callf(x0)\n            pop.append(x0c)\n            pop_f.append(f0)\n            # initialize individual sigma around base_sigma\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population was created, do pure random search until budget exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Main loop\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            pop_count = len(pop)\n            if pop_count == 1:\n                parent_i = 0\n            else:\n                k = min(3, pop_count)\n                inds = self.rng.choice(pop_count, size=k, replace=False)\n                vals = [pop_f[i] for i in inds]\n                parent_i = int(inds[int(np.argmin(vals))])\n\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # degenerate, try a coordinate direction\n                d = np.zeros(self.dim)\n                d[self.rng.randint(self.dim)] = 1.0\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())  # randomized step scale\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.18, np.mean(ub - lb))\n                continue  # go to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            # make r orthogonal to d (remove component along d)\n            proj = np.dot(r, d) * d\n            r = r - proj\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                step_mag = sigma * (0.4 + 0.3 * self.rng.rand())\n                x_try = np.clip(x_parent + step_mag * r, lb, ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, np.mean(ub - lb))\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                step = self.rng.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale vector relative to domain\n                scale_vec = (ub - lb) * (0.25 + 0.25 * self.rng.rand())\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace the worst if jump is beneficial\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * self.rng.rand())\n                # continue after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                # choose two best indices but add some stochasticity\n                order = np.argsort(pop_f)\n                a = int(order[0])\n                b = int(order[1]) if len(order) > 1 else a\n                beta = float(0.5 + 0.25 * self.rng.randn())  # biased toward equal mix\n                beta = max(0.05, min(0.95, beta))\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.02 * (ub - lb)) * self.rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    # replace parent\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                # continue main loop\n                continue\n\n            # adapt parent sigma on failure (shrink)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < 0.03 and remaining > 0 and len(pop) > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n        # finished budget or terminated\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()\n\n\n# Simple usage example (not to run here in the submission):\n# optimizer = ADLS(budget=1000, dim=5, seed=1)\n# fbest, xbest = optimizer(my_blackbox_func)", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.576 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11248616121152477, 0.1650198379500517, 0.8757844540297511, 0.9586614455144794, 0.7550642669643128, 0.9470033400542771, 0.29169651497815097, 0.6134927265896628, 0.8788397496424991, 0.1587673268063775]}, "task_prompt": ""}
{"id": "4273eb90-7bab-41e2-9703-2870e24d35e3", "fitness": 0.4114222442516315, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a small population of adaptive step-size points combining randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to balance exploitation and basin escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    Main idea:\n    - Maintain a modest population of candidate points each with an adaptive sigma (step-size).\n    - Use tournament selection to pick parents, perform directional trials along random normalized directions,\n      refine with local backtracking (fractional steps), attempt orthogonal perturbations to diversify,\n      occasionally perform heavy-tailed (Cauchy-like) Lévy jumps to escape basins, and recombine top solutions.\n    - Replace worst individuals when beneficial and occasionally rejuvenate population with random samples.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimension\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n        # record best found across runs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Determine bounds (support scalar or array)\n        if hasattr(func, \"bounds\") and getattr(func.bounds, \"lb\", None) is not None and getattr(func.bounds, \"ub\", None) is not None:\n            lb = np.atleast_1d(np.array(func.bounds.lb, dtype=float))\n            ub = np.atleast_1d(np.array(func.bounds.ub, dtype=float))\n        else:\n            # default for Many BBOB per specification\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # Ensure correct shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = int(self.budget)\n\n        # wrapper to call func and track budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If no budget simply return nothing useful\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # algorithm hyper-parameters (kept conservative and adaptive)\n        base_sigma = 0.5 * np.maximum(1e-3, np.linalg.norm(ub - lb) / np.sqrt(self.dim))  # base scale\n        n_init = min(self.pop_size, max(1, remaining))  # number to initialize (can't exceed remaining)\n        k = min(3, n_init)  # tournament size\n        alpha_factor = 1.0  # scaling factor for directional step (multiplicative with sigma)\n        levy_prob = 0.12\n        orth_prob = 0.35\n        recomb_prob = 0.18\n        rejuvenate_prob = 0.06\n        backtrack_fracs = (0.5, 0.25, -0.5, -0.25)\n\n        # Initialize population\n        pop = []         # list of np arrays\n        pop_f = []       # list of floats\n        pop_sigma = []   # list of sigmas (floats)\n\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n\n        # If we couldn't create any initial point (very small budget), fallback to random search until budget used\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop\n        while remaining > 0:\n            # recompute indices\n            best_i = int(np.argmin(pop_f))\n            worst_i = int(np.argmax(pop_f))\n            # pick parent via small tournament\n            tsize = min(k, len(pop))\n            inds = np.random.choice(len(pop), size=tsize, replace=False)\n            vals = [pop_f[i] for i in inds]\n            parent_local_index = int(np.argmin(vals))\n            parent_i = int(inds[parent_local_index])\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            improved = False\n\n            # sample random direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                continue\n            d = d / nd\n\n            # primary directional trial (stochasticized step length)\n            step_len = sigma * alpha_factor * max(1e-12, 1.0 + 0.3 * np.random.randn())\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # accept and increase sigma a bit\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(np.linalg.norm(ub - lb), sigma * 1.12)\n                improved = True\n            else:\n                # smaller sigma on failure (conservative)\n                pop_sigma[parent_i] = max(1e-8, sigma * 0.92)\n\n            # local backtracking / small-step refinement along the same direction\n            if not improved:\n                for frac in backtrack_fracs:\n                    if remaining <= 0:\n                        break\n                    xb = np.clip(x_parent + frac * step_len * d, lb, ub)\n                    try:\n                        fb, xb = callf(xb)\n                    except RuntimeError:\n                        fb = np.inf\n                    if fb < pop_f[parent_i]:\n                        pop[parent_i] = xb.copy()\n                        pop_f[parent_i] = fb\n                        pop_sigma[parent_i] = min(np.linalg.norm(ub - lb), pop_sigma[parent_i] * 1.06)\n                        improved = True\n                        break\n\n            # orthogonal perturbation attempt to escape narrow ridges\n            if remaining > 0 and (improved is False or np.random.rand() < orth_prob):\n                r = np.random.randn(self.dim)\n                # project r to be orthogonal to d\n                proj = np.dot(r, d)\n                r = r - proj * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_o = np.clip(x_parent + 0.6 * pop_sigma[parent_i] * r, lb, ub)\n                    try:\n                        f_o, x_o = callf(x_o)\n                    except RuntimeError:\n                        f_o = np.inf\n                    if f_o < pop_f[parent_i]:\n                        pop[parent_i] = x_o.copy()\n                        pop_f[parent_i] = f_o\n                        pop_sigma[parent_i] = min(np.linalg.norm(ub - lb), pop_sigma[parent_i] * 1.08)\n                        improved = True\n                    else:\n                        # maybe replace worst if orthogonal found a promising but not parent-beating candidate\n                        if f_o < pop_f[worst_i]:\n                            pop[worst_i] = x_o.copy()\n                            pop_f[worst_i] = f_o\n                            pop_sigma[worst_i] = base_sigma * (0.7 + 0.5 * np.random.rand())\n\n            # occasional Lévy-like jump to escape deep basins\n            if remaining > 0 and np.random.rand() < levy_prob:\n                # Cauchy-like heavy-tailed jump vector\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization to keep heavy-tail but avoid numerical blowups\n                denom = (np.linalg.norm(step) / np.sqrt(self.dim)) + 1e-9\n                step = step / denom\n                # scale by a multiple of sigma (allows very long jump sometimes)\n                scale_vec = pop_sigma[parent_i] * (2.5 + 2.0 * np.random.rand())\n                x_levy = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_levy, x_levy = callf(x_levy)\n                except RuntimeError:\n                    f_levy = np.inf\n                # if good, replace worst or parent\n                if f_levy < pop_f[worst_i]:\n                    pop[worst_i] = x_levy.copy()\n                    pop_f[worst_i] = f_levy\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n                if f_levy < pop_f[parent_i]:\n                    pop[parent_i] = x_levy.copy()\n                    pop_f[parent_i] = f_levy\n                    pop_sigma[parent_i] = min(np.linalg.norm(ub - lb), pop_sigma[parent_i] * 1.2)\n\n            # recombination exploitation between two best\n            if remaining > 0 and len(pop) >= 2 and np.random.rand() < recomb_prob:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # add small isotropic noise proportional to median sigma\n                median_sigma = np.median(pop_sigma)\n                mix = np.clip(mix + median_sigma * 0.15 * np.random.randn(self.dim), lb, ub)\n                try:\n                    f_mix, mix = callf(mix)\n                except RuntimeError:\n                    f_mix = np.inf\n                if f_mix < pop_f[worst_i]:\n                    pop[worst_i] = mix.copy()\n                    pop_f[worst_i] = f_mix\n                    pop_sigma[worst_i] = base_sigma * 0.5\n\n            # occasional random rejuvenation of worst\n            if remaining > 0 and np.random.rand() < rejuvenate_prob:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    f_new = np.inf\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # ensure the population size remains stable (can happen if replaced indices changed)\n            # if we have budget left and population smaller than desired, try to add individuals\n            while remaining > 0 and len(pop) < self.pop_size:\n                x_add = np.random.uniform(lb, ub)\n                try:\n                    f_add, x_add = callf(x_add)\n                except RuntimeError:\n                    break\n                pop.append(x_add.copy())\n                pop_f.append(f_add)\n                pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.411 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10528971393685171, 0.1890594368816646, 0.4505075863280815, 0.9625064784093954, 0.3242523210543391, 0.9572622490849252, 0.26587812971515057, 0.35454267248830595, 0.310173404611794, 0.19475045000580704]}, "task_prompt": ""}
{"id": "91abe60b-3fa1-4f4b-a324-dae1bb27ed52", "fitness": 0.4192187322484048, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional local search with orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by default)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # placeholders for best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds (func.bounds.lb/ub may be scalar or array-like)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.flatten()\n        ub = ub.flatten()\n        # guard dimension consistency\n        if lb.size != self.dim or ub.size != self.dim:\n            raise ValueError(\"Bounds dimension mismatch with provided dim\")\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure x is numpy array of correct shape and within bounds\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random sampling until budget is exhausted\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))  # initial scale\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search until budget exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # if population size has shrunk (shouldn't normally happen) refill a bit\n            if len(pop) < 2 and remaining > 0:\n                x0 = self.rng.uniform(lb, ub)\n                f0, x0 = callf(x0)\n                pop.append(x0.copy())\n                pop_f.append(f0)\n                pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n                continue\n\n            # pick a parent via small tournament to balance exploration/exploitation\n            tour_size = min(3, len(pop))\n            inds = self.rng.choice(len(pop), size=tour_size, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # degenerate; pick a coordinate direction\n                d = np.zeros(self.dim)\n                d[self.rng.randint(0, self.dim)] = 1.0\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * self.rng.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = self.rng.randn(self.dim)\n                # remove component along d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    # small fallback orthogonal: pick a random coordinate\n                    r = np.zeros(self.dim)\n                    r[self.rng.randint(0, self.dim)] = 1.0\n                    nr = 1.0\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = self.rng.standard_cauchy(self.dim)\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_jump = np.clip(x_parent + scale_vec * step, lb, ub)\n                try:\n                    f_jump, x_jump = callf(x_jump)\n                except RuntimeError:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump.copy()\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                else:\n                    # maybe replace parent if better than it\n                    if f_jump < pop_f[parent_i]:\n                        pop[parent_i] = x_jump.copy()\n                        pop_f[parent_i] = f_jump\n                        pop_sigma[parent_i] = max(1e-12, sigma * 0.7)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                # pick two distinct best indices (bias toward good ones)\n                sorted_inds = np.argsort(pop_f)\n                a = int(sorted_inds[0])\n                b = int(sorted_inds[1]) if len(sorted_inds) > 1 else int(sorted_inds[0])\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = self.rng.randn(self.dim) * (0.2 * sigma)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    # try to inject into population by replacing the worst if it's better\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                else:\n                    # maybe replace worst if it's better\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.6\n\n            # adapt parent sigma on failure (no improvement this cycle)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < 0.02 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.419 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07999339224653168, 0.15885399899136, 0.8442857902767991, 0.9770521416860576, 0.265085425911877, 0.2106392750505992, 0.23534936131467388, 0.5773038230915168, 0.7017545892798176, 0.14186952463481428]}, "task_prompt": ""}
{"id": "8e6a9b14-c183-4a38-8144-94d8b7e90d54", "fitness": 0.4430148878930251, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based directional local searches with adaptive step-sizes, orthogonal refinements and occasional heavy-tailed Lévy-like jumps to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by dim if None)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population increasing with dimension but capped\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # determine bounds if available, otherwise assume [-5, 5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # make sure bounds have correct shape\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        span[span == 0.0] = 1e-12\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # evaluation wrapper that respects budget and clips to bounds\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # base sigma scale relative to search range\n        base_sigma = 0.15 * np.mean(span)\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(float(f0))\n            # initialize sigma with a small randomized factor\n            pop_sigma.append(max(1e-12, base_sigma * (0.5 + np.random.rand())))\n\n        # If budget was so small that nothing got created, fallback to random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # main optimization loop\n        while remaining > 0:\n            n_pop = len(pop)\n\n            # choose parent by small tournament to balance exploration/exploitation\n            tsize = min(3, n_pop)\n            inds = np.random.choice(n_pop, tsize, replace=False)\n            parent_i = inds[np.argmin([pop_f[i] for i in inds])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # create search direction:\n            if np.random.rand() < 0.6:\n                # direct toward global best sometimes\n                d = (self.x_opt - x_parent) if (self.x_opt is not None) else (np.random.randn(self.dim))\n            else:\n                # difference of two random population members\n                i1, i2 = np.random.choice(n_pop, 2, replace=False)\n                d = pop[i1] - pop[i2]\n\n            # ensure direction is not degenerate\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d)\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # accept improvement and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = float(f_try)\n                pop_sigma[parent_i] = min(sigma * 1.18, np.mean(span))\n                improved = True\n            else:\n                # local backtracking / small-step refinement along direction\n                for frac in (0.5, 0.25, -0.25, -0.5):\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = float(f_try)\n                        pop_sigma[parent_i] = min(sigma * 1.12, np.mean(span))\n                        improved = True\n                        break\n\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # subtract projection onto d to get orthogonal component\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    r = np.random.randn(self.dim)\n                    r = r - np.dot(r, d) * d\n                    nr = np.linalg.norm(r)\n                r = r / (nr + 1e-12)\n                step_scale = sigma * (0.6 + 0.6 * np.random.rand())\n                x_try = np.clip(x_parent + step_scale * r, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = float(f_try)\n                    pop_sigma[parent_i] = max(1e-12, sigma * 1.05)\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and np.random.rand() < 0.08:\n                # Cauchy-like heavy-tailed vector but robustly clipped/scaled\n                step = np.random.standard_cauchy(size=self.dim)\n                # squash extreme tails with tanh but keep heavy tail behavior\n                step = np.tanh(step)  # now in (-1,1) but heavy-tailed mapping\n                scale_vec = 0.6 * span\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace the worst if it's better, else consider keeping as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = float(f_try)\n                    pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n                    continue\n                else:\n                    # occasionally insert as exploration even if not better than worst\n                    if np.random.rand() < 0.12:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = float(f_try)\n                        pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0 and np.random.rand() < 0.12:\n                # pick two best\n                order = np.argsort(pop_f)\n                b1, b2 = order[0], order[min(1, len(order)-1)]\n                beta = np.random.rand()\n                mix = beta * pop[b1] + (1.0 - beta) * pop[b2]\n                noise = 0.05 * base_sigma * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace worst if improved, else maybe replace parent if improved\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = float(f_try)\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n                    continue\n                elif f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = float(f_try)\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.95)\n                    continue\n\n            # adapt parent sigma on failure (small decrease to focus)\n            pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.94)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = float(f_new)\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.443 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11205753019028741, 0.15677760324535805, 0.8794553766923396, 0.9641633512785717, 0.3563650351324609, 0.9119314764057621, 0.22133859659163868, 0.4270484415610388, 0.26690394918688554, 0.13410751864590897]}, "task_prompt": ""}
{"id": "587be328-decf-466f-9878-c4b551475cb8", "fitness": 0.5321871205893725, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional search with orthogonal refinements and occasional heavy-tailed Lévy escapes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by dim if None)\n    - seed: optional RNG seed for reproducibility\n\n    The algorithm maintains a modest population of candidate points with adaptive\n    sigmas. Each iteration picks a parent (small tournament), performs directional\n    trials along a sampled unit direction, tries orthogonal small refinements,\n    sometimes performs heavy-tailed Lévy-like jumps, and occasionally recombines\n    the best members. Sigmas adapt up/down based on successes/failures. The\n    implementation strictly respects the evaluation budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # scale population moderately with dimension but keep it modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        # Determine bounds (Many BBOB uses func.bounds.lb / ub)\n        if hasattr(func, \"bounds\") and getattr(func.bounds, \"lb\", None) is not None:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        else:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Broadcast scalar bounds if necessary\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # Ensure shapes\n        lb = lb.ravel()[: self.dim].astype(float)\n        ub = ub.ravel()[: self.dim].astype(float)\n\n        # Basic checks\n        widths = ub - lb\n        widths[widths <= 0] = 1.0\n        base_sigma = max(1e-8, 0.25 * float(np.mean(widths)))\n\n        # internal state\n        evals = 0\n        self.f_opt = float(\"inf\")\n        self.x_opt = np.clip(self.rng.uniform(lb, ub), lb, ub)\n\n        # call wrapper that enforces budget and updates best found\n        def callf(x):\n            nonlocal evals\n            if evals >= self.budget:\n                # signal to upper loop that budget exhausted\n                raise StopIteration()\n            x = np.asarray(x, dtype=float).ravel()[: self.dim]\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            evals += 1\n            if f < self.f_opt or self.x_opt is None:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # Initialize population (as many as budget allows up to pop_size)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, max(1, self.budget))  # at least 1 if budget allows\n        try:\n            for _ in range(n_init):\n                x0 = self.rng.uniform(lb, ub)\n                f0, x0 = callf(x0)\n                pop.append(x0.copy())\n                pop_f.append(f0)\n                # diversify initial sigmas\n                pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.random()))\n        except StopIteration:\n            # budget exhausted during initialization\n            return self.f_opt, self.x_opt\n\n        # If only one point could be created (very tiny budget), fallback to returning it\n        if len(pop) == 0:\n            return self.f_opt, self.x_opt\n\n        # Helper: normalize vector to unit (safe)\n        def safe_unit(v):\n            v = np.asarray(v, dtype=float)\n            n = np.linalg.norm(v)\n            if n <= 1e-12:\n                # random unit if degenerate\n                v = self.rng.normal(size=self.dim)\n                n = np.linalg.norm(v) + 1e-12\n            return v / n\n\n        # Main loop\n        try:\n            while evals < self.budget:\n                remaining = self.budget - evals\n                # tournament selection for parent\n                k = min(3, len(pop))\n                inds = self.rng.choice(len(pop), k, replace=False)\n                vals = np.array([pop_f[i] for i in inds])\n                parent_local_idx = int(inds[int(np.argmin(vals))])  # index in pop\n                x_parent = pop[parent_local_idx].copy()\n                sigma = pop_sigma[parent_local_idx]\n\n                # sample a random search direction (normalized)\n                d = safe_unit(self.rng.normal(size=self.dim))\n\n                # primary directional trial with stochasticized step-length\n                # step uses log-normal multiplier to allow multiplicative variability\n                step_scale = sigma * np.exp(0.25 * self.rng.normal())\n                step_scale *= (0.8 + 0.6 * self.rng.random())\n                x_try = x_parent + step_scale * d\n                f_try, x_try = callf(x_try)\n\n                improved = False\n                if f_try < pop_f[parent_local_idx]:\n                    # success: accept, increase sigma moderately\n                    pop[parent_local_idx] = x_try.copy()\n                    pop_f[parent_local_idx] = f_try\n                    pop_sigma[parent_local_idx] = max(sigma * 1.12, 1e-12)\n                    improved = True\n                else:\n                    # local backtracking / small-step refinement along direction (few tries)\n                    back_step = step_scale\n                    for _bt in range(3):\n                        back_step *= 0.45\n                        x_try = x_parent + back_step * d\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_local_idx]:\n                            pop[parent_local_idx] = x_try.copy()\n                            pop_f[parent_local_idx] = f_try\n                            pop_sigma[parent_local_idx] = max(sigma * (1.06 + 0.02 * self.rng.random()), 1e-12)\n                            improved = True\n                            break\n\n                if improved:\n                    # small orthogonal refinement to exploit local ridge\n                    # try a few tiny orthogonal moves\n                    for _ort in range(2):\n                        r = self.rng.normal(size=self.dim)\n                        # make r orthogonal to d\n                        r = r - np.dot(r, d) * d\n                        r = safe_unit(r)\n                        alpha = 0.12 * pop_sigma[parent_local_idx] * (0.5 + self.rng.random())\n                        x_try = pop[parent_local_idx] + alpha * r\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_local_idx]:\n                            pop[parent_local_idx] = x_try.copy()\n                            pop_f[parent_local_idx] = f_try\n                            pop_sigma[parent_local_idx] = max(pop_sigma[parent_local_idx] * 1.04, 1e-12)\n                else:\n                    # try an orthogonal perturbation for local diversification\n                    r = self.rng.normal(size=self.dim)\n                    r = r - np.dot(r, d) * d\n                    r = safe_unit(r)\n                    nr_tries = 2\n                    rmag = max(1e-12, sigma * 0.4)\n                    for _r in range(nr_tries):\n                        x_try = x_parent + rmag * r * (0.6 + 0.8 * self.rng.random())\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_local_idx]:\n                            pop[parent_local_idx] = x_try.copy()\n                            pop_f[parent_local_idx] = f_try\n                            pop_sigma[parent_local_idx] = max(sigma * 1.03, 1e-12)\n                            improved = True\n                            break\n\n                # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n                if (self.rng.random() < 0.08) and (remaining > 0):\n                    # Cauchy-like heavy-tailed vector normalized by a robust scale\n                    v = self.rng.standard_cauchy(size=self.dim)\n                    v = safe_unit(v)\n                    # scale anchored to base_sigma but with extra leverage\n                    lev = base_sigma * (1.0 + 4.0 * self.rng.random())\n                    x_try = self.rng.choice(pop) + v * lev * (1.0 + abs(self.rng.standard_cauchy()) * 0.8)\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.random())\n                    # continue main loop after jump attempt\n                    # (do not adapt parent sigma from earlier failure here)\n                    continue\n\n                # recombination exploitation: mix two best and small noise occasionally\n                if self.rng.random() < 0.14 and len(pop) >= 2:\n                    # pick two best distinct indices\n                    order = np.argsort(pop_f)\n                    a, b = int(order[0]), int(order[1])\n                    beta = self.rng.random()\n                    mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                    mix += 0.06 * base_sigma * self.rng.normal(size=self.dim)\n                    f_try, x_try = callf(mix)\n                    # if improve parent or replace worst\n                    if f_try < pop_f[parent_local_idx]:\n                        pop[parent_local_idx] = x_try.copy()\n                        pop_f[parent_local_idx] = f_try\n                        pop_sigma[parent_local_idx] = max(pop_sigma[parent_local_idx] * 1.08, 1e-12)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try.copy()\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.random())\n\n                # adapt parent sigma on failure\n                if not improved:\n                    pop_sigma[parent_local_idx] = max(sigma * 0.85, 1e-12)\n                else:\n                    # small momentum: boost the best individual's sigma slightly\n                    pop_sigma[parent_local_idx] = min(pop_sigma[parent_local_idx] * 1.12, max(widths) * 2.0)\n\n                # occasional population rejuvenation by replacing the worst with a random sample\n                if (self.rng.random() < 0.03) and (remaining > 0):\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = self.rng.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.random())\n\n                # keep population size stable (in case of dynamic operations)\n                if len(pop) > self.pop_size:\n                    # prune worst\n                    order = np.argsort(pop_f)\n                    keep_inds = order[: self.pop_size]\n                    pop = [pop[i] for i in keep_inds]\n                    pop_f = [pop_f[i] for i in keep_inds]\n                    pop_sigma = [pop_sigma[i] for i in keep_inds]\n                elif len(pop) < 1:\n                    # replenish if something weird happens\n                    x_rand = self.rng.uniform(lb, ub)\n                    f_rand, x_rand = callf(x_rand)\n                    pop.append(x_rand.copy())\n                    pop_f.append(f_rand)\n                    pop_sigma.append(base_sigma * (0.7 + 0.6 * self.rng.random()))\n\n        except StopIteration:\n            # budget exhausted, return best found so far\n            pass\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.532 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10232969864106611, 0.30410496132280107, 0.8747121500404182, 0.9469453620321557, 0.38701849497340624, 0.9305993319019055, 0.26253390910609975, 0.4806443725239571, 0.859465863323612, 0.17351706202830186]}, "task_prompt": ""}
{"id": "fc77637e-d635-484f-aa06-aa98a1b76b21", "fitness": 0.3660520913915593, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a small-population metaheuristic combining randomized directional local searches, orthogonal refinements, and occasional heavy-tailed Lévy-like jumps with adaptive step-sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales modestly with dim)\n    - seed: optional RNG seed\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimension but remains reasonable\n            self.pop_size = max(4, min(40, 6 + self.dim // 2))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # best-so-far\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # determine bounds (support typical BBOB wrappers: func.bounds.lb/ub)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback to canonical [-5, 5] for all dims\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure correct shapes\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # budget tracking\n        remaining = int(self.budget)\n\n        # best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = float(func(x))\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        if remaining < max(10, self.pop_size):\n            # pure random search with remaining budget\n            for _ in range(remaining):\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            remaining = 0\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # initial scale based on domain size\n        domain_scale = np.maximum(1e-12, (ub - lb))\n        global_scale = float(np.mean(domain_scale))\n        sigma0 = max(1e-12, global_scale * 0.25)\n\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # initialize heterogeneous sigmas\n            pop_sigma.append(sigma0 * (0.5 + np.random.rand() * 1.5))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Keep arrays consistent\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # main loop\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            parent_i = int(min(inds, key=lambda ii: pop_f[ii]))  # best among sampled\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            improved = False\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n                dn = 1.0\n            d = d / dn\n\n            # primary directional trial with stochasticized step-length\n            # log-normal multiplicative noise for step-length\n            step_mult = np.exp(0.2 * np.random.randn())  # modest variation\n            s = sigma * step_mult * (0.5 + np.random.rand()*1.5)\n            x_try = np.clip(x_parent + s * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < f_parent:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(global_scale * 2.0, sigma * (1.0 + 0.12))\n                improved = True\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                for back in range(3):\n                    if remaining <= 0:\n                        break\n                    s2 = s * (0.5 ** (back + 1))\n                    x_try2 = np.clip(x_parent + s2 * d, lb, ub)\n                    try:\n                        f_try2, x_try2 = callf(x_try2)\n                    except RuntimeError:\n                        break\n                    if f_try2 < f_parent:\n                        pop[parent_i] = x_try2.copy()\n                        pop_f[parent_i] = f_try2\n                        pop_sigma[parent_i] = max(1e-12, sigma * (1.0 + 0.06))\n                        improved = True\n                        break\n\n            if improved:\n                # small local intensification: try a tiny orthogonal perturbation as well\n                if remaining > 0:\n                    r = np.random.randn(self.dim)\n                    nr = np.linalg.norm(r)\n                    if nr > 1e-12:\n                        # make orthogonal to main direction\n                        r = r - (r.dot(d)) * d\n                        nr = np.linalg.norm(r)\n                        if nr > 1e-12:\n                            r = r / nr\n                            x_try_o = np.clip(pop[parent_i] + 0.4 * pop_sigma[parent_i] * r, lb, ub)\n                            try:\n                                f_try_o, x_try_o = callf(x_try_o)\n                            except RuntimeError:\n                                break\n                            if f_try_o < pop_f[parent_i]:\n                                pop[parent_i] = x_try_o.copy()\n                                pop_f[parent_i] = f_try_o\n                                pop_sigma[parent_i] = min(global_scale*2.0, pop_sigma[parent_i] * 1.05)\n                # continue main loop\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    # project out component along d to get roughly orthogonal\n                    r = r - (r.dot(d)) * d\n                    nr = np.linalg.norm(r)\n                    if nr > 1e-12:\n                        r = r / nr\n                        x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                        try:\n                            f_try, x_try = callf(x_try)\n                        except RuntimeError:\n                            break\n                        if f_try < f_parent:\n                            pop[parent_i] = x_try.copy()\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = max(1e-12, sigma * 1.07)\n                            improved = True\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if (not improved) and (np.random.rand() < 0.08) and (remaining > 0):\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(self.dim)\n                # robust scaling: use median absolute value\n                med = np.median(np.abs(step))\n                if med < 1e-12:\n                    med = np.mean(np.abs(step)) + 1e-12\n                step = step / med\n                # scale leap size relative to sigma and domain\n                leap_scale = sigma * (1.0 + np.random.rand() * 4.0)\n                # clip to reasonable magnitude\n                step_vec = step / max(1e-12, np.linalg.norm(step)) * leap_scale\n                x_lev = np.clip(x_parent + step_vec, lb, ub)\n                try:\n                    f_lev, x_lev = callf(x_lev)\n                except RuntimeError:\n                    break\n                if f_lev < f_parent:\n                    # replace a poor solution in pop to encourage exploration\n                    worst_i = int(max(range(len(pop)), key=lambda ii: pop_f[ii]))\n                    pop[worst_i] = x_lev.copy()\n                    pop_f[worst_i] = f_lev\n                    # set a conservative sigma for the replaced individual\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    improved = True\n                else:\n                    # occasionally keep as a new candidate by replacing worst with small prob\n                    if np.random.rand() < 0.12:\n                        worst_i = int(max(range(len(pop)), key=lambda ii: pop_f[ii]))\n                        pop[worst_i] = x_lev.copy()\n                        pop_f[worst_i] = f_lev\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                child = 0.5 * (pop[a] + pop[b]) + 0.05 * sigma * np.random.randn(self.dim)\n                child = np.clip(child, lb, ub)\n                try:\n                    f_child, child = callf(child)\n                except RuntimeError:\n                    break\n                # replace worst if child is promising\n                worst_i = int(max(range(len(pop)), key=lambda ii: pop_f[ii]))\n                if f_child < pop_f[worst_i]:\n                    pop[worst_i] = child.copy()\n                    pop_f[worst_i] = f_child\n                    pop_sigma[worst_i] = max(1e-12, (pop_sigma[a] + pop_sigma[b]) * 0.5 * 0.9)\n\n            # adapt parent sigma on failure\n            if not improved:\n                pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.92)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if (np.random.rand() < 0.03) and (remaining > 0):\n                x_rand = np.random.uniform(lb, ub)\n                try:\n                    f_rand, x_rand = callf(x_rand)\n                except RuntimeError:\n                    break\n                worst_i = int(max(range(len(pop)), key=lambda ii: pop_f[ii]))\n                if f_rand < pop_f[worst_i]:\n                    pop[worst_i] = x_rand.copy()\n                    pop_f[worst_i] = f_rand\n                    pop_sigma[worst_i] = sigma0 * (0.5 + np.random.rand())\n\n            # small housekeeping: if population has holes (rare), fill them\n            while len(pop) < self.pop_size and remaining > 0:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop.append(x_new.copy())\n                pop_f.append(f_new)\n                pop_sigma.append(sigma0 * (0.5 + np.random.rand()))\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.366 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12549970869080807, 0.14856799571254886, 0.5851152876360591, 0.17031419481362764, 0.7992073190217905, 0.9762881477602735, 0.21432976646113744, 0.20392696182058978, 0.30300111131842566, 0.13427042068033201]}, "task_prompt": ""}
{"id": "28ce3440-160d-4256-b19e-0a5ed397013b", "fitness": 0.2294402013617721, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining adaptive per-individual step-sizes, randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy jumps to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by dim and budget if None)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # population scales with dimensionality but stays modest\n        if pop_size is None:\n            # try to keep a modest population relative to dim and budget\n            ps = max(4, min(6 + 2 * self.dim, max(4, self.budget // 30)))\n            self.pop_size = int(ps)\n        else:\n            self.pop_size = int(max(2, pop_size))\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        # book-keeping\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            if self.evals >= self.budget:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds for safety\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is extremely small -> random search fallback\n        if self.budget <= 2 or self.pop_size <= 1:\n            # very small budget, pure random search\n            while self.evals < self.budget:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (respect budget)\n        pop_size = min(self.pop_size, max(2, self.budget // 5))\n        pop = np.empty((pop_size, self.dim), dtype=float)\n        values = np.empty(pop_size, dtype=float)\n        # initial sigma per individual: relative to box size (start conservative)\n        box_scale = np.maximum(ub - lb, 1e-12)\n        init_sigma = 0.2 * np.linalg.norm(box_scale) / np.sqrt(self.dim)  # global scale\n        pop_sigma = np.ones(pop_size, dtype=float) * init_sigma\n\n        for i in range(pop_size):\n            if self.evals >= self.budget:\n                # cannot evaluate more; shrink population\n                pop_size = i\n                pop = pop[:pop_size]\n                values = values[:pop_size]\n                pop_sigma = pop_sigma[:pop_size]\n                break\n            x = self.rng.uniform(lb, ub)\n            f, x_clipped = callf(x)\n            pop[i] = x_clipped\n            values[i] = f\n\n        if pop_size == 0:\n            # budget exhausted during initialization: return best so far\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # main loop: directional local searches with orthogonal tries and Lévy jumps\n        while self.evals < self.budget:\n            # small tournament selection to pick a parent (bias towards good individuals)\n            k = min(3, pop_size)\n            inds = self.rng.choice(pop_size, size=k, replace=False)\n            # choose best among sampled\n            parent_i = int(inds[np.argmin(values[inds])])\n            parent_x = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.normal(size=self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length (log-normal tweak)\n            step_mult = np.exp(self.rng.normal(loc=0.0, scale=0.5))\n            step_len = sigma * (0.5 + self.rng.rand()) * step_mult\n            x_try = parent_x + step_len * d\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n\n            try:\n                f_try, _ = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < values[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                values[parent_i] = f_try\n                pop_sigma[parent_i] = pop_sigma[parent_i] * (1.0 + 0.12 * (1.0 - np.exp(-0.1 * step_len)))\n                # small local backtracking/refinement along the same direction (try smaller steps)\n                for back in range(3):\n                    if self.evals >= self.budget:\n                        break\n                    step_len *= 0.5\n                    x_back = pop[parent_i] + step_len * d\n                    x_back = np.minimum(np.maximum(x_back, lb), ub)\n                    f_back, _ = callf(x_back)\n                    if f_back < values[parent_i]:\n                        pop[parent_i] = x_back\n                        values[parent_i] = f_back\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.05, 1e-12)\n                    else:\n                        # no improvement: continue smaller steps\n                        continue\n                continue  # go to next iteration\n\n            # failure at primary step: small backtracking (try few smaller steps before adapting sigma)\n            improved = False\n            for bt in range(2):\n                if self.evals >= self.budget:\n                    break\n                step_len *= 0.5\n                x_bt = parent_x + step_len * d\n                x_bt = np.minimum(np.maximum(x_bt, lb), ub)\n                try:\n                    f_bt, _ = callf(x_bt)\n                except RuntimeError:\n                    break\n                if f_bt < values[parent_i]:\n                    pop[parent_i] = x_bt\n                    values[parent_i] = f_bt\n                    pop_sigma[parent_i] = pop_sigma[parent_i] * 1.08\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # adapt sigma on failure\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # orthogonal perturbation for local diversification\n            if self.evals < self.budget:\n                ort = self.rng.normal(size=self.dim)\n                # subtract projection onto d to make it (approximately) orthogonal\n                ort = ort - np.dot(ort, d) * d\n                norm_ort = np.linalg.norm(ort) + 1e-12\n                ort = ort / norm_ort\n                ort_step = 0.6 * sigma * (0.5 + self.rng.rand())\n                x_ort = parent_x + ort_step * ort\n                x_ort = np.minimum(np.maximum(x_ort, lb), ub)\n                try:\n                    f_ort, _ = callf(x_ort)\n                except RuntimeError:\n                    break\n                if f_ort < values[parent_i]:\n                    pop[parent_i] = x_ort\n                    values[parent_i] = f_ort\n                    pop_sigma[parent_i] *= 1.12\n                    continue\n                else:\n                    pop_sigma[parent_i] *= 0.95\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.07 and self.evals < self.budget:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = self.rng.standard_cauchy(size=self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                jump_scale = 2.0 + 3.0 * self.rng.rand()\n                x_jump = parent_x + sigma * jump_scale * step\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                try:\n                    f_jump, _ = callf(x_jump)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else maybe store as candidate\n                worst_i = int(np.argmax(values))\n                if f_jump < values[worst_i]:\n                    pop[worst_i] = x_jump\n                    values[worst_i] = f_jump\n                    pop_sigma[worst_i] = pop_sigma[parent_i] * 0.8\n                else:\n                    # sometimes keep as a candidate by replacing parent if it's not worse than parent\n                    if f_jump < values[parent_i]:\n                        pop[parent_i] = x_jump\n                        values[parent_i] = f_jump\n                        pop_sigma[parent_i] = pop_sigma[parent_i] * 0.9\n                continue  # after jump attempt continue main loop\n\n            # recombination exploitation: mix two best and small noise\n            if self.rng.rand() < 0.12 and self.evals < self.budget:\n                best_i = int(np.argmin(values))\n                # pick second best distinct\n                idxs = list(range(pop_size))\n                idxs.remove(best_i)\n                second_i = int(idxs[np.argmin(values[idxs])])\n                mix = 0.6 + 0.3 * self.rng.rand()\n                child = mix * pop[best_i] + (1.0 - mix) * pop[second_i]\n                child += self.rng.normal(scale=0.1 * sigma, size=self.dim)\n                child = np.minimum(np.maximum(child, lb), ub)\n                try:\n                    f_child, _ = callf(child)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else maybe replace worst\n                if f_child < values[parent_i]:\n                    pop[parent_i] = child\n                    values[parent_i] = f_child\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.05, 1e-12)\n                else:\n                    worst_i = int(np.argmax(values))\n                    if f_child < values[worst_i]:\n                        pop[worst_i] = child\n                        values[worst_i] = f_child\n                        pop_sigma[worst_i] = pop_sigma[parent_i] * 0.7\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < 0.03 and self.evals < self.budget:\n                worst_i = int(np.argmax(values))\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new_clipped = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new_clipped\n                values[worst_i] = f_new\n                pop_sigma[worst_i] = init_sigma * (0.5 + self.rng.rand())\n\n            # if population has stalled too long (small improvements), inject exploration\n            if (np.ptp(values) < 1e-6 * (1.0 + abs(self.f_opt))) and self.rng.rand() < 0.02 and self.evals < self.budget:\n                worst_i = int(np.argmax(values))\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new_clipped = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new_clipped\n                values[worst_i] = f_new\n                pop_sigma[worst_i] = init_sigma\n\n        # finished budget or stopped\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.229 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.138226104552862, 0.1577734579314054, 0.3792100697759657, 0.3940809470652723, 0.20376430330498863, 0.2227473246253029, 0.20639939125927442, 0.21909386111835438, 0.2471272089843224, 0.12597934499997254]}, "task_prompt": ""}
{"id": "5784db29-5604-4d63-aacf-78f263f80d2e", "fitness": 0.5368236907400854, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining normalized directional local searches, orthogonal refinements, recombination and occasional Cauchy (Lévy-like) jumps to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a modest function of dim and budget)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population size depending on budget and dimension\n        if pop_size is None:\n            self.pop_size = int(max(4, min(2 * self.dim, max(6, self.budget // 50))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        # storage for best result after __call__\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Setup bounds (default to [-5,5] per problem statement if missing)\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure bounds shape matches dimension\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # Helper to clip and evaluate while respecting budget\n        used = 0\n        budget = self.budget\n\n        # best tracking\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            nonlocal used, budget\n            if used >= budget:\n                return np.inf\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # enforce bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            used += 1\n            # update best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # Fallback: if budget is extremely small, random search only\n        if budget <= 5:\n            while used < budget:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population (try to create pop_size individuals; if budget too small adjust)\n        pop_size = min(self.pop_size, max(2, budget // 3))  # ensure some budget left for search\n        popX = np.empty((pop_size, self.dim), dtype=float)\n        popF = np.empty(pop_size, dtype=float)\n        # initial sigma per individual: fraction of range\n        initial_sigma = 0.15 * (ub - lb)\n        sigma = np.tile(initial_sigma.mean(), pop_size)  # scalar-like step-size per individual\n\n        actual_initialized = 0\n        for i in range(pop_size):\n            if used >= budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = callf(x)\n            popX[i, :] = x\n            popF[i] = f\n            actual_initialized += 1\n\n        # If we couldn't initialize any individual, fallback to random search for remainder\n        if actual_initialized == 0:\n            while used < budget:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Trim arrays to number actually initialized\n        if actual_initialized < pop_size:\n            popX = popX[:actual_initialized, :].copy()\n            popF = popF[:actual_initialized].copy()\n            sigma = sigma[:actual_initialized].copy()\n            pop_size = actual_initialized\n\n        # Helper to find worst\n        def worst_idx():\n            return int(np.argmax(popF))\n\n        # Main optimization loop\n        # Parameters\n        inc = 1.12\n        dec = 0.88\n        tournament_k = min(3, pop_size)\n        levy_prob = 0.06\n        orth_prob = 0.35\n        recomb_prob = 0.25\n        rejuvenate_prob = 0.02\n        backtrack_tries = 3\n\n        # Keep going until budget exhausted\n        while used < budget:\n            # compute some population statistics for scales\n            pop_mean = np.mean(popX, axis=0)\n            pop_mad = np.median(np.abs(popX - np.median(popX, axis=0)), axis=0)\n            robust_scale = np.maximum(pop_mad.mean(), 1e-6 * (ub - lb).mean())\n            sigma_mean = float(np.mean(sigma))\n\n            # parent selection via small tournament\n            cand_idx = self.rng.choice(pop_size, size=tournament_k, replace=False)\n            parent_idx = cand_idx[int(np.argmin(popF[cand_idx]))]\n            parent_x = popX[parent_idx].copy()\n            parent_f = float(popF[parent_idx])\n            parent_sigma = float(sigma[parent_idx])\n\n            improved = False\n\n            # sample a normalized random direction\n            v = self.rng.normal(size=self.dim)\n            v_norm = np.linalg.norm(v)\n            if v_norm == 0:\n                v = np.ones(self.dim)\n                v_norm = np.sqrt(self.dim)\n            v = v / v_norm\n\n            # primary directional trial with stochasticized step-length\n            step_len = parent_sigma * np.exp(self.rng.normal(0, 0.15))\n            trial_x = parent_x + step_len * v\n            trial_f = callf(trial_x)\n            if trial_f < parent_f:\n                popX[parent_idx] = np.minimum(np.maximum(trial_x, lb), ub)\n                popF[parent_idx] = trial_f\n                sigma[parent_idx] = min((ub - lb).mean(), parent_sigma * inc)\n                improved = True\n            else:\n                # backtracking / small-step refinements\n                for t in range(backtrack_tries):\n                    if used >= budget:\n                        break\n                    s_t = step_len * (0.5 ** (t + 1))\n                    x_t = parent_x + s_t * v\n                    f_t = callf(x_t)\n                    if f_t < parent_f:\n                        popX[parent_idx] = np.minimum(np.maximum(x_t, lb), ub)\n                        popF[parent_idx] = f_t\n                        sigma[parent_idx] = min((ub - lb).mean(), parent_sigma * (1.0 + 0.08 * (t + 1)))\n                        improved = True\n                        break\n\n            if used >= budget:\n                break\n\n            # orthogonal perturbation for local diversification (sometimes)\n            if not improved and self.rng.rand() < orth_prob:\n                y = self.rng.normal(size=self.dim)\n                # make orthogonal to v: y <- y - proj_v(y)\n                y = y - np.dot(y, v) * v\n                yn = np.linalg.norm(y)\n                if yn < 1e-12:\n                    # fallback random direction\n                    y = self.rng.normal(size=self.dim)\n                    yn = np.linalg.norm(y)\n                    if yn == 0:\n                        y = np.ones(self.dim)\n                        yn = np.sqrt(self.dim)\n                y = y / yn\n                x_o = parent_x + parent_sigma * 0.9 * y\n                f_o = callf(x_o)\n                if f_o < parent_f:\n                    popX[parent_idx] = np.minimum(np.maximum(x_o, lb), ub)\n                    popF[parent_idx] = f_o\n                    sigma[parent_idx] = min((ub - lb).mean(), parent_sigma * 1.08)\n                    improved = True\n                else:\n                    # maybe replace worst to keep diversity\n                    w = worst_idx()\n                    if f_o < popF[w]:\n                        popX[w] = np.minimum(np.maximum(x_o, lb), ub)\n                        popF[w] = f_o\n                        sigma[w] = parent_sigma * 0.8\n\n            if used >= budget:\n                break\n\n            # occasional Lévy-like (Cauchy) jump to escape basins\n            if self.rng.rand() < levy_prob:\n                # heavy-tailed vector\n                c = self.rng.standard_cauchy(size=self.dim)\n                # robust normalize to avoid catastrophic scales but keep heavy-tail behavior\n                c_abs_med = np.median(np.abs(c))\n                if c_abs_med == 0 or not np.isfinite(c_abs_med):\n                    c_abs_med = 1.0\n                c = c / c_abs_med\n                # scale relative to population robust scale and sigma_mean\n                jump_scale = max(0.5 * sigma_mean, 0.8 * robust_scale)\n                levy_step = c * jump_scale * (1.0 + self.rng.rand())\n                x_lev = parent_x + levy_step\n                x_lev = np.minimum(np.maximum(x_lev, lb), ub)\n                f_lev = callf(x_lev)\n                if f_lev < parent_f:\n                    # accept into population replacing worst (encourages diversity)\n                    w = worst_idx()\n                    popX[w] = x_lev\n                    popF[w] = f_lev\n                    sigma[w] = max(1e-8, sigma_mean * 0.9)\n                    improved = True\n                else:\n                    # keep as candidate replacing worst sometimes to inject diversity\n                    if f_lev < popF[worst_idx()]:\n                        w = worst_idx()\n                        popX[w] = x_lev\n                        popF[w] = f_lev\n                        sigma[w] = max(1e-8, sigma_mean * 0.85)\n\n            if used >= budget:\n                break\n\n            # recombination exploitation: mix two best and try small noise\n            if self.rng.rand() < recomb_prob and pop_size >= 2:\n                best_two = np.argsort(popF)[:2]\n                mix = 0.5 * (popX[best_two[0]] + popX[best_two[1]])\n                mix += self.rng.normal(0, 0.25 * sigma_mean, size=self.dim)\n                mix = np.minimum(np.maximum(mix, lb), ub)\n                f_mix = callf(mix)\n                if f_mix < parent_f:\n                    popX[parent_idx] = mix\n                    popF[parent_idx] = f_mix\n                    sigma[parent_idx] = max(1e-9, parent_sigma * 1.05)\n                    improved = True\n                else:\n                    # maybe replace worst\n                    w = worst_idx()\n                    if f_mix < popF[w]:\n                        popX[w] = mix\n                        popF[w] = f_mix\n                        sigma[w] = max(1e-9, sigma_mean * 0.6)\n\n            if used >= budget:\n                break\n\n            # adapt parent sigma on failure\n            if not improved:\n                sigma[parent_idx] = max(1e-9, parent_sigma * dec)\n            else:\n                # small additional positive reinforcement for those that improved earlier\n                sigma[parent_idx] = min((ub - lb).mean(), sigma[parent_idx] * 1.02)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < rejuvenate_prob and used < budget:\n                w = worst_idx()\n                x_new = self.rng.uniform(lb, ub)\n                f_new = callf(x_new)\n                if f_new < popF[w]:\n                    popX[w] = x_new\n                    popF[w] = f_new\n                    sigma[w] = sigma_mean * 0.9\n\n            # If population has converged too tightly, inject a random sample\n            if sigma_mean < 1e-6 * (ub - lb).mean() and used < budget:\n                w = worst_idx()\n                x_new = self.rng.uniform(lb, ub)\n                f_new = callf(x_new)\n                if f_new < popF[w]:\n                    popX[w] = x_new\n                    popF[w] = f_new\n                    sigma[w] = (ub - lb).mean() * 0.1\n\n            # Safety: if budget nearly exhausted, finish by random sampling of remaining budget\n            if budget - used <= 5:\n                while used < budget:\n                    x = self.rng.uniform(lb, ub)\n                    callf(x)\n                break\n\n        # finished\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.537 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17346167840355264, 0.15857488174062384, 0.9024679508322256, 0.9205718626481871, 0.314178681998195, 0.9327612826639102, 0.2521192236178429, 0.7177053698653644, 0.8383259937807432, 0.15806998185020826]}, "task_prompt": ""}
{"id": "aa3683c2-f7a9-4d31-97e4-89cd6bc04519", "fitness": 0.34498308178694204, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search mixing directional local searches, orthogonal refinements, recombination and occasional Lévy jumps to balance exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults adaptively with dim)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # Use a local RNG so we don't disturb global state\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # obtain bounds and normalize to arrays of length dim\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # protect against degenerate bounds\n        span = ub - lb\n        span[span <= 0] = 1e-8\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x.copy()\n\n        # If budget is extremely small or zero, do fallback random sampling with available budget\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(span))  # initial scale for steps\n        pop = np.zeros((n_init, self.dim), dtype=float)\n        pop_f = np.full(n_init, np.inf, dtype=float)\n        pop_sigma = np.full(n_init, base_sigma, dtype=float)\n\n        for i in range(n_init):\n            # sample uniformly in the box\n            x0 = lb + self.rng.rand(self.dim) * span\n            f0, x0 = callf(x0)\n            pop[i] = x0\n            pop_f[i] = f0\n            pop_sigma[i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if pop.shape[0] == 0 or remaining <= 0:\n            while remaining > 0:\n                x = lb + self.rng.rand(self.dim) * span\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, pop.shape[0])\n            inds = self.rng.choice(pop.shape[0], k, replace=False)\n            values = pop_f[inds]\n            parent_i = int(inds[int(np.argmin(values))])\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.2, np.mean(span))\n                    continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            r = r - np.dot(r, d) * d  # make orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(span))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = self.rng.standard_cauchy(self.dim)\n                denom = max(1e-6, float(np.median(np.abs(step))))\n                step = step / denom\n                scale_vec = 0.2 * span\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    # if jump helps parent also accept\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                    continue\n\n            # recombination exploitation: mix two best and small noise\n            order = np.argsort(pop_f)\n            if order.size >= 2:\n                a, b = int(order[0]), int(order[1])\n            else:\n                a = b = int(order[0])\n            beta = self.rng.rand()\n            mix = beta * pop[a] + (1.0 - beta) * pop[b]\n            noise = (0.01 * span) * self.rng.randn(self.dim)\n            x_try = np.clip(mix + noise, lb, ub)\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = lb + self.rng.rand(self.dim) * span\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.345 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09697238463291435, 0.15116175816182176, 0.5947447465708351, 0.19672348917756133, 0.625473796676908, 0.9679596805395136, 0.21413136932163745, 0.18172448040097067, 0.25799927211127227, 0.16293984027598574]}, "task_prompt": ""}
{"id": "d856b1d7-0905-49a4-84f1-812e5eca429e", "fitness": 0.5304132464985472, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional local searches with orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(40, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # use a private RNG so we don't alter global numpy state\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds (func.bounds.lb/ub might be scalar or array-like)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            nonlocal_best = False\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n                nonlocal_best = True\n            return float(f), x.copy(), nonlocal_best\n\n        # quick fallback if budget <= 0\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialization: sample initial population\n        pop = []\n        pop_f = []\n        # base scale for sigmas: relative to search range\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n        pop_sigma = []\n\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0_eval, _ = callf(x0)\n            pop.append(x0_eval)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * self.rng.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop\n        # parameters\n        p_levy = 0.08\n        p_rejuvenate = 0.02\n        tournament_k = 3\n        small_noise_scale = 0.01  # relative to domain\n        orth_step_scale = 0.6\n        levy_scale_rel = 0.2\n\n        # a stagnation counter to occasionally increase global diversity\n        stagnation = 0\n        last_best = self.f_opt\n\n        while remaining > 0:\n            # choose a parent by tournament (small k)\n            k = min(tournament_k, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length (allow some randomness)\n            alpha = max(1e-12, sigma * (1.0 + 0.5 * self.rng.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try, was_best = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                stagnation = 0\n                last_best = self.f_opt\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try, was_best = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    improved = True\n                    stagnation = 0\n                    last_best = self.f_opt\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                # choose a reasonable orthogonal step size relative to sigma and domain\n                orth_step = orth_step_scale * sigma * (0.8 + 0.4 * self.rng.rand())\n                x_try = np.clip(x_parent + orth_step * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try, was_best = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        stagnation = 0\n                        last_best = self.f_opt\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < p_levy and remaining > 0:\n                # use standardized Cauchy (heavy-tailed) vector\n                step = self.rng.standard_cauchy(size=self.dim)\n                # robust normalization: divide by 90th percentile of absolute values to avoid single huge components\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = levy_scale_rel * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try, was_best = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n                        stagnation = 0\n                        last_best = self.f_opt\n                    else:\n                        # sometimes keep a candidate by replacing worst with some probability\n                        if self.rng.rand() < 0.15:\n                            worst_i = int(np.argmax(pop_f))\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (small_noise_scale * (ub - lb)) * self.rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try, was_best = callf(x_try)\n                worst_i = int(np.argmax(pop_f))\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.05, np.mean(ub - lb))\n                    stagnation = 0\n                    last_best = self.f_opt\n                else:\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n                        stagnation = 0\n                        last_best = self.f_opt\n\n            # adapt parent sigma on failure (mild decrease)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < p_rejuvenate:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new, was_best = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n                stagnation = 0\n                last_best = self.f_opt\n\n            # update stagnation and maybe increase diversity if stuck\n            if self.f_opt < last_best - 1e-12:\n                last_best = self.f_opt\n                stagnation = 0\n            else:\n                stagnation += 1\n\n            if stagnation > max(10, 2 * self.dim):\n                # inject a random individual to the worst slot and slightly increase all sigmas\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, x_new, was_best = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + self.rng.rand())\n                # boost sigmas a bit to help escape\n                pop_sigma = [min(s * 2.0, np.mean(ub - lb)) for s in pop_sigma]\n                stagnation = 0\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.530 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15388409649296741, 0.198251554639593, 0.8916089367527258, 0.9670805469527882, 0.2993261531907301, 0.9520275041320794, 0.29905298911009537, 0.49429054895407676, 0.9029295443142453, 0.145680590446171]}, "task_prompt": ""}
{"id": "14388bb7-fc2e-4def-829a-b74c5d143459", "fitness": 0.29677387428674606, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based search that combines randomized normalized directional local searches, orthogonal refinements, recombination, and occasional heavy-tailed Lévy-like jumps with adaptive per-individual step-sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a modest function of dim)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimension\n            self.pop_size = max(6, int(4 + 2.5 * np.sqrt(self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        # use a local RNG for reproducibility\n        self.rng = np.random.RandomState(seed)\n        # best seen\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Retrieve bounds and ensure full-dim arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            raise ValueError(\"Bounds length must match self.dim\")\n\n        # Local budget tracker\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale (relative to search box)\n\n        for i in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # per-individual sigma randomized a bit\n            pop_sigma.append(base_sigma * (0.75 + 0.5 * self.rng.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Convert to arrays for some fast ops when needed\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        # We'll keep iterating while evaluations remain\n        # Some control params\n        frac_list = (0.5, 0.25, -0.5, -0.25)\n        levy_prob = 0.08\n        rej_prob = 0.02  # rejuvenation probability\n        tournament_k = 3\n\n        # track unsuccessful trials count per individual to adapt sigma occasionally\n        fail_counts = [0] * len(pop)\n\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(tournament_k, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            improved_any = False\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * self.rng.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, np.max(ub - lb))\n                fail_counts[parent_i] = 0\n                improved_any = True\n                continue  # move to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in frac_list:\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    fail_counts[parent_i] = 0\n                    improved = True\n                    improved_any = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            r = r - np.dot(r, d) * d  # project to orthogonal subspace\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        fail_counts[parent_i] = 0\n                        improved_any = True\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < levy_prob and remaining > 0:\n                step = self.rng.standard_cauchy(size=self.dim)\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # choose a robust scale vector: linked to local sigma and box size\n                box_scale = (ub - lb)\n                scale_vec = np.maximum(1e-12, sigma * (0.6 + 2.0 * self.rng.rand(self.dim)) + 0.2 * box_scale)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                        fail_counts[worst_i] = 0\n                        improved_any = True\n                # continue loop after jump attempt (go to next parent selection)\n                if improved_any:\n                    continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = 0.4 + 0.2 * self.rng.rand()  # mix weight\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = self.rng.randn(self.dim) * (0.08 * sigma)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    fail_counts[parent_i] = 0\n                    improved_any = True\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                        fail_counts[worst_i] = 0\n                        improved_any = True\n\n            # adapt parent sigma on failure\n            if not improved_any:\n                # slightly decay sigma for the parent; occasional reset if many consecutive failures\n                fail_counts[parent_i] = fail_counts[parent_i] + 1\n                pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * (0.92 if fail_counts[parent_i] < 10 else 0.8))\n                # if it has failed a lot, probabilistically reinitialize that individual\n                if fail_counts[parent_i] > 25 and self.rng.rand() < 0.3 and remaining > 0:\n                    x_new = self.rng.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop[parent_i] = x_new\n                    pop_f[parent_i] = f_new\n                    pop_sigma[parent_i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n                    fail_counts[parent_i] = 0\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < rej_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n                fail_counts[worst_i] = 0\n\n            # trim/expand book-keeping lists if population sizes changed (they don't here, but keep consistent)\n            # ensure lists lengths match\n            if len(pop_sigma) != len(pop_f):\n                # safety: sync lengths if something odd happened\n                while len(pop_sigma) < len(pop_f):\n                    pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n                while len(pop_sigma) > len(pop_f):\n                    pop_sigma.pop()\n\n        # finished budget or loop ended\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.297 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11761945745250035, 0.1510428301857667, 0.5728713823308857, 0.9356769866051688, 0.18789200811858198, 0.18187764025523456, 0.20519228162988412, 0.25359923799549255, 0.23803747200961367, 0.12392944628433189]}, "task_prompt": ""}
{"id": "80247bd2-5d3a-4ce0-a515-71c093680e27", "fitness": 0.4785120086589175, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, occasional Lévy jumps, and small recombination to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    Main idea:\n      Maintain a modest population of candidate points each with an adaptive\n      step-size (sigma). Repeatedly perform directional local searches from\n      selected parents (stochastic step length along random direction),\n      attempt small backtracking refinements, orthogonal perturbations for\n      local diversification, occasional Lévy-like heavy-tailed jumps to escape\n      basins, and simple recombination of top individuals. Step sizes are\n      adapted per individual based on success/failure. The algorithm strictly\n      respects the evaluation budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize black-box func using at most self.budget evaluations.\n        func(x) expects x shape (dim,) and has attributes bounds.lb and bounds.ub\n        (scalars or arrays). Returns (f_best, x_best).\n        \"\"\"\n        # Prepare bounds as arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # clamp scenario where bounds inconsistent\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            # fallback to [-5, 5] per instruction if provided bounds mismatch\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure x is numpy array of correct shape\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = np.zeros((self.pop_size, self.dim), dtype=float)\n        pop_f = np.full(self.pop_size, np.inf, dtype=float)\n        pop_sigma = np.zeros(self.pop_size, dtype=float)\n\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale\n        for i in range(n_init):\n            # sample uniformly in box\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop[i, :] = x0\n            pop_f[i] = f0\n            pop_sigma[i] = base_sigma * (0.8 + 0.4 * np.random.rand())\n\n        # if we could not fill population because of tiny budget, do random search with remaining points\n        if n_init < self.pop_size:\n            # use remaining evaluations to sample random solutions\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        # Some algorithm hyper-parameters (kept simple and robust)\n        p_levy = 0.08\n        rejuvenation_prob = 0.03\n        max_backtracks = 4\n        backtrack_fracs = (0.5, 0.25, -0.5, -0.25)\n        tournament_k = 3\n\n        # small bound scale for noise\n        bound_scale = np.maximum(1e-12, ub - lb)\n\n        while remaining > 0:\n            # ensure we have up-to-date valid population (in case some entries are inf)\n            valid_idx = np.where(np.isfinite(pop_f))[0]\n            if valid_idx.size == 0:\n                # something went wrong; sample a random point\n                x = np.random.uniform(lb, ub)\n                callf(x)\n                continue\n\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(tournament_k, valid_idx.size)\n            inds = np.random.choice(valid_idx, size=k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n\n            x_parent = pop[parent_i].copy()\n            sigma = max(pop_sigma[parent_i], 1e-12)\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # fallback to coordinate direction\n                d = np.zeros(self.dim)\n                d[np.random.randint(self.dim)] = 1.0\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15, np.mean(bound_scale))\n                # continue to next iteration (exploit success)\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in backtrack_fracs[:max_backtracks]:\n                if remaining <= 0:\n                    break\n                x_bt = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_bt, x_bt = callf(x_bt)\n                if f_bt < pop_f[parent_i]:\n                    pop[parent_i] = x_bt\n                    pop_f[parent_i] = f_bt\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            # build r orthogonal to d: r - (r.dot(d))*d\n            r = np.random.randn(self.dim)\n            nd_r = np.linalg.norm(r)\n            if nd_r > 1e-12:\n                # project out component along d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    # small orthogonal step scaled by sigma\n                    ortho_step = 0.5 * sigma * (0.5 + np.random.rand())\n                    x_o = np.clip(x_parent + ortho_step * r, lb, ub)\n                    if remaining <= 0:\n                        break\n                    f_o, x_o = callf(x_o)\n                    if f_o < pop_f[parent_i]:\n                        pop[parent_i] = x_o\n                        pop_f[parent_i] = f_o\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(bound_scale))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < p_levy and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.median(np.abs(step)) + 1e-12\n                step = step / denom  # keep heavy-tail but avoid extreme amplitude\n                # scale vector by a combination of population sigma and bound scale\n                scale_vec = np.maximum(1e-8, pop_sigma[parent_i]) * (0.5 + np.random.rand()) + 0.02 * bound_scale\n                x_j = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining <= 0:\n                    break\n                f_j, x_j = callf(x_j)\n                # if it's good replace the worst in population, else maybe store as candidate (we replace worst if beneficial)\n                worst_i = int(np.argmax(pop_f))\n                if f_j < pop_f[worst_i]:\n                    pop[worst_i] = x_j\n                    pop_f[worst_i] = f_j\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0 and valid_idx.size >= 2:\n                # select two best indices\n                top2 = np.argsort(pop_f)[:2]\n                a, b = top2[0], top2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * bound_scale) * np.random.randn(self.dim)\n                x_mix = np.clip(mix + noise, lb, ub)\n                f_mix, x_mix = callf(x_mix)\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # adapt parent sigma on failure (slightly decrease)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < rejuvenation_prob and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.479 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10953439062667125, 0.16539195687943076, 0.6115977751325988, 0.820151794155836, 0.8497622520331557, 0.9621539397769917, 0.2655553815950159, 0.4155698746937524, 0.4450166924013633, 0.14038602929435917]}, "task_prompt": ""}
{"id": "878f60b5-b35c-468d-9acd-f8c4dde24d74", "fitness": 0.3936148400099212, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive directional local searches with orthogonal refinements, recombination and occasional Lévy jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size; by default scales modestly with dim\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modestly scale population with dimensionality\n            self.pop_size = max(6, int(4 + 2 * np.sqrt(self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        # bounds as arrays (handle scalar bounds)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # safety: ensure dims match\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.resize(lb, self.dim)\n            ub = np.resize(ub, self.dim)\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            if remaining <= 0:\n                # cannot evaluate anymore\n                return np.inf, x\n            remaining -= 1\n            f = float(func(x))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize base sigma from bounds range\n        base_sigma = 0.2 * np.mean(ub - lb)  # moderate default step\n        base_sigma = max(base_sigma, 1e-9)\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        for _ in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = lb + self.rng.rand(self.dim) * (ub - lb)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # each individual has its own adaptive sigma\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * self.rng.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                callf(x)\n            return self.f_opt, None if self.x_opt is None else self.x_opt.copy()\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), size=k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            d = d / (nd + 1e-12)\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            # success: accept and slightly increase sigma\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            back_scales = [0.5, 0.25, 0.1]\n            for bs in back_scales:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + bs * alpha * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            r = r - np.dot(r, d) * d  # make orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12 and remaining > 0:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = self.rng.standard_cauchy(self.dim)\n                # robust normalization to avoid infinite extremes while keeping heavy tails\n                scale = np.median(np.abs(step)) + 1e-12\n                step = step / scale\n                # scale by sigma and a random heavy factor\n                heavy_factor = 1.0 + 3.0 * np.abs(self.rng.standard_cauchy())\n                step = step * sigma * np.sqrt(self.dim) * heavy_factor\n                x_try = np.clip(x_parent + step, lb, ub)\n                f_try, x_try = callf(x_try)\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(base_sigma * (0.7 + 0.6 * self.rng.rand()), 1e-12)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                b0, b1 = best2[0], best2[1]\n                beta = self.rng.rand()\n                mix = pop[b0] + beta * (pop[b1] - pop[b0])\n                noise = 0.1 * sigma * self.rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n            # adapt parent sigma on failure (mild reduction to encourage exploration->exploitation)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = lb + self.rng.rand(self.dim) * (ub - lb)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.394 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10387255089358238, 0.15271829407602588, 0.5881559679507488, 0.9132341735184877, 0.3013039575410723, 0.8311775145623469, 0.22551519738468928, 0.43164624634673565, 0.25187682680960377, 0.13664767101591901]}, "task_prompt": ""}
{"id": "9a108aaa-ab74-4dec-9233-fad93737bc1c", "fitness": 0.5695847977090815, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that combine randomized directional local searches, orthogonal refinements, recombination and occasional Lévy-like jumps to robustly explore and exploit continuous domains.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but remains modest\n            pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        self.pop_size = int(pop_size)\n        self.seed = seed\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # Robustly build full-dim lb/ub arrays regardless of how func exposes them\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        if lb.shape != (self.dim,) or ub.shape != (self.dim,):\n            raise ValueError(\"Bounds must be scalars or vectors of length dim\")\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best found\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return np.inf, x\n            # clip to bounds to be safe\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If no budget, return immediately\n        if remaining <= 0:\n            return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()\n\n        # Initialize a modest population with one evaluation per individual (if budget allows)\n        pop_list = []\n        pop_f_list = []\n        pop_sigma_list = []\n        domain_scale = ub - lb\n        mean_scale = float(np.mean(domain_scale))\n        base_sigma = max(1e-12, 0.15 * mean_scale)  # a sensible initial step size\n\n        for _ in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop_list.append(x0)\n            pop_f_list.append(f0)\n            pop_sigma_list.append(base_sigma)\n\n        # If budget too small to create any population point, fallback to pure random search\n        if len(pop_list) == 0:\n            while remaining > 0:\n                x = rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()\n\n        # Turn lists into arrays for efficient operations\n        pop = np.array(pop_list)           # shape (n, dim)\n        pop_f = np.array(pop_f_list)      # shape (n,)\n        pop_sigma = np.array(pop_sigma_list)  # shape (n,)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, recombination, and Lévy jumps\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = rng.choice(len(pop), k, replace=False)\n            values = pop_f[inds]\n            parent_i = int(inds[int(np.argmin(values))])\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length (heavy-tail via Cauchy)\n            # step length uses individual's sigma scaled by an absolute Cauchy to keep heavy tails\n            step_factor = abs(rng.standard_cauchy())\n            # guard extreme values, keep heavy tail but bounded\n            step_factor = np.clip(step_factor, 1e-6, 1e3)\n            step_len = sigma * step_factor\n            x_try = np.clip(x_parent + d * step_len, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, max(mean_scale, sigma * 1.2))\n                continue\n            else:\n                # mild contraction on failure\n                pop_sigma[parent_i] = max(sigma * 0.92, 1e-12)\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + d * (step_len * frac), lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, max(mean_scale, sigma * 1.1))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = rng.randn(self.dim)\n                # make r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    r = rng.randn(self.dim)\n                    r = r - np.dot(r, d) * d\n                    nr = np.linalg.norm(r) + 1e-12\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.15, max(mean_scale, sigma * 1.15))\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if rng.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = rng.standard_cauchy(self.dim)\n                denom = np.median(np.abs(step)) + 1e-9\n                step = step / denom\n                scale_vec = 0.2 * domain_scale\n                # apply elementwise scaled heavy-tail\n                x_jump = np.clip(x_parent + scale_vec * step, lb, ub)\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < pop_f[parent_i]:\n                    pop[parent_i] = x_jump\n                    pop_f[parent_i] = f_jump\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.6)\n                else:\n                    # try to replace worst if jump produced something better than the worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop\n                continue\n\n            # recombination exploitation: mix two of the best and add small noise\n            if rng.rand() < 0.15 and remaining > 0 and len(pop) >= 2:\n                # pick two distinct among the top few\n                top_k = min(4, len(pop))\n                candidates = np.argsort(pop_f)[:top_k]\n                a, b = rng.choice(candidates, 2, replace=False)\n                beta = rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * domain_scale) * rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # if it's good replace parent or worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    continue\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (small decay)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.985, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if rng.rand() < 0.03 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.570 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08801231062606663, 0.1946749014145196, 0.8678694275216579, 0.932341877700354, 0.8671030170182686, 0.9425478320896228, 0.25523361345870466, 0.5302112702546184, 0.8415446474206453, 0.17630907958635755]}, "task_prompt": ""}
{"id": "c150a195-242e-4cdc-af48-56a846d539a5", "fitness": 0.4437313179490737, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a small-population, adaptive step-size search that alternates directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None it's derived from dim and budget)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        if seed is not None:\n            np.random.seed(int(seed))\n\n    def __call__(self, func):\n        # read bounds, make sure they are arrays of length dim\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # internal state\n        self.remaining = int(self.budget)\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        # safe function caller: clips, checks budget and updates best\n        def callf(x):\n            x = np.array(x, dtype=float)\n            # ensure correct shape\n            if x.size != self.dim:\n                x = x.ravel()[:self.dim].copy()\n                if x.size != self.dim:\n                    raise ValueError(\"Dimension mismatch in callf\")\n            x = np.clip(x, lb, ub)\n            if self.remaining <= 0:\n                # signal budget exhausted\n                raise StopIteration()\n            f = float(func(x))\n            self.remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # determine base sigma from bounds (typical step-size)\n        base_sigma = 0.25 * np.mean(ub - lb)\n        if base_sigma <= 0:\n            base_sigma = 1.0\n\n        # choose population size reasonably small but dimension-aware\n        if self.pop_size is None:\n            pop_size = int(max(4, min(self.dim * 4, max(2, self.budget // 12))))\n        else:\n            pop_size = max(2, int(self.pop_size))\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        try:\n            for i in range(pop_size):\n                if self.remaining <= 0:\n                    break\n                x0 = np.random.uniform(lb, ub)\n                f0, x0 = callf(x0)\n                pop.append(x0)\n                pop_f.append(f0)\n                pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n        except StopIteration:\n            pass\n\n        # If no population could be created, fallback to pure random search with remaining budget\n        if len(pop) == 0:\n            try:\n                while self.remaining > 0:\n                    x = np.random.uniform(lb, ub)\n                    callf(x)\n            except StopIteration:\n                pass\n            return self.f_opt, self.x_opt\n\n        # Main loop: repeatedly perform directional local searches, orthogonal tries, and occasional Lévy jumps\n        try:\n            while self.remaining > 0:\n                # pick a parent via small tournament (size 2 or 3)\n                tour_size = min(3, len(pop))\n                inds = np.random.choice(len(pop), size=tour_size, replace=False)\n                values = [pop_f[i] for i in inds]\n                parent_i = inds[int(np.argmin(values))]\n                x_parent = pop[parent_i].copy()\n                sigma = pop_sigma[parent_i]\n\n                # sample a random search direction (normalized)\n                d = np.random.normal(size=self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    # degenerate direction, skip to next iteration\n                    continue\n                d = d / nd\n\n                # primary directional trial with stochasticized step-length (Cauchy-like scalar)\n                s = np.random.standard_cauchy()\n                # limit extreme scale but keep heavy-tail behavior\n                s = np.sign(s) * min(6.0, max(0.01, abs(s)))\n                step_len = sigma * (1.0 + 0.4 * np.tanh(s))  # positive multiplier\n                x_try = np.clip(x_parent + step_len * d, lb, ub)\n                f_try, x_try = callf(x_try)\n\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(base_sigma * 10.0, pop_sigma[parent_i] * 1.12)\n                    # small local refinement along the successful direction\n                    for shrink in (0.5, 0.25):\n                        if self.remaining <= 0:\n                            break\n                        x_ref = np.clip(x_parent + step_len * shrink * d, lb, ub)\n                        f_ref, x_ref = callf(x_ref)\n                        if f_ref < pop_f[parent_i]:\n                            pop[parent_i] = x_ref\n                            pop_f[parent_i] = f_ref\n                            pop_sigma[parent_i] = min(base_sigma * 10.0, pop_sigma[parent_i] * 1.05)\n                    continue\n\n                # local backtracking / small-step refinement along direction (few tries)\n                improved = False\n                for k in range(3):\n                    if self.remaining <= 0:\n                        break\n                    factor = 0.5 ** (k + 1)\n                    x_bt = np.clip(x_parent + step_len * factor * d, lb, ub)\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt < pop_f[parent_i]:\n                        pop[parent_i] = x_bt\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 1.06)\n                        improved = True\n                        break\n                if improved:\n                    continue\n\n                # orthogonal perturbation for local diversification\n                r = np.random.normal(size=self.dim)\n                # orthogonalize relative to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_ort = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    f_ort, x_ort = callf(x_ort)\n                    if f_ort < pop_f[parent_i]:\n                        pop[parent_i] = x_ort\n                        pop_f[parent_i] = f_ort\n                        pop_sigma[parent_i] *= 1.04\n                        continue\n\n                # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n                p_levy = 0.08\n                if np.random.rand() < p_levy and self.remaining > 0:\n                    step_vec = np.random.standard_cauchy(self.dim)\n                    # robust normalization to avoid singular scale\n                    scale = np.median(np.abs(step_vec)) + 1e-9\n                    step_vec = step_vec / scale\n                    step_scale = sigma * (2.0 + 8.0 * np.random.rand())  # large multiplier\n                    x_jump = np.clip(x_parent + step_scale * step_vec, lb, ub)\n                    f_jump, x_jump = callf(x_jump)\n                    worst_i = int(np.argmax(pop_f))\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.4 * np.random.rand())\n                    if f_jump < pop_f[parent_i]:\n                        pop[parent_i] = x_jump\n                        pop_f[parent_i] = f_jump\n                    # continue main loop after jump attempt\n                    continue\n\n                # recombination exploitation: mix two best and small noise\n                best2 = np.argsort(pop_f)[:2]\n                mix = 0.6 * pop[best2[0]] + 0.4 * pop[best2[1]]\n                noise = np.random.normal(scale=0.3 * sigma, size=self.dim)\n                x_rec = np.clip(mix + noise, lb, ub)\n                f_rec, x_rec = callf(x_rec)\n                worst_i = int(np.argmax(pop_f))\n                if f_rec < pop_f[parent_i]:\n                    pop[parent_i] = x_rec\n                    pop_f[parent_i] = f_rec\n                    pop_sigma[parent_i] = min(base_sigma * 10.0, pop_sigma[parent_i] * 1.08)\n                    # if it's also better than the worst, consider injecting\n                    if f_rec < pop_f[worst_i]:\n                        pop[worst_i] = x_rec\n                        pop_f[worst_i] = f_rec\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                elif f_rec < pop_f[worst_i]:\n                    pop[worst_i] = x_rec\n                    pop_f[worst_i] = f_rec\n                    pop_sigma[worst_i] = base_sigma * 0.5\n                else:\n                    # adapt parent sigma on failure (conservative shrink)\n                    pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.94)\n\n                # occasional population rejuvenation by replacing the worst with a random sample\n                if np.random.rand() < 0.02 and self.remaining > 0:\n                    worst_i = int(np.argmax(pop_f))\n                    x_rand = np.random.uniform(lb, ub)\n                    f_rand, x_rand = callf(x_rand)\n                    pop[worst_i] = x_rand\n                    pop_f[worst_i] = f_rand\n                    pop_sigma[worst_i] = base_sigma * (0.8 + 0.4 * np.random.rand())\n\n                # keep sigma values within reason to avoid collapse or blow-up\n                for i in range(len(pop_sigma)):\n                    pop_sigma[i] = np.clip(pop_sigma[i], base_sigma * 1e-6, base_sigma * 20.0)\n\n        except StopIteration:\n            # budget exhausted\n            pass\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.444 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15403277001640037, 0.15927009123484637, 0.5267623426087544, 0.8607360127126071, 0.5469058210566089, 0.6310569309862911, 0.29052804896988915, 0.5012689611443566, 0.5695001601637382, 0.19725204059724522]}, "task_prompt": ""}
{"id": "8e74521a-21f7-45cc-aedb-ec9bc6d52201", "fitness": 0.4926087202938807, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimiser combining randomized directional local searches, orthogonal refinements, adaptive step-sizes and occasional heavy-tailed (Lévy-like) jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (adaptive by default)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population size heuristic: grows with dimension but kept modest\n        if pop_size is None:\n            self.pop_size = max(4, min(2 * self.dim, 40))\n        else:\n            self.pop_size = max(1, int(pop_size))\n        # best-so-far\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # determine bounds if provided by func, otherwise use [-5,5] default\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure correct shapes\n        if lb.ndim == 0:\n            lb = np.full(self.dim, float(lb))\n        if ub.ndim == 0:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # internal evaluation wrapper to track remaining budget and best\n        remaining = int(self.budget)\n\n        def callf(x):\n            nonlocal remaining\n            # never call if no budget remains\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population using a fraction of the budget\n        # ensure at least 1 init if possible\n        n_init = min(self.pop_size, max(1, self.budget // 10))\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base sigma scale derived from bounds\n        bound_range = np.linalg.norm(ub - lb)\n        if bound_range <= 0:\n            bound_range = 1.0\n        base_sigma_scalar = max(1e-12, 0.2 * bound_range / max(1.0, np.sqrt(self.dim)))\n\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize sigma randomly around base\n            pop_sigma.append(base_sigma_scalar * (0.5 + np.random.rand()))\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                try:\n                    f, x = callf(x)\n                except RuntimeError:\n                    break\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # helper to find best and worst indices\n        def best_idx():\n            return int(np.argmin(pop_f))\n        def worst_idx():\n            return int(np.argmax(pop_f))\n\n        # main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # ensure lists are consistent\n            n_pop = len(pop)\n            if n_pop == 0:\n                break\n\n            # pick a parent via small tournament to balance exploration/exploitation\n            if n_pop >= 2:\n                a, b = np.random.randint(0, n_pop), np.random.randint(0, n_pop)\n                parent_i = a if pop_f[a] < pop_f[b] else b\n            else:\n                parent_i = 0\n\n            x_parent = pop[parent_i].copy()\n            parent_sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            dir_vec = np.random.normal(size=self.dim)\n            dir_norm = np.linalg.norm(dir_vec)\n            if dir_norm <= 1e-12:\n                dir_vec = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                dir_vec = dir_vec / dir_norm\n\n            # primary directional trial with stochasticized step-length\n            # use log-normal perturbation to sigma to add noise\n            step_length = parent_sigma * np.exp(np.random.normal(scale=0.2))\n            x_try = x_parent + step_length * dir_vec\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(parent_sigma * 1.2, bound_range * 2.0)\n                continue  # go to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for shrink in (0.5, 0.2, 0.05):\n                if remaining <= 0:\n                    break\n                small_step = step_length * shrink\n                x_bt = x_parent + small_step * dir_vec\n                x_bt = np.minimum(np.maximum(x_bt, lb), ub)\n                try:\n                    f_bt, x_bt = callf(x_bt)\n                except RuntimeError:\n                    break\n                if f_bt < pop_f[parent_i]:\n                    pop[parent_i] = x_bt\n                    pop_f[parent_i] = f_bt\n                    pop_sigma[parent_i] = max(parent_sigma * (1.0 + 0.1 * (1.0 - shrink)), 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                # sample a vector and make it orthogonal to dir_vec\n                v = np.random.normal(size=self.dim)\n                v = v - np.dot(v, dir_vec) * dir_vec\n                vnorm = np.linalg.norm(v)\n                if vnorm > 1e-12:\n                    v /= vnorm\n                    x_ort = x_parent + parent_sigma * 0.5 * v\n                    x_ort = np.minimum(np.maximum(x_ort, lb), ub)\n                    try:\n                        f_ort, x_ort = callf(x_ort)\n                    except RuntimeError:\n                        break\n                    if f_ort < pop_f[parent_i]:\n                        pop[parent_i] = x_ort\n                        pop_f[parent_i] = f_ort\n                        pop_sigma[parent_i] = max(parent_sigma * 1.05, 1e-12)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.06 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale (90th percentile)\n                step = np.random.standard_cauchy(size=self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale to be noticeably larger than local sigma but bounded\n                jump_scale = max(5.0, 2.0 + np.random.rand() * 8.0) * parent_sigma\n                x_jump = x_parent + step * jump_scale\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                try:\n                    f_jump, x_jump = callf(x_jump)\n                except RuntimeError:\n                    break\n                if f_jump < pop_f[parent_i]:\n                    # accept big improvement into parent\n                    pop[parent_i] = x_jump\n                    pop_f[parent_i] = f_jump\n                    pop_sigma[parent_i] = max(parent_sigma * 0.8, 1e-12)\n                else:\n                    # try placing candidate into population by replacing the worst if it's better\n                    w = worst_idx()\n                    if f_jump < pop_f[w]:\n                        pop[w] = x_jump\n                        pop_f[w] = f_jump\n                        pop_sigma[w] = base_sigma_scalar * 0.5\n                # after a jump try next main iteration\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0 and np.random.rand() < 0.18:\n                bi = best_idx()\n                # pick another good partner (tournament among randoms)\n                rnds = np.random.choice(len(pop), size=min(4, len(pop)), replace=False)\n                partner = rnds[np.argmin([pop_f[i] for i in rnds if i != bi] + [np.inf])]\n                if partner == bi:\n                    partner = (bi + 1) % len(pop)\n                x_new = 0.5 * (pop[bi] + pop[partner]) + np.random.normal(scale=0.02 * base_sigma_scalar, size=self.dim)\n                x_new = np.minimum(np.maximum(x_new, lb), ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                # replace worst or replace parent depending on quality\n                w = worst_idx()\n                if f_new < pop_f[parent_i]:\n                    pop[parent_i] = x_new\n                    pop_f[parent_i] = f_new\n                    pop_sigma[parent_i] = max(parent_sigma * 0.9, 1e-12)\n                    if f_new < pop_f[w]:\n                        pop_sigma[w] = base_sigma_scalar * 0.5\n                elif f_new < pop_f[w]:\n                    pop[w] = x_new\n                    pop_f[w] = f_new\n                    pop_sigma[w] = base_sigma_scalar * 0.5\n\n            # adapt parent sigma on failure (slight tightening)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.97, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.03 and remaining > 0:\n                w = worst_idx()\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[w] = x_new\n                pop_f[w] = f_new\n                pop_sigma[w] = base_sigma_scalar * (0.5 + np.random.rand())\n\n            # dynamic population trimming or expansion (when budget allows, inject a new sample)\n            if remaining > max(2, self.dim // 2) and len(pop) < self.pop_size and np.random.rand() < 0.05:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma_scalar * (0.5 + np.random.rand()))\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.493 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1537605999215167, 0.21337914878593, 0.5763547882444366, 0.8837692958842702, 0.7253898220824673, 0.7784403120303659, 0.2649813615769494, 0.4870854316979588, 0.676449651663646, 0.16647679105126545]}, "task_prompt": ""}
{"id": "e398eb01-2add-41a8-bb07-1690e4c8b736", "fitness": 0.1753867397410645, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy (Cauchy) jumps to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    Key ideas:\n    - Maintain a modest population of candidate points, each with an adaptive step-size (sigma).\n    - Perform directional local searches by sampling normalized directions and stochastic step lengths.\n    - If a directional attempt fails, try smaller backtracking steps and orthogonal refinements.\n    - Occasionally perform heavy-tailed (Cauchy-like) Lévy jumps to escape basins.\n    - Use recombination of the best individuals to exploit promising areas.\n    - Replace the worst periodically and adapt sigmas based on success/failure.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None,\n                 init_sigma=0.25, levy_prob=0.06, rejuvenation_ratio=0.02):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modestly scale with dimension\n            self.pop_size = max(4, min(30, 8 + int(1.5 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.init_sigma = float(init_sigma)\n        self.levy_prob = float(levy_prob)\n        self.rejuvenation_ratio = float(rejuvenation_ratio)\n        self.seed = seed\n        if seed is not None:\n            # use a private RandomState to avoid changing global RNG\n            self.rng = np.random.RandomState(seed)\n        else:\n            self.rng = np.random\n\n    def __call__(self, func):\n        # Extract bounds (support scalar or vector)\n        lb = np.asarray(func.bounds.lb)\n        ub = np.asarray(func.bounds.ub)\n        if lb.size == 1:\n            lb = np.repeat(lb.item(), self.dim)\n        if ub.size == 1:\n            ub = np.repeat(ub.item(), self.dim)\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # Quick sanity\n        if self.budget <= 0:\n            raise ValueError(\"Budget must be positive\")\n\n        # Helper to clip and evaluate while tracking budget and best\n        eval_count = 0\n        f_best = np.inf\n        x_best = None\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def callf(x):\n            nonlocal eval_count, f_best, x_best\n            if eval_count >= self.budget:\n                raise RuntimeError(\"Function evaluation budget exceeded\")\n            x = np.asarray(x).reshape(self.dim)\n            x = clip(x)\n            fx = func(x)\n            eval_count += 1\n            if fx < f_best:\n                f_best = float(fx)\n                x_best = x.copy()\n            return float(fx), x.copy()\n\n        # If budget is very small, do simple random search\n        if self.budget < 4:\n            for _ in range(self.budget):\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return f_best, x_best\n\n        # Initialize population (use as many initial evals as allowed but at least 2)\n        pop_size = min(self.pop_size, max(2, self.budget // 5))\n        pop_x = np.zeros((pop_size, self.dim))\n        pop_f = np.zeros(pop_size)\n        pop_sigma = np.zeros(pop_size)\n        span = ub - lb\n        # init sigmas proportional to problem scale\n        for i in range(pop_size):\n            pop_x[i] = self.rng.uniform(lb, ub)\n            pop_f[i], pop_x[i] = callf(pop_x[i])\n            pop_sigma[i] = max(1e-12, self.init_sigma * np.linalg.norm(span) / np.sqrt(self.dim))\n\n        # If we exhausted budget during initialization\n        if eval_count >= self.budget:\n            return f_best, x_best\n\n        # Precompute some routines\n        def normalized(v):\n            v = np.asarray(v, dtype=float)\n            n = np.linalg.norm(v)\n            return v / (n + 1e-16)\n\n        iters_since_rejuv = 0\n        # Main loop: continue until budget exhausted\n        while eval_count < self.budget:\n            remaining = self.budget - eval_count\n            # Select parent by small tournament (k=3)\n            k = min(3, pop_size)\n            inds = self.rng.choice(pop_size, size=k, replace=False)\n            parent_i = int(inds[np.argmin(pop_f[inds])])\n            parent = pop_x[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # Directional search: sample a random unit direction\n            d = self.rng.randn(self.dim)\n            d = normalized(d)\n\n            # Sample a stochastic step length: log-normal-ish multiplicative noise\n            step_len = sigma * (1.0 + 0.6 * self.rng.randn()) * (1.0 + 0.4 * self.rng.rand())\n            step_len = max(step_len, 1e-12)\n\n            improved = False\n\n            # Primary directional trial\n            if remaining <= 0:\n                break\n            x_try = clip(parent + step_len * d)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n            remaining = self.budget - eval_count\n\n            if f_try < pop_f[parent_i]:\n                # accept improvement\n                pop_x[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max(pop_sigma[parent_i] * 1.12, 1e-12), np.linalg.norm(span) * 1e3)\n                improved = True\n            else:\n                # Backtracking: try smaller step sizes (local refinement)\n                factors = [0.5, 0.25, 0.1]\n                for fac in factors:\n                    if remaining <= 0:\n                        break\n                    s = step_len * fac\n                    x_try = clip(parent + s * d)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    remaining = self.budget - eval_count\n                    if f_try < pop_f[parent_i]:\n                        pop_x[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * (1.05 + 0.02 * self.rng.rand()), 1e-12)\n                        improved = True\n                        break\n\n            # Orthogonal refinement: try a small orthogonal step for diversification\n            if (not improved) and remaining > 0:\n                r = self.rng.randn(self.dim)\n                # Make r orthogonal to d (subtract projection)\n                r = r - np.dot(r, d) * d\n                r = normalized(r)\n                orth_scale = sigma * 0.4 * (0.7 + 0.6 * self.rng.rand())\n                x_try = clip(parent + orth_scale * r)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                remaining = self.budget - eval_count\n                if f_try < pop_f[parent_i]:\n                    pop_x[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.03, 1e-12)\n                    improved = True\n\n            # Occasional Lévy-like heavy-tailed jump to escape basins\n            if (not improved) and (self.rng.rand() < self.levy_prob) and remaining > 0:\n                # Cauchy-like: standard Cauchy per component, scaled by a robust scale\n                c = self.rng.standard_cauchy(size=self.dim)\n                # robust scale: based on sigma and span\n                levy_scale = 4.0 * sigma + 0.05 * np.linalg.norm(span) / np.sqrt(self.dim)\n                # normalize heavy tail to avoid extreme blow-ups: clip extreme values relative to scale\n                c = np.clip(c, -20.0, 20.0)\n                jump = clip(parent + levy_scale * c / (1.0 + np.linalg.norm(c) / 10.0))\n                try:\n                    f_jump, jump = callf(jump)\n                except RuntimeError:\n                    break\n                remaining = self.budget - eval_count\n                # If jump helps, insert it replacing the worst; else maybe shrink parent's sigma\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_i]:\n                    pop_x[worst_i] = jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    improved = True\n                else:\n                    # reduce parent's sigma modestly to encourage exploration refinement\n                    pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.85)\n\n            # Recombination exploitation: mix two best and small noise\n            if remaining > 0:\n                # pick two best\n                order = np.argsort(pop_f)\n                a, b = int(order[0]), int(order[1]) if pop_size > 1 else int(order[0])\n                mix = 0.5 * (pop_x[a] + pop_x[b])\n                noise = self.rng.randn(self.dim) * (0.2 * np.mean([pop_sigma[a], pop_sigma[b]]) + 1e-12)\n                child = clip(mix + noise)\n                try:\n                    f_child, child = callf(child)\n                except RuntimeError:\n                    break\n                remaining = self.budget - eval_count\n                # Replace worst if improved\n                worst_i = int(np.argmax(pop_f))\n                if f_child < pop_f[worst_i]:\n                    pop_x[worst_i] = child\n                    pop_f[worst_i] = f_child\n                    pop_sigma[worst_i] = max(1e-12, 0.6 * np.mean([pop_sigma[a], pop_sigma[b]]))\n\n            # Adapt parent sigma on failure if not already adapted\n            if not improved:\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.88, 1e-12)\n            else:\n                # small success-based exploitation increase\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.08, np.linalg.norm(span) * 1e3)\n\n            # Occasional population rejuvenation: replace the worst with a random sample\n            iters_since_rejuv += 1\n            if iters_since_rejuv >= max(1, int(1.0 / max(1e-9, self.rejuvenation_ratio))):\n                # rejuvenate a few individuals proportionally to population size and remaining budget\n                n_repl = max(1, int(self.rejuvenation_ratio * pop_size))\n                for _ in range(n_repl):\n                    if eval_count >= self.budget:\n                        break\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = self.rng.uniform(lb, ub)\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        break\n                    pop_x[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = max(self.init_sigma * 0.5, 1e-12)\n                iters_since_rejuv = 0\n\n            # If population diversity collapsed (sigmas too small), nudge worsts with random\n            if np.median(pop_sigma) < 1e-8:\n                worst_i = int(np.argmax(pop_f))\n                if eval_count < self.budget:\n                    x_new = self.rng.uniform(lb, ub)\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        break\n                    pop_x[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = self.init_sigma * (0.5 + self.rng.rand())\n\n            # small safe guard: if only one eval left, do random probe\n            if (self.budget - eval_count) == 1:\n                x = self.rng.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n\n        # Return best seen\n        return f_best, x_best", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.175 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11338847257640128, 0.1567364504969514, 0.22986108235202118, 0.1985794527804463, 0.1727858889850331, 0.18692267556046616, 0.2019357203716836, 0.18385388239693345, 0.16703936272123032, 0.14276440916947786]}, "task_prompt": ""}
{"id": "a32c91c9-1623-44a6-ae40-372fa6578458", "fitness": "-inf", "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based continuous optimizer combining randomized directional local searches, orthogonal refinements, and occasional Lévy-like jumps with adaptive per-individual step sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled if None)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # sensible default population scaling: modestly larger than dim but limited by budget\n        if pop_size is None:\n            self.pop_size = max(4, min(2 * self.dim, int(max(4, np.sqrt(self.budget)))))\n        else:\n            self.pop_size = int(pop_size)\n        # state filled during run\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # retrieve bounds if available, else use [-5,5] per specification\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n        # ensure correct shapes\n        if lb.shape != (self.dim,):\n            lb = np.full(self.dim, float(lb.ravel()[0]) if np.size(lb) else -5.0)\n        if ub.shape != (self.dim,):\n            ub = np.full(self.dim, float(ub.ravel()[0]) if np.size(ub) else 5.0)\n\n        # clamp function wrapper that tracks budget and best\n        def callf(x):\n            if self.eval_count >= self.budget:\n                raise RuntimeError(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float).ravel()\n            # ensure full dim\n            if x.size != self.dim:\n                x = np.resize(x, self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)  # call the black box\n            self.eval_count += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # fallback small-budget random search\n        if self.budget <= 2:\n            self.f_opt = np.inf\n            self.x_opt = None\n            while self.eval_count < self.budget:\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # initialize population (x, f, sigma)\n        pop = []\n        # initial sigma scale based on box size\n        global_scale = 0.1 * np.linalg.norm(ub - lb)  # typical step size scale\n        # try to build a population without exceeding budget (keep at least 1 eval left)\n        init_allowed = max(1, min(self.pop_size, self.budget - 1))\n        for _ in range(init_allowed):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            sigma0 = global_scale * (0.5 + self.rng.random())  # per-individual adaptive sigma\n            pop.append({'x': x0, 'f': f0, 'sigma': float(sigma0), 'age': 0})\n        if len(pop) == 0:\n            # budget too small; random search as last resort\n            while self.eval_count < self.budget:\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # helper to get index of worst/best\n        def idx_best():\n            return min(range(len(pop)), key=lambda i: pop[i]['f'])\n        def idx_worst():\n            return max(range(len(pop)), key=lambda i: pop[i]['f'])\n\n        # main loop: use remaining budget for directed searches, orthogonal tries, Levy jumps\n        stagn_count = 0\n        iter_count = 0\n        while self.eval_count < self.budget:\n            iter_count += 1\n            # small tournament selection to pick a parent (size 2 or 3)\n            tsize = min(3, len(pop))\n            candidates = self.rng.choice(len(pop), size=tsize, replace=False)\n            parent_idx = min(candidates, key=lambda i: pop[i]['f'])\n            parent = pop[parent_idx]\n\n            # sample a random search direction and normalize (avoid zero)\n            dirv = self.rng.normal(size=self.dim)\n            nrm = np.linalg.norm(dirv)\n            if nrm == 0.0:\n                dirv = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                dirv = dirv / nrm\n\n            # stochasticized step-length (log-normal tweak)\n            step_scale = parent['sigma'] * np.exp(0.05 * self.rng.normal())\n            # primary directional trial\n            if self.eval_count >= self.budget:\n                break\n            x_trial = parent['x'] + step_scale * dirv\n            x_trial = np.minimum(np.maximum(x_trial, lb), ub)\n            f_trial, x_trial = callf(x_trial)\n            improved = False\n            if f_trial < parent['f']:\n                # accept improvement\n                parent['x'] = x_trial\n                parent['f'] = f_trial\n                parent['sigma'] = min(parent['sigma'] * 1.12, (ub - lb).max())\n                parent['age'] = 0\n                improved = True\n                stagn_count = 0\n            else:\n                # local backtracking / small-step refinement along direction\n                back_attempts = 3\n                factor = 0.5\n                cur_step = step_scale * factor\n                for bt in range(back_attempts):\n                    if self.eval_count >= self.budget:\n                        break\n                    x_bt = parent['x'] + cur_step * dirv\n                    x_bt = np.minimum(np.maximum(x_bt, lb), ub)\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt < parent['f']:\n                        parent['x'] = x_bt\n                        parent['f'] = f_bt\n                        parent['sigma'] = min(parent['sigma'] * 1.08, (ub - lb).max())\n                        parent['age'] = 0\n                        improved = True\n                        stagn_count = 0\n                        break\n                    cur_step *= factor\n\n            # orthogonal perturbation for local diversification\n            if self.eval_count < self.budget:\n                # build a vector orthogonal to dirv component\n                v = self.rng.normal(size=self.dim)\n                # remove projection on dirv\n                v = v - np.dot(v, dirv) * dirv\n                vn = np.linalg.norm(v)\n                if vn == 0:\n                    v = self.rng.normal(size=self.dim)\n                    vn = np.linalg.norm(v)\n                v = v / vn\n                ortho_step = 0.6 * parent['sigma'] * (0.5 + self.rng.random())\n                x_ort = parent['x'] + ortho_step * v\n                x_ort = np.minimum(np.maximum(x_ort, lb), ub)\n                f_ort, x_ort = callf(x_ort)\n                if f_ort < parent['f']:\n                    parent['x'] = x_ort\n                    parent['f'] = f_ort\n                    parent['sigma'] = min(parent['sigma'] * 1.06, (ub - lb).max())\n                    parent['age'] = 0\n                    improved = True\n                    stagn_count = 0\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed)\n            # probability scales inversely with how recently improvements happened\n            levy_prob = 0.015 + 0.03 * min(1.0, stagn_count / 50.0)\n            if self.rng.random() < levy_prob and self.eval_count < self.budget:\n                # Cauchy-like heavy-tailed vector\n                # sample coordinate-wise Cauchy, then normalize direction and scale by a robust heavy-tailed factor\n                v_cauchy = self.rng.standard_cauchy(size=self.dim)\n                # robust scale factor from a single Cauchy sample (heavy-tailed magnitude)\n                mag = np.abs(self.rng.standard_cauchy()) + 0.5\n                # normalize vector to avoid numerical explosions\n                vnorm = np.linalg.norm(v_cauchy)\n                if vnorm == 0 or not np.isfinite(vnorm):\n                    v_cauchy = self.rng.normal(size=self.dim)\n                    vnorm = np.linalg.norm(v_cauchy)\n                dir_jump = v_cauchy / vnorm\n                # jump magnitude based on parent sigma and box size, clipped\n                max_jump = 1.5 * np.linalg.norm(ub - lb)\n                jump_mag = np.clip(parent['sigma'] * mag * (1.0 + 2.0 * self.rng.random()), 0.2 * parent['sigma'], max_jump)\n                x_jump = parent['x'] + jump_mag * dir_jump\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < parent['f']:\n                    parent['x'] = x_jump\n                    parent['f'] = f_jump\n                    parent['sigma'] = min(max(parent['sigma'] * 1.25, 1e-12), (ub - lb).max())\n                    parent['age'] = 0\n                    improved = True\n                    stagn_count = 0\n                else:\n                    # if it's competitive, replace worst; otherwise keep as candidate by possibly replacing worst\n                    worst_idx = idx_worst()\n                    if f_jump < pop[worst_idx]['f']:\n                        pop[worst_idx] = {'x': x_jump, 'f': f_jump, 'sigma': max(parent['sigma'] * 0.8, 1e-12), 'age': 0}\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and self.eval_count < self.budget:\n                # pick two distinct best indices probabilistically favoring low f\n                sorted_idx = sorted(range(len(pop)), key=lambda i: pop[i]['f'])\n                i1 = sorted_idx[0]\n                # second best chosen among top few\n                i2 = sorted_idx[1] if len(sorted_idx) > 1 else sorted_idx[0]\n                x_child = 0.5 * (pop[i1]['x'] + pop[i2]['x'])\n                # add small gaussian perturbation proportional to average sigma\n                avg_sigma = 0.5 * (pop[i1]['sigma'] + pop[i2]['sigma'])\n                x_child = x_child + avg_sigma * 0.2 * self.rng.normal(size=self.dim)\n                x_child = np.minimum(np.maximum(x_child, lb), ub)\n                f_child, x_child = callf(x_child)\n                # replace parent if improved, else possibly replace worst\n                if f_child < parent['f']:\n                    parent['x'] = x_child\n                    parent['f'] = f_child\n                    parent['sigma'] = min(avg_sigma * 1.05, (ub - lb).max())\n                    parent['age'] = 0\n                    improved = True\n                    stagn_count = 0\n                else:\n                    worst_idx = idx_worst()\n                    if f_child < pop[worst_idx]['f']:\n                        pop[worst_idx] = {'x': x_child, 'f': f_child, 'sigma': avg_sigma * 0.9, 'age': 0}\n\n            # adapt parent sigma on failure\n            if not improved:\n                parent['sigma'] = max(parent['sigma'] * 0.92, 1e-12)\n                parent['age'] += 1\n                stagn_count += 1\n            else:\n                # small exploitation: slightly shrink sigma after success to refine\n                parent['sigma'] = max(parent['sigma'] * 0.98, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if (iter_count % 40 == 0) and (self.eval_count < self.budget):\n                worst_idx = idx_worst()\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                # replace if better than worst (or always when worst is stale)\n                if f_new < pop[worst_idx]['f'] or pop[worst_idx]['age'] > 20:\n                    pop[worst_idx] = {'x': x_new, 'f': f_new, 'sigma': global_scale * (0.5 + self.rng.random()), 'age': 0}\n\n            # occasionally trim or expand population to maintain diversity and budget adherence\n            # if budget is tight, shrink population\n            if self.budget - self.eval_count < max(2, self.dim // 2) and len(pop) > 2:\n                # remove the worst to save bookkeeping (no extra evals)\n                pop.pop(idx_worst())\n\n            # safety: ensure population items reference same parent object updated\n            pop[parent_idx] = parent\n\n            # if total evaluations near budget, break\n            if self.eval_count >= self.budget:\n                break\n\n            # small loop termination safety to avoid infinite loops\n            if iter_count > 1000000:\n                break\n\n        return float(self.f_opt), self.x_opt.copy() if self.x_opt is not None else None", "configspace": "", "generation": 0, "feedback": "In the code, line 255, in __call__, the following error occurred:\nIndexError: list assignment index out of range\nOn line: pop[parent_idx] = parent", "error": "In the code, line 255, in __call__, the following error occurred:\nIndexError: list assignment index out of range\nOn line: pop[parent_idx] = parent", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4088df43-a21e-488a-9806-f3f21ad88bc5", "fitness": 0.45700822292834903, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer using randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy jumps with per-individual adaptive step-sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales modestly with dimensionality\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # bounds from func; assume func.bounds.lb/ub exist (scalars or arrays)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # ensure dimensionality matches\n        if lb.size != self.dim or ub.size != self.dim:\n            raise ValueError(\"Bounds size does not match declared dim\")\n\n        remaining = int(self.budget)\n        f_opt = np.inf\n        x_opt = None\n\n        # helper to evaluate while respecting budget and tracking best\n        def callf(x):\n            nonlocal remaining, f_opt, x_opt\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).copy()\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n            return f, x\n\n        # quick fallback: if budget is tiny, do pure random sampling\n        if remaining <= 0:\n            return float(f_opt), (None if x_opt is None else x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # per-individual adaptive sigma with slight randomness\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n\n        # if no population could be created, exhaust budget by random sampling\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return float(f_opt), (None if x_opt is None else x_opt.copy())\n\n        # main loop\n        while remaining > 0:\n            # tournament selection (small tournament)\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction and normalize\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial (stochastic step-length)\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            # keep alpha in a reasonable range to avoid huge steps\n            alpha = np.clip(alpha, -5.0 * np.mean(ub - lb), 5.0 * np.mean(ub - lb))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # remove component along d to make orthogonal\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                mag = 0.6 * sigma  # orthogonal magnitude\n                x_try = np.clip(x_parent + mag * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization to keep heavy-tail but avoid infinities\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # replace worst if better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # prefer replacing parent if improved, else try replacing worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (conservative shrink)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        return float(f_opt), (None if x_opt is None else x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.457 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14890156093058948, 0.17388452828136391, 0.6623665910050875, 0.9278230662908223, 0.2848274842171752, 0.7125308227858529, 0.2541575314068787, 0.41870740503376747, 0.8534842829628114, 0.13339895636914212]}, "task_prompt": ""}
{"id": "e4b9f2da-e2c2-498a-ac3c-479f099d7457", "fitness": 0.40016217640463053, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer that mixes adaptive directional local searches, orthogonal refinements and occasional heavy-tailed Lévy-like jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimensionality\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        # best found values\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds (BBOB style expects func.bounds.lb / ub)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        # ensure full-dim bounds\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # clamp budget sanity\n        remaining = int(max(0, self.budget))\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # if budget is extremely small fallback to pure random probing\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (use as many as budget allows)\n        n_init = min(self.pop_size, max(1, remaining))\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(float(f0))\n            # initialize sigma with a spread around base\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n        # if no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # small tournament selection to pick a parent\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / (nd + 1e-12)\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along the direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # small isotropic noise scaled by typical length\n                noise_scale = 0.05 * np.mean(ub - lb)\n                noise = noise_scale * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure to encourage exploration\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.400 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13392392779300832, 0.15581598010012665, 0.4978991534957703, 0.5446549945113277, 0.6413285511571681, 0.9240476625415117, 0.2875451683708743, 0.39767001127516655, 0.2769540928041846, 0.1417822219971676]}, "task_prompt": ""}
{"id": "e40d9a02-2ab5-4e21-94b4-e0f62c954506", "fitness": 0.5950041951391788, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — maintains a small population with adaptive step-sizes, combines randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed (Lévy/Cauchy) jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults modestly with dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Normalize bounds to full-dim arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # safety: consistent shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to a few random evaluations\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Base sigma scale (scalar) as a fraction of problem range\n        avg_range = float(np.mean(ub - lb))\n        base_sigma = max(1e-12, 0.2 * avg_range)\n\n        # Initialize population (as many as allowed by budget and pop_size)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)  # ensure we don't try to evaluate more than budget\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # per-individual exploration scale with slight randomness\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n        # If no population could be created (very small budget), do pure random search / return best found\n        if len(pop) == 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # small tournament selection to pick parent\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.35 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, max(1e-12, np.mean(ub - lb)))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = sigma * 1.05\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification (remove component along d)\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r -= np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(size=self.dim)\n                denom = np.median(np.abs(step)) + 1e-9\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                mix = 0.6 * pop[best2[0]] + 0.4 * pop[best2[1]]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # replace parent if improved, else possibly replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    else:\n                        # try to inject into population by replacing the worst if it's better\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (slight shrink to focus search)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.05 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                if remaining > 0:\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        break\n                    if f_new < pop_f[worst_i]:\n                        pop[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n\n            # loop continues until budget exhausted\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.595 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1593146142930738, 0.17096455208374217, 0.8817664772120917, 0.9724952213594158, 0.9066925824713764, 0.9319088222395682, 0.29468487889014916, 0.5820103675907398, 0.89780200144648, 0.15240243380515206]}, "task_prompt": ""}
{"id": "7c0e829f-38ef-4714-b306-3eaee4054e95", "fitness": 0.3438456896252326, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based, adaptive step-size search mixing randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to robustly explore/exploit continuous domains.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling; keeps diversity while staying budget-friendly\n            self.pop_size = max(6, min(40, int(4 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n    def __call__(self, func):\n        # try to get bounds from function, otherwise assume [-5, 5]\n        lb_val = -5.0\n        ub_val = 5.0\n        bounds = getattr(func, \"bounds\", None)\n        if bounds is not None:\n            # many wrappers store bounds.lb / bounds.ub as scalars or arrays\n            try:\n                lb_raw = getattr(bounds, \"lb\", None)\n                ub_raw = getattr(bounds, \"ub\", None)\n                if lb_raw is not None and ub_raw is not None:\n                    lb_arr = np.asarray(lb_raw, dtype=float)\n                    ub_arr = np.asarray(ub_raw, dtype=float)\n                    if lb_arr.size == 1:\n                        lb_val = float(lb_arr.item())\n                    else:\n                        lb_val = None\n                    if ub_arr.size == 1:\n                        ub_val = float(ub_arr.item())\n                    else:\n                        ub_val = None\n                    if lb_val is None or ub_val is None:\n                        # use full arrays but ensure dim\n                        lb = np.asarray(lb_arr, dtype=float).reshape(-1)\n                        ub = np.asarray(ub_arr, dtype=float).reshape(-1)\n                        if lb.size != self.dim or ub.size != self.dim:\n                            lb = np.full(self.dim, -5.0)\n                            ub = np.full(self.dim, 5.0)\n                    else:\n                        lb = np.full(self.dim, lb_val)\n                        ub = np.full(self.dim, ub_val)\n                else:\n                    lb = np.full(self.dim, -5.0)\n                    ub = np.full(self.dim, 5.0)\n            except Exception:\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.full(self.dim, lb_val)\n            ub = np.full(self.dim, ub_val)\n\n        # core state\n        remaining = int(self.budget)\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and global best\n        def callf(x):\n            nonlocal remaining\n            # ensure x is numpy array with correct shape\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds pre-evaluation\n            x = np.minimum(np.maximum(x, lb), ub)\n            if remaining <= 0:\n                # cannot call the function anymore; return global best\n                return self.f_opt, self.x_opt\n            # evaluate once and decrement budget\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # base sigma (scalar) initial scale relative to domain\n        domain_scale = np.maximum(ub - lb, 1e-9).mean()\n        base_sigma = max(1e-6, 0.08 * domain_scale)\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = self.pop_size\n        for _ in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            # if callf couldn't evaluate because budget exhausted, break\n            if remaining < 0:\n                break\n            pop.append(x0)\n            pop_f.append(float(f0))\n            # individual sigma randomized around base_sigma\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * np.random.rand()))\n\n        # if no population could be created due to tiny budget, return best found (maybe None)\n        if len(pop) == 0:\n            return self.f_opt, self.x_opt\n\n        # algorithm hyperparameters\n        levy_prob = 0.08\n        rejuvenation_prob = 0.02\n        beta_recomb = 0.7\n        max_sigma = max(1.0 * domain_scale, base_sigma * 100.0)\n        min_sigma = 1e-12\n\n        # main loop: directional attempts, orthogonal tries, Levy jumps, recombination, rejuvenation\n        while remaining > 0:\n            n_pop = len(pop)\n\n            # tournament selection for parent\n            k = min(3, n_pop)\n            inds = np.random.choice(n_pop, k, replace=False)\n            vals = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(vals))]\n            sigma = float(pop_sigma[parent_i])\n            x_parent = pop[parent_i].copy()\n\n            # sample random direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / (nd + 1e-12)\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            improved = False\n            if f_try is not None and f_try < pop_f[parent_i]:\n                # accept improvement and increase sigma slightly\n                pop[parent_i] = x_try\n                pop_f[parent_i] = float(f_try)\n                pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.12)\n                improved = True\n                # continue main loop\n                continue\n\n            # backtracking / small-step refinement along direction (few tries)\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try is not None and f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = float(f_try)\n                    pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.06)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                r = r - np.dot(r, d) * d  # make orthogonal to d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try is not None and f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = float(f_try)\n                        pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.10)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and np.random.rand() < levy_prob:\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.median(np.abs(step)) + 1e-9\n                if denom == 0:\n                    denom = np.mean(np.abs(step)) + 1e-9\n                step = step / denom\n                # scale of jump proportional to sigma but can be large\n                scale_factor = sigma * (1.5 + 3.0 * np.random.rand())\n                x_try = np.clip(x_parent + step * scale_factor, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try is not None:\n                    worst_i = int(np.argmax(pop_f))\n                    # accept if better than worst\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = float(f_try)\n                        pop_sigma[worst_i] = max(min_sigma, sigma * 0.5)\n                    else:\n                        # with small chance keep as new candidate replacing worst (diversity)\n                        if np.random.rand() < 0.03:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = float(f_try)\n                            pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = beta_recomb\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = np.random.randn(self.dim) * (0.06 * sigma + 1e-12)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try is not None:\n                    worst_i = int(np.argmax(pop_f))\n                    # better than worst -> inject\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = float(f_try)\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                    # or replace parent if it improves\n                    elif f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = float(f_try)\n                        pop_sigma[parent_i] = max(min_sigma, pop_sigma[parent_i] * 0.9)\n\n            # adapt parent sigma on failure (shrink if no improvement this iteration)\n            if not improved:\n                new_sigma = max(min_sigma, sigma * 0.95)\n                pop_sigma[parent_i] = new_sigma\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < rejuvenation_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new is not None:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = float(f_new)\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # done (budget exhausted or loop naturally ended)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.344 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11001792963825585, 0.17226301927284615, 0.573280847003663, 0.8925029233172821, 0.24045457871166742, 0.6518908347093051, 0.2667485902603538, 0.20817309802320505, 0.19204332621606257, 0.13108174909968506]}, "task_prompt": ""}
{"id": "1d02e93c-b452-4230-b79b-c2475ebc6e7a", "fitness": 0.5608970208785045, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a compact population-based optimizer combining directional local searches, orthogonal refinements, adaptive step-sizes and occasional heavy-tailed Lévy jumps to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    Parameters\n    ----------\n    budget : int\n        Maximum number of function evaluations.\n    dim : int\n        Problem dimensionality.\n    pop_size : int, optional\n        Population size. If None it will be set based on dim.\n    seed : int, optional\n        RNG seed for reproducibility.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # best seen (updated during run)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Read and prepare bounds (support scalar or array)\n        lb_arr = np.asarray(func.bounds.lb, dtype=float)\n        ub_arr = np.asarray(func.bounds.ub, dtype=float)\n        if lb_arr.size == 1:\n            lb = np.full(self.dim, float(lb_arr.item()))\n        else:\n            lb = lb_arr.flatten()\n        if ub_arr.size == 1:\n            ub = np.full(self.dim, float(ub_arr.item()))\n        else:\n            ub = ub_arr.flatten()\n        # Safety: ensure shapes\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to call func while tracking remaining budget and best seen\n        def callf(x):\n            nonlocal remaining\n            x = np.asarray(x, dtype=float).ravel()\n            # clip input to bounds\n            x_clip = np.minimum(np.maximum(x, lb), ub)\n            if remaining <= 0:\n                # no remaining budget: return a dummy (inf) without calling func\n                return np.inf, x_clip\n            try:\n                f = float(func(x_clip))\n            except Exception as e:\n                # In case func expects shape (dim,), ensure that form\n                f = float(func(x_clip.copy()))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x_clip.copy()\n            return f, x_clip\n\n        # If budget extremely small, fallback to random sampling\n        if remaining <= 0:\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial global scale\n        pop = np.zeros((n_init, self.dim), dtype=float)\n        pop_f = np.full(n_init, np.inf, dtype=float)\n        pop_sigma = np.full(n_init, base_sigma, dtype=float)\n\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop[i, :] = x0\n            pop_f[i] = f0 if np.isfinite(f0) else np.inf\n            pop_sigma[i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # main loop: directional local searches, orthogonal tries, Lévy jumps, recombination, rejuvenation\n        # Hyper-parameters (kept small to be broadly useful)\n        max_backtracks = 3\n        frac_steps = [0.5, 0.25, 0.125]\n        levy_prob = 0.06\n        recomb_beta = 0.6\n        rejuvenation_prob = 0.02\n        tournament_k = max(2, min(4, len(pop)))\n        max_iters = 10**9  # loop until remaining exhausted\n\n        it = 0\n        while remaining > 0 and it < max_iters:\n            it += 1\n\n            # choose parent via small tournament (prefer better individuals)\n            k = min(tournament_k, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = pop_f[inds]\n            parent_idx = inds[int(np.argmin(values))]\n            x_parent = pop[parent_idx].copy()\n            sigma = float(pop_sigma[parent_idx])\n\n            # occasional Lévy jump to escape local basins\n            if (np.random.rand() < levy_prob) and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                # normalize heavy-tail vector to avoid pathological huge leaps while keep tail\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                if np.isfinite(f_try):\n                    # if it's good, replace the worst, else maybe keep as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + 0.8 * np.random.rand())\n                # after a jump attempt, continue to next iteration\n                continue\n\n            # sample a random direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                continue\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            if not np.isfinite(alpha) or abs(alpha) < 1e-12:\n                alpha = sigma * (0.1 + 0.9 * np.random.rand())\n\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            if np.isfinite(f_try) and f_try < pop_f[parent_idx]:\n                # success: accept and slightly increase sigma\n                pop[parent_idx] = x_try\n                pop_f[parent_idx] = f_try\n                pop_sigma[parent_idx] = min(sigma * 1.2, np.max(ub - lb))\n                continue  # go to next iteration after success\n\n            # local backtracking / small-step refinement along direction\n            improved = False\n            for frac in frac_steps[:max_backtracks]:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if np.isfinite(f_try) and f_try < pop_f[parent_idx]:\n                    pop[parent_idx] = x_try\n                    pop_f[parent_idx] = f_try\n                    pop_sigma[parent_idx] = min(sigma * (1.05 + 0.2 * np.random.rand()), np.max(ub - lb))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d  # make orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                ortho_step = 0.6 * sigma * (0.5 + np.random.rand())\n                x_try = np.clip(x_parent + ortho_step * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if np.isfinite(f_try) and f_try < pop_f[parent_idx]:\n                    pop[parent_idx] = x_try\n                    pop_f[parent_idx] = f_try\n                    pop_sigma[parent_idx] = min(sigma * 1.15, np.max(ub - lb))\n                    continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                # pick two best (lowest fitness)\n                best_inds = np.argsort(pop_f)[:2]\n                a, b = best_inds[0], best_inds[1]\n                mix = recomb_beta * pop[a] + (1.0 - recomb_beta) * pop[b]\n                noise = (0.02 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if np.isfinite(f_try):\n                    if f_try < pop_f[parent_idx]:\n                        pop[parent_idx] = x_try\n                        pop_f[parent_idx] = f_try\n                        pop_sigma[parent_idx] = base_sigma * (0.6 + 0.8 * np.random.rand())\n                    else:\n                        # try to inject into population by replacing the worst if it's better\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (shrink slightly to focus search)\n            pop_sigma[parent_idx] = max(1e-12, pop_sigma[parent_idx] * (0.92 + 0.08 * np.random.rand()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < rejuvenation_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if np.isfinite(f_new):\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # enlarge population if budget remains and we had room earlier\n            if len(pop) < self.pop_size and remaining > 0:\n                # add a new random individual\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if np.isfinite(f_new):\n                    pop = np.vstack([pop, x_new.reshape(1, -1)])\n                    pop_f = np.concatenate([pop_f, np.array([f_new])])\n                    pop_sigma = np.concatenate([pop_sigma, np.array([base_sigma * (0.6 + 0.8 * np.random.rand())])])\n\n        # finished budget or other stopping condition\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.561 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13199018165841347, 0.17194840321461013, 0.8271826115205915, 0.9380944953664245, 0.8910780222346464, 0.8981880204030923, 0.27697677525845366, 0.5112522178861443, 0.7976843399461931, 0.16457514129647577]}, "task_prompt": ""}
{"id": "cb07978f-b2d0-4059-a884-82358a537234", "fitness": 0.5492271607006558, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a small adaptive population that alternates normalized directional local searches, orthogonal refinements, recombination, occasional Lévy-like heavy-tailed jumps and population rejuvenation to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - seed: optional RNG seed\n    Brief: maintains a small population of candidate points with individual adaptive step sizes (sigma).\n    It performs directional local searches (random normalized directions), orthogonal small refinements,\n    recombination of top candidates, occasional Lévy-like jumps to escape basins, and occasional random rejuvenation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # prepare bounds as full-dim arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # bookkeeping\n        remaining = int(self.budget)\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise StopIteration()\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # initial population\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))  # initial scale relative to bounds\n        # try to use about one quarter of the budget for initialization but at least 1 individual\n        n_init = min(self.pop_size, max(1, self.budget // 8))\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        try:\n            for i in range(n_init):\n                x0 = np.random.uniform(lb, ub)\n                f0, x0 = callf(x0)\n                pop.append(x0)\n                pop_f.append(f0)\n                # initialize sigma per individual with some jitter\n                pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n        except StopIteration:\n            # budget exhausted during initialization: return best found so far\n            return self.f_opt, self.x_opt\n\n        pop = np.array(pop).reshape(-1, self.dim)\n        pop_f = np.array(pop_f, dtype=float)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n\n        # fallback: if for some reason pop is empty (very small budget), do pure random search\n        if pop.shape[0] == 0:\n            try:\n                while remaining > 0:\n                    x = np.random.uniform(lb, ub)\n                    callf(x)\n            except StopIteration:\n                pass\n            return self.f_opt, self.x_opt\n\n        # control parameters\n        min_sigma = 1e-12\n        max_sigma = np.mean(ub - lb)\n        p_levy = 0.06           # prob of trying a Levy jump on failure\n        p_recomb = 0.25         # prob of performing recombination\n        p_rejuv = 0.02          # prob to rejuvenate worst\n        tournament_k = max(2, min(self.pop_size, int(1 + np.round(0.2 * self.pop_size))))\n\n        # main loop\n        try:\n            while remaining > 0:\n                # pick a parent via small tournament\n                inds = np.random.choice(pop.shape[0], tournament_k, replace=False)\n                parent_i = inds[np.argmin(pop_f[inds])]\n                x_parent = pop[parent_i].copy()\n                f_parent = pop_f[parent_i]\n                sigma = float(pop_sigma[parent_i])\n                improved = False\n\n                # sample a random search direction (normalized)\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    d = np.random.randn(self.dim)\n                    nd = np.linalg.norm(d) + 1e-12\n                d = d / nd\n\n                # primary directional trial with stochasticized step-length\n                alpha = sigma * (1.0 + 0.5 * np.random.randn())\n                # keep alpha reasonably bounded\n                alpha = np.clip(alpha, -5 * max_sigma, 5 * max_sigma)\n                x_try = np.clip(x_parent + alpha * d, lb, ub)\n                f_try, x_try = callf(x_try)\n\n                if f_try < f_parent:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.2, max_sigma)\n                    continue  # go to next iteration (exploitation along good direction)\n\n                # local backtracking / small-step refinement along direction (few tries)\n                back_alpha = alpha\n                for bt in range(3):\n                    back_alpha *= 0.5\n                    x_try = np.clip(x_parent + back_alpha * d, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < f_parent:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(max_sigma, sigma * (1.05 + 0.05 * bt))\n                        improved = True\n                        break\n                if improved:\n                    continue\n\n                # try an orthogonal perturbation for local diversification\n                r = np.random.randn(self.dim)\n                # make r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    step_scale = sigma * (0.4 + 0.4 * np.random.rand())\n                    x_try = np.clip(x_parent + step_scale * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < f_parent:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(max_sigma, sigma * 1.1)\n                        continue\n\n                # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n                if np.random.rand() < p_levy:\n                    # Cauchy-like heavy-tailed vector\n                    step = np.random.standard_cauchy(size=self.dim)\n                    # robust normalization: divide by high percentile to avoid catastrophic scale\n                    denom = np.percentile(np.abs(step), 90) + 1e-12\n                    step = step / denom\n                    # scale vector with a larger multiplier to attempt escape\n                    scale_vec = sigma * (4.0 + 6.0 * np.random.rand())  # relatively large jumps\n                    x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(min_sigma, sigma * 0.5)\n                    # also if it improves parent, accept it\n                    if f_try < f_parent:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(min_sigma, sigma * 0.7)\n                    continue\n\n                # recombination exploitation: mix two best and small noise\n                if (pop.shape[0] >= 2) and (np.random.rand() < p_recomb):\n                    best2 = np.argsort(pop_f)[:2]\n                    a, b = best2[0], best2[1]\n                    beta = 0.5 + 0.3 * (np.random.rand() - 0.5)  # biased mix around 0.5\n                    mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                    # small Gaussian jitter proportional to average sigma\n                    avg_sigma = float(np.mean(pop_sigma))\n                    jitter = 0.1 * avg_sigma * np.random.randn(self.dim)\n                    x_try = np.clip(mix + jitter, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    # replace worst if it helps, else maybe replace parent\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                    elif f_try < f_parent:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(max_sigma, sigma * 1.05)\n                    continue\n\n                # if we reached here no improvement in this iteration: adapt parent sigma down slightly\n                pop_sigma[parent_i] = max(min_sigma, pop_sigma[parent_i] * 0.85)\n\n                # occasional population rejuvenation by replacing the worst with a random sample\n                if np.random.rand() < p_rejuv and remaining > 0:\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        except StopIteration:\n            # budget exhausted - just return current best\n            pass\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.549 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.166414350916543, 0.17445797199305046, 0.8571067032592729, 0.9659130264226348, 0.5451508050087019, 0.9299417425060724, 0.2677767325370395, 0.5333473378905396, 0.9078219159455503, 0.1443410205271518]}, "task_prompt": ""}
{"id": "b1101fc3-e9ad-4c8e-b128-23c61f832235", "fitness": 0.5936103373899648, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size points combining randomized directional local searches, orthogonal refinements, recombination and occasional Lévy-like jumps to balance intensive local exploitation and heavy-tailed exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by default)\n    - seed: optional RNG seed\n    - init_sigma: initial relative step fraction (fraction of problem diagonal norm)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None, init_sigma=0.18):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        self.seed = seed\n        self.init_sigma = float(init_sigma)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # bounds as numpy arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # safeguard dims\n        assert lb.shape == (self.dim,) and ub.shape == (self.dim,), \"bounds must match dimension\"\n\n        # best-so-far\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        remaining = int(self.budget)\n\n        # helper evaluator that tracks budget and updates global best\n        def callf(x):\n            nonlocal remaining, lb, ub\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip, evaluate, decrement\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # immediate return if no budget\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # population sizing heuristic\n        if self.pop_size is None:\n            pop_size = int(min(40, max(4, 4 + 2 * self.dim)))\n        else:\n            pop_size = int(max(2, self.pop_size))\n\n        # initialize population (limit by remaining budget)\n        n_init = min(pop_size, remaining)\n        pop = []\n        pop_f = []\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n        pop = np.array(pop) if len(pop) > 0 else np.zeros((0, self.dim))\n        pop_f = np.array(pop_f) if len(pop_f) > 0 else np.array([])\n\n        # if no initial members created (very small budget), random sample until budget exhausted\n        if pop.shape[0] == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # per-individual adaptive sigma (as fraction of problem diagonal norm)\n        diag_norm = np.linalg.norm(ub - lb)\n        pop_sigma = np.full(pop.shape[0], self.init_sigma)\n\n        # algorithmic probabilities and hyperparameters\n        p_levy = 0.07\n        p_recombine = 0.18\n        p_rejuvenate = 0.02\n        tournament_k = min(3, pop.shape[0])\n        success_inc = 1.15\n        failure_dec = 0.85\n        max_attempts = 6\n\n        # main loop\n        while remaining > 0:\n            # pick a parent via small tournament (choose best among k random)\n            if pop.shape[0] == 1:\n                parent_i = 0\n            else:\n                inds = np.random.choice(pop.shape[0], tournament_k, replace=False)\n                parent_i = int(inds[np.argmin(pop_f[inds])])\n\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n            # random search direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                # degenerate direction -> random restart\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with noisy step-length\n            alpha = sigma * diag_norm * (1.0 + 0.15 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n\n            improved = False\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    # accept into population (replace parent)\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(2.0, sigma * success_inc)\n                    improved = True\n                else:\n                    # local backtracking / small-step refinement along same direction\n                    for frac in (0.5, 0.25, -0.5, -0.25):\n                        if remaining <= 0:\n                            break\n                        x_bt = np.clip(x_parent + alpha * frac * d, lb, ub)\n                        f_bt, x_bt = callf(x_bt)\n                        if f_bt < pop_f[parent_i]:\n                            pop[parent_i] = x_bt\n                            pop_f[parent_i] = f_bt\n                            pop_sigma[parent_i] = min(2.0, sigma * (1.0 + 0.08))\n                            improved = True\n                            break\n            # try an orthogonal perturbation for local diversification if no improvement\n            if (not improved) and (remaining > 0):\n                r = np.random.randn(self.dim)\n                # make r orthogonal to d\n                proj = np.dot(r, d) * d\n                r_orth = r - proj\n                nr = np.linalg.norm(r_orth)\n                if nr > 1e-12:\n                    r_orth = r_orth / nr\n                    ortho_step = 0.6 * alpha\n                    x_ort = np.clip(x_parent + ortho_step * r_orth, lb, ub)\n                    f_ort, x_ort = callf(x_ort)\n                    if f_ort < pop_f[parent_i]:\n                        pop[parent_i] = x_ort\n                        pop_f[parent_i] = f_ort\n                        pop_sigma[parent_i] = min(2.0, sigma * (1.0 + 0.10))\n                        improved = True\n\n            # occasional Lévy-like jump to escape local basins\n            if (not improved) and (remaining > 0) and (np.random.rand() < p_levy):\n                # robust Cauchy-like heavy-tail vector\n                cauchy = np.random.standard_cauchy(self.dim)\n                # reduce extremes by normalizing by a robust scale (median absolute)\n                mad = np.median(np.abs(cauchy)) + 1e-9\n                scale_vec = cauchy / mad\n                # finite clipping to avoid overflow\n                scale_vec = np.clip(scale_vec, -1e6, 1e6)\n                step_scale = sigma * (1.0 + abs(np.random.standard_cauchy()))\n                step = step_scale * (diag_norm / np.sqrt(self.dim))  # typical step magnitude\n                x_jump = np.clip(x_parent + step * scale_vec / (np.linalg.norm(scale_vec) + 1e-12), lb, ub)\n                if remaining > 0:\n                    f_jump, x_jump = callf(x_jump)\n                    # replace worst if better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = sigma * 0.9\n                # continue main loop\n\n            # recombination exploitation: mix two best with noise\n            if remaining > 0 and (np.random.rand() < p_recombine) and pop.shape[0] >= 2:\n                # pick two distinct parents biased towards good ones\n                sorted_idx = np.argsort(pop_f)\n                a, b = sorted_idx[0], sorted_idx[1] if pop.shape[0] > 1 else sorted_idx[0]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = np.random.randn(self.dim) * (0.03 * diag_norm * np.mean(pop_sigma))\n                x_mix = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    f_mix, x_mix = callf(x_mix)\n                    # replace worst if improved, else possibly replace worst with a random new candidate sometimes\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = np.mean(pop_sigma) * 0.9\n\n            # on failure, try to inject small novel candidates by mutating worst\n            if (not improved) and (remaining > 0):\n                # small chance to try replacing worst with random uniform (rejuvenation)\n                if np.random.rand() < p_rejuvenate:\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    if f_new < pop_f[worst_i]:\n                        pop[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = self.init_sigma\n                else:\n                    # adapt parent sigma on failure (shrink)\n                    pop_sigma[parent_i] = max(1e-6, sigma * failure_dec)\n\n            # ensure population arrays stay consistent sizes if we ever try to expand/shrink\n            if pop.shape[0] != pop_f.shape[0]:\n                # repair (should not generally happen)\n                min_len = min(pop.shape[0], pop_f.shape[0])\n                pop = pop[:min_len].copy()\n                pop_f = pop_f[:min_len].copy()\n                pop_sigma = pop_sigma[:min_len].copy()\n\n            # safety break if no remaining evaluations\n            if remaining <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.594 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14240658276043205, 0.1715000413446015, 0.8864902036667419, 0.9497120474604186, 0.9168071504274902, 0.9322935640258283, 0.23457768501763077, 0.6079300400479695, 0.9229021004109523, 0.17148395873758304]}, "task_prompt": ""}
{"id": "f588a78d-3d52-4d94-8ee1-fce2b9a65d3e", "fitness": 0.2983977504206338, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — combines per-individual adaptive step-sizes, directional local searches, orthogonal refinement and occasional heavy-tailed (Lévy/Cauchy) jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (adaptive default based on dim & budget)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size  # may be adjusted later depending on budget\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # Results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # get bounds from func if available, else assume [-5,5]\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n\n        # ensure full-dim arrays\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # helper: clip and evaluate while tracking budget and best\n        remaining = int(self.budget)\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                # no budget left; return a sentinel (shouldn't normally happen)\n                return np.inf, x.copy()\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            nonlocal_best_update = False\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n                nonlocal_best_update = True\n            return float(f), x.copy()\n\n        # If budget is extremely small, fallback to simple random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n        if remaining < 5:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # choose/populate population size respecting budget\n        if self.pop_size is None:\n            # base pop grows mildly with dim but kept small relative to budget\n            default_pop = max(4, min(16, 4 + self.dim // 2))\n            # ensure enough budget to evaluate initial population and some iterations\n            max_by_budget = max(2, min(default_pop, max(2, (remaining // 5))))\n            pop_size = max(2, max_by_budget)\n        else:\n            pop_size = int(self.pop_size)\n            pop_size = max(2, min(pop_size, max(2, remaining // 2)))\n\n        pop_size = min(pop_size, remaining)  # cannot create more individuals than budget\n        if pop_size < 2:\n            # fallback random if not enough budget for a population\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # initial population uniformly sampled\n        pop = np.array([np.random.uniform(lb, ub) for _ in range(pop_size)])\n        pop_f = np.zeros(pop_size, dtype=float)\n        for i in range(pop_size):\n            if remaining <= 0:\n                pop_f[i] = np.inf\n                continue\n            pop_f[i], pop[i] = callf(pop[i])\n\n        # base sigma scale using bound width; scalar per individual\n        bound_width = ub - lb\n        # typical per-dim magnitude\n        base_scale = 0.25 * np.linalg.norm(bound_width) / max(1.0, np.sqrt(self.dim))\n        base_sigma = max(base_scale, 1e-8)\n        pop_sigma = np.array([base_sigma * (0.8 + 0.4 * np.random.rand()) for _ in range(pop_size)])\n\n        # Main loop: directional local searches + orthogonal + Levy jumps + recombination + rejuvenation\n        while remaining > 0:\n            # some useful indices\n            worst_i = int(np.argmax(pop_f))\n            best_i = int(np.argmin(pop_f))\n            # two best for recombination\n            best2 = np.argsort(pop_f)[:2] if pop_size >= 2 else [best_i]\n\n            # parent selection: small tournament\n            k = min(3, pop_size)\n            candidates = np.random.choice(pop_size, size=k, replace=False)\n            parent_i = int(candidates[np.argmin(pop_f[candidates])])\n            parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            d_norm = np.linalg.norm(d)\n            if d_norm < 1e-12:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / d_norm\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = parent + alpha * d\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max(sigma * (1.1 + 0.05 * np.random.randn()), 1e-12), np.linalg.norm(bound_width))\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                back_alpha = alpha\n                accepted = False\n                for bt in range(3):\n                    if remaining <= 0:\n                        break\n                    back_alpha *= 0.5\n                    x_try = parent + back_alpha * d\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        accepted = True\n                        break\n                if not accepted:\n                    # try an orthogonal perturbation for local diversification\n                    if remaining > 0:\n                        v = np.random.randn(self.dim)\n                        # subtract projection onto d to get orthogonal component\n                        v = v - (v.dot(d)) * d\n                        vn = np.linalg.norm(v)\n                        if vn > 1e-12:\n                            v = v / vn\n                            ortho_step = 0.7 * sigma * (0.8 + 0.4 * np.random.rand())\n                            x_try = parent + ortho_step * v\n                            f_try, x_try = callf(x_try)\n                            if f_try < pop_f[parent_i]:\n                                pop[parent_i] = x_try\n                                pop_f[parent_i] = f_try\n                                pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n\n                    # adapt parent sigma downwards on failure\n                    pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional Lévy-like (Cauchy) jump to escape local basins\n            if remaining > 0 and (np.random.rand() < 0.07):\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(self.dim)\n                # robust scale: normalize by median abs and scale by sigma\n                mad = np.median(np.abs(step)) if np.median(np.abs(step)) > 1e-12 else 1.0\n                step = (step / mad) * (3.0 * pop_sigma[parent_i])\n                x_jump = pop[parent_i] + step\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < pop_f[worst_i]:\n                    # replace the worst with a good jump\n                    pop[worst_i] = x_jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(1e-12, pop_sigma[parent_i] * 0.6)\n                else:\n                    # sometimes accept jump as exploratory individual replacing worst if at least not worse\n                    if f_jump < pop_f[parent_i]:\n                        pop[parent_i] = x_jump\n                        pop_f[parent_i] = f_jump\n                        pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.7)\n\n            # recombination exploitation: mix two best and add small noise\n            if remaining > 0 and pop_size >= 2 and (np.random.rand() < 0.15):\n                a, b = int(best2[0]), int(best2[1])\n                x_a, x_b = pop[a], pop[b]\n                mix = 0.5 * (x_a + x_b)\n                noise = 0.01 * bound_width * np.random.randn(self.dim)\n                x_child = mix + noise\n                f_child, x_child = callf(x_child)\n                if f_child < pop_f[parent_i]:\n                    pop[parent_i] = x_child\n                    pop_f[parent_i] = f_child\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.05, 1e-12)\n                elif f_child < pop_f[worst_i]:\n                    pop[worst_i] = x_child\n                    pop_f[worst_i] = f_child\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (np.random.rand() < 0.03):\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # safety: if population degenerates (e.g., some inf), refill tiny randoms\n            if remaining > 0 and np.isinf(pop_f).any():\n                inf_idx = np.where(np.isinf(pop_f))[0]\n                for ii in inf_idx:\n                    if remaining <= 0:\n                        break\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop[ii] = x_new\n                    pop_f[ii] = f_new\n                    pop_sigma[ii] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # dynamic resizing: if remaining very low, shrink population to save evaluations\n            if remaining < max(3, self.dim // 2) and pop.shape[0] > 2:\n                keep = max(2, remaining)  # keep a couple best ones\n                ids = np.argsort(pop_f)[:keep]\n                pop = pop[ids].copy()\n                pop_f = pop_f[ids].copy()\n                pop_sigma = pop_sigma[ids].copy()\n                pop_size = pop.shape[0]\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.298 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10198343579121294, 0.15958399459941375, 0.39962576366164193, 0.6169102948425983, 0.23388667506253713, 0.4739379047886869, 0.23833221184053865, 0.36366584186300444, 0.25986586975818515, 0.13618551199851947]}, "task_prompt": ""}
{"id": "b715a44a-a121-4e04-b98b-888dafe354c6", "fitness": 0.4662144353818408, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a compact population-based optimizer combining adaptive directional local searches, orthogonal refinements, recombination, and occasional heavy-tailed (Cauchy) jumps to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (adaptive default)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimensionality but stays bounded\n            self.pop_size = max(4, min(20, int(4 + dim // 2)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # final best\n        self.f_opt = np.inf\n        self.x_opt = None\n        # keep a minimum sigma floor to avoid numerical stagnation\n        self.sigma_min = 1e-8\n\n    def __call__(self, func):\n        # determine bounds (try func.bounds, fallback to [-5,5])\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.ravel()[:self.dim]\n        ub = ub.ravel()[:self.dim]\n\n        # evaluation wrapper that clips, counts budget and updates global best\n        self.evals = 0\n\n        def callf(x):\n            x = np.asarray(x, dtype=float).ravel()\n            if x.size != self.dim:\n                x = x[:self.dim]\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            if self.evals >= self.budget:\n                # no budget left; return a large value and don't update\n                return np.inf, x\n            f = float(func(x))\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # fallback to pure random sampling if budget tiny\n        if self.budget <= 2:\n            # extremely small budget -> random sampling\n            for _ in range(self.budget):\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # initialize population (use as many as budget allows)\n        n_init = min(self.pop_size, max(1, self.budget // 10))  # ensure not to consume entire budget\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        init_sigma_scale = 0.2 * (ub - lb).mean()  # initial typical step\n        for i in range(n_init):\n            if self.evals >= self.budget:\n                break\n            x = lb + self.rng.rand(self.dim) * (ub - lb)\n            f, x = callf(x)\n            pop.append(x)\n            pop_f.append(f)\n            # initialize sigma per individual with some diversity\n            pop_sigma.append(max(self.sigma_min, init_sigma_scale * (0.5 + self.rng.rand())))\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # if no population created (very small budget), do random search with remaining\n        if pop.shape[0] == 0:\n            while self.evals < self.budget:\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        iter_count = 0\n        while self.evals < self.budget:\n            iter_count += 1\n            remaining = self.budget - self.evals\n\n            n = pop.shape[0]\n\n            # small tournament selection (k=3 or less)\n            k = min(3, n)\n            inds = self.rng.choice(n, size=k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            parent_x = pop[parent_i].copy()\n            parent_f = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-16:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            # stochasticized step-length (log-normal-ish, allowing occasional larger steps)\n            step_base = sigma * (1.0 + 0.5 * self.rng.randn())\n            # stabilize\n            step_base = max(step_base, self.sigma_min)\n            # primary directional trial\n            x_try = parent_x + d * step_base\n            f_try, x_try = callf(x_try)\n            if f_try < parent_f:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = sigma * 1.12\n            else:\n                # backtracking: try smaller steps along same direction (few tries)\n                improved = False\n                bt_step = step_base\n                for bt in range(3):\n                    if self.evals >= self.budget:\n                        break\n                    bt_step *= 0.5\n                    bt_x = parent_x + d * bt_step\n                    bt_f, bt_x = callf(bt_x)\n                    if bt_f < parent_f:\n                        pop[parent_i] = bt_x\n                        pop_f[parent_i] = bt_f\n                        pop_sigma[parent_i] = max(self.sigma_min, sigma * (1.05 + 0.02 * self.rng.rand()))\n                        improved = True\n                        break\n                if not improved:\n                    # reduce sigma gently on failure\n                    pop_sigma[parent_i] = max(self.sigma_min, sigma * 0.88)\n\n            # orthogonal perturbation for local diversification\n            if self.evals < self.budget:\n                # create vector orthogonal to d\n                v = self.rng.randn(self.dim)\n                v = v - np.dot(v, d) * d\n                nv = np.linalg.norm(v)\n                if nv > 1e-16:\n                    v = v / nv\n                    ortho_step = pop_sigma[parent_i] * 0.6\n                    x_o = pop[parent_i] + v * ortho_step\n                    f_o, x_o = callf(x_o)\n                    if f_o < pop_f[parent_i]:\n                        pop[parent_i] = x_o\n                        pop_f[parent_i] = f_o\n                        pop_sigma[parent_i] = max(self.sigma_min, pop_sigma[parent_i] * 1.08)\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if (self.rng.rand() < 0.06) and (self.evals < self.budget):\n                # Cauchy-like heavy-tailed vector via tan(pi*(u-0.5))\n                u = self.rng.rand(self.dim)\n                c = np.tan(np.pi * (u - 0.5))\n                # robust scaling: scale by median abs to avoid all-inf scale\n                med = np.median(np.abs(c)) + 1e-12\n                c = c / med\n                # combine with current best direction to keep some guidance\n                guidance = (self.x_opt - parent_x) if (self.x_opt is not None) else (ub - lb) * 0.1\n                # normalize guidance\n                gnorm = np.linalg.norm(guidance)\n                if gnorm > 0:\n                    guidance = guidance / gnorm\n                jump_scale = max(1.0, 5.0 * pop_sigma.mean())\n                jump_vec = 0.5 * guidance + 0.5 * c\n                # normalize jump_vec and scale\n                jnorm = np.linalg.norm(jump_vec)\n                if jnorm > 1e-16:\n                    jump_vec = jump_vec / jnorm\n                jump = parent_x + jump_vec * jump_scale * (1.0 + 2.0 * self.rng.rand())\n                jf, jump = callf(jump)\n                if jf < pop_f.max():\n                    # replace the worst if this is better\n                    worst = np.argmax(pop_f)\n                    pop[worst] = jump\n                    pop_f[worst] = jf\n                    pop_sigma[worst] = max(self.sigma_min, pop_sigma.mean() * 1.2)\n\n            # recombination exploitation: mix two best and small noise\n            if self.evals < self.budget and n >= 2:\n                best_two = np.argsort(pop_f)[:2]\n                x1 = pop[best_two[0]]\n                x2 = pop[best_two[1]]\n                mix = 0.5 * (x1 + x2)\n                mix += self.rng.randn(self.dim) * (0.2 * pop_sigma[best_two].min())\n                mf, mix = callf(mix)\n                if mf < pop_f[parent_i]:\n                    pop[parent_i] = mix\n                    pop_f[parent_i] = mf\n                    pop_sigma[parent_i] = max(self.sigma_min, pop_sigma[parent_i] * 1.06)\n                else:\n                    # try to inject into population replacing worst if better\n                    worst = np.argmax(pop_f)\n                    if mf < pop_f[worst]:\n                        pop[worst] = mix\n                        pop_f[worst] = mf\n                        pop_sigma[worst] = max(self.sigma_min, pop_sigma.mean() * 0.9)\n\n            # adapt parent sigma on repeated failure or success (population-wide)\n            # encourage diversity if population is converging (small std of f)\n            if self.evals < self.budget:\n                f_std = pop_f.std()\n                if f_std < 1e-6 + abs(pop_f.mean()) * 1e-6:\n                    # wide jitter to escape potential stagnation\n                    idx = self.rng.randint(n)\n                    pop_sigma[idx] = max(self.sigma_min, pop_sigma[idx] * (1.0 + 0.2 * self.rng.rand()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if (self.rng.rand() < 0.02) and (self.evals < self.budget):\n                worst = np.argmax(pop_f)\n                rnd = lb + self.rng.rand(self.dim) * (ub - lb)\n                rf, rnd = callf(rnd)\n                pop[worst] = rnd\n                pop_f[worst] = rf\n                pop_sigma[worst] = max(self.sigma_min, init_sigma_scale * (0.5 + self.rng.rand()))\n\n            # keep population size stable and prune if needed (in rare cases)\n            if pop.shape[0] > self.pop_size:\n                # remove worst extra\n                sorted_idx = np.argsort(pop_f)\n                keep_idx = sorted_idx[:self.pop_size]\n                pop = pop[keep_idx]\n                pop_f = pop_f[keep_idx]\n                pop_sigma = pop_sigma[keep_idx]\n\n            # If budget is very low left, do pure random probes\n            if (self.budget - self.evals) <= max(1, self.dim // 4):\n                while self.evals < self.budget:\n                    x = lb + self.rng.rand(self.dim) * (ub - lb)\n                    callf(x)\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.466 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11942449749158168, 0.16626345236242113, 0.9268343554368177, 0.9781601885779108, 0.2339524095946357, 0.965026415191289, 0.2533641326949002, 0.5713021281082458, 0.31340588693431726, 0.1344108874262887]}, "task_prompt": ""}
{"id": "df016f61-6082-4a96-bee0-08e3fb54d789", "fitness": 0.2541259244203406, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size points that mix randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed (Lévy-like) jumps to both explore and exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: RNG seed (optional)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # reasonable default population: scale with dim but keep modest\n        if pop_size is None:\n            self.pop_size = max(6, min(40, 4 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        # bookkeeping\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        # default bounds (BBOB uses -5..5), but we'll try to read from func.bounds when available\n        self.lb_default = -5.0 * np.ones(self.dim)\n        self.ub_default = 5.0 * np.ones(self.dim)\n\n    def __call__(self, func):\n        # get bounds from func if possible, otherwise use defaults\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape != (self.dim,):\n                lb = np.broadcast_to(lb, (self.dim,))\n            if ub.shape != (self.dim,):\n                ub = np.broadcast_to(ub, (self.dim,))\n        except Exception:\n            lb = self.lb_default.copy()\n            ub = self.ub_default.copy()\n\n        # helper: clip and evaluate while tracking remaining budget and best\n        self.remaining = int(self.budget)\n        def callf(x):\n            # x -> numpy array, clipped\n            if self.remaining <= 0:\n                return None  # no budget\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            # validate numeric\n            if not np.isfinite(f):\n                # treat non-finite as very bad\n                f = np.inf\n            self.remaining -= 1\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # If extremely small budget, fallback to random search\n        if self.budget <= 2 or self.remaining <= 0:\n            # do at least one uniform sample if possible\n            while self.remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop = []\n        initial_sigma = 0.2 * (ub - lb)  # vector but we'll use scalar step-size as median\n        init_sigma_scalar = float(np.median(initial_sigma))\n        for i in range(self.pop_size):\n            if self.remaining <= 0:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = callf(x)\n            if f is None:\n                break\n            member = {\n                'x': x.copy(),\n                'f': float(f),\n                'sigma': max(1e-8, init_sigma_scalar * self.rng.uniform(0.5, 1.5))\n            }\n            pop.append(member)\n\n        # If no population created (very small budget), do pure random sampling with remaining budget\n        if len(pop) == 0:\n            while self.remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Helper utilities\n        def normed(v):\n            v = np.asarray(v, dtype=float)\n            n = np.linalg.norm(v)\n            if n <= 0:\n                # return a random unit vector\n                v = self.rng.normal(size=self.dim)\n                n = np.linalg.norm(v) + 1e-12\n            return v / n\n\n        def get_best_indices(k=2):\n            # return indices of k best individuals\n            idxs = np.argsort([m['f'] for m in pop])\n            return [int(i) for i in idxs[:k]]\n\n        def replace_worst(member):\n            # replace worst in pop if member is better than worst\n            worst_idx = int(np.argmax([m['f'] for m in pop]))\n            if member['f'] < pop[worst_idx]['f']:\n                pop[worst_idx] = member\n                return True\n            return False\n\n        # Main adaptive loop\n        iter_count = 0\n        # parameters\n        tournament_k = min(3, max(2, len(pop)))\n        levy_prob = 0.06\n        rejuvenation_prob = 0.03\n        recomb_prob = 0.25\n        orth_prob = 0.4\n\n        while self.remaining > 0:\n            iter_count += 1\n            # pick parent by small tournament to balance exploration/exploitation\n            t_idx = self.rng.choice(len(pop), size=tournament_k, replace=False)\n            parent_idx = int(min(t_idx, key=lambda i: pop[int(i)]['f']))\n            parent = pop[parent_idx]\n            px = parent['x'].copy()\n            psigma = float(max(1e-12, parent['sigma']))\n\n            # sample a random search direction\n            direction = normed(self.rng.normal(size=self.dim))\n\n            # primary directional trial with stochasticized step-length\n            # stochasticize by log-normal multiplier\n            mult = float(np.exp(self.rng.normal(0, 0.35)))\n            step = psigma * mult\n            cand1 = px + step * direction\n            cand1 = np.minimum(np.maximum(cand1, lb), ub)\n            f1 = callf(cand1)\n            if f1 is None:\n                break\n            if f1 < parent['f']:\n                # accept and slightly increase sigma\n                parent['x'] = cand1\n                parent['f'] = f1\n                parent['sigma'] = psigma * 1.18\n                # maybe replace worst if this is very good relative\n                replace_worst(parent)\n                continue  # successful directional move, go next iteration\n\n            # local backtracking / small-step refinement along the same direction\n            refinement_success = False\n            small_scale = step * 0.5\n            # try a few smaller steps with some randomness\n            for k in range(3):\n                if self.remaining <= 0:\n                    break\n                trial = px + small_scale * (0.5 + self.rng.random()) * direction\n                trial = np.minimum(np.maximum(trial, lb), ub)\n                ftr = callf(trial)\n                if ftr is None:\n                    break\n                if ftr < parent['f']:\n                    parent['x'] = trial\n                    parent['f'] = ftr\n                    parent['sigma'] = psigma * 1.12\n                    refinement_success = True\n                    break\n                small_scale *= 0.5\n            if refinement_success:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if self.rng.random() < orth_prob and self.remaining > 0:\n                v = self.rng.normal(size=self.dim)\n                # make orthogonal to direction\n                v = v - np.dot(v, direction) * direction\n                vn = normed(v)\n                orth_step = psigma * 0.6 * (0.5 + self.rng.random())\n                orth_candidate = px + orth_step * vn\n                orth_candidate = np.minimum(np.maximum(orth_candidate, lb), ub)\n                forth = callf(orth_candidate)\n                if forth is None:\n                    break\n                if forth < parent['f']:\n                    parent['x'] = orth_candidate\n                    parent['f'] = forth\n                    parent['sigma'] = psigma * 1.15\n                    replace_worst(parent)\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.random() < levy_prob and self.remaining > 0:\n                # sample a robust Cauchy-like scalar and form a jump direction\n                # use generator.standard_cauchy but clamp extreme values\n                try:\n                    c = float(self.rng.standard_cauchy())\n                except Exception:\n                    c = self.rng.normal()  # fallback\n                # clamp heavy tails to avoid overflow but keep heavy-tail behavior\n                c = np.clip(c, -80.0, 80.0)\n                jump_direction = normed(self.rng.normal(size=self.dim))\n                jump_scale = psigma * (5.0 + abs(c))  # amplify by heavy tail\n                jump = px + jump_scale * jump_direction\n                jump = np.minimum(np.maximum(jump, lb), ub)\n                fj = callf(jump)\n                if fj is None:\n                    break\n                if fj < parent['f']:\n                    # strong improvement: replace worst to inject diversity\n                    new_member = {'x': jump.copy(), 'f': float(fj), 'sigma': max(1e-12, psigma * 0.8)}\n                    replace_worst(new_member)\n                    # also accept as new parent\n                    parent['x'] = jump\n                    parent['f'] = fj\n                    parent['sigma'] = new_member['sigma'] * 1.05\n                    continue\n                else:\n                    # keep it as candidate replacing worst only if reasonably good\n                    worst_idx = int(np.argmax([m['f'] for m in pop]))\n                    if fj < pop[worst_idx]['f']:\n                        pop[worst_idx] = {'x': jump.copy(), 'f': float(fj), 'sigma': max(1e-12, psigma * 0.6)}\n                    # continue main loop\n                    # adapt parent sigma on failure to encourage exploration\n                    parent['sigma'] = psigma * 0.86\n                    continue\n\n            # recombination exploitation: mix two best and small noise\n            if self.rng.random() < recomb_prob and self.remaining > 0:\n                best_idx, second_idx = get_best_indices(2)\n                a = pop[int(best_idx)]['x']\n                b = pop[int(second_idx)]['x']\n                # linear recombination with slight asymmetry + small gaussian noise\n                alpha = 0.4 + 0.2 * self.rng.random()\n                offspring = a + alpha * (b - a) + 0.01 * (ub - lb) * self.rng.normal(size=self.dim)\n                offspring = np.minimum(np.maximum(offspring, lb), ub)\n                fo = callf(offspring)\n                if fo is None:\n                    break\n                offspring_member = {'x': offspring.copy(), 'f': float(fo), 'sigma': max(1e-12, (pop[int(best_idx)]['sigma'] + pop[int(second_idx)]['sigma']) * 0.6)}\n                # try to inject into population by replacing worst if it's better\n                replaced = replace_worst(offspring_member)\n                if replaced:\n                    # if injected and better than parent, adopt it\n                    if offspring_member['f'] < parent['f']:\n                        parent.update(offspring_member)\n                        parent['sigma'] *= 1.08\n                    continue\n                else:\n                    # if not injected, decrease parent sigma slightly to escape stagnation\n                    parent['sigma'] = psigma * 0.9\n\n            # adapt parent sigma on failure if nothing improved\n            parent['sigma'] = max(1e-12, parent['sigma'] * 0.87)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if (self.rng.random() < rejuvenation_prob) and self.remaining > 0:\n                cand = self.rng.uniform(lb, ub)\n                fc = callf(cand)\n                if fc is None:\n                    break\n                worst_idx = int(np.argmax([m['f'] for m in pop]))\n                pop[worst_idx] = {'x': cand.copy(), 'f': float(fc), 'sigma': max(1e-12, init_sigma_scalar * self.rng.uniform(0.3, 1.2))}\n\n            # safety guard: if population collapsed (all identical or same f), re-seed a random member\n            if self.remaining > 0:\n                fs = np.array([m['f'] for m in pop])\n                if np.allclose(fs, fs[0]) and len(pop) > 1 and self.rng.random() < 0.05:\n                    worst_idx = int(np.argmax(fs))\n                    xrand = self.rng.uniform(lb, ub)\n                    fr = callf(xrand)\n                    if fr is None:\n                        break\n                    pop[worst_idx] = {'x': xrand.copy(), 'f': float(fr), 'sigma': max(1e-12, init_sigma_scalar * self.rng.uniform(0.3, 1.2))}\n\n        # finished budget or stopped\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.254 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11875396088404688, 0.15593519137310474, 0.40888936399421716, 0.17779243092141406, 0.280222457206746, 0.6604096262879238, 0.21919005607212816, 0.19211618159835675, 0.19795162668245747, 0.1299983491830109]}, "task_prompt": ""}
{"id": "f2109c2b-76ad-4e2b-b42b-83d3ac7251a4", "fitness": 0.42210181765715815, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based adaptive step-size method combining randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps for robust global-local search.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (by default scales modestly with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but remains modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = None if seed is None else int(seed)\n        # internal rng\n        self.rng = np.random.RandomState(self.seed)\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # retrieve bounds, allow scalar or array bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback to default [-5,5] if bounds not provided\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # broadcast scalar bounds to full dimension\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # ensure valid bounds\n        span = ub - lb\n        span[span <= 0] = 1.0  # avoid zero span\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # early exit if no budget\n        if remaining <= 0:\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # initial scale heuristics\n        base_sigma = max(1e-8, 0.25 * np.mean(span))\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search until budget exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # tournament selection of a parent\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(span))\n                continue  # continue main loop\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            back_factors = [0.5, 0.25, 0.1]\n            for bf in back_factors:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * bf * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            # make r orthogonal to d: r <- r - (r.d) d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(span))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = self.rng.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * span\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n            else:\n                mix = x_parent.copy()\n            noise = (0.01 * span) * self.rng.randn(self.dim)\n            x_try = np.clip(mix + noise, lb, ub)\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (if we got here, no success for this parent)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n            # small housekeeping: if population lost members (shouldn't normally happen), refill\n            if len(pop) < self.pop_size and remaining > 0:\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.422 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14335412418179583, 0.14894570050871248, 0.5453683408641032, 0.9654150374929128, 0.3739309776886115, 0.771104181952065, 0.2212010508474046, 0.3878931776787151, 0.4933102245466767, 0.17049536081058347]}, "task_prompt": ""}
{"id": "01abae81-ce2f-410f-866f-b3c505201501", "fitness": 0.4686552891700659, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — an adaptive population-based continuous optimizer combining randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy-like jumps with adaptive step-sizes and small tournament selection.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None, auto-scaled with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that scales with dimension\n            self.pop_size = int(min(40, max(6, 8 + int(2 * np.sqrt(self.dim)))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Bounds handling (func.bounds.lb/ub may be scalar or arrays)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # safe fallback in case bounds are inverted\n        span = np.maximum(ub - lb, 1e-12)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                # No budget left: do not evaluate\n                return np.inf, np.asarray(x, dtype=float)\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is zero, return immediately\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        n_init = min(self.pop_size, remaining)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-8, 0.25 * np.mean(span))  # initial scale\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma)\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f, dtype=float)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            if k == 1:\n                inds = np.array([np.random.randint(len(pop))])\n            else:\n                inds = np.random.choice(len(pop), k, replace=False)\n            # choose the best among the tournament (exploitation) with some chance to pick worse (diversity)\n            if np.random.rand() < 0.85:\n                parent_i = inds[int(np.argmin(pop_f[inds]))]\n            else:\n                parent_i = inds[np.random.randint(len(inds))]\n\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n            parent_f = float(pop_f[parent_i])\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            alpha = float(max(1e-12, np.abs(alpha)))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            if f_try < parent_f:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(span))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, 0.125, 0.0625):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # remove component along d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_orth, x_orth = callf(x_try)\n                    if f_orth < pop_f[parent_i]:\n                        pop[parent_i] = x_orth\n                        pop_f[parent_i] = f_orth\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(span))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.median(np.abs(step)) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * span\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * span) * np.random.randn(self.dim)\n                x_mix = np.clip(mix + noise, lb, ub)\n                f_mix, x_mix = callf(x_mix)\n                worst_i = int(np.argmax(pop_f))\n                # replace parent if improved, else possibly replace worst\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    # try to inject into population by replacing the worst if it's better\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                else:\n                    # if mix is somewhat good, may replace worst to maintain diversity\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = base_sigma * 0.6\n\n            # adapt parent sigma on failure (if none of the above improved)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma\n                else:\n                    # sometimes still replace to inject diversity\n                    if np.random.rand() < 0.5:\n                        pop[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = base_sigma\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.469 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12041611154232235, 0.1603482347163433, 0.47911022941034087, 0.9430331202863127, 0.6849403469252211, 0.7200639050865215, 0.25667631806966906, 0.39406919997987866, 0.7905853604663681, 0.13731006521768152]}, "task_prompt": ""}
{"id": "ebe40464-6ace-4f5f-98de-f7c30c951fa1", "fitness": 0.4286455794199032, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based directional local searches with adaptive step-sizes, orthogonal refinements and occasional heavy-tailed Lévy jumps to balance robust exploration and focused exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_size: override population size\n    - seed: random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        # sensible default population scaling with dim (keeps population modest)\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # prepare bounds as arrays of correct dimension\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # safety: ensure input dim consistent\n        if lb.size != self.dim or ub.size != self.dim:\n            # try to adapt if function provides different dim\n            if lb.size == ub.size:\n                self.dim = lb.size\n            else:\n                raise ValueError(\"Bounds size mismatch with specified dim\")\n\n        remaining = int(self.budget)\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        # wrapper to evaluate function while tracking remaining budget and best-so-far\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                # store a copy\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, do pure random sampling\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population with random samples (as many as budget allows up to pop_size)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale based on search range\n\n        for i in range(n_init):\n            x0 = rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * rng.rand()))\n            if remaining <= 0:\n                break\n\n        # fallback: if no population (very tiny budget)\n        if len(pop) == 0:\n            while remaining > 0:\n                x = rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # main optimization loop\n        # we repeatedly perform directional local searches, orthogonal tries, Levy jumps, recombination\n        while remaining > 0:\n            # choose a parent via a small tournament (prefer better individuals)\n            k = min(3, len(pop))\n            inds = rng.choice(len(pop), k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            sigma = float(pop_sigma[parent_i])\n            x_parent = pop[parent_i].copy()\n\n            # generate a randomized search direction (normalized)\n            d = rng.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # step length combines sigma with a small random multiplier\n            alpha = sigma * (0.8 + 0.4 * rng.rand()) * (1.0 + 0.4 * rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma for that individual\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, 0.125):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = rng.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if rng.rand() < 0.15 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = rng.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale = np.mean(ub - lb) * (0.4 + 0.6 * rng.rand())\n                x_try = np.clip(x_parent + scale * step, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if rng.rand() < 0.25 and len(pop) >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # small Gaussian jitter proportional to sigma\n                mix = mix + 0.05 * sigma * rng.randn(self.dim)\n                x_try = np.clip(mix, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # replace parent if improved, else maybe replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try.copy()\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5\n                continue\n\n            # adapt parent sigma on failure (shrink to focus)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if rng.rand() < 0.05 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * rng.rand())\n\n        # finished (budget exhausted or loop ended)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.429 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1392039672916432, 0.15413144946992496, 0.7452276318493022, 0.8400557941229496, 0.3677029165839656, 0.9105725292400023, 0.24619347784519496, 0.42731164649544795, 0.3118696267515748, 0.14418675454902596]}, "task_prompt": ""}
{"id": "a79c7c4a-9025-49be-959e-c887bfd2eaa7", "fitness": 0.34155221854389994, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining randomized directional local searches, orthogonal refinements, adaptive step-sizes and occasional heavy-tailed Lévy-like jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by dim if None)\n    - seed: optional RNG seed for reproducibility\n\n    The optimizer expects func to expose bounds via func.bounds.lb and func.bounds.ub\n    (scalars or arrays). It will not call func more times than `budget`.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows slowly with dimension\n            self.pop_size = max(4, min(40, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n    def __call__(self, func):\n        # prepare bounds as full-dim arrays\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback to default [-5,5] if func doesn't expose bounds\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # ensure shapes match\n        if lb.shape[0] != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.shape[0] != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        # initialize bookkeeping\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        remaining = int(self.budget)\n\n        # helper to safely call func while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure numpy array and clipped within bounds\n            x = np.asarray(x, dtype=float)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt or self.x_opt is None:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to pure random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # base scale from range\n        range_vec = ub - lb\n        mean_range = max(1e-12, float(np.mean(range_vec)))\n        base_sigma = max(1e-12, 0.2 * mean_range)  # initial scale\n\n        # Initialize population (random samples)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(float(f0))\n            # sigma initialized proportional to range with some diversity\n            pop_sigma.append(base_sigma * (0.5 + np.random.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Precompute some constants\n        max_sigma = mean_range\n        min_sigma = 1e-12\n\n        # Main loop\n        while remaining > 0:\n            # pick a parent via small tournament (k=3)\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), size=k, replace=False)\n            # choose best among tournament\n            values = np.array([pop_f[i] for i in inds])\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # alpha scales with sigma and a random multiplier\n            alpha = sigma * (0.6 + 1.8 * np.random.rand())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max_sigma, sigma * (1.05 + 0.2 * np.random.rand()))\n                continue  # go to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, 0.1):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(min_sigma, sigma * (0.9 - 0.1 * np.random.rand()))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # subtract projection on d to make it orthogonal\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(max_sigma, sigma * (1.02 + 0.1 * np.random.rand()))\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale vector by a robust scale combining base_sigma and current sigma\n                scale_vec = base_sigma * (0.5 + np.random.rand()) + 0.5 * sigma\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try.copy()\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(min_sigma, sigma * 0.5)\n                # continue loop after jump attempt (no explicit continue; algorithm proceeds)\n\n            if remaining <= 0:\n                break\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * range_vec) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                worst_i = int(np.argmax(pop_f))\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(min_sigma, sigma * 0.95)\n                else:\n                    # inject into population by replacing the worst if it's better\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n\n            # adapt parent sigma on failure (if none of the above improved parent)\n            if pop_f[parent_i] >= self.f_opt:  # no local improvement relative to global best\n                # slightly reduce sigma to focus search locally\n                pop_sigma[parent_i] = max(min_sigma, sigma * (0.85 - 0.1 * np.random.rand()))\n            else:\n                # encourage exploration around successful parents\n                pop_sigma[parent_i] = min(max_sigma, sigma * (1.02 + 0.05 * np.random.rand()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.06 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # small chance to add a slightly mutated clone of a good individual\n            if np.random.rand() < 0.05 and remaining > 0:\n                best_i = int(np.argmin(pop_f))\n                perturb = (0.1 * range_vec) * np.random.randn(self.dim) * (0.5 + np.random.rand())\n                x_new = np.clip(pop[best_i] + perturb, lb, ub)\n                f_new, x_new = callf(x_new)\n                worst_i = int(np.argmax(pop_f))\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.4 * np.random.rand())\n\n            # If population is small and budget remains, try to grow diversity by adding random individuals\n            if len(pop) < self.pop_size and remaining > 0:\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop.append(x_new.copy())\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.6 + 0.5 * np.random.rand()))\n\n        # finished budget or exhausted loop\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.342 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12412676299969505, 0.16424424321318631, 0.43012319844074, 0.5988573869175863, 0.366868856327937, 0.5193534749442887, 0.2325438163712189, 0.36711924955898434, 0.46349486341535184, 0.1487903332500109]}, "task_prompt": ""}
{"id": "b578f425-6d14-4b82-9806-1168f6db4f08", "fitness": 0.42426486588950213, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based search combining randomized directional local searches, orthogonal refinements, occasional Lévy (Cauchy) escapes, recombination of the best, and adaptive step-size per individual.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimension but stays capped\n            self.pop_size = min(max(6, 4 * self.dim), 40)\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        # best found values\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        # read bounds from the provided func (Many BBOB style). Guarantee full-dim arrays.\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        else:\n            lb = np.asarray(lb, dtype=float)\n            ub = np.asarray(ub, dtype=float)\n        # safety\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.clip(x, lb, ub)\n            f = func(x)\n            remaining -= 1\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n        n_init = min(self.pop_size, remaining)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0.copy())\n            pop_f.append(float(f0))\n            # individual sigmas are randomized around base\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * np.random.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return self.f_opt, self.x_opt\n\n        # convert to numpy arrays for faster ops and indexing\n        pop = np.array(pop, dtype=float)\n        pop_f = np.array(pop_f, dtype=float)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        # Wrap the loop body in try/except to break cleanly once budget is exhausted\n        while remaining > 0:\n            try:\n                # pick a parent via small tournament to balance exploration/exploitation\n                k = min(3, len(pop))\n                inds = np.random.choice(len(pop), k, replace=False)\n                # choose the best among the tournament\n                parent_i = inds[int(np.argmin(pop_f[inds]))]\n                x_parent = pop[parent_i].copy()\n                sigma = float(pop_sigma[parent_i])\n\n                # sample a random search direction (normalized)\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    nd = 1e-12\n                d = d / nd\n\n                # primary directional trial with stochasticized step-length\n                # alpha scales with sigma but has stochastic multiplicative factor (lognormal)\n                alpha = sigma * max(1e-12, np.random.lognormal(mean=0.0, sigma=0.25))\n                x_try = np.clip(x_parent + alpha * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                improved = False\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.12, base_sigma * 100.0)\n                    improved = True\n\n                    # local backtracking / small-step refinement along direction (few tries)\n                    # progressively smaller steps along same direction to exploit found improvement\n                    for j in range(2):\n                        if remaining <= 0:\n                            break\n                        alpha2 = alpha * (0.5 ** (j + 1))\n                        x_back = np.clip(x_parent + alpha2 * d, lb, ub)\n                        f_back, x_back = callf(x_back)\n                        if f_back < pop_f[parent_i]:\n                            pop[parent_i] = x_back.copy()\n                            pop_f[parent_i] = f_back\n                            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.05, 1e-12)\n\n                else:\n                    # adapt parent sigma on failure (shrink)\n                    pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n                # try an orthogonal perturbation for local diversification\n                r = np.random.randn(self.dim)\n                # remove projection on d to get approximately orthogonal step\n                r = r - (r.dot(d)) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    if remaining > 0:\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try.copy()\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n\n                # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n                if (np.random.rand() < 0.08) and (remaining > 0):\n                    # Cauchy-like heavy-tailed vector normalized by a robust scale\n                    step = np.random.standard_cauchy(self.dim)\n                    denom = np.median(np.abs(step)) + 1e-8\n                    step = step / denom\n                    # scale per-dimension to keep reasonable amplitude\n                    scale_vec = base_sigma * (1.0 + 3.0 * np.random.rand(self.dim))\n                    x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                    if remaining > 0:\n                        f_try, x_try = callf(x_try)\n                        # if it's good replace the worst in population, else maybe keep it as candidate\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try.copy()\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n\n                # recombination exploitation: mix two best and small noise with some probability\n                if (np.random.rand() < 0.12) and (len(pop) > 1) and (remaining > 0):\n                    best2 = np.argsort(pop_f)[:2]\n                    a, b = best2[0], best2[1]\n                    beta = np.random.uniform(0.3, 0.7)\n                    mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                    noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                    x_try = np.clip(mix + noise, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    # replace worst if better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n                # occasional population rejuvenation by replacing the worst with a random sample\n                if (np.random.rand() < 0.05) and (remaining > 0):\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n                # keep arrays consistent types in case they were mutated\n                # (not strictly necessary but keeps shapes predictable)\n                pop = np.asarray(pop, dtype=float)\n                pop_f = np.asarray(pop_f, dtype=float)\n                pop_sigma = np.asarray(pop_sigma, dtype=float)\n\n            except RuntimeError:\n                # budget exhausted while attempting evaluations inside the loop\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.424 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1505947531371108, 0.17457671456493995, 0.4718181848146582, 0.9028880551980127, 0.46043324885941717, 0.6179310784018164, 0.28832949239559735, 0.3883817857946005, 0.618504361746753, 0.16919098398211496]}, "task_prompt": ""}
{"id": "172a44a0-7ad4-4da0-bb40-20e6913c36a2", "fitness": 0.24997513579983482, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — combines tournament-selected directional local searches with adaptive per-individual step sizes, orthogonal refinements and occasional Lévy (Cauchy-like) jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional parameters can be tweaked for different behavior.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # population size: scale with problem dimension but keep moderate\n        if pop_size is None:\n            self.pop_size = max(4, min(40, int(2 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n\n        # control params (sensible defaults)\n        self.base_sigma_frac = 0.2   # base sigma as fraction of domain width\n        self.tournament_k = 3        # tournament size to pick parent\n        self.levy_prob = 0.06        # probability of performing a heavy-tailed jump\n        self.rejuv_prob = 0.02       # probability of random rejuvenation of worst\n        self.orth_frac = 0.6         # orthogonal perturbation fraction\n        self.max_backtracks = 4      # local backtracking tries on success\n        self.verbose = False\n\n    def __call__(self, func):\n        # bounds handling\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        # ensure full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        domain_width = np.maximum(ub - lb, 1e-12)\n        base_sigma = self.base_sigma_frac * np.mean(domain_width)\n\n        # state\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        remaining = int(self.budget)\n        # helper to evaluate while tracking budget and best solution\n        def callf(x):\n            nonlocal remaining\n            # ensure numpy array of correct shape\n            x = np.asarray(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            if remaining <= 0:\n                # Shouldn't happen if callers check remaining, but safe-guard:\n                return None, x\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if f < self.f_opt or self.x_opt is None:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If extremely small budget, do a small random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)  # cannot initialize more points than budget allows\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            if f0 is None:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize sigma per individual with small randomization\n            pop_sigma.append(max(1e-12, base_sigma * (0.6 + 0.8 * np.random.rand())))\n\n        # If we couldn't create at least one individual, fallback to random sampling\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return self.f_opt, self.x_opt\n\n        # If budget left but population smaller than intended, re-scale pop_size to actual\n        self.pop_size = len(pop)\n\n        # Main optimization loop\n        while remaining > 0:\n            # pick parent via small tournament (minimization)\n            k = min(self.tournament_k, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            vals = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(vals))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # choose direction: mostly random unit vector or difference vector\n            if np.random.rand() < 0.6 and len(pop) >= 2:\n                # directional difference between two random distinct individuals\n                i1, i2 = np.random.choice(len(pop), 2, replace=False)\n                d = pop[i1] - pop[i2]\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    d = np.random.randn(self.dim)\n                    nd = np.linalg.norm(d)\n                d = d / nd\n            else:\n                # pure random unit direction\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    d = np.ones(self.dim) / np.sqrt(self.dim)\n                else:\n                    d = d / nd\n\n            # sample a stochastic step length: use Gaussian multiplicative and occasional Cauchy factor\n            # Cauchy-like: heavy tail via tan(pi*(u-0.5))\n            u = np.random.rand()\n            cauchy_factor = np.tan(np.pi * (u - 0.5))\n            step_len = sigma * (1.0 + 0.6 * np.random.randn()) * (1.0 + 0.15 * cauchy_factor)\n            # clip step length to a reasonable multiple of domain to avoid overflow\n            max_step = 3.0 * np.max(domain_width)\n            step_len = np.clip(step_len, -max_step, max_step)\n\n            # primary directional trial\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n            improved = False\n            if f_try is not None and f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = max(1e-12, sigma * (1.06 + 0.02 * np.random.randn()))\n                improved = True\n\n                # local backtracking / refinement: try smaller steps along same direction\n                for bt in range(self.max_backtracks):\n                    if remaining <= 0:\n                        break\n                    small_step = 0.5 ** (bt + 1) * sigma * (0.8 + 0.4 * np.random.rand())\n                    x_bt = np.clip(pop[parent_i] + small_step * d, lb, ub)\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt is None:\n                        break\n                    if f_bt < pop_f[parent_i]:\n                        pop[parent_i] = x_bt\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 1.03)\n                    else:\n                        # if no improvement, slightly reduce sigma\n                        pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.98)\n                        break\n\n            else:\n                # primary failed: try orthogonal perturbation for diversification\n                r = np.random.randn(self.dim)\n                # make r orthogonal to d\n                proj = np.dot(r, d) * d\n                r = r - proj\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_orth = np.clip(x_parent + self.orth_frac * sigma * r, lb, ub)\n                    if remaining > 0:\n                        f_orth, x_orth = callf(x_orth)\n                        if f_orth is not None and f_orth < pop_f[parent_i]:\n                            pop[parent_i] = x_orth\n                            pop_f[parent_i] = f_orth\n                            pop_sigma[parent_i] = max(1e-12, sigma * 1.04)\n                            improved = True\n\n                # recombination exploitation: mix two best and small gaussian noise\n                if not improved and len(pop) >= 2:\n                    best_inds = np.argsort(pop_f)[:2]\n                    mix = 0.6 * pop[best_inds[0]] + 0.4 * pop[best_inds[1]]\n                    noise = 0.3 * sigma * np.random.randn(self.dim)\n                    x_mix = np.clip(mix + noise, lb, ub)\n                    if remaining > 0:\n                        f_mix, x_mix = callf(x_mix)\n                        if f_mix is not None:\n                            # replace the parent if mix is better, else maybe replace worst\n                            if f_mix < pop_f[parent_i]:\n                                pop[parent_i] = x_mix\n                                pop_f[parent_i] = f_mix\n                                pop_sigma[parent_i] = max(1e-12, sigma * 0.9)\n                                improved = True\n                            else:\n                                # inject into population by replacing worst if it's better\n                                worst_i = int(np.argmax(pop_f))\n                                if f_mix < pop_f[worst_i]:\n                                    pop[worst_i] = x_mix\n                                    pop_f[worst_i] = f_mix\n                                    pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n\n                # if still not improved, decrease parent's sigma\n                if not improved:\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.85)\n\n            # occasional Lévy-like jump to escape local basins\n            if remaining > 0 and np.random.rand() < self.levy_prob:\n                # generate heavy-tailed per-dim step using Cauchy (tangent transform)\n                u = np.random.rand(self.dim)\n                cauchy_vec = np.tan(np.pi * (u - 0.5))\n                # normalize to robust scale using 90th percentile\n                denom = np.percentile(np.abs(cauchy_vec), 90) + 1e-12\n                cauchy_vec = cauchy_vec / denom\n                scale_vec = 0.8 * np.random.rand(self.dim) * domain_width  # scale by domain width per-dim\n                x_jump = np.clip(x_parent + cauchy_vec * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_jump, x_jump = callf(x_jump)\n                    if f_jump is not None:\n                        # if jump is good, replace worst; otherwise keep as candidate by maybe replacing worst if somewhat close\n                        worst_i = int(np.argmax(pop_f))\n                        if f_jump < pop_f[worst_i]:\n                            pop[worst_i] = x_jump\n                            pop_f[worst_i] = f_jump\n                            pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < self.rejuv_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new is not None and f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = max(1e-12, base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n            # keep population arrays consistent length (in case of any unforeseen change)\n            if len(pop) == 0:\n                break\n\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.250 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0928868289329694, 0.1611156384327992, 0.9391439245990867, 0.17475565201994814, 0.22454153436606694, 0.21443177107610256, 0.21352853128335392, 0.17271308295858734, 0.16973836219586036, 0.13689603213357304]}, "task_prompt": ""}
{"id": "af87c1d8-3171-4731-9caa-5630aec4665f", "fitness": 0.5528710010149736, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer that combines adaptive directional local searches, orthogonal refinements, recombination and occasional Lévy-like heavy-tailed jumps to balance exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_size: number of candidate solutions in the population (defaults to modest scaling with dim)\n    - init_sigma: initial relative step-size (fraction of search-range)\n    - seed: optional random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, init_sigma=0.2, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # keep population modest but scaling with dimension\n            self.pop_size = max(6, min(40, 4 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        self.init_sigma = float(init_sigma)\n        self.seed = seed\n\n    def __call__(self, func):\n        # seed rng if requested\n        if self.seed is not None:\n            np.random.seed(self.seed)\n\n        # determine bounds: try to use func.bounds if available, otherwise assume [-5,5] per prompt\n        try:\n            lb = np.asarray(func.bounds.lb)\n            ub = np.asarray(func.bounds.ub)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # coerce scalars to arrays of correct dim\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        # ensure valid shapes\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        # remaining budget and best trackers\n        remaining = int(self.budget)\n        f_opt = np.inf\n        x_opt = None\n\n        # helper to evaluate while tracking budget and best (does not call if budget exhausted)\n        def callf(x_in):\n            nonlocal remaining, f_opt, x_opt\n            x_in = np.asarray(x_in, dtype=float).reshape(self.dim,)\n            # clip to bounds to be safe\n            x_in = np.minimum(np.maximum(x_in, lb), ub)\n            if remaining <= 0:\n                return float(np.inf), x_in\n            # evaluate\n            fval = float(func(x_in))\n            remaining -= 1\n            if fval < f_opt:\n                f_opt = fval\n                x_opt = x_in.copy()\n            return fval, x_in.copy()\n\n        # quick fallback: if budget tiny, do random sampling\n        if remaining <= 0:\n            return f_opt, x_opt\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_range = ub - lb\n        base_sigma = self.init_sigma * np.maximum(base_range, 1e-8)  # vector of characteristic step sizes\n\n        while len(pop) < self.pop_size and remaining > 0:\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # per-individual sigma is vector scale: use scalar multiplier representing fraction of range\n            pop_sigma.append(np.maximum(np.linalg.norm(base_sigma) / np.sqrt(self.dim), 1e-8))\n\n        # if no population created (very small budget), return best seen\n        if len(pop) == 0:\n            return f_opt, x_opt\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # parameters\n        tournament_k = min(3, len(pop))\n        min_sigma = 1e-8\n        max_sigma = np.linalg.norm(ub - lb) * 2.0\n        levy_prob = 0.06\n        recomb_prob = 0.15\n        rejuvenation_prob = 0.02\n\n        # Main loop\n        while remaining > 0:\n            # pick a parent via small tournament\n            inds = np.random.choice(len(pop), size=tournament_k, replace=False)\n            values = pop_f[inds]\n            parent_idx = inds[int(np.argmin(values))]\n            x_parent = pop[parent_idx].copy()\n            sigma_parent = pop_sigma[parent_idx]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # choose a scalar step from normal around sigma_parent\n            step_len = sigma_parent * max(1e-6, (1.0 + 0.3 * np.random.randn()))\n            x_try = x_parent + d * step_len\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_idx]:\n                # success: accept and slightly increase sigma\n                pop[parent_idx] = x_try\n                pop_f[parent_idx] = f_try\n                pop_sigma[parent_idx] = min(max_sigma, pop_sigma[parent_idx] * 1.12)\n                # continue to next iteration to intensify around success\n                continue\n            else:\n                # failure: try local backtracking / small-step refinement along direction\n                improved = False\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    if remaining <= 0:\n                        break\n                    x_bt = x_parent + d * (step_len * frac)\n                    x_bt = np.minimum(np.maximum(x_bt, lb), ub)\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt < pop_f[parent_idx]:\n                        pop[parent_idx] = x_bt\n                        pop_f[parent_idx] = f_bt\n                        pop_sigma[parent_idx] = min(max_sigma, pop_sigma[parent_idx] * 1.06)\n                        improved = True\n                        break\n                if improved:\n                    continue\n                # if still not improved, shrink sigma moderately\n                pop_sigma[parent_idx] = max(min_sigma, pop_sigma[parent_idx] * 0.85)\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make r orthogonal to d\n                proj = np.dot(r, d) * d\n                r_perp = r - proj\n                nr = np.linalg.norm(r_perp)\n                if nr > 1e-12:\n                    r_perp = r_perp / nr\n                    scale = 0.6 * pop_sigma[parent_idx]\n                    x_orth = x_parent + r_perp * scale\n                    x_orth = np.minimum(np.maximum(x_orth, lb), ub)\n                    f_orth, x_orth = callf(x_orth)\n                    if f_orth < pop_f[parent_idx]:\n                        pop[parent_idx] = x_orth\n                        pop_f[parent_idx] = f_orth\n                        pop_sigma[parent_idx] = min(max_sigma, pop_sigma[parent_idx] * 1.08)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and np.random.rand() < levy_prob:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale (90th percentile)\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale to a fraction of the search range, randomized\n                global_scale = 0.12 + 0.18 * np.random.rand()  # between ~0.12 and 0.3\n                scale_vec = global_scale * (ub - lb)\n                x_jump = x_parent + step * scale_vec\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < pop_f[parent_idx]:\n                    pop[parent_idx] = x_jump\n                    pop_f[parent_idx] = f_jump\n                    pop_sigma[parent_idx] = max(min_sigma, pop_sigma[parent_idx] * 1.2)\n                    continue\n                else:\n                    # try to inject into population by replacing the worst if better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(min_sigma, pop_sigma[parent_idx] * 0.6)\n                    # continue after jump attempt\n\n            # recombination exploitation: mix two best and add small noise\n            if remaining > 0 and np.random.rand() < recomb_prob and len(pop) >= 2:\n                best_inds = np.argsort(pop_f)\n                a, b = best_inds[0], best_inds[1]\n                mix = 0.5 * (pop[a] + pop[b])\n                noise = 0.06 * pop_sigma[parent_idx] * np.random.randn(self.dim)\n                x_mix = np.clip(mix + noise, lb, ub)\n                f_mix, x_mix = callf(x_mix)\n                if f_mix < pop_f[parent_idx]:\n                    pop[parent_idx] = x_mix\n                    pop_f[parent_idx] = f_mix\n                    pop_sigma[parent_idx] = max(min_sigma, pop_sigma[parent_idx] * 1.05)\n                else:\n                    # possibly replace worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = max(min_sigma, pop_sigma[parent_idx] * 0.5)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < rejuvenation_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = abs(pop_sigma[worst_i]) * 0.8\n                else:\n                    # even if not better, sometimes accept to maintain diversity\n                    if np.random.rand() < 0.1:\n                        pop[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = max(min_sigma, pop_sigma[worst_i] * 0.7)\n\n            # small safeguard to avoid infinite loops if remaining didn't change for some reason\n            # (shouldn't happen because callf is used inside the loop)\n            # continue main while\n\n        # finished budget\n        # store into object for external inspection\n        self.f_opt = f_opt\n        self.x_opt = x_opt\n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.553 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1515317141458663, 0.15854184582385622, 0.8511309149212203, 0.9565761342893604, 0.8565208222409978, 0.9149406872894104, 0.24294276504114987, 0.4109683488527115, 0.852579005844314, 0.13297777170084946]}, "task_prompt": ""}
{"id": "6e2ab8d7-164c-4fb5-bf8c-b925e3d97e96", "fitness": 0.504538227545117, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points exploring via randomized directional local searches, orthogonal refinements and occasional Cauchy/Lévy jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a modest function of dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dim but stays modest\n            self.pop_size = int(max(4, min(40, 4 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # detect bounds if available, else fallback to [-5, 5] per-dimension\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # if provided bounds are scalars expand them\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # ensure shapes match dim\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # internal state\n        remaining = int(self.budget)\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            x = np.clip(x, lb, ub)\n            # evaluate\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f\n\n        # If budget is zero, return immediately\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (as many as budget allows up to pop_size)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # initial sigma scale relative to search range\n        range_mean = float((ub - lb).mean())\n        sigma0 = max(1e-9, 0.2 * range_mean)\n\n        for _ in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x = lb + self.rng.uniform(size=self.dim) * (ub - lb)\n            f = callf(x)\n            pop.append(x)\n            pop_f.append(f)\n            pop_sigma.append(sigma0)\n\n        # If no population could be created (very small budget), do pure random search until budget exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = lb + self.rng.uniform(size=self.dim) * (ub - lb)\n                f = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: directional local searches, orthogonal tries, and Lévy jumps\n        iter_count = 0\n        no_improve = 0\n\n        while remaining > 0:\n            iter_count += 1\n\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), size=k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = int(inds[int(np.argmin(values))])\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.normal(size=self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                # skip a degenerate direction\n                pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.95)\n                continue\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * np.exp(self.rng.normal(0, 0.35))  # log-normal perturbation of step length\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try = callf(x_try)\n            if f_try < f_parent:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.2, max(ub - lb).max())\n                no_improve = 0\n                continue  # \"exploit\" success and continue main loop\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.6, 0.3, 0.1):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try = callf(x_try)\n                if f_try < f_parent:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                    improved = True\n                    no_improve = 0\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.normal(size=self.dim)\n            r = r - np.dot(r, d) * d  # orthogonalize to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                if remaining > 0:\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    f_try = callf(x_try)\n                    if f_try < f_parent:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                        no_improve = 0\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if (self.rng.rand() < 0.07) and (remaining > 0):\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = self.rng.standard_cauchy(size=self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                jump_scale = 0.6 * range_mean\n                x_jump = np.clip(x_parent + jump_scale * (step / denom), lb, ub)\n                f_jump = callf(x_jump)\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = sigma0\n                    no_improve = 0\n                    continue\n                # else keep trying (we don't force acceptance)\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                idxs = np.argsort(pop_f)\n                a, b = int(idxs[0]), int(idxs[1])\n                # blend two best plus a small gaussian injection\n                offspring = 0.5 * (pop[a] + pop[b]) + 0.02 * (ub - lb) * self.rng.normal(size=self.dim)\n                offspring = np.clip(offspring, lb, ub)\n                f_off = callf(offspring)\n                worst_i = int(np.argmax(pop_f))\n                if f_off < pop_f[worst_i]:\n                    pop[worst_i] = offspring\n                    pop_f[worst_i] = f_off\n                    pop_sigma[worst_i] = sigma0\n                    no_improve = 0\n\n            # adapt parent sigma on failure\n            if pop_f[parent_i] >= f_parent:\n                # failed attempt(s): reduce sigma modestly\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n                no_improve += 1\n            else:\n                no_improve = 0\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if (iter_count % 50 == 0) or (no_improve > 30):\n                worst_i = int(np.argmax(pop_f))\n                if remaining > 0:\n                    x_new = lb + self.rng.uniform(size=self.dim) * (ub - lb)\n                    f_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = sigma0\n                no_improve = 0\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.505 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16443191484761033, 0.3356474347806472, 0.5436663026193345, 0.9302263240856025, 0.6914632576374902, 0.8386134907923954, 0.2764323555953937, 0.4351354225275059, 0.6539983729956405, 0.17576739956955045]}, "task_prompt": ""}
{"id": "2cc33e92-68b2-4759-bda0-8bb1129553fd", "fitness": 0.4705567251248352, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — combines randomized directional local searches, orthogonal refinements, adaptive step-sizes and occasional heavy-tailed (Lévy-like) jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size; if None it's auto-scaled with dim\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        if pop_size is None:\n            # population grows slowly with dimension but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # algorithm hyper-parameters (sensible defaults, adaptive in run)\n        self.min_sigma = 1e-12\n        self.succ_inc = 1.12     # increase sigma on success\n        self.fail_dec = 0.90     # decrease sigma on failure\n        self.max_levy_prob = 0.06  # chance to try heavy-tailed jump per iteration\n        self.orth_step_frac = 0.5  # fraction of step used for orthogonal tries\n        self.rejuvenation_prob = 0.03  # chance to replace worst by random\n        self.tournament_k = 2    # tournament size for parent selection\n\n    def __call__(self, func):\n        # prepare bounds (allow scalars or arrays)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        # broadcast to full-dim arrays if necessary\n        if lb.size == 1:\n            lb = np.ones(self.dim) * lb.item()\n        if ub.size == 1:\n            ub = np.ones(self.dim) * ub.item()\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # internal counters and best trackers\n        eval_count = 0\n        f_best = np.inf\n        x_best = None\n\n        # helper: clipped evaluation with budget tracking and best update\n        def callf(x):\n            nonlocal eval_count, f_best, x_best\n            if eval_count >= self.budget:\n                return None, None\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            eval_count += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x\n\n        remaining = lambda: max(0, self.budget - eval_count)\n\n        # If budget is extremely small, do random search\n        if self.budget <= 5 or remaining() <= 0:\n            # tiny random search\n            while remaining() > 0:\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return f_best, x_best\n\n        # Initialize population with random points\n        n_init = min(self.pop_size, remaining())\n        pop = np.zeros((n_init, self.dim))\n        pop_f = np.zeros(n_init)\n        pop_sigma = np.zeros(n_init)\n        # initial scale based on search range\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n        for i in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f, x = callf(x)\n            if f is None:\n                break\n            pop[i] = x\n            pop_f[i] = f\n            pop_sigma[i] = base_sigma * (1.0 + 0.3 * self.rng.randn())\n            # keep sigma positive\n            pop_sigma[i] = max(pop_sigma[i], base_sigma * 0.2)\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0 or remaining() <= 0:\n            while remaining() > 0:\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return f_best, x_best\n\n        # Track per-parent consecutive failures to adapt sigma\n        fail_counts = np.zeros(len(pop), dtype=int)\n\n        # Main loop\n        while remaining() > 0:\n            # recompute best/worst indices\n            best_i = int(np.argmin(pop_f))\n            worst_i = int(np.argmax(pop_f))\n            # pick a parent via small tournament to balance exploration/exploitation\n            if len(pop) >= self.tournament_k and self.tournament_k > 1:\n                choices = self.rng.choice(len(pop), size=self.tournament_k, replace=False)\n                parent_i = int(choices[np.argmin(pop_f[choices])])\n            else:\n                parent_i = self.rng.randint(len(pop))\n            x_parent = pop[parent_i].copy()\n            sigma = max(pop_sigma[parent_i], self.min_sigma)\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())  # allow some randomness\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try is None:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept, slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = max(sigma * self.succ_inc, self.min_sigma)\n                fail_counts[parent_i] = 0\n                # if this is a new global best, attempt a small local refinement\n                # small gaussian around x_try\n                if f_try < f_best:\n                    for _ in range(min(2, remaining())):\n                        x_local = np.clip(x_try + 0.1 * sigma * self.rng.randn(self.dim), lb, ub)\n                        f_loc, x_local = callf(x_local)\n                        if f_loc is None:\n                            break\n                        if f_loc < pop_f[parent_i]:\n                            pop[parent_i] = x_local\n                            pop_f[parent_i] = f_loc\n                            pop_sigma[parent_i] = max(pop_sigma[parent_i] * self.succ_inc, self.min_sigma)\n                            x_try = x_local\n                            f_try = f_loc\n                # continue main loop\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            # try smaller fractional steps along -d and +d\n            backtrack_factors = [0.5, 0.25, 0.125]\n            for bf in backtrack_factors:\n                if remaining() <= 0:\n                    break\n                x_bt = np.clip(x_parent + (alpha * bf) * d, lb, ub)\n                f_bt, x_bt = callf(x_bt)\n                if f_bt is None:\n                    break\n                if f_bt < pop_f[parent_i]:\n                    pop[parent_i] = x_bt\n                    pop_f[parent_i] = f_bt\n                    pop_sigma[parent_i] = max(sigma * self.succ_inc, self.min_sigma)\n                    fail_counts[parent_i] = 0\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining() > 0:\n                # create a vector orthogonal to d by subtracting projection\n                v = self.rng.randn(self.dim)\n                v = v - (v.dot(d)) * d\n                nv = np.linalg.norm(v)\n                if nv > 0:\n                    v = v / nv\n                    x_ort = np.clip(x_parent + self.orth_step_frac * alpha * v, lb, ub)\n                    f_ort, x_ort = callf(x_ort)\n                    if f_ort is None:\n                        break\n                    if f_ort < pop_f[parent_i]:\n                        pop[parent_i] = x_ort\n                        pop_f[parent_i] = f_ort\n                        pop_sigma[parent_i] = max(sigma * self.succ_inc, self.min_sigma)\n                        fail_counts[parent_i] = 0\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < self.max_levy_prob and remaining() > 0:\n                # Cauchy-like heavy-tailed vector (ratio of normals) scaled by robust scale\n                # We construct a vector with Cauchy marginals and normalize to avoid astronomical steps\n                cauchy_vec = self.rng.randn(self.dim) / (self.rng.randn(self.dim) + 1e-12)\n                # robust scale: median absolute deviation-ish from current population\n                if len(pop) >= 2:\n                    scale = max(base_sigma, np.median(np.abs(pop - pop.mean(axis=0))))\n                else:\n                    scale = base_sigma\n                # normalize then multiply by scale*multiplier\n                nv = np.linalg.norm(cauchy_vec)\n                if nv == 0:\n                    levy_dir = np.ones(self.dim) / np.sqrt(self.dim)\n                else:\n                    levy_dir = cauchy_vec / nv\n                leap_scale = scale * (5.0 + 10.0 * self.rng.rand())  # broad leaps\n                x_leap = np.clip(x_parent + leap_scale * levy_dir, lb, ub)\n                f_leap, x_leap = callf(x_leap)\n                if f_leap is None:\n                    break\n                # if it's good replace the worst in population, else maybe adopt as new candidate with slightly reduced sigma\n                if f_leap < pop_f[worst_i]:\n                    pop[worst_i] = x_leap\n                    pop_f[worst_i] = f_leap\n                    pop_sigma[worst_i] = max(scale * 0.5, self.min_sigma)\n                else:\n                    # keep as candidate by replacing parent with low probability\n                    if f_leap < pop_f[parent_i] or self.rng.rand() < 0.05:\n                        pop[parent_i] = x_leap\n                        pop_f[parent_i] = f_leap\n                        pop_sigma[parent_i] = max(scale * 0.3, self.min_sigma)\n                # continue main loop after attempted jump\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2:\n                # pick two best\n                best_two = np.argsort(pop_f)[:2]\n                a, b = best_two[0], best_two[1]\n                mix = 0.6 + 0.2 * self.rng.rand()\n                child = np.clip(mix * pop[a] + (1.0 - mix) * pop[b] + 0.02 * sigma * self.rng.randn(self.dim), lb, ub)\n                f_child, child = callf(child)\n                if f_child is None:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_child < pop_f[parent_i]:\n                    pop[parent_i] = child\n                    pop_f[parent_i] = f_child\n                    pop_sigma[parent_i] = max(sigma * self.succ_inc, self.min_sigma)\n                    fail_counts[parent_i] = 0\n                elif f_child < pop_f[worst_i]:\n                    pop[worst_i] = child\n                    pop_f[worst_i] = f_child\n                    pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure\n            fail_counts[parent_i] += 1\n            if fail_counts[parent_i] > 6:\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * self.fail_dec, self.min_sigma)\n                fail_counts[parent_i] = 0  # reset after shrink\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < self.rejuvenation_prob and remaining() > 0:\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new is None:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * 0.5\n\n        # finished budget or no remaining evaluations\n        return f_best, x_best", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.471 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15769074073358536, 0.1590856209485665, 0.5714546826269213, 0.9796844184316369, 0.6130478420047957, 0.7054484005081316, 0.21584960465926906, 0.5587159863686064, 0.6095887408516729, 0.13500121411516652]}, "task_prompt": ""}
{"id": "a7f5c3cd-9d68-4dc7-9c35-b4bcf13e3222", "fitness": 0.49022919494623807, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that combine randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy-like jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to small function of dim)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimension but stays small\n            self.pop_size = max(4, min(40, 4 + int(1.5 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        # Determine bounds; some test harnesses provide func.bounds.lb / ub.\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            # Default to problem statement bounds\n            lb = -5.0 * np.ones(self.dim, dtype=float)\n            ub = 5.0 * np.ones(self.dim, dtype=float)\n\n        # Ensure full-dim bounds\n        if lb.ndim == 0:\n            lb = np.full(self.dim, lb, dtype=float)\n        if ub.ndim == 0:\n            ub = np.full(self.dim, ub, dtype=float)\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # helper to evaluate while tracking budget and best\n        evals_used = 0\n\n        def callf(x):\n            nonlocal evals_used, lb, ub\n            # ensure x is numpy array of correct shape\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            if evals_used >= self.budget:\n                # budget exhausted, don't call func\n                return np.inf, x\n            f = func(x)\n            evals_used += 1\n            # update global best\n            nonlocal_f_opt = None\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is extremely small, fallback to random search\n        if self.budget <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop_size = min(self.pop_size, max(1, self.budget))  # don't create more individuals than budget\n        pop_x = np.empty((pop_size, self.dim), dtype=float)\n        pop_f = np.full(pop_size, np.inf, dtype=float)\n        pop_sigma = np.empty(pop_size, dtype=float)\n        # initial sigma scale relative to search-space\n        span = ub - lb\n        global_sigma0 = 0.2 * np.linalg.norm(span) / np.sqrt(self.dim)  # a problem-scaled starting sigma\n\n        # Fill initial population (evaluate as many as budget allows)\n        for i in range(pop_size):\n            if evals_used >= self.budget:\n                break\n            x0 = rng.uniform(lb, ub)\n            f0, x0_clipped = callf(x0)\n            pop_x[i] = x0_clipped\n            pop_f[i] = f0\n            # randomized initial sigma per individual\n            pop_sigma[i] = global_sigma0 * (0.5 + rng.random())  # between 0.5 and 1.5 * global_sigma0\n\n        # If no population could be created (extremely small budget), do pure random search\n        if np.isinf(pop_f).all():\n            # try random sampling until budget expended\n            while evals_used < self.budget:\n                x = rng.uniform(lb, ub)\n                f, xclip = callf(x)\n            return self.f_opt, self.x_opt\n\n        # If some individuals were not filled because of low budget, shrink arrays\n        valid = ~np.isinf(pop_f)\n        pop_x = pop_x[valid]\n        pop_f = pop_f[valid]\n        pop_sigma = pop_sigma[valid]\n        pop_size = pop_x.shape[0]\n\n        # book-keeping: failure counters for sigma adaptation\n        fail_counts = np.zeros(pop_size, dtype=int)\n\n        # Main loop\n        # probabilities and schedules\n        levy_prob = 0.06\n        rejuvenate_prob = 0.03\n        orth_prob = 0.35\n        backtrack_tries = 3\n        max_iters = 10**9  # we will break by budget\n        iter_count = 0\n\n        while evals_used < self.budget and iter_count < max_iters:\n            iter_count += 1\n\n            # pick a parent via small tournament to balance exploration/exploitation\n            # tournament of 3\n            if pop_size >= 3:\n                inds = rng.choice(pop_size, 3, replace=False)\n                parent_idx = inds[np.argmin(pop_f[inds])]\n            else:\n                parent_idx = 0\n\n            x_parent = pop_x[parent_idx].copy()\n            f_parent = pop_f[parent_idx]\n            sigma = pop_sigma[parent_idx]\n\n            # sample a random search direction (normalized)\n            d = rng.normal(size=self.dim)\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                d = np.ones(self.dim)\n                dn = np.linalg.norm(d)\n            d = d / dn\n\n            # primary directional trial with stochasticized step-length\n            step_scale = sigma * (0.8 + rng.random() * 1.6)  # randomize a bit\n            step = d * (rng.standard_normal() * step_scale)\n            x_trial = np.minimum(np.maximum(x_parent + step, lb), ub)\n            if evals_used < self.budget:\n                f_trial, x_trial = callf(x_trial)\n            else:\n                break\n\n            if f_trial < f_parent:\n                # success: accept and slightly increase sigma\n                pop_x[parent_idx] = x_trial\n                pop_f[parent_idx] = f_trial\n                pop_sigma[parent_idx] = min(pop_sigma[parent_idx] * 1.08, np.linalg.norm(span))\n                fail_counts[parent_idx] = 0\n                f_parent = f_trial\n                x_parent = x_trial.copy()\n            else:\n                # unsuccessful: local backtracking / small-step refinement along direction (few tries)\n                improved = False\n                small_sigma = sigma * 0.5\n                for k in range(backtrack_tries):\n                    if evals_used >= self.budget:\n                        break\n                    frac = 0.5 ** (k + 1)\n                    step_k = d * (np.sign(rng.standard_normal()) * small_sigma * frac)\n                    x_k = np.minimum(np.maximum(x_parent + step_k, lb), ub)\n                    f_k, x_k = callf(x_k)\n                    if f_k < f_parent:\n                        pop_x[parent_idx] = x_k\n                        pop_f[parent_idx] = f_k\n                        pop_sigma[parent_idx] = min(pop_sigma[parent_idx] * 1.06, np.linalg.norm(span))\n                        fail_counts[parent_idx] = 0\n                        improved = True\n                        f_parent = f_k\n                        x_parent = x_k.copy()\n                        break\n                if not improved:\n                    # increase failure and shrink sigma modestly\n                    fail_counts[parent_idx] += 1\n                    pop_sigma[parent_idx] = max(pop_sigma[parent_idx] * 0.92, 1e-12)\n\n            # try an orthogonal perturbation for local diversification (with some probability)\n            if rng.random() < orth_prob and evals_used < self.budget:\n                # create vector orthogonal to d\n                v = rng.normal(size=self.dim)\n                v = v - np.dot(v, d) * d\n                vn = np.linalg.norm(v)\n                if vn > 1e-12:\n                    v = v / vn\n                    step_o = v * (pop_sigma[parent_idx] * (0.4 + rng.random() * 0.8) * rng.standard_normal())\n                    x_o = np.minimum(np.maximum(x_parent + step_o, lb), ub)\n                    if evals_used < self.budget:\n                        f_o, x_o = callf(x_o)\n                        # accept only if better; else ignore\n                        if f_o < pop_f[parent_idx]:\n                            pop_x[parent_idx] = x_o\n                            pop_f[parent_idx] = f_o\n                            pop_sigma[parent_idx] = min(pop_sigma[parent_idx] * 1.04, np.linalg.norm(span))\n                            fail_counts[parent_idx] = 0\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if rng.random() < levy_prob and evals_used < self.budget:\n                # Cauchy-like heavy-tailed vector\n                c = rng.standard_cauchy(self.dim)\n                # clip extremes to avoid overflow but keep heavy-tail\n                c = np.clip(c, -50, 50)\n                # robust scale: use median absolute deviation-like factor\n                mad = np.median(np.abs(c - np.median(c))) + 1e-9\n                scale = max(1.0, mad)\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                c_norm = np.linalg.norm(c)\n                if c_norm == 0:\n                    c = rng.normal(size=self.dim)\n                    c_norm = np.linalg.norm(c)\n                delta = c / c_norm * (0.4 * np.linalg.norm(span) * (1.0 + rng.exponential()))\n                # create jump from a random population member (not always parent)\n                jump_base_idx = rng.integers(pop_size)\n                x_jump_base = pop_x[jump_base_idx]\n                x_jump = np.minimum(np.maximum(x_jump_base + delta, lb), ub)\n                if evals_used < self.budget:\n                    f_jump, x_jump = callf(x_jump)\n                else:\n                    break\n                if f_jump < pop_f.min():\n                    # if jump is very good, replace the worst in population\n                    worst = np.argmax(pop_f)\n                    pop_x[worst] = x_jump\n                    pop_f[worst] = f_jump\n                    pop_sigma[worst] = global_sigma0 * (0.8 + rng.random() * 0.8)\n                    fail_counts[worst] = 0\n                else:\n                    # occasionally keep the jump as a candidate by replacing a random poor individual\n                    if rng.random() < 0.25:\n                        worst = np.argmax(pop_f)\n                        if f_jump < pop_f[worst]:\n                            pop_x[worst] = x_jump\n                            pop_f[worst] = f_jump\n                            pop_sigma[worst] = global_sigma0 * (0.6 + rng.random() * 0.9)\n                            fail_counts[worst] = 0\n\n            if evals_used >= self.budget:\n                break\n\n            # recombination exploitation: mix two best and small noise\n            if pop_size >= 2 and evals_used < self.budget:\n                best_idx = np.argmin(pop_f)\n                # pick second best distinct\n                others = [i for i in range(pop_size) if i != best_idx]\n                if others:\n                    second_idx = min(others, key=lambda i: pop_f[i])\n                    x1 = pop_x[best_idx]\n                    x2 = pop_x[second_idx]\n                    alpha = rng.random() * 1.2 - 0.1  # allow slight extrapolation\n                    child = x1 + alpha * (x2 - x1) + rng.normal(scale=0.02 * np.linalg.norm(span), size=self.dim)\n                    child = np.minimum(np.maximum(child, lb), ub)\n                    if evals_used < self.budget:\n                        f_child, child = callf(child)\n                    else:\n                        break\n                    if f_child < pop_f[parent_idx]:\n                        pop_x[parent_idx] = child\n                        pop_f[parent_idx] = f_child\n                        pop_sigma[parent_idx] = max(pop_sigma[parent_idx] * 1.03, 1e-12)\n                        fail_counts[parent_idx] = 0\n                    else:\n                        # maybe replace the worst\n                        worst = np.argmax(pop_f)\n                        if f_child < pop_f[worst]:\n                            pop_x[worst] = child\n                            pop_f[worst] = f_child\n                            pop_sigma[worst] = global_sigma0 * (0.6 + rng.random())\n\n            if evals_used >= self.budget:\n                break\n\n            # adapt parent sigma on sustained failure\n            if fail_counts[parent_idx] >= 3:\n                pop_sigma[parent_idx] = max(pop_sigma[parent_idx] * 0.7, 1e-12)\n                fail_counts[parent_idx] = 0  # reset after a shrink\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if rng.random() < rejuvenate_prob and evals_used < self.budget:\n                worst = np.argmax(pop_f)\n                x_new = rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst]:\n                    pop_x[worst] = x_new\n                    pop_f[worst] = f_new\n                    pop_sigma[worst] = global_sigma0 * (0.5 + rng.random())\n\n            # small sanity: if population has collapsed (all identical), introduce diversity\n            if evals_used < self.budget:\n                if np.allclose(pop_x, pop_x[0], atol=1e-12):\n                    idx = rng.integers(pop_size)\n                    x_new = rng.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    if f_new < pop_f[idx]:\n                        pop_x[idx] = x_new\n                        pop_f[idx] = f_new\n                        pop_sigma[idx] = global_sigma0 * (0.6 + rng.random())\n\n            # if budget nearly exhausted, will naturally exit the loop\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.490 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17493602409630604, 0.16672998409362894, 0.7182769575556696, 0.9492662686612923, 0.35633449304225373, 0.9339588956367895, 0.2839586500912902, 0.3616714120189015, 0.7815508472526409, 0.17560841701360785]}, "task_prompt": ""}
{"id": "1205ac46-ff73-4100-ae9f-dec8846d8295", "fitness": 0.4267044304106314, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional search with orthogonal refinements and occasional Lévy jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (autocalculated if None)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # use a RandomState for reproducibility across numpy versions\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # convert bounds to arrays of correct dimension\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # safety: ensure lb and ub shapes\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            # must be called only when remaining > 0\n            if remaining <= 0:\n                raise RuntimeError(\"No remaining budget for function evaluations\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, fallback to pure random sampling\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []         # list of np.array points\n        pop_f = []       # list of fitness values\n        pop_sigma = []   # adaptive step sizes per individual\n\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale\n\n        # number of individuals to try initialize (bounded by budget)\n        n_init = min(self.pop_size, max(2, remaining))\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # small randomized diversity in sigma\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n\n        # If no population could be created (very small budget), finish with best found\n        if len(pop) == 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_local = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_local, x_local = callf(x_local)\n                if f_local < pop_f[parent_i]:\n                    pop[parent_i] = x_local\n                    pop_f[parent_i] = f_local\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_ort = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining <= 0:\n                    break\n                f_ort, x_ort = callf(x_ort)\n                if f_ort < pop_f[parent_i]:\n                    pop[parent_i] = x_ort\n                    pop_f[parent_i] = f_ort\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            # trigger with moderate probability to keep search balanced\n            if self.rng.rand() < 0.25 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = self.rng.standard_cauchy(self.dim)\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_jump = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining <= 0:\n                    break\n                f_jump, x_jump = callf(x_jump)\n                # if it's good replace the worst in population\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * self.rng.randn(self.dim)\n                x_mix = np.clip(mix + noise, lb, ub)\n                if remaining <= 0:\n                    break\n                f_mix, x_mix = callf(x_mix)\n                # replace parent if improved, else possibly replace worst\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                if remaining <= 0:\n                    break\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.427 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11232737397174175, 0.17628939817053835, 0.7506141569029947, 0.9589560031627796, 0.26635279073514884, 0.9521547806444615, 0.27130229743493806, 0.3806406278292648, 0.24308410817354964, 0.15532276708089643]}, "task_prompt": ""}
{"id": "b65727f7-00ec-4596-81b1-8216428fd343", "fitness": 0.40603996749590454, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional local search with orthogonal refinements and occasional heavy-tailed Lévy jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a small function of dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # get bounds and ensure full-dim arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # safety: ensure shapes match self.dim\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.resize(lb, self.dim)\n            ub = np.resize(ub, self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # evaluation wrapper that tracks budget and global best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, do pure random sampling until budget exhausted\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))  # initial scale\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # individual adaptive step-size\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * np.random.rand()))\n\n        # If no population could be created (very small budget), continue random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            # choose best among tournament\n            best_in_tourn = inds[int(np.argmin(pop_f[inds]))]\n            parent_i = int(best_in_tourn)\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n            worst_i = int(np.argmax(pop_f))\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = max(sigma * 1.10, 1e-12)\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # remove component parallel to d to get approximate orthogonal\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                # try a smaller step orthogonal to the direction\n                orth_step = max(0.5 * sigma, 1e-12)\n                x_try = np.clip(x_parent + orth_step * r, lb, ub)\n                if remaining <= 0:\n                    break\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector; robust-normalize to avoid extreme blow-ups\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # if it's good replace the worst in population, else maybe keep as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    worst_i = int(np.argmax(pop_f))\n                    # replace parent if improved\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        # try to inject into population by replacing the worst if it's better\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5\n                    else:\n                        # maybe replace worst if better\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (contraction)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.04 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # if population has shrunk somehow, ensure arrays consistent\n            # (this should not normally happen, but keep robustness)\n            if not isinstance(pop, np.ndarray):\n                pop = np.array(pop)\n            if not isinstance(pop_f, np.ndarray):\n                pop_f = np.array(pop_f)\n            if not isinstance(pop_sigma, np.ndarray):\n                pop_sigma = np.array(pop_sigma)\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.406 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12428376275243458, 0.15458185132292335, 0.4846338971256926, 0.9649336019612076, 0.6618057833356683, 0.5854279618734217, 0.2548647752726376, 0.41093670711492847, 0.23152440702542354, 0.18740692717470764]}, "task_prompt": ""}
{"id": "fb12f745-639d-4a3c-b768-d81f1ac85630", "fitness": 0.4809318691772727, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed (Lévy/Cauchy-like) jumps to balance robust global exploration with focused local exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # moderately sized population: grows with dim but not too large\n            self.pop_size = max(6, min(40, 4 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read and normalize bounds to arrays of length dim\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # safety: clip bounds to expected [-inf, +inf] if user provided weird\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to call func while tracking budget and best-so-far\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                # budget exhausted: return current best\n                return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n            x = np.asarray(x, dtype=float)\n            # ensure correct shape and clip to bounds\n            if x.shape != (self.dim,):\n                x = x.ravel()[:self.dim]\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is trivially small, just random-sample until exhausted\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base scale ~ quarter of domain or small if domain is tiny\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            if x0 is None:\n                break\n            pop.append(x0.copy())\n            pop_f.append(float(f0))\n            pop_sigma.append(base_sigma * (1.0 + 0.3 * np.random.randn()))\n            # ensure non-negative sigma\n            pop_sigma[-1] = max(1e-12, abs(pop_sigma[-1]))\n\n        # If no candidate could be evaluated, return current best\n        if len(pop) == 0:\n            return self.f_opt, self.x_opt\n\n        # If we have fewer samples than requested population, shrink pop_size\n        pop_size = len(pop)\n        pop = pop[:pop_size]\n        pop_f = pop_f[:pop_size]\n        pop_sigma = pop_sigma[:pop_size]\n\n        # Main optimization loop\n        # We'll perform a mixture of directional local searches, orthogonal tries, recombination and occasional heavy-tailed jumps.\n        while remaining > 0:\n            # small tournament selection among population indices\n            k = min(3, pop_size)\n            inds = np.random.choice(pop_size, k, replace=False)\n            values = [pop_f[i] for i in inds]\n            # choose the index of the best among the sampled tournament\n            rel_best = int(np.argmin(values))\n            parent_i = int(inds[rel_best])\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # 1) Directional trial\n            # pick a random direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            # stochasticized step-length around sigma\n            alpha = sigma * max(1e-12, (1.0 + 0.4 * np.random.randn()))\n            x_try = x_parent + alpha * d\n            x_try = np.clip(x_try, lb, ub)\n            f_try, x_try = callf(x_try)\n            if x_try is None:\n                break  # budget exhausted\n\n            if f_try < pop_f[parent_i]:\n                # success: accept, increase sigma modestly\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15 + 1e-12, max(1e-1 * base_sigma, sigma * 2.0))\n                continue\n\n            # 2) Local backtracking: reduce step-length a few times\n            improved = False\n            backtrack_steps = 3\n            alpha_bt = alpha\n            for bt in range(backtrack_steps):\n                alpha_bt *= 0.5\n                x_try = x_parent + alpha_bt * d\n                x_try = np.clip(x_try, lb, ub)\n                f_try, x_try = callf(x_try)\n                if x_try is None:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 1.05, 1e-12)\n                    improved = True\n                    break\n            if x_try is None:\n                break\n            if improved:\n                continue\n\n            # 3) Orthogonal refinement: try small perturbation orthogonal to d\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                orth_scale = 0.6 * sigma\n                x_try = x_parent + orth_scale * r * (1.0 + 0.3 * np.random.randn())\n                x_try = np.clip(x_try, lb, ub)\n                f_try, x_try = callf(x_try)\n                if x_try is None:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 1.08, 1e-12)\n                    continue\n\n            # 4) Occasional Lévy-like heavy-tailed jump to escape basins\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like (heavy tailed) vector scaled by robust estimate of sigma\n                # generate independent Cauchy samples and normalize direction\n                delta = np.tan(np.pi * (np.random.rand(self.dim) - 0.5))  # standard Cauchy\n                ndelta = np.linalg.norm(delta)\n                if ndelta < 1e-12:\n                    delta = np.random.randn(self.dim)\n                    ndelta = np.linalg.norm(delta)\n                delta = delta / ndelta\n                scale_jump = max(0.5, 8.0 * sigma)  # allow big jumps but clipped by domain\n                x_try = x_parent + scale_jump * delta\n                x_try = np.clip(x_try, lb, ub)\n                f_try, x_try = callf(x_try)\n                if x_try is None:\n                    break\n                # If jump found a solution better than worst, replace worst; otherwise maybe keep as new candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try.copy()\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(sigma * 0.8, 1e-12)\n                # always slightly reduce parent's sigma on failure to avoid repeatedly trying same move\n                pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n                continue  # after jump, move to next iteration\n\n            # 5) Recombination exploitation: mix two best individuals and add small noise\n            # choose two best indices\n            sorted_idx = np.argsort(pop_f)\n            a = int(sorted_idx[0])\n            b = int(sorted_idx[1]) if len(sorted_idx) > 1 else a\n            beta = np.random.rand()\n            mix = beta * pop[a] + (1.0 - beta) * pop[b]\n            noise = 0.02 * (ub - lb) * np.random.randn(self.dim)\n            x_try = np.clip(mix + noise, lb, ub)\n            f_try, x_try = callf(x_try)\n            if x_try is None:\n                break\n            if f_try < pop_f[parent_i]:\n                # improved relative to parent\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n            else:\n                # possibly replace worst if it's better\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try.copy()\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent's sigma on failure\n            pop_sigma[parent_i] = max(sigma * 0.87, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a fresh random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if x_new is None:\n                    break\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.8 + 0.4 * np.random.rand())\n\n            # if population shrank (should not normally happen) ensure we keep sizes consistent\n            # (we avoid removing individuals in this implementation)\n\n        # finished or budget exhausted\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.481 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1432991133245154, 0.16315560361328263, 0.5995388201035365, 0.9499432438772277, 0.513979124412411, 0.8439028142979592, 0.28011654455002033, 0.4266648962268593, 0.7274307528930417, 0.16128777847387332]}, "task_prompt": ""}
{"id": "af90957f-d97e-46a5-a385-68a4c866c422", "fitness": 0.29339192245692153, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that uses randomized directional local searches, orthogonal refinements and occasional Lévy jumps to explore and exploit the continuous search space.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to small multiple of dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population scales with dimensionality but stays modest\n        if pop_size is None:\n            self.pop_size = max(6, min(40, int(3 * np.sqrt(max(1, self.dim)))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed) if seed is not None else np.random\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # get bounds and ensure correct shape\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # safety: enforce shapes\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                # no remaining evaluations; return current best as a fallback\n                return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Base scale and initial sigma heuristics\n        span = ub - lb\n        base_sigma = 0.08 * np.maximum(1e-9, np.linalg.norm(span) / np.sqrt(max(1, self.dim)))\n        base_sigma = max(base_sigma, 1e-6)\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        for i in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # initialize sigma around base_sigma scaled by uniform factor\n            pop_sigma.append(base_sigma * (0.5 + self.rng.rand()))\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        # parameters\n        levy_prob = 0.08\n        reinject_prob = 0.02\n        recomb_prob = 0.12\n        tournament_k = 3\n        frac_list = (0.5, 0.25, -0.5, -0.25)\n\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(tournament_k, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # stochastic step-length around sigma (log-normal jitter)\n            step_scale = sigma * np.exp(0.5 * self.rng.randn())  # multiplicative jitter\n            alpha = step_scale\n\n            # primary directional trial with stochasticized step-length\n            if remaining <= 0:\n                break\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(span))\n                improved = True\n                # continue to next iteration to exploit\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            for frac in frac_list:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            # create a random vector then orthogonalize relative to d\n            r = self.rng.randn(self.dim)\n            # remove component along d to make it orthogonal\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r) + 1e-12\n            r = r / nr\n            # small orthogonal step\n            ortho_scale = 0.6 * sigma\n            if remaining > 0:\n                x_try = np.clip(x_parent + ortho_scale * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    improved = True\n            if improved:\n                continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < levy_prob and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = self.rng.standard_cauchy(size=self.dim)\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                step = step / (np.linalg.norm(step) + 1e-12)\n                # scale relative to span and a random heavy step multiplier\n                levy_scale = np.clip(0.2 + 2.0 * self.rng.rand(), 0.1, 3.0)\n                x_levy = np.clip(x_parent + levy_scale * np.mean(span) * step, lb, ub)\n                f_levy, x_levy = callf(x_levy)\n                # if it's good replace the worst in population\n                worst_i = int(np.argmax(pop_f))\n                if f_levy < pop_f[worst_i]:\n                    pop[worst_i] = x_levy.copy()\n                    pop_f[worst_i] = f_levy\n                    pop_sigma[worst_i] = base_sigma * 0.5\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise (with some probability)\n            if self.rng.rand() < recomb_prob and remaining > 0 and len(pop) >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = 0.6 + 0.4 * self.rng.rand()  # biased towards best\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.02 * span) * self.rng.randn(self.dim)\n                x_recomb = np.clip(mix + noise, lb, ub)\n                f_recomb, x_recomb = callf(x_recomb)\n                # replace parent if improved, else possibly replace worst\n                if f_recomb < pop_f[parent_i]:\n                    pop[parent_i] = x_recomb.copy()\n                    pop_f[parent_i] = f_recomb\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_recomb < pop_f[worst_i]:\n                        pop[worst_i] = x_recomb.copy()\n                        pop_f[worst_i] = f_recomb\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                    continue\n                else:\n                    # try replacing worst if recomb is good relative to worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_recomb < pop_f[worst_i]:\n                        pop[worst_i] = x_recomb.copy()\n                        pop_f[worst_i] = f_recomb\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                        continue\n\n            # adapt parent sigma on failure (shrink a bit)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < reinject_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.5 + self.rng.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.293 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10882534111996678, 0.16293639237207613, 0.4885925808077074, 0.16411016742119322, 0.19891040836640106, 0.73619081214094, 0.2144214135786503, 0.4482186087942849, 0.2547447228462204, 0.1569687771217756]}, "task_prompt": ""}
{"id": "10d359ba-d7d2-4c23-b238-a19ca4e3b71a", "fitness": 0.5220004564967291, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive directional local searches with orthogonal refinements and occasional heavy-tailed Lévy jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults based on dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaled with dimensionality but capped\n            self.pop_size = max(4, min(20, 4 + int(self.dim // 1)))\n        else:\n            self.pop_size = int(max(2, pop_size))\n        self.seed = None if seed is None else int(seed)\n        # internal RNG\n        self._rng = np.random.RandomState(self.seed)\n\n    def __call__(self, func):\n        # prepare bounds (func.bounds.lb / ub may be scalars or arrays)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        assert lb.shape == (self.dim,) and ub.shape == (self.dim,), \"Bounds must match dimensionality\"\n\n        remaining = int(self.budget)\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        # helper evaluator that tracks remaining budget and updates best found\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            # clip to bounds before calling\n            x = np.minimum(np.maximum(x, lb), ub)\n            # consume budget and evaluate\n            remaining -= 1\n            f = float(func(x))\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                # keep a copy\n                self.x_opt = x.copy()\n            return f, x\n\n        # fallback: if zero budget, return immediately\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population uniformly in bounds\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        # base scale relative to search range\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            x0 = self._rng.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize per-individual step sizes with small randomization\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * self._rng.rand()))\n\n        # if we couldn't evaluate any point (very low budget), do random best-try with remaining budget\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self._rng.uniform(lb, ub)\n                try:\n                    f, x = callf(x)\n                except RuntimeError:\n                    break\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # ensure population arrays are numpy friendly\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = self._rng.choice(len(pop), k, replace=False)\n            # choose the best among the tournament as parent\n            parent_i = inds[int(np.argmin([pop_f[i] for i in inds]))]\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            improved = False\n\n            # sample a random search direction (normalized)\n            d = self._rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self._rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            step_scale = sigma * (0.6 + 1.2 * self._rng.rand())  # randomize step-length around sigma\n            x_try = np.minimum(np.maximum(x_parent + step_scale * d, lb), ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and increase sigma moderately\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.25, np.mean(ub - lb))\n                improved = True\n\n            # local backtracking / small-step refinement along direction (few tries)\n            if not improved:\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    if remaining <= 0:\n                        break\n                    x_try = np.minimum(np.maximum(x_parent + frac * step_scale * d, lb), ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        # slightly reduce sigma to refine\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        improved = True\n                        break\n\n            # try an orthogonal perturbation for local diversification\n            if not improved and remaining > 0:\n                r = self._rng.randn(self.dim)\n                # make r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = np.minimum(np.maximum(x_parent + 0.6 * sigma * r, lb), ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        improved = True\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and self._rng.rand() < 0.08:\n                step = self._rng.standard_cauchy(self.dim)\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale relative to overall range and individual sigma\n                scale = 0.6 * np.mean(ub - lb) * min(1.0, sigma / (np.mean(pop_sigma) + 1e-12))\n                x_try = np.minimum(np.maximum(x_parent + step * scale, lb), ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else maybe keep it as candidate probabilistically\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.8 + 0.6 * self._rng.rand())\n                else:\n                    # with small probability keep it (replace a random worse individual) to diversify\n                    if self._rng.rand() < 0.06:\n                        # pick a random index (not the current parent) to replace\n                        idxs = [i for i in range(len(pop)) if i != parent_i]\n                        if idxs:\n                            rep = self._rng.choice(idxs)\n                            pop[rep] = x_try\n                            pop_f[rep] = f_try\n                            pop_sigma[rep] = base_sigma * (0.6 + 0.8 * self._rng.rand())\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                # pick two best distinct indices\n                sorted_inds = np.argsort(pop_f)\n                a, b = int(sorted_inds[0]), int(sorted_inds[1])\n                beta = self._rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = 0.01 * (ub - lb) * self._rng.randn(self.dim)\n                x_try = np.minimum(np.maximum(mix + noise, lb), ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    improved = True\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure\n            if not improved:\n                # slowly reduce sigma to encourage local refinement\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.96, 1e-12)\n            else:\n                # successful parent gets slight sigma increase (handled at acceptance points),\n                # add a mild global diversity boost occasionally\n                if self._rng.rand() < 0.01 and remaining > 0:\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = self._rng.uniform(lb, ub)\n                    try:\n                        f_new, x_new = callf(x_new)\n                        pop[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self._rng.rand())\n                    except RuntimeError:\n                        break\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self._rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self._rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self._rng.rand())\n                except RuntimeError:\n                    break\n\n        # finished budget or exhausted\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.522 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1420619034899212, 0.16357640461235012, 0.763283106919661, 0.9624328581003798, 0.6812292198485117, 0.8574126242868978, 0.2554698907728421, 0.4786740720584267, 0.7617254076116309, 0.15413907726666987]}, "task_prompt": ""}
{"id": "18acc0ea-feb0-49fe-a5a5-f75512db9eba", "fitness": 0.4105030650946505, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — combines adaptive directional local moves, orthogonal refinement, stochastic recombination and occasional Lévy jumps to robustly explore and exploit continuous search spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed for reproducibility\n\n    Main idea:\n      Maintain a modest population of points with adaptive step-size \"sigmas\".\n      Use randomized directional local searches (along normalized random directions),\n      short backtracking refinements, orthogonal perturbations for local diversification,\n      occasional heavy-tailed (Lévy/Cauchy-like) jumps to escape basins, and\n      recombination among good solutions. Sigmas adapt based on successes/failures.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # default population: modestly scales with dim, but not huge\n        if pop_size is None:\n            self.pop_size = max(4, min(50, 4 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        # Bounds: accept scalars or arrays; enforce length self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # initialize trackers\n        remaining = int(self.budget)\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        def callf(x):\n            # Evaluate x (clip to bounds) while tracking remaining budget and best found\n            nonlocal remaining\n            if remaining <= 0:\n                # Should not happen in normal flow; guard anyway\n                return float(np.inf), None\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                # copy to avoid external mutation\n                self.x_opt = x.copy()\n            return f, x\n\n        # If no budget, return immediately\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population (as many as budget allows up to pop_size)\n        n_init = min(self.pop_size, remaining)\n        pop = self.rng.uniform(lb, ub, size=(n_init, self.dim))\n        pop_f = np.empty(n_init, dtype=float)\n        for i in range(n_init):\n            f_i, x_i = callf(pop[i])\n            # store evaluated (clipped) x and f\n            pop[i] = x_i\n            pop_f[i] = f_i\n\n        # If even initialization consumed all budget, return best found\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Per-individual adaptive sigma (as fraction of (ub-lb))\n        base_sigma = 0.2\n        pop_sigma = np.full(n_init, base_sigma, dtype=float)\n\n        # Main optimization loop\n        while remaining > 0:\n            # choose a parent by a tiny tournament selection (favor better individuals)\n            k = min(3, n_init)\n            inds = self.rng.choice(n_init, k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # randomized search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-16:\n                # degenerate direction; skip this iteration\n                # small random jitter instead\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-16\n            d = d / nd\n\n            # PRIMARY directional trial: stochastic step-length along d\n            # step_len drawn from normal scaled by sigma (fraction of range)\n            step_len = self.rng.randn() * sigma\n            step = step_len * (ub - lb) * d  # element-wise scaling by domain size\n            x_try = np.clip(x_parent + step, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept, increase sigma slightly\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(1.0, sigma * 1.12)\n                continue\n            else:\n                # failure: modest shrink\n                pop_sigma[parent_i] = max(1e-12, sigma * 0.96)\n\n            # LOCAL backtracking / small-step refinement along same direction\n            improved = False\n            alpha = 0.5\n            frac = 1.0\n            for _ in range(4):  # few decreasing step tries\n                frac *= alpha\n                x_try = np.clip(x_parent + step * frac, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(1.0, sigma * 1.06)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # ORTHOGONAL perturbation to diversify locally\n            r = self.rng.randn(self.dim)\n            # make orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                # scale orthogonal move by a fraction of domain and current sigma\n                ortho_scale = 0.6 * sigma * (ub - lb)\n                x_try = np.clip(x_parent + r * ortho_scale, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(1.0, sigma * 1.04)\n                        continue\n\n            # OCCASIONAL Lévy-like (Cauchy) jump to escape basins\n            if self.rng.rand() < 0.08 and remaining > 0:\n                step_raw = self.rng.standard_cauchy(size=self.dim)\n                denom = np.median(np.abs(step_raw)) + 1e-12\n                step_norm = step_raw / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step_norm * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if jump is promising replace the worst, else keep population unchanged\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue to next main iteration\n                continue\n\n            # RECOMBINATION exploitation: mix two best individuals and add small noise\n            if self.rng.rand() < 0.20:\n                best_inds = np.argsort(pop_f)[:2]\n                if best_inds.size >= 2:\n                    a, b = best_inds[0], best_inds[1]\n                else:\n                    # fallback: use parent and a random mate\n                    a = parent_i\n                    b = self.rng.randint(n_init)\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = self.rng.randn(self.dim) * (0.05 * (ub - lb))\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(1.0, sigma * 1.03)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                continue\n\n            # ADAPT on failure: slightly reduce sigma of parent to encourage finer search\n            pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.97)\n\n            # OCCASIONAL population rejuvenation: replace worst by a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                if remaining <= 0:\n                    break\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma\n\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.411 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.141310170427698, 0.16659020468496832, 0.5039603979379598, 0.8275173822690051, 0.32706472123146024, 0.7189035593225959, 0.2740749818803102, 0.44007557574212897, 0.5053787007684909, 0.20015495668188743]}, "task_prompt": ""}
{"id": "e280af1a-45fe-45b9-b94e-d4b13f05d940", "fitness": 0.4449643560457348, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based heuristic that mixes directional local searches, orthogonal refinements, recombination and occasional Cauchy/Lévy-style jumps with per-solution adaptive step sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional args:\n      - pop_size: size of population (default scales with dimension)\n      - seed: random seed\n      - verbose: if True prints some progress\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population modestly scales with dimensionality but kept practical\n            self.pop_size = max(4, min(40, 4 + int(self.dim * 1.5)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.verbose = verbose\n\n        # result placeholders\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize `func` using at most self.budget evaluations.\n        `func` is expected to accept a 1-D numpy array of length self.dim and return a scalar.\n        Many BBOB wrappers expose bounds via func.bounds.lb / func.bounds.ub (arrays or scalars).\n        If bounds are missing, we assume the standard [-5, 5]^dim.\n        \"\"\"\n        # extract bounds if available, otherwise default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # Broadcast scalar bounds to vectors\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # Safety: ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # internal budget counter stored in mutable so nested helper can modify it\n        remaining = [int(self.budget)]\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            if remaining[0] <= 0:\n                # budget exhausted; return +inf (should not happen normally)\n                return float(\"inf\"), x\n            f = float(func(x))\n            remaining[0] -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # initialize global best\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        # If budget extremely small, fallback to random search\n        if remaining[0] <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base sigma ~ quarter of domain mean width\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n        n_init = min(self.pop_size, remaining[0])\n        for i in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0c = callf(x0)\n            pop.append(x0c)\n            pop_f.append(f0)\n            # randomized per-individual sigma around base\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * self.rng.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            # Random search until budget exhausted\n            while remaining[0] > 0:\n                xt = self.rng.uniform(lb, ub)\n                f, xt = callf(xt)\n            return self.f_opt, self.x_opt\n\n        # Fill up population if budget remains but initial fill smaller than target\n        while len(pop) < self.pop_size and remaining[0] > 0:\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0c = callf(x0)\n            pop.append(x0c)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * self.rng.rand()))\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop\n        # Parameters controlling behavior\n        levy_prob = 0.08\n        ortho_tries = 3\n        backtrack_fracs = [0.6, 0.3, 0.15]\n        recomb_prob = 0.35\n        tournament_k = 2\n        rejuvenation_prob = 0.02\n\n        while remaining[0] > 0:\n            # pick a parent via small tournament (favor lower f)\n            if len(pop) == 1:\n                parent_i = 0\n            else:\n                # sample k distinct indices\n                candidates = self.rng.choice(len(pop), size=min(tournament_k, len(pop)), replace=False)\n                # pick best among them\n                parent_i = int(min(candidates, key=lambda i: pop_f[i]))\n\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n                nd = 1.0\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length (Cauchy-ish for heavy tails)\n            # Draw a robust Cauchy step length bounded by domain scale\n            # Using standard Cauchy but compressing extremes to avoid numerical blow-ups\n            raw_cauchy = self.rng.standard_cauchy()\n            # cap extreme values\n            raw_cauchy = np.clip(raw_cauchy, -50.0, 50.0)\n            step_scale = sigma * (1.0 + 0.7 * raw_cauchy)  # heavy-tail multiplicative factor\n            # ensure step_scale not insane\n            dom_scale = np.linalg.norm(ub - lb) / np.sqrt(self.dim)\n            step_scale = np.clip(step_scale, -5.0 * dom_scale, 5.0 * dom_scale)\n\n            x_try = np.clip(x_parent + step_scale * d, lb, ub)\n            if remaining[0] <= 0:\n                break\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = max(sigma * (1.0 + 0.08), 1e-12)\n                # quick local refinement: try small backtracks along same direction\n                for frac in [0.5, 0.25]:\n                    if remaining[0] <= 0:\n                        break\n                    xt = np.clip(x_try + 0.5 * frac * sigma * d, lb, ub)\n                    ft, xt = callf(xt)\n                    if ft < pop_f[parent_i]:\n                        pop[parent_i] = xt\n                        pop_f[parent_i] = ft\n                        x_try = xt\n                        f_try = ft\n                continue  # move to next main iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in backtrack_fracs:\n                if remaining[0] <= 0:\n                    break\n                x_back = np.clip(x_parent + (frac * sigma) * d, lb, ub)\n                f_back, x_back = callf(x_back)\n                if f_back < pop_f[parent_i]:\n                    pop[parent_i] = x_back\n                    pop_f[parent_i] = f_back\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    improved = True\n                    break\n                # also try opposite direction small step\n                if remaining[0] <= 0:\n                    break\n                x_back2 = np.clip(x_parent - (frac * sigma) * d, lb, ub)\n                f_back2, x_back2 = callf(x_back2)\n                if f_back2 < pop_f[parent_i]:\n                    pop[parent_i] = x_back2\n                    pop_f[parent_i] = f_back2\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            for t in range(ortho_tries):\n                if remaining[0] <= 0:\n                    break\n                r = self.rng.randn(self.dim)\n                # make orthogonal to d\n                proj = np.dot(r, d) * d\n                r = r - proj\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    continue\n                r = r / nr\n                # step length smaller than sigma\n                step = 0.6 * sigma * (0.3 + 0.7 * self.rng.rand())\n                x_ortho = np.clip(x_parent + step * r, lb, ub)\n                f_ortho, x_ortho = callf(x_ortho)\n                if f_ortho < pop_f[parent_i]:\n                    pop[parent_i] = x_ortho\n                    pop_f[parent_i] = f_ortho\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < levy_prob and remaining[0] > 0:\n                # generate a heavy-tailed vector: elementwise Cauchy scaled to domain\n                # To keep directions diverse, multiply an orthonormal direction set\n                raw = self.rng.standard_cauchy(size=self.dim)\n                raw = np.clip(raw, -100.0, 100.0)\n                # robust scale vector\n                scale_vec = 0.18 * (ub - lb)\n                delta = raw * scale_vec\n                # normalize magnitude to avoid collapsing to huge values (keep direction heavy-tailed)\n                norm_delta = np.linalg.norm(delta)\n                if norm_delta > 0:\n                    delta = delta / (1.0 + norm_delta / (np.linalg.norm(ub - lb) + 1e-12))\n                x_jump = np.clip(x_parent + delta, lb, ub)\n                if remaining[0] > 0:\n                    f_jump, x_jump = callf(x_jump)\n                    # If it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(base_sigma * 0.6, 1e-12)\n                    # If jump improved parent, accept it\n                    if f_jump < pop_f[parent_i]:\n                        pop[parent_i] = x_jump\n                        pop_f[parent_i] = f_jump\n                        pop_sigma[parent_i] = max(sigma * 1.05, 1e-12)\n                    # continue main loop\n                    continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining[0] > 0 and self.rng.rand() < recomb_prob:\n                best2_idx = np.argsort(pop_f)[:2]\n                a, b = int(best2_idx[0]), int(best2_idx[1])\n                # blend with random convex coeff\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # small additive gaussian noise scaled by average sigma\n                avg_sigma = np.mean(pop_sigma)\n                noise = self.rng.randn(self.dim) * (0.12 * avg_sigma)\n                x_mix = np.clip(mix + noise, lb, ub)\n                if remaining[0] > 0:\n                    f_mix, x_mix = callf(x_mix)\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[parent_i]:\n                        pop[parent_i] = x_mix\n                        pop_f[parent_i] = f_mix\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                        continue\n                    else:\n                        # try to inject into population by replacing the worst if it's better\n                        if f_mix < pop_f[worst_i]:\n                            pop[worst_i] = x_mix\n                            pop_f[worst_i] = f_mix\n                            pop_sigma[worst_i] = max(0.8 * avg_sigma, 1e-12)\n\n            # adapt parent sigma on failure (slightly reduce exploration)\n            pop_sigma[parent_i] = max(sigma * 0.87, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < rejuvenation_prob and remaining[0] > 0:\n                worst_i = int(np.argmax(pop_f))\n                xr = self.rng.uniform(lb, ub)\n                fr, xr = callf(xr)\n                pop[worst_i] = xr\n                pop_f[worst_i] = fr\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * self.rng.rand())\n\n            # safety: if population became too small (shouldn't), reinitialize some members\n            if len(pop) < 2 and remaining[0] > 0:\n                xr = self.rng.uniform(lb, ub)\n                fr, xr = callf(xr)\n                pop.append(xr)\n                pop_f.append(fr)\n                pop_sigma.append(base_sigma * (0.7 + 0.6 * self.rng.rand()))\n\n            # occasional small housekeeping: shrink population if budget very low\n            if remaining[0] < 0.05 * self.budget and len(pop) > 6:\n                # prune worst individuals to focus on best ones\n                keep_k = max(4, int(0.6 * len(pop)))\n                order = np.argsort(pop_f)[:keep_k]\n                pop = [pop[i] for i in order]\n                pop_f = [pop_f[i] for i in order]\n                pop_sigma = [pop_sigma[i] for i in order]\n\n        if self.verbose:\n            print(\"ADLS finished: f_opt=\", self.f_opt, \"evaluations used=\", self.budget - remaining[0])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.445 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19406062811477542, 0.1584978718288289, 0.868540336594569, 0.33073540529645107, 0.2796603177152629, 0.8938331591790376, 0.23018392934514287, 0.45913684833385515, 0.8839283246566739, 0.15106673939275117]}, "task_prompt": ""}
{"id": "b33334e8-6c3f-4f70-b2a7-80971759aea1", "fitness": 0.4938640478466267, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining directional local searches with orthogonal refinements, adaptive step-sizes, occasional Lévy-like heavy-tailed jumps and light recombination to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population size scales modestly with dimension\n        if pop_size is None:\n            self.pop_size = min(40, max(6, 6 + 2 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # placeholders for results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # determine bounds from func if available, else default to [-5,5] per-dim\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure lb/ub shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # budget counter\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # safe call wrapper: handles clipping, shape and budget decrement\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to pure random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        if remaining < self.pop_size:\n            # random search until budget exhausted\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        # initial scale based on search-space size\n        space_scale = np.mean(ub - lb)\n        base_sigma = max(1e-8, 0.25 * space_scale)\n        max_sigma = max(1e-3, space_scale)\n\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (1.0 + 0.5 * np.random.rand()))  # slight diversity\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n\n        # If no population (very small budget), do random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Main optimization loop\n        # Tunable probabilities\n        levy_prob = 0.05\n        rejuvenate_prob = 0.02\n\n        # Pre-allocated vectors\n        while remaining > 0:\n            # pick a parent via small tournament (size 3 or all if small pop)\n            tour_size = min(3, len(pop))\n            inds = np.random.choice(len(pop), size=tour_size, replace=False)\n            parent_i = inds[np.argmin([pop_f[i] for i in inds])]\n            parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            d_norm = np.linalg.norm(d)\n            if d_norm == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / d_norm\n\n            # primary directional trial with stochasticized step-length (Cauchy-like for heavy tails)\n            # step length drawn from folded Cauchy scaled by sigma\n            s = np.abs(np.random.standard_cauchy())  # heavy-tailed positive scalar\n            s = np.clip(s, 0.0, 10.0)  # avoid absurd values\n            step_len = sigma * (0.8 + 0.4 * np.random.rand()) * s\n            x_try = parent + step_len * d\n            x_try = np.clip(x_try, lb, ub)\n\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                # success: accept and slightly increase sigma\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.08, max_sigma)\n                else:\n                    # local backtracking / small-step refinement along direction (few tries)\n                    improved = False\n                    for k in range(1, 4):\n                        small_step = step_len * (0.5 ** k)\n                        x_back = parent + small_step * d\n                        x_back = np.clip(x_back, lb, ub)\n                        if remaining <= 0:\n                            break\n                        f_back, x_back = callf(x_back)\n                        if f_back < pop_f[parent_i]:\n                            pop[parent_i] = x_back\n                            pop_f[parent_i] = f_back\n                            pop_sigma[parent_i] = min(sigma * 1.04, max_sigma)\n                            improved = True\n                            break\n                    if not improved:\n                        # decrease sigma a bit due to failure\n                        pop_sigma[parent_i] = max(1e-8, sigma * 0.92)\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # remove component along d\n                r = r - np.dot(r, d) * d\n                r_norm = np.linalg.norm(r)\n                if r_norm == 0:\n                    # if degenerate, use random direction\n                    r = np.random.randn(self.dim)\n                    r_norm = np.linalg.norm(r)\n                r = r / (r_norm + 1e-12)\n                orth_step = pop_sigma[parent_i] * (0.5 + 0.5 * np.random.rand())\n                x_try = parent + orth_step * r\n                x_try = np.clip(x_try, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.06, max_sigma)\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and (np.random.rand() < levy_prob):\n                # Cauchy-like heavy-tailed vector, normalized by robust scale\n                step = np.random.standard_cauchy(size=self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                # scale per-dim relative to search space\n                scale_vec = 0.2 * (ub - lb)\n                # scale step to avoid extreme scale but keep heavy-tail property\n                step = step / denom * scale_vec\n                # apply jump to a random individual (or current parent)\n                target_i = np.random.randint(len(pop))\n                x_try = pop[target_i] + step\n                x_try = np.clip(x_try, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good, replace the worst in population\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n                    else:\n                        # otherwise maybe keep as candidate by replacing slightly worse if that helps diversity\n                        if f_try < pop_f[target_i]:\n                            pop[target_i] = x_try\n                            pop_f[target_i] = f_try\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best_inds = np.argsort(pop_f)[:2]\n                mix = 0.5 * (pop[best_inds[0]] + pop[best_inds[1]])\n                noise = 0.05 * space_scale * np.random.randn(self.dim) * (0.5 + np.random.rand())\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # replace parent if improved, else possibly replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.05, max_sigma)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < rejuvenate_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n\n            # safety: if population size is less than desired (shouldn't happen), try to grow it\n            while len(pop) < self.pop_size and remaining > 0:\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma = np.append(pop_sigma, base_sigma * (0.5 + np.random.rand()))\n\n            # If budget small left, spend on focused local sampling around best\n            if remaining < 10 and remaining > 0:\n                best_i = int(np.argmin(pop_f))\n                for _ in range(remaining):\n                    small = 0.1 * base_sigma * np.random.randn(self.dim)\n                    x_try = np.clip(pop[best_i] + small, lb, ub)\n                    callf(x_try)\n                break  # budget exhausted by final spending\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.494 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17442852150910726, 0.2458132275986903, 0.5710804201228243, 0.9287806779255341, 0.5651300328203135, 0.8164663565035419, 0.29860508991445533, 0.4495305979638726, 0.7251418180203351, 0.16366373608759255]}, "task_prompt": ""}
{"id": "486d016e-23da-44d0-b6ed-bf748dfcdb5e", "fitness": 0.17557413272082725, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy) jumps to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size; by default scales modestly with dim\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population: grows with dimension but limited\n            self.pop_size = int(max(6, min(4 * self.dim, 40)))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.RandomState(None if seed is None else int(seed))\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # ---- Prepare bounds ----\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback to problem statement default\n            lb = -5.0\n            ub = 5.0\n\n        # make full-dim arrays\n        if lb.ndim == 0:\n            lb = np.full(self.dim, float(lb))\n        if ub.ndim == 0:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # internal counters\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).ravel()\n            if x.size != self.dim:\n                raise ValueError(\"Input has incorrect dimension\")\n            # clip\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # Very small budget fallback: pure random sampling\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        if remaining < max(3, self.pop_size // 2):\n            # simple random search if budget too small to initialize population\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # ---- Initialize population ----\n        pop_n = min(self.pop_size, remaining)  # don't create more than budget allows\n        pop_x = np.empty((pop_n, self.dim), dtype=float)\n        pop_f = np.empty(pop_n, dtype=float)\n        pop_sigma = np.empty(pop_n, dtype=float)\n\n        # initialize candidates uniformly and assign initial sigma proportional to box size\n        box_scale = np.mean(ub - lb)\n        for i in range(pop_n):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop_x[i, :] = x0\n            pop_f[i] = f0\n            pop_sigma[i] = box_scale * (0.05 + 0.2 * self.rng.rand())  # initial step sizes\n\n        # If budget got exhausted during init\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Precompute some constants\n        min_sigma = 1e-8 * box_scale\n        max_sigma = box_scale\n\n        # Main loop\n        iter_no = 0\n        # parameters\n        tournament_k = 2\n        backtrack_tries = 3\n        orthogonal_prob = 0.4\n        levy_prob = 0.08\n        recomb_prob = 0.12\n        rejuvenate_prob = 0.04\n\n        while remaining > 0:\n            iter_no += 1\n\n            # select parent by small tournament\n            inds = self.rng.choice(pop_n, tournament_k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            parent_x = pop_x[parent_i].copy()\n            parent_f = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random direction (normalized)\n            d = self.rng.randn(self.dim)\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                # fallback to uniform direction\n                d = self.rng.uniform(-1.0, 1.0, size=self.dim)\n                dn = np.linalg.norm(d)\n            d = d / dn\n\n            # primary directional trial: stochasticize step-length (lognormal-like multiplier)\n            step_mul = 2.0 ** (self.rng.randn() * 0.5 + self.rng.rand() * 0.5)  # mix Gaussian and uniform\n            step = sigma * step_mul\n            x_try = parent_x + step * d\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n\n            success = False\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < parent_f:\n                    # accept\n                    pop_x[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(max_sigma, sigma * 1.15)\n                    success = True\n                else:\n                    # degrade sigma slightly\n                    pop_sigma[parent_i] = max(min_sigma, sigma * 0.92)\n\n            # local backtracking / small-step refinement along direction (few tries)\n            if (not success) and (remaining > 0):\n                small_sigma = max(min_sigma, sigma * 0.3)\n                for bt in range(backtrack_tries):\n                    step_bt = small_sigma * (0.5 ** bt)\n                    x_bt = parent_x + step_bt * d\n                    x_bt = np.minimum(np.maximum(x_bt, lb), ub)\n                    if remaining <= 0:\n                        break\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt < pop_f[parent_i]:\n                        pop_x[parent_i] = x_bt\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = min(max_sigma, small_sigma * 1.1)\n                        success = True\n                        break\n\n            # orthogonal perturbation for diversification\n            if (not success) and (self.rng.rand() < orthogonal_prob) and (remaining > 0):\n                # generate vector orthogonal to d via Gram-Schmidt\n                v = self.rng.randn(self.dim)\n                proj = np.dot(v, d) * d\n                ort = v - proj\n                norm_ort = np.linalg.norm(ort)\n                if norm_ort > 1e-12:\n                    ort = ort / norm_ort\n                    ort_step = sigma * (0.6 + 0.8 * self.rng.rand())\n                    x_ort = parent_x + ort_step * ort\n                    x_ort = np.minimum(np.maximum(x_ort, lb), ub)\n                    f_ort, x_ort = callf(x_ort)\n                    if f_ort < pop_f[parent_i]:\n                        pop_x[parent_i] = x_ort\n                        pop_f[parent_i] = f_ort\n                        pop_sigma[parent_i] = min(max_sigma, sigma * 1.1)\n                        success = True\n\n            # occasional Lévy-like (Cauchy) jump to escape local basins\n            if (not success) and (self.rng.rand() < levy_prob) and (remaining > 0):\n                # heavy-tailed Cauchy step scaled by robust scale of population\n                # robust scale: median absolute deviation of population positions\n                med = np.median(pop_x, axis=0)\n                mad = np.median(np.abs(pop_x - med), axis=0)\n                robust_scale = np.maximum(1e-6, np.median(mad))\n                # generate Cauchy vector and normalize to robust scale\n                cauchy = self.rng.standard_cauchy(self.dim)\n                # guard against extremes\n                cauchy = np.clip(cauchy, -1e3, 1e3)\n                cauchy_norm = np.linalg.norm(cauchy)\n                if cauchy_norm == 0:\n                    cauchy = self.rng.randn(self.dim)\n                    cauchy_norm = np.linalg.norm(cauchy)\n                jump = (cauchy / cauchy_norm) * robust_scale * (1.0 + self.rng.rand() * 5.0)\n                x_jump = parent_x + jump\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                if remaining > 0:\n                    f_jump, x_jump = callf(x_jump)\n                    # if it's good, replace worst; else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop_x[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(min_sigma, sigma * 0.8)\n                        # don't continue parent exploitation on this iteration\n                        continue\n\n            # recombination exploitation: mix two best and small noise\n            if (self.rng.rand() < recomb_prob) and (remaining > 0):\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1] if best2.shape[0] > 1 else best2[0]\n                beta = 0.3 + 0.4 * self.rng.rand()\n                mix = beta * pop_x[a] + (1.0 - beta) * pop_x[b]\n                # small Gaussian perturbation scaled by average sigma\n                avg_sigma = max(min_sigma, np.mean(pop_sigma))\n                mix += self.rng.randn(self.dim) * (0.08 * avg_sigma)\n                mix = np.minimum(np.maximum(mix, lb), ub)\n                if remaining > 0:\n                    f_mix, mix = callf(mix)\n                    if f_mix < pop_f[parent_i]:\n                        pop_x[parent_i] = mix\n                        pop_f[parent_i] = f_mix\n                        pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.05)\n                    else:\n                        # maybe try to inject into worst\n                        worst_i = int(np.argmax(pop_f))\n                        if f_mix < pop_f[worst_i]:\n                            pop_x[worst_i] = mix\n                            pop_f[worst_i] = f_mix\n                            pop_sigma[worst_i] = avg_sigma * 0.7\n\n            # adapt parent sigma on failure\n            if not success:\n                pop_sigma[parent_i] = max(min_sigma, pop_sigma[parent_i] * (0.92 + 0.08 * self.rng.rand()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if (self.rng.rand() < rejuvenate_prob) and (remaining > 0):\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, x_new = callf(x_new)\n                    pop_x[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = box_scale * (0.02 + 0.15 * self.rng.rand())\n\n            # early stopping if perfect optimum found (function may have domain-specific min)\n            # not enforced, just break if budget exhausted.\n            # loop continues until remaining == 0\n\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.176 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10097026670878351, 0.15818485859675568, 0.2328833120049174, 0.21504163918741814, 0.13466067873679222, 0.16830006312877455, 0.19434003038441483, 0.23574502401379838, 0.19133458874288434, 0.12428086570373342]}, "task_prompt": ""}
{"id": "77b2911f-3d92-4365-86f2-19643d0946e1", "fitness": 0.19711756004763542, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that combine randomized directional local searches, orthogonal refinements and occasional Lévy-like jumps to explore and exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by dim if None)\n    - seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dim but stays reasonable\n            self.pop_size = max(2, min(12 + dim // 2, 30))\n        else:\n            self.pop_size = int(max(1, pop_size))\n        self.seed = seed\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n\n        # Bounds (ensure they are full arrays)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        search_range = ub - lb\n        # global tracking\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        evals = 0\n        max_evals = self.budget\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal evals, max_evals, lb, ub\n            if evals >= max_evals:\n                return np.inf\n            x = np.asarray(x, dtype=float).reshape(-1)\n            # clip before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            evals += 1\n            # update global best\n            nonlocal_best = False\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # If budget extremely small, fallback to simple random search\n        if max_evals <= 5:\n            self.f_opt = np.inf\n            self.x_opt = None\n            while evals < max_evals:\n                x = rng.uniform(lb, ub)\n                f = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population (ensure not to exceed budget)\n        pop = []\n        pop_f = []\n        # adaptive sigma per individual (scalar step size relative to search range)\n        sigma = []\n        init_pop = min(self.pop_size, max(1, max_evals // 10))\n        for i in range(init_pop):\n            if evals >= max_evals:\n                break\n            x = rng.uniform(lb, ub)\n            f = callf(x)\n            pop.append(x)\n            pop_f.append(f)\n            # initialize sigma as fraction of overall range, randomized a bit\n            sigma.append(0.2 * np.mean(search_range) * (1.0 + 0.5 * rng.random()))\n\n        pop = np.array(pop) if len(pop) > 0 else np.zeros((0, self.dim))\n        pop_f = np.array(pop_f) if len(pop_f) > 0 else np.array([])\n        sigma = np.array(sigma) if len(sigma) > 0 else np.array([])\n\n        # If no population could be created (very small budget), do pure random search\n        if pop.shape[0] == 0:\n            self.f_opt = np.inf\n            self.x_opt = None\n            while evals < max_evals:\n                x = rng.uniform(lb, ub)\n                f = callf(x)\n            return self.f_opt, self.x_opt\n\n        # fill up remaining population slots if budget still allows\n        while pop.shape[0] < self.pop_size and evals < max_evals:\n            x = rng.uniform(lb, ub)\n            f = callf(x)\n            pop = np.vstack([pop, x])\n            pop_f = np.append(pop_f, f)\n            sigma = np.append(sigma, 0.2 * np.mean(search_range) * (1.0 + 0.5 * rng.random()))\n\n        # main loop: keep working until budget exhausted\n        # meaning of one \"iteration\": one parent directional exploration plus a few minor tries\n        iter_count = 0\n        # parameters\n        tournament_k = min(3, max(2, pop.shape[0] // 3))\n        backtrack_tries = 3\n        orth_prob = 0.6\n        levy_prob = 0.06\n        rejuvenation_prob = 0.03\n        recomb_prob = 0.5\n        sigma_increase = 1.10\n        sigma_decrease = 0.90\n        sigma_min = 1e-8 * np.linalg.norm(search_range)\n        sigma_max = np.linalg.norm(search_range) * 1.0\n\n        while evals < max_evals:\n            iter_count += 1\n            n_pop = pop.shape[0]\n            # pick parent by small tournament (favor good individuals)\n            idxs = rng.choice(n_pop, size=tournament_k, replace=False)\n            parent_idx = idxs[np.argmin(pop_f[idxs])]\n            parent = pop[parent_idx].copy()\n            parent_f = pop_f[parent_idx]\n            parent_sigma = sigma[parent_idx]\n\n            # sample a random direction\n            v = rng.normal(size=self.dim)\n            v_norm = np.linalg.norm(v)\n            if v_norm == 0:\n                v = rng.standard_normal(self.dim)\n                v_norm = np.linalg.norm(v)\n            v = v / v_norm\n\n            improved = False\n\n            # primary directional trial with stochasticized step-length\n            s = parent_sigma * max(1e-12, 1.0 + 0.5 * rng.normal())\n            cand = parent + s * v\n            cand = np.minimum(np.maximum(cand, lb), ub)\n            f_cand = callf(cand)\n            if f_cand < parent_f:\n                # accept and replace parent\n                pop[parent_idx] = cand\n                pop_f[parent_idx] = f_cand\n                sigma[parent_idx] = min(sigma_max, parent_sigma * sigma_increase)\n                improved = True\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                s_local = s\n                for bt in range(backtrack_tries):\n                    if evals >= max_evals:\n                        break\n                    s_local *= 0.5\n                    cand = parent + s_local * v\n                    cand = np.minimum(np.maximum(cand, lb), ub)\n                    f_cand = callf(cand)\n                    if f_cand < parent_f:\n                        pop[parent_idx] = cand\n                        pop_f[parent_idx] = f_cand\n                        sigma[parent_idx] = min(sigma_max, parent_sigma * sigma_increase * 0.8)\n                        improved = True\n                        break\n\n            # orthogonal perturbation for local diversification\n            if not improved and rng.random() < orth_prob and evals < max_evals:\n                w = rng.normal(size=self.dim)\n                # make w orthogonal to v (Gram-Schmidt)\n                w = w - np.dot(w, v) * v\n                w_norm = np.linalg.norm(w)\n                if w_norm > 1e-12:\n                    w = w / w_norm\n                    s_orth = 0.6 * parent_sigma\n                    cand = parent + s_orth * w * (0.5 + rng.random())\n                    cand = np.minimum(np.maximum(cand, lb), ub)\n                    f_cand = callf(cand)\n                    if f_cand < parent_f:\n                        pop[parent_idx] = cand\n                        pop_f[parent_idx] = f_cand\n                        sigma[parent_idx] = min(sigma_max, parent_sigma * sigma_increase * 0.9)\n                        improved = True\n\n            # occasional Lévy-like jump to escape local basins\n            if rng.random() < levy_prob and evals < max_evals:\n                # heavy-tailed vector (Cauchy / standard_cauchy)\n                # generate per-dim standard cauchy then robustly scale to avoid immediate outliers\n                cauchy_vec = rng.standard_cauchy(size=self.dim)\n                # robust scale using median absolute deviation\n                mad = np.median(np.abs(cauchy_vec - np.median(cauchy_vec)))\n                if mad <= 0:\n                    mad = 1.0\n                cvec = cauchy_vec / mad\n                # normalize and apply heavy-tailed magnitude but clip\n                cvec = cvec / max(1e-12, np.linalg.norm(cvec))\n                jump_scale = np.median(search_range) * (1.0 + 5.0 * rng.random())  # can be large\n                jump = cvec * jump_scale\n                # limit jump to a fraction of the global range to avoid useless extreme out-of-bounds\n                jump = np.clip(jump, -2.0 * search_range, 2.0 * search_range)\n                cand = parent + jump\n                cand = np.minimum(np.maximum(cand, lb), ub)\n                f_cand = callf(cand)\n                if f_cand < self.f_opt:\n                    # really good, insert into population replacing worst\n                    worst_idx = int(np.argmax(pop_f))\n                    pop[worst_idx] = cand\n                    pop_f[worst_idx] = f_cand\n                    sigma[worst_idx] = max(sigma_min, parent_sigma * 0.8)\n                else:\n                    # keep as candidate: maybe replace worst if it's not too bad\n                    worst_idx = int(np.argmax(pop_f))\n                    if f_cand < pop_f[worst_idx]:\n                        pop[worst_idx] = cand\n                        pop_f[worst_idx] = f_cand\n                        sigma[worst_idx] = max(sigma_min, parent_sigma * 0.6)\n                # After jump attempt continue loop (counts evaluated)\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if rng.random() < recomb_prob and evals < max_evals:\n                # pick two best distinct indices\n                sorted_idx = np.argsort(pop_f)\n                best_idx = sorted_idx[0]\n                second_idx = sorted_idx[1] if sorted_idx.size > 1 else best_idx\n                x_recomb = 0.5 * (pop[best_idx] + pop[second_idx])\n                x_recomb = x_recomb + 0.05 * parent_sigma * rng.normal(size=self.dim)\n                x_recomb = np.minimum(np.maximum(x_recomb, lb), ub)\n                f_recomb = callf(x_recomb)\n                if f_recomb < parent_f:\n                    pop[parent_idx] = x_recomb\n                    pop_f[parent_idx] = f_recomb\n                    sigma[parent_idx] = min(sigma_max, parent_sigma * 1.05)\n                    improved = True\n                else:\n                    # possibly replace worst if recomb is better\n                    worst_idx = int(np.argmax(pop_f))\n                    if f_recomb < pop_f[worst_idx]:\n                        pop[worst_idx] = x_recomb\n                        pop_f[worst_idx] = f_recomb\n                        sigma[worst_idx] = max(sigma_min, parent_sigma * 0.5)\n\n            # adapt parent sigma on failure\n            if not improved:\n                sigma[parent_idx] = max(sigma_min, sigma[parent_idx] * sigma_decrease)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if rng.random() < rejuvenation_prob and evals < max_evals:\n                new_x = rng.uniform(lb, ub)\n                new_f = callf(new_x)\n                worst_idx = int(np.argmax(pop_f))\n                if new_f < pop_f[worst_idx]:\n                    pop[worst_idx] = new_x\n                    pop_f[worst_idx] = new_f\n                    sigma[worst_idx] = 0.2 * np.mean(search_range) * (1.0 + 0.3 * rng.random())\n\n            # safety: maintain sigma within bounds\n            sigma = np.clip(sigma, sigma_min, sigma_max)\n\n            # if population lost a member somehow (shouldn't happen), refill it\n            if pop.shape[0] < self.pop_size and evals < max_evals:\n                x = rng.uniform(lb, ub)\n                f = callf(x)\n                pop = np.vstack([pop, x])\n                pop_f = np.append(pop_f, f)\n                sigma = np.append(sigma, 0.2 * np.mean(search_range) * (1.0 + 0.5 * rng.random()))\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.197 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11886209337128717, 0.16971884383628422, 0.3533978872586686, 0.1760259185370574, 0.1825741648288387, 0.2278934978165309, 0.18733378998265482, 0.1993936965675226, 0.208153878888249, 0.1478218293892607]}, "task_prompt": ""}
{"id": "46a38511-dcd6-4992-a3b9-aac3113cb9c6", "fitness": 0.5600871555211384, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — maintains a small population of adaptive-step points that perform randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy jumps to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None, scaled with dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows slowly with dimension\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # best found will be stored here after running\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds as arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        # accept scalar bounds\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n            ub = np.full(self.dim, ub.item())\n\n        # budget bookkeeping\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and global best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            nonlocal_update = False\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                # store copy\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # initialize best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # trivial fallback if no budget\n        if remaining <= 0:\n            return self.f_opt, None\n\n        # population containers\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        # initial sigma scale: fraction of search range\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n\n        # create initial population (as many as budget allows, up to pop_size)\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # slight randomization per-individual\n            pop_sigma.append(base_sigma * (0.75 + 0.5 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # if no population was created (very small budget), perform pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main search loop\n        # counters for success-based adaptation\n        success_counter = np.zeros(len(pop), dtype=int)\n        fail_counter = np.zeros(len(pop), dtype=int)\n\n        # helper to replace worst with candidate\n        def replace_worst_if_better(f_candidate, x_candidate, sigma_candidate=None):\n            if len(pop_f) == 0:\n                return\n            worst_i = int(np.argmax(pop_f))\n            if f_candidate < pop_f[worst_i]:\n                pop[worst_i] = x_candidate.copy()\n                pop_f[worst_i] = float(f_candidate)\n                if sigma_candidate is not None:\n                    pop_sigma[worst_i] = float(sigma_candidate)\n                else:\n                    pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n\n        # search\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = int(inds[int(np.argmin(values))])\n\n            x_parent = pop[parent_i].copy()\n            parent_f = pop_f[parent_i]\n            sigma = float(pop_sigma[parent_i])\n            max_sigma = max(1e-12, np.mean(ub - lb))\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            # guard alpha from being tiny zero\n            if abs(alpha) < 1e-16:\n                alpha = sigma * (0.5 + 0.5 * np.random.rand())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < parent_f:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.25, max_sigma)\n                success_counter[parent_i] += 1\n                fail_counter[parent_i] = 0\n                continue\n            else:\n                fail_counter[parent_i] += 1\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.15, max_sigma)\n                    success_counter[parent_i] += 1\n                    fail_counter[parent_i] = 0\n                    improved = True\n                    break\n                else:\n                    fail_counter[parent_i] += 1\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d  # orthogonalize to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.12, max_sigma)\n                        success_counter[parent_i] += 1\n                        fail_counter[parent_i] = 0\n                        continue\n                    else:\n                        fail_counter[parent_i] += 1\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population\n                    replace_worst_if_better(f_try, x_try, sigma_candidate=max(1e-12, sigma * 0.5))\n                continue  # skip recombination on jump iterations\n\n            # recombination exploitation: mix two best and small noise (occasionally)\n            if np.random.rand() < 0.25:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.015 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # preferentially replace parent if better, else replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        success_counter[parent_i] += 1\n                        fail_counter[parent_i] = 0\n                    else:\n                        replace_worst_if_better(f_try, x_try, sigma_candidate=base_sigma * 0.5)\n                # continue main loop\n                continue\n\n            # no success above: reduce step size for this parent moderately\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # adaptive sigma adjustments based on simple success/failure history\n            if success_counter[parent_i] >= 3:\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.15, max_sigma)\n                success_counter[parent_i] = 0\n            if fail_counter[parent_i] >= 6:\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.7, 1e-12)\n                fail_counter[parent_i] = 0\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.03:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                # evaluate new sample before replacing to respect budget accounting\n                if remaining > 0:\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n                    # reset counters for that slot\n                    success_counter[worst_i] = 0\n                    fail_counter[worst_i] = 0\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.560 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14073571255475648, 0.15798600789011163, 0.8980109272538879, 0.8170217484242749, 0.9415539802854309, 0.9589398564272917, 0.2668824548849823, 0.4251838682354453, 0.8552399245895419, 0.1393170746656618]}, "task_prompt": ""}
{"id": "3102d828-c475-462b-b30f-33be94448567", "fitness": 0.501796459165636, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based heuristic that mixes adaptive directional local searches, orthogonal refinements, recombination and occasional Lévy-like heavy-tailed jumps to robustly explore and exploit continuous search spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds handling: allow scalar or array-like bounds from the benchmark wrapper\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # initialize tracking\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate a point while respecting budget and updating best\n        def callf(x):\n            nonlocal remaining\n            x = np.asarray(x, dtype=float)\n            x = np.clip(x, lb, ub)\n            # if no budget left, do not call func (return +inf so it's treated as non-improving)\n            if remaining <= 0:\n                return float(np.inf), x\n            # evaluate\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # if no budget, immediately return\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial step scale\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If couldn't create any population due to very small budget -> fallback to random sampling\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: directional local searches, orthogonal tries, Lévy jumps, recombination, rejuvenation\n        while remaining > 0:\n            # select a parent via a small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a normalized random direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n            if remaining <= 0:\n                break\n\n            # local backtracking / small-step refinement along direction\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n                if remaining <= 0:\n                    break\n            if improved:\n                continue\n            if remaining <= 0:\n                break\n\n            # orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                # slightly encourage sigma when orthogonal attempt yields reasonable moves\n                pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n            if remaining <= 0:\n                break\n\n            # occasional Lévy-like heavy-tailed jump to escape basins\n            if np.random.rand() < 0.08 and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                # normalize heavy-tailed vector to avoid extremely huge steps but preserve tail\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace the worst if the jump is good\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try.copy()\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # after a jump, continue to next iteration\n                continue\n            if remaining <= 0:\n                break\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n            if remaining <= 0:\n                break\n\n            # adapt parent's sigma on failure (conservative shrink)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation: replace worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished (budget exhausted or loop exit)\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.502 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13095581860754668, 0.1515618220999183, 0.515797985063333, 0.9790109996340911, 0.7117996822950425, 0.8937840306821384, 0.26420965828012555, 0.3971871574352528, 0.8112221529501086, 0.16243528460880285]}, "task_prompt": ""}
{"id": "c0e687cc-5209-4856-8ec4-d63d061efac2", "fitness": 0.46603727789914606, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based search combining randomized directional local searches, orthogonal refinements, occasional Lévy jumps and recombination with adaptive step-sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest function of dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # keep population modest but scale with dim\n            self.pop_size = max(4, min(40, int(2 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds may be scalars or arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        # ensure full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        # clip helper\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        remaining = int(self.budget)\n\n        # safe function caller that tracks remaining budget and global best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = clip(x)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x.copy()\n\n        # If budget is tiny, do pure random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        if remaining < 5 or self.pop_size <= 1:\n            # random search until budget exhausted\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        # base sigma: fraction of search range\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # slightly randomize initial sigmas\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population created (very small budget), finish\n        if len(pop) == 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # ensure lists are numpy-friendly lengths\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # control parameters\n        tournament_k = max(2, min(5, int(np.ceil(len(pop) / 3.0))))\n        backtrack_fracs = [0.5, 0.25, 0.125]\n        levy_prob = 0.06  # chance to attempt a Levy jump each main iteration\n        recomb_prob = 0.12\n        rejuvenate_prob = 0.02\n        iter_since_improve = 0\n        max_iters_without_improve = max(50, 10 * self.dim)\n\n        # main loop\n        while remaining > 0:\n            # precompute sorted best indices\n            best_order = np.argsort(pop_f)\n            best_i = int(best_order[0])\n            # pick a parent using small tournament\n            inds = np.random.choice(len(pop), size=min(tournament_k, len(pop)), replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            improved = False\n\n            # 1) primary directional trial\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = clip(x_parent + alpha * d)\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.25, np.mean(ub - lb))\n                    improved = True\n                    iter_since_improve = 0\n                else:\n                    # local backtracking (smaller fractions along same direction)\n                    for frac in backtrack_fracs:\n                        if remaining <= 0:\n                            break\n                        x_bt = clip(x_parent + alpha * frac * d)\n                        f_bt, x_bt = callf(x_bt)\n                        if f_bt < pop_f[parent_i]:\n                            pop[parent_i] = x_bt\n                            pop_f[parent_i] = f_bt\n                            pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                            improved = True\n                            iter_since_improve = 0\n                            break\n\n            if improved:\n                continue\n\n            # 2) orthogonal perturbation (try to escape along orthogonal direction)\n            r = np.random.randn(self.dim)\n            # make orthogonal to d\n            proj = np.dot(r, d) * d\n            r = r - proj\n            nr = np.linalg.norm(r)\n            if nr < 1e-12:\n                r = np.random.randn(self.dim)\n                proj = np.dot(r, d) * d\n                r = r - proj\n                nr = np.linalg.norm(r) + 1e-12\n            r = r / nr\n            step_scale = sigma * (0.6 + 0.8 * np.random.rand())\n            x_ort = clip(x_parent + step_scale * r)\n            if remaining > 0:\n                f_ort, x_ort = callf(x_ort)\n                if f_ort < pop_f[parent_i]:\n                    pop[parent_i] = x_ort\n                    pop_f[parent_i] = f_ort\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    improved = True\n                    iter_since_improve = 0\n\n            if improved:\n                continue\n\n            # 3) occasional Lévy-like heavy-tailed jump (Cauchy-like)\n            if (np.random.rand() < levy_prob) and remaining > 0:\n                # sample heavy-tailed vector, normalize by robust scale\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 80) + 1e-12\n                step = step / denom\n                # scale by range and parent's sigma\n                scale_vec = 0.5 * (ub - lb) * (0.2 + 0.8 * np.random.rand()) * (sigma / (base_sigma + 1e-12))\n                x_levy = clip(x_parent + step * scale_vec)\n                f_levy, x_levy = callf(x_levy)\n                # replace worst if better\n                worst_i = int(np.argmax(pop_f))\n                if f_levy < pop_f[worst_i]:\n                    pop[worst_i] = x_levy\n                    pop_f[worst_i] = f_levy\n                    pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n                    iter_since_improve = 0\n                    improved = True\n                # continue main loop whether accepted or not\n                if improved:\n                    continue\n\n            # 4) recombination exploitation: mix two best and test\n            if (len(pop) >= 2) and (np.random.rand() < recomb_prob) and remaining > 0:\n                a, b = int(best_order[0]), int(best_order[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.02 * (ub - lb)) * np.random.randn(self.dim)\n                x_mix = clip(mix + noise)\n                f_mix, x_mix = callf(x_mix)\n                # try to replace parent if better, else replace worst if still better\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    iter_since_improve = 0\n                    improved = True\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                        iter_since_improve = 0\n                        improved = True\n                if improved:\n                    continue\n\n            # if none improved, adapt sigma (shrink)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n            iter_since_improve += 1\n\n            # occasional population rejuvenation\n            if (np.random.rand() < rejuvenate_prob or iter_since_improve > max_iters_without_improve) and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.4 + 0.8 * np.random.rand())\n                iter_since_improve = 0\n\n            # if population size smaller than desired and we still have budget, try to grow modestly\n            if len(pop) < self.pop_size and remaining > 0 and np.random.rand() < 0.15:\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.6 + 0.6 * np.random.rand()))\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.466 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10455077329993334, 0.16356315340890737, 0.7546534655398633, 0.9115242434812005, 0.3307979648074134, 0.915686811790232, 0.2315367354478589, 0.3929238804353503, 0.7249844533484697, 0.13015129743223164]}, "task_prompt": ""}
{"id": "f57fcb6f-b6f6-4ad9-8636-59b00225f495", "fitness": 0.20776136141631127, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining adaptive per-individual step-sizes, randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy-like jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None, auto-scaled with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # best found so far\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds as full-dim arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.repeat(lb.item(), self.dim)\n        if ub.size == 1:\n            ub = np.repeat(ub.item(), self.dim)\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt or self.x_opt is None:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []        # list of solution vectors\n        pop_f = []      # corresponding function values\n        pop_sigma = []  # per-individual step-size (scale)\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # diverse initial sigma per individual\n            pop_sigma.append(base_sigma * (0.5 + np.random.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                try:\n                    f, x = callf(x)\n                except RuntimeError:\n                    break\n            return self.f_opt, self.x_opt\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            pop_size_now = len(pop)\n            # pick a parent via small tournament to balance exploration/exploitation\n            tour_size = min(3, pop_size_now)\n            inds = np.random.choice(pop_size_now, size=tour_size, replace=False)\n            # choose the best among tournament\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # step length sampled log-normally around sigma for diversity\n            step_len = sigma * max(1e-12, np.exp(0.2 * np.random.randn()))\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + step_len * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12 and remaining > 0:\n                r = r / nr\n                ortho_step = max(1e-12, 0.6 * sigma) * r\n                x_try = np.clip(x_parent + ortho_step, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                else:\n                    # With small probability keep this as a new individual (rejuvenation)\n                    if np.random.rand() < 0.15 and len(pop) < self.pop_size:\n                        pop.append(x_try)\n                        pop_f.append(f_try)\n                        pop_sigma.append(max(1e-12, sigma * 0.6))\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                # pick two best distinct indices\n                sorted_idx = np.argsort(pop_f)\n                i1, i2 = sorted_idx[0], sorted_idx[1]\n                beta = np.random.rand()\n                mix = beta * pop[i1] + (1.0 - beta) * pop[i2]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n\n            # adapt parent sigma on failure (conservative shrink)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                # if better than worst -> replace; else maybe replace parent\n                worst_i = int(np.argmax(pop_f))\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n                elif f_new < pop_f[parent_i]:\n                    pop[parent_i] = x_new\n                    pop_f[parent_i] = f_new\n                    pop_sigma[parent_i] = base_sigma * (0.5 + np.random.rand())\n\n            # keep population size bounded and occasionally cull worst duplicates\n            if len(pop) > self.pop_size:\n                # remove worst extras\n                idx_keep = np.argsort(pop_f)[: self.pop_size]\n                pop = [pop[i] for i in idx_keep]\n                pop_f = [pop_f[i] for i in idx_keep]\n                pop_sigma = [pop_sigma[i] for i in idx_keep]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.208 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1349764141221833, 0.16160903839110075, 0.49445147960483726, 0.15888333554415435, 0.1511365187391196, 0.1686474303379547, 0.23473059052885192, 0.22935683641602855, 0.21848575971324968, 0.12533621076563262]}, "task_prompt": ""}
{"id": "bf939a60-5d2d-47f1-a9fb-047e7efa731c", "fitness": 0.20688257187560985, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, recombination and occasional Lévy jumps to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by default)\n    - seed: optional RNG seed\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimensionality\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        self.seed = None if seed is None else int(seed)\n\n        # public best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Setup RNG\n        if self.seed is None:\n            rng = np.random.default_rng()\n        else:\n            rng = np.random.default_rng(self.seed)\n\n        # bounds: ensure arrays of length dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # clamp to problem-supplied dimension safety\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        remaining = int(self.budget)\n\n        # helper to call func while tracking budget and global best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # Clip to bounds for safety\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = float(func(x))\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is zero or negative, just return defaults\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialization: sample initial population\n        n_init = min(self.pop_size, remaining)\n        pop = np.empty((n_init, self.dim), dtype=float)\n        pop_f = np.empty(n_init, dtype=float)\n        # initial sigma scale based on problem size\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n        pop_sigma = np.full(n_init, base_sigma, dtype=float)\n        pop_fail = np.zeros(n_init, dtype=int)\n\n        for i in range(n_init):\n            x0 = rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop[i] = x0\n            pop_f[i] = f0\n\n            if remaining <= 0:\n                break\n\n        # if no population could be created due to extremely small budget, do random evaluations until budget exhausted\n        if pop.shape[0] == 0:\n            while remaining > 0:\n                x = rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # main loop: while we have evaluations left, perform mixed operators\n        min_sigma = 1e-12\n        max_sigma = max(1e-8, np.mean(ub - lb))  # do not explode sigma beyond domain scale\n\n        # operator weights (sum doesn't need to be 1; we'll use cumulative sampling)\n        weights = {\n            \"direction\": 45,\n            \"orthogonal\": 20,\n            \"levy\": 8,\n            \"recomb\": 15,\n            \"rejuvenate\": 6,\n            \"random_probe\": 6\n        }\n        ops = list(weights.keys())\n        cum = np.cumsum([weights[k] for k in ops])\n        total_w = cum[-1]\n\n        # helper: tournament selection (small tournament size)\n        def select_parent():\n            tsize = min(3, pop.shape[0])\n            inds = rng.integers(0, pop.shape[0], size=tsize)\n            best = inds[np.argmin(pop_f[inds])]\n            return best\n\n        # main evolutionary-improvement loop\n        while remaining > 0:\n            parent_i = select_parent()\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # choose operator\n            r = rng.integers(1, total_w + 1)\n            op = ops[np.searchsorted(cum, r)]\n\n            # ---------- Directional local search ----------\n            if op == \"direction\":\n                # random normalized direction\n                d = rng.normal(size=self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    d = rng.normal(size=self.dim)\n                    nd = np.linalg.norm(d) + 1e-12\n                d = d / nd\n\n                # stochasticized step-length (positive)\n                step_len = sigma * max(1e-12, 1.0 + 0.6 * rng.normal())\n                x_try = np.clip(x_parent + step_len * d, lb, ub)\n                improved = False\n\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        # accept\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(max_sigma, sigma * 1.15)\n                        pop_fail[parent_i] = 0\n                        improved = True\n                    else:\n                        pop_fail[parent_i] += 1\n\n                # backtracking / refinement: try shorter steps along same direction\n                if (not improved) and (remaining > 0):\n                    for factor in (0.5, 0.25, 0.1):\n                        if remaining <= 0:\n                            break\n                        x_try = np.clip(x_parent + step_len * factor * d, lb, ub)\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = min(max_sigma, sigma * (1.05 + 0.1 * factor))\n                            pop_fail[parent_i] = 0\n                            improved = True\n                            break\n                        else:\n                            pop_fail[parent_i] += 1\n\n                # if stagnated, shrink sigma moderately\n                if not improved:\n                    pop_sigma[parent_i] = max(min_sigma, sigma * 0.9)\n                    # if many successive failures, shrink more aggressively\n                    if pop_fail[parent_i] >= 5:\n                        pop_sigma[parent_i] = max(min_sigma, pop_sigma[parent_i] * 0.6)\n                        pop_fail[parent_i] = 0\n\n                continue  # go to next loop iteration\n\n            # ---------- Orthogonal perturbation ----------\n            if op == \"orthogonal\":\n                # get a direction to be orthogonal to (use a random direction)\n                d = rng.normal(size=self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n                d = d / nd\n                rvec = rng.normal(size=self.dim)\n                # remove projection on d\n                rvec = rvec - np.dot(rvec, d) * d\n                nr = np.linalg.norm(rvec)\n                if nr < 1e-12:\n                    rvec = rng.normal(size=self.dim)\n                    nr = np.linalg.norm(rvec) + 1e-12\n                rvec = rvec / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * rvec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(max_sigma, sigma * 1.08)\n                        pop_fail[parent_i] = 0\n                    else:\n                        pop_fail[parent_i] += 1\n                        pop_sigma[parent_i] = max(min_sigma, sigma * 0.95)\n                continue\n\n            # ---------- Lévy-like heavy-tailed jumps ----------\n            if op == \"levy\":\n                # choose a candidate base (either parent or a random individual)\n                if rng.random() < 0.6:\n                    base = x_parent\n                else:\n                    base = pop[rng.integers(0, pop.shape[0])]\n                # Cauchy-like heavy-tailed vector\n                try:\n                    step = rng.standard_cauchy(size=self.dim)\n                except AttributeError:\n                    # fallback if older numpy\n                    step = np.random.standard_cauchy(size=self.dim)\n                # scale and robust-normalize to avoid single extreme coordinate dominating scale\n                step = step * (10.0 * sigma)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                x_try = np.clip(base + step, lb, ub)\n\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's better than the worst, replace the worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(min_sigma, sigma * 0.5)\n                        pop_fail[worst_i] = 0\n                continue\n\n            # ---------- Recombination / exploitation ----------\n            if op == \"recomb\":\n                if pop.shape[0] >= 2:\n                    best_inds = np.argsort(pop_f)[:2]\n                    a, b = best_inds[0], best_inds[1]\n                    beta = rng.random()\n                    mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                    noise = rng.normal(scale=0.08 * sigma, size=self.dim)\n                    x_try = np.clip(mix + noise, lb, ub)\n                    if remaining > 0:\n                        f_try, x_try = callf(x_try)\n                        # try to insert into population: replace parent or worst\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = min(max_sigma, sigma * 0.95)\n                            pop_fail[parent_i] = 0\n                        else:\n                            worst_i = int(np.argmax(pop_f))\n                            if f_try < pop_f[worst_i]:\n                                pop[worst_i] = x_try\n                                pop_f[worst_i] = f_try\n                                pop_sigma[worst_i] = max(min_sigma, sigma * 0.5)\n                                pop_fail[worst_i] = 0\n                else:\n                    # fallback to a random probe\n                    x_try = rng.uniform(lb, ub)\n                    if remaining > 0:\n                        callf(x_try)\n                continue\n\n            # ---------- Rejuvenation (replace worst with random) ----------\n            if op == \"rejuvenate\":\n                worst_i = int(np.argmax(pop_f))\n                x_new = rng.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, x_new = callf(x_new)\n                    if f_new < pop_f[worst_i]:\n                        pop[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = base_sigma\n                        pop_fail[worst_i] = 0\n                continue\n\n            # ---------- Random probe (light exploration) ----------\n            if op == \"random_probe\":\n                x_try = np.clip(x_parent + rng.normal(scale=0.3 * sigma, size=self.dim), lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # accept into parent if improved\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(max_sigma, sigma * 1.02)\n                        pop_fail[parent_i] = 0\n                    else:\n                        pop_fail[parent_i] += 1\n                        # small shrink on failure\n                        pop_sigma[parent_i] = max(min_sigma, sigma * 0.97)\n                continue\n\n        # finished budget or run out\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.207 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0896388592367594, 0.16163737269148393, 0.2895210580190424, 0.35662402367751267, 0.22121102985295327, 0.15721923858477838, 0.2203426281719998, 0.22879739946755273, 0.21669252391299942, 0.12714158514101626]}, "task_prompt": ""}
{"id": "db4f548f-ac80-45b5-af78-039fb2067cc9", "fitness": 0.29714819912296575, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that mix randomized directional local searches, orthogonal refinements, recombination and occasional Lévy (Cauchy) jumps to balance fast local exploitation with occasional heavy-tailed exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a modest scale of dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dim but stays limited\n            self.pop_size = max(6, min(40, 4 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # Bounds: this interface expects func.bounds.lb / ub (scalars or arrays)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # state\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper for safe function calls: checks budget and records global best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            x = np.clip(x, lb, ub)\n            f = func(x)\n            remaining -= 1\n            # track global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # trivial fallback if budget extremely small\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population uniformly (or as many as budget allows)\n        pop = []\n        pop_f = []\n        pop_sigma = []  # adaptive step-size per individual (vector scale, scalar)\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize sigma relative to range and problem dimension\n            init_sigma = 0.2 * np.mean(ub - lb) * max(0.1, 1.0 / max(1.0, self.dim / 8.0))\n            pop_sigma.append(init_sigma)\n\n        # if no population could be built (very small budget), do random sampling\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return self.f_opt, self.x_opt\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # bookkeeping\n        evals_since_improve = 0\n        iter_since_rejuv = 0\n        max_no_improve_rejuv = max(20, 5 * self.dim)\n\n        # main loop: continue until budget exhausted\n        while remaining > 0:\n            # small tournament selection to pick a parent\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            improved_parent = False\n\n            # sample a direction and normalize\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d)\n            d = d / (nd + 1e-16)\n\n            # stochasticized step-length (mix uniform and log-uniform behavior)\n            # draws that favor small steps but occasionally larger ones\n            rand_scale = (np.random.rand() ** 2) + 0.02\n            step_len = sigma * (0.5 + rand_scale * 1.5)\n\n            # primary directional trial\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    # success: accept, slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.25, np.mean(ub - lb))\n                    improved_parent = True\n                    evals_since_improve = 0\n                    iter_since_rejuv = 0\n                else:\n                    evals_since_improve += 1\n                    iter_since_rejuv += 1\n            else:\n                break\n\n            # local backtracking: if directional attempt failed, try halving step a few times\n            if not improved_parent and remaining > 0:\n                backtrack_steps = 4\n                st = step_len\n                for bt in range(backtrack_steps):\n                    st *= 0.5\n                    x_try = np.clip(x_parent + st * d, lb, ub)\n                    if remaining <= 0:\n                        break\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(st * 1.2, 1e-12)\n                        improved_parent = True\n                        evals_since_improve = 0\n                        iter_since_rejuv = 0\n                        break\n                    else:\n                        evals_since_improve += 1\n                        iter_since_rejuv += 1\n\n            # orthogonal refinement: perturb orthogonally to discovered direction to diversify\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # project out component along d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    # fallback to pure random small perturbation\n                    r = np.random.randn(self.dim)\n                    nr = np.linalg.norm(r)\n                r = r / (nr + 1e-16)\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                        improved_parent = True\n                        evals_since_improve = 0\n                        iter_since_rejuv = 0\n                    else:\n                        evals_since_improve += 1\n                        iter_since_rejuv += 1\n\n            # occasional Lévy-like jump (Cauchy) to escape basins\n            if remaining > 0 and np.random.rand() < 0.08:\n                # Cauchy-like heavy-tailed vector, scaled robustly by search range\n                cauchy_vec = np.random.standard_cauchy(self.dim)\n                # avoid extreme outliers by a soft cap based on percentile scaling\n                # scale vector to have typical magnitude ~0.2*(ub-lb)\n                scale_vec = 0.12 * (ub - lb)\n                # normalize robustly by median abs to avoid extreme values\n                med = np.median(np.abs(cauchy_vec))\n                if med < 1e-12:\n                    med = 1.0\n                cvec = (cauchy_vec / med) * scale_vec\n                x_try = np.clip(x_parent + cvec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = np.mean(pop_sigma) if pop_sigma else sigma\n                        improved_parent = True\n                        evals_since_improve = 0\n                        iter_since_rejuv = 0\n                    else:\n                        # with small probability keep it by replacing worst (diversify)\n                        if np.random.rand() < 0.02:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = sigma * 0.8\n\n            # recombination exploitation: mix two good individuals plus small noise\n            if remaining > 0 and len(pop) >= 2 and np.random.rand() < 0.6:\n                # pick second parent (not same)\n                j = parent_i\n                if len(pop) > 1:\n                    choices = list(range(len(pop)))\n                    choices.remove(parent_i)\n                    j = np.random.choice(choices)\n                x2 = pop[j]\n                # biased mix towards the better of the two\n                ratio = 0.6 if pop_f[parent_i] < pop_f[j] else 0.4\n                mixed = ratio * x_parent + (1.0 - ratio) * x2\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mixed + noise, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # replace parent if improved; else try to replace the worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        improved_parent = True\n                        evals_since_improve = 0\n                        iter_since_rejuv = 0\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = sigma * 0.9\n                            iter_since_rejuv = 0\n\n            # adapt sigma on failure or success\n            if improved_parent:\n                # gentle increase for successful parent (diversify successful step lengths)\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, np.mean(ub - lb))\n            else:\n                # reduce sigma a bit on repeated failures to focus search locally\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.88, 1e-12)\n\n            # occasionally replace the worst with a fresh random sample to rejuvenate\n            if remaining > 0 and (iter_since_rejuv >= max_no_improve_rejuv or np.random.rand() < 0.03):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = 0.2 * np.mean(ub - lb)\n                iter_since_rejuv = 0\n\n            # If population size is smaller than target (rare), try to grow it with random individuals\n            while len(pop) < self.pop_size and remaining > 0:\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(0.12 * np.mean(ub - lb))\n\n            # safety: keep arrays consistent lengths\n            assert len(pop) == len(pop_f) == len(pop_sigma)\n\n            # small diversification: with low prob, perturb a random member\n            if remaining > 0 and np.random.rand() < 0.02:\n                i = np.random.randint(len(pop))\n                perturb = (0.08 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(pop[i] + perturb, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[i]:\n                        pop[i] = x_try\n                        pop_f[i] = f_try\n                        pop_sigma[i] = max(pop_sigma[i] * 0.95, 1e-12)\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.297 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10842445897743569, 0.1772931430522129, 0.48782622053171143, 0.4420425043862988, 0.1961383843112725, 0.4819066472236593, 0.2452831091588873, 0.31956276177118215, 0.3392270459109399, 0.17377771590605717]}, "task_prompt": ""}
{"id": "6aca3920-82b9-49e2-a57f-775c488c1e6c", "fitness": 0.2746992630823833, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining randomized directional local searches, orthogonal refinements, adaptive per-individual step-sizes and occasional heavy-tailed Lévy jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size if pop_size is not None else max(6, min(40, 4 + self.dim))\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # try to get bounds from func, otherwise default to [-5,5]\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure bounds have correct shape\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        span = ub - lb\n        base_sigma = max(1e-12, 0.25 * float(np.mean(span)))  # global scale reference\n\n        # state\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n        evals = 0\n\n        # safe evaluation wrapper that enforces budget and clipping\n        def callf(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None  # budget exhausted\n            x = np.asarray(x, dtype=float)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            evals += 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        remaining = self.budget - evals\n        # If extremely small budget, fallback to pure random sampling\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n        if self.budget < max(10, 2 * self.dim):\n            # simple random search until budget exhausted\n            while evals < self.budget:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population (evaluate each member once)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # ensure we don't allocate more population than budget allows\n        max_pop = max(1, min(self.pop_size, max(1, self.budget // 5)))\n        for i in range(max_pop):\n            if evals >= self.budget:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            if f0 is None:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.5 + np.random.rand()))  # slight diversity in sigma\n\n        # fallback if no population created (shouldn't happen)\n        if len(pop) == 0:\n            while evals < self.budget:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n        n_pop = len(pop)\n\n        # Main loop\n        iterations = 0\n        while evals < self.budget:\n            iterations += 1\n            remaining = self.budget - evals\n\n            # --- Parent selection: small tournament to balance exploration/exploitation\n            tour_k = min(3, n_pop)\n            inds = np.random.choice(n_pop, size=tour_k, replace=False)\n            parent_i = int(inds[np.argmin(pop_f[inds])])\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a randomized direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = max(1e-12, sigma * (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try is None:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max(sigma * 1.12, 1e-12), max(span))\n                # quick local refinement: try a couple of smaller steps along d\n                for s in [0.5, 0.25]:\n                    if evals >= self.budget:\n                        break\n                    x2 = np.clip(x_try + (alpha * s) * d, lb, ub)\n                    f2, x2 = callf(x2)\n                    if f2 is None:\n                        break\n                    if f2 < pop_f[parent_i]:\n                        pop[parent_i] = x2\n                        pop_f[parent_i] = f2\n                        x_try = x2\n                continue  # go to next main iteration\n\n            # If primary failed, try local backtracking (few tries)\n            back_success = False\n            alpha_b = alpha\n            for step_shrink in (0.5, 0.25, 0.1):\n                if evals >= self.budget:\n                    break\n                alpha_b *= step_shrink\n                x_b = np.clip(x_parent + alpha_b * d, lb, ub)\n                f_b, x_b = callf(x_b)\n                if f_b is None:\n                    break\n                if f_b < pop_f[parent_i]:\n                    pop[parent_i] = x_b\n                    pop_f[parent_i] = f_b\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    back_success = True\n                    break\n            if back_success:\n                continue\n\n            # Orthogonal perturbation for local diversification\n            if evals < self.budget:\n                r = np.random.randn(self.dim)\n                # project out the direction component to make it orthogonal-ish\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr == 0:\n                    r = np.random.randn(self.dim)\n                    nr = np.linalg.norm(r)\n                r = r / nr\n                orth_scale = sigma * (0.7 + 0.6 * np.random.rand())\n                x_o = np.clip(x_parent + orth_scale * r, lb, ub)\n                f_o, x_o = callf(x_o)\n                if f_o is None:\n                    break\n                if f_o < pop_f[parent_i]:\n                    pop[parent_i] = x_o\n                    pop_f[parent_i] = f_o\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    continue\n\n            # Occasional Lévy-like jump to escape local basins (heavy-tailed) with small probability\n            if np.random.rand() < 0.08 and evals < self.budget:\n                # generate Cauchy-like heavy-tailed vector\n                u = np.random.rand(self.dim) - 0.5\n                step = np.tan(np.pi * u)  # Cauchy samples\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                if denom != 0:\n                    step = step / denom\n                levy_scale = base_sigma * (5.0 * (0.5 + np.random.rand()))\n                x_l = np.clip(x_parent + step * levy_scale, lb, ub)\n                f_l, x_l = callf(x_l)\n                if f_l is None:\n                    break\n                # If good, replace the worst; otherwise maybe replace the parent with lower chance\n                worst_i = int(np.argmax(pop_f))\n                if f_l < pop_f[worst_i]:\n                    pop[worst_i] = x_l\n                    pop_f[worst_i] = f_l\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                else:\n                    # occasionally keep as new candidate replacing parent if not much worse\n                    if f_l < (pop_f[parent_i] + 1e-6):\n                        pop[parent_i] = x_l\n                        pop_f[parent_i] = f_l\n                        pop_sigma[parent_i] = max(1e-12, sigma * 0.6)\n                continue\n\n            # Recombination exploitation: mix two best with small noise\n            if np.random.rand() < 0.12 and evals < self.budget and n_pop >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                mix = 0.5 * (pop[best2[0]] + pop[best2[1]])\n                noise = np.random.randn(self.dim) * (0.1 * base_sigma)\n                x_mix = np.clip(mix + noise, lb, ub)\n                f_mix, x_mix = callf(x_mix)\n                if f_mix is None:\n                    break\n                # try to inject into population by replacing the worst if it's better\n                worst_i = int(np.argmax(pop_f))\n                if f_mix < pop_f[worst_i]:\n                    pop[worst_i] = x_mix\n                    pop_f[worst_i] = f_mix\n                    pop_sigma[worst_i] = max(1e-12, 0.8 * np.mean(pop_sigma))\n\n            # adapt parent sigma on failure (if we reached here primary/backtracking/orthogonal failed)\n            pop_sigma[parent_i] = max(1e-12, sigma * 0.85)\n\n            # occasional population rejuvenation: replace worst with uniform random sample\n            if np.random.rand() < 0.06 and evals < self.budget:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new is None:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n\n            # small maintenance: keep pop arrays consistent\n            # occasionally shrink or expand sigma diversity\n            if iterations % 50 == 0:\n                pop_sigma = np.clip(pop_sigma * (0.95 + 0.1 * np.random.rand(n_pop)), 1e-12, max(span))\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.275 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1313129885998483, 0.17078040118614246, 0.40167019901271606, 0.4162726440064973, 0.22587495962608284, 0.44922928895880954, 0.2607378007431609, 0.2925048618706947, 0.24288461989558463, 0.15572486692429632]}, "task_prompt": ""}
{"id": "9997cb3b-a6fa-423b-8f49-18ecc271e308", "fitness": 0.47972304145630523, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based search combining directional local searches with orthogonal refinements, adaptive per-individual step-sizes, occasional heavy-tailed Lévy-like jumps and population rejuvenation to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_size: number of population members (defaults to a modest scaling with dim)\n    - seed: random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # keep population modest but scaled with dimensionality\n            self.pop_size = int(min(max(6, 2 * self.dim), 40))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n\n    def __call__(self, func):\n        # determine bounds robustly (many BBOB wrappers provide func.bounds.lb/ub)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # default bounds according to the task description\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # expand scalar bounds to full-dim arrays if needed\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        # safety\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # tracking best\n        f_opt = np.inf\n        x_opt = None\n\n        remaining = int(self.budget)\n        if remaining <= 0:\n            return f_opt, x_opt\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining, f_opt, x_opt\n            if remaining <= 0:\n                raise RuntimeError(\"No budget left for evaluation\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = float(func(x))\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n            return f, x\n\n        # Very small budgets -> random search fallback\n        if self.budget < 5:\n            # simple random sampling until budget exhausted\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return f_opt, x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # initialize one candidate at center (helpful on some BB functions)\n        center = 0.5 * (lb + ub)\n        try:\n            f_c, x_c = callf(center)\n            pop.append(x_c.copy())\n            pop_f.append(f_c)\n            pop_sigma.append(max(np.linalg.norm(ub - lb) * 0.05, 1e-8))\n        except RuntimeError:\n            pass\n\n        # fill rest randomly while budget allows\n        while len(pop) < self.pop_size and remaining > 0:\n            x0 = self.rng.uniform(lb, ub)\n            try:\n                f0, x0c = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0c.copy())\n            pop_f.append(f0)\n            # initialize sigma proportional to domain size but randomized a bit\n            init_sigma = max(np.linalg.norm(ub - lb) * (0.02 + 0.03 * self.rng.rand()), 1e-8)\n            pop_sigma.append(init_sigma)\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return f_opt, x_opt\n\n        pop = np.array(pop, dtype=float)\n        pop_f = np.array(pop_f, dtype=float)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n\n        # main loop\n        while remaining > 0:\n            # pick a parent via small tournament (size 3)\n            k = min(3, len(pop))\n            candidates = self.rng.choice(len(pop), size=k, replace=False)\n            parent_i = candidates[int(np.argmin(pop_f[candidates]))]\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            d_norm = np.linalg.norm(d) + 1e-12\n            d = d / d_norm\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.minimum(np.maximum(x_parent + alpha * d, lb), ub)\n\n            if remaining <= 0:\n                break\n\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < f_parent:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15, np.linalg.norm(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            back_scales = [0.6, 0.3, 0.15]\n            for bs in back_scales:\n                if remaining <= 0:\n                    break\n                x_bt = np.minimum(np.maximum(x_parent + alpha * bs * d, lb), ub)\n                try:\n                    f_bt, x_bt = callf(x_bt)\n                except RuntimeError:\n                    break\n                if f_bt < f_parent:\n                    pop[parent_i] = x_bt.copy()\n                    pop_f[parent_i] = f_bt\n                    # slightly reduce sigma to focus search\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = self.rng.randn(self.dim)\n                # remove component along d to make it (approximately) orthogonal\n                r = r - np.dot(r, d) * d\n                r_norm = np.linalg.norm(r)\n                if r_norm > 1e-12:\n                    r = r / r_norm\n                    mag = sigma * (0.6 + 0.4 * self.rng.rand())\n                    x_o = np.minimum(np.maximum(x_parent + mag * r, lb), ub)\n                    try:\n                        f_o, x_o = callf(x_o)\n                    except RuntimeError:\n                        break\n                    if f_o < f_parent:\n                        pop[parent_i] = x_o.copy()\n                        pop_f[parent_i] = f_o\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # generate Cauchy-like heavy-tailed vector (tan of uniform)\n                u = self.rng.rand(self.dim) - 0.5\n                step = np.tan(np.pi * u)\n                # robust scale normalization to avoid extreme coordinates but keep heavy-tail\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale relative to domain and sigma\n                global_scale = 0.5 * np.linalg.norm(ub - lb) * (0.3 + 1.4 * self.rng.rand())\n                step = step * global_scale\n                x_jump = np.minimum(np.maximum(x_parent + step, lb), ub)\n                try:\n                    f_jump, x_jump = callf(x_jump)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                if f_jump < np.max(pop_f):\n                    worst_i = int(np.argmax(pop_f))\n                    pop[worst_i] = x_jump.copy()\n                    pop_f[worst_i] = f_jump\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                mix = 0.5 * (pop[best2[0]] + pop[best2[1]])\n                noise = sigma * 0.15 * self.rng.randn(self.dim)\n                x_mix = np.minimum(np.maximum(mix + noise, lb), ub)\n                try:\n                    f_mix, x_mix = callf(x_mix)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_mix < f_parent:\n                    pop[parent_i] = x_mix.copy()\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix.copy()\n                        pop_f[worst_i] = f_mix\n                # continue to next iteration\n                continue\n\n            # adapt parent sigma on failure (anneal)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = max(np.linalg.norm(ub - lb) * 0.02 * (0.5 + self.rng.rand()), 1e-12)\n\n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.480 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13220969819544393, 0.16512097338053033, 0.7713002865044537, 0.9392122248614487, 0.6858856289932428, 0.9254660639561371, 0.2570372971251277, 0.46709675305016785, 0.3051816073516702, 0.14871988114482915]}, "task_prompt": ""}
{"id": "339e39e6-9cb1-4f2c-b213-7eed3e081241", "fitness": 0.5094112378442015, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy) jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimensionality but remains bounded\n            self.pop_size = max(4, min(50, int(6 + self.dim * 1.2)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # determine bounds (support scalar or array bounds)\n        try:\n            lb_raw = func.bounds.lb\n            ub_raw = func.bounds.ub\n        except Exception:\n            lb_raw, ub_raw = -5.0, 5.0\n\n        lb = np.atleast_1d(lb_raw).astype(float)\n        ub = np.atleast_1d(ub_raw).astype(float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        # clamp to requested dimension\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.full(self.dim, lb.ravel()[0])\n            ub = np.full(self.dim, ub.ravel()[0])\n\n        # helper to track budget and best solution\n        self.evals = 0\n        def callf(x):\n            if self.evals >= self.budget:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).ravel()\n            if x.size != self.dim:\n                raise ValueError(\"Candidate has wrong dimensionality\")\n            # clip before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is extremely small, fallback to random search\n        if self.budget <= 5:\n            # simple random samples until budget exhausted\n            while self.evals < self.budget:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # initialize population (but ensure we don't overshoot budget)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = 0.1 * (ub - lb).mean()  # base step-length scale\n        n_init = min(self.pop_size, max(1, self.budget // 10))\n        for i in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            try:\n                f, x = callf(x)\n            except RuntimeError:\n                break\n            pop.append(x.copy())\n            pop_f.append(f)\n            # initialize adaptive sigma for each individual with small randomization\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.rand()))\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while self.evals < self.budget:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        pop = np.stack(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # main loop\n        # probabilities and constants\n        p_jump = 0.03\n        p_rejuvenate = 0.02\n        p_recombine = 0.15\n        max_iter = 10**9  # loop limited by budget via callf\n        iter_count = 0\n\n        while self.evals < self.budget:\n            iter_count += 1\n            # pick a parent via small tournament to balance exploration / exploitation\n            tour_k = min(3, pop.shape[0])\n            contenders = self.rng.choice(pop.shape[0], tour_k, replace=False)\n            parent_i = contenders[np.argmin(pop_f[contenders])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / dn\n\n            # primary directional trial with stochasticized step-length (Cauchy-ish length scaled)\n            # draw a Cauchy-like magnitude and temper extremes\n            u = self.rng.rand()\n            # use a stabilized Cauchy (inverse transform)\n            cauchy_r = np.tan(np.pi * (u - 0.5))\n            step_len = sigma * (0.7 + 0.6 * self.rng.rand()) * (1.0 + 0.6 * cauchy_r)\n            # cap step length to reasonable fraction of domain\n            domain_scale = np.linalg.norm(ub - lb) / np.sqrt(self.dim)\n            step_len = np.sign(step_len) * min(abs(step_len), 2.0 * domain_scale)\n\n            x_try = x_parent + step_len * d\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min((ub - lb).mean(), sigma * 1.12)\n                improved = True\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                n_back = 3\n                back_scale = 0.5\n                for _ in range(n_back):\n                    small_step = step_len * back_scale * (0.4 + 0.6 * self.rng.rand())\n                    x_small = x_parent + small_step * d\n                    x_small = np.minimum(np.maximum(x_small, lb), ub)\n                    try:\n                        f_small, x_small = callf(x_small)\n                    except RuntimeError:\n                        break\n                    if f_small < pop_f[parent_i]:\n                        pop[parent_i] = x_small.copy()\n                        pop_f[parent_i] = f_small\n                        pop_sigma[parent_i] = min((ub - lb).mean(), sigma * 1.08)\n                        improved = True\n                        break\n                    back_scale *= 0.6\n\n            # try an orthogonal perturbation for local diversification\n            if not improved:\n                w = self.rng.randn(self.dim)\n                # make w orthogonal to d\n                proj = np.dot(w, d) * d\n                w = w - proj\n                wn = np.linalg.norm(w)\n                if wn > 1e-12:\n                    w = w / wn\n                    orth_step = 0.6 * sigma * (0.5 + 0.8 * self.rng.rand())\n                    x_orth = pop[parent_i] + orth_step * w\n                    x_orth = np.minimum(np.maximum(x_orth, lb), ub)\n                    try:\n                        f_orth, x_orth = callf(x_orth)\n                    except RuntimeError:\n                        break\n                    if f_orth < pop_f[parent_i]:\n                        pop[parent_i] = x_orth.copy()\n                        pop_f[parent_i] = f_orth\n                        pop_sigma[parent_i] = min((ub - lb).mean(), sigma * 1.06)\n                        improved = True\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if (not improved) and (self.rng.rand() < p_jump) and (self.evals < self.budget):\n                # create heavy-tailed vector (Cauchy entries), normalize and scale to a fraction of domain\n                cauchy_vec = np.tan(np.pi * (self.rng.rand(self.dim) - 0.5))\n                # robust scale\n                med = np.median(np.abs(cauchy_vec)) + 1e-9\n                cauchy_vec = cauchy_vec / med\n                scale_vec = 0.2 * (ub - lb)\n                jump = scale_vec * cauchy_vec / (np.linalg.norm(cauchy_vec) + 1e-12)\n                x_jump = pop[parent_i] + jump\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                try:\n                    f_jump, x_jump = callf(x_jump)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = np.argmax(pop_f)\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump.copy()\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n                # continue main loop after jump attempt\n\n            # recombination exploitation: mix two best and small noise\n            if (self.rng.rand() < p_recombine) and (self.evals < self.budget):\n                # pick top two\n                if pop.shape[0] >= 2:\n                    best_idx = np.argsort(pop_f)[:2]\n                    x_new = 0.5 * (pop[best_idx[0]] + pop[best_idx[1]])\n                else:\n                    x_new = pop[parent_i].copy()\n                x_new = x_new + (0.02 * (ub - lb) * (self.rng.randn(self.dim)))\n                x_new = np.minimum(np.maximum(x_new, lb), ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                if f_new < pop_f[parent_i]:\n                    pop[parent_i] = x_new.copy()\n                    pop_f[parent_i] = f_new\n                    pop_sigma[parent_i] = max(1e-8, sigma * 1.05)\n                else:\n                    # maybe replace worst if it's better\n                    worst_i = np.argmax(pop_f)\n                    if f_new < pop_f[worst_i]:\n                        pop[worst_i] = x_new.copy()\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * self.rng.rand())\n\n            # adapt parent sigma on failure\n            if not improved:\n                pop_sigma[parent_i] = max(1e-8, pop_sigma[parent_i] * 0.92)\n            else:\n                pop_sigma[parent_i] = min((ub - lb).mean(), pop_sigma[parent_i] * 1.03)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if (self.rng.rand() < p_rejuvenate) and (self.evals < self.budget):\n                worst_i = np.argmax(pop_f)\n                x_rand = self.rng.uniform(lb, ub)\n                try:\n                    f_rand, x_rand = callf(x_rand)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_rand.copy()\n                pop_f[worst_i] = f_rand\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * self.rng.rand())\n\n            # if population is small and we still have budget, try to grow it slightly\n            if pop.shape[0] < self.pop_size and (self.evals + 1 < self.budget):\n                add_x = self.rng.uniform(lb, ub)\n                try:\n                    f_add, add_x = callf(add_x)\n                except RuntimeError:\n                    break\n                pop = np.vstack([pop, add_x.copy()])\n                pop_f = np.concatenate([pop_f, np.array([f_add])])\n                pop_sigma = np.concatenate([pop_sigma, np.array([base_sigma * (0.6 + 0.8 * self.rng.rand())])])\n\n            # safety: if budget nearly exhausted, break early\n            if self.evals >= self.budget:\n                break\n\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.509 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14394313034250228, 0.2147203826217844, 0.8177151692146765, 0.9342352915874628, 0.37512211182840516, 0.8963047182664384, 0.28071379789996687, 0.4472728349168994, 0.8149158512672509, 0.16916909049662832]}, "task_prompt": ""}
{"id": "9cf6b97c-73a5-42e3-820d-ef0c8fe4da24", "fitness": 0.3940285602113263, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements and occasional Lévy-like jumps to balance local exploitation and global escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: number of individuals in population (optional)\n    - seed: RNG seed (optional)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows slowly with dimension\n            self.pop_size = max(4, min(30, int(4 + np.sqrt(self.dim) * 2)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # book-keeping\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Get bounds if available otherwise use [-5,5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            # if scalar or shorter, expand to dim\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item(), dtype=float)\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item(), dtype=float)\n        else:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure shapes\n        if lb.shape[0] != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.shape[0] != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        span = ub - lb\n        # safe minima\n        MIN_SIG = 1e-8\n        MAX_SIG = max(1.0, np.max(span) * 2.0)\n\n        # helper evaluation function that respects budget and updates global best\n        def callf(x):\n            x = np.asarray(x, dtype=float).ravel()[:self.dim]\n            # clip\n            x = np.minimum(np.maximum(x, lb), ub)\n            if self.evals >= self.budget:\n                # no budget left; return large value and do not increment\n                return np.inf, x\n            f = func(x)\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget extremely small, fallback to random search\n        if self.budget <= 0:\n            return self.f_opt, self.x_opt\n        if self.budget < self.pop_size:\n            # do simple random search\n            while self.evals < self.budget:\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        initial_sigma = max(np.min(span) * 0.5, 0.5)  # somewhat large initial step\n        for _ in range(self.pop_size):\n            if self.evals >= self.budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f, x = callf(x)\n            ind = {\"x\": x, \"f\": f, \"sigma\": initial_sigma}\n            pop.append(ind)\n        # If we couldn't create a full pop (very small budget), continue with random samples until budget exhausted\n        if len(pop) == 0:\n            while self.evals < self.budget:\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # auxiliary helpers\n        def pop_best_idx():\n            return np.argmin([ind[\"f\"] for ind in pop])\n\n        def pop_worst_idx():\n            return np.argmax([ind[\"f\"] for ind in pop])\n\n        def tournament_select(k=3):\n            # small tournament among k random individuals\n            k = min(k, len(pop))\n            idxs = self.rng.choice(len(pop), size=k, replace=False)\n            best = idxs[0]\n            bestf = pop[best][\"f\"]\n            for i in idxs[1:]:\n                if pop[i][\"f\"] < bestf:\n                    best = i\n                    bestf = pop[i][\"f\"]\n            return best\n\n        # main loop\n        stagnation = 0\n        max_no_improve = max(5, int(5 + self.dim / 2.0))\n        levy_prob_base = 0.03  # base chance per iteration to attempt a Levy jump\n        iter_count = 0\n\n        while self.evals < self.budget:\n            iter_count += 1\n            # choose a parent via small tournament\n            p_idx = tournament_select(k=2 + (1 if self.rng.rand() < 0.2 else 0))\n            parent = pop[p_idx]\n            x_parent = parent[\"x\"]\n            f_parent = parent[\"f\"]\n            sigma = parent[\"sigma\"]\n            # sample a random direction\n            d = self.rng.normal(size=self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0.0:\n                d = self.rng.uniform(-1.0, 1.0, size=self.dim)\n                nd = np.linalg.norm(d)\n            d = d / (nd + 1e-12)\n\n            # primary directional trial with stochasticized step-length\n            step_scale = sigma * np.exp(self.rng.normal(0, 0.5)) * (0.5 + self.rng.rand() * 0.5)\n            x_trial = x_parent + (step_scale * d)\n            # clip and evaluate if budget available\n            if self.evals >= self.budget:\n                break\n            f_trial, x_trial = callf(x_trial)\n\n            improved = False\n            if f_trial < f_parent:\n                # accept\n                pop[p_idx] = {\"x\": x_trial, \"f\": f_trial, \"sigma\": min(MAX_SIG, sigma * 1.12)}\n                improved = True\n                stagnation = 0\n            else:\n                # local backtracking / small-step refinement along direction\n                backtries = 3\n                back_scale = 0.5\n                for bt in range(backtries):\n                    if self.evals >= self.budget:\n                        break\n                    step_scale *= back_scale\n                    x_bt = x_parent + (step_scale * d)\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt < f_parent:\n                        pop[p_idx] = {\"x\": x_bt, \"f\": f_bt, \"sigma\": min(MAX_SIG, sigma * (1.0 + 0.08 * (1 + bt)))}\n                        improved = True\n                        stagnation = 0\n                        break\n\n            # orthogonal perturbation for diversification\n            if not improved and self.evals < self.budget:\n                # produce vector orthogonal to d\n                v = self.rng.normal(size=self.dim)\n                # remove projection\n                v = v - d * np.dot(v, d)\n                nv = np.linalg.norm(v)\n                if nv < 1e-12:\n                    v = self.rng.normal(size=self.dim)\n                    v = v - d * np.dot(v, d)\n                    nv = np.linalg.norm(v) + 1e-12\n                v = v / nv\n                ortho_step = sigma * 0.6 * (0.5 + self.rng.rand() * 0.5)\n                x_ort = x_parent + ortho_step * v\n                if self.evals < self.budget:\n                    f_ort, x_ort = callf(x_ort)\n                    if f_ort < f_parent:\n                        pop[p_idx] = {\"x\": x_ort, \"f\": f_ort, \"sigma\": min(MAX_SIG, sigma * 1.07)}\n                        improved = True\n                        stagnation = 0\n\n            # occasional Lévy-like jump to escape basins (probability increases with stagnation)\n            levy_prob = min(0.35, levy_prob_base + 0.01 * stagnation)\n            if (not improved) and (self.rng.rand() < levy_prob) and (self.evals < self.budget):\n                # Cauchy-like heavy-tailed vector\n                c = self.rng.standard_cauchy(size=self.dim)\n                # robust scale normalization to keep scale predictable\n                scale = np.median(np.abs(c)) + 1e-8\n                delta = c / scale\n                # apply heavy-tailed magnitude relative to problem span and sigma\n                mag = max(0.2, self.rng.exponential(scale=1.0)) * np.median(span) * 0.5\n                delta = delta / (np.linalg.norm(delta) + 1e-12) * mag\n                # center jump on the current best (escape from local basin) or parent with small prob\n                if self.rng.rand() < 0.8 and self.x_opt is not None:\n                    center = self.x_opt\n                else:\n                    center = x_parent\n                x_lev = center + delta\n                # clip and evaluate\n                if self.evals < self.budget:\n                    f_lev, x_lev = callf(x_lev)\n                    widx = pop_worst_idx()\n                    # if good, replace worst, else maybe keep as candidate in population (with tiny chance)\n                    if f_lev < pop[widx][\"f\"]:\n                        pop[widx] = {\"x\": x_lev, \"f\": f_lev, \"sigma\": max(MIN_SIG, sigma * 0.8)}\n                        stagnation = 0\n                    else:\n                        # keep it as candidate not replacing bests, but maybe replace parent if slightly better\n                        if f_lev < f_parent:\n                            pop[p_idx] = {\"x\": x_lev, \"f\": f_lev, \"sigma\": max(MIN_SIG, sigma * 0.9)}\n                            stagnation = 0\n                        else:\n                            # slight chance to accept as new individual replacing worst to maintain diversity\n                            if self.rng.rand() < 0.02:\n                                pop[widx] = {\"x\": x_lev, \"f\": f_lev, \"sigma\": initial_sigma * 0.6}\n                    # after a jump attempt, continue main loop\n                    continue\n\n            # recombination exploitation: mix two best and small noise\n            if self.evals < self.budget:\n                # pick two distinct best candidates\n                idxs_sorted = np.argsort([ind[\"f\"] for ind in pop])\n                if len(idxs_sorted) >= 2:\n                    b1, b2 = idxs_sorted[0], idxs_sorted[1]\n                    x1, x2 = pop[b1][\"x\"], pop[b2][\"x\"]\n                    alpha = self.rng.beta(2.0, 2.0)  # biased to middle values but random\n                    child = alpha * x1 + (1 - alpha) * x2\n                    # small gaussian perturbation scaled by median sigma\n                    median_sigma = np.median([ind[\"sigma\"] for ind in pop])\n                    child += self.rng.normal(scale=0.05 * median_sigma, size=self.dim)\n                    # clip and evaluate\n                    if self.evals < self.budget:\n                        f_child, child = callf(child)\n                        if f_child < f_parent:\n                            pop[p_idx] = {\"x\": child, \"f\": f_child, \"sigma\": max(MIN_SIG, parent[\"sigma\"] * 1.03)}\n                            improved = True\n                            stagnation = 0\n                        else:\n                            # inject into population if better than worst\n                            widx = pop_worst_idx()\n                            if f_child < pop[widx][\"f\"]:\n                                pop[widx] = {\"x\": child, \"f\": f_child, \"sigma\": max(MIN_SIG, median_sigma * 0.8)}\n\n            # adapt parent sigma on failure / success\n            if not improved:\n                parent[\"sigma\"] = max(MIN_SIG, parent[\"sigma\"] * 0.92)\n                stagnation += 1\n            else:\n                # small global decay to avoid runaway sigmas\n                parent[\"sigma\"] = min(MAX_SIG, parent[\"sigma\"] * 0.995)\n\n            # occasional population rejuvenation\n            if (self.rng.rand() < 0.01 or stagnation > max_no_improve) and (self.evals < self.budget):\n                widx = pop_worst_idx()\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[widx] = {\"x\": x_new, \"f\": f_new, \"sigma\": initial_sigma * 0.7}\n                stagnation = 0 if f_new < self.f_opt else stagnation\n\n            # prevent over-run: break condition checked at top of loop\n            # iterate until budget consumed\n\n        # finished\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.394 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1154374431568822, 0.15393917303684745, 0.9394177044982286, 0.18863635914320587, 0.2402453172169341, 0.9615100885603315, 0.2697494958165745, 0.6910735820154428, 0.2528092144024834, 0.12746722426633272]}, "task_prompt": ""}
{"id": "3c868abf-98e4-40f7-932c-398f1a3d2a1e", "fitness": 0.33832992582670995, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements and occasional Lévy jumps to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed for reproducibility\n\n    The algorithm maintains a small population of candidate points each with an adaptive step-size (sigma).\n    It performs directional local searches (random directions, backtracking/forward fractions), orthogonal\n    refinements, occasional heavy-tailed Lévy-like jumps, and recombination of the best candidates.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales mildly with dimensionality but remains modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # persistent best\n        self.f_opt = np.inf\n        self.x_opt = None\n        # RNG\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        # extract bounds and make them full-dimensional arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # robust safety: ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        # helper to call function while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best if improved\n            nonlocal_best = False\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n                nonlocal_best = True\n            return f, x, nonlocal_best\n\n        # quick fallback: if budget zero, return defaults\n        if remaining <= 0:\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        pop_success = []  # track recent successes for adaptive sigma\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale\n        for i in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            try:\n                f0, x0, _ = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n            pop_success.append(0)\n            if remaining <= 0:\n                break\n\n        # if we couldn't create any population (extremely small budget), do random remaining samples\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # helper to get index of worst in population\n        def worst_index():\n            return int(np.argmax(pop_f))\n\n        # Main optimization loop\n        # The loop continues until budget exhausted\n        while remaining > 0:\n            # pick a parent via small tournament selection\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction and normalize\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # generate again more robustly if degenerate\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with randomized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * self.rng.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try, improved = callf(x_try)\n            except RuntimeError:\n                break\n\n            if improved or f_try < pop_f[parent_i]:\n                # success: accept; increase sigma moderately\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                pop_success[parent_i] = min(10, pop_success[parent_i] + 1)\n                continue  # good move, go to next iteration\n\n            # local backtracking / small-step refinement along the same direction\n            improved_local = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try, improved = callf(x_try)\n                except RuntimeError:\n                    break\n                if improved or f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    pop_success[parent_i] = min(10, pop_success[parent_i] + 1)\n                    improved_local = True\n                    break\n            if improved_local:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            # remove component parallel to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try, improved = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if improved or f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        pop_success[parent_i] = min(10, pop_success[parent_i] + 1)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = self.rng.standard_cauchy(self.dim)\n                # robust normalization to maintain heavy-tail but avoid extreme raw magnitudes\n                denom = max(1e-12, np.percentile(np.abs(step), 90))\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_jump = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_jump, x_jump, improved = callf(x_jump)\n                    except RuntimeError:\n                        break\n                    # replace worst in population if jump is good\n                    wi = worst_index()\n                    if improved or f_jump < pop_f[wi]:\n                        pop[wi] = x_jump\n                        pop_f[wi] = f_jump\n                        pop_sigma[wi] = max(1e-12, sigma * 0.5)\n                        pop_success[wi] = 0\n                # continue to next mechanisms\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * self.rng.randn(self.dim)\n                x_mix = np.clip(mix + noise, lb, ub)\n                try:\n                    f_mix, x_mix, improved = callf(x_mix)\n                except RuntimeError:\n                    break\n                # If mix is good replace parent or worst\n                if improved or f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    pop_success[parent_i] = min(10, pop_success[parent_i] + 1)\n                    # inject into population by replacing the worst if better\n                    wi = worst_index()\n                    if f_mix < pop_f[wi]:\n                        pop[wi] = x_mix\n                        pop_f[wi] = f_mix\n                        pop_sigma[wi] = base_sigma * 0.5\n                        pop_success[wi] = 0\n\n            # adapt parent's sigma on failure (slightly decrease)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n            # if parent has had many consecutive failures, try big diversification for that individual\n            if pop_success[parent_i] == 0 and self.rng.rand() < 0.05 and remaining > 0:\n                # replace parent by a perturbed version of the centroid of population\n                centroid = np.mean(pop, axis=0)\n                perturb = (0.5 * base_sigma) * self.rng.randn(self.dim)\n                x_new = np.clip(centroid + perturb, lb, ub)\n                try:\n                    f_new, x_new, improved = callf(x_new)\n                except RuntimeError:\n                    break\n                if improved or f_new < pop_f[parent_i]:\n                    pop[parent_i] = x_new\n                    pop_f[parent_i] = f_new\n                    pop_sigma[parent_i] = base_sigma * (0.6 + 0.6 * self.rng.rand())\n                    pop_success[parent_i] = 0\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                wi = worst_index()\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new, improved = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[wi] = x_new\n                pop_f[wi] = f_new\n                pop_sigma[wi] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n                pop_success[wi] = 0\n\n            # adaptive population maintenance:\n            # occasionally (rare) add a new candidate near the best found so far and remove worst\n            if remaining > 0 and self.rng.rand() < 0.01:\n                best_i = int(np.argmin(pop_f))\n                wi = worst_index()\n                local_step = (0.1 * base_sigma) * self.rng.randn(self.dim)\n                x_new = np.clip(pop[best_i] + local_step, lb, ub)\n                try:\n                    f_new, x_new, improved = callf(x_new)\n                except RuntimeError:\n                    break\n                if improved or f_new < pop_f[wi]:\n                    pop[wi] = x_new\n                    pop_f[wi] = f_new\n                    pop_sigma[wi] = base_sigma * 0.5\n                    pop_success[wi] = 0\n\n            # keep population arrays consistent lengths (in case of rare anomalies)\n            if len(pop) != len(pop_f) or len(pop) != len(pop_sigma) or len(pop) != len(pop_success):\n                # rebuild consistent arrays preserving best individuals\n                zipped = list(zip(pop, pop_f, pop_sigma, pop_success))\n                zipped = sorted(zipped, key=lambda t: t[1])\n                zipped = zipped[:self.pop_size]\n                pop, pop_f, pop_sigma, pop_success = map(list, zip(*zipped))\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.338 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10536142511068358, 0.1712200717596326, 0.46702309770271033, 0.21701419134187305, 0.6524422149790324, 0.18207985021913065, 0.23997505287762977, 0.4132955194745698, 0.7918346754378595, 0.1430531593639779]}, "task_prompt": ""}
{"id": "509d606d-efa9-4ffa-86f7-283ba0abd88a", "fitness": 0.567159001099401, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that alternate directional local searches, orthogonal refinements, recombination and occasional Lévy-like jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds handling (func.bounds.lb / ub may be scalar or array-like)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # sanitize bounds\n        assert lb.shape == (self.dim,) and ub.shape == (self.dim,)\n        assert np.all(ub >= lb)\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best found\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.clip(x, lb, ub)\n            remaining -= 1\n            f = float(func(x))\n            # update global best\n            nonlocal_f_opt = f  # local alias for clarity\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # trivial fallback: if budget extremely small, random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial typical scale\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # individualized sigmas to allow varied step sizes\n            pop_sigma.append(base_sigma * (0.5 + np.random.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # parameters\n        max_scale = np.mean(ub - lb)\n        tournament_k = min(3, len(pop))\n\n        # main loop: use remaining budget\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(tournament_k, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, max_scale)\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, 0.125):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d  # make orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, max_scale)\n                        pop_f[parent_i] = f_try\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.15 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                # avoid infinite values by using tan(pi*(u-0.5)) sampling, then robust-normalize\n                u = np.random.rand(self.dim)\n                step = np.tan(np.pi * (u - 0.5))\n                # normalize by a robust scale to avoid a single huge coordinate dominating\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(pop[a] * beta + pop[b] * (1.0 - beta) + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (slight cooling)\n            pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.98)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.567 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11016463671329879, 0.14508604298860384, 0.8769103728635574, 0.9589782241854494, 0.8867376574614129, 0.9381858666308321, 0.25103276745720604, 0.48235113807553964, 0.8634904465905014, 0.15865285802760876]}, "task_prompt": ""}
{"id": "6d850749-a733-49ab-a645-c8277a5dfaca", "fitness": 0.4953825821702188, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining randomized directional local searches, orthogonal refinements, adaptive step-size control and occasional heavy-tailed (Lévy-like) jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed for reproducibility\n\n    The callable func must accept a solution vector x and return a scalar objective.\n    func.bounds.lb / func.bounds.ub are used if present; otherwise [-5, 5] is assumed.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimensionality\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # RNG: use RandomState for stable old-style interface\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # infer bounds if possible\n        if hasattr(func, \"bounds\") and getattr(func.bounds, \"lb\", None) is not None and getattr(func.bounds, \"ub\", None) is not None:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        else:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # budget tracking\n        remaining = int(self.budget)\n\n        # helper: evaluate while tracking remaining budget and best-so-far\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise StopIteration(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget trivial, fallback to a tiny random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n        for _ in range(n_init):\n            # uniform vector sample\n            x0 = lb + self.rng.rand(self.dim) * (ub - lb)\n            try:\n                f0, x0 = callf(x0)\n            except StopIteration:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.rand()))\n        if len(pop) == 0:\n            return self.f_opt, self.x_opt\n\n        # Main loop\n        while remaining > 0:\n            # current indices and worst/best\n            pf = np.array(pop_f)\n            worst_i = int(np.argmax(pf))\n            best_i = int(np.argmin(pf))\n\n            # small tournament selection for parent\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), size=k, replace=False)\n            parent_i = inds[int(np.argmin([pop_f[i] for i in inds]))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # randomized direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # skip this iteration if degenerate direction\n                continue\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except StopIteration:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_bt = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_bt, x_bt = callf(x_bt)\n                except StopIteration:\n                    break\n                if f_bt < pop_f[parent_i]:\n                    pop[parent_i] = x_bt\n                    pop_f[parent_i] = f_bt\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.95)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification (small magnitude)\n            r = self.rng.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                step = 0.5 * sigma\n                x_o = np.clip(x_parent + step * r, lb, ub)\n                try:\n                    f_o, x_o = callf(x_o)\n                except StopIteration:\n                    break\n                if f_o < pop_f[parent_i]:\n                    pop[parent_i] = x_o\n                    pop_f[parent_i] = f_o\n                    pop_sigma[parent_i] = max(1e-12, sigma * 1.1)\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.15:  # 15% chance for a jump attempt\n                # generate Cauchy-like heavy-tailed vector then normalize to robust scale\n                step_vec = self.rng.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step_vec), 90) + 1e-12\n                levy = (step_vec / denom) * sigma * (2.5 + 1.5 * self.rng.rand())  # variable magnitude\n                x_l = np.clip(x_parent + levy, lb, ub)\n                try:\n                    f_l, x_l = callf(x_l)\n                except StopIteration:\n                    break\n                # if it's good, replace the worst in population\n                if f_l < pop_f[worst_i]:\n                    pop[worst_i] = x_l\n                    pop_f[worst_i] = f_l\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                    continue  # successful escape attempt\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and self.rng.rand() < 0.7:\n                best2 = np.argsort(pf)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * self.rng.randn(self.dim)\n                x_mix = np.clip(mix + noise, lb, ub)\n                try:\n                    f_mix, x_mix = callf(x_mix)\n                except StopIteration:\n                    break\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.9)\n                elif f_mix < pop_f[worst_i]:\n                    pop[worst_i] = x_mix\n                    pop_f[worst_i] = f_mix\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.7)\n\n            # adapt parent sigma slightly on failures to maintain diversity\n            # small random walk on sigma, but clipped to sensible range\n            pop_sigma[parent_i] = np.clip(\n                pop_sigma[parent_i] * (1.0 + 0.04 * (self.rng.rand() - 0.5)),\n                1e-12,\n                np.mean(ub - lb)\n            )\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                x_new = lb + self.rng.rand(self.dim) * (ub - lb)\n                try:\n                    f_new, x_new = callf(x_new)\n                except StopIteration:\n                    break\n                # always inject to worst slot (keeps diversity)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.495 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09399437936747757, 0.16081267134915322, 0.7505645521451275, 0.9644370282922695, 0.6991759234638837, 0.6448954028234863, 0.25333389571306975, 0.48953183113589394, 0.7328282856244689, 0.16425185178735846]}, "task_prompt": ""}
{"id": "2973fc38-58b5-487a-ad3a-22a28ab5d68d", "fitness": 0.566130885859873, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based adaptive step-size method that mixes randomized directional local searches, orthogonal refinements and occasional Lévy-like heavy-tailed jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # scale with dimensionality but keep modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds from provided function object\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback: assume common [-5,5] if not provided\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n\n        # ensure bounds are full-dim arrays\n        if lb.ndim == 0:\n            lb = np.repeat(lb, self.dim)\n        if ub.ndim == 0:\n            ub = np.repeat(ub, self.dim)\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n        # sanity\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            nonlocal_best = (f < self.f_opt)\n            if nonlocal_best:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search initialization\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale (quarter of range)\n        for i in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            try:\n                f0, x0_clipped = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0_clipped.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * self.rng.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                try:\n                    f, x = callf(x)\n                except RuntimeError:\n                    break\n            return self.f_opt, self.x_opt\n\n        # Convert lists to numpy arrays where convenient\n        pop = [p.copy() for p in pop]\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        iter_since_improve = 0\n        total_iters = 0\n        # probabilities and parameters\n        p_jump = 0.08\n        p_recomb = 0.12\n        tournament_k = max(2, min(4, int(np.sqrt(len(pop)))))  # small tournament\n        max_iters = 100000000  # safety large\n        while remaining > 0 and total_iters < max_iters:\n            total_iters += 1\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(tournament_k, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # make step length random log-normal around sigma\n            step_len = sigma * np.exp(0.4 * self.rng.randn())\n            x_try = np.minimum(np.maximum(x_parent + step_len * d, lb), ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.25, np.mean(ub - lb))\n                improved = True\n                iter_since_improve = 0\n                continue\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                for frac in (0.5, 0.25, 0.1):\n                    if remaining <= 0:\n                        break\n                    x_try = np.minimum(np.maximum(x_parent + (step_len * frac) * d, lb), ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                        improved = True\n                        iter_since_improve = 0\n                        break\n                if improved:\n                    continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = self.rng.randn(self.dim)\n                # make orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    orth_step = sigma * (0.4 + 0.6 * self.rng.rand()) * r\n                    x_try = np.minimum(np.maximum(x_parent + orth_step, lb), ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                        improved = True\n                        iter_since_improve = 0\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < p_jump and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = self.rng.standard_cauchy(self.dim)\n                # robust denom avoid infs/extremes\n                med = np.median(np.abs(step))\n                denom = med if med > 1e-8 else (np.mean(np.clip(np.abs(step), 0, 1e6)) + 1e-8)\n                scale_vec = 0.6 * np.mean(ub - lb) * (0.4 + self.rng.rand())\n                step = (step / denom) * scale_vec\n                x_try = np.minimum(np.maximum(x_parent + step, lb), ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(max(sigma, scale_vec), np.mean(ub - lb))\n                    improved = True\n                    iter_since_improve = 0\n                else:\n                    # if it's good compared to the worst, replace worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * self.rng.rand())\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if self.rng.rand() < p_recomb and remaining > 0 and len(pop) >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = self.rng.rand()\n                mix = pop[a] + beta * (pop[b] - pop[a])\n                noise = (0.02 * (ub - lb)) * self.rng.randn(self.dim)\n                x_try = np.minimum(np.maximum(mix + noise, lb), ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, np.mean(ub - lb))\n                    improved = True\n                    iter_since_improve = 0\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + 0.6 * self.rng.rand())\n                continue\n\n            # adapt parent sigma on failure\n            if not improved:\n                pop_sigma[parent_i] = max(1e-10, pop_sigma[parent_i] * (0.95 - 0.02 * self.rng.rand()))\n                iter_since_improve += 1\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if iter_since_improve > 50 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * self.rng.rand())\n                iter_since_improve = 0\n                continue\n\n            # small population maintenance: occasionally shrink or expand sigma of worst\n            if self.rng.rand() < 0.03 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                pop_sigma[worst_i] = base_sigma * (0.5 + self.rng.rand())\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.566 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15096949213512667, 0.16931687813774476, 0.8334523188773832, 0.9215401295171781, 0.8820330009734324, 0.9084630376775378, 0.3005126902760549, 0.5168443406598067, 0.81993880349942, 0.15823816684504588]}, "task_prompt": ""}
{"id": "3a480c33-e23e-4836-a549-78f61409cad9", "fitness": 0.5655404654450964, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based heuristic combining randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed (Lévy-like) jumps with adaptive step-sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None it's set adaptively from dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows slowly with dimension\n            self.pop_size = max(4, min(30, int(6 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Determine bounds (BBOB convention often provides func.bounds)\n        if hasattr(func, \"bounds\"):\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n            # broadcast scalars to full dimension\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item(), dtype=float)\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item(), dtype=float)\n        else:\n            # default as stated: [-5, 5] per dimension\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure correct shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = self.budget\n\n        def callf(x):\n            \"\"\"\n            Safe evaluator: clips to bounds, calls func only if budget remains,\n            updates global best, and returns (f, x_clipped).\n            If budget is exhausted, returns (np.inf, x_clipped) without calling func.\n            \"\"\"\n            nonlocal remaining\n            x = np.asarray(x, dtype=float).ravel()\n            if x.size != self.dim:\n                x = np.resize(x, self.dim)\n            x = np.clip(x, lb, ub)\n            if remaining <= 0:\n                return np.inf, x\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # quick fallback if budget is zero\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop = []\n        pop_f = []\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial global scale\n        pop_sigma = []\n\n        for _ in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma)\n\n        pop = np.array(pop) if len(pop) > 0 else np.zeros((0, self.dim))\n        pop_f = np.array(pop_f) if len(pop_f) > 0 else np.array([])\n        pop_sigma = np.array(pop_sigma) if len(pop_sigma) > 0 else np.array([])\n\n        # if we couldn't initialize any member (very small budget), fallback to random\n        if pop.shape[0] == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # main loop\n        # parameters\n        tour_size = min(3, pop.shape[0])\n        levy_prob = 0.08\n        rejuvenate_prob = 0.02\n\n        while remaining > 0:\n            n_pop = pop.shape[0]\n            # tournament selection for parent\n            inds = np.random.choice(n_pop, tour_size, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max(sigma * 1.15, 1e-12), np.max(ub - lb))\n                continue  # go to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                alpha2 = alpha * frac\n                x_try = np.clip(x_parent + alpha2 * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(max(sigma * 1.05, 1e-12), np.max(ub - lb))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            # create a random vector, remove component along d => orthogonal\n            for attempt in range(3):\n                if remaining <= 0:\n                    break\n                r = np.random.randn(self.dim)\n                # subtract projection on d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    continue\n                r = r / nr\n                length = sigma * (0.5 + 0.5 * np.random.rand())  # smaller orthogonal step\n                x_try = np.clip(x_parent + length * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(max(sigma * 1.05, 1e-12), np.max(ub - lb))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < levy_prob and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                scale_vec = 0.2 * (ub - lb)\n                step = (step / denom) * scale_vec\n                x_try = np.clip(x_parent + step, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.5)\n                else:\n                    # if this jump produced a decent candidate, replace worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two good individuals and add small noise\n            # pick second parent preferentially among better half\n            idx_sorted = np.argsort(pop_f)\n            best_i = idx_sorted[0]\n            second_i = idx_sorted[1] if pop.shape[0] > 1 else best_i\n            # mix between parent_i and a top performer (could be best or random good)\n            if np.random.rand() < 0.6:\n                mate_i = best_i if best_i != parent_i else second_i\n            else:\n                mate_i = np.random.choice(idx_sorted[:max(1, pop.shape[0]//2)])\n            mix = np.random.rand() * pop[parent_i] + (1.0 - np.random.rand()) * pop[mate_i]\n            beta = np.random.rand()\n            noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n            x_try = np.clip(mix * beta + (1 - beta) * pop[mate_i] + noise, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                # possibly propagate into worst slot\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                continue\n            else:\n                # adapt parent sigma on failure (slightly shrink)\n                pop_sigma[parent_i] = max(1e-12, sigma * 0.95)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < rejuvenate_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma\n\n            # tiny housekeeping: ensure sigma not too small or huge\n            pop_sigma = np.clip(pop_sigma, 1e-12, np.max(ub - lb))\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.566 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14237866957315948, 0.17284485496177016, 0.8234255494404902, 0.9464982580879804, 0.846651661146638, 0.9020550740743929, 0.2656509942067208, 0.5508699764339977, 0.8436078237766698, 0.16142179274914437]}, "task_prompt": ""}
{"id": "d794533e-9131-47f2-b05d-70b5fee15238", "fitness": 0.5822213287124889, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size probes that perform randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy) jumps to balance exploitation and heavy-tailed exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a small multiple of dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # heuristic default pop_size: grows with dim but stays modest\n        if pop_size is None:\n            self.pop_size = max(4, min(30, 6 + self.dim // 2))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.RandomState(seed)\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds (BBOB style: func.bounds.lb / ub)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # broadcast scalar bounds to full-dim arrays if necessary\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # Clip sanity\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        # internal remaining budget counter\n        remaining = int(self.budget)\n\n        # helper to call objective while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No remaining budget to evaluate function.\")\n            x = np.asarray(x, dtype=float).ravel()\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population uniformly inside bounds\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        box_range = ub - lb\n        base_sigma = 0.2 * np.maximum(1e-12, np.mean(box_range))  # typical scale\n        for i in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = lb + self.rng.rand(self.dim) * box_range\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # individual sigma: randomized around base_sigma, scaled by problem box\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = lb + self.rng.rand(self.dim) * box_range\n                try:\n                    f, x = callf(x)\n                except RuntimeError:\n                    break\n            return self.f_opt, self.x_opt\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        # various hyperparameters (stochastic)\n        p_levy = 0.06              # probability to attempt a Levy jump each iteration\n        p_rejuv = 0.03             # probability to rejuvenate worst\n        max_sigma = np.mean(box_range) * 1.5\n        min_sigma = 1e-12\n\n        # loop until budget exhausted\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # step length sampled log-uniform around sigma\n            alpha = sigma * np.exp(0.2 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.2)\n                continue  # continue with next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                step = frac * alpha\n                x_try = np.clip(x_parent + step * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    remaining = 0\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.15)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = self.rng.randn(self.dim)\n                # make orthogonal to d (Gram-Schmidt)\n                r = r - (r.dot(d)) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    step = 0.6 * sigma * (0.5 + self.rng.rand())\n                    x_try = np.clip(x_parent + step * r, lb, ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(max_sigma, pop_sigma[parent_i] * 1.1)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < p_levy and remaining > 0:\n                # sample Cauchy (standard) per-dimension, then robustly scale\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization to avoid inf extremes\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                scaled = (step / denom) * (0.5 * np.mean(box_range)) * (1.0 + self.rng.rand())\n                x_try = np.clip(x_parent + scaled, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # decide whether to insert into population: replace worst if better\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(min_sigma, sigma * 0.5)\n                else:\n                    # keep as occasional candidate by possibly replacing parent if marginally better\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(min_sigma, sigma * 0.8)\n                # continue main loop\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                b0, b1 = best2[0], best2[1]\n                beta = self.rng.rand()\n                mix = beta * pop[b0] + (1.0 - beta) * pop[b1]\n                noise = (0.02 * box_range) * self.rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(min_sigma, pop_sigma[parent_i] * 0.9)\n                elif f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (no improvement for this parent in this cycle)\n            # small shrink to encourage exploration-contraction\n            pop_sigma[parent_i] = max(min_sigma, pop_sigma[parent_i] * 0.96)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < p_rejuv and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = lb + self.rng.rand(self.dim) * box_range\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.5 + self.rng.rand())\n\n            # ensure population arrays remain consistent length in case something changed\n            # (they shouldn't in this implementation, but keep safe)\n            if len(pop) == 0:\n                break\n\n        # finished budget or early stop\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.582 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15836683898381165, 0.15832548159361337, 0.8278151520310301, 0.9553847092206396, 0.8523124600056616, 0.8916505476739061, 0.3228270012703497, 0.6335373898012392, 0.856332645431754, 0.16566106111288392]}, "task_prompt": ""}
{"id": "bbc8309e-1715-474b-82dd-8d75881d97db", "fitness": "-inf", "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, recombination and occasional Lévy (Cauchy) jumps to robustly explore/exploit continuous landscapes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed\n    Notes:\n      - The target function `func` is expected to provide bounds as func.bounds.lb and func.bounds.ub\n        (arrays of length dim). If not present, defaults to [-5, 5] per dimension.\n      - __call__ returns (best_value, best_x)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # obtain bounds or use default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float).reshape(self.dim)\n            ub = np.asarray(func.bounds.ub, dtype=float).reshape(self.dim)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining, self, lb, ub\n            if remaining <= 0:\n                return None, None\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if (self.f_opt is None) or (f < self.f_opt):\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # Initialize population uniformly\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        # initial sigma scale relative to bounds\n        base_scale = np.maximum(1e-12, 0.25 * (ub - lb))\n        for i in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x = np.random.uniform(lb, ub)\n            f, x_eval = callf(x)\n            if f is None:\n                break\n            pop.append(x_eval)\n            pop_f.append(f)\n            # individual sigma starts with a random fraction of base_scale\n            pop_sigma.append(np.maximum(1e-12, base_scale * (0.25 + 0.75 * np.random.rand())))\n\n        # if no population created due to tiny budget, fallback to random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, _ = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # main loop: directional local searches, orthogonal tries, Lévy jumps, recombination, rejuvenation\n        tournament_k = min(3, len(pop))\n        while remaining > 0:\n            # pick parent via small tournament\n            inds = np.random.choice(len(pop), tournament_k, replace=False)\n            values = pop_f[inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = np.maximum(1e-12, pop_sigma[parent_i]).copy()\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # degenerate, skip and continue\n                pop_sigma[parent_i] = sigma * 0.9\n                continue\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # step length sampled log-normal-ish: sigma * (1 + gaussian noise)\n            alpha = np.abs(sigma * (1.0 + 0.4 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try is None:\n                break\n\n            # success: accept and slightly increase sigma\n            if f_try < pop_f[parent_i]:\n                pop_f[parent_i] = f_try\n                pop[parent_i] = x_try\n                pop_sigma[parent_i] = np.minimum(np.maximum(sigma * 1.2, 1e-12), (ub - lb).max())\n                continue  # move to next iteration\n\n            # backtracking / small-step refinement along direction (few tries)\n            back_alpha = alpha\n            improved = False\n            for bt in range(3):\n                back_alpha *= 0.6\n                x_bt = np.clip(x_parent + back_alpha * d, lb, ub)\n                f_bt, x_bt = callf(x_bt)\n                if f_bt is None:\n                    break\n                if f_bt < pop_f[parent_i]:\n                    pop_f[parent_i] = f_bt\n                    pop[parent_i] = x_bt\n                    pop_sigma[parent_i] = np.maximum(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # remove projection on d to make it (approximately) orthogonal\n            proj = np.dot(r, d) * d\n            r = r - proj\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                ortho_step = 0.5 * sigma * r\n                x_o = np.clip(x_parent + ortho_step, lb, ub)\n                f_o, x_o = callf(x_o)\n                if f_o is None:\n                    break\n                if f_o < pop_f[parent_i]:\n                    pop_f[parent_i] = f_o\n                    pop[parent_i] = x_o\n                    pop_sigma[parent_i] = np.maximum(sigma * 0.95, 1e-12)\n                    continue\n\n            # occasional Lévy-like (Cauchy) jump to escape local basins\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector per-dimension\n                u = np.random.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                # normalize then scale by robust vector scale\n                scale_vec = 0.2 * (ub - lb)\n                # keep magnitude controlled by sigma and scale_vec\n                jump = cauchy\n                jm_norm = np.linalg.norm(jump)\n                if jm_norm > 0:\n                    jump = jump / jm_norm\n                x_jump = np.clip(x_parent + (1.5 * sigma) * jump * (scale_vec / (scale_vec.mean() + 1e-12)), lb, ub)\n                f_jump, x_jump = callf(x_jump)\n                if f_jump is None:\n                    break\n                # if it's good replace the worst in population, else maybe keep as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = np.maximum(sigma * 0.9, 1e-12)\n\n            # recombination exploitation: mix two best and small noise\n            best2 = np.argsort(pop_f)[:2]\n            if len(best2) == 2:\n                w = np.random.rand()\n                x_mix = w * pop[best2[0]] + (1 - w) * pop[best2[1]]\n                x_mix += 0.05 * sigma * np.random.randn(self.dim)  # small noise\n                x_mix = np.clip(x_mix, lb, ub)\n                f_mix, x_mix = callf(x_mix)\n                if f_mix is None:\n                    break\n                # replace parent or worst if beneficial\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = np.maximum(sigma * 0.95, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = np.maximum(sigma * 0.9, 1e-12)\n\n            # adapt parent sigma on failure (shrink)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.05 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new is None:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = np.maximum(base_scale * (0.25 + 0.75 * np.random.rand()), 1e-12)\n\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "In the code, line 202, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)", "error": "In the code, line 202, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "003aa3ea-a3c4-4823-aa78-e527c10e906c", "fitness": 0.1607986639303338, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive local searches along randomized directions with orthogonal refinements and occasional heavy-tailed Lévy jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional arguments:\n    - pop_size: override population size (default scales with dim)\n    - seed: optional random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimension but stays bounded\n            self.pop_size = max(4, min(40, int(2 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n    def __call__(self, func):\n        # prepare bounds as full-dim arrays (Many BBOB provides scalars or arrays)\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        # handle scalar bounds and zero-d arrays\n        if np.isscalar(lb):\n            lb = np.full(self.dim, float(lb))\n        else:\n            lb = np.asarray(lb, dtype=float).reshape(self.dim,)\n        if np.isscalar(ub):\n            ub = np.full(self.dim, float(ub))\n        else:\n            ub = np.asarray(ub, dtype=float).reshape(self.dim,)\n\n        # remaining budget and best trackers\n        remaining = int(self.budget)\n        best_f = np.inf\n        best_x = np.zeros(self.dim, dtype=float)\n\n        # safe evaluation wrapper to count budget and update global best\n        def callf(x):\n            nonlocal remaining, best_f, best_x\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            x = np.clip(x, lb, ub)\n            if remaining <= 0:\n                return np.inf, x\n            try:\n                f = float(func(x))\n            except Exception:\n                # in case func expects different shape, still attempt\n                f = float(func(np.copy(x)))\n            remaining -= 1\n            if f < best_f:\n                best_f = f\n                best_x = x.copy()\n            return f, x\n\n        # Tiny-budget fallback: pure random search\n        if remaining <= 0:\n            return best_f, best_x\n        if remaining < max(5, self.dim // 2):\n            # very small budget: just random sampling\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return best_f, best_x\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base scale proportional to search range\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n\n        for i in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # each individual has adaptive step-size, slightly randomized initially\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n        # if we couldn't initialize any individual (extremely low budget), do random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return best_f, best_x\n\n        pop = np.vstack(pop)\n        pop_f = np.array(pop_f, dtype=float)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n        pop_size = pop.shape[0]\n\n        # parameters controlling behaviors\n        p_ortho = 0.25               # chance to try an orthogonal perturbation\n        p_jump = 0.06                # chance for a Lévy-like jump attempt\n        p_rejuvenate = 0.02          # chance to randomly replace worst\n        tournament_k = min(3, pop_size)\n        max_backtrack = 3\n        small_noise_scale = 0.02     # for recombination noise relative to sigma\n\n        # main optimization loop\n        while remaining > 0:\n            # pick a parent by small tournament (prefer better solutions)\n            candidates = np.random.choice(pop_size, size=tournament_k, replace=False)\n            parent_i = candidates[np.argmin(pop_f[candidates])]\n            worst_i = int(np.argmax(pop_f))\n            sigma = pop_sigma[parent_i]\n            x_parent = pop[parent_i].copy()\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # step length is sigma multiplied by a log-normal-ish perturbation to encourage variety\n            s = sigma * max(1e-12, (1.0 + 0.6 * np.random.randn()))\n            x_try = np.clip(x_parent + s * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min((ub - lb).max(), pop_sigma[parent_i] * 1.12 + 1e-12)\n                # local backtracking / small-step refinement along direction (few tries)\n                scale = 0.6\n                for bt in range(max_backtrack):\n                    if remaining <= 0:\n                        break\n                    x_bt = np.clip(x_try + (scale * s) * d, lb, ub)\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt < pop_f[parent_i]:\n                        pop[parent_i] = x_bt\n                        pop_f[parent_i] = f_bt\n                        x_try = x_bt\n                        scale *= 0.5\n                        continue\n                    else:\n                        break\n                continue  # go to next iteration after successful directional improvement\n\n            else:\n                # directional attempt failed: reduce sigma a bit for this parent\n                pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.92)\n\n            # try an orthogonal perturbation for local diversification\n            if np.random.rand() < p_ortho and remaining > 0:\n                r = np.random.randn(self.dim)\n                # make orthogonal to d\n                r = r - np.dot(r, d) * d\n                rn = np.linalg.norm(r) + 1e-12\n                r = r / rn\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] *= 1.05\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < p_jump and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                c = np.random.standard_cauchy(self.dim)\n                # robust scale: divide by median absolute to avoid infinite explosion\n                mad = np.median(np.abs(c)) + 1e-12\n                c = c / mad\n                # limit extreme tail a bit by tanh\n                c = np.tanh(c) + 0.05 * np.random.randn(self.dim)\n                # scale the jump relative to sigma and search range\n                jump_scale = max(1.0, 8.0 * sigma / (base_sigma + 1e-12))\n                x_jump = np.clip(x_parent + 0.6 * jump_scale * (ub - lb) * c / (np.linalg.norm(c) + 1e-12), lb, ub)\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < pop_f[worst_i]:\n                    # if it's good, replace the worst\n                    pop[worst_i] = x_jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = pop_sigma[parent_i] * 0.8\n                else:\n                    # else, maybe keep as a new candidate replacing parent if slightly better\n                    if f_jump < pop_f[parent_i]:\n                        pop[parent_i] = x_jump\n                        pop_f[parent_i] = f_jump\n                        pop_sigma[parent_i] *= 1.02\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if pop_size >= 2 and remaining > 0:\n                best2_idx = np.argsort(pop_f)[:2]\n                b0, b1 = best2_idx[0], best2_idx[1]\n                beta = np.random.rand()\n                x_rec = beta * pop[b0] + (1.0 - beta) * pop[b1]\n                # add tiny adaptive noise\n                noise = small_noise_scale * pop_sigma[parent_i] * np.random.randn(self.dim)\n                x_rec = np.clip(x_rec + noise, lb, ub)\n                f_rec, x_rec = callf(x_rec)\n                # replace worst if it's better than worst\n                if f_rec < pop_f[worst_i]:\n                    pop[worst_i] = x_rec\n                    pop_f[worst_i] = f_rec\n                    pop_sigma[worst_i] = pop_sigma[parent_i] * 0.7\n                # also possibly inject into parent\n                if f_rec < pop_f[parent_i]:\n                    pop[parent_i] = x_rec\n                    pop_f[parent_i] = f_rec\n                    pop_sigma[parent_i] *= 1.08\n\n            # adapt parent sigma slightly on failure to encourage exploration\n            pop_sigma[parent_i] = np.clip(pop_sigma[parent_i] * (0.98 + 0.04 * np.random.rand()), 1e-12, (ub - lb).max())\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < p_rejuvenate and remaining > 0:\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n        return best_f, best_x", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.161 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07259945804212342, 0.16509791058611134, 0.2195038717837512, 0.1458835955172798, 0.17357312436174932, 0.18102953761763307, 0.19611486384665577, 0.1648570938611421, 0.16609415292670482, 0.12323303076018699]}, "task_prompt": ""}
{"id": "27ce1ac1-5c30-410c-8eac-c55906850b89", "fitness": 0.4090610093710024, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional searches with orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimensionality but is bounded\n            self.pop_size = int(min(max(4, 3 * self.dim), 40))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        # Ensure bounds are arrays of correct shape\n        lb = np.array(func.bounds.lb, dtype=float).ravel()\n        ub = np.array(func.bounds.ub, dtype=float).ravel()\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        assert lb.size == self.dim and ub.size == self.dim, \"Bounds must match dimensionality\"\n\n        domain_range = ub - lb\n        domain_norm = float(np.linalg.norm(domain_range)) if np.linalg.norm(domain_range) > 0 else 1.0\n\n        # Remaining budget container to allow modification inside nested callf\n        remaining = [int(self.budget)]\n\n        # best-so-far trackers\n        f_opt = [np.inf]\n        x_opt = [None]\n\n        def callf(x):\n            \"\"\"Evaluate x (clipped). Decrement budget. Update best if improved.\n               Returns (f_val, x_clipped) or (None, None) if budget exhausted.\"\"\"\n            x = np.asarray(x, dtype=float).ravel()\n            if x.size != self.dim:\n                # try to reshape\n                x = np.resize(x, self.dim)\n            x = np.clip(x, lb, ub)\n            if remaining[0] <= 0:\n                return None, None\n            remaining[0] -= 1\n            f = func(x)\n            if f < f_opt[0]:\n                f_opt[0] = float(f)\n                x_opt[0] = x.copy()\n            return float(f), x.copy()\n\n        # Fallback to random search if extremely small budget\n        if remaining[0] <= 0:\n            return f_opt[0], x_opt[0]\n        if remaining[0] < max(4, 2 * self.dim):\n            # simple uniform random search\n            while remaining[0] > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return f_opt[0], x_opt[0]\n\n        # Initialize population with uniform samples\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        init_sigma_base = 0.25 * domain_norm  # initial scalar-step size scale\n        for i in range(self.pop_size):\n            if remaining[0] <= 0:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0c = callf(x0)\n            if f0 is None:\n                break\n            pop.append(x0c)\n            pop_f.append(f0)\n            # sigma is an adaptive scalar (relative to domain_norm) with small randomization\n            sigma0 = init_sigma_base * (0.5 + self.rng.rand())\n            pop_sigma.append(float(max(sigma0, 1e-12)))\n\n        pop = [np.asarray(x, dtype=float) for x in pop]\n        pop_f = np.array(pop_f, dtype=float) if len(pop_f) > 0 else np.array([])\n        pop_sigma = np.array(pop_sigma, dtype=float) if len(pop_sigma) > 0 else np.array([])\n\n        if len(pop) == 0:\n            # if no population could be created (very tight budget), fallback to random sampling with remaining budget\n            while remaining[0] > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return f_opt[0], x_opt[0]\n\n        # Helper to find index of worst and best\n        def best_indices(k=1):\n            idx = np.argsort(pop_f)  # ascending (best first)\n            return idx[:k] if k > 1 else idx[0]\n\n        # Main loop\n        # Parameters\n        tournament_k = min(3, len(pop))\n        p_levy = 0.06\n        p_rejuvenate = 0.02\n        increase_factor = 1.08\n        decrease_factor = 0.92\n        min_sigma = 1e-12\n        max_sigma = 2.0 * domain_norm\n\n        # loop until budget exhausted\n        while remaining[0] > 0:\n            # Choose a parent by small tournament (to bias to better solutions)\n            t_idx = self.rng.choice(len(pop), size=tournament_k, replace=False)\n            parent_i = int(t_idx[np.argmin(pop_f[t_idx])])\n            x_parent = pop[parent_i].copy()\n            f_parent = float(pop_f[parent_i])\n            sigma = float(pop_sigma[parent_i])\n\n            # Sample a random search direction\n            r = self.rng.normal(size=self.dim)\n            nr = np.linalg.norm(r)\n            if nr < 1e-12:\n                # fallback to uniform direction\n                r = self.rng.randn(self.dim)\n                nr = np.linalg.norm(r) + 1e-12\n            r_hat = r / nr\n\n            # Primary directional trial with stochasticized step-length\n            step_scale = sigma * max(1e-12, 1.0 + 0.4 * self.rng.normal())\n            x_try = np.clip(x_parent + step_scale * r_hat, lb, ub)\n            if remaining[0] <= 0:\n                break\n            f_try, x_tryc = callf(x_try)\n            if f_try is None:\n                break\n\n            success = False\n            if f_try < f_parent:\n                # accept and slightly increase sigma\n                pop[parent_i] = x_tryc\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max_sigma, sigma * increase_factor)\n                success = True\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                local_step = step_scale\n                for back in range(3):\n                    if remaining[0] <= 0:\n                        break\n                    local_step *= 0.5\n                    x_bt = np.clip(x_parent + local_step * r_hat, lb, ub)\n                    f_bt, x_btc = callf(x_bt)\n                    if f_bt is None:\n                        break\n                    if f_bt < f_parent:\n                        pop[parent_i] = x_btc\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = min(max_sigma, sigma * (1.0 + 0.03 * (back + 1)))\n                        success = True\n                        break\n\n            # Orthogonal perturbation for local diversification\n            if remaining[0] > 0 and not success:\n                # make a vector orthogonal to r_hat\n                v = self.rng.normal(size=self.dim)\n                proj = np.dot(v, r_hat) * r_hat\n                orth = v - proj\n                nor = np.linalg.norm(orth)\n                if nor > 1e-12:\n                    orth_hat = orth / nor\n                    x_orth = np.clip(x_parent + 0.6 * sigma * orth_hat, lb, ub)\n                    f_orth, x_orthc = callf(x_orth)\n                    if f_orth is None:\n                        break\n                    if f_orth < f_parent:\n                        pop[parent_i] = x_orthc\n                        pop_f[parent_i] = f_orth\n                        pop_sigma[parent_i] = min(max_sigma, sigma * (1.0 + 0.05))\n                        success = True\n\n            # Occasional Lévy-like jump to escape local basins\n            if remaining[0] > 0 and (not success) and (self.rng.rand() < p_levy):\n                # Cauchy-like heavy-tailed vector (component-wise), robustly scaled\n                v = self.rng.standard_cauchy(size=self.dim)\n                med = np.median(np.abs(v))\n                if med <= 0 or not np.isfinite(med):\n                    med = 1.0\n                v = v / med\n                scale_vec = 0.18 * domain_range  # elementwise scale\n                x_levy = x_parent + v * scale_vec\n                # normalize to avoid extreme excursions: clip to a few domain ranges away\n                x_levy = np.clip(x_levy, lb - 1.0 * domain_range, ub + 1.0 * domain_range)\n                x_levy = np.clip(x_levy, lb, ub)\n                f_levy, x_levyc = callf(x_levy)\n                if f_levy is None:\n                    break\n                if f_levy < np.max(pop_f):\n                    # replace the worst with this jump if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    pop[worst_i] = x_levyc\n                    pop_f[worst_i] = f_levy\n                    pop_sigma[worst_i] = max(min_sigma, sigma * 0.5)\n                else:\n                    # keep it as a candidate by replacing parent rarely\n                    if f_levy < f_parent:\n                        pop[parent_i] = x_levyc\n                        pop_f[parent_i] = f_levy\n                        pop_sigma[parent_i] = max(min_sigma, sigma * 0.6)\n\n            # Recombination exploitation: mix two best individuals and add small noise\n            if remaining[0] > 0 and self.rng.rand() < 0.35 and len(pop) >= 2:\n                # pick two best distinct indices\n                sorted_idx = np.argsort(pop_f)\n                i1 = int(sorted_idx[0])\n                i2 = int(sorted_idx[1]) if sorted_idx.size > 1 else int(sorted_idx[0])\n                alpha = 0.3 + 0.4 * self.rng.rand()  # between 0.3 and 0.7\n                noise = 0.02 * sigma * self.rng.normal(size=self.dim)\n                x_recomb = np.clip(alpha * pop[i1] + (1 - alpha) * pop[i2] + noise, lb, ub)\n                f_recomb, x_recombc = callf(x_recomb)\n                if f_recomb is None:\n                    break\n                if f_recomb < f_parent:\n                    pop[parent_i] = x_recombc\n                    pop_f[parent_i] = f_recomb\n                    pop_sigma[parent_i] = min(max_sigma, sigma * (1.0 + 0.04))\n                else:\n                    # try inject into population by replacing worst if it helps\n                    worst_i = int(np.argmax(pop_f))\n                    if f_recomb < pop_f[worst_i]:\n                        pop[worst_i] = x_recombc\n                        pop_f[worst_i] = f_recomb\n                        pop_sigma[worst_i] = max(min_sigma, sigma * 0.5)\n\n            # Adapt parent sigma on failure\n            if not success:\n                pop_sigma[parent_i] = max(min_sigma, sigma * decrease_factor)\n\n            # Occasional population rejuvenation by replacing the worst with a random sample\n            if remaining[0] > 0 and self.rng.rand() < p_rejuvenate:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_newc = callf(x_new)\n                if f_new is None:\n                    break\n                pop[worst_i] = x_newc\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = max(min_sigma, init_sigma_base * (0.5 + self.rng.rand()))\n\n            # Safety: if any sigma went NaN or inf, reset\n            nan_mask = ~np.isfinite(pop_sigma)\n            if np.any(nan_mask):\n                pop_sigma[nan_mask] = init_sigma_base\n\n            # If population collapsed (all identical), diversify a bit\n            if remaining[0] > 0:\n                unique_count = len({tuple(np.round(x, 12)) for x in pop})\n                if unique_count <= max(1, len(pop) // 4) and self.rng.rand() < 0.3:\n                    # replace half of worst with random\n                    kreplace = max(1, len(pop) // 2)\n                    worsts = np.argsort(pop_f)[-kreplace:]\n                    for wi in worsts:\n                        if remaining[0] <= 0:\n                            break\n                        x_new = self.rng.uniform(lb, ub)\n                        f_new, x_newc = callf(x_new)\n                        if f_new is None:\n                            break\n                        pop[wi] = x_newc\n                        pop_f[wi] = f_new\n                        pop_sigma[wi] = init_sigma_base * (0.5 + self.rng.rand())\n\n        # End main loop\n        return float(f_opt[0]), (None if x_opt[0] is None else x_opt[0].copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.409 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16528845871172537, 0.1571033875493557, 0.3907094496164285, 0.9383979799980209, 0.29551311566074, 0.9363444932075123, 0.27085619359768154, 0.49805783869189746, 0.24598103197923027, 0.19235814469743273]}, "task_prompt": ""}
{"id": "7edfd437-87fd-4fec-bead-f46c67cb7d1b", "fitness": 0.4853294358145874, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based continuous optimizer combining adaptive directional searches, orthogonal refinements, recombination and occasional Cauchy/Lévy jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional random seed for reproducibility\n\n    Usage:\n        solver = ADLS(budget=10000, dim=10)\n        f_opt, x_opt = solver(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # scale pop with problem dimension but keep moderate\n            self.pop_size = max(6, min(40, 4 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        self._rng = np.random.RandomState(seed)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Read bounds and ensure they are arrays of correct shape\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim, \"Bounds must match dimension\"\n\n        # Internal evaluation wrapper to track budget and best\n        self._evals = 0\n        def callf(x):\n            # ensure x is numpy array of correct shape\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            if self._evals >= self.budget:\n                # budget exhausted, do not call func\n                return np.inf, x\n            self._evals += 1\n            f = func(x)\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget extremely small, fallback to simple random search\n        if self.budget <= 2:\n            # sample random points up to budget\n            for _ in range(self.budget):\n                x = self._rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Determine population size respecting budget\n        pop_n = min(self.pop_size, max(2, self.budget // 3))\n        if pop_n <= 2:\n            # if too small budget for population, pure random search\n            while self._evals < self.budget:\n                x = self._rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population uniformly across bounds\n        pop_x = np.zeros((pop_n, self.dim))\n        pop_f = np.full(pop_n, np.inf)\n        # initial sigma per individual (scalar) as fraction of domain width\n        domain_scale = np.maximum(ub - lb, 1e-8)\n        base_sigma = 0.2 * np.linalg.norm(domain_scale) / np.sqrt(self.dim)  # global scale\n        pop_sigma = np.full(pop_n, base_sigma)\n        for i in range(pop_n):\n            if self._evals >= self.budget:\n                break\n            x = self._rng.uniform(lb, ub)\n            f, x = callf(x)\n            pop_x[i] = x\n            pop_f[i] = f\n\n        # If only few evaluations used due to tiny budget, finish\n        if self._evals >= self.budget:\n            return self.f_opt, self.x_opt\n\n        # Ensure we have valid population (replace any inf by random samples)\n        for i in range(pop_n):\n            if np.isinf(pop_f[i]):\n                if self._evals < self.budget:\n                    x = self._rng.uniform(lb, ub)\n                    f, x = callf(x)\n                    pop_x[i] = x\n                    pop_f[i] = f\n                else:\n                    pop_f[i] = 1e300\n\n        # Main optimization loop\n        stagnation_counter = 0\n        last_best = self.f_opt\n        # parameters\n        tournament_k = min(3, pop_n)\n        p_levy = 0.06  # probability of a Lévy/Cauchy jump\n        p_rejuvenate = 0.02\n        max_local_refinements = 3\n\n        while self._evals < self.budget:\n            remaining = self.budget - self._evals\n\n            # pick a parent via small tournament\n            contenders = self._rng.choice(pop_n, size=tournament_k, replace=False)\n            idx = contenders[np.argmin(pop_f[contenders])]\n            parent_x = pop_x[idx].copy()\n            parent_f = pop_f[idx]\n            parent_sigma = pop_sigma[idx]\n\n            # sample a random search direction (normalized)\n            d = self._rng.normal(size=self.dim)\n            d_norm = np.linalg.norm(d)\n            if d_norm == 0:\n                d = np.ones(self.dim)\n                d_norm = np.linalg.norm(d)\n            d = d / d_norm\n\n            # stochasticized step-length (log-normal multiplier)\n            step_multiplier = 10 ** self._rng.normal(loc=0.0, scale=0.18)\n            step = d * parent_sigma * step_multiplier\n\n            # primary directional trial\n            f1, x1 = callf(parent_x + step)\n            if f1 < parent_f:\n                # accept and slightly increase sigma\n                pop_x[idx] = x1\n                pop_f[idx] = f1\n                pop_sigma[idx] = parent_sigma * (1.08 + 0.02 * self._rng.randn())\n                stagnation_counter = 0\n            else:\n                # local backtracking / small-step refinement along direction\n                improved = False\n                for j, frac in enumerate([0.5, 0.25, 0.1][:max_local_refinements]):\n                    if self._evals >= self.budget:\n                        break\n                    small_step = d * parent_sigma * frac * (1 + 0.5 * self._rng.randn())\n                    f2, x2 = callf(parent_x + small_step)\n                    if f2 < parent_f:\n                        pop_x[idx] = x2\n                        pop_f[idx] = f2\n                        pop_sigma[idx] = parent_sigma * (1.04 + 0.02 * self._rng.randn())\n                        improved = True\n                        stagnation_counter = 0\n                        break\n                if not improved:\n                    # try orthogonal perturbation for local diversification\n                    v = self._rng.normal(size=self.dim)\n                    # make it orthogonal to d\n                    v = v - np.dot(v, d) * d\n                    v_norm = np.linalg.norm(v)\n                    if v_norm > 1e-12:\n                        v = v / v_norm\n                        orth_scale = parent_sigma * 0.6 * (1 + 0.5 * self._rng.randn())\n                        f3, x3 = callf(parent_x + v * orth_scale)\n                        if f3 < parent_f:\n                            pop_x[idx] = x3\n                            pop_f[idx] = f3\n                            pop_sigma[idx] = parent_sigma * (1.06 + 0.03 * self._rng.randn())\n                            stagnation_counter = 0\n                        else:\n                            # adapt parent sigma on failure (slight reduction)\n                            pop_sigma[idx] = max(1e-12, parent_sigma * 0.92)\n                            stagnation_counter += 1\n                    else:\n                        pop_sigma[idx] = max(1e-12, parent_sigma * 0.92)\n                        stagnation_counter += 1\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if (self._rng.rand() < p_levy) and (self._evals < self.budget):\n                # Cauchy-like heavy-tailed vector\n                # Use standard Cauchy per-component then normalize to robust scale\n                cauchy = self._rng.standard_cauchy(size=self.dim)\n                # cap extremes to avoid inf/nan\n                cauchy = np.clip(cauchy, -1e6, 1e6)\n                # robust scale: median absolute deviation\n                mad = np.median(np.abs(cauchy - np.median(cauchy))) + 1e-9\n                if mad == 0:\n                    mad = np.mean(np.abs(cauchy)) + 1e-9\n                levy_dir = cauchy / (mad * np.sqrt(self.dim))\n                levy_norm = np.linalg.norm(levy_dir)\n                if levy_norm == 0:\n                    levy_dir = self._rng.normal(size=self.dim)\n                    levy_norm = np.linalg.norm(levy_dir)\n                levy_dir = levy_dir / levy_norm\n                levy_step_scale = base_sigma * (5.0 + 20.0 * self._rng.rand())  # heavy long step\n                f_levy, x_levy = callf(parent_x + levy_dir * levy_step_scale)\n                if f_levy < pop_f.max():\n                    # replace worst in population with this candidate if it's good\n                    worst = np.argmax(pop_f)\n                    pop_x[worst] = x_levy\n                    pop_f[worst] = f_levy\n                    pop_sigma[worst] = max(base_sigma * 0.5, np.std(x_levy - pop_x.mean(axis=0)))\n                # continue to next iteration\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            best_idx = int(np.argmin(pop_f))\n            # find second best distinct\n            other_idx_candidates = [i for i in range(pop_n) if i != best_idx]\n            if other_idx_candidates:\n                second_idx = min(other_idx_candidates, key=lambda i: pop_f[i])\n                child = 0.6 * pop_x[best_idx] + 0.4 * pop_x[second_idx]\n                child += self._rng.normal(scale=0.07 * base_sigma, size=self.dim)\n                f_child, x_child = callf(child)\n                if f_child < pop_f[idx]:\n                    pop_x[idx] = x_child\n                    pop_f[idx] = f_child\n                    pop_sigma[idx] = max(1e-12, (pop_sigma[best_idx] + pop_sigma[second_idx]) * 0.6)\n                else:\n                    # maybe replace the worst if it's better\n                    worst = np.argmax(pop_f)\n                    if f_child < pop_f[worst]:\n                        pop_x[worst] = x_child\n                        pop_f[worst] = f_child\n                        pop_sigma[worst] = base_sigma\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self._rng.rand() < p_rejuvenate and self._evals < self.budget:\n                worst = int(np.argmax(pop_f))\n                x_new = self._rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop_x[worst] = x_new\n                pop_f[worst] = f_new\n                pop_sigma[worst] = base_sigma\n\n            # small housekeeping: keep sigma within reasonable bounds\n            pop_sigma = np.clip(pop_sigma, base_sigma * 1e-6, base_sigma * 50)\n\n            # stop early if budget used\n            if self._evals >= self.budget:\n                break\n\n            # adapt global behavior based on stagnation\n            if self.f_opt < last_best - 1e-12:\n                last_best = self.f_opt\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n                if stagnation_counter > 40:\n                    # trigger a stronger diversification: randomize one individual's sigma\n                    i = self._rng.randint(pop_n)\n                    pop_sigma[i] = base_sigma * (1 + 5.0 * self._rng.rand())\n                    # small random restart of one individual\n                    if self._evals < self.budget:\n                        x_rnd = self._rng.uniform(lb, ub)\n                        f_rnd, x_rnd = callf(x_rnd)\n                        worst = int(np.argmax(pop_f))\n                        if f_rnd < pop_f[worst]:\n                            pop_x[worst] = x_rnd\n                            pop_f[worst] = f_rnd\n                    stagnation_counter = 0\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.485 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15014071436757082, 0.14846571871259473, 0.60549741637608, 0.9179038312468665, 0.6509839789506685, 0.7909905171829597, 0.29367340986482926, 0.41771465074571557, 0.6954886255221248, 0.18243549517646462]}, "task_prompt": ""}
{"id": "be4ec2a7-2bed-430e-8621-19ccadd259b3", "fitness": 0.487244642879777, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining randomized directional local searches, orthogonal refinements, adaptive step-size sigmas and occasional heavy-tailed Lévy-like jumps for robust global-local exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_size: number of candidate solutions in the population\n    - seed: RNG seed for reproducibility\n    - verbose: print simple progress info (False by default)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales mildly with dimensionality\n            self.pop_size = max(4, min(40, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.verbose = bool(verbose)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds as full-dim arrays\n        lb_raw = np.array(func.bounds.lb, dtype=float)\n        ub_raw = np.array(func.bounds.ub, dtype=float)\n        if lb_raw.size == 1:\n            lb = np.full(self.dim, float(lb_raw.item()))\n        else:\n            lb = lb_raw.astype(float).copy()\n            if lb.size != self.dim:\n                lb = np.resize(lb, self.dim)\n        if ub_raw.size == 1:\n            ub = np.full(self.dim, float(ub_raw.item()))\n        else:\n            ub = ub_raw.astype(float).copy()\n            if ub.size != self.dim:\n                ub = np.resize(ub, self.dim)\n\n        # ensure lb < ub elementwise\n        span = ub - lb\n        span[span <= 0] = 1.0  # fallback safe span if bad bounds\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # safe evaluation wrapper that tracks remaining budget and updates best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x.copy()\n\n        # If budget is zero or negative, return trivial\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population with random samples and individual sigmas\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-12, 0.25 * float(np.mean(span)))  # overall scale\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # per-individual sigma randomized around base_sigma\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If population is empty due to tiny budget, fallback to random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main optimization loop\n        # We'll iterate until budget exhausted; each iteration tries to improve one parent\n        itr = 0\n        while remaining > 0:\n            itr += 1\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(np.mean(span), sigma * 1.12)\n                if self.verbose:\n                    print(f\"itr{itr}: parent {parent_i} direct success f={f_try:.4e} sigma-> {pop_sigma[parent_i]:.3e}\")\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    if self.verbose:\n                        print(f\"itr{itr}: parent {parent_i} backtrack success f={f_try:.4e}\")\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d  # make orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                step_len = 0.6 * sigma * (0.7 + 0.6 * np.random.rand())\n                x_try = np.clip(x_parent + step_len * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.2, np.mean(span))\n                        if self.verbose:\n                            print(f\"itr{itr}: parent {parent_i} orthogonal success f={f_try:.4e}\")\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.18 * span  # directional scaling per-dimension\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                        if self.verbose:\n                            print(f\"itr{itr}: Lvy replaced worst {worst_i} f={f_try:.4e}\")\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.015 * span) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # replace better of parent or worst if improved\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        if self.verbose:\n                            print(f\"itr{itr}: recomb improved parent {parent_i} f={f_try:.4e}\")\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * (0.4 + 0.6 * np.random.rand())\n                            if self.verbose:\n                                print(f\"itr{itr}: recomb inserted at worst {worst_i} f={f_try:.4e}\")\n\n            # adapt parent sigma on failure (conservative shrink)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                if self.verbose:\n                    print(f\"itr{itr}: rejuvenated worst {worst_i} f={f_new:.4e}\")\n\n            # optional small local re-evaluation of best with reduced sigma to refine exploitation\n            if remaining > 0 and np.random.rand() < 0.03:\n                best_i = int(np.argmin(pop_f))\n                s_small = max(1e-12, 0.25 * pop_sigma[best_i])\n                x_try = np.clip(pop[best_i] + s_small * np.random.randn(self.dim), lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[best_i]:\n                    pop[best_i] = x_try\n                    pop_f[best_i] = f_try\n                    pop_sigma[best_i] = max(pop_sigma[best_i] * 0.95, 1e-12)\n                    if self.verbose:\n                        print(f\"itr{itr}: refine best {best_i} -> f={f_try:.4e}\")\n\n            # if population degenerates (all very similar), inject diversity\n            if remaining > 0 and np.std(pop_sigma) < 1e-9 and np.random.rand() < 0.2:\n                idx = np.random.randint(len(pop))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[idx] = x_new\n                pop_f[idx] = f_new\n                pop_sigma[idx] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # end while remaining\n\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.487 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11169136964625881, 0.14721796897790806, 0.47906185651266775, 0.9543006142578152, 0.8912228547149827, 0.7361860635232478, 0.2715493761897033, 0.47501840534818074, 0.6741069511073301, 0.13209096851967594]}, "task_prompt": ""}
{"id": "df7a7826-db08-40fd-9bae-831bd8ec7e92", "fitness": 0.40062751368066385, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimiser using adaptive per-individual step sizes, randomized directional local searches, orthogonal refinements and occasional heavy‑tailed Lévy jumps to both exploit local structure and escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n      - pop_size: size of population (defaults adaptive to dim)\n      - seed: RNG seed for reproducibility\n    Usage:\n      opt = ADLS(budget=10000, dim=10)\n      fbest, xbest = opt(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # derive bounds\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        # broadcast if scalar\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # safety\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to call func within budget, clip to bounds and update best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if (self.x_opt is None) or (f < self.f_opt):\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, do random sampling until budget exhausted\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        span = ub - lb\n        base_sigma = max(1e-8, 0.25 * np.mean(span))\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # per-individual sigma randomized around base\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # main loop\n        # we iterate until budget depleted; each internal step consumes 1+ evaluations via callf guarded by remaining\n        while remaining > 0:\n            # pick a parent via small tournament (promotes both exploration and exploitation)\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            vals = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(vals))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # degenerate direction, try a small random perturbation\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # stochasticized step-length from sigma\n            alpha = sigma * max(1e-12, (1.0 + 0.25 * np.random.randn()))\n\n            # Primary directional trial\n            x_try = x_parent + alpha * d\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n            else:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.15, np.mean(span))\n                continue  # proceed to next iteration\n\n            # Local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = x_parent + alpha * frac * d\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # Try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # remove projection onto d to make it orthogonal\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = x_parent + 0.6 * sigma * r\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, np.mean(span))\n                        continue\n\n            # Occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if (np.random.rand() < 0.08) and (remaining > 0):\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.18 * span  # scaled to problem range\n                x_try = x_parent + step * scale_vec\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, pop_sigma[parent_i] * 0.6)\n                # continue to next iteration after jump attempt\n                continue\n\n            # Recombination exploitation: mix two best individuals and try small noise around the mix\n            if len(pop) >= 2 and (np.random.rand() < 0.5) and (remaining > 0):\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * span) * np.random.randn(self.dim)\n                x_try = mix + noise\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # replace parent if improved, else possibly replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                        continue\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.6\n                            continue\n\n            # If we reach here, no improvement occurred this iteration: reduce parent's sigma slightly\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if (remaining > 0) and (np.random.rand() < 0.02):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.4 * np.random.rand())\n\n            # If population is smaller than intended (rare), try to add new individuals\n            if (remaining > 0) and (len(pop) < self.pop_size) and (np.random.rand() < 0.1):\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.7 + 0.6 * np.random.rand()))\n\n            # safeguard: if budget is very low, do a few random probes\n            if remaining <= max(1, int(0.03 * self.budget)) and remaining > 0:\n                # do purely random rest\n                while remaining > 0:\n                    x = np.random.uniform(lb, ub)\n                    callf(x)\n                break\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.401 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10914539097605958, 0.1500090116619326, 0.538850173100232, 0.9320324064635344, 0.6311334794253289, 0.21163709534964814, 0.2584632091853183, 0.44197366303572316, 0.5422772418122368, 0.19075346579662478]}, "task_prompt": ""}
{"id": "947e8f0b-3d67-4eb8-9630-aba8bfce2239", "fitness": 0.4361782046506292, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining randomized directional local searches, orthogonal refinements, adaptive step-sizes and occasional heavy-tailed Lévy-like jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population size that scales with dimensionality\n            self.pop_size = max(4, min(40, int(6 + 1.5 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds from func; Many Affine BBOB typically gives scalars or arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # sanity: ensure shapes match dim\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        # remaining budget (function evaluations)\n        remaining = int(self.budget)\n\n        # initialize best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            # do not call if budget exhausted\n            if remaining <= 0:\n                return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n            # ensure x is array and clipped to bounds\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update best\n            if f < self.f_opt or self.x_opt is None:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget extremely small, fallback to random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # base step-size scale relative to domain\n        domain_scale = np.maximum(ub - lb, 1e-12)\n        base_sigma = 0.2 * np.linalg.norm(domain_scale) / np.sqrt(self.dim)\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            if x0 is None:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # each individual has its own adaptive sigma (perturbation scale)\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # bookkeeping helpers\n        def worst_index():\n            return int(np.argmax(pop_f))\n        def best_index():\n            return int(np.argmin(pop_f))\n\n        # main loop\n        while remaining > 0:\n            # select parent via small tournament\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), size=k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-16\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (0.8 + 0.6 * np.random.rand())  # step length along direction\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.25, np.max(domain_scale))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, 0.125):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.85, 1e-16)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r -= np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                ortho_scale = sigma * (0.5 + 0.5 * np.random.rand())\n                x_try = np.clip(x_parent + ortho_scale * r, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-16)\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.12 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.median(np.abs(step)) + 1e-12\n                step = step / denom\n                # scale relative to domain and individual's sigma\n                scale_vec = 0.25 * domain_scale * (0.5 + np.random.rand() * 1.5)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                # if it's good, replace the worst in population\n                wi = worst_index()\n                if f_try < pop_f[wi]:\n                    pop[wi] = x_try\n                    pop_f[wi] = f_try\n                    pop_sigma[wi] = max(1e-16, sigma * 0.6)\n                # small chance to keep it as new parent (diversify)\n                if np.random.rand() < 0.05 and f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-16, sigma * 0.8)\n                # continue main loop\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = np.random.randn(self.dim) * (0.03 * domain_scale) * (0.5 + 0.5 * np.random.rand())\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-16, sigma * 1.05)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    wi = worst_index()\n                    if f_try < pop_f[wi]:\n                        pop[wi] = x_try\n                        pop_f[wi] = f_try\n                        pop_sigma[wi] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (slight contraction)\n            pop_sigma[parent_i] = max(sigma * 0.88, 1e-16)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.03:\n                wi = worst_index()\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if x_new is None:\n                    break\n                pop[wi] = x_new\n                pop_f[wi] = f_new\n                pop_sigma[wi] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.436 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11744921662344421, 0.14887402260392057, 0.43781393419711434, 0.9434144580366126, 0.5167406777019967, 0.7169979419058634, 0.3181892641910071, 0.3608429533937262, 0.6612222469064993, 0.14023733094610658]}, "task_prompt": ""}
{"id": "a63346eb-e237-49e2-81d4-eb068d659f46", "fitness": 0.45091530139082636, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a small adaptive population uses randomized directional local searches, orthogonal refinements and occasional Lévy jumps plus recombination and rejuvenation to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults adaptively to problem dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size if pop_size is not None else max(4, min(12, self.dim + 1))\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Retrieve bounds robustly. Many benchmarks provide func.bounds.lb/ub.\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0\n            ub = 5.0\n        # broadcast to full-dim arrays if scalars or single-element arrays\n        lb = np.full(self.dim, float(np.asarray(lb).ravel()[0])) if np.size(lb) == 1 else np.asarray(lb, dtype=float)\n        ub = np.full(self.dim, float(np.asarray(ub).ravel()[0])) if np.size(ub) == 1 else np.asarray(ub, dtype=float)\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim, \"Bounds must match self.dim\"\n\n        remaining = int(self.budget)\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds to guarantee valid input\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Set some algorithm constants / scales\n        span = ub - lb\n        avg_span = float(np.mean(span))\n        base_sigma = max(1e-12, 0.15 * avg_span)\n\n        # Initialize population (each member has a point, fitness, and sigma)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        n_init = min(self.pop_size, remaining)  # can't initialize more than remaining evals\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # individual sigma sampled around base_sigma (scalar)\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n        # If budget too small to form a population, fall back to random search until exhaustion\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Main loop: continue until budget exhausted\n        while remaining > 0:\n            # Choose a parent via small tournament\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            # pick the index with smallest fitness among the tournament\n            parent_i = inds[int(np.argmin([pop_f[i] for i in inds]))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            if remaining <= 0:\n                break\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(span) * 2.0)\n                success = True\n            else:\n                success = False\n\n            # local backtracking / small-step refinement along direction (few tries)\n            if remaining > 0:\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    if remaining <= 0:\n                        break\n                    xt = np.clip(x_parent + alpha * frac * d, lb, ub)\n                    ft, xt = callf(xt)\n                    if ft < pop_f[parent_i]:\n                        pop[parent_i] = xt.copy()\n                        pop_f[parent_i] = ft\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        success = True\n                        # shrink alpha to refine further in next iterations\n                        alpha *= 0.7\n\n            # If improved, continue to next main iteration (allow intensification)\n            if success:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    # fallback: random orthonormal vector via Gram-Schmidt attempt\n                    r = np.random.randn(self.dim)\n                    r = r - np.dot(r, d) * d\n                    nr = np.linalg.norm(r) + 1e-12\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(span) * 2.0)\n                    continue  # local improvement found\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # sample heavy-tailed step (Cauchy-like)\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization to avoid extreme absolute scale\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * span\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * span) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # if improved relative to parent, replace parent; may also replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n\n            # adapt parent sigma on failure (if no success above)\n            if not success:\n                pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.02 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget or no remaining evaluations\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.451 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09555514790601372, 0.14109767853807154, 0.7452332701502472, 0.9464963454133557, 0.23212884046258997, 0.7636960127744112, 0.24310006576844656, 0.42458734981423574, 0.7650256100292822, 0.15223269305160958]}, "task_prompt": ""}
{"id": "f670f6cf-6db4-4815-b420-782c8bd236db", "fitness": 0.40349399501537475, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — maintains a small population with adaptive step-sizes, performs randomized directional local searches with backtracking and orthogonal refinements, and uses occasional Lévy-like heavy-tailed jumps and recombination to escape basins and inject diversity.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest value scaled by dim)\n    - seed: optional RNG seed for reproducibility\n    - base_sigma: baseline step scale (relative to search range)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None, base_sigma=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        if seed is not None:\n            np.random.seed(int(seed))\n        # base sigma relative to bound range (bounds default [-5,5] -> range 10)\n        if base_sigma is None:\n            self.base_sigma = 0.1 * 10.0  # default 10% of full range\n        else:\n            self.base_sigma = float(base_sigma)\n        # placeholders filled in __call__\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # ensure bounds are full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # internal state\n        self.f_opt = np.inf\n        self.x_opt = None\n        remaining = int(self.budget)\n\n        # safe evaluator wrapper to track budget and best\n        def callf(x):\n            nonlocal remaining\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            if remaining <= 0:\n                raise RuntimeError(\"Budget exceeded\")\n            remaining -= 1\n            # clip before evaluating for safety\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            # update best\n            if f < self.f_opt or self.x_opt is None:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If no budget, return immediately\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population (as many as budget allows, up to pop_size)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(float(f0))\n            # initialize adaptive sigma around base_sigma with small randomness\n            pop_sigma.append(self.base_sigma * (0.6 + 0.8 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # if no population could be created (very small budget), do pure random search for remainder\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return self.f_opt, self.x_opt\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # helper to replace worst member\n        def replace_worst(x_new, f_new, sigma_new=None):\n            worst_i = int(np.argmax(pop_f))\n            pop[worst_i] = x_new.copy()\n            pop_f[worst_i] = float(f_new)\n            if sigma_new is None:\n                pop_sigma[worst_i] = self.base_sigma * (0.5 + 0.5 * np.random.rand())\n            else:\n                pop_sigma[worst_i] = float(sigma_new)\n\n        # main optimization loop\n        while remaining > 0:\n            # small tournament selection for parent (k=3)\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), size=k, replace=False)\n            parent_i = inds[int(np.argmin(pop_f[inds]))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = float(f_try)\n                pop_sigma[parent_i] = min(sigma * 1.2, np.max(ub - lb))\n                continue  # go to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for factor in (0.5, 0.25, 0.1):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * factor * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = float(f_try)\n                    pop_sigma[parent_i] = max(sigma * (1.0 + 0.1 * np.random.rand()), 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            proj = np.dot(r, d) * d\n            r = r - proj\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                ortho_step = sigma * 0.6 * (0.5 + np.random.rand())\n                x_try = np.clip(x_parent + ortho_step * r, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = float(f_try)\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.max(ub - lb))\n                    continue\n                else:\n                    # minor successful injection: maybe replace worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = float(f_try)\n                        pop_sigma[worst_i] = self.base_sigma * 0.5\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.15 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(size=self.dim)\n                # robust normalization to avoid catastrophic extremes\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale by adaptive magnitude derived from parent sigma and global range\n                scale_vec = max(sigma, 0.5 * self.base_sigma) * (1.0 + np.random.rand() * 3.0)\n                x_try = np.clip(x_parent + step * (scale_vec / (1.0 + np.sqrt(self.dim))), lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try.copy()\n                    pop_f[worst_i] = float(f_try)\n                    pop_sigma[worst_i] = self.base_sigma * (0.2 + np.random.rand() * 0.8)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and np.random.rand() < 0.25:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = pop[best2[0]].copy(), pop[best2[1]].copy()\n                beta = np.random.rand()\n                mix = a * (1.0 - beta) + b * beta\n                noise = np.random.randn(self.dim) * (0.2 * sigma * (0.5 + np.random.rand()))\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = float(f_try)\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = float(f_try)\n                        pop_sigma[worst_i] = self.base_sigma * 0.5\n                continue\n\n            # adapt parent sigma on failure\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.03:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                replace_worst(x_new, f_new, sigma_new=self.base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n            # safety: if population collapsed or identical, inject diversity\n            if remaining > 0:\n                f_spread = np.std(pop_f) if pop_f.size > 1 else 0.0\n                if f_spread < 1e-9 and np.random.rand() < 0.2:\n                    x_new = np.random.uniform(lb, ub)\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        break\n                    replace_worst(x_new, f_new)\n\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.403 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11002407289886662, 0.14711802977657584, 0.8835960130279267, 0.968851071913073, 0.2503717000325161, 0.9465851662148627, 0.25470084977503193, 0.1607851668448287, 0.18853228936939037, 0.12437559030067602]}, "task_prompt": ""}
{"id": "29404d7f-a156-43be-b672-ed553df1adc4", "fitness": 0.44513654610847037, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search that alternates normalized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy-like jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: number of population members (optional)\n    - seed: RNG seed for reproducibility (optional)\n    - base_sigma: initial sigma as fraction of average range (optional)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None, base_sigma=0.10):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        self.seed = seed\n        self.base_sigma_frac = float(base_sigma)\n        # use a local RNG object\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        # read bounds if available, else assume [-5,5] per spec\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # Ensure full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        self.dim = lb.size  # override dim if necessary\n        remaining = int(self.budget)\n\n        # keep best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            x = np.asarray(x, dtype=float)\n            x = np.clip(x, lb, ub)\n            if remaining <= 0:\n                # budget exhausted: return infinite cost\n                return np.inf, x\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, fallback to random sampling\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # population size default relative to dimension but modest\n        if self.pop_size is None:\n            pop_size = int(min(max(6, 2 * self.dim), 40))\n        else:\n            pop_size = int(self.pop_size)\n\n        # absolute base sigma scale (mean range * fraction)\n        mean_range = float(np.mean(ub - lb))\n        base_sigma = max(1e-12, self.base_sigma_frac * mean_range)\n\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        # initialize population with a few random points (bounded by budget)\n        while len(pop) < pop_size and remaining > 0:\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(float(f0))\n            # give each member a slightly different initial sigma\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.rand()))\n\n        # If no population created (very tiny budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # main loop: continue until budget exhausted\n        # loop iterations will adaptively consume remaining evaluations\n        while remaining > 0:\n            # small tournament to pick parent (balance exploration/exploitation)\n            k = min(3, len(pop))\n            inds = self.rng.randint(0, len(pop), size=k)\n            # choose the best among sampled inds\n            best_local = inds[int(np.argmin([pop_f[i] for i in inds]))]\n            parent_i = best_local\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # fallback to small random direction\n                d = self.rng.randn(self.dim)\n                nd = max(1e-12, np.linalg.norm(d))\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # accept and slightly increase sigma (successful exploitation)\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, mean_range)\n                improved = True\n            else:\n                # local backtracking / smaller step refinement along same direction\n                for factor in (0.5, 0.25, 0.1):\n                    if remaining <= 0:\n                        break\n                    alpha2 = alpha * factor\n                    x_try2 = np.clip(x_parent + alpha2 * d, lb, ub)\n                    f_try2, x_try2 = callf(x_try2)\n                    if f_try2 < pop_f[parent_i]:\n                        pop[parent_i] = x_try2.copy()\n                        pop_f[parent_i] = f_try2\n                        pop_sigma[parent_i] = min(sigma * 1.15, mean_range)\n                        improved = True\n                        break\n                if not improved:\n                    # failure: reduce sigma for parent\n                    pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            if remaining <= 0:\n                break\n\n            # try an orthogonal perturbation for local diversification (few tries)\n            # create a vector orthogonal to d\n            r = self.rng.randn(self.dim)\n            # remove projection on d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr < 1e-12:\n                r = self.rng.randn(self.dim)\n                nr = max(1e-12, np.linalg.norm(r))\n            r = r / nr\n            x_try = np.clip(pop[parent_i] + 0.6 * pop_sigma[parent_i] * r, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, mean_range)\n                continue  # successful orthogonal improvement, continue main loop\n\n            if remaining <= 0:\n                break\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = self.rng.standard_cauchy(size=self.dim)\n                # robust scale to avoid absolute extremes but keep heavy-tails\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale relative to sigma and problem range\n                scale = pop_sigma[parent_i] * (0.8 + 1.2 * self.rng.rand())\n                step_vec = step * scale\n                # clip vector magnitude to avoid numerical blowups\n                max_norm = 10.0 * mean_range\n                norm_step = np.linalg.norm(step_vec)\n                if norm_step > max_norm:\n                    step_vec = step_vec / norm_step * max_norm\n                x_try = np.clip(x_parent + step_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    # if jump improved, replace parent and boost sigma\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.4, mean_range)\n                else:\n                    # maybe replace worst in population if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * self.rng.rand())\n\n            if remaining <= 0:\n                break\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and self.rng.rand() < 0.20:\n                # pick two best\n                idxs = np.argsort(pop_f)[:2]\n                a, b = idxs[0], idxs[1]\n                beta = self.rng.rand()\n                noise = (0.01 * (ub - lb)) * self.rng.randn(self.dim)\n                x_try = np.clip(beta * pop[a] + (1.0 - beta) * pop[b] + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n                else:\n                    # inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                # if budget allowed and evaluated, replace\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n                else:\n                    # still replace sometimes to increase diversity if not too bad\n                    if self.rng.rand() < 0.5:\n                        pop[worst_i] = x_new.copy()\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n            # ensure population arrays are consistent sizes (in case some replaced)\n            # maintain simple exploitation: if any member extremely bad, nudge it\n            if remaining > 0 and self.rng.rand() < 0.05:\n                worst_i = int(np.argmax(pop_f))\n                # nudge the worst toward the best\n                best_i = int(np.argmin(pop_f))\n                direction = pop[best_i] - pop[worst_i]\n                pop[worst_i] = np.clip(pop[worst_i] + 0.3 * direction + 0.05 * mean_range * self.rng.randn(self.dim), lb, ub)\n                f_new, pop[worst_i] = callf(pop[worst_i])\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * self.rng.rand())\n\n            # tiny housekeeping: if some sigma collapsed to zero, reset a bit\n            for i in range(len(pop_sigma)):\n                if pop_sigma[i] < 1e-12:\n                    pop_sigma[i] = base_sigma * (0.5 + self.rng.rand())\n\n            # continue until remaining exhausted\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.445 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12824711127191524, 0.16386967911609518, 0.7627447269510012, 0.951006338002648, 0.30381305790821045, 0.9124642707792538, 0.25141292340367927, 0.5713839890431626, 0.2595613842561867, 0.1468619803525517]}, "task_prompt": ""}
{"id": "b5e51cd2-977d-46b6-9199-920fb25adeb2", "fitness": 0.18285347910844976, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional searches with orthogonal refinements and occasional heavy-tailed Lévy jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    Main idea:\n      Maintain a small population of candidates with adaptive step-sizes (sigmas).\n      For each iteration pick a parent by a small tournament, perform a randomized\n      directional local search (with backtracking), try orthogonal refinements,\n      occasionally perform Lévy-like heavy-tailed jumps to escape basins, and\n      replace worst/populate by successful trials. Sigmas adapt up/down based on\n      success to balance exploration/exploitation.\n\n    Usage:\n      optimizer = ADLS(budget=10000, dim=10, pop_size=None, seed=123)\n      f_best, x_best = optimizer(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population size scales with dim, but bounded by budget and kept modest\n        if pop_size is None:\n            self.pop_size = max(4, min(6 + self.dim, self.budget // 10, 20))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # Determine bounds: prefer func.bounds if available, else default [-5,5]\n        if hasattr(func, \"bounds\") and getattr(func.bounds, \"lb\", None) is not None:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb.item()))\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub.item()))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # Ensure shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        span[span == 0.0] = 1e-12\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            if remaining <= 0:\n                # budget exhausted; return best so far without calling\n                return self.f_opt, self.x_opt\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population uniformly and evaluate\n        pop = []\n        pop_f = np.full(self.pop_size, np.inf, dtype=float)\n        pop_sigma = np.full(self.pop_size, 0.15 * np.linalg.norm(span), dtype=float)  # initial step-scale\n        for i in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f[i] = f0\n        # If population incomplete due to tiny budget, shrink arrays\n        pop = np.array(pop)\n        cur_pop_size = pop.shape[0]\n        if cur_pop_size == 0:\n            # no evaluations possible\n            return self.f_opt, self.x_opt\n        if cur_pop_size < self.pop_size:\n            pop_sigma = pop_sigma[:cur_pop_size]\n            pop_f = pop_f[:cur_pop_size]\n            self.pop_size = cur_pop_size\n\n        # Main loop\n        # hyperparameters\n        tournament_k = min(3, self.pop_size)\n        backtrack_tries = 3\n        orth_prob = 0.4\n        levy_prob = 0.08\n        rejuvenation_prob = 0.02\n        recomb_prob = 0.06\n\n        while remaining > 0:\n            # pick parent by small tournament (favor better)\n            if self.pop_size == 1:\n                parent_i = 0\n            else:\n                cand = np.random.randint(0, self.pop_size, size=tournament_k)\n                parent_i = cand[np.argmin(pop_f[cand])]\n\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n            # sample direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.ones(self.dim)\n                nd = np.linalg.norm(d)\n            d = d / nd\n\n            # stochasticize step length: base alpha scaled by sigma and random multiplier\n            alpha = sigma * (0.9 + 0.4 * np.random.rand())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n\n            # Primary directional trial\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2 + 1e-12, np.linalg.norm(span))  # increase sigma gently\n                continue\n            else:\n                # Backtracking: try smaller steps along the same direction\n                improved = False\n                alpha_bt = alpha\n                for bt in range(backtrack_tries):\n                    alpha_bt *= 0.5\n                    if alpha_bt < 1e-16:\n                        break\n                    x_bt = np.clip(x_parent + alpha_bt * d, lb, ub)\n                    if remaining <= 0:\n                        break\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt < pop_f[parent_i]:\n                        pop[parent_i] = x_bt\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = max(1e-12, sigma * (0.9 + 0.05 * np.random.rand()))\n                        improved = True\n                        break\n                if improved:\n                    continue\n                # No improvement along direction: shrink sigma moderately\n                pop_sigma[parent_i] = max(1e-12, sigma * 0.7)\n\n            # Orthogonal refinement (attempt small orthogonal move to diversify local search)\n            if np.random.rand() < orth_prob and remaining > 0:\n                # create an orthogonal direction to d\n                rnd = np.random.randn(self.dim)\n                proj = d * np.dot(rnd, d)\n                ort = rnd - proj\n                norm_ort = np.linalg.norm(ort)\n                if norm_ort < 1e-12:\n                    ort = np.random.randn(self.dim)\n                    norm_ort = np.linalg.norm(ort)\n                ort = ort / norm_ort\n                orth_scale = 0.6 * sigma\n                x_ort = np.clip(x_parent + orth_scale * ort, lb, ub)\n                f_ort, x_ort = callf(x_ort)\n                if f_ort < pop_f[parent_i]:\n                    pop[parent_i] = x_ort\n                    pop_f[parent_i] = f_ort\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.95)\n                    continue\n\n            # Occasional Lévy-like jump to escape local minima\n            if np.random.rand() < levy_prob and remaining > 0:\n                # heavy-tailed Cauchy components, scaled to robust percentile to avoid extremes\n                step = np.random.standard_cauchy(size=self.dim)\n                denom = (np.percentile(np.abs(step), 90) + 1e-12)\n                step = step / denom\n                # scale by a fraction of the domain and current sigma\n                scale_vec = 0.25 * span * (0.5 + np.random.rand(self.dim))\n                step_vec = step * scale_vec * (0.5 + 2.0 * np.random.rand())\n                x_jump = np.clip(x_parent + step_vec * (sigma / (np.linalg.norm(span) + 1e-12)), lb, ub)\n                f_jump, x_jump = callf(x_jump)\n                # if jump is good, insert into population replacing the worst\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                else:\n                    # else maybe keep jump as an extra candidate for recombination (if budget allows)\n                    pass\n\n            # Recombination exploitation: mix two best and small noise\n            if np.random.rand() < recomb_prob and remaining > 0 and self.pop_size >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                x_mix = 0.5 * (pop[best2[0]] + pop[best2[1]])\n                # small adaptive noise\n                mix_noise = np.random.randn(self.dim) * (0.2 * np.mean(pop_sigma) + 1e-12)\n                x_mix = np.clip(x_mix + mix_noise, lb, ub)\n                f_mix, x_mix = callf(x_mix)\n                worst_i = int(np.argmax(pop_f))\n                if f_mix < pop_f[worst_i]:\n                    pop[worst_i] = x_mix\n                    pop_f[worst_i] = f_mix\n                    pop_sigma[worst_i] = max(1e-12, 0.8 * np.mean(pop_sigma))\n\n            # If none of the above improved the parent, try small local random perturbation\n            if remaining > 0:\n                small_pert = np.random.randn(self.dim) * (0.2 * pop_sigma[parent_i])\n                x_pert = np.clip(x_parent + small_pert, lb, ub)\n                f_pert, x_pert = callf(x_pert)\n                if f_pert < pop_f[parent_i]:\n                    pop[parent_i] = x_pert\n                    pop_f[parent_i] = f_pert\n                    pop_sigma[parent_i] = pop_sigma[parent_i] * 1.05\n                else:\n                    # slightly reduce sigma to focus search\n                    pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.95)\n\n            # Occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < rejuvenation_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = max(1e-12, 0.12 * np.linalg.norm(span))\n\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.183 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09473581302143386, 0.16401224336446496, 0.3153826688246759, 0.1765628764724474, 0.14969566242370658, 0.20186689521567325, 0.20363702163431374, 0.2069019607404663, 0.20446827604698858, 0.11127137334032677]}, "task_prompt": ""}
{"id": "043d2191-96d1-437a-80ca-f15ede066c42", "fitness": 0.3236537508050322, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional search with orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps for robust exploration/exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (by default scales with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows slowly with dimension\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # will be set after running\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Helper: determine bounds if available, otherwise default [-5, 5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # Ensure bounds are full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        # internal budget tracker\n        remaining = int(self.budget)\n\n        # evaluation helper that clips input, enforces budget and updates global best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"Budget exhausted, attempted extra function evaluation.\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        if remaining < max(10, self.pop_size):\n            # pure random search with remaining budget\n            for _ in range(remaining):\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            remaining = 0\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        for i in range(self.pop_size):\n            x = self.rng.uniform(lb, ub)\n            f, x = callf(x)\n            pop.append(x)\n            pop_f.append(f)\n            # initial sigma: proportional to problem size (1/4 of range) scaled by noise\n            range_scale = np.mean(ub - lb)\n            pop_sigma.append(max(range_scale * 0.25 * (0.5 + self.rng.rand()), 1e-12))\n\n            if remaining <= 0:\n                break\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # If no population could be created (very small budget), do pure random search\n        if pop.shape[0] == 0:\n            return self.f_opt, self.x_opt\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        it = 0\n        stagnation = 0\n        best_history = [self.f_opt]\n        while remaining > 0:\n            it += 1\n\n            # pick a parent via small tournament to balance exploration/exploitation\n            tour = self.rng.randint(0, pop.shape[0], size=min(3, pop.shape[0]))\n            parent_i = tour[np.argmin(pop_f[tour])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = x_parent + alpha * d\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15 + 1e-16, np.mean(ub - lb))\n                stagnation = 0\n            else:\n                # failure: try small backtracking / refinements along direction\n                improved = False\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    if remaining <= 0:\n                        break\n                    x_try = x_parent + frac * alpha * d\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        improved = True\n                        break\n                if improved:\n                    stagnation = 0\n                else:\n                    # try an orthogonal perturbation for local diversification\n                    if remaining <= 0:\n                        break\n                    r = self.rng.randn(self.dim)\n                    r = r - np.dot(r, d) * d  # make orthogonal to d\n                    rn = np.linalg.norm(r) + 1e-12\n                    r = r / rn\n                    # orthogonal step scale proportional to sigma\n                    orth_scale = sigma * (0.5 + 0.5 * self.rng.rand())\n                    x_try = x_parent + orth_scale * r\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        stagnation = 0\n                    else:\n                        # occasional Lévy-like jump to escape local basins (heavy-tailed)\n                        if self.rng.rand() < 0.12:  # jump with small probability\n                            if remaining <= 0:\n                                break\n                            # Cauchy-like heavy-tailed vector\n                            cauchy_vec = np.tan(np.pi * (self.rng.rand(self.dim) - 0.5))\n                            # robust scale to avoid extreme blow-up\n                            scale = np.median(np.abs(cauchy_vec)) + 1e-12\n                            levy_step = (cauchy_vec / (scale)) * sigma * (1.0 + self.rng.rand())\n                            # normalize heavy-tail to be within a reasonable portion of range\n                            max_allowed = np.mean(ub - lb) * 0.7\n                            norm_factor = max(1.0, np.linalg.norm(levy_step) / max_allowed)\n                            levy_step = levy_step / norm_factor\n                            x_try = x_parent + levy_step\n                            f_try, x_try = callf(x_try)\n                            if f_try < pop_f[parent_i]:\n                                # if good, replace the parent\n                                pop[parent_i] = x_try\n                                pop_f[parent_i] = f_try\n                                pop_sigma[parent_i] = max(sigma * 1.05, 1e-12)\n                                stagnation = 0\n                            else:\n                                # maybe replace the worst with it if it's competitive\n                                worst_i = np.argmax(pop_f)\n                                if f_try < pop_f[worst_i]:\n                                    pop[worst_i] = x_try\n                                    pop_f[worst_i] = f_try\n                                    pop_sigma[worst_i] = max(sigma * 0.5, 1e-12)\n                                stagnation += 1\n                        else:\n                            # no jump, adjust sigma down due to failure\n                            pop_sigma[parent_i] = max(sigma * 0.88, 1e-12)\n                            stagnation += 1\n\n            # recombination exploitation: mix two best and small noise occasionally\n            if remaining > 0 and self.rng.rand() < 0.18:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1] if best2.size > 1 else (best2[0], best2[0])\n                beta = self.rng.rand()\n                x_new = beta * pop[a] + (1 - beta) * pop[b] + 0.1 * pop_sigma[a] * self.rng.randn(self.dim)\n                # evaluate and inject into population if good\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[parent_i]:\n                    pop[parent_i] = x_new\n                    pop_f[parent_i] = f_new\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.98, 1e-12)\n                    stagnation = 0\n                else:\n                    # maybe replace the worst\n                    worst_i = np.argmax(pop_f)\n                    if f_new < pop_f[worst_i]:\n                        pop[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = pop_sigma[parent_i] * 0.7\n\n            # adapt parent sigma on failure to encourage exploration over time\n            # small random drift of sigmas\n            pop_sigma *= (1.0 + 0.01 * (self.rng.randn(pop_sigma.size)))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (stagnation > 12 or self.rng.rand() < 0.03):\n                worst_i = np.argmax(pop_f)\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = np.mean(ub - lb) * (0.1 + 0.4 * self.rng.rand())\n                stagnation = 0\n\n            # if budget is nearing end, spend remaining on local intensification around best\n            if remaining <= max(5, int(0.02 * self.budget)):\n                # intensify around current best\n                best_i = np.argmin(pop_f)\n                for _ in range(remaining):\n                    perturb = 0.1 * pop_sigma[best_i] * self.rng.randn(self.dim)\n                    x_try = pop[best_i] + perturb\n                    callf(x_try)\n                break\n\n            # track progress\n            best_history.append(self.f_opt)\n            # if no improvement for long time, increase chance of Lévy jumps\n            if len(best_history) > 30 and best_history[-1] >= best_history[-15]:\n                # increase diversity by nudging some sigmas up\n                pop_sigma *= (1.0 + 0.05 * self.rng.rand(pop_sigma.size))\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.324 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14974958198352029, 0.17035183221471029, 0.5197297039317819, 0.16723045421120242, 0.2555762986019787, 0.9665306171904399, 0.23632024634901638, 0.369480627350544, 0.2251394100386146, 0.1764287361785135]}, "task_prompt": ""}
{"id": "11643b58-41eb-48ae-a19b-66f11c0c2e42", "fitness": 0.5711485023968966, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a compact population-based continuous optimizer using adaptive per-individual step-sizes, randomized directional local searches, orthogonal refinements and occasional heavy-tailed (Lévy/Cauchy-like) jumps to balance exploitation and global escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by dim if None)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size if pop_size is not None else min(20, max(6, 2 * self.dim))\n        self.rng = np.random.RandomState(seed)\n        # result placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds, allow scalar bounds\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.ravel()[:self.dim]\n        ub = ub.ravel()[:self.dim]\n\n        # Ensure sensible bounds (problem statement uses [-5,5] but keep general)\n        span = ub - lb\n        mean_span = float(np.mean(span))\n\n        # internal counters\n        used = 0\n        budget = self.budget\n\n        # safe call wrapper: enforce budget and clipping, track best\n        def callf(x):\n            nonlocal used, budget, lb, ub\n            if used >= budget:\n                return None  # no budget left\n            x = np.asarray(x, dtype=float).ravel()[:self.dim]\n            x = np.clip(x, lb, ub)\n            f = func(x.copy())\n            used += 1\n            # update global best\n            if f is not None and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        remaining = lambda: budget - used\n\n        # Very small budgets: fallback to random search\n        if budget <= 10:\n            for _ in range(budget):\n                x = lb + self.rng.rand(self.dim) * span\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining())\n        for i in range(n_init):\n            if remaining() <= 0:\n                break\n            x = lb + self.rng.rand(self.dim) * span\n            f = callf(x)\n            if f is None:\n                break\n            pop.append(x)\n            pop_f.append(f)\n            # initialize sigmas as a fraction of mean span with some randomness\n            pop_sigma.append(max(1e-6, mean_span * (0.1 + 0.2 * self.rng.rand())))\n        pop = np.array(pop) if pop else np.zeros((0, self.dim))\n        pop_f = np.array(pop_f) if len(pop_f) else np.array([])\n        pop_sigma = np.array(pop_sigma) if len(pop_sigma) else np.array([])\n\n        # If we couldn't populate, do random search with remaining budget\n        if pop.shape[0] == 0:\n            for _ in range(remaining()):\n                x = lb + self.rng.rand(self.dim) * span\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # helper to get indices\n        def best_indices():\n            idx = np.argsort(pop_f)\n            return idx\n\n        # Main loop\n        while remaining() > 0:\n            # recompute best/worst\n            idx_sorted = best_indices()\n            best_i = idx_sorted[0]\n            worst_i = idx_sorted[-1]\n            # small tournament selection for parent\n            tour_k = min(3, pop.shape[0])\n            contestants = self.rng.choice(pop.shape[0], size=tour_k, replace=False)\n            parent_i = contestants[np.argmin(pop_f[contestants])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction\n            d = self.rng.randn(self.dim)\n            d_norm = np.linalg.norm(d) + 1e-12\n            d = d / d_norm\n\n            # primary directional trial with stochasticized step-length\n            # multiplicative log-normal jitter on sigma\n            step_len = sigma * np.exp(0.3 * self.rng.randn())\n            x_try = x_parent + step_len * d\n            x_try = np.clip(x_try, lb, ub)\n            f_try = callf(x_try)\n            if f_try is None:\n                break\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max(pop_sigma[parent_i] * 1.08, 1e-8), mean_span)\n                continue  # proceed to next generation\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining() <= 0:\n                    break\n                x_try = x_parent + (step_len * frac) * d\n                x_try = np.clip(x_try, lb, ub)\n                f_try = callf(x_try)\n                if f_try is None:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.06, mean_span)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            # make orthogonal direction to d\n            v = self.rng.randn(self.dim)\n            v = v - np.dot(v, d) * d\n            v_norm = np.linalg.norm(v) + 1e-12\n            v = v / v_norm\n            # try a small orthogonal move\n            orth_step = sigma * 0.6 * (0.5 + 0.5 * self.rng.rand())\n            x_try = x_parent + orth_step * v\n            x_try = np.clip(x_try, lb, ub)\n            f_try = callf(x_try)\n            if f_try is None:\n                break\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.07, mean_span)\n                continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining() > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                # sample Cauchy scalar per dimension via tan(pi*(u-0.5))\n                u = self.rng.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                # scale by robust scale from population sigmas and spans\n                robust_scale = max(np.median(pop_sigma), 1e-6)\n                jump = (0.5 * mean_span) * cauchy / (np.median(np.abs(cauchy)) + 1e-12)\n                jump = np.clip(jump, -5 * mean_span, 5 * mean_span)\n                x_jump = x_parent + jump * (robust_scale / (mean_span + 1e-12))\n                x_jump = np.clip(x_jump, lb, ub)\n                f_jump = callf(x_jump)\n                if f_jump is None:\n                    break\n                if f_jump < pop_f[worst_i]:\n                    # insert into population replacing worst\n                    pop[worst_i] = x_jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = min(max(robust_scale * 1.2, 1e-8), mean_span)\n                else:\n                    # maybe keep as candidate (replace parent if better than parent)\n                    if f_jump < pop_f[parent_i]:\n                        pop[parent_i] = x_jump\n                        pop_f[parent_i] = f_jump\n                        pop_sigma[parent_i] = min(robust_scale * 1.1, mean_span)\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if pop.shape[0] >= 2 and remaining() > 0:\n                a, b = idx_sorted[0], idx_sorted[1]\n                alpha = 0.6 + 0.2 * self.rng.rand()\n                child = alpha * pop[a] + (1 - alpha) * pop[b]\n                # small scaled noise proportional to mean span and small sigma\n                noise = (0.01 * span) * self.rng.randn(self.dim)\n                child = np.clip(child + noise, lb, ub)\n                f_child = callf(child)\n                if f_child is None:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_child < pop_f[parent_i]:\n                    pop[parent_i] = child\n                    pop_f[parent_i] = f_child\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.05, 1e-8)\n                    continue\n                elif f_child < pop_f[worst_i]:\n                    pop[worst_i] = child\n                    pop_f[worst_i] = f_child\n                    pop_sigma[worst_i] = max(np.median(pop_sigma) * 0.9, 1e-8)\n                    continue\n\n            # adapt parent sigma on failure (reduce to tighten local search)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-8)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < 0.03 and remaining() > 0:\n                x_new = lb + self.rng.rand(self.dim) * span\n                f_new = callf(x_new)\n                if f_new is None:\n                    break\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = max(mean_span * 0.1 * (0.5 + self.rng.rand()), 1e-8)\n                # continue main loop\n\n            # ensure we do not exceed pop_size if budget allowed new individuals occasionally\n            if self.rng.rand() < 0.02 and remaining() > 0 and pop.shape[0] < self.pop_size:\n                x_new = lb + self.rng.rand(self.dim) * span\n                f_new = callf(x_new)\n                if f_new is None:\n                    break\n                pop = np.vstack([pop, x_new])\n                pop_f = np.append(pop_f, f_new)\n                pop_sigma = np.append(pop_sigma, max(mean_span * 0.1 * (0.5 + self.rng.rand()), 1e-8))\n\n            # safety: if population size grew beyond target, prune worst\n            if pop.shape[0] > self.pop_size:\n                idx_sort = np.argsort(pop_f)\n                keep = idx_sort[:self.pop_size]\n                pop = pop[keep]\n                pop_f = pop_f[keep]\n                pop_sigma = pop_sigma[keep]\n\n        # finished budget or no more remaining\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.571 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15285261358795788, 0.16115006265227172, 0.8051438055321012, 0.9320288469286286, 0.867931638680762, 0.8896340088804708, 0.3106377253227979, 0.5821193708561669, 0.860065962421966, 0.14992098910584328]}, "task_prompt": ""}
{"id": "b873e808-6dd3-479e-9db3-677739279e03", "fitness": 0.5084682073710751, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based continuous optimizer using adaptive per-individual step-sizes, randomized directional local searches, orthogonal refinements and occasional Lévy/Cauchy jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a modest scale of dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # choose a modest population that scales with dim but stays small\n        if pop_size is None:\n            self.pop_size = max(4, min(16 + dim // 2, 4 * dim))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        # Prepare bounds as arrays of right shape\n        lb = np.asarray(func.bounds.lb)\n        ub = np.asarray(func.bounds.ub)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n        # range used for scaling steps\n        space_range = ub - lb\n        range_scale = np.maximum(1e-12, space_range)\n\n        # internal budget and best trackers\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        def clip_x(x):\n            return np.minimum(ub, np.maximum(lb, x))\n\n        # helper to call func while tracking budget and best\n        def callf(x):\n            nonlocal evals, f_opt, x_opt\n            if evals >= self.budget:\n                # budget exhausted\n                return np.inf\n            x = np.asarray(x, dtype=float)\n            if x.shape != (self.dim,):\n                x = x.reshape(self.dim)\n            x = clip_x(x)\n            f = func(x)\n            evals += 1\n            if np.isfinite(f) and f < f_opt:\n                f_opt = float(f)\n                x_opt = x.copy()\n            return float(f)\n\n        # If budget is extremely small, do random search\n        if self.budget <= 2 or self.pop_size < 1:\n            # pure random search until budget exhausted\n            while evals < self.budget:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return f_opt, x_opt\n\n        # Initialize population: sample up to pop_size points (stop early if budget small)\n        pop_size = min(self.pop_size, max(1, self.budget // 3))\n        pop_x = np.empty((pop_size, self.dim))\n        pop_f = np.empty(pop_size)\n        for i in range(pop_size):\n            if evals >= self.budget:\n                # not enough budget for full population: fallback to best-so-far result\n                pop_x = pop_x[:i]\n                pop_f = pop_f[:i]\n                pop_size = i\n                break\n            x = self.rng.uniform(lb, ub)\n            pop_x[i, :] = x\n            pop_f[i] = callf(x)\n\n        if pop_size == 0:\n            # no budget to evaluate anything\n            return f_opt, x_opt\n\n        # per-individual adaptive sigma (fraction of domain)\n        base_sigma = 0.1  # initial fraction of domain range\n        sigmas = np.full(pop_size, base_sigma)\n        # small failure counters per individual - to reduce sigma on repeated failures\n        fails = np.zeros(pop_size, dtype=int)\n\n        # convenience: find index of best/worst\n        def idx_best():\n            return int(np.nanargmin(pop_f))\n\n        def idx_worst():\n            return int(np.nanargmax(pop_f))\n\n        # main loop: iterate until budget exhausted\n        # parameters controlling behaviors\n        p_levy = 0.06  # probability to attempt a Lévy jump each iteration\n        p_rejuvenate = 0.03  # chance to inject full-random sample\n        tournament_size = 3\n        max_local_backtracks = 3\n        orthogonal_scale = 0.4\n        levy_scale_factor = 0.5  # controls typical magnitude of a levy jump relative to domain\n        recomb_noise = 0.02\n\n        while evals < self.budget:\n            # pick a parent via small tournament\n            if pop_size == 1:\n                pid = 0\n            else:\n                idxs = self.rng.choice(pop_size, size=min(tournament_size, pop_size), replace=False)\n                pid = int(idxs[np.argmin(pop_f[idxs])])\n\n            parent = pop_x[pid].copy()\n            parent_f = float(pop_f[pid])\n            sigma = float(sigmas[pid])\n\n            # sample a random search direction (normalized)\n            d = self.rng.normal(size=self.dim)\n            d_norm = np.linalg.norm(d)\n            if d_norm < 1e-12:\n                # fallback to axis perturbation\n                j = self.rng.integers(0, self.dim)\n                d = np.zeros(self.dim); d[j] = 1.0\n                d_norm = 1.0\n            d /= d_norm\n\n            # stochasticized step-length (normal scaled by sigma and domain range)\n            step_length = sigma * (self.rng.normal(loc=0.0, scale=1.0) * 0.6 + 1.0)\n            step = d * (step_length * range_scale)\n            trial = clip_x(parent + step)\n            f_trial = callf(trial)\n            if f_trial < parent_f:\n                # success: accept and slightly increase sigma\n                pop_x[pid] = trial\n                pop_f[pid] = f_trial\n                sigmas[pid] = min(1.0, sigma * 1.15)\n                fails[pid] = 0\n                continue  # next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            small_sigma = sigma * 0.5\n            for bt in range(max_local_backtracks):\n                frac = 0.5 ** (bt + 1)\n                trial = clip_x(parent + d * (frac * small_sigma * range_scale))\n                f_trial = callf(trial)\n                if f_trial < parent_f:\n                    pop_x[pid] = trial\n                    pop_f[pid] = f_trial\n                    sigmas[pid] = min(1.0, sigma * (1.0 + 0.08 + 0.05 * bt))\n                    fails[pid] = 0\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            # make a vector orthogonal to d\n            o = self.rng.normal(size=self.dim)\n            # remove component along d\n            o -= np.dot(o, d) * d\n            o_norm = np.linalg.norm(o)\n            if o_norm > 1e-12:\n                o /= o_norm\n                trial = clip_x(parent + o * (orthogonal_scale * sigma * range_scale))\n                f_trial = callf(trial)\n                if f_trial < parent_f:\n                    pop_x[pid] = trial\n                    pop_f[pid] = f_trial\n                    sigmas[pid] = sigma * 1.08\n                    fails[pid] = 0\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (Cauchy heavy-tailed)\n            if self.rng.random() < p_levy and evals < self.budget:\n                # generate Cauchy-like heavy-tailed vector per-dim\n                u = self.rng.random(size=self.dim)\n                c = np.tan(np.pi * (u - 0.5))  # standard Cauchy samples\n                # robust scale of population spread (median absolute deviation)\n                if pop_size >= 2:\n                    med = np.median(pop_x, axis=0)\n                    mad = np.median(np.abs(pop_x - med), axis=0)\n                    robust = np.maximum(mad, 1e-6 * range_scale)\n                else:\n                    robust = 0.2 * range_scale\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                c_norm = np.linalg.norm(c)\n                if c_norm < 1e-12:\n                    c = self.rng.normal(size=self.dim)\n                    c_norm = np.linalg.norm(c)\n                c = c / c_norm\n                levy_step = c * (levy_scale_factor * range_scale * (0.5 + self.rng.random()))\n                trial = clip_x(parent + levy_step)\n                f_trial = callf(trial)\n                if f_trial < parent_f:\n                    # replace parent with jump success\n                    pop_x[pid] = trial\n                    pop_f[pid] = f_trial\n                    sigmas[pid] = max(1e-6, sigma * 1.3)\n                    fails[pid] = 0\n                else:\n                    # maybe replace the worst with this candidate if it helps\n                    worst = idx_worst()\n                    if f_trial < pop_f[worst]:\n                        pop_x[worst] = trial\n                        pop_f[worst] = f_trial\n                        sigmas[worst] = base_sigma\n                        fails[worst] = 0\n                # continue main loop\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if pop_size >= 2:\n                best_idx = idx_best()\n                # pick second best distinct\n                others = [i for i in range(pop_size) if i != best_idx]\n                if others:\n                    sec = min(others, key=lambda i: pop_f[i])\n                    alpha = self.rng.random() * 0.9 + 0.05\n                    child = alpha * pop_x[best_idx] + (1 - alpha) * pop_x[sec]\n                    child += self.rng.normal(scale=recomb_noise * range_scale)\n                    child = clip_x(child)\n                    f_child = callf(child)\n                    if f_child < parent_f:\n                        pop_x[pid] = child\n                        pop_f[pid] = f_child\n                        sigmas[pid] = max(1e-6, sigma * (1.0 + 0.1 * (alpha)))\n                        fails[pid] = 0\n                    else:\n                        # maybe replace worst if child is better\n                        worst = idx_worst()\n                        if f_child < pop_f[worst]:\n                            pop_x[worst] = child\n                            pop_f[worst] = f_child\n                            sigmas[worst] = base_sigma\n                            fails[worst] = 0\n\n            # adapt parent sigma on failure\n            fails[pid] += 1\n            if fails[pid] >= 3:\n                # reduce sigma moderately if repeated failures\n                sigmas[pid] = max(1e-6, sigmas[pid] * 0.8)\n                fails[pid] = 0\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.random() < p_rejuvenate and evals < self.budget:\n                worst = idx_worst()\n                x_new = self.rng.uniform(lb, ub)\n                f_new = callf(x_new)\n                if f_new < pop_f[worst]:\n                    pop_x[worst] = x_new\n                    pop_f[worst] = f_new\n                    sigmas[worst] = base_sigma\n                    fails[worst] = 0\n\n        # finished budget; ensure best is returned\n        if x_opt is None:\n            # pick best from population if global best wasn't set (shouldn't happen but safe)\n            ib = idx_best()\n            return float(pop_f[ib]), pop_x[ib].copy()\n        return float(f_opt), x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.508 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12042830918733727, 0.16196347153511392, 0.6904905042883782, 0.8958027850074609, 0.7572448064805004, 0.8333440409940729, 0.290573845999598, 0.4870075066836964, 0.6673613491148913, 0.18046545441970196]}, "task_prompt": ""}
{"id": "2afbc66a-0360-4ebe-9b5a-75ac90dde761", "fitness": 0.4722202122641688, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy-like jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    Main idea:\n    - Maintain a small population of candidate solutions each with its own adaptive step-size (sigma).\n    - At each iteration pick a parent by a small tournament, try a directional step (with stochasticized length),\n      a few backtracking fractions, an orthogonal perturbation, occasional heavy-tailed (Cauchy-like) jumps,\n      and simple recombination. Adapt sigma on success/failure and occasionally rejuvenate the population.\n    - Budget-aware: never exceeds the allowed number of function evaluations.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # sensible default population size depending on dimensionality and budget\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n\n        # result placeholders\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Extract bounds and make sure they are full-dimension arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # Clip bounds sanity\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n        span = ub - lb\n        # robust fallback if span has zeros\n        span[span == 0.0] = 1.0\n\n        remaining = int(self.budget)\n        # clear best\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        # budget-aware evaluator\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt or self.x_opt is None:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        n_init = min(self.pop_size, max(1, remaining))\n        pop = np.zeros((n_init, self.dim), dtype=float)\n        pop_f = np.zeros(n_init, dtype=float)\n        pop_sigma = np.zeros(n_init, dtype=float)\n\n        # base_sigma is a reasonable fraction of the search span\n        base_sigma = max(1e-12, 0.25 * np.mean(span))\n\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop[i, :] = x0\n            pop_f[i] = f0\n            # give each a slightly different initial sigma\n            pop_sigma[i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # If we couldn't create any individuals (very tiny budget), do random search until budget exhausted\n        if pop.shape[0] == 0 or remaining <= 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # main loop\n        while remaining > 0:\n            # ensure we operate on current population size\n            n = pop.shape[0]\n            # tournament selection (small k)\n            k = min(3, n)\n            inds = np.random.choice(n, k, replace=False)\n            values = pop_f[inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random direction and normalize\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-16, (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and increase sigma a bit\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(span))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(span))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * span\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two random distinct individuals and small noise\n            if n >= 2 and remaining > 0:\n                a, b = np.random.choice(n, 2, replace=False)\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * span) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (mildly reduce)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # if population is small and budget remains, consider adding new individuals occasionally\n            if remaining > 0 and n < self.pop_size and np.random.rand() < 0.05:\n                # add a new random individual (if budget allows)\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop = np.vstack([pop, x_new.reshape(1, -1)])\n                pop_f = np.concatenate([pop_f, np.array([f_new])])\n                pop_sigma = np.concatenate([pop_sigma, np.array([base_sigma * (0.7 + 0.6 * np.random.rand())])])\n\n            # occasionally shrink population if evaluations become scarce (keep best ones)\n            if remaining < max(1, int(0.05 * self.budget)) and pop.shape[0] > 4:\n                keep = max(4, int(np.ceil(0.5 * pop.shape[0])))\n                best_inds = np.argsort(pop_f)[:keep]\n                pop = pop[best_inds]\n                pop_f = pop_f[best_inds]\n                pop_sigma = pop_sigma[best_inds]\n\n        # finished budget or loop break\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.472 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11718065355155671, 0.1572697997841166, 0.4866324724380866, 0.9152761877505877, 0.48626722230339703, 0.9537188760903366, 0.2567182508312461, 0.4580009179930553, 0.7308735026418014, 0.16026423925750466]}, "task_prompt": ""}
{"id": "ed65e6c2-f133-4b42-88a7-707aad444b9f", "fitness": 0.4683585426118836, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining directional local searches, orthogonal refinements, adaptive per-individual step-sizes and occasional heavy-tailed (Cauchy) jumps to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if pop_size is None:\n            # population scales with dimensionality but remains modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # prepare bounds (support scalar or array)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # safe-guards\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        remaining = int(self.budget)\n        f_opt = np.inf\n        x_opt = None\n\n        # evaluation helper: clips, checks budget, updates best\n        def callf(x):\n            nonlocal remaining, f_opt, x_opt\n            if remaining <= 0:\n                # no budget left — caller should not request evaluation, return inf\n                return float(np.inf), x.copy()\n            x = np.asarray(x, dtype=float).copy()\n            # ensure vector is of correct length\n            if x.size != self.dim:\n                x = np.resize(x, self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n            return f, x.copy()\n\n        # extremely small budget: fallback to simple random sampling\n        if remaining <= 0:\n            return float(f_opt), (None if x_opt is None else x_opt.copy())\n\n        # initial population creation (as many as allowed up to pop_size)\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))  # initial scale\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining, max(2, self.pop_size))\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n        # if somehow no population created, try single random draw or return\n        if len(pop) == 0:\n            return float(f_opt), (None if x_opt is None else x_opt.copy())\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # algorithm parameters\n        levy_prob = 0.08\n        stagnation = 0\n        stagnation_limit = max(10, 5 * self.dim)\n        iter_count = 0\n\n        # main optimization loop: use remaining budget\n        while remaining > 0:\n            iter_count += 1\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / (nd + 1e-12)\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            x_try = x_parent + alpha * d\n            x_try = np.clip(x_try, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.25, np.mean(ub - lb))\n                stagnation = 0\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = x_parent + alpha * frac * d\n                x_try = np.clip(x_try, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    stagnation = 0\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    step_scale = sigma * (0.4 + 0.6 * np.random.rand())\n                    x_try = np.clip(x_parent + step_scale * r * (0.5 + np.random.randn()*0.2), lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        stagnation = 0\n                        continue\n\n            # occasional Lévy-like (Cauchy) jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < levy_prob and remaining > 0:\n                # sample standard Cauchy via tan(pi*(u-0.5))\n                u = np.random.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                # robust scale based on bounds and sigma\n                scale_vec = (ub - lb) * 0.2  # up to 20% of range, heavy-tailed\n                # normalize to avoid absurd extremes: divide by 90th percentile of abs(cauchy)\n                denom = np.percentile(np.abs(cauchy), 90) + 1e-12\n                step = (cauchy / denom) * scale_vec * (0.8 + 1.5 * np.random.rand())\n                x_try = np.clip(x_parent + step, lb, ub)\n                f_try, x_try = callf(x_try)\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n                    stagnation = 0\n                else:\n                    stagnation += 1\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1] if len(best2) > 1 else best2[0]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.02 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    stagnation = 0\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.4 + 0.8 * np.random.rand())\n                        stagnation = 0\n                    else:\n                        stagnation += 1\n\n            # adapt parent sigma on failure slowly\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation (random injection)\n            if (stagnation > stagnation_limit and remaining > 0) or (np.random.rand() < 0.02 and remaining > 0):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n                stagnation = 0\n\n            # keep population sizes and arrays consistent (in case)\n            if len(pop) != len(pop_f) or len(pop) != len(pop_sigma):\n                # synchronize lengths: pad sigma if missing\n                while len(pop_sigma) < len(pop):\n                    pop_sigma.append(base_sigma)\n                while len(pop_f) < len(pop):\n                    pop_f.append(float(np.inf))\n\n        return float(f_opt), (None if x_opt is None else x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.468 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12358116407018271, 0.15245321095230135, 0.5254206337142919, 0.9536291299280663, 0.6475104519889077, 0.765836512597149, 0.25972732316672775, 0.3733150031177749, 0.7282653031160649, 0.15384669346736923]}, "task_prompt": ""}
{"id": "a8adf61b-23bb-4e74-9f3b-25dc21d75f75", "fitness": 0.3335658747784317, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based directional local searches with orthogonal refinements and occasional heavy-tailed jumps; adapts per-individual step-sizes to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None, auto-scaled with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimensionality\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # get bounds (support scalar or array-like)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        else:\n            lb = lb.reshape(self.dim)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        else:\n            ub = ub.reshape(self.dim)\n\n        # small safety\n        eps = 1e-12\n\n        # initialize best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            # x: numpy array\n            if remaining <= 0:\n                raise RuntimeError(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float).ravel()\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = float(func(x))\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # base scale for step sizes (absolute)\n        range_vec = ub - lb\n        mean_range = float(np.mean(range_vec))\n        base_sigma = max(1e-8, 0.12 * mean_range)\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # per-individual sigma (absolute)\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search (single eval done above)\n        if len(pop) == 0:\n            if remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # convert to lists that will be mutated\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + eps\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            step_len = sigma * (0.7 + 0.6 * np.random.rand())\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, np.mean(range_vec))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + step_len * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # accept if improves\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.15, np.mean(range_vec))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                # limit extremes to avoid evaluating impossible values (but keep heavy tail)\n                step = np.tanh(step)  # keeps heavy-tail-ish but bounded\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                scale_vec = 0.25 * range_vec\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # if it's good replace the worst in population, else maybe adjust parent's sigma\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n                        continue\n                    else:\n                        # if it's somewhat promising, slightly increase parent's sigma to explore more\n                        if f_try < pop_f[parent_i] + 1e-6:\n                            pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.2, np.mean(range_vec))\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = np.random.randn(self.dim) * (0.15 * sigma)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # replace parent if improved, else possibly replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (slightly decrease to focus search)\n            # if no improvement in this iteration for parent, shrink sigma a bit\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.96, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # if population size is lower than intended and budget remains, grow by injecting random samples\n            while len(pop) < self.pop_size and remaining > 0:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.6 + 0.6 * np.random.rand()))\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.334 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12212245438916447, 0.15669433424001766, 0.7530302763251819, 0.17360628492254604, 0.365871582038815, 0.9675999588142356, 0.20406031986323636, 0.2285897787082002, 0.2203390832459705, 0.14374467523694878]}, "task_prompt": ""}
{"id": "ec929cd8-bdf5-40ba-a4d2-dfb32e56c723", "fitness": 0.5419690394307135, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional search with orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size; if None it's set based on dim\n    - seed: optional RNG seed for reproducibility\n\n    Main idea:\n    Maintain a small population of candidate solutions each with its own adaptive step-size (sigma).\n    At each step perform directional local trials (with backtracking), orthogonal refinements,\n    occasional Cauchy/Lévy jumps to escape basins, and simple recombination of best members.\n    Sigmas are increased on success and slightly decreased on failure; the worst members are periodically\n    rejuvenated to maintain diversity.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but remains modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # Algorithm hyper-parameters (tunable)\n        self.levy_prob = 0.08            # chance to try a heavy-tailed jump each iteration\n        self.rejuvenate_prob = 0.03     # chance to replace the worst with a fresh sample\n        self.tournament_k = 3           # small tournament to choose parent\n        self.base_sigma_frac = 0.25     # base sigma as fraction of domain width mean\n        self.max_sigma_scale = 1.0      # maximum sigma relative to domain width\n        self.min_sigma = 1e-12\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds as arrays (support scalar or array)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # Basic validation / clamp\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim, \"Bounds must match problem dimension\"\n        assert np.all(ub > lb), \"Upper bounds must be greater than lower bounds\"\n\n        # remaining evaluations\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No remaining function evaluations\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        n_init = min(self.pop_size, remaining)\n        pop = np.empty((n_init, self.dim), dtype=float)\n        pop_f = np.empty(n_init, dtype=float)\n        pop_sigma = np.empty(n_init, dtype=float)\n\n        domain_mean_width = float(np.mean(ub - lb))\n        base_sigma = max(self.min_sigma, self.base_sigma_frac * domain_mean_width)\n\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop[i] = x0\n            pop_f[i] = f0\n            pop_sigma[i] = base_sigma * (0.8 + 0.4 * np.random.rand())\n\n        # If no population (very small budget) -> random search done in initialization\n        if pop.shape[0] == 0:\n            return self.f_opt, self.x_opt\n\n        # Pre-allocate some reusable arrays\n        d = np.zeros(self.dim)\n        r = np.zeros(self.dim)\n\n        # Main loop: directional local searches, orthogonal tries, Lévy jumps, recombination, rejuvenation\n        # We'll loop until evaluations exhausted\n        while remaining > 0:\n            # choose parent via small tournament\n            k = min(self.tournament_k, pop.shape[0])\n            competitors = np.random.choice(pop.shape[0], size=k, replace=False)\n            parent_i = int(competitors[np.argmin(pop_f[competitors])])\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            dn = np.linalg.norm(d)\n            if dn <= 1e-16:\n                d = np.random.randn(self.dim)\n                dn = np.linalg.norm(d)\n            d /= dn\n\n            improved = False\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.2, self.max_sigma_scale * domain_mean_width)\n                    improved = True\n                    # go to next iteration\n                    continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            if remaining > 0 and not improved:\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, self.min_sigma)\n                        improved = True\n                        break\n                if improved:\n                    continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r /= nr\n                    for scale in (0.6, 0.3, -0.6):\n                        if remaining <= 0:\n                            break\n                        x_try = np.clip(x_parent + sigma * 0.6 * scale * r, lb, ub)\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = max(sigma * 0.9, self.min_sigma)\n                            improved = True\n                            break\n                    if improved:\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and (np.random.rand() < self.levy_prob):\n                # Cauchy-like heavy-tailed vector\n                # Use standard Cauchy: tan(pi*(u-0.5))\n                u = np.random.rand(self.dim)\n                step = np.tan(np.pi * (u - 0.5))\n                # robust scaling to avoid numerical explosion: scale each dimension by 90th percentile of abs(step)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                if denom == 0:\n                    denom = 1.0\n                scaled = (step / denom)\n                scale_vec = 0.2 * (ub - lb)  # relative jump scale per-dimension\n                x_try = np.clip(x_parent + scaled * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(self.min_sigma, sigma * 0.5)\n                        # consider this a success in global sense\n                        continue\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0:\n                best2_idx = np.argsort(pop_f)[:2]\n                if best2_idx.size >= 2:\n                    a, b = best2_idx[0], best2_idx[1]\n                    w = np.random.rand()\n                    child = w * pop[a] + (1 - w) * pop[b]\n                    # small gaussian perturbation proportional to local sigma\n                    small_noise = np.random.randn(self.dim) * (0.05 * (pop_sigma[a] + pop_sigma[b]) * 0.5)\n                    x_try = np.clip(child + small_noise, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    # replace parent if improved, else possibly replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(self.min_sigma, 0.9 * sigma)\n                        continue\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5 if 'base_sigma' in locals() else max(self.min_sigma, sigma * 0.5)\n\n            # adapt parent sigma on failure (mild shrink)\n            pop_sigma[parent_i] = max(self.min_sigma, pop_sigma[parent_i] * 0.85)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (np.random.rand() < self.rejuvenate_prob):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.542 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1029758514572845, 0.1871005032368701, 0.8708869721398063, 0.968780485127346, 0.5490169716906157, 0.9530506204400141, 0.22473221726603365, 0.5301835556267795, 0.9235870456476927, 0.10937617167469293]}, "task_prompt": ""}
{"id": "72086e48-48aa-4415-9ba1-7d7d29797efc", "fitness": 0.5467734032917096, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer that combines randomized directional local searches, orthogonal refinements, adaptive step-sizes and occasional Lévy-like jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None auto-scaled from dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimension\n            self.pop_size = int(max(6, min(40, 3 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.x_opt = None\n        self.f_opt = float(\"inf\")\n\n    def __call__(self, func):\n        # Prepare bounds as full-dim arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        # Safety: if bounds shapes mismatch dimension, try broadcast\n        if lb.size != self.dim:\n            lb = np.broadcast_to(lb.flatten()[0], self.dim)\n        if ub.size != self.dim:\n            ub = np.broadcast_to(ub.flatten()[0], self.dim)\n\n        # tracking remaining evaluations\n        remaining = self.budget\n\n        # helper to evaluate function while tracking budget and global best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No remaining budget to call the function.\")\n            # ensure numpy array and clip to bounds\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.clip(x, lb, ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget extremely small, fallback to pure random sampling\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population with random samples (or fewer if budget limited)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        max_scale = np.mean(ub - lb)\n        base_sigma = max(1e-8, 0.25 * max_scale)\n\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.5 + np.random.rand()))  # some diversity\n\n        # If no evaluations left or no population could be created, return best found so far\n        if len(pop) == 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: use remaining evaluations for local directional searches, orthogonal tries and Lévy jumps\n        # Hyperparameters\n        levy_prob = 0.12\n        rejuvenation_prob = 0.06\n        orthogonal_factor = 0.6\n        recomb_noise_scale = 0.02 * (ub - lb)\n        backtrack_fracs = (0.5, 0.25, -0.5, -0.25)\n\n        while remaining > 0:\n            # small tournament to pick a parent (balance exploration/exploitation)\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_rel = np.argmin(values)\n            parent_i = inds[parent_rel]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15, max_scale)\n                # continue to next iteration (exploit further from successful parent)\n                continue\n\n            # local backtracking / small-step refinement along same direction (few tries)\n            improved = False\n            for frac in backtrack_fracs:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.08, max_scale)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + orthogonal_factor * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, max_scale)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < levy_prob and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(size=self.dim)\n                denom = np.mean(np.abs(step)) + 1e-12\n                step = step / denom\n                scale_vec = 0.18 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # if it's good replace the worst in population, else maybe keep as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n                        # after a successful jump continue main loop\n                        continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best_inds = np.argsort(pop_f)\n                a, b = best_inds[0], best_inds[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = np.random.randn(self.dim) * (recomb_noise_scale * (0.5 + np.random.rand()))\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.05, max_scale)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n\n            # adapt parent sigma on failure (slightly reduce)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < rejuvenation_prob and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.5 + np.random.rand())\n\n        # finished budget or loop exit\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.547 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17932488218288922, 0.1679059864215655, 0.7307548061070205, 0.945604155824634, 0.8612161293179214, 0.8798347222377172, 0.3005337708290059, 0.43564975789747606, 0.75780130888267, 0.20910851321619583]}, "task_prompt": ""}
{"id": "1404c5f3-5e8f-47f9-941b-c3c0b769922c", "fitness": 0.3712655399726471, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive directional local searches with orthogonal refinement and occasional heavy-tailed Lévy jumps for robust exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # scale modestly with dimensionality\n            self.pop_size = max(4, min(30, int(6 + 1.5 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Determine bounds if provided by func, else default [-5,5]\n        try:\n            # many benchmark harnesses expose bounds as arrays or scalars\n            lb_raw = func.bounds.lb\n            ub_raw = func.bounds.ub\n            lb = np.asarray(lb_raw).reshape(-1) if np.ndim(lb_raw) else None\n            ub = np.asarray(ub_raw).reshape(-1) if np.ndim(ub_raw) else None\n            if lb is None or ub is None or lb.size == 0:\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n            else:\n                # if bounds are scalars broadcast to dim\n                if lb.size == 1:\n                    lb = np.full(self.dim, float(lb.item()))\n                if ub.size == 1:\n                    ub = np.full(self.dim, float(ub.item()))\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        # helper to evaluate while tracking budget and best\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure correct shape and clip\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If no budget, return immediately\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        # base sigma scale based on domain size (scalar)\n        domain_scale = np.mean(ub - lb)\n        base_sigma = max(1e-12, 0.08 * domain_scale)  # baseline step-size\n\n        for i in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize individual sigma around base_sigma with randomization\n            pop_sigma.append(base_sigma * (0.5 + 1.5 * np.random.rand()))\n\n        # If no population could be created (very small budget), do pure random sampling leftover\n        if len(pop) == 0:\n            # try to evaluate at least one point if none yet\n            try:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            except RuntimeError:\n                pass\n            return self.f_opt, self.x_opt\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        it = 0\n        # main loop: use remaining budget for directional searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            it += 1\n            # small tournament selection to pick parent\n            k = min(3, pop.shape[0])\n            inds = np.random.choice(pop.shape[0], size=k, replace=False)\n            # pick the best among tournament\n            parent_i = inds[int(np.argmin(pop_f[inds]))]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # normalized random direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # fallback to random direction if degenerate\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # stochasticize step length\n            step_len = sigma * max(1e-12, 1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n\n            improved = False\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.25, base_sigma * 20.0)\n                improved = True\n                # encourage local refinement by a quick small-step exploration along d\n                # try a single smaller step to potentially improve further\n                if remaining > 0:\n                    small = 0.4 * step_len\n                    x_try2 = np.clip(x_try + small * d, lb, ub)\n                    try:\n                        f_try2, x_try2 = callf(x_try2)\n                        if f_try2 < pop_f[parent_i]:\n                            pop[parent_i] = x_try2\n                            pop_f[parent_i] = f_try2\n                            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-14)\n                    except RuntimeError:\n                        pass\n                # continue main loop\n                if remaining <= 0:\n                    break\n                continue\n\n            # local backtracking / smaller-step refinement along direction\n            for frac in (0.5, 0.25, 0.125):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + frac * step_len * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-14)\n                    improved = True\n                    break\n            if improved:\n                if remaining <= 0:\n                    break\n                continue\n\n            # orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make orthogonal to d\n                r = r - (r.dot(d)) * d\n                rn = np.linalg.norm(r)\n                if rn > 1e-12:\n                    r = r / rn\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_i]:\n                            pop[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = max(sigma * 0.9, 1e-14)\n                            improved = True\n                    except RuntimeError:\n                        break\n            if improved:\n                if remaining <= 0:\n                    break\n                continue\n\n            # occasional Lévy-like jump (heavy-tailed) with small probability\n            if remaining > 0 and (np.random.rand() < 0.07):\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization to avoid complete blow-up\n                denom = np.median(np.abs(step)) + 1e-12\n                step = step / denom\n                # scale component-wise by a fraction of domain\n                scale_vec = 0.18 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    # if it's good replace parent\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.7, 1e-14)\n                    continue\n                else:\n                    # maybe replace the worst if jump found something moderately good\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n                if remaining <= 0:\n                    break\n\n            # recombination exploitation: mix two best & small noise\n            if remaining > 0 and pop.shape[0] >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                alpha = 0.6 + 0.2 * np.random.rand()\n                child = alpha * pop[a] + (1.0 - alpha) * pop[b]\n                child += 0.03 * (ub - lb) * np.random.randn(self.dim)\n                child = np.clip(child, lb, ub)\n                try:\n                    f_try, x_try = callf(child)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                    else:\n                        # else replace parent\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = base_sigma * 0.7\n\n            # adapt parent sigma on failure\n            # if no improvement this round, shrink sigma slightly to concentrate search\n            if not improved:\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-14)\n\n            # occasional population rejuvenation: replace worst with random sample\n            if remaining > 0 and (it % max(5, int(5 + self.dim/2)) == 0):\n                worst_i = int(np.argmax(pop_f))\n                # generate new random candidate\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n                else:\n                    # still refresh sigma and keep the point occasionally\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget or exhausted loop\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.371 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13507015253873544, 0.15563068497308397, 0.4263963906157102, 0.6525566496996336, 0.24291707395106876, 0.7414916304338116, 0.21862069396609296, 0.312736897992984, 0.7023255543705514, 0.12490967118479956]}, "task_prompt": ""}
{"id": "0b8121ce-5828-4028-9bac-2e3bc87d3ccb", "fitness": 0.4616216370277099, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based continuous optimizer combining randomized directional local searches, orthogonal refinements and occasional Lévy-like jumps with per-individual adaptive step-sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population default: modestly larger for higher dims but capped\n        if pop_size is None:\n            self.pop_size = min(40, max(6, int(4 + 2 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # bounds handling: ensure arrays of correct shape\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # ensure shape matches dim\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        # global tracking\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            # if out of budget, return current best without calling\n            if remaining <= 0:\n                return self.f_opt, self.x_opt.copy() if self.x_opt is not None else None\n            x = np.asarray(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # quick fallback for extremely small budgets: random sampling\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population (as many as budget allows up to pop_size)\n        pop = []\n        pop_f = []\n        # initial scale relative to search range\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n        pop_sigma = []\n\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            # sample uniformly in bounds\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            if x0 is None:\n                break\n            pop.append(x0.copy())\n            pop_f.append(float(f0))\n            # initialize per-individual sigma with some heterogeneity\n            pop_sigma.append(base_sigma * np.exp(np.random.normal(0, 0.3)))\n            if remaining <= 0:\n                break\n\n        pop = np.array(pop) if len(pop) > 0 else np.zeros((0, self.dim))\n        pop_f = np.array(pop_f) if len(pop_f) > 0 else np.array([])\n        pop_sigma = np.array(pop_sigma) if len(pop_sigma) > 0 else np.array([])\n\n        # If no population created (very small budget), brute-force random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return self.f_opt, self.x_opt\n\n        # main loop: directional searches, orthogonal tries, Lévy jumps, recombination, rejuvenation\n        stagnation = 0\n        iters = 0\n        while remaining > 0:\n            iters += 1\n            # pick a parent via small tournament (size 2 or 3)\n            k = 2 if len(pop) < 4 else 3\n            cand_idx = np.random.choice(len(pop), size=min(k, len(pop)), replace=False)\n            parent_i = int(cand_idx[np.argmin(pop_f[cand_idx])])\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.normal(0, 1, size=self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * np.abs(np.random.normal(1.0, 0.35))  # positive scale\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n            if x_try is None:\n                break\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # success: accept and increase sigma slightly\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = float(f_try)\n                pop_sigma[parent_i] = min(sigma * 1.12, np.mean(ub - lb))\n                improved = True\n                stagnation = 0\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                back_steps = 3\n                factor = 0.5\n                for bs in range(back_steps):\n                    small_alpha = alpha * (factor ** (bs + 1))\n                    x_bt = np.clip(x_parent + small_alpha * d, lb, ub)\n                    f_bt, x_bt = callf(x_bt)\n                    if x_bt is None:\n                        break\n                    if f_bt < pop_f[parent_i]:\n                        pop[parent_i] = x_bt.copy()\n                        pop_f[parent_i] = float(f_bt)\n                        pop_sigma[parent_i] = min(sigma * 1.08, np.mean(ub - lb))\n                        improved = True\n                        stagnation = 0\n                        break\n                if not improved:\n                    # reduce sigma moderately on failure\n                    pop_sigma[parent_i] = max(sigma * 0.88, 1e-12)\n                    stagnation += 1\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.normal(0, 1, self.dim)\n                # project out d to make r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 0:\n                    r = r / nr\n                    ortho_step = 0.6 * pop_sigma[parent_i]\n                    x_o = np.clip(pop[parent_i] + ortho_step * r, lb, ub)\n                    f_o, x_o = callf(x_o)\n                    if x_o is None:\n                        break\n                    if f_o < pop_f[parent_i]:\n                        pop[parent_i] = x_o.copy()\n                        pop_f[parent_i] = float(f_o)\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.06, np.mean(ub - lb))\n                        stagnation = 0\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and np.random.rand() < 0.08:\n                # Cauchy-like heavy-tailed vector (standard cauchy) with robust normalization\n                step = np.random.standard_cauchy(size=self.dim)\n                # cap extreme outliers by percentile to avoid invalid huge steps\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_jump = np.clip(x_parent + scale_vec * step, lb, ub)\n                f_jump, x_jump = callf(x_jump)\n                if x_jump is None:\n                    break\n                # replace worst if jump is good\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump.copy()\n                    pop_f[worst_i] = float(f_jump)\n                    pop_sigma[worst_i] = base_sigma * 0.5\n                    stagnation = 0\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0 and len(pop) >= 2 and np.random.rand() < 0.12:\n                best2 = np.argsort(pop_f)[:2]\n                w = np.random.rand()\n                child = w * pop[best2[0]] + (1 - w) * pop[best2[1]]\n                child += np.random.normal(0, 0.05 * np.mean(ub - lb), size=self.dim)\n                child = np.clip(child, lb, ub)\n                f_child, child = callf(child)\n                if child is None:\n                    break\n                # replace worst if better\n                worst_i = int(np.argmax(pop_f))\n                if f_child < pop_f[worst_i]:\n                    pop[worst_i] = child.copy()\n                    pop_f[worst_i] = float(f_child)\n                    pop_sigma[worst_i] = base_sigma * 0.5\n                    stagnation = 0\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (stagnation > 8 or np.random.rand() < 0.03):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if x_new is None:\n                    break\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = float(f_new)\n                pop_sigma[worst_i] = base_sigma * np.exp(np.random.normal(0, 0.5))\n                stagnation = 0\n\n            # occasionally grow diversity by slightly perturbing a random individual\n            if remaining > 0 and np.random.rand() < 0.05:\n                i_pert = np.random.randint(len(pop))\n                trial = pop[i_pert] + np.random.normal(0, 0.3 * pop_sigma[i_pert], size=self.dim)\n                trial = np.clip(trial, lb, ub)\n                f_trial, trial = callf(trial)\n                if trial is None:\n                    break\n                if f_trial < pop_f[i_pert]:\n                    pop[i_pert] = trial.copy()\n                    pop_f[i_pert] = float(f_trial)\n                    pop_sigma[i_pert] = min(pop_sigma[i_pert] * 1.05, np.mean(ub - lb))\n                    stagnation = 0\n                else:\n                    pop_sigma[i_pert] = max(pop_sigma[i_pert] * 0.93, 1e-12)\n\n            # ensure population arrays remain consistent if budget forces early exit\n            if remaining <= 0:\n                break\n\n            # optional shrink population if evaluations are very scarce\n            if remaining < len(pop) // 2 and len(pop) > 3:\n                # drop the worst half to focus remaining budget on best individuals\n                keep = max(3, int(len(pop) // 2))\n                idx = np.argsort(pop_f)[:keep]\n                pop = pop[idx].copy()\n                pop_f = pop_f[idx].copy()\n                pop_sigma = pop_sigma[idx].copy()\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.462 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15009441584527827, 0.1999485921066484, 0.5673838141544216, 0.9265043110076072, 0.32743028328290336, 0.8968517781071383, 0.279552219573615, 0.34966780511900497, 0.7516966002304241, 0.16708655085005875]}, "task_prompt": ""}
{"id": "9f6730b0-60ac-45f0-abbf-1be0e6ad71b1", "fitness": 0.6101040829433475, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy) jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality (required)\n    - pop_size: optional population size\n    - seed: RNG seed (optional)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # read bounds and ensure arrays of correct shape\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        # If bounds are scalar, expand\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        # ensure shapes match dim\n        if lb.size != self.dim or ub.size != self.dim:\n            raise ValueError(\"Bounds size doesn't match solver dim\")\n\n        # quick fallback if budget very small: pure random search\n        if self.budget <= 2:\n            self.f_opt = np.inf\n            self.x_opt = None\n            remaining = self.budget\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f = func(x)\n                remaining -= 1\n                if f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n            return self.f_opt, self.x_opt\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure numpy array float and correct shape\n            x = np.array(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            nonlocal_update_best(f, x)\n            return f, x\n\n        def nonlocal_update_best(f, x):\n            # use outer scope variables self.f_opt/self.x_opt\n            if f < getattr(self, \"f_opt\", np.inf):\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n\n        # Initialize population\n        # choose population based on dim and budget but modest\n        default_pop = max(4, min(self.dim + 2, 20))\n        if self.pop_size is None:\n            pop_size = min(default_pop, max(2, self.budget // 12))\n        else:\n            pop_size = max(2, int(self.pop_size))\n            pop_size = min(pop_size, max(2, self.budget // 4))\n\n        # initial scale relative to bounds\n        base_scale = max(1e-8, 0.25 * np.mean(ub - lb))\n\n        pop = []\n        pop_sigma = []\n        pop_f = []\n\n        # seed initial population around a random center\n        center = np.random.uniform(lb, ub)\n        # ensure at least one global random sample\n        for i in range(pop_size):\n            # add small perturbations around center and randoms\n            if i == 0:\n                x = center.copy()\n            else:\n                if np.random.rand() < 0.5:\n                    x = center + np.random.normal(0, base_scale, size=self.dim)\n                else:\n                    x = np.random.uniform(lb, ub)\n            x = np.clip(x, lb, ub)\n            try:\n                f, x = callf(x)\n            except RuntimeError:\n                break\n            pop.append(x.copy())\n            pop_f.append(f)\n            pop_sigma.append(base_scale * (1.0 + 0.5 * np.random.rand()))\n\n        # If budget used up during init, handle\n        if len(pop) == 0:\n            # fallback: random sampling until exhausted\n            self.f_opt = np.inf\n            self.x_opt = None\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f = func(x)\n                remaining -= 1\n                if f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n            return self.f_opt, self.x_opt\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # main loop\n        stagnation = 0\n        iters = 0\n        while remaining > 0:\n            iters += 1\n            # small tournament selection\n            k = min(3, len(pop))\n            candidates = np.random.choice(len(pop), size=k, replace=False)\n            parent_i = candidates[np.argmin(pop_f[candidates])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random direction\n            d = np.random.normal(0, 1, size=self.dim)\n            nr = np.linalg.norm(d)\n            if nr <= 0:\n                continue\n            d = d / nr\n\n            # primary directional trial with stochasticized step-length (Cauchy for heavy-tail)\n            # sample step multiplier from truncated Cauchy to avoid infinite steps\n            raw_c = np.random.standard_cauchy()\n            step_len = sigma * (1.0 + 0.8 * np.tanh(raw_c))  # tanh to avoid extremely large\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n            success = False\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i] - 1e-12:\n                # accept and increase sigma a bit\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.18, np.max(ub - lb))\n                success = True\n                stagnation = 0\n            else:\n                # local backtracking: try a few fractional steps along d\n                frac_list = [0.5, 0.25, 0.125, 0.0625]\n                for frac in frac_list:\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + frac * sigma * d, lb, ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i] - 1e-12:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n                        success = True\n                        stagnation = 0\n                        break\n\n            # orthogonal perturbation for local diversification\n            if remaining > 0 and nr > 1e-12:\n                r = np.random.normal(0, 1, size=self.dim)\n                # make orthogonal to d: subtract projection\n                r = r - np.dot(r, d) * d\n                rr = np.linalg.norm(r)\n                if rr > 1e-12:\n                    r = r / rr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_i] - 1e-12:\n                            pop[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                            success = True\n                            stagnation = 0\n                    except RuntimeError:\n                        break\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            jump_prob = 0.035 + 0.0005 * self.dim  # small scaling with dim\n            if (np.random.rand() < jump_prob) and remaining > 0:\n                # Cauchy vector per-dim, but normalize to robust scale\n                raw = np.random.standard_cauchy(size=self.dim)\n                # robust scale to limit extremely large values while preserving heavy-tail\n                med = np.median(np.abs(raw)) + 1e-12\n                levy_vec = raw / med\n                jump_scale = base_scale * (1.0 + 5.0 * np.random.rand())\n                x_jump = np.clip(x_parent + jump_scale * levy_vec, lb, ub)\n                try:\n                    f_jump, x_jump = callf(x_jump)\n                except RuntimeError:\n                    break\n                # if it's good, replace the worst in population\n                worst_i = np.argmax(pop_f)\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump.copy()\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(base_scale * 0.5, 1e-12)\n                    success = True\n                    stagnation = 0\n                else:\n                    # sometimes keep as candidate by replacing parent (explorative)\n                    if np.random.rand() < 0.1:\n                        pop[parent_i] = x_jump.copy()\n                        pop_f[parent_i] = f_jump\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.7, 1e-12)\n                        success = True\n                        stagnation = 0\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0:\n                # pick two best\n                best_idx = np.argsort(pop_f)[:2]\n                if len(best_idx) == 2:\n                    x_mix = 0.5 * (pop[best_idx[0]] + pop[best_idx[1]])\n                else:\n                    x_mix = pop[best_idx[0]].copy()\n                # small gaussian noise scaled by mean sigma\n                mix_noise = np.random.normal(0, 0.25 * np.mean(pop_sigma), size=self.dim)\n                x_mix = np.clip(x_mix + mix_noise, lb, ub)\n                try:\n                    f_mix, x_mix = callf(x_mix)\n                except RuntimeError:\n                    break\n                # replace parent or worst depending on improvement\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix.copy()\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.05, 1e-12)\n                    success = True\n                    stagnation = 0\n                else:\n                    worst_i = np.argmax(pop_f)\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix.copy()\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = max(0.8 * np.mean(pop_sigma), 1e-12)\n                        success = True\n                        stagnation = 0\n\n            # adapt parent sigma on failure\n            if not success:\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.88, 1e-12)\n                stagnation += 1\n            else:\n                # mild increase on success for that individual\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * (1.05 + 0.03 * np.random.rand()),\n                                          np.max(ub - lb))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (np.random.rand() < 0.06 or stagnation > 12):\n                worst_i = np.argmax(pop_f)\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_scale * (0.5 + np.random.rand())\n\n            # small population shrink/grow: if some individuals are obsolete (very close), reinitialize one\n            if remaining > 0 and len(pop) > 1:\n                # if two individuals extremely close, re-seed the worse\n                dmat = np.linalg.norm(pop - pop.mean(axis=0), axis=1)\n                if np.min(dmat) < 1e-8:\n                    worst_i = np.argmax(pop_f)\n                    x_new = np.random.uniform(lb, ub)\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        break\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_scale * (0.3 + 0.7 * np.random.rand())\n\n            # safety: keep arrays consistent lengths (in case budget exhausted during steps)\n            if remaining <= 0:\n                break\n\n            # small safeguard: if stagnation extremely long, perform a bigger Lévy jump\n            if stagnation > 40 and remaining > 0:\n                worst_i = np.argmax(pop_f)\n                raw = np.random.standard_cauchy(size=self.dim)\n                med = np.median(np.abs(raw)) + 1e-12\n                levy_vec = raw / med\n                jump_scale = base_scale * (2.0 + 8.0 * np.random.rand())\n                x_jump = np.clip(pop[worst_i] + jump_scale * levy_vec, lb, ub)\n                try:\n                    f_jump, x_jump = callf(x_jump)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_jump.copy()\n                pop_f[worst_i] = f_jump\n                pop_sigma[worst_i] = base_scale * 0.5\n                stagnation = 0\n\n        # finished budget or exhausted loop\n        # ensure best returned\n        if getattr(self, \"f_opt\", None) is None:\n            # fallback to best in population\n            idx = np.argmin(pop_f)\n            self.f_opt = float(pop_f[idx])\n            self.x_opt = pop[idx].copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.610 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09846467886612897, 0.16854999901730205, 0.860801891691015, 0.9598384081587479, 0.8884185246643306, 0.9094708517765246, 0.30166869567260135, 0.855659477905328, 0.880894157760645, 0.17727414392085217]}, "task_prompt": ""}
{"id": "46b16feb-3ac7-486a-84f0-6c4f9ab3258c", "fitness": 0.21851321298010387, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based heuristic combining randomized directional local searches, orthogonal refinements, adaptive step-sizes and occasional heavy-tailed Lévy-like jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled if None)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget_total = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        if seed is not None:\n            self.rng = np.random.RandomState(seed)\n        else:\n            self.rng = np.random.RandomState()\n        # safety minimums\n        if self.budget_total < 1:\n            raise ValueError(\"budget must be >= 1\")\n\n    def __call__(self, func):\n        # helper: read bounds and ensure arrays of correct shape\n        lb = np.asarray(func.bounds.lb)\n        ub = np.asarray(func.bounds.ub)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        # global bookkeeping\n        remaining = int(self.budget_total)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # safe evaluation helper that clips to bounds and updates global best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return np.inf\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if np.isfinite(f) and f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # If budget extremely small, fallback to robust random search\n        if self.budget_total < 20:\n            self.f_opt = np.inf\n            self.x_opt = None\n            for _ in range(remaining):\n                x = self.rng.uniform(lb, ub)\n                f = callf(x)\n            return self.f_opt, self.x_opt\n\n        # determine population size\n        if self.pop_size is None:\n            # scale with dim but cap relative to budget\n            default = max(6, 2 * self.dim)\n            cap = max(2, int(self.budget_total / 20))\n            n_pop = min(default, cap)\n        else:\n            n_pop = int(max(2, self.pop_size))\n        # ensure we can at least initialize population\n        n_pop = min(n_pop, max(1, remaining))\n        if n_pop < 1:\n            # fall back to random search\n            for _ in range(remaining):\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # base sigma scale relative to bound size\n        domain_scale = np.mean(ub - lb)\n        base_sigma = max(1e-12, domain_scale * 0.2)\n\n        # initialize population\n        pop = np.zeros((n_pop, self.dim))\n        pop_f = np.full(n_pop, np.inf)\n        pop_sigma = np.zeros(n_pop)\n        for i in range(n_pop):\n            if remaining <= 0:\n                break\n            pop[i] = self.rng.uniform(lb, ub)\n            pop_f[i] = callf(pop[i])\n            pop_sigma[i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n        # compact if we couldn't fill due to low budget\n        valid = np.isfinite(pop_f)\n        if not np.all(valid):\n            pop = pop[valid]\n            pop_f = pop_f[valid]\n            pop_sigma = pop_sigma[valid]\n            n_pop = pop.shape[0]\n        if n_pop == 0:\n            # no initial evals succeeded; random remainder\n            for _ in range(remaining):\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # main loop\n        # probabilities and parameters\n        p_levy = 0.06\n        p_rejuvenate = 0.03\n        p_recombine = 0.25\n        backtrack_tries = 3\n\n        while remaining > 0:\n            # quick recalculations\n            worst_i = int(np.argmax(pop_f))\n            best_i = int(np.argmin(pop_f))\n            # small tournament selection for parent\n            k = min(3, n_pop)\n            candidates = self.rng.choice(n_pop, k, replace=False)\n            parent_i = candidates[np.argmin(pop_f[candidates])]\n            parent_x = pop[parent_i].copy()\n            parent_f = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length (log-normal tweak)\n            step_multiplier = np.exp(0.2 * self.rng.randn())\n            step = sigma * step_multiplier\n            x_try = parent_x + step * d\n            f_try = callf(x_try) if remaining > 0 else np.inf\n            if f_try < parent_f:\n                # accept, slightly increase sigma\n                pop[parent_i] = np.minimum(np.maximum(x_try, lb), ub)\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * (1.05 + 0.02 * self.rng.rand()), domain_scale)\n                continue\n\n            # local backtracking / small-step refinement along direction\n            improved = False\n            for t in range(backtrack_tries):\n                if remaining <= 0:\n                    break\n                small_sigma = sigma * (0.6 ** (t + 1))\n                x_try = parent_x + small_sigma * d\n                f_try = callf(x_try)\n                if f_try < parent_f:\n                    pop[parent_i] = np.minimum(np.maximum(x_try, lb), ub)\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(small_sigma * 1.02, 1e-12)\n                    improved = True\n                    break\n                else:\n                    # slightly adjust sigma downward for this parent\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = self.rng.randn(self.dim)\n                # remove projection on d to make orthogonal-ish\n                r = r - (r.dot(d)) * d\n                norm_r = np.linalg.norm(r)\n                if norm_r == 0:\n                    # fallback to random small perturbation\n                    r = self.rng.randn(self.dim)\n                    norm_r = np.linalg.norm(r)\n                r = r / norm_r\n                ortho_scale = sigma * (0.4 + 0.6 * self.rng.rand())\n                x_try = parent_x + ortho_scale * r\n                f_try = callf(x_try)\n                if f_try < parent_f:\n                    pop[parent_i] = np.minimum(np.maximum(x_try, lb), ub)\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.08, domain_scale)\n                    continue\n                else:\n                    pop_sigma[parent_i] = max(sigma * 0.96, 1e-12)\n\n            # occasional Lévy-like jump to escape local basins\n            if self.rng.rand() < p_levy and remaining > 0:\n                # Cauchy-like heavy-tailed vector (standard Cauchy)\n                z = self.rng.standard_cauchy(self.dim)\n                # robust normalization: scale by 80th-percentile to avoid absolute extremes\n                absz = np.abs(z)\n                q80 = np.percentile(absz, 80) if absz.size > 0 else 1.0\n                if q80 <= 0:\n                    q80 = 1.0\n                z = z / q80\n                # scale jump by a factor tied to sigma and domain (to allow escape)\n                jump_scale = sigma * (3.0 + 4.0 * self.rng.rand())\n                x_jump = parent_x + jump_scale * z\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                f_jump = callf(x_jump) if remaining > 0 else np.inf\n                if f_jump < parent_f:\n                    # replace parent with jump and slightly increase sigma\n                    pop[parent_i] = x_jump.copy()\n                    pop_f[parent_i] = f_jump\n                    pop_sigma[parent_i] = min(sigma * (1.2 + 0.3 * self.rng.rand()), domain_scale)\n                    continue\n                else:\n                    # if jump better than worst, inject it there\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump.copy()\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(sigma * 0.9, 1e-12)\n                        continue\n                    else:\n                        # otherwise slightly shrink parent's sigma to favor exploitation\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n\n            # recombination exploitation: mix two best and small noise\n            if self.rng.rand() < p_recombine and n_pop >= 2 and remaining > 0:\n                idxs = np.argsort(pop_f)\n                a, b = idxs[0], idxs[1]\n                mix = 0.5 * (pop[a] + pop[b])\n                noise = 0.1 * pop_sigma[parent_i] * self.rng.randn(self.dim)\n                x_try = mix + noise\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                f_try = callf(x_try)\n                if f_try < parent_f:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.06, domain_scale)\n                    # possibly inject into population replacing the worst\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(pop_sigma[worst_i] * 0.95, 1e-12)\n                    continue\n                else:\n                    # if mix is good compared to worst, replace worst\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(0.9 * pop_sigma[worst_i], 1e-12)\n\n            # adapt parent sigma on failure (decay)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.97, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < p_rejuvenate and remaining > 0:\n                x_new = self.rng.uniform(lb, ub)\n                f_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n\n            # if population shrinks to 1, try to re-inflate using the best + noise (budget permitting)\n            if n_pop == 1 and remaining > 0:\n                x_new = pop[0] + 0.5 * domain_scale * self.rng.randn(self.dim)\n                x_new = np.minimum(np.maximum(x_new, lb), ub)\n                f_new = callf(x_new)\n                if np.isfinite(f_new):\n                    pop = np.vstack([pop, x_new])\n                    pop_f = np.concatenate([pop_f, [f_new]])\n                    pop_sigma = np.concatenate([pop_sigma, [base_sigma * (0.8 + 0.4 * self.rng.rand())]])\n                    n_pop = pop.shape[0]\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.219 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11082821093644624, 0.17097728348844177, 0.28315680557059764, 0.1992865935450603, 0.21679770272692822, 0.32425117339641385, 0.2611558909392655, 0.24587415828950676, 0.22767097987205664, 0.14513333103632187]}, "task_prompt": ""}
{"id": "f75909e6-62f2-48a7-84ca-997d99e43a79", "fitness": 0.5016353024284144, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search mixing randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest multiple of dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimension\n            self.pop_size = max(6, min(40, 4 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds from func; allow scalar bounds or arrays\n        lb = np.asarray(func.bounds.lb)\n        ub = np.asarray(func.bounds.ub)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        # internal budget tracker\n        evals_used = 0\n        budget = self.budget\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal evals_used, budget, lb, ub\n            if evals_used >= budget:\n                raise StopIteration(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            evals_used += 1\n            # update global best\n            nonlocal_best_update(f, x)\n            return float(f), x\n\n        def nonlocal_best_update(f, x):\n            nonlocal evals_used\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, dtype=float)\n\n        # If budget is extremely small, fallback to pure random search\n        if budget <= 10:\n            # simple uniform random sampling\n            for _ in range(budget):\n                x = self.rng.uniform(lb, ub)\n                f = func(x)\n                evals_used += 1\n                nonlocal_best_update(f, x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        sigma0 = 0.05 * np.linalg.norm(span) / max(1.0, np.sqrt(self.dim))\n        # create a heterogenous initial sigma scale\n        for i in range(self.pop_size):\n            if evals_used >= budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals_used += 1\n            sigma = sigma0 * (1.0 + 0.5 * self.rng.standard_normal())\n            sigma = float(max(1e-8, abs(sigma)))\n            pop.append({'x': np.array(x, dtype=float), 'f': float(f), 'sigma': sigma})\n            nonlocal_best_update(f, x)\n\n        # If no population could be created (very small initial budget), do pure random search\n        if len(pop) == 0:\n            while evals_used < budget:\n                x = self.rng.uniform(lb, ub)\n                f = func(x)\n                evals_used += 1\n                nonlocal_best_update(f, x)\n            return self.f_opt, self.x_opt\n\n        # utility to get best/worst indices\n        def best_idx(pop_list):\n            return int(np.argmin([p['f'] for p in pop_list]))\n\n        def worst_idx(pop_list):\n            return int(np.argmax([p['f'] for p in pop_list]))\n\n        # Main loop: use remaining budget\n        while evals_used < budget:\n            # pick a parent via a small tournament selection\n            k = min(3, len(pop))\n            contenders = self.rng.integers(0, len(pop), size=k)\n            parent_idx = min(contenders, key=lambda i: pop[i]['f'])\n            parent = pop[parent_idx]\n            px = parent['x'].copy()\n            pf = parent['f']\n            sigma = parent['sigma']\n\n            # sample a random search direction (normalized)\n            d = self.rng.normal(size=self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                # degenerate; use uniform direction\n                d = self.rng.uniform(-1.0, 1.0, size=self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # multiplicative noise on sigma\n            step_len = sigma * np.exp(0.15 * self.rng.standard_normal())\n            trial_x = px + step_len * d\n            # clamp\n            trial_x = np.minimum(np.maximum(trial_x, lb), ub)\n            if evals_used >= budget:\n                break\n            f_trial, trial_x = callf(trial_x)\n            if f_trial < pf:\n                # success: accept and slightly increase sigma\n                pop[parent_idx] = {'x': trial_x, 'f': f_trial, 'sigma': min(sigma * 1.15, max(span))}\n                parent = pop[parent_idx]\n            else:\n                # failure: try some local backtracking / refinement (few tries)\n                improved = False\n                back_steps = 3\n                for j in range(back_steps):\n                    if evals_used >= budget:\n                        break\n                    small_step = step_len * (0.5 ** (j + 1)) * (1.0 + 0.2 * self.rng.standard_normal())\n                    tx = px + small_step * d\n                    tx = np.minimum(np.maximum(tx, lb), ub)\n                    ftx, tx = callf(tx)\n                    if ftx < pf:\n                        pop[parent_idx] = {'x': tx, 'f': ftx, 'sigma': min(sigma * 1.08, max(span))}\n                        improved = True\n                        break\n                if not improved:\n                    # try an orthogonal perturbation for local diversification\n                    v = self.rng.normal(size=self.dim)\n                    # remove component along d\n                    v -= np.dot(v, d) * d\n                    vn = np.linalg.norm(v)\n                    if vn < 1e-12:\n                        v = self.rng.normal(size=self.dim)\n                        vn = np.linalg.norm(v) + 1e-12\n                    v = v / vn\n                    orth_step = sigma * 0.7 * (1.0 + 0.3 * self.rng.standard_normal())\n                    tx = px + orth_step * v\n                    tx = np.minimum(np.maximum(tx, lb), ub)\n                    if evals_used < budget:\n                        ftx, tx = callf(tx)\n                        if ftx < pf:\n                            pop[parent_idx] = {'x': tx, 'f': ftx, 'sigma': min(sigma * 1.06, max(span))}\n                        else:\n                            # adapt parent sigma on failure (contraction)\n                            pop[parent_idx]['sigma'] = max(1e-8, sigma * 0.96)\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.random() < 0.06 and evals_used < budget:\n                # Cauchy-like heavy-tailed vector\n                c = self.rng.standard_cauchy(size=self.dim)\n                # robust scale normalization to avoid numerical extremes\n                denom = (np.mean(np.abs(c)) + 1e-12)\n                c = c / denom\n                # scale by a robust population spread (median sigma)\n                median_sigma = float(np.median([p['sigma'] for p in pop]))\n                jump_scale = max(1.0, 8.0 * median_sigma)\n                delta = c * jump_scale\n                # cap delta to box span to avoid extreme values\n                max_allowed = 2.0 * span\n                delta = np.sign(delta) * np.minimum(np.abs(delta), max_allowed)\n                tx = parent['x'] + delta\n                tx = np.minimum(np.maximum(tx, lb), ub)\n                if evals_used < budget:\n                    ftx, tx = callf(tx)\n                    # If it's good, replace the worst in population to inject novelty\n                    if ftx < pop[worst_idx(pop)]['f']:\n                        pop[worst_idx(pop)] = {'x': tx, 'f': ftx, 'sigma': max(1e-8, median_sigma * 0.8)}\n                    else:\n                        # if it's moderately good (better than parent), accept as parent\n                        if ftx < parent['f']:\n                            pop[parent_idx] = {'x': tx, 'f': ftx, 'sigma': max(1e-8, parent['sigma'] * 0.9)}\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and self.rng.random() < 0.3 and evals_used < budget:\n                idxs = np.argsort([p['f'] for p in pop])[:2]\n                a = pop[int(idxs[0])]['x']\n                b = pop[int(idxs[1])]['x']\n                mix = 0.5 * (a + b)\n                noise = (self.rng.normal(size=self.dim) * 0.2 * np.median([p['sigma'] for p in pop]))\n                child = mix + noise\n                child = np.minimum(np.maximum(child, lb), ub)\n                if evals_used < budget:\n                    fchild, child = callf(child)\n                    if fchild < parent['f']:\n                        pop[parent_idx] = {'x': child, 'f': fchild, 'sigma': max(1e-8, parent['sigma'] * 1.02)}\n                    else:\n                        # try to inject into population by replacing the worst if it's better\n                        widx = worst_idx(pop)\n                        if fchild < pop[widx]['f']:\n                            pop[widx] = {'x': child, 'f': fchild, 'sigma': max(1e-8, 0.8 * np.median([p['sigma'] for p in pop]))}\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.random() < 0.02 and evals_used < budget:\n                widx = worst_idx(pop)\n                rx = self.rng.uniform(lb, ub)\n                if evals_used < budget:\n                    frx, rx = callf(rx)\n                    pop[widx] = {'x': rx, 'f': frx, 'sigma': sigma0 * (1.0 + 0.5 * self.rng.random())}\n\n            # keep population size stable (in case budget ended mid-block)\n            # and small housekeeping: clip sigmas to a reasonable range\n            for p in pop:\n                # keep sigma in a robust range relative to problem span\n                max_sigma = max(np.linalg.norm(span), 1e-6)\n                p['sigma'] = float(np.clip(p['sigma'], 1e-12, max_sigma))\n\n            # If population contains more elements than allowed (shouldn't happen), trim worst\n            while len(pop) > self.pop_size:\n                pop.pop(worst_idx(pop))\n\n            # If we somehow ran out of budget, break gracefully\n            if evals_used >= budget:\n                break\n\n        # finished budget\n        # ensure x_opt is set even if initial values didn't improve\n        if self.x_opt is None:\n            # pick best in population\n            best_p = min(pop, key=lambda p: p['f'])\n            self.x_opt = best_p['x']\n            self.f_opt = best_p['f']\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.502 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15435911343396946, 0.15134744441683246, 0.6298418076780421, 0.9285460969207163, 0.7040787782443708, 0.7850902977529571, 0.22925609287712578, 0.598128269655699, 0.6923588698923546, 0.14334625341207696]}, "task_prompt": ""}
{"id": "0774b547-f2f8-4e61-8cac-7bcb1fdb99d5", "fitness": 0.34970711786170694, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional searches with orthogonal refinements and occasional Lévy jumps for robust exploration/exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed\n    - init_sigma: optional initial scale (can be scalar or per-dimension vector fraction of search range)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None, init_sigma=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.init_sigma = init_sigma\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # resolve bounds if available, else default to [-5, 5]\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # make sure lb/ub are full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # sanity: ensure shapes correct\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # characteristic scale of search space per-dimension\n        range_vec = np.maximum(ub - lb, 1e-12)\n        mean_range = float(np.mean(range_vec))\n\n        # initial sigma default: a fraction of range if not provided\n        if self.init_sigma is None:\n            base_sigma_scalar = 0.1 * mean_range\n            base_sigma_vec = 0.1 * range_vec\n        else:\n            # allow scalar or vector fraction\n            if np.isscalar(self.init_sigma):\n                base_sigma_scalar = float(self.init_sigma) * mean_range\n                base_sigma_vec = float(self.init_sigma) * range_vec\n            else:\n                arr = np.asarray(self.init_sigma, dtype=float)\n                if arr.size == 1:\n                    base_sigma_scalar = float(arr.item()) * mean_range\n                    base_sigma_vec = float(arr.item()) * range_vec\n                else:\n                    base_sigma_vec = arr.reshape(self.dim)\n                    base_sigma_scalar = float(np.mean(base_sigma_vec))\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper wrapper for evaluating function while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # if zero budget return immediately\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population uniformly\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # per-individual sigma as scalar positive number (use scalar for step length)\n            pop_sigma.append(max(1e-12, base_sigma_scalar * (0.6 + 0.8 * np.random.rand())))\n            if remaining <= 0:\n                break\n\n        # If budget was too small to create any population, do random sampling with remaining budget\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop\n        # Keep going until budget is exhausted\n        while remaining > 0:\n            # small tournament selection (if population small, sample with replacement)\n            k = min(3, len(pop))\n            if len(pop) == k:\n                inds = np.arange(len(pop))\n            else:\n                inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = int(inds[int(np.argmin(values))])\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15 + 1e-16, mean_range)\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                # construct candidate from parent and fractional step (do not chain on previous x_try)\n                x_cand = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_cand, x_cand = callf(x_cand)\n                except RuntimeError:\n                    break\n                if f_cand < pop_f[parent_i]:\n                    pop[parent_i] = x_cand\n                    pop_f[parent_i] = f_cand\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # project r to orthogonal complement of d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                if remaining > 0:\n                    x_cand = np.clip(x_parent + 0.5 * sigma * r, lb, ub)\n                    try:\n                        f_cand, x_cand = callf(x_cand)\n                    except RuntimeError:\n                        break\n                    if f_cand < pop_f[parent_i]:\n                        pop[parent_i] = x_cand\n                        pop_f[parent_i] = f_cand\n                        pop_sigma[parent_i] = min(sigma * 1.1 + 1e-16, mean_range)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(self.dim)\n                # robust scale to avoid single huge outlier scaling everything\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * range_vec\n                x_cand = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_cand, x_cand = callf(x_cand)\n                except RuntimeError:\n                    break\n                # replace worst if better, else optionally keep as temporary\n                worst_i = int(np.argmax(pop_f))\n                if f_cand < pop_f[worst_i]:\n                    pop[worst_i] = x_cand\n                    pop_f[worst_i] = f_cand\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                # continue main loop\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * range_vec) * np.random.randn(self.dim)\n                x_cand = np.clip(mix + noise, lb, ub)\n                try:\n                    f_cand, x_cand = callf(x_cand)\n                except RuntimeError:\n                    break\n                # replace parent if improved\n                if f_cand < pop_f[parent_i]:\n                    pop[parent_i] = x_cand\n                    pop_f[parent_i] = f_cand\n                    # inject into population: replace worst if candidate is better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_cand < pop_f[worst_i]:\n                        pop[worst_i] = x_cand\n                        pop_f[worst_i] = f_cand\n                        pop_sigma[worst_i] = max(1e-12, base_sigma_scalar * 0.5)\n\n            # adapt parent sigma on failure\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = max(1e-12, base_sigma_scalar * (0.7 + 0.6 * np.random.rand()))\n\n            # if population size is less than desired and budget remains, try to add a random individual\n            if len(pop) < self.pop_size and remaining > 0 and np.random.rand() < 0.1:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(max(1e-12, base_sigma_scalar * (0.6 + 0.6 * np.random.rand())))\n\n            # keep population size stable and prune if necessary (remove worst)\n            if len(pop) > self.pop_size:\n                worst_i = int(np.argmax(pop_f))\n                pop.pop(worst_i)\n                pop_f.pop(worst_i)\n                pop_sigma.pop(worst_i)\n\n        # finished or budget exhausted\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.350 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13508864521948039, 0.1523722579745047, 0.5001787675064159, 0.9086111647455876, 0.15542377341409475, 0.7326051338326711, 0.18598357060620285, 0.42003423110313975, 0.178515478315592, 0.12825815589938094]}, "task_prompt": ""}
{"id": "f92c27aa-48a4-4166-b249-6e16e2b86a75", "fitness": 0.46027292071683, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining randomized directional local searches, orthogonal refinements, recombination and occasional heavy‑tailed Lévy-like jumps with adaptive step-sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n      - pop_size: override population size (otherwise auto-scaled with dim)\n      - seed: random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales moderately with dimension\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # Setup bounds (support scalar or array bounds). Many BBOB wrappers provide func.bounds.lb/ub\n        if hasattr(func, \"bounds\") and hasattr(func.bounds, \"lb\") and hasattr(func.bounds, \"ub\"):\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        else:\n            # fallback to canonical [-5, 5]\n            lb = -5.0\n            ub = 5.0\n            lb = np.full(self.dim, lb)\n            ub = np.full(self.dim, ub)\n\n        # ensure bounds are arrays of correct dimension\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        # internal state\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate function and track budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget exhausted already\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize sigma slightly randomized\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # if no population could be created (extremely small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # Main loop\n        # The algorithm runs until budget exhausted, using directional local moves,\n        # orthogonal perturbations, Lévy jumps, recombination, and occasional rejuvenation.\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = pop_f[inds]\n            parent_i = int(inds[int(np.argmin(values))])\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            if np.abs(alpha) < 1e-14:\n                alpha = sigma * (0.1 + 0.1 * np.random.rand())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            # Evaluate primary direction trial if budget allows\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue  # go to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d  # make orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.linalg.norm(step) + 1e-12\n                step = step / denom\n                # scale by a fraction of search range and by current sigma\n                scale_vec = 0.2 * (ub - lb)\n                # add randomness to scale to maintain heavy-tailed effect\n                coeff = max(0.05, np.abs(np.random.standard_cauchy()) * 0.1)\n                x_try = np.clip(x_parent + coeff * step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    # small chance to also inject into parent as diversification\n                    elif np.random.rand() < 0.02:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # small gaussian perturbation scaled by sigma\n                noise = np.random.randn(self.dim) * (0.05 * sigma)\n                x_mix = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_mix, x_mix = callf(x_mix)\n                    except RuntimeError:\n                        break\n                    # replace parent if improved, else possibly replace worst\n                    if f_mix < pop_f[parent_i]:\n                        pop[parent_i] = x_mix\n                        pop_f[parent_i] = f_mix\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_mix < pop_f[worst_i]:\n                            pop[worst_i] = x_mix\n                            pop_f[worst_i] = f_mix\n                            pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (if still not improved)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.06 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # if population size is less than desired (e.g., due to extremely small init), try to grow it\n            if len(pop) < self.pop_size and remaining > 0 and np.random.rand() < 0.2:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop = np.vstack([pop, x_new])\n                pop_f = np.append(pop_f, f_new)\n                pop_sigma = np.append(pop_sigma, base_sigma * (0.8 + 0.4 * np.random.rand()))\n\n            # keep arrays consistent shapes\n            if isinstance(pop, list):\n                pop = np.array(pop)\n            if isinstance(pop_f, list):\n                pop_f = np.array(pop_f)\n            if isinstance(pop_sigma, list):\n                pop_sigma = np.array(pop_sigma)\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.460 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10673193716610563, 0.15919534535335722, 0.8248002524564984, 0.9537060454323458, 0.5244099850099415, 0.9646722142375703, 0.2657033006488021, 0.37372334353502157, 0.28996083329155964, 0.13982595003709763]}, "task_prompt": ""}
{"id": "6708d32d-b916-49bb-a55b-495d7c4de36b", "fitness": 0.5944214467472758, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local probes, orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to explore continuous spaces efficiently.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: number of population points (optional; derived from dim if None)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Determine bounds: prefer func.bounds if available, else default [-5,5]\n        if hasattr(func, \"bounds\") and getattr(func.bounds, \"lb\", None) is not None and getattr(func.bounds, \"ub\", None) is not None:\n            lb_raw = np.array(func.bounds.lb, dtype=float)\n            ub_raw = np.array(func.bounds.ub, dtype=float)\n            # handle scalar bounds\n            if lb_raw.size == 1:\n                lb = np.full(self.dim, float(lb_raw.item()))\n            else:\n                lb = lb_raw.reshape(-1)[:self.dim].astype(float)\n                if lb.size < self.dim:\n                    lb = np.pad(lb, (0, self.dim - lb.size), 'edge')\n            if ub_raw.size == 1:\n                ub = np.full(self.dim, float(ub_raw.item()))\n            else:\n                ub = ub_raw.reshape(-1)[:self.dim].astype(float)\n                if ub.size < self.dim:\n                    ub = np.pad(ub, (0, self.dim - ub.size), 'edge')\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # robustly ensure lb < ub\n        span = ub - lb\n        span[span <= 0] = 1.0\n\n        remaining = int(self.budget)\n\n        # helper evaluator: clips and tracks remaining budget and best\n        def callf(x):\n            nonlocal remaining\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            if remaining <= 0:\n                return np.inf, x\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # if no budget, return defaults\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # base sigma relative to problem scale\n        base_sigma = 0.2 * np.median(span)\n\n        # Initialize population (uniform samples)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # initial sigma: base_sigma * random factor\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        pop = np.vstack(pop)\n        pop_f = np.array(pop_f, dtype=float)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n\n        # Main loop\n        while remaining > 0:\n            # pick a parent via small tournament\n            k = min(3, pop.shape[0])\n            inds = self.rng.choice(pop.shape[0], k, replace=False)\n            # choose index of best among sampled inds\n            parent_i = inds[int(np.argmin(pop_f[inds]))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, 10.0 * base_sigma)\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                step_alpha = alpha * frac\n                x_try = np.clip(x_parent + step_alpha * d, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try_tmp, x_try_tmp = callf(x_try)\n                if f_try_tmp < pop_f[parent_i]:\n                    pop[parent_i] = x_try_tmp\n                    pop_f[parent_i] = f_try_tmp\n                    # modest sigma adaptation on local success\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.06, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification: remove component along d\n            r = self.rng.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                ortho_step = 0.8 * sigma\n                x_try = np.clip(x_parent + ortho_step * r, lb, ub)\n                if remaining > 0:\n                    f_try_tmp, x_try_tmp = callf(x_try)\n                    if f_try_tmp < pop_f[parent_i]:\n                        pop[parent_i] = x_try_tmp\n                        pop_f[parent_i] = f_try_tmp\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.05, 1e-12)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # sample Cauchy-like heavy-tailed step per-dimension\n                step = self.rng.standard_cauchy(size=self.dim)\n                # robust scaling to avoid insane extremes\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = (step / denom) * (0.2 * span)\n                x_try = np.clip(x_parent + step, lb, ub)\n                if remaining > 0:\n                    f_try_tmp, x_try_tmp = callf(x_try)\n                    # if it's good replace the worst in population\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try_tmp < pop_f[worst_i]:\n                        pop[worst_i] = x_try_tmp\n                        pop_f[worst_i] = f_try_tmp\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * self.rng.rand())\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if pop.shape[0] >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = self.rng.rand()\n                noise = (0.015 * span) * self.rng.randn(self.dim)\n                x_recomb = np.clip(beta * pop[a] + (1 - beta) * pop[b] + noise, lb, ub)\n                if remaining > 0:\n                    f_try_tmp, x_recomb_tmp = callf(x_recomb)\n                    # replace parent if improved, else possibly replace worst\n                    if f_try_tmp < pop_f[parent_i]:\n                        pop[parent_i] = x_recomb_tmp\n                        pop_f[parent_i] = f_try_tmp\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try_tmp < pop_f[worst_i]:\n                            pop[worst_i] = x_recomb_tmp\n                            pop_f[worst_i] = f_try_tmp\n                            pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * self.rng.rand())\n\n            # adapt parent sigma on failure (shrink to focus local search)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n            # If budget low, attempt focused local intensification on best\n            if remaining <= max(3, int(0.03 * self.budget)):\n                # small directed perturbations around best\n                best_i = int(np.argmin(pop_f))\n                for _ in range(min(remaining, 5)):\n                    small_step = (0.04 * span) * self.rng.randn(self.dim)\n                    x_try = np.clip(pop[best_i] + small_step, lb, ub)\n                    f_try_tmp, x_try_tmp = callf(x_try)\n                    if f_try_tmp < pop_f[best_i]:\n                        pop[best_i] = x_try_tmp\n                        pop_f[best_i] = f_try_tmp\n                        pop_sigma[best_i] = max(pop_sigma[best_i] * 1.03, 1e-12)\n                    else:\n                        pop_sigma[best_i] = max(pop_sigma[best_i] * 0.97, 1e-12)\n                    if remaining <= 0:\n                        break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.594 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16765834056415174, 0.23604967680107114, 0.8538900721916143, 0.9730591891941184, 0.8806087753205933, 0.9080953363426401, 0.29090382166310225, 0.5926991797315708, 0.8964686265325429, 0.1447814491313537]}, "task_prompt": ""}
{"id": "59ffd824-7279-4b8f-a319-4f4b6b91b88c", "fitness": 0.3977244465868357, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a small adaptive population using randomized directional local searches, orthogonal refinements and occasional Lévy-like jumps to balance intensive local refinement and global escapes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-chosen if None)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # scale modestly with dimension but keep small population for expensive evaluations\n            self.pop_size = max(4, min(40, int(6 + 1.5 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds and make them full-dim arrays\n        lb_a = np.array(func.bounds.lb, dtype=float)\n        ub_a = np.array(func.bounds.ub, dtype=float)\n        if lb_a.size == 1:\n            lb = np.full(self.dim, lb_a.item(), dtype=float)\n        else:\n            if lb_a.size != self.dim:\n                # attempt broadcasting or reshape\n                lb = np.resize(lb_a, self.dim).astype(float)\n            else:\n                lb = lb_a.astype(float)\n\n        if ub_a.size == 1:\n            ub = np.full(self.dim, ub_a.item(), dtype=float)\n        else:\n            if ub_a.size != self.dim:\n                ub = np.resize(ub_a, self.dim).astype(float)\n            else:\n                ub = ub_a.astype(float)\n\n        # helper wrapper to evaluate while tracking budget and best-so-far\n        remaining = int(self.budget)\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).flatten()\n            if x.size == 1 and self.dim > 1:\n                x = np.full(self.dim, x.item(), dtype=float)\n            if x.size != self.dim:\n                raise ValueError(\"Candidate has wrong dimension\")\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = float(func(x))\n            if f < self.f_opt or self.x_opt is None:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # if budget extremely small, fallback to pure random sampling\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base scalar sigma related to search space size\n        base_sigma_scalar = max(1e-12, 0.25 * np.mean(ub - lb))\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            # give each individual a scalar adaptive step-size\n            pop_sigma.append(base_sigma_scalar * (0.6 + 0.8 * np.random.rand()))\n            pop_f.append(f0)\n            if remaining <= 0:\n                break\n\n        # If population could not be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        pop = [np.asarray(x, dtype=float).copy() for x in pop]\n        pop_f = [float(v) for v in pop_f]\n        pop_sigma = [float(s) for s in pop_sigma]\n\n        # parameters\n        p_levy = 0.08\n        p_recombine = 0.25\n        rejuv_prob = 0.02\n        max_sigma_cap = np.mean(ub - lb)  # avoid increasing sigma beyond space size\n\n        # main loop: iterate until budget exhausted\n        while remaining > 0:\n            # small tournament to pick a parent\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            vals = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(vals))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # primary directional trial\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d /= nd\n\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            # propose\n            if remaining <= 0:\n                break\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max_sigma_cap, sigma * 1.2)\n                continue  # success, next iteration\n\n            # local backtracking / small-step refinement along direction (a few tried fractions)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + frac * alpha * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.9)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make orthonormal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(max_sigma_cap, sigma * 1.1)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins\n            if remaining > 0 and np.random.rand() < p_levy:\n                # heavy-tailed step using standard Cauchy, normalized to robust scale\n                step = np.random.standard_cauchy(size=self.dim)\n                denom = np.median(np.abs(step)) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)  # direction-wise scales\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                # if it's good replace the worst in population, else keep it as trial only\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.5)\n                else:\n                    # maybe replace the worst if it's promising\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if remaining > 0 and np.random.rand() < p_recombine and len(pop) >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace parent if improved, else maybe replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 1.05)\n                    continue\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                        continue\n\n            # adapt parent sigma on failure (slightly reduce)\n            pop_sigma[parent_i] = max(1e-12, sigma * 0.85)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (np.random.rand() < rejuv_prob):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma_scalar * (0.5 + np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.398 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11216297374332296, 0.17418859780024465, 0.4879163047595455, 0.7078080954544106, 0.23217310492695387, 0.9324595702184482, 0.18818547359942606, 0.19239206700212685, 0.7956070013789389, 0.154351276984939]}, "task_prompt": ""}
{"id": "26434e09-8cc5-4740-bbbd-41a77de70870", "fitness": 0.536722923005111, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points combining randomized normalized directional local searches, orthogonal refinements, population recombination and occasional heavy-tailed Lévy/Cauchy jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional fixed population size\n    - seed: optional RNG seed\n    Behavior:\n      - maintains a small population of candidate points, each with its own adaptive sigma\n      - performs randomized directional trials (normalized directions)\n      - refines by local backtracking along directions and orthogonal tries\n      - occasionally performs heavy-tailed (Cauchy-like) jumps to escape basins\n      - mixes two best solutions for exploitation\n      - adapts sigmas per individual and rejuvenates worst individuals occasionally\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # default modest pop size that grows slowly with dim\n        if pop_size is None:\n            self.pop_size = int(max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))) if hasattr(self, 'dim') else None\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # use local RNG to avoid changing global state\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Get bounds (the BBOB wrapper provides func.bounds.lb/ub)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback to [-5,5] if not provided\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure proper shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # consistent internal RNG\n        rng = self.rng\n\n        # reset best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best found\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x_clipped = np.minimum(np.maximum(x, lb), ub)\n            # Do evaluation\n            f = float(func(x_clipped))\n            remaining -= 1\n            # update global best\n            nonlocal_best = False\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x_clipped.copy()\n                nonlocal_best = True\n            return f, x_clipped, nonlocal_best\n\n        # If budget is extremely small, fallback to random sampling\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # population initialization\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        # base scale relative to search box\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n\n        n_init = min(self.pop_size if self.pop_size is not None else 8, remaining)\n        for i in range(n_init):\n            x0 = rng.uniform(lb, ub)\n            f0, x0c, _ = callf(x0)\n            pop.append(x0c)\n            # init sigma around base_sigma with some diversity\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * rng.rand()))\n            pop_f.append(f0)\n            if remaining <= 0:\n                break\n\n        # If no population due to tiny budget, do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = rng.uniform(lb, ub)\n                f, xc, _ = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # ensure pop arrays are numpy arrays where convenient\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop\n        while remaining > 0:\n            # small tournament selection of a parent (balance exploration/exploitation)\n            k = min(3, len(pop))\n            inds = rng.choice(len(pop), k, replace=False)\n            # pick the best among sampled\n            best_idx_in_inds = int(np.argmin([pop_f[i] for i in inds]))\n            parent_i = inds[best_idx_in_inds]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a normalized random direction\n            d = rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # degenerate direction -> small random perturbation\n                d = rng.randn(self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    continue\n            d = d / nd\n\n            # stochasticized step-length: use parent's sigma scaled by random factor\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * rng.randn()))\n            # cap alpha to box size\n            alpha = np.clip(alpha, 1e-12, np.mean(ub - lb) * 1.2)\n\n            # primary directional trial\n            x_try = x_parent + alpha * d\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n            try:\n                f_try, x_try, improved = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # accept and slightly increase sigma (success)\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                # small local backtrack / refinement along the same direction\n                # try a few fractional steps (smaller steps) for fine tuning\n                for frac in (0.5, 0.25, 0.125):\n                    if remaining <= 0:\n                        break\n                    xt = x_parent + alpha * frac * d\n                    xt = np.minimum(np.maximum(xt, lb), ub)\n                    try:\n                        ft, xt, _ = callf(xt)\n                    except RuntimeError:\n                        break\n                    if ft < pop_f[parent_i]:\n                        pop[parent_i] = xt\n                        pop_f[parent_i] = ft\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                        break  # accepted refinement\n                continue  # move to next iteration\n\n            # if primary failed, try small backtracking attempts from parent\n            improved_any = False\n            for frac in (0.6, 0.4, 0.2):\n                if remaining <= 0:\n                    break\n                xt = x_parent + alpha * frac * d\n                xt = np.minimum(np.maximum(xt, lb), ub)\n                try:\n                    ft, xt, _ = callf(xt)\n                except RuntimeError:\n                    break\n                if ft < pop_f[parent_i]:\n                    pop[parent_i] = xt\n                    pop_f[parent_i] = ft\n                    pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n                    improved_any = True\n                    break\n            if improved_any:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = rng.randn(self.dim)\n            # remove component along d -> make orthogonal\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                step = 0.6 * sigma * r\n                x_try = np.minimum(np.maximum(x_parent + step, lb), ub)\n                try:\n                    f_try, x_try, _ = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.08, np.mean(ub - lb))\n                    continue\n\n            # Occasional heavy-tailed Lévy/Cauchy-like jump to escape local basins\n            if rng.rand() < 0.08:\n                # sample Cauchy per-dimension, robust-normalize to keep direction but heavy tails\n                step = rng.standard_cauchy(size=self.dim)\n                denom = np.median(np.abs(step)) + 1e-12\n                step = step / denom\n                # scale vector relative to box size, but randomize magnitude\n                scale_vec = (0.1 + 0.5 * rng.rand()) * (ub - lb)\n                x_try = x_parent + step * scale_vec\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                try:\n                    f_try, x_try, _ = callf(x_try)\n                except RuntimeError:\n                    break\n                # if it's good, replace the worst in population; else maybe keep as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.4 + 0.6 * rng.rand())\n                else:\n                    # sometimes keep as temporary candidate by replacing parent if similar\n                    if f_try < pop_f[parent_i] + 1e-12:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a_i, b_i = int(best2[0]), int(best2[1])\n                xa = pop[a_i]\n                xb = pop[b_i]\n                beta = rng.rand()\n                mix = beta * xa + (1.0 - beta) * xb\n                noise = rng.randn(self.dim) * (0.05 * np.mean(ub - lb) * rng.rand())\n                x_try = np.minimum(np.maximum(mix + noise, lb), ub)\n                try:\n                    f_try, x_try, _ = callf(x_try)\n                except RuntimeError:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.5 + 0.4 * rng.rand())\n                    continue\n                else:\n                    # maybe replace parent if better\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n\n            # adapt sigma on failure: slowly reduce sigma for the parent to focus local search\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * (0.98 - 0.02 * rng.rand()), 1e-12)\n            # occasionally (small prob) increase sigma to encourage diversification\n            if rng.rand() < 0.01:\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * (1.2 + 0.6 * rng.rand()), np.mean(ub - lb))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                xr = rng.uniform(lb, ub)\n                try:\n                    fr, xr, _ = callf(xr)\n                except RuntimeError:\n                    break\n                pop[worst_i] = xr\n                pop_f[worst_i] = fr\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * rng.rand())\n\n            # keep population size bounded (no growth here) and ensure lists consistent\n            # occasionally prune extremely bad individuals if budget is tiny\n            if len(pop) > 0 and remaining <= max(0, int(self.budget * 0.01)):\n                # small pruning: keep best half if near end\n                if len(pop) > 4:\n                    order = np.argsort(pop_f)\n                    keep = order[:max(4, int(len(pop) // 2))]\n                    pop = [pop[i] for i in keep]\n                    pop_f = [pop_f[i] for i in keep]\n                    pop_sigma = [pop_sigma[i] for i in keep]\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.537 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11606237940294206, 0.1563319481797849, 0.7801412353270937, 0.9194022146150177, 0.8385670350942758, 0.8904567316831818, 0.22784579540772543, 0.496166859028661, 0.8270479184471348, 0.11520711286529262]}, "task_prompt": ""}
{"id": "039623c0-461e-4c25-9023-b46184192e5d", "fitness": 0.49984557890676723, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based search combining adaptive directional local searches, orthogonal refinements, occasional heavy-tailed Lévy/Cauchy jumps and population rejuvenation to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # bounds handling: accept scalars or arrays and ensure shape (dim,)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        else:\n            lb = lb.astype(float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        else:\n            ub = ub.astype(float)\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        # state\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n        remaining = int(self.budget)\n\n        # safe evaluation wrapper\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget extremely small, fallback to plain random search\n        if remaining < 2 or self.budget < 2:\n            # do pure random until budget exhausted\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # initial population: if budget smaller than desired pop_size, random-search fallback\n        if remaining < self.pop_size:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # initial population creation\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n        for i in range(self.pop_size):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))  # diversity in sigma\n\n        pop = np.array(pop)            # shape (pop_size, dim)\n        pop_f = np.array(pop_f)        # shape (pop_size,)\n        pop_sigma = np.array(pop_sigma)\n\n        # operation parameters\n        levy_prob = 0.08\n        recomb_prob = 0.07\n        rejuvenate_period = max(10, int(0.2 * self.pop_size))  # every so often\n        iters_since_rejuv = 0\n\n        # main loop\n        while remaining > 0:\n            # pick parent via small tournament\n            k = min(3, pop.shape[0])\n            candidates = np.random.choice(pop.shape[0], size=k, replace=False)\n            parent_i = candidates[np.argmin(pop_f[candidates])]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd > 1e-16:\n                d = d / nd\n            else:\n                # fallback small random direction\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n\n            # primary directional trial with stochasticized step-length\n            # step length draws from normal scaled by sigma and a uniform multiplier\n            alpha = sigma * np.random.randn() * (0.5 + np.random.rand())\n            x_try = np.minimum(np.maximum(x_parent + alpha * d, lb), ub)\n            if remaining > 0:\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # accept if improved relative to parent\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.25, np.mean(ub - lb))\n                    iters_since_rejuv += 1\n                    continue  # move to next main iteration\n\n            # local backtracking / small-step refinement along same direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.minimum(np.maximum(x_parent + alpha * frac * d, lb), ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                    improved = True\n                    break\n            if improved:\n                iters_since_rejuv += 1\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining <= 0:\n                break\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                step_len = 0.8 * sigma\n                x_try = np.minimum(np.maximum(x_parent + step_len * r, lb), ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                        iters_since_rejuv += 1\n                        continue\n\n            # occasional Lévy-like heavy-tailed jump to escape local basins\n            if remaining > 0 and np.random.rand() < levy_prob:\n                # draw Cauchy-like heavy-tailed steps and normalize to robust scale\n                step = np.random.standard_cauchy(size=self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                normed = step / denom\n                scale_vec = 0.25 * (ub - lb)\n                x_try = np.minimum(np.maximum(x_parent + normed * scale_vec, lb), ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n                        iters_since_rejuv += 1\n                        continue\n                    else:\n                        # keep as candidate replacement with small chance\n                        if np.random.rand() < 0.15:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * (0.5 + 0.6 * np.random.rand())\n                            iters_since_rejuv += 1\n                            continue\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0 and np.random.rand() < recomb_prob:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                noise = (0.02 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.minimum(np.maximum(beta * pop[a] + (1 - beta) * pop[b] + noise, lb), ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n                        iters_since_rejuv += 1\n                        continue\n\n            # if we reached here, the parent didn't improve: reduce its sigma slightly\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            iters_since_rejuv += 1\n            if iters_since_rejuv >= rejuvenate_period and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                iters_since_rejuv = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.500 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1330926939587327, 0.16321255069191587, 0.8579800074301622, 0.947497325018209, 0.34932267911186465, 0.9303199869859868, 0.2642846653749462, 0.4418404535760705, 0.7335890506206217, 0.17731637629916297]}, "task_prompt": ""}
{"id": "d8e83615-5b7f-4e01-abc1-690062dbc983", "fitness": 0.48491101599547004, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points using randomized directional local searches, orthogonal refinements and occasional Lévy jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to small function of dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimensionality\n            self.pop_size = max(4, min(12, int(4 + 1.5 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = max(1, int(pop_size))\n        self.seed = seed\n        if seed is not None:\n            self.rng = np.random.RandomState(seed)\n        else:\n            self.rng = np.random\n\n    def __call__(self, func):\n        # Setup bounds robustly (accept scalar or array)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback: assume typical BBOB bounds [-5,5]\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n\n        # Ensure full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # domain scale and basic sigmas\n        domain_scale = np.maximum(ub - lb, 1e-9)\n        base_sigma = 0.08 * np.mean(domain_scale)  # base step scale\n        max_sigma = 0.5 * np.mean(domain_scale)\n\n        # internal state\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        remaining = int(self.budget)  # remaining evaluations\n\n        # safe evaluate that tracks budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                # budget exhausted; do not call func\n                return np.inf, x.copy()\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget is extremely small, fallback to simple random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n        if remaining < max(10, self.dim):\n            # simple random - spread evaluations across domain\n            for _ in range(remaining):\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population (try to allocate at least 2 individuals)\n        pop_size = min(self.pop_size, max(2, remaining // 6))\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # ensure at least two initial individuals\n        n_init = max(2, pop_size)\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # diverse initial sigmas around base_sigma\n            pop_sigma.append(max(1e-8, base_sigma * (0.8 + 0.8 * self.rng.rand())))\n        # If we couldn't create a population (very small budget), do random search with leftover budget\n        if len(pop) == 0:\n            for _ in range(remaining):\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # helper: choose parent by small tournament to balance exploration/exploitation\n        def choose_parent():\n            if len(pop) == 1:\n                return 0\n            a, b = self.rng.randint(0, len(pop), size=2)\n            return a if pop_f[a] <= pop_f[b] else b\n\n        # main loop\n        while remaining > 0:\n            parent_i = choose_parent()\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-8, 1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < f_parent:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, max_sigma)\n                # small local exploitation: backtrack with decreasing fractions\n                for frac in (0.5, 0.3, 0.15):\n                    if remaining <= 0:\n                        break\n                    x_back = np.clip(x_parent + alpha * frac * d, lb, ub)\n                    f_back, x_back = callf(x_back)\n                    if f_back < pop_f[parent_i]:\n                        pop[parent_i] = x_back\n                        pop_f[parent_i] = f_back\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, max_sigma)\n                continue\n            else:\n                # failure: try refined small steps along direction (local backtracking)\n                improved = False\n                for frac in (0.4, 0.2, 0.1):\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + (alpha * 0.6) * frac * d, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, max_sigma)\n                        improved = True\n                        break\n                if improved:\n                    continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = self.rng.randn(self.dim)\n                # make orthogonal to d\n                r -= np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr < 1e-9:\n                    r = self.rng.randn(self.dim)\n                    r -= np.dot(r, d) * d\n                    nr = np.linalg.norm(r) + 1e-12\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, max_sigma)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = self.rng.standard_cauchy(size=self.dim)\n                # robust denom to avoid extreme scale blow-ups\n                denom = np.median(np.abs(step)) + 1e-9\n                step = step / denom\n                # scale vector that respects domain and individual's sigma\n                scale = base_sigma * (2.0 + np.abs(self.rng.standard_cauchy()) * 0.6)\n                step = step * scale\n                # cap to some fraction of domain\n                step = np.clip(step, -1.5 * domain_scale, 1.5 * domain_scale)\n                x_try = np.clip(x_parent + step, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    if f_try < max(pop_f):\n                        worst_i = int(np.argmax(pop_f))\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-9, base_sigma * 0.5)\n                    # continue loop after jump attempt\n                    continue\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                mix = 0.5 * (pop[a] + pop[b])\n                noise = 0.05 * self.rng.randn(self.dim) * np.mean(domain_scale)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < f_parent:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-9, pop_sigma[parent_i] * 0.9)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-9, base_sigma * 0.5)\n\n            # adapt parent sigma on failure (shrink a bit)\n            pop_sigma[parent_i] = max(1e-9, pop_sigma[parent_i] * (0.92 + 0.08 * self.rng.rand()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < 0.03 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = max(1e-9, base_sigma * 0.6)\n\n            # keep population size consistent (if budget early ended causing less inds just continue)\n            # also ensure arrays remain consistent types (already lists)\n\n        # finished budget\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.485 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13022722591544134, 0.2396265305737223, 0.7460368231885777, 0.9396539310853297, 0.3199268805472947, 0.8696612768700331, 0.2987233042121329, 0.4387835216854682, 0.7126389538318455, 0.15383171204485513]}, "task_prompt": ""}
{"id": "ff4022b0-985f-43bc-beac-6c9de4dd796a", "fitness": 0.557836512033205, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — maintains a modest population of adaptive-step points that perform randomized directional local searches, orthogonal refinements and occasional heavy-tailed (Lévy/Cauchy-like) jumps to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_size: number of population members (auto-scaled if None)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # sensible default: scale with dim but keep modest and leave budget for iterations\n            self.pop_size = max(4, min(6 + 3 * self.dim, max(4, self.budget // 30)))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        # prepare bounds (BBOB functions provide func.bounds.lb / ub)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        # expand scalar bounds\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            # ensure x is numpy array and clipped to bounds\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            if self.evals >= self.budget:\n                # budget exhausted: return current known best (no evaluation)\n                return self.f_opt, x.copy()\n            f = func(x)\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget is zero, return defaults\n        if self.budget <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population (uniform random sampling)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base sigma roughly proportional to domain size\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n\n        # limit initial population to not exhaust budget\n        init_pop = min(self.pop_size, max(1, self.budget // 10))\n        for _ in range(init_pop):\n            if self.evals >= self.budget:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # give each member a slightly different initial sigma\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.rand()))\n\n        # If no population could be created (very small budget), do pure random search for remaining budget\n        if len(pop) == 0:\n            while self.evals < self.budget:\n                x = self.rng.uniform(lb, ub)\n                f, x = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Ensure sizes are consistent\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n        pop_size = len(pop)\n\n        # Main loop\n        # We'll iterate until budget is used up\n        while self.evals < self.budget:\n            remaining = self.budget - self.evals\n            # pick a parent via small tournament\n            tour_k = min(3, pop_size)\n            inds = self.rng.choice(pop_size, tour_k, replace=False)\n            parent_i = inds[np.argmin([pop_f[i] for i in inds])]\n            parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # random search direction (normalized)\n            r = self.rng.randn(self.dim)\n            nr = np.linalg.norm(r)\n            if nr == 0:\n                r = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                r = r / nr\n\n            # primary directional trial with stochasticized step-length (log-normal-like)\n            step_scale = sigma * np.exp(0.3 * (self.rng.randn()))  # multiplicative randomness\n            x_try = parent + (step_scale * r)\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n            f_try, x_try = callf(x_try)\n\n            # record improvement\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(max(sigma * 1.10, 1e-12), max(base_sigma * 10, sigma * 10))\n                # local backtracking / small-step refinement along same direction\n                for _ in range(3):\n                    if self.evals >= self.budget:\n                        break\n                    # smaller steps along same direction to polish\n                    step_scale *= 0.5\n                    x_try2 = pop[parent_i] + step_scale * r\n                    x_try2 = np.minimum(np.maximum(x_try2, lb), ub)\n                    f_try2, x_try2 = callf(x_try2)\n                    if f_try2 < pop_f[parent_i]:\n                        pop[parent_i] = x_try2\n                        pop_f[parent_i] = f_try2\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.06, base_sigma * 10)\n                    else:\n                        # no further improvement, stop backtracking\n                        break\n            else:\n                # failure: slightly reduce sigma\n                pop_sigma[parent_i] = max(sigma * 0.92, 1e-12)\n\n            # try an orthogonal perturbation for local diversification\n            if self.evals < self.budget:\n                # create some orthogonal vector\n                v = self.rng.randn(self.dim)\n                # remove projection on r\n                proj = np.dot(v, r)\n                v = v - proj * r\n                nv = np.linalg.norm(v)\n                if nv > 0:\n                    v = v / nv\n                    ortho_scale = pop_sigma[parent_i] * (0.6 + 0.8 * self.rng.rand())\n                    x_try = pop[parent_i] + ortho_scale * v\n                    x_try = np.minimum(np.maximum(x_try, lb), ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = pop_sigma[parent_i] * 1.07\n\n            # occasional Lévy-like (Cauchy heavy-tailed) jump to escape local basins\n            if (self.rng.rand() < 0.08) and (self.evals < self.budget):\n                # generate Cauchy-like vector and scale robustly\n                c = np.tan(np.pi * (self.rng.rand(self.dim) - 0.5))  # standard Cauchy marginals\n                # normalize to avoid extremely huge steps but keep heavy tails\n                c_med = np.median(np.abs(c)) + 1e-12\n                c = c / c_med\n                # scale by a factor that can be several times the base sigma\n                jump_scale = base_sigma * (1.5 + 6.0 * self.rng.rand())\n                x_jump = pop[parent_i] + jump_scale * c\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < pop_f[parent_i]:\n                    pop[parent_i] = x_jump\n                    pop_f[parent_i] = f_jump\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i], jump_scale * 0.6)\n                else:\n                    # maybe inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(1e-12, jump_scale * 0.4)\n\n            # recombination exploitation: mix two best and small noise\n            if (self.evals < self.budget) and (pop_size >= 2):\n                # pick two best\n                best_idx = np.argsort(pop_f)[:2]\n                a, b = int(best_idx[0]), int(best_idx[1])\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # small gaussian perturbation scaled by average sigma\n                avg_sigma = max(1e-12, 0.5 * (pop_sigma[a] + pop_sigma[b]))\n                mix += avg_sigma * 0.5 * self.rng.randn(self.dim)\n                mix = np.minimum(np.maximum(mix, lb), ub)\n                f_mix, x_mix = callf(mix)\n                # replace a parent or the worst if beneficial\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = max(avg_sigma * 0.6, 1e-12)\n\n            # adapt parent sigma on failure a bit more aggressively if many failures\n            # small random walk on sigma to maintain diversity\n            for i in range(pop_size):\n                if self.rng.rand() < 0.03:\n                    pop_sigma[i] = max(1e-12, pop_sigma[i] * np.exp(0.2 * (self.rng.randn())))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if (self.rng.rand() < 0.02) and (self.evals < self.budget):\n                worst_i = int(np.argmax(pop_f))\n                x_rand = self.rng.uniform(lb, ub)\n                f_rand, x_rand = callf(x_rand)\n                pop[worst_i] = x_rand\n                pop_f[worst_i] = f_rand\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * self.rng.rand())\n\n            # safety: if some population member has not improved for long time, slightly nudge it\n            # (we don't keep long-term counters to keep code simple; occasional nudges above suffice)\n\n            # end of one iteration; loop will continue until budget exhausted\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.558 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1747172865052633, 0.16792573615972506, 0.9498092106497574, 0.9781250561360065, 0.27512345500971913, 0.9630898683758594, 0.30770023392803736, 0.6899600877401133, 0.8752911590266885, 0.19662302680087906]}, "task_prompt": ""}
{"id": "c963b9ef-c906-414d-9237-5933cc0499f2", "fitness": 0.5084284487236406, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based search combining randomized directional local searches, orthogonal refinements, adaptive step-sizes and occasional heavy-tailed Lévy-like jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size; if None it is chosen relative to dim and budget\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # Heuristic default population sizing: modest, scales with dim but also limited by budget\n        if pop_size is None:\n            self.pop_size = max(2, min(20 + self.dim, max(2, self.budget // 10)))\n        else:\n            self.pop_size = int(max(1, pop_size))\n        # bookkeeping\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds as arrays of length dim\n        if hasattr(func, \"bounds\"):\n            lb = np.asarray(func.bounds.lb)\n            ub = np.asarray(func.bounds.ub)\n        else:\n            # fallback to global constants if not provided\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # normalize lb/ub into full arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        else:\n            lb = lb.reshape(-1)[:self.dim]\n            if lb.size < self.dim:\n                lb = np.full(self.dim, float(lb[0]))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        else:\n            ub = ub.reshape(-1)[:self.dim]\n            if ub.size < self.dim:\n                ub = np.full(self.dim, float(ub[0]))\n\n        # clamp/pop_size to available budget (we need at least 1 evaluation reserved)\n        self.pop_size = min(self.pop_size, max(1, self.budget))\n        remaining = self.budget\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate and track budget/best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"Budget exhausted\")\n            # ensure numpy array shape (dim,)\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            self.eval_count += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is tiny, perform pure random sampling\n        if self.budget <= 5 or self.pop_size <= 1:\n            # Simple random search until budget exhausted\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population uniformly\n        pop_x = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        pop_f = np.full(self.pop_size, np.inf)\n        # base sigma relative to search range\n        range_mean = float(np.mean(ub - lb))\n        base_sigma = max(1e-12, range_mean * 0.1)\n        pop_sigma = np.full(self.pop_size, base_sigma)\n\n        # Evaluate initial population (stop early if budget runs out)\n        for i in range(self.pop_size):\n            if remaining <= 0:\n                break\n            f, x = callf(pop_x[i])\n            pop_x[i] = x\n            pop_f[i] = f\n\n        # If after initialization we have no evaluated individuals (budget was 0), return\n        if self.eval_count == 0:\n            return self.f_opt, self.x_opt\n\n        # Main search loop\n        while remaining > 0:\n            # small tournament selection (k=2 or 3) to pick a parent\n            k = 2 if self.pop_size < 4 else np.random.choice([2, 3], p=[0.6, 0.4])\n            contestants = np.random.randint(0, self.pop_size, size=k)\n            parent_i = contestants[np.argmin(pop_f[contestants])]\n            parent_x = pop_x[parent_i].copy()\n            parent_f = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = parent_x + alpha * d\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n\n            # Evaluate primary trial if budget allows\n            success = False\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < parent_f:\n                    # accept and slightly increase sigma\n                    pop_x[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                    success = True\n                else:\n                    # local backtracking: try a few halving steps along direction\n                    small_alpha = alpha\n                    for t in range(3):\n                        small_alpha *= 0.5\n                        if remaining <= 0:\n                            break\n                        x2 = parent_x + small_alpha * d\n                        x2 = np.minimum(np.maximum(x2, lb), ub)\n                        f2, x2 = callf(x2)\n                        if f2 < parent_f:\n                            pop_x[parent_i] = x2\n                            pop_f[parent_i] = f2\n                            pop_sigma[parent_i] = min(sigma * 1.08, np.mean(ub - lb))\n                            success = True\n                            break\n\n            # If directional attempts failed, try orthogonal diversification\n            if not success and remaining > 0:\n                # create an orthogonal direction by projecting random vector off d\n                v = np.random.randn(self.dim)\n                v = v - (v.dot(d)) * d\n                nv = np.linalg.norm(v)\n                if nv == 0:\n                    v = np.random.randn(self.dim)\n                    nv = np.linalg.norm(v)\n                v = v / nv\n                beta = sigma * (0.4 + 0.6 * np.random.rand())\n                for t in range(3):\n                    if remaining <= 0:\n                        break\n                    x_o = parent_x + (beta * (0.5 ** t)) * v\n                    x_o = np.minimum(np.maximum(x_o, lb), ub)\n                    f_o, x_o = callf(x_o)\n                    if f_o < parent_f:\n                        pop_x[parent_i] = x_o\n                        pop_f[parent_i] = f_o\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        success = True\n                        break\n\n            # Occasional Lévy-like heavy-tailed jump to escape basins\n            if (not success) and remaining > 0 and (np.random.rand() < 0.06):\n                # sample Cauchy-like heavy-tail vector\n                v = np.random.standard_cauchy(self.dim)\n                # robust normalizer to avoid infinite / extremely huge steps\n                v = v / (np.std(v) + 1e-12)\n                # scale by the search range and a random amplification factor\n                scale = range_mean * (0.5 + 5.0 * np.random.rand())\n                step = v * scale\n                x_jump = parent_x + step\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                if remaining > 0:\n                    f_jump, x_jump = callf(x_jump)\n                    # replace worst if jump is good, else maybe keep as candidate in population\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop_x[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                        success = True\n                    else:\n                        # occasionally accept the jump into the population to increase diversity\n                        if np.random.rand() < 0.02:\n                            replace_i = worst_i\n                            pop_x[replace_i] = x_jump\n                            pop_f[replace_i] = f_jump\n                            pop_sigma[replace_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # Recombination exploitation step: mix two best with small noise\n            if remaining > 0 and np.random.rand() < 0.25:\n                best2 = np.argsort(pop_f)[:2]\n                mix = 0.5 * (pop_x[best2[0]] + pop_x[best2[1]])\n                noise = np.random.randn(self.dim) * (0.05 * range_mean * np.random.rand())\n                child = mix + noise\n                child = np.minimum(np.maximum(child, lb), ub)\n                if remaining > 0:\n                    f_child, child = callf(child)\n                    # try to replace the worst if improved\n                    worst_i = int(np.argmax(pop_f))\n                    if f_child < pop_f[worst_i]:\n                        pop_x[worst_i] = child\n                        pop_f[worst_i] = f_child\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                    # also possibly replace parent if improved\n                    if f_child < pop_f[parent_i]:\n                        pop_x[parent_i] = child\n                        pop_f[parent_i] = f_child\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n\n            # Adapt parent sigma on failure to encourage exploration or shrink when stuck\n            if not success:\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.88, 1e-12)\n            else:\n                # mild rewarding of sigma for success\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.02, np.mean(ub - lb))\n\n            # Occasional rejuvenation: replace worst with a fresh random sample\n            if remaining > 0 and np.random.rand() < 0.03:\n                worst_i = int(np.argmax(pop_f))\n                new_x = np.random.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, new_x = callf(new_x)\n                    pop_x[worst_i] = new_x\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # If population has degenerated (all identical/very close), inject diversity\n            if np.allclose(pop_x, pop_x[0], atol=1e-12) and remaining > 0:\n                idx = np.random.randint(0, self.pop_size)\n                new_x = np.random.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, new_x = callf(new_x)\n                    pop_x[idx] = new_x\n                    pop_f[idx] = f_new\n                    pop_sigma[idx] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # safety break if budget exhausted\n            if remaining <= 0:\n                break\n\n        # finished\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.508 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14431519191534403, 0.17116827613701247, 0.6347813097808961, 0.8960001182797652, 0.77479981532639, 0.7915020753925974, 0.26889838437905345, 0.5242755589367443, 0.7077786471518273, 0.17076510993677618]}, "task_prompt": ""}
{"id": "281dd651-5fbb-40b0-b827-b3f604c8d0c6", "fitness": 0.4040490619556941, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based metaheuristic combining randomized directional local searches, orthogonal refinements, adaptive step-sizes and occasional heavy-tailed Lévy-like jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (adaptive by default)\n    - seed: optional random seed\n    Note: functions are assumed to accept a 1-D numpy array and return a scalar.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # sensible default population scaling with dimension but limited by budget\n        if pop_size is None:\n            # base population: between 6 and 6*dim but never more than budget/4\n            self.pop_size = max(2, min(int(6 + self.dim), max(2, self.budget // 8)))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        # internal trackers\n        self._evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds (ensure full-dim arrays)\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback to [-5,5] as requested\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # if bounds are scalars or wrong shape, expand\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # clipped call helper that respects budget and updates best\n        def callf(x):\n            if self._evals >= self.budget:\n                return None  # budget exhausted\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            self._evals += 1\n            if f is None:\n                # in case the function returns None (shouldn't happen), treat as inf\n                f = np.inf\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # If budget is extremely small, fallback to pure random search\n        if self.budget <= 2 or self._evals >= self.budget:\n            # trivial random search\n            while self._evals < self.budget:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population (seeded uniformly)\n        pop_size = min(self.pop_size, max(2, self.budget // 3))\n        X = np.zeros((pop_size, self.dim))\n        F = np.zeros(pop_size)\n        sigma = np.zeros(pop_size)  # adaptive step sizes per individual\n\n        # initialize\n        for i in range(pop_size):\n            if self._evals >= self.budget:\n                # not enough budget to fully initialize population\n                pop_size = i\n                X = X[:pop_size]\n                F = F[:pop_size]\n                sigma = sigma[:pop_size]\n                break\n            x = self.rng.uniform(lb, ub)\n            f = callf(x)\n            if f is None:\n                break\n            X[i] = x\n            F[i] = f\n            # initial sigma set to a fraction of domain width with randomization\n            sigma[i] = np.mean(ub - lb) * (0.05 + 0.05 * self.rng.rand())\n\n        if pop_size == 0:\n            # no budget to evaluate even one point => return inf\n            return self.f_opt, self.x_opt\n\n        # precompute some bookkeeping\n        iteration = 0\n        # probabilities / hyperparameters\n        p_levy = 0.06\n        p_rejuvenate = 0.02\n        tournament_k = min(3, pop_size)\n        backtrack_tries = 3\n        orthogonal_tries = 1\n        levy_scale_factor = 0.8  # controls magnitude of heavy-tail jumps\n        sigma_increase = 1.12\n        sigma_decrease = 0.85\n\n        # helper to get worst/best indices\n        def best_idx():\n            return int(np.argmin(F))\n\n        def worst_idx():\n            return int(np.argmax(F))\n\n        # Main optimization loop\n        while self._evals < self.budget:\n            iteration += 1\n\n            # 1) select parent by small tournament\n            candidates = self.rng.randint(0, pop_size, size=tournament_k)\n            parent_idx = candidates[np.argmin(F[candidates])]\n            x_parent = X[parent_idx].copy()\n            f_parent = float(F[parent_idx])\n            s_parent = float(sigma[parent_idx])\n\n            # 2) sample a random direction (normalized)\n            dir_vec = self.rng.randn(self.dim)\n            nn = np.linalg.norm(dir_vec)\n            if nn == 0:\n                dir_vec = np.ones(self.dim)\n                nn = np.linalg.norm(dir_vec)\n            dir_vec /= nn\n\n            # 3) primary directional trial with stochasticized step-length\n            # step-length drawn from lognormal-ish factor to promote occasional larger steps\n            step_len = s_parent * np.exp(0.5 * (self.rng.randn() - 0.0))\n            x_trial = x_parent + step_len * dir_vec\n            x_trial = np.minimum(np.maximum(x_trial, lb), ub)\n\n            if self._evals >= self.budget:\n                break\n            f_trial = callf(x_trial)\n            if f_trial is None:\n                break\n\n            if f_trial < f_parent:\n                # success: accept and increase sigma\n                X[parent_idx] = x_trial\n                F[parent_idx] = f_trial\n                sigma[parent_idx] = s_parent * sigma_increase\n                # small exploitation: try a few smaller backtracking steps\n                for bt in range(backtrack_tries):\n                    if self._evals >= self.budget:\n                        break\n                    small_step = (step_len * (0.5 ** (bt + 1)))\n                    x_bt = X[parent_idx] + small_step * dir_vec\n                    x_bt = np.minimum(np.maximum(x_bt, lb), ub)\n                    f_bt = callf(x_bt)\n                    if f_bt is None:\n                        break\n                    if f_bt < F[parent_idx]:\n                        X[parent_idx] = x_bt\n                        F[parent_idx] = f_bt\n                        sigma[parent_idx] *= 1.05\n                    else:\n                        # not improving further\n                        break\n            else:\n                # failure: reduce sigma slightly\n                sigma[parent_idx] = max(1e-12, s_parent * sigma_decrease)\n\n                # 4) orthogonal perturbation tries to diversify locally\n                for ot in range(orthogonal_tries):\n                    if self._evals >= self.budget:\n                        break\n                    v = self.rng.randn(self.dim)\n                    # make orthogonal to dir_vec\n                    v = v - np.dot(v, dir_vec) * dir_vec\n                    nv = np.linalg.norm(v)\n                    if nv == 0:\n                        continue\n                    v /= nv\n                    x_orth = x_parent + (s_parent * 0.6) * v\n                    x_orth = np.minimum(np.maximum(x_orth, lb), ub)\n                    f_orth = callf(x_orth)\n                    if f_orth is None:\n                        break\n                    if f_orth < f_parent:\n                        X[parent_idx] = x_orth\n                        F[parent_idx] = f_orth\n                        sigma[parent_idx] *= 1.08\n                        break  # accepted orthogonal improvement\n\n            # 5) occasional Lévy-like heavy-tailed jump to escape basins\n            if (self.rng.rand() < p_levy) and (self._evals < self.budget):\n                # Cauchy-like heavy-tailed vector (standard Cauchy)\n                cauchy_vec = self.rng.standard_cauchy(self.dim)\n                # robust scale: median absolute deviation of population (fallback to domain scale)\n                try:\n                    mad = np.median(np.abs(X - np.median(X, axis=0)), axis=0)\n                    robust_scale = np.median(mad) if np.all(np.isfinite(mad)) else np.mean(ub - lb)\n                except Exception:\n                    robust_scale = np.mean(ub - lb)\n                if robust_scale <= 0 or not np.isfinite(robust_scale):\n                    robust_scale = np.mean(ub - lb)\n                # scale and normalize to keep heavy-tail property but avoid explosion\n                cvec = cauchy_vec\n                # clip extreme Cauchy values to avoid numerical blow-ups, but retain tails\n                cvec = np.clip(cvec, -1e6, 1e6)\n                # scale and apply\n                jump = levy_scale_factor * robust_scale * cvec / (1.0 + np.median(np.abs(cvec)))\n                # combine with random direction to keep dimensional balance\n                jump_dir = jump\n                # normalize jump magnitude to some fraction of domain but keep heavy-tail sign\n                jnorm = np.linalg.norm(jump_dir)\n                if jnorm > 0:\n                    jump_dir = jump_dir / jnorm * (robust_scale * (1.0 + self.rng.randn() * 0.5))\n                x_jump = X[worst_idx()] + jump_dir\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                if self._evals >= self.budget:\n                    break\n                f_jump = callf(x_jump)\n                if f_jump is None:\n                    break\n                # if jump is good, replace worst; else sometimes keep as candidate to replace worst later\n                if f_jump < F[worst_idx()]:\n                    idx_w = worst_idx()\n                    X[idx_w] = x_jump\n                    F[idx_w] = f_jump\n                    sigma[idx_w] = max(1e-12, np.mean(sigma) * 0.6)\n\n            # 6) recombination exploitation: mix two best plus small noise\n            if self._evals < self.budget:\n                b1 = best_idx()\n                # second best via temporary masking\n                mask = np.ones(pop_size, dtype=bool)\n                mask[b1] = False\n                if mask.sum() > 0:\n                    b2 = np.argmin(F[mask])\n                    # map index back\n                    if b2 >= b1:\n                        # since we removed b1, the index shifts\n                        b2 = b2 + 1\n                else:\n                    b2 = b1\n                # form recombination\n                x_recomb = X[b1] + 0.6 * (X[b2] - X[b1]) + 0.02 * np.mean(ub - lb) * self.rng.randn(self.dim)\n                x_recomb = np.minimum(np.maximum(x_recomb, lb), ub)\n                if self._evals >= self.budget:\n                    break\n                f_recomb = callf(x_recomb)\n                if f_recomb is None:\n                    break\n                if f_recomb < F[parent_idx]:\n                    # replace parent\n                    X[parent_idx] = x_recomb\n                    F[parent_idx] = f_recomb\n                    sigma[parent_idx] = max(1e-12, (sigma[parent_idx] + np.mean(sigma)) * 0.7)\n                else:\n                    # maybe replace worst if better\n                    idxw = worst_idx()\n                    if f_recomb < F[idxw]:\n                        X[idxw] = x_recomb\n                        F[idxw] = f_recomb\n                        sigma[idxw] = max(1e-12, np.mean(sigma) * 0.8)\n\n            # 7) adapt parent sigma on repeated failures modestly\n            # small adaptive nudges: encourage exploration if stagnating\n            if self._evals < self.budget:\n                # if parent still worse than median, nudge its sigma upward occasionally\n                medF = np.median(F)\n                if F[parent_idx] > medF and self.rng.rand() < 0.15:\n                    sigma[parent_idx] = min(np.mean(ub - lb), sigma[parent_idx] * 1.2)\n\n            # 8) occasional population rejuvenation by replacing the worst with a random sample\n            if (self.rng.rand() < p_rejuvenate) and (self._evals < self.budget):\n                x_new = self.rng.uniform(lb, ub)\n                if self._evals >= self.budget:\n                    break\n                f_new = callf(x_new)\n                if f_new is None:\n                    break\n                idxw = worst_idx()\n                if f_new < F[idxw]:\n                    X[idxw] = x_new\n                    F[idxw] = f_new\n                    sigma[idxw] = np.mean(sigma) * 0.5\n\n            # small housekeeping: ensure sigmas stay positive and bounded\n            sigma = np.maximum(sigma, 1e-12)\n            sigma = np.minimum(sigma, np.linalg.norm(ub - lb) * 2.0)\n\n        # finished budget or break\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.404 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09655969923154739, 0.16235994814788235, 0.3956974087478623, 0.9694818688440113, 0.399591046649604, 0.6711871324916909, 0.2645180185398879, 0.3933512850897959, 0.5376181119824721, 0.15012609983218672]}, "task_prompt": ""}
{"id": "6a357f5e-cc45-47eb-b293-ff38264d6a1b", "fitness": 0.5001370237339955, "name": "ADLE", "description": "Adaptive Directional Lévy Ensemble (ADLE) — population-based directional search combining adaptive per-agent step-sizes, memory of successful directions, orthogonal refinements and occasional Lévy jumps for robust exploration/exploitation.", "code": "import numpy as np\n\nclass ADLE:\n    \"\"\"\n    Adaptive Directional Lévy Ensemble (ADLE)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None scale with dim)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimension but remains efficient\n            self.pop_size = max(4, min(40, int(4 + 2.5 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # read bounds, allow scalars or arrays/lists\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # ensure bounds shapes\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # evaluation wrapper: clips to bounds, checks budget, updates best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # quick exit\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population by uniform sampling\n        n_init = min(self.pop_size, remaining)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base per-dim scale (vector) and scalar mean used for adjustments\n        range_vec = ub - lb\n        base_sigma_scalar = max(1e-12, 0.2 * np.mean(range_vec))\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # each member has its own sigma (scalar)\n            pop_sigma.append(base_sigma_scalar * (0.7 + 0.6 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If budget too small to create any population, do random search fallback\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Direction memory: store last successful directions (normalized), global memory\n        dir_memory = []\n        max_mem = min(50, 5 * self.dim)\n\n        # track stagnation counter to trigger rejuvenation\n        stagnant = 0\n        last_best = self.f_opt\n\n        # main loop\n        while remaining > 0:\n            # tournament selection (small k) to pick parent: balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            parent_i = inds[np.argmin([pop_f[j] for j in inds])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # construct search direction:\n            # either a weighted combination of stored good directions or fresh random\n            if dir_memory and np.random.rand() < 0.6:\n                # mix a few random entries from memory\n                m = min(3, len(dir_memory))\n                picks = np.random.choice(len(dir_memory), m, replace=False)\n                d = np.sum([dir_memory[p] * (0.5 + np.random.rand()) for p in picks], axis=0)\n            else:\n                d = np.random.randn(self.dim)\n\n            # normalize direction robustly\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # adaptive step-length relative to sigma and problem scale\n            alpha = sigma * max(1e-12, (1.0 + 0.4 * np.random.randn()))\n            step = d * alpha\n\n            x_try = np.clip(x_parent + step, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # accept and grow sigma slightly, store success direction\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, max(1e-12, np.mean(range_vec)))\n                # store direction scaled to unit and clipped\n                dir_memory.append(d.copy())\n                if len(dir_memory) > max_mem:\n                    dir_memory.pop(0)\n                stagnant = 0\n                continue\n\n            # local backtracking refinement along direction (small fractions)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + step * frac, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n                    dir_memory.append(np.sign(frac) * d.copy())\n                    if len(dir_memory) > max_mem:\n                        dir_memory.pop(0)\n                    improved = True\n                    stagnant = 0\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation to the attempted direction (diversify locally)\n            r = np.random.randn(self.dim)\n            # project out d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.7 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.08, max(1e-12, np.mean(range_vec)))\n                        dir_memory.append(r.copy())\n                        if len(dir_memory) > max_mem:\n                            dir_memory.pop(0)\n                        stagnant = 0\n                        continue\n\n            # occasional Lévy-like jump to escape deep basins (heavy-tail)\n            if np.random.rand() < 0.09 and remaining > 0:\n                step_c = np.random.standard_cauchy(self.dim)\n                # robust normalization to keep heavy-tail but not extremes\n                denom = np.percentile(np.abs(step_c), 85) + 1e-12\n                step_c = step_c / denom\n                # scale vector proportional to problem range\n                scale_vec = 0.18 * range_vec\n                x_try = np.clip(x_parent + step_c * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if good, replace the worst; otherwise keep candidate in small pool\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                        stagnant = 0\n                    else:\n                        # sometimes keep the jump in memory for direction suggestions\n                        if np.random.rand() < 0.15:\n                            v = x_try - x_parent\n                            nv = np.linalg.norm(v)\n                            if nv > 1e-12:\n                                dir_memory.append((v / nv).copy())\n                                if len(dir_memory) > max_mem:\n                                    dir_memory.pop(0)\n                continue\n\n            # recombination: mix two best individuals and try a small local search\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.015 * range_vec) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # if improves parent, replace parent, else maybe replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                    dir_memory.append((x_try - pop[parent_i]) / (np.linalg.norm(x_try - pop[parent_i]) + 1e-12))\n                    if len(dir_memory) > max_mem:\n                        dir_memory.pop(0)\n                    stagnant = 0\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma_scalar * (0.4 + 0.6 * np.random.rand())\n                        stagnant = 0\n\n            # on failure adapt sigma down for parent\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.86, 1e-12)\n\n            # occasional population rejuvenation when stagnating: replace worst with random sample\n            if remaining > 0:\n                if stagnant > max(30, 5 * self.dim) or (np.random.rand() < 0.02):\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma_scalar * (0.6 + 0.6 * np.random.rand())\n                    stagnant = 0\n\n            # update stagnation counter\n            if self.f_opt < last_best - 1e-15:\n                last_best = self.f_opt\n                stagnant = 0\n            else:\n                stagnant += 1\n\n            # occasionally prune/populate direction memory to keep it relevant\n            if len(dir_memory) > 0 and np.random.rand() < 0.05:\n                # keep only recent / useful directions\n                keep = max(1, int(0.6 * len(dir_memory)))\n                dir_memory = dir_memory[-keep:]\n\n        # finished budget or loop exit\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLE scored 0.500 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1204420808187402, 0.162980256915494, 0.46234232921222285, 0.9546297953930927, 0.9331340201478893, 0.7714427601579212, 0.2589118000875056, 0.4194179294814566, 0.7919010332914205, 0.12616823183421222]}, "task_prompt": ""}
{"id": "1149013d-205c-4a0e-be15-941ed787db69", "fitness": 0.46765049584147117, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size probes that perform randomized directional local searches, orthogonal refinements and occasional Lévy jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with sqrt(dim))\n    - seed: optional RNG seed for reproducibility\n\n    The algorithm maintains a small population of candidate solutions with adaptive\n    per-individual step-sizes (sigma). Each iteration selects a parent via a small\n    tournament, performs a directional trial (with backtracking), an orthogonal try,\n    occasional Lévy-like jumps to escape basins, recombination of top individuals,\n    and occasional population rejuvenation. All function calls are strictly counted\n    and never exceed the provided budget.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population, grows slowly with dimension\n            pop_size = int(max(4, min(30, int(4 + 2 * np.sqrt(self.dim)))))\n        self.pop_size = int(pop_size)\n        self.seed = seed\n        # prefer new Generator API for reproducibility\n        self.rng = np.random.default_rng(seed)\n        # best seen\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # extract bounds and make full-dim arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper: evaluate under budget control; returns (f, clipped_x)\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).copy()\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If there's no budget, return trivial result\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population uniformly in bounds\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n        for i in range(n_init):\n            x0_raw = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0_raw)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.random()))\n            if remaining <= 0:\n                break\n\n        # If very small budget prevented creating any individual, do pure random draws\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main optimization loop: consume remaining budget\n        while remaining > 0:\n            # tournament selection size\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), size=k, replace=False)\n            # select best among sampled indices (small tournament)\n            sampled_fs = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(sampled_fs))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # robust random search direction\n            d = self.rng.normal(size=self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # fallback: pick coordinate direction if degenerate\n                d = np.zeros(self.dim, dtype=float)\n                d[self.rng.integers(0, self.dim)] = 1.0\n                nd = 1.0\n            d = d / nd\n\n            # stochastic step-size for this attempt (scales with sigma)\n            alpha = sigma * (0.5 + self.rng.random() * 1.5)\n            # primary directional try\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            worst_i = int(np.argmax(pop_f))\n\n            if f_try < pop_f[parent_i]:\n                # accept improvement and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                continue  # move to next iteration (exploit success)\n\n            # local backtracking / smaller step refinement\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation to diversify locally\n            r = self.rng.normal(size=self.dim)\n            # make orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        continue\n\n            # occasional Lévy-like heavy-tailed jump to escape local minima\n            if self.rng.random() < 0.08 and remaining > 0:\n                # generate Cauchy-like heavy-tailed vector via generator\n                step = self.rng.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale jump relative to sigma and problem scale\n                jump_scale = max(sigma * 2.0, 0.5 * np.mean(ub - lb))\n                x_try = np.clip(x_parent + step * jump_scale, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # if jump is beneficial, replace the worst in population\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                        # continue main loop to exploit newly injected candidate\n                        continue\n\n            # recombination/exploitation: mix two best solutions and add small noise\n            if remaining > 0:\n                if len(pop) >= 2:\n                    best2 = np.argsort(pop_f)[:2]\n                    a, b = best2[0], best2[1]\n                else:\n                    a = b = 0\n                beta = self.rng.random()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * self.rng.normal(size=self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # Replace parent or the worst if improved\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent's sigma on repeated failures to encourage exploration\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation: replace the worst with a random sample\n            if self.rng.random() < 0.06 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.random())\n\n        # finished or budget exhausted\n        # ensure attributes reflect final best\n        self.f_opt = float(self.f_opt)\n        self.x_opt = None if self.x_opt is None else self.x_opt.copy()\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.468 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09707846727950964, 0.14645939140289088, 0.49149739380003843, 0.9616846730318661, 0.6698028623726495, 0.6205604450155675, 0.25706163296912654, 0.3951424112442824, 0.885143515726781, 0.1520741655719995]}, "task_prompt": ""}
{"id": "249e5d97-d4f9-432d-9c1e-bc22c0b82fc8", "fitness": 0.5236909437501269, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (adapted from dim if None)\n    - seed: optional RNG seed for reproducibility\n\n    The algorithm keeps a small population with adaptive step sizes (sigma).\n    It performs randomized directional local searches from selected parents,\n    tries orthogonal refinements, occasionally performs heavy-tailed (Cauchy-like)\n    jumps to escape basins, and uses recombination of good individuals.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        # sensible default population scaling with dimensionality\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(6 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        # state for best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n        # read bounds from func; allow scalar or array\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).copy()\n            # ensure within bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget very small just do uniform random sampling\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = np.maximum(1e-8, 0.25 * np.mean(ub - lb))\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # initialize sigma per individual with small random perturbation\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * rng.rand()))\n            if remaining <= 0:\n                break\n\n        # if no population created, pure random search until budget exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # main loop\n        stagnation_counter = 0  # track stagnation to increase jump probability\n        iter_since_improve = 0\n        while remaining > 0:\n            # small tournament selection\n            k = min(3, len(pop))\n            inds = rng.choice(len(pop), size=k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_local = int(inds[int(np.argmin(values))])\n            x_parent = pop[parent_local].copy()\n            sigma = float(pop_sigma[parent_local])\n\n            improved_this_round = False\n\n            # random search direction (normalized)\n            d = rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                # fallback to random direction\n                d = rng.randn(self.dim)\n                nd = np.linalg.norm(d)\n                if nd <= 1e-12:\n                    d = np.ones(self.dim)\n                    nd = np.linalg.norm(d)\n            d = d / nd\n\n            # primary directional trial with noisy step length\n            alpha = sigma * (1.0 + 0.5 * rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_local]:\n                pop[parent_local] = x_try.copy()\n                pop_f[parent_local] = f_try\n                pop_sigma[parent_local] = min(sigma * 1.2, np.mean(ub - lb))\n                improved_this_round = True\n                iter_since_improve = 0\n            else:\n                # local backtracking / small-step refinement along direction\n                for frac in (0.5, 0.25, 0.125):\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_local]:\n                        pop[parent_local] = x_try.copy()\n                        pop_f[parent_local] = f_try\n                        pop_sigma[parent_local] = min(sigma * 1.1, np.mean(ub - lb))\n                        improved_this_round = True\n                        iter_since_improve = 0\n                        break\n\n            # orthogonal perturbation for local diversification\n            if not improved_this_round and remaining > 0:\n                r = rng.randn(self.dim)\n                # make orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    if remaining > 0:\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_local]:\n                            pop[parent_local] = x_try.copy()\n                            pop_f[parent_local] = f_try\n                            pop_sigma[parent_local] = min(sigma * 1.15, np.mean(ub - lb))\n                            improved_this_round = True\n                            iter_since_improve = 0\n\n            # occasional Lévy-like jump to escape local basins\n            # probability increases with stagnation\n            p_jump = 0.03 + min(0.2, 0.001 * iter_since_improve)\n            if (not improved_this_round) and (rng.rand() < p_jump) and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = rng.standard_cauchy(size=self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[parent_local]:\n                    # replace parent directly\n                    pop[parent_local] = x_try.copy()\n                    pop_f[parent_local] = f_try\n                    pop_sigma[parent_local] = max(1e-12, sigma * 0.6)\n                    improved_this_round = True\n                    iter_since_improve = 0\n                elif f_try < pop_f[worst_i]:\n                    # replace worst\n                    pop[worst_i] = x_try.copy()\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * rng.rand())\n\n            # recombination exploitation: mix two best and small noise\n            if (not improved_this_round) and len(pop) >= 2 and remaining > 0:\n                best2_idx = np.argsort(pop_f)[:2]\n                a, b = int(best2_idx[0]), int(best2_idx[1])\n                beta = rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_local]:\n                    pop[parent_local] = x_try.copy()\n                    pop_f[parent_local] = f_try\n                    pop_sigma[parent_local] = max(sigma * 0.9, 1e-12)\n                    improved_this_round = True\n                    iter_since_improve = 0\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # on failure reduce sigma slightly\n            if not improved_this_round:\n                pop_sigma[parent_local] = max(sigma * 0.85, 1e-12)\n                iter_since_improve += 1\n            else:\n                iter_since_improve = 0\n\n            # occasional population rejuvenation\n            if rng.rand() < 0.02 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * rng.rand())\n\n            # keep population size stable (in case budget stopped during init)\n            # remove duplicates (very close individuals) by perturbing slightly\n            if remaining > 0 and len(pop) > 1:\n                for i in range(len(pop)):\n                    for j in range(i + 1, len(pop)):\n                        if np.linalg.norm(pop[i] - pop[j]) < 1e-12:\n                            pop[j] = np.clip(pop[j] + 1e-6 * rng.randn(self.dim), lb, ub)\n\n            # if budget exhausted break loop\n            if remaining <= 0:\n                break\n\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.524 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10918809238633387, 0.1570737877250713, 0.7429682305332663, 0.861463380875685, 0.7685380788206504, 0.9278666178434338, 0.2426923276699443, 0.469255491088055, 0.7768537750264294, 0.18100965553239923]}, "task_prompt": ""}
{"id": "98b83c2f-caf8-454c-bfc8-38c32378b02e", "fitness": 0.46604730383966403, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search that mixes directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        # population scales with dimensionality but remains modest\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the black-box func using at most self.budget evaluations.\n        func should accept a 1-D numpy array of length self.dim and return a scalar.\n        func.bounds.lb and func.bounds.ub are used (scalars or arrays).\n        Returns (f_opt, x_opt).\n        \"\"\"\n        # prepare bounds as arrays of correct length\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        # safety check\n        assert lb.shape == (self.dim,) and ub.shape == (self.dim,), \"Bounds must match dim\"\n\n        remaining = int(self.budget)\n\n        # best-so-far\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        # helper to call func while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return None, None\n            x = np.asarray(x, dtype=float).ravel()\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                # if func raises, consume budget but return inf\n                f = float(\"inf\")\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # trivial fallback if no budget\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale related to domain\n        n_init = min(self.pop_size, remaining)\n        for _ in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            if f0 is None:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # give each individual a slightly different sigma\n            pop_sigma.append(base_sigma * (0.5 + 0.9 * self.rng.rand()))\n        # If budget allowed more individuals than created, keep pop_size as actual length\n        self.pop_size = len(pop)\n\n        # If no population created due to extremely small budget, do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # helper to get worst index quickly\n        def worst_index():\n            return int(np.argmax(pop_f))\n\n        # main loop\n        while remaining > 0:\n            # tournament selection: pick a small subset and choose its best\n            tour_size = min(3, len(pop))\n            inds = self.rng.choice(len(pop), size=tour_size, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            improved = False\n\n            # sample a random normalized search direction\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * self.rng.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try is not None and f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.12, np.mean(ub - lb))\n                continue  # next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try is None:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                # make r orthogonal-ish to d for complementary search\n                r = r - (r @ d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try is not None and f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # use element-wise Cauchy (standard) scaled by domain fraction, but clip extremes\n                scale_vec = 0.2 * (ub - lb)\n                # sample Cauchy but limit extremes by a robust factor\n                step = self.rng.standard_cauchy(size=self.dim)\n                # threshold to avoid infinite jumps while preserving heavy-tail\n                max_abs = 25.0\n                step = np.clip(step, -max_abs, max_abs)\n                step = step * scale_vec\n                # normalize to avoid blowing up: multiply by a random radius (heavy-tailed)\n                radius = np.abs(self.rng.standard_cauchy())  # heavy tail scalar\n                radius = np.clip(radius, 0.1, 50.0)\n                x_try = np.clip(x_parent + step * (0.5 + 0.5 * self.rng.rand()) , lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try is not None:\n                    if f_try < pop_f[parent_i]:\n                        # replace parent with jump\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = base_sigma * (0.5 + self.rng.rand())\n                    else:\n                        # maybe replace worst with this candidate\n                        w = worst_index()\n                        if f_try < pop_f[w]:\n                            pop[w] = x_try\n                            pop_f[w] = f_try\n                            pop_sigma[w] = base_sigma * (0.4 + 0.6 * self.rng.rand())\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = 0.2 * sigma * self.rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try is not None:\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    else:\n                        # try to inject into population by replacing the worst if it's better\n                        w = worst_index()\n                        if f_try < pop_f[w]:\n                            pop[w] = x_try\n                            pop_f[w] = f_try\n                            pop_sigma[w] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (if not improved by the above)\n            pop_sigma[parent_i] = max(sigma * 0.87, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                w = worst_index()\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new is not None:\n                    pop[w] = x_new\n                    pop_f[w] = f_new\n                    pop_sigma[w] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.466 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1363028055983423, 0.1522410639063828, 0.6542546227154231, 0.975411120209644, 0.8435310198108485, 0.9298292071857854, 0.25037850655871274, 0.2141614362541423, 0.3580913679221649, 0.14627188823519455]}, "task_prompt": ""}
{"id": "ebfd03db-6219-45f3-8217-36d063486d5a", "fitness": 0.4124728574088972, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, recombination and occasional Lévy-like heavy-tailed jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (adaptive default based on dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        if pop_size is None:\n            # population scales with dimensionality but remains modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # try to read bounds from func if available, otherwise use [-5,5]^dim\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, float(lb))\n            if ub.size == 1:\n                ub = np.full(self.dim, float(ub))\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure proper shapes\n        lb = lb.flatten()[:self.dim]\n        ub = ub.flatten()[:self.dim]\n\n        # robust maximum scale for sigma\n        max_scale = float(np.maximum(1e-12, np.mean(ub - lb)))\n        base_sigma = max(1e-8, 0.25 * max_scale)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).flatten()[:self.dim]\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x.copy()\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            # elementwise uniform\n            x0 = lb + self.rng.rand(self.dim) * (ub - lb)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # main loop\n        while remaining > 0:\n            # index of worst (for replacements)\n            worst_i = int(np.argmax(pop_f))\n            # pick a parent via small tournament\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), size=k, replace=False)\n            parent_i = int(inds[np.argmin(pop_f[inds])])\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # random normalized direction\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # stochasticized step-length (positive)\n            step_len = sigma * max(1e-12, 1.0 + 0.4 * self.rng.randn())\n            x_try = np.clip(x_parent + d * step_len, lb, ub)\n\n            improved = False\n            # primary directional trial\n            if remaining > 0:\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    # accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.15, max_scale)\n                    improved = True\n                    # slightly inject to worst if it's better\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(base_sigma * 0.4, pop_sigma[worst_i])\n                    continue  # go to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            if not improved:\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + d * (step_len * frac), lb, ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.8, 1e-12)\n                        improved = True\n                        break\n                if improved:\n                    continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = self.rng.randn(self.dim)\n                # make orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    orth_step = step_len * (0.4 + 0.6 * self.rng.rand())\n                    x_try = np.clip(x_parent + r * orth_step, lb, ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        improved = True\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and self.rng.rand() < 0.08:\n                # Cauchy-like heavy-tailed vector\n                step = self.rng.standard_cauchy(self.dim)\n                # robust normalization to keep heavy tails but avoid absolute extremes\n                denom = np.median(np.abs(step)) + 1e-12\n                step = step / denom\n                # scale vector with a random multiplier\n                scale_vec = base_sigma * (2.0 + 6.0 * self.rng.rand())\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * self.rng.rand())\n\n            # recombination exploitation: mix two best and add small noise\n            if remaining > 0 and self.rng.rand() < 0.20:\n                best_two = np.argsort(pop_f)[:2]\n                a, b = best_two[0], best_two[1] if len(best_two) > 1 else best_two[0]\n                alpha = 0.2 + 0.6 * self.rng.rand()\n                x_new = (1 - alpha) * pop[a] + alpha * pop[b]\n                x_new = x_new + (self.rng.randn(self.dim) * (0.05 * (pop_sigma[a] + pop_sigma[b])))\n                x_new = np.clip(x_new, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        break\n                    worst_i = int(np.argmax(pop_f))\n                    if f_new < pop_f[worst_i]:\n                        pop[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = base_sigma * (0.5 + 0.6 * self.rng.rand())\n\n            # adapt parent sigma on failure\n            if not improved:\n                # shrink a bit\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.96, 1e-12)\n                # occasional small random kick to sigma to avoid stagnation\n                if self.rng.rand() < 0.03:\n                    pop_sigma[parent_i] = min(max_scale, pop_sigma[parent_i] * (1.0 + 0.5 * self.rng.rand()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                x_new = lb + self.rng.rand(self.dim) * (ub - lb)\n                if remaining > 0:\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        break\n                    worst_i = int(np.argmax(pop_f))\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.412 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1225480267438811, 0.16137996254920906, 0.48543485171388834, 0.946272944674575, 0.3719256292943397, 0.6409160784572967, 0.25935934352208645, 0.4422252456570924, 0.5283392744244295, 0.16632721705217368]}, "task_prompt": ""}
{"id": "7a5f7dbb-6a25-4879-bc31-036b7f8d0b01", "fitness": 0.5668681222006718, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining randomized directional local searches, orthogonal refinement, adaptive per-individual step-sizes, recombination and occasional heavy-tailed Lévy/Cauchy jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows slowly with dimension\n            self.pop_size = max(4, min(40, 4 + int(self.dim / 2)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # Attempt to read bounds from func; provide defaults if not present\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # make sure bounds are full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n        span = ub - lb\n        mean_span = float(np.mean(span))\n        if mean_span <= 0:\n            raise ValueError(\"Invalid bounds: ub must be greater than lb elementwise\")\n\n        # bookkeeping\n        remaining = int(self.budget)\n        self.f_opt = float(np.inf)\n        self.x_opt = None\n\n        # safe evaluator that tracks remaining budget and best-so-far\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                # budget exhausted, do not call function\n                return float(np.inf), x\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = float(func(x))\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # Very small budgets: pure random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n        if remaining < 3:\n            # sample a few random points until budget exhausted\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (as many as budget allows)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-12, 0.08 * mean_span)  # baseline step size\n        initial_pop = min(self.pop_size, remaining)\n        for i in range(initial_pop):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # per-individual sigma randomized around base\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # if we couldn't create any population member (very small budget),\n        # fallback to random sampling\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # parameters\n        tournament_k = 3\n        frac_backtrack = (0.5, 0.25, -0.5, -0.25)\n        levy_prob = 0.08  # probability to attempt a heavy-tailed jump each iteration\n        rejuvenate_prob = 0.02  # occasional replace-worst with random\n        max_iters_without_improve = 50\n        stagnant = [0] * len(pop)  # per-individual stagnation counters\n\n        # main loop\n        while remaining > 0:\n            # pick parent via small tournament\n            k = min(tournament_k, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            vals = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(vals))]\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # 1) primary directional trial\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())  # stochasticized step-length\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            success = False\n            if f_try < f_parent:\n                # accept\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, mean_span)\n                stagnant[parent_i] = 0\n                success = True\n            else:\n                # local backtracking / fractional steps along d\n                for frac in frac_backtrack:\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.06, mean_span)\n                        stagnant[parent_i] = 0\n                        success = True\n                        break\n\n            # 2) orthogonal refinement if primary didn't succeed\n            if not success and remaining > 0:\n                r = np.random.randn(self.dim)\n                # make r orthogonal to d: r = r - (r·d) d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    step = 0.6 * sigma * (0.8 + 0.4 * np.random.rand())\n                    x_try = np.clip(x_parent + step * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.08, mean_span)\n                        stagnant[parent_i] = 0\n                        success = True\n\n            # 3) occasional Lévy-like heavy-tailed jump to escape basins\n            if (not success) and (remaining > 0) and (np.random.rand() < levy_prob):\n                # Cauchy-like heavy tails: sample independent Cauchy variates\n                # scale them by a fraction of the box span\n                step = np.tan(np.pi * (np.random.rand(self.dim) - 0.5))  # Cauchy via inverse CDF\n                # robust normalization to avoid infinite steps while preserving heavy tail\n                denom = np.maximum(1.0, np.median(np.abs(step)))\n                step = step / (denom + 1e-12)\n                scale_vec = 0.22 * span * (0.7 + 0.6 * np.random.rand(self.dim))\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, min(pop_sigma[parent_i] * 1.3, mean_span))\n                    stagnant[parent_i] = 0\n                    success = True\n                else:\n                    # if new candidate is good relative to worst, replace worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, pop_sigma[parent_i] * 0.9)\n\n            # 4) recombination exploitation: mix two best + small noise\n            if remaining > 0 and np.random.rand() < 0.25:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                x_mix = 0.6 * pop[a] + 0.4 * pop[b] + np.random.randn(self.dim) * (0.03 * mean_span)\n                x_try = np.clip(x_mix, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace worst if better, else inject replacing a parent sometimes\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n\n            # adapt sigma on failure\n            if not success:\n                # penalize sigma slightly\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n                stagnant[parent_i] += 1\n            else:\n                stagnant[parent_i] = 0\n\n            # occasional population rejuvenation by replacing worst with a random sample\n            if remaining > 0 and (np.random.rand() < rejuvenate_prob or max(stagnant) > max_iters_without_improve):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n                stagnant[worst_i] = 0\n\n            # if population is small but we still have budget, try to add diversity by sampling occasional new members\n            if remaining > 0 and len(pop) < self.pop_size and np.random.rand() < 0.2:\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n                stagnant.append(0)\n\n            # shrink population size if evaluations get low to focus effort\n            if remaining < max(5, int(self.budget * 0.05)) and len(pop) > 6:\n                # keep best 6\n                besti = np.argsort(pop_f)[:6]\n                pop = [pop[i] for i in besti]\n                pop_f = [pop_f[i] for i in besti]\n                pop_sigma = [pop_sigma[i] for i in besti]\n                stagnant = [stagnant[i] for i in besti]\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.567 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14662863348831334, 0.17627120113934813, 0.9167095765662518, 0.9674186418627405, 0.7677120659261831, 0.951639613990895, 0.2768058950432164, 0.4223286939165882, 0.8652838451237114, 0.17788305494947]}, "task_prompt": ""}
{"id": "0d92bc52-7154-4954-bd0a-c2ba7e3b8642", "fitness": 0.4915606737282617, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy-like jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None a heuristic is used)\n    - seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(int(seed))\n        # default pop sizing: grows with dimension but limited by budget\n        if pop_size is None:\n            default = max(4, 2 * self.dim)\n            # keep at most budget//8 in population so we have room for iterations\n            self.pop_size = int(min(default, max(1, self.budget // 8)))\n        else:\n            self.pop_size = max(1, int(pop_size))\n\n        # internal bests\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # get bounds if available, else use [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n\n        # ensure shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # safe evaluator that tracks remaining budget\n        def callf(x):\n            nonlocal remaining\n            # clip and ensure 1-d array\n            x2 = np.asarray(x, dtype=float).reshape(self.dim)\n            x2 = np.minimum(np.maximum(x2, lb), ub)\n            if remaining <= 0:\n                return np.inf, x2.copy()\n            # call objective\n            f = float(func(x2))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x2.copy()\n            return f, x2.copy()\n\n        # If budget is too small, do pure random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population (evaluate)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # initial sigma scale uses problem scale\n        global_scale = np.mean(ub - lb)\n        init_sigma = max(1e-6, 0.2 * global_scale)\n\n        for _ in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(init_sigma)\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # if we couldn't make a population (very small budget), do random search until budget exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # main loop\n        stagnation_counter = 0\n        iter_counter = 0\n        # parameters\n        tournament_k = min(3, len(pop))\n        backtrack_fracs = (0.5, 0.25, -0.5, -0.25)\n        p_jump_base = 0.04  # base probability for a Lévy jump\n        p_recombine = 0.15\n        p_rejuvenate = 0.05\n\n        while remaining > 0:\n            iter_counter += 1\n            # update some bookkeeping\n            worst_i = int(np.argmax(pop_f))\n            best_sorted_idx = np.argsort(pop_f)\n            best_i = int(best_sorted_idx[0])\n            # sometimes select second best for recombination\n            best2_idx = best_sorted_idx[:2] if len(pop) >= 2 else best_sorted_idx[:1]\n\n            # small tournament selection for parent\n            inds = np.random.choice(len(pop), size=tournament_k, replace=False)\n            vals = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(vals))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # random normalized direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochastic step-length\n            alpha = abs(sigma * (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, global_scale)\n                improved = True\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # local backtracking / small-step refinement along direction\n            for frac in backtrack_fracs:\n                if remaining <= 0:\n                    break\n                x_bt = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_bt, x_bt = callf(x_bt)\n                if f_bt < pop_f[parent_i]:\n                    pop[parent_i] = x_bt\n                    pop_f[parent_i] = f_bt\n                    pop_sigma[parent_i] = max(1e-12, sigma * (1.0 + 0.1 * np.random.randn()))\n                    improved = True\n                    stagnation_counter = 0\n                    break  # keep the better backtrack\n                else:\n                    stagnation_counter += 1\n\n            # orthogonal perturbation to diversify locally\n            if remaining > 0:\n                # produce random vector then remove component along d to get orthogonal-ish vector\n                r = np.random.randn(self.dim)\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r) + 1e-12\n                r = r / nr\n                scale = 0.6 * sigma\n                x_orth = np.clip(pop[parent_i] + scale * r, lb, ub)\n                f_orth, x_orth = callf(x_orth)\n                if f_orth < pop_f[parent_i]:\n                    pop[parent_i] = x_orth\n                    pop_f[parent_i] = f_orth\n                    pop_sigma[parent_i] = min(global_scale, sigma * 1.1)\n                    improved = True\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            # probability increases with stagnation\n            p_jump = min(0.5, p_jump_base + 0.002 * stagnation_counter)\n            if remaining > 0 and np.random.rand() < p_jump:\n                # Cauchy-like heavy-tailed vector\n                u = np.random.rand(self.dim)\n                # Cauchy via tan(pi*(u-0.5))\n                step = np.tan(np.pi * (u - 0.5))\n                # scale relative to domain and current sigma\n                step = step * (0.8 * sigma)\n                # robust normalization to avoid extreme outliers while preserving heavy-tail\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                x_jump = np.clip(pop[parent_i] + step, lb, ub)\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < pop_f[parent_i]:\n                    # replace parent\n                    pop[parent_i] = x_jump\n                    pop_f[parent_i] = f_jump\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.7)\n                    improved = True\n                    stagnation_counter = 0\n                else:\n                    # try to replace worst if jump is competitive\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n\n            # recombination exploitation: mix two best and add small noise\n            if remaining > 0 and len(pop) >= 2 and np.random.rand() < p_recombine:\n                a = pop[best2_idx[0]]\n                b = pop[best2_idx[1]] if len(best2_idx) > 1 else pop[best2_idx[0]]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_rec = np.clip(0.5 * (a + b) + noise, lb, ub)\n                f_rec, x_rec = callf(x_rec)\n                if f_rec < pop_f[parent_i]:\n                    pop[parent_i] = x_rec\n                    pop_f[parent_i] = f_rec\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.9)\n                    improved = True\n                    stagnation_counter = 0\n                else:\n                    # maybe inject into population by replacing the worst\n                    if f_rec < pop_f[worst_i]:\n                        pop[worst_i] = x_rec\n                        pop_f[worst_i] = f_rec\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                        stagnation_counter = 0\n                    else:\n                        stagnation_counter += 1\n\n            # adapt parent sigma on failure/success\n            if not improved:\n                # reduce sigma modestly to focus search\n                pop_sigma[parent_i] = max(1e-12, sigma * 0.85)\n            else:\n                pop_sigma[parent_i] = min(global_scale, pop_sigma[parent_i] * (1.0 + 0.05 * np.random.randn()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < p_rejuvenate:\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = init_sigma\n                    stagnation_counter = 0\n\n            # keep arrays consistent lengths (in case budget ended mid-ops)\n            # enforce arrays are lists of numpy arrays but lengths consistent\n            # If budget exhausted naturally, loop will exit\n\n            # occasional cleaning: if some pop entries are NaN or inf, replace\n            for i in range(len(pop)):\n                if not np.isfinite(pop_f[i]):\n                    if remaining > 0:\n                        x_r = np.random.uniform(lb, ub)\n                        f_r, x_r = callf(x_r)\n                        pop[i] = x_r\n                        pop_f[i] = f_r\n                        pop_sigma[i] = init_sigma\n                    else:\n                        pop_f[i] = np.inf\n\n            # if we are stuck for many iterations, aggressively diversify\n            if stagnation_counter > 40 and remaining > 0:\n                # replace half of the population (worst half) with randoms\n                k = max(1, len(pop) // 2)\n                worst_idxs = np.argsort(pop_f)[-k:]\n                for wi in worst_idxs:\n                    if remaining <= 0:\n                        break\n                    x_r = np.random.uniform(lb, ub)\n                    f_r, x_r = callf(x_r)\n                    pop[wi] = x_r\n                    pop_f[wi] = f_r\n                    pop_sigma[wi] = init_sigma\n                stagnation_counter = 0\n\n            # safety break if budget exhausted checked by loop condition\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.492 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13743552890887567, 0.1582273516579613, 0.8101097854164865, 0.8872260907720108, 0.23683333831309483, 0.9309655176308766, 0.2762898923360163, 0.4507315101740902, 0.8824353420917005, 0.14535237998150363]}, "task_prompt": ""}
{"id": "c4a6a21a-f085-42a2-9e0e-f1c2c17073c8", "fitness": 0.4576622535079363, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, recombination and occasional Lévy (Cauchy) escapes to balance exploitation and exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to proportional to dim)\n    - seed: optional RNG seed for reproducibility\n\n    Usage: optimizer = ADLS(budget=10000, dim=10); f_opt, x_opt = optimizer(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        # default population proportional to dimensionality but not too large\n        if pop_size is None:\n            self.pop_size = max(4, min(int(4 * self.dim), max(4, self.budget // 20)))\n        else:\n            self.pop_size = int(pop_size)\n        # state to return after call\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # Prepare bounds as full-dim arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        mean_span = float(np.mean(span))\n\n        # evaluation bookkeeping\n        self.eval_count = 0\n\n        def callf(x):\n            # Clip input to bounds, ensure correct shape\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            if self.eval_count >= self.budget:\n                # no remaining evaluations\n                raise RuntimeError(\"No evaluations remaining\")\n            f = float(func(x))\n            self.eval_count += 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, fallback to pure random search\n        if self.budget <= 2 or self.dim <= 0:\n            for _ in range(self.budget):\n                x = rng.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, max(2, self.budget // 10))\n        # initial sigma scale: a fraction of variable range\n        init_sigma = max(1e-8, 0.2 * mean_span)\n\n        for i in range(n_init):\n            if self.eval_count >= self.budget:\n                break\n            x = rng.uniform(lb, ub)\n            try:\n                f, x = callf(x)\n            except RuntimeError:\n                break\n            pop.append(x)\n            pop_f.append(f)\n            pop_sigma.append(init_sigma * (1.0 + 0.5 * rng.randn()))  # slight variance among individuals\n\n        if len(pop) == 0:\n            # budget was zero to start with; just return\n            return self.f_opt, self.x_opt\n\n        pop = np.vstack(pop)\n        pop_f = np.array(pop_f, dtype=float)\n        pop_sigma = np.maximum(1e-12, np.array(pop_sigma, dtype=float))\n\n        # Main adaptive loop\n        while self.eval_count < self.budget:\n            remaining = self.budget - self.eval_count\n            # choose small tournament size based on population and remaining budget\n            k = min(3, len(pop))\n            inds = rng.choice(len(pop), size=k, replace=False)\n            # choose parent as best of tournament (biased to better solutions)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample direction and normalize\n            d = rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # stochastic step length, log-normal multiplicative noise\n            alpha = max(1e-12, sigma * np.exp(0.1 * rng.randn()))\n\n            # Primary directional trial\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            success = False\n            if f_try < pop_f[parent_i]:\n                # Accept improvement\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.15, mean_span)\n                success = True\n            else:\n                # local backtracking / small-step refinement along same direction\n                # try a few smaller steps\n                backtrials = 3\n                for t in range(backtrials):\n                    factor = 0.5 ** (t + 1)\n                    x_bt = np.clip(x_parent + alpha * factor * d, lb, ub)\n                    if self.eval_count >= self.budget:\n                        break\n                    try:\n                        f_bt, x_bt = callf(x_bt)\n                    except RuntimeError:\n                        break\n                    if f_bt < f_try:\n                        f_try = f_bt\n                        x_try = x_bt\n                    if f_bt < pop_f[parent_i]:\n                        pop[parent_i] = x_bt\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * (1.1 + 0.05 * rng.rand()), mean_span)\n                        success = True\n                        break\n\n            # Orthogonal perturbation for diversification\n            if not success and self.eval_count < self.budget:\n                r = rng.randn(self.dim)\n                # make r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    ortho_step = 0.6 * sigma * rng.rand()\n                    x_o = np.clip(x_parent + ortho_step * r, lb, ub)\n                    try:\n                        f_o, x_o = callf(x_o)\n                    except RuntimeError:\n                        break\n                    if f_o < pop_f[parent_i]:\n                        pop[parent_i] = x_o\n                        pop_f[parent_i] = f_o\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, mean_span)\n                        success = True\n\n            # Occasional Lévy-like jump (heavy tail) to escape basins\n            if (rng.rand() < 0.08) and (self.eval_count < self.budget):\n                # sample Cauchy (standard) then scale robustly\n                levy = rng.standard_cauchy(size=self.dim)\n                # robust scale to avoid extreme outliers: divide by median(abs)\n                denom = np.median(np.abs(levy)) + 1e-12\n                step = (levy / denom) * (0.8 * mean_span * (0.5 + rng.rand()))\n                x_lev = np.clip(x_parent + step, lb, ub)\n                try:\n                    f_lev, x_lev = callf(x_lev)\n                except RuntimeError:\n                    break\n                # if improvement, replace parent; otherwise maybe inject into population replacing worst\n                if f_lev < pop_f[parent_i]:\n                    pop[parent_i] = x_lev\n                    pop_f[parent_i] = f_lev\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.2, mean_span)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_lev < pop_f[worst_i]:\n                        pop[worst_i] = x_lev\n                        pop_f[worst_i] = f_lev\n                        pop_sigma[worst_i] = max(1e-12, pop_sigma[worst_i] * 0.9)\n\n            # Recombination exploitation: mix two individuals with small noise\n            if (self.eval_count < self.budget) and (len(pop) >= 2):\n                # choose two distinct individuals biased towards better half\n                sorted_idx = np.argsort(pop_f)\n                top_half = sorted_idx[: max(2, len(sorted_idx) // 2)]\n                a, b = rng.choice(top_half, size=2, replace=False)\n                beta = 0.2 + 0.6 * rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                mix += 0.05 * mean_span * rng.randn(self.dim)  # small mutation\n                mix = np.clip(mix, lb, ub)\n                try:\n                    f_mix, mix = callf(mix)\n                except RuntimeError:\n                    break\n                # replace parent if better, else replace worst if better\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = min((pop_sigma[a] + pop_sigma[b]) * 0.6 + 1e-12, mean_span)\n                    success = True\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = max(1e-12, 0.5 * (pop_sigma[a] + pop_sigma[b]))\n\n            # Adapt sigma on failure\n            if not success:\n                # shrink sigma moderately for this parent to focus search\n                pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.92)\n            else:\n                # small chance to increase others' sigma to encourage exploration\n                if rng.rand() < 0.02:\n                    j = rng.randint(len(pop))\n                    pop_sigma[j] = min(pop_sigma[j] * (1.0 + 0.2 * rng.rand()), mean_span)\n\n            # occasional rejuvenation replacing worst with a random sample\n            if (rng.rand() < 0.015) and (self.eval_count < self.budget):\n                worst_i = int(np.argmax(pop_f))\n                x_new = rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = init_sigma * (0.5 + rng.rand())\n\n            # If population is small relative to remaining budget, allow growth: add a random candidate occasionally\n            if (len(pop) < self.pop_size) and (rng.rand() < 0.05) and (self.eval_count < self.budget):\n                x_new = rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop = np.vstack([pop, x_new])\n                pop_f = np.append(pop_f, f_new)\n                pop_sigma = np.append(pop_sigma, init_sigma * (0.5 + rng.rand()))\n\n            # Ensure arrays are kept consistent shapes if budget ended inside the loop\n            if self.eval_count >= self.budget:\n                break\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.458 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19409500345440744, 0.174316082982794, 0.5218533300858721, 0.8403396864833477, 0.5345849635592139, 0.724328047285042, 0.2835171185734081, 0.4678431121103025, 0.651925426002719, 0.1838197645422568]}, "task_prompt": ""}
{"id": "3463f6ff-002a-476a-b6b2-5067c92b759c", "fitness": 0.5661646292116307, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — maintains a modest population with per-individual adaptive step-sizes, uses randomized directional local searches, orthogonal refinements, recombination and occasional Lévy-like jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size; by default scales modestly with dimension\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # modest population that grows a bit with dimension\n        if pop_size is None:\n            self.pop_size = max(4, min(40, 6 + int(self.dim / 2)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        # determine bounds (Many BBOB uses func.bounds.lb / ub); fallback to [-5,5]\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n\n        # ensure correct shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # internal state\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            # if no evals left, return inf (no new evaluation)\n            if remaining <= 0:\n                return np.inf\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x.copy())  # call black-box\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # fallback random search if budget is extremely small\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initial population\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x = lb + self.rng.rand(self.dim) * (ub - lb)\n            f = callf(x)\n            pop.append(x.copy())\n            pop_f.append(f)\n            # individual sigma around base with slight variation\n            pop_sigma.append(base_sigma * (0.5 + self.rng.rand()))\n        pop = np.array(pop) if len(pop) else np.zeros((0, self.dim))\n        pop_f = np.array(pop_f) if len(pop_f) else np.array([])\n        pop_sigma = np.array(pop_sigma) if len(pop_sigma) else np.array([])\n\n        # if no population was possible (very small budget), fallback to pure random sampling\n        if pop.shape[0] == 0:\n            while remaining > 0:\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # helper to find worst and best indices\n        def best_indices(k=2):\n            idx = np.argsort(pop_f)\n            return idx[:k] if idx.size else np.array([], dtype=int)\n\n        # main loop\n        while remaining > 0:\n            # choose a parent via small tournament to balance exploration/exploitation\n            k = min(3, pop.shape[0])\n            cand_idx = self.rng.choice(pop.shape[0], size=k, replace=False)\n            # pick best among candidates\n            parent_i = cand_idx[np.argmin(pop_f[cand_idx])]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d)\n            d /= max(nd, 1e-12)\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, 1.0 + 0.3 * self.rng.randn())  # vary step length\n            x_try = np.minimum(np.maximum(x_parent + alpha * d, lb), ub)\n            f_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, np.mean(ub - lb))\n                continue  # go to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            backtrack_factors = [0.5, 0.25, 0.1]\n            improved = False\n            for bf in backtrack_factors:\n                if remaining <= 0:\n                    break\n                x_try = np.minimum(np.maximum(x_parent + (alpha * bf) * d, lb), ub)\n                f_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.08, np.mean(ub - lb))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            # construct orthogonal vector to d\n            r = self.rng.randn(self.dim)\n            # subtract projection on d\n            r -= d * np.dot(r, d)\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r /= nr\n                x_try = np.minimum(np.maximum(x_parent + 0.6 * sigma * r, lb), ub)\n                f_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, np.mean(ub - lb))\n                    continue\n                else:\n                    # maybe replace worst if orthogonal is strong\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(pop_sigma[worst_i] * 0.9, 1e-12)\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.06 and remaining > 0:\n                # Cauchy-like heavy-tailed scalar\n                # sample from standard Cauchy via tan(pi*(u-0.5))\n                u = self.rng.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))\n                step = cauchy * (0.6 * sigma) * self.rng.randn(self.dim)\n                # normalize to robust scale to avoid truly infinite jumps but keep heavy-tail\n                denom = max(1e-8, np.median(np.abs(step)) * (1.0 + self.rng.rand()))\n                step = step / denom\n                # scale into bounds reasonable\n                step = np.clip(step, -0.5 * (ub - lb), 0.5 * (ub - lb))\n                x_jump = np.minimum(np.maximum(x_parent + step, lb), ub)\n                f_jump = callf(x_jump)\n                if f_jump < np.max(pop_f):\n                    # replace worst\n                    worst_i = int(np.argmax(pop_f))\n                    pop[worst_i] = x_jump.copy()\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(0.5 * sigma, 1e-12)\n                    continue\n                # else, maybe keep as candidate by replacing parent on moderate success\n                if f_jump < pop_f[parent_i]:\n                    pop[parent_i] = x_jump.copy()\n                    pop_f[parent_i] = f_jump\n                    pop_sigma[parent_i] = max(0.8 * sigma, 1e-12)\n                    continue\n\n            # recombination exploitation: mix two best and small noise\n            if self.rng.rand() < 0.25 and pop.shape[0] >= 2 and remaining > 0:\n                bi = best_indices(2)\n                if bi.size >= 2:\n                    a, b = bi[0], bi[1]\n                    w = self.rng.beta(2.0, 2.0)  # biased average\n                    offspring = w * pop[a] + (1 - w) * pop[b]\n                    noise = (0.02 * (ub - lb)) * self.rng.randn(self.dim)\n                    offspring = np.minimum(np.maximum(offspring + noise, lb), ub)\n                    f_off = callf(offspring)\n                    # replace parent if improved, else maybe replace worst\n                    if f_off < pop_f[parent_i]:\n                        pop[parent_i] = offspring.copy()\n                        pop_f[parent_i] = f_off\n                        pop_sigma[parent_i] = np.mean([pop_sigma[a], pop_sigma[b]]) * 0.9\n                        continue\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_off < pop_f[worst_i]:\n                            pop[worst_i] = offspring.copy()\n                            pop_f[worst_i] = f_off\n                            pop_sigma[worst_i] = np.mean([pop_sigma[a], pop_sigma[b]]) * 0.9\n                            continue\n\n            # adapt parent sigma on failure\n            pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.97)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < 0.04 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = lb + self.rng.rand(self.dim) * (ub - lb)\n                f_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.5 + self.rng.rand())\n\n            # if population stagnates (no improvement for many steps), try injecting a new random\n            # simple heuristic: if sigma of parent too small, nudge it\n            if pop_sigma[parent_i] < 1e-6 * np.mean(ub - lb):\n                pop_sigma[parent_i] = base_sigma * (0.3 + 0.7 * self.rng.rand())\n\n            # if population size dropped unexpectedly (shouldn't), reinitialize a random individual\n            if pop.shape[0] < self.pop_size and remaining > 0:\n                # try to grow back population if budget\n                needed = min(self.pop_size - pop.shape[0], remaining)\n                for _ in range(needed):\n                    x_new = lb + self.rng.rand(self.dim) * (ub - lb)\n                    f_new = callf(x_new)\n                    pop = np.vstack([pop, x_new.reshape(1, -1)])\n                    pop_f = np.append(pop_f, f_new)\n                    pop_sigma = np.append(pop_sigma, base_sigma * (0.5 + self.rng.rand()))\n                    if remaining <= 0:\n                        break\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.566 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16074450982305022, 0.15545920833497173, 0.8151556168112384, 0.9403671293189665, 0.8837870045526438, 0.8774401379022843, 0.2530983696646205, 0.5776483953175329, 0.8472559990135516, 0.15068992137744786]}, "task_prompt": ""}
{"id": "f30b0861-d44f-4820-a567-ca954640533e", "fitness": 0.20773104220839894, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer that combines per-individual adaptive step-sizes, randomized directional line searches, orthogonal refinements and occasional heavy-tailed (Cauchy/Lévy-like) jumps to both exploit local valleys and escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-adapted if None)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        if seed is not None:\n            np.random.seed(seed)\n\n        # Outputs (set after run)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        # Obtain bounds from func (assumed available as arrays)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.repeat(lb, self.dim)\n            ub = np.repeat(ub, self.dim)\n        # ensure correct shapes\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        # Internal state\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n        def callf(x):\n            # Clip, ensure shape, and evaluate function while counting budget\n            if self.evals >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is extremely small, fallback to pure random search\n        if self.budget <= max(20, 2 * self.dim):\n            # quick random sampling\n            try:\n                while self.evals < self.budget:\n                    x = np.random.uniform(lb, ub)\n                    callf(x)\n            except StopIteration:\n                pass\n            return self.f_opt, self.x_opt\n\n        # Determine population size adaptively\n        if self.pop_size is None:\n            # scale with dimension, keep modest\n            pop = min(20, max(4, int(4 + self.dim // 2)))\n            # but ensure we can evaluate initial population comfortably\n            pop = min(pop, max(2, self.budget // (2 * self.dim + 1)))\n            self.pop_size = pop\n        else:\n            self.pop_size = max(2, int(self.pop_size))\n            self.pop_size = min(self.pop_size, max(2, self.budget // (2 * self.dim + 1)))\n\n        # Initialize population\n        X = np.empty((self.pop_size, self.dim), dtype=float)\n        F = np.empty(self.pop_size, dtype=float)\n        Sigma = np.empty(self.pop_size, dtype=float)\n\n        # Base scale from bounds\n        base_scale = np.mean(ub - lb)\n        init_sigma = max(1e-6, 0.15 * base_scale)  # initial step-size scale\n\n        # Try to fill population, but gracefully degrade if budget low\n        for i in range(self.pop_size):\n            if self.evals >= self.budget:\n                # If we cannot evaluate more, shrink population\n                self.pop_size = i\n                X = X[:i]\n                F = F[:i]\n                Sigma = Sigma[:i]\n                break\n            x = np.random.uniform(lb, ub)\n            f, x = callf(x)\n            X[i] = x\n            F[i] = f\n            Sigma[i] = init_sigma * (1.0 + 0.3 * np.random.randn())  # small per-individual variation\n            Sigma[i] = max(1e-8, Sigma[i])\n\n        if self.pop_size == 0:\n            # fallback safety\n            try:\n                while self.evals < self.budget:\n                    x = np.random.uniform(lb, ub)\n                    callf(x)\n            except StopIteration:\n                pass\n            return self.f_opt, self.x_opt\n\n        # Main loop: iterate using remaining budget\n        try:\n            while self.evals < self.budget:\n                remaining = self.budget - self.evals\n                # Pick parent by small tournament to balance exploration/exploitation\n                tour_size = min(3, self.pop_size)\n                contenders = np.random.choice(self.pop_size, tour_size, replace=False)\n                parent_idx = contenders[np.argmin(F[contenders])]\n                x_parent = X[parent_idx].copy()\n                f_parent = F[parent_idx]\n                sigma_parent = Sigma[parent_idx]\n\n                # Random search direction (normalized)\n                d = np.random.normal(size=self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n                d /= nd\n\n                # Primary directional trial: stochastic step-length\n                step_multiplier = np.exp(0.2 * np.random.randn())  # lognormal noise\n                step_len = sigma_parent * step_multiplier\n                x_trial = x_parent + step_len * d\n                f_trial, x_trial = callf(x_trial)\n                if f_trial < f_parent:\n                    # success: accept and slightly increase sigma\n                    X[parent_idx] = x_trial\n                    F[parent_idx] = f_trial\n                    Sigma[parent_idx] = sigma_parent * (1.12 + 0.02 * np.random.randn())\n                    Sigma[parent_idx] = max(1e-8, Sigma[parent_idx])\n                    # continue exploring along the same direction with a few refinements\n                    # small backtracking / line search-like refinements\n                    for _ in range(3):\n                        if self.evals >= self.budget:\n                            break\n                        step_len *= 0.7\n                        x_ref = X[parent_idx] + step_len * d\n                        f_ref, x_ref = callf(x_ref)\n                        if f_ref < F[parent_idx]:\n                            X[parent_idx] = x_ref\n                            F[parent_idx] = f_ref\n                            Sigma[parent_idx] *= 1.05\n                        else:\n                            Sigma[parent_idx] *= 0.98\n                else:\n                    # failure: reduce sigma a bit\n                    Sigma[parent_idx] = max(1e-8, sigma_parent * 0.92)\n\n                # Orthogonal perturbation: diversify locally\n                if self.evals < self.budget:\n                    v = np.random.normal(size=self.dim)\n                    # make v orthogonal to d\n                    v -= np.dot(v, d) * d\n                    nv = np.linalg.norm(v)\n                    if nv > 1e-12:\n                        v /= nv\n                        orth_step = Sigma[parent_idx] * 0.6 * (0.5 + np.random.rand())\n                        x_orth = X[parent_idx] + orth_step * v\n                        f_orth, x_orth = callf(x_orth)\n                        # accept if improves, else maybe replace worst\n                        if f_orth < F[parent_idx]:\n                            X[parent_idx] = x_orth\n                            F[parent_idx] = f_orth\n                            Sigma[parent_idx] *= 1.06\n                        else:\n                            # if better than worst, replace worst\n                            worst_idx = np.argmax(F)\n                            if f_orth < F[worst_idx]:\n                                X[worst_idx] = x_orth\n                                F[worst_idx] = f_orth\n                                Sigma[worst_idx] = Sigma[parent_idx] * 0.8\n\n                # Occasional Lévy-like (Cauchy) jump to escape basins\n                if np.random.rand() < 0.06 and self.evals < self.budget:\n                    # Draw heavy-tailed vector\n                    c = np.random.standard_cauchy(size=self.dim)\n                    # robust scale to avoid absolute explosion\n                    robust = np.median(np.abs(c)) + 1e-6\n                    levy_scale = max(1e-6, Sigma[parent_idx] * (3.0 + 2.0 * np.random.rand()))\n                    delta = (c / robust) * levy_scale\n                    # clip overall magnitude relative to bounds\n                    max_mag = np.linalg.norm(ub - lb) * 1.5\n                    mag = np.linalg.norm(delta)\n                    if mag > max_mag:\n                        delta = delta / mag * max_mag\n                    x_jump = X[parent_idx] + delta\n                    f_jump, x_jump = callf(x_jump)\n                    if f_jump < self.f_opt:\n                        # major improvement, replace worst with this new bright spot\n                        worst_idx = np.argmax(F)\n                        X[worst_idx] = x_jump\n                        F[worst_idx] = f_jump\n                        Sigma[worst_idx] = max(1e-8, Sigma[parent_idx] * 1.2)\n                    else:\n                        # sometimes keep as candidate replacing worst with some probability if not too bad\n                        if f_jump < np.median(F) and np.random.rand() < 0.4:\n                            worst_idx = np.argmax(F)\n                            X[worst_idx] = x_jump\n                            F[worst_idx] = f_jump\n                            Sigma[worst_idx] = Sigma[parent_idx] * 0.9\n\n                # Recombination exploitation: mix two best\n                if self.evals < self.budget:\n                    # find two best\n                    idxs_sorted = np.argsort(F)\n                    a, b = idxs_sorted[0], idxs_sorted[1] if self.pop_size > 1 else idxs_sorted[0]\n                    w = 0.3 + 0.4 * np.random.rand()\n                    x_recomb = w * X[a] + (1 - w) * X[b] + 0.02 * Sigma[a] * np.random.randn(self.dim)\n                    f_recomb, x_recomb = callf(x_recomb)\n                    # replace worst if good, or replace parent if improved\n                    worst_idx = np.argmax(F)\n                    if f_recomb < F[parent_idx]:\n                        X[parent_idx] = x_recomb\n                        F[parent_idx] = f_recomb\n                        Sigma[parent_idx] = (Sigma[a] + Sigma[b]) * 0.5\n                    elif f_recomb < F[worst_idx]:\n                        X[worst_idx] = x_recomb\n                        F[worst_idx] = f_recomb\n                        Sigma[worst_idx] = (Sigma[a] + Sigma[b]) * 0.5\n\n                # Adapt parent sigma on persistent failure (small chance to expand if stagnating)\n                if self.evals < self.budget:\n                    # small chance to increase exploration for a random individual\n                    if np.random.rand() < 0.02:\n                        j = np.random.randint(0, self.pop_size)\n                        Sigma[j] *= 1.6\n                    # shrink very large sigmas to keep search meaningful\n                    for j in range(self.pop_size):\n                        Sigma[j] = min(Sigma[j], np.linalg.norm(ub - lb) * 2.0)\n\n                # Occasional population rejuvenation\n                if np.random.rand() < 0.03 and self.evals < self.budget:\n                    worst_idx = np.argmax(F)\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    X[worst_idx] = x_new\n                    F[worst_idx] = f_new\n                    Sigma[worst_idx] = init_sigma * (0.8 + 0.4 * np.random.rand())\n\n                # If budget is nearly exhausted, finish quickly\n                if (self.budget - self.evals) < max(1, self.dim // 2):\n                    # explore remaining budget with local tweaks\n                    while self.evals < self.budget:\n                        j = np.random.randint(0, self.pop_size)\n                        x_try = X[j] + 0.5 * Sigma[j] * np.random.randn(self.dim)\n                        callf(x_try)\n                    break\n\n        except StopIteration:\n            # Budget exhausted; tidy up\n            pass\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.208 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08372406361121976, 0.17380001037891635, 0.6218870107964314, 0.15874080595122708, 0.21323996010158797, 0.1916080747421245, 0.20448989360004444, 0.11913756202463233, 0.17081286259248607, 0.13987017828531945]}, "task_prompt": ""}
{"id": "a8eed18f-7b2c-43aa-9e08-7c510b128dbf", "fitness": 0.4375878190434056, "name": "ADLS", "description": "Adaptive Directional Lévy Search — a population-based adaptive step-size directional local search with orthogonal refinements and occasional normalized Lévy-like jumps to balance exploitation and global escapes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a modest size depending on dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = int(pop_size) if pop_size is not None else max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read and normalize bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n        # safety: if func provided different dimensionality, adjust\n        if lb.size != self.dim or ub.size != self.dim:\n            # try to infer dim from bounds\n            self.dim = lb.size\n        # state\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper evaluator that enforces budget and tracks best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # quick exit if no budget\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population with random samples (uniform)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        # base sigma scale relative to domain size\n        base_sigma = max(1e-12, 0.25 * np.mean(np.abs(ub - lb)))\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # fallback: if budget was extremely small and no initial individuals, do a single random eval\n        if len(pop) == 0:\n            if remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: directional local search + orthogonal tries + Levy jumps + recombination + rejuvenation\n        while remaining > 0:\n            # pick a parent via a small tournament\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), size=k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(np.abs(ub - lb)))\n                continue\n\n            # local backtracking / small-step refinement along the same direction\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d  # make r orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(np.abs(ub - lb)))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (normalized heavy-tailed vector)\n            if np.random.rand() < 0.08 and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization to retain heavy-tail but avoid infinities\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # replace worst if improved\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                continue  # go to next iteration after jump attempt\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = 0.5 + 0.3 * (np.random.rand() - 0.5)  # mix coefficient around 0.5\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on general failure to improve\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional small random rejuvenation\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished or budget exhausted\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.438 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12033001946622246, 0.16580449640108097, 0.5796207052503259, 0.8882636021296436, 0.23136601839038196, 0.9205076552030166, 0.24780030432008704, 0.40093080565661054, 0.6846127311874268, 0.1366418524292602]}, "task_prompt": ""}
{"id": "fc7be782-132b-41ed-9d06-a1c91104c8e1", "fitness": 0.5070625504509936, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a small adaptive population that performs randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy jumps to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n\n    def __call__(self, func):\n        # Setup RNG\n        rng = np.random.RandomState(self.seed)\n\n        # Get bounds and ensure full-dim arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # Safety: clip bounds\n        lb = np.minimum(lb, ub)\n        ub = np.maximum(lb, ub)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Helper evaluation wrapper that enforces budget and tracks best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = float(func(x))\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget exhausted already\n        if remaining <= 0:\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialization: sample an initial population\n        pop = []         # list of numpy arrays\n        pop_f = []       # corresponding objective values\n        pop_sigma = []   # step-size per individual\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            # sample uniformly in box (vectorized)\n            x0 = lb + rng.rand(self.dim) * (ub - lb)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * rng.rand()))\n\n        # If no population could be created, do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = lb + rng.rand(self.dim) * (ub - lb)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop\n        iter_count = 0\n        max_sigma = np.mean(ub - lb)\n        p_jump = 0.08  # probability of a heavy-tailed jump\n        rejuvenate_every = max(10, int(5 + self.pop_size / 2))\n\n        while remaining > 0:\n            iter_count += 1\n\n            # Select a parent via small tournament\n            k = min(3, len(pop))\n            inds = rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # Generate a random search direction (normalized)\n            d = rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # extremely unlikely, skip this iteration\n                pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                continue\n            d = d / nd\n\n            # Primary directional trial with stochastic step-length\n            alpha = sigma * (1.0 + 0.5 * rng.randn())\n            x_try = x_parent + alpha * d\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, max_sigma)\n                continue  # move to next iteration (exploit success)\n\n            # Local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, 0.125):\n                if remaining <= 0:\n                    break\n                x_try = x_parent + alpha * frac * d\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # Try an orthogonal perturbation for local diversification\n            r = rng.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = x_parent + 0.6 * sigma * r\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, max_sigma)\n                        continue\n\n            # Occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if rng.rand() < p_jump and remaining > 0:\n                step = rng.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = x_parent + step * scale_vec\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # replace the worst in population if better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * rng.rand())\n                    # continue to next iteration to allow local search after jump\n                    continue\n\n            # Recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * rng.randn(self.dim)\n                x_try = mix + noise\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # Adapt parent sigma on failure (conservative shrink)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n\n            # Occasional population rejuvenation by replacing the worst with a random sample\n            if iter_count % rejuvenate_every == 0 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = lb + rng.rand(self.dim) * (ub - lb)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * rng.rand())\n\n        # finished or budget exhausted\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.507 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12054592146603016, 0.15585005857139356, 0.8451495775232305, 0.9500227336071821, 0.2870677488682758, 0.9330235110595947, 0.2414429028356967, 0.5031258519422193, 0.9000600189160286, 0.13433717972028392]}, "task_prompt": ""}
{"id": "52c47f0b-0eb6-4581-921f-520e98cb1c8e", "fitness": 0.41133766430069746, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive local searches using randomized directions, orthogonal refinements and occasional heavy-tailed Lévy/Cauchy jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by default)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # Get bounds and ensure they are arrays of correct dimension\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        if lb.size != self.dim or ub.size != self.dim:\n            # fallback: assume [-5,5] if mismatch (Many Affine BBOB uses [-5,5])\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # initialize tracking\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure numpy array and clip to bounds\n            x = np.array(x, dtype=float)\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is too small, fallback to random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale relative to box\n        n_init = min(self.pop_size, remaining)  # cannot init more than remaining evals\n        for i in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize different sigmas for diversity\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.rand()))\n\n        # If no population could be created, do pure random search with whatever budget\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Ensure lists are numpy-friendly\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        # parameters\n        tournament_k = max(2, min(len(pop), 3))\n        levy_prob = 0.08  # probability of heavy-tailed jump each iteration\n        rejuvenation_prob = 0.02\n        max_iters = 10**9  # just loop until budget exhausted\n\n        it = 0\n        while remaining > 0 and it < max_iters:\n            it += 1\n            # pick a parent via small tournament to balance exploration/exploitation\n            inds = self.rng.choice(len(pop), size=tournament_k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * self.rng.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                improved = True\n                # continue to next iteration to exploit\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 1.1)\n                    improved = True\n                    break\n\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.randn(self.dim)\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        improved = True\n\n            if improved:\n                continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < levy_prob and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = self.rng.standard_cauchy(self.dim)\n                # robust normalization: avoid one huge coordinate dominating\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.25 * (ub - lb) * (0.5 + self.rng.rand())  # randomize jump scale\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    # if jump is excellent, inject into parent\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(1e-12, sigma * 0.8)\n                # continue main loop\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                # identify two best\n                best_idx = np.argsort(pop_f)[:2]\n                a, b = best_idx[0], best_idx[1]\n                mix = 0.5 * (pop[a] + pop[b])\n                beta = self.rng.rand()\n                noise = (0.01 * (ub - lb)) * self.rng.randn(self.dim)\n                x_try = np.clip(mix + noise * beta, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.9)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (slightly increase to encourage exploration)\n            # if no improvement occurred in this cycle, modestly inflate sigma\n            if not improved:\n                pop_sigma[parent_i] = min(np.mean(ub - lb), pop_sigma[parent_i] * (1.03 + 0.02 * self.rng.randn()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < rejuvenation_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * self.rng.rand())\n\n            # ensure we keep list sizes consistent (no-op but safe)\n            if len(pop) != len(pop_f) or len(pop) != len(pop_sigma):\n                # reconstruct minimal consistent population\n                m = min(len(pop), len(pop_f), len(pop_sigma))\n                pop = pop[:m]\n                pop_f = pop_f[:m]\n                pop_sigma = pop_sigma[:m]\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.411 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11301264725804971, 0.15567433200468916, 0.49833314638732795, 0.9083860814649608, 0.44889402189431993, 0.5918393907175631, 0.2570357374771596, 0.4382041902645907, 0.5594781259215522, 0.14251896961676103]}, "task_prompt": ""}
{"id": "efcf7373-3f78-45e5-bacb-ccc0284fb4a3", "fitness": 0.5062807213813183, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy/Cauchy jumps to robustly explore/exploit continuous landscapes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None it's set based on dim)\n    - seed: random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest scaling with dimensionality\n            self.pop_size = max(4, int(6 + np.ceil(1.5 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.RandomState(seed)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # determine bounds (try to use func.bounds if present, otherwise use [-5,5])\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub =  5.0 * np.ones(self.dim)\n\n        # ensure bounds arrays have correct shape\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # budget bookkeeping via closure\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds for safety\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # trivial exit\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        span = np.mean(ub - lb)\n        base_sigma = max(1e-8, 0.25 * span)\n\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            # sample uniformly in bounds\n            x0 = lb + self.rng.rand(self.dim) * (ub - lb)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n            # if budget exhausted break\n            if remaining <= 0:\n                break\n\n        # if no population (very small budget), fallback to random sampling until budget consumed\n        if len(pop) == 0:\n            while remaining > 0:\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # ensure population arrays are numpy arrays for easier indexing (list okay too)\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop\n        while remaining > 0:\n            # pick a parent via small tournament\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * self.rng.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            # attempt evaluation if budget available\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.2, span)\n                    continue  # go to next iteration to exploit success\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                # create orthogonal vector by removing component along d\n                r = self.rng.randn(self.dim)\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    r = self.rng.randn(self.dim)\n                    r = r - np.dot(r, d) * d\n                    nr = np.linalg.norm(r) + 1e-12\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, span)\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # generate Cauchy-like heavy-tailed step vector\n                u = self.rng.rand(self.dim) - 0.5\n                # use tan(pi*(u)) to get Cauchy; clip extremes\n                step = np.tan(np.pi * np.clip(u, -0.499999, 0.499999))\n                denom = np.mean(np.abs(step)) + 1e-12\n                step = (step / denom) * (span * (0.5 + self.rng.rand()))\n                x_try = np.clip(x_parent + step, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population, else maybe keep it as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                # pick two best\n                best_inds = np.argsort(pop_f)[:2]\n                a, b = best_inds[0], best_inds[1]\n                mix = 0.6 * pop[a] + 0.4 * pop[b]\n                noise = (0.01 * (ub - lb)) * self.rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (no improvement this round)\n            # reduce sigma slightly to encourage finer search\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = lb + self.rng.rand(self.dim) * (ub - lb)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n\n            # if population became smaller than configured (shouldn't normally happen), reinitialize some\n            if len(pop) < self.pop_size and remaining > 0:\n                x_new = lb + self.rng.rand(self.dim) * (ub - lb)\n                f_new, x_new = callf(x_new)\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.506 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10171735238033319, 0.14554539895155627, 0.7588007428911456, 0.9496688319283496, 0.5393295729423822, 0.9567583396301755, 0.24358793323442018, 0.4528514967163285, 0.7625991148286209, 0.1519484303098716]}, "task_prompt": ""}
{"id": "5311e548-e735-418c-8c8e-e311b810b72c", "fitness": 0.339514411241726, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy-style jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a modest function of dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        if pop_size is None:\n            # modest population that grows slowly with dimension\n            self.pop_size = max(4, min(30, int(6 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read and normalize bounds to full-dim arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # safety: ensure shapes\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            # evaluate while tracking budget and clipping to bounds\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # extremely small budgets fallback to pure random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population with uniform samples until budget or population is full\n        pop = []\n        pop_f = []\n        pop_sigma = []  # per-individual step-scale\n        n_init = min(self.pop_size, remaining)\n        # initial sigma is a fraction of domain size\n        domain_scale = float(np.mean(ub - lb))\n        init_sigma = max(1e-8, 0.25 * domain_scale)\n\n        for _ in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(init_sigma)\n\n        # if we could not create any individual due to extremely small budget, return best found\n        if len(pop) == 0:\n            return self.f_opt, self.x_opt\n\n        # main loop: use remaining budget for directional local searches, orthogonal tries, Lévy jumps and recombination\n        while remaining > 0:\n            # pick a parent via a small tournament (k-way)\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                # rare degenerate case, retry\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            # step-length draws a mild multiplicative noise to adapt\n            step_len = sigma * max(1e-12, 1.0 + 0.4 * np.random.randn())\n            x_try = x_parent + step_len * d\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(np.mean(ub - lb), sigma * 1.12)\n                continue\n\n            # local backtracking: reduce step a few times\n            improved = False\n            backtrack_factor = 0.5\n            for t in range(3):\n                step_len *= backtrack_factor\n                x_try = x_parent + step_len * d\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.97)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation to diversify locally\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d (Gram-Schmidt)\n            proj = np.dot(r, d) * d\n            r = r - proj\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = x_parent + 0.6 * sigma * r\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(np.mean(ub - lb), sigma * 1.08)\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(size=self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale should allow escape but be bounded by domain scale\n                levy_scale = domain_scale * (0.5 + 2.0 * np.random.rand()) * (sigma / (domain_scale + 1e-12))\n                x_try = x_parent + levy_scale * step\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try.copy()\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after leap attempt\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and np.random.rand() < 0.6:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b] + 0.06 * sigma * np.random.randn(self.dim)\n                try:\n                    f_mix, x_mix = callf(mix)\n                except RuntimeError:\n                    break\n                # Replace parent if improved, else try to inject into worst\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix.copy()\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix.copy()\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.7)\n                continue\n\n            # adapt parent sigma on failure (slightly reduce)\n            pop_sigma[parent_i] = max(1e-12, sigma * 0.85)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = max(1e-12, 0.5 * domain_scale)\n\n        # finished budget or loop termination\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.340 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10749634399446939, 0.15971943654512066, 0.8653850723402825, 0.1720235088924288, 0.24723080923169094, 0.9581995034273125, 0.25583190491107166, 0.2947792620922943, 0.21377899106431675, 0.12069927991827234]}, "task_prompt": ""}
{"id": "72549e5e-47b3-4dbf-a81c-9d8bf5bbdc51", "fitness": 0.4245447004840478, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based directional local searches with orthogonal refinements and occasional Lévy (Cauchy) jumps, adapting per-point step-sizes to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by default)\n    - seed: random seed (optional)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        # population scales with dimensionality but stays modest\n        self.pop_size = (pop_size if pop_size is not None\n                         else max(4, min(30, int(4 + 2 * np.sqrt(self.dim)))))\n        # placeholders for result\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n    def __call__(self, func):\n        # determine bounds (many benchmarks put bounds in func.bounds)\n        if hasattr(func, \"bounds\"):\n            try:\n                lb = np.array(func.bounds.lb, dtype=float)\n                ub = np.array(func.bounds.ub, dtype=float)\n            except Exception:\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure lb/ub are arrays of correct shape\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        # base sigma scale relative to box size\n        base_sigma = 0.2 * np.maximum(1e-12, ub - lb)\n\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            # don't call if no budget\n            if remaining <= 0:\n                raise RuntimeError(\"No remaining budget for evaluations\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            nonlocal_update = False\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, do simple random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # Try to create up to pop_size individuals (stop if budget exhausted)\n        for _ in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            # per-dimension sigma represented as scalar scale * base_sigma vector's mean\n            sigma0 = np.mean(base_sigma) * (0.8 + 0.4 * np.random.rand())\n            pop_sigma.append(float(sigma0))\n            pop_f.append(float(f0))\n\n        # If no population could be created (very small budget), do pure random search until budget ends\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Ensure lists are numpy-friendly\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # main loop\n        iters = 0\n        # parameters\n        p_jump = 0.07  # base probability for Lévy jump\n        rejuvenate_every = max(10, min(40, int(5 + self.dim / 2)))\n        while remaining > 0:\n            iters += 1\n\n            # compute current best/worst indices\n            best_i = int(np.argmin(pop_f))\n            worst_i = int(np.argmax(pop_f))\n            # small tournament selection\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = int(inds[int(np.argmin(values))])\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = float(pop_sigma[parent_i])  # scalar\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                continue\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            step_len = max(1e-8, np.abs(np.random.normal(loc=1.0, scale=0.6)))\n            x_try = np.clip(x_parent + sigma * step_len * d, lb, ub)\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < f_parent:\n                    # accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = float(f_try)\n                    pop_sigma[parent_i] = min(float(np.mean(ub - lb)), sigma * 1.2)\n                    # slight local exploitation: try to push further in same direction\n                    for extra_scale in (1.5, 2.0):\n                        if remaining <= 0:\n                            break\n                        x_far = np.clip(x_parent + sigma * step_len * extra_scale * d, lb, ub)\n                        f_far, x_far = callf(x_far)\n                        if f_far < pop_f[parent_i]:\n                            pop[parent_i] = x_far\n                            pop_f[parent_i] = float(f_far)\n                            pop_sigma[parent_i] = min(float(np.mean(ub - lb)), sigma * 1.3)\n                    continue  # go to next iteration\n                else:\n                    # failure: will try backtracking and orthogonal moves below\n                    pass\n\n            # local backtracking / small-step refinement along direction (few tries)\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + sigma * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = float(f_try)\n                    pop_sigma[parent_i] = min(float(np.mean(ub - lb)), sigma * 1.15)\n                    break  # improvement, go to next major iteration\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = float(f_try)\n                        pop_sigma[parent_i] = min(float(np.mean(ub - lb)), sigma * 1.1)\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and np.random.rand() < p_jump:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                sn = np.linalg.norm(step)\n                if sn < 1e-12:\n                    sn = 1.0\n                # scale vector to a reasonable fraction of box\n                scale_vec = 0.2 * (ub - lb)\n                # scale by a random heavy-tailed scalar to keep variability\n                heavy_scale = max(0.05, np.random.pareto(1.2) * 0.5)\n                x_jump = np.clip(x_parent + (step / sn) * heavy_scale * scale_vec, lb, ub)\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < pop_f[parent_i]:\n                    pop[parent_i] = x_jump\n                    pop_f[parent_i] = float(f_jump)\n                    pop_sigma[parent_i] = max(1e-12, sigma * (1.0 + 0.5 * heavy_scale))\n                else:\n                    # if jump is better than worst, replace worst to rejuvenate population\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = float(f_jump)\n                        pop_sigma[worst_i] = float(np.mean(base_sigma)) * (0.6 + 0.8 * np.random.rand())\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0 and len(pop) >= 2:\n                sorted_ids = np.argsort(pop_f)\n                a = pop[int(sorted_ids[0])]\n                b = pop[int(sorted_ids[1])]\n                beta = np.random.rand()\n                mix = (1 - beta) * a + beta * b\n                noise = 0.05 * sigma * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # if good, replace parent or a worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = float(f_try)\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.9)\n                else:\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = float(f_try)\n                        pop_sigma[worst_i] = float(np.mean(base_sigma)) * (0.7 + 0.6 * np.random.rand())\n\n            # adapt parent sigma on failure (slightly reduce to focus)\n            pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.92)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (iters % rejuvenate_every == 0 or np.random.rand() < 0.03):\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = float(f_new)\n                    pop_sigma[worst_i] = float(np.mean(base_sigma)) * (0.7 + 0.6 * np.random.rand())\n\n            # cap and regularize all sigmas to keep stability\n            for i in range(len(pop_sigma)):\n                pop_sigma[i] = float(np.clip(pop_sigma[i], 1e-12, np.mean(ub - lb)))\n\n            # reduce jump probability slowly to focus more locally as budget decreases\n            p_jump = max(0.01, p_jump * 0.999)\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.425 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17744109846978606, 0.161514712631322, 0.9127324361034646, 0.9437518991132499, 0.27696360886881977, 0.1697255251941292, 0.23243304646584162, 0.3627218272569358, 0.8644747299619735, 0.1436881207749554]}, "task_prompt": ""}
{"id": "ac51c66a-8d55-4393-b0f1-92e97e15cd2d", "fitness": 0.34608465165933444, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining adaptive per-individual step-sizes, randomized directional local searches with backtracking and orthogonal refinement, occasional Lévy-like jumps for escape, and small recombination/rejuvenation to maintain diversity.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_size: number of population members (default scales with dim)\n    - seed: RNG seed for reproducibility\n    - init_sigma: initial step-scale relative to search range (default 0.25)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None, init_sigma=0.25):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimension but stays reasonable\n            self.pop_size = int(min(max(6, 4 + 2 * self.dim), 60))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed) if seed is not None else np.random.RandomState()\n        self.init_sigma = float(init_sigma)\n\n    def __call__(self, func):\n        # Determine bounds (support functions that supply bounds and fallback to [-5,5])\n        try:\n            lb_raw = func.bounds.lb\n            ub_raw = func.bounds.ub\n            lb = np.asarray(lb_raw, dtype=float).reshape(self.dim)\n            ub = np.asarray(ub_raw, dtype=float).reshape(self.dim)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Ensure shapes\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # search range\n        rng_range = ub - lb\n        base_sigma = max(1e-12, self.init_sigma * max(1e-12, np.mean(rng_range)))\n\n        # bookkeeping\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper wrapper to evaluate while tracking remaining budget\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                # no more budget, return +inf and do not call the function\n                return np.inf\n            # ensure numpy and clipped\n            x = np.asarray(x, dtype=float)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f\n\n        # If budget very small, do pure random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        if remaining < self.pop_size:\n            # fallback to random search for tiny budgets\n            for _ in range(remaining):\n                x = self.rng.uniform(lb, ub)\n                _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        for i in range(self.pop_size):\n            x = self.rng.uniform(lb, ub)\n            f = callf(x)\n            pop.append(np.asarray(x, dtype=float))\n            pop_f.append(float(f))\n            pop_sigma.append(base_sigma)\n            if remaining <= 0:\n                break\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n\n        # If no population could be created (extremely small budget), do nothing\n        if pop.shape[0] == 0:\n            return self.f_opt, self.x_opt\n\n        # parameters\n        tournament_k = min(3, pop.shape[0])\n        orthogonal_frac = 0.5\n        backtrack_fracs = [0.5, 0.25, 0.125]  # fractions for backtracking\n        levy_prob = 0.07  # occasional heavy-tailed jump probability\n        rejuvenate_prob = 0.03  # occasional replace worst with random\n        recombine_prob = 0.12  # try recombination occasionally\n        max_iters = 10**9  # safety; actual limit is remaining evaluations\n\n        # main loop\n        iters = 0\n        stagnation_counts = np.zeros(pop.shape[0], dtype=int)  # track failures per individual\n        while remaining > 0 and iters < max_iters:\n            iters += 1\n\n            # pick parent via small tournament\n            candidates = self.rng.randint(0, pop.shape[0], size=tournament_k)\n            parent_i = candidates[np.argmin(pop_f[candidates])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random normalized search direction\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # fallback to isotropic\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            step_scale = sigma * (1.0 + 0.2 * self.rng.randn())  # small noise on step length\n            x_try = np.clip(x_parent + step_scale * d, lb, ub)\n            f_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # success: accept, increase sigma slightly\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = max(1e-12, sigma * 1.15)\n                stagnation_counts[parent_i] = 0\n                continue  # next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            accepted = False\n            for frac in backtrack_fracs:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + frac * step_scale * d, lb, ub)\n                f_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.95)\n                    stagnation_counts[parent_i] = 0\n                    accepted = True\n                    break\n            if accepted:\n                continue\n\n            # orthogonal perturbation for local diversification\n            # create an orthogonal vector by subtracting projection onto d\n            v = self.rng.randn(self.dim)\n            v = v - np.dot(v, d) * d\n            nv = np.linalg.norm(v)\n            if nv > 1e-12:\n                v = v / nv\n                x_try = np.clip(x_parent + orthogonal_frac * sigma * v, lb, ub)\n                f_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 1.05)\n                    stagnation_counts[parent_i] = 0\n                    continue\n\n            # occasional Lévy-like jump to escape local basins\n            if self.rng.rand() < levy_prob and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = self.rng.standard_cauchy(self.dim)\n                # robust scaling to keep step sizes reasonable but heavy-tailed\n                scale_vec = 0.2 * rng_range\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try = callf(x_try)\n                # if it's good replace the worst in population\n                worst_i = int(np.argmax(pop_f))\n                worst_f = pop_f[worst_i]\n                if f_try < worst_f:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    # optionally also accept as parent replacement\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                else:\n                    # maybe keep as candidate by replacing parent if slightly better than parent\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                # continue after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if self.rng.rand() < recombine_prob and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                # arithmetic blend plus small Gaussian perturbation\n                child = 0.6 * pop[a] + 0.4 * pop[b] + self.rng.randn(self.dim) * (0.05 * rng_range)\n                child = np.clip(child, lb, ub)\n                f_child = callf(child)\n                if f_child < pop_f[parent_i]:\n                    pop[parent_i] = child\n                    pop_f[parent_i] = f_child\n                    pop_sigma[parent_i] = max(1e-12, sigma * 1.05)\n                    stagnation_counts[parent_i] = 0\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_child < pop_f[worst_i]:\n                        pop[worst_i] = child\n                        pop_f[worst_i] = f_child\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                continue\n\n            # adapt parent sigma on failure\n            stagnation_counts[parent_i] += 1\n            pop_sigma[parent_i] = max(1e-12, sigma * 0.85)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < rejuvenate_prob and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma\n\n            # if an individual stagnates for many tries, nudge it with a medium random perturbation\n            if stagnation_counts[parent_i] > 8 and remaining > 0:\n                nudge = self.rng.randn(self.dim) * (0.1 * rng_range)\n                x_new = np.clip(x_parent + nudge, lb, ub)\n                f_new = callf(x_new)\n                if f_new < pop_f[parent_i]:\n                    pop[parent_i] = x_new\n                    pop_f[parent_i] = f_new\n                    pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 1.1)\n                    stagnation_counts[parent_i] = 0\n                else:\n                    pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.7)\n                    stagnation_counts[parent_i] = 0  # reset to allow future attempts\n\n            # safety: if budget exhausted, break\n            if remaining <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.346 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1290224136734358, 0.1702173354346247, 0.498152885409504, 0.6262491793769089, 0.26560697942473377, 0.7790752646661367, 0.2522659040058455, 0.32955707535181633, 0.2744274643393786, 0.13627201491095942]}, "task_prompt": ""}
{"id": "cc789ecf-09a5-4a4c-a877-a8c47aa83db0", "fitness": 0.40814063237857184, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a small population of adaptive step-size probes using randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy) jumps to balance fast local exploitation and heavy-tailed exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_size: override population size\n    - seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = None if pop_size is None else int(pop_size)\n        self.seed = seed\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # Get bounds (fall back to [-5,5] if not provided)\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # If bounds are scalars expand them\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        # Safe clip utility\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # internal state\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # remaining evaluations\n        remaining = int(self.budget)\n\n        # wrapped evaluation to track budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No remaining budget for function evaluations\")\n            x = np.asarray(x, dtype=float)\n            x = clip(x)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # Very small budget: fallback to pure random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n        if remaining < 5:\n            # random draws until budget exhausted\n            while remaining > 0:\n                x = rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Determine population size (modest, scales with dim and budget)\n        if self.pop_size is None:\n            self.pop_size = int(max(3, min(20, 3 + self.dim // 2, max(3, remaining // 10))))\n        self.pop_size = max(1, min(self.pop_size, remaining))  # cannot exceed remaining evals\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n        for i in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize sigma with some noise so individuals diversify\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * rng.rand()))\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # If for some reason no population created (very tight budget), random search remaining\n        if len(pop) == 0:\n            while remaining > 0:\n                x = rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main optimization loop\n        # parameters\n        p_levy = 0.06  # prob of heavy-tailed jump\n        p_rejuvenate = 0.03\n        beta_ortho = 0.5  # orthogonal step factor relative to sigma\n        decay_on_fail = 0.90\n        grow_on_success = 1.10\n        backtrack_tries = 3\n\n        # While budget remains, perform directional local searches and occasional jumps\n        while remaining > 0:\n            # pick a parent by small tournament selection\n            k = min(3, len(pop))\n            idxs = rng.choice(len(pop), k, replace=False)\n            parent_i = idxs[np.argmin([pop_f[i] for i in idxs])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random direction\n            d = rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-16:\n                d = rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-16\n            d /= nd\n\n            # stochasticized step-length (positive)\n            alpha = sigma * max(1e-12, 1.0 + 0.6 * rng.randn())\n            x_try = clip(x_parent + alpha * d)\n\n            # Primary directional trial\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                # success\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(np.mean(ub - lb), sigma * grow_on_success)\n                    continue  # continue main loop\n\n            # Local backtracking / small-step refinement along the same direction\n            improved = False\n            small_alpha = alpha * 0.5\n            for bt in range(backtrack_tries):\n                if remaining <= 0:\n                    break\n                x_bt = clip(x_parent + (small_alpha * (0.5 ** bt)) * d)\n                f_bt, x_bt = callf(x_bt)\n                if f_bt < pop_f[parent_i]:\n                    pop[parent_i] = x_bt\n                    pop_f[parent_i] = f_bt\n                    pop_sigma[parent_i] = min(np.mean(ub - lb), sigma * grow_on_success)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # Try an orthogonal perturbation for local diversification\n            # produce a random vector and remove its projection on d\n            r = rng.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_ortho = clip(x_parent + beta_ortho * sigma * r)\n                if remaining > 0:\n                    f_ortho, x_ortho = callf(x_ortho)\n                    if f_ortho < pop_f[parent_i]:\n                        pop[parent_i] = x_ortho\n                        pop_f[parent_i] = f_ortho\n                        pop_sigma[parent_i] = min(np.mean(ub - lb), sigma * grow_on_success)\n                        continue\n                    else:\n                        # slightly increase sigma for orthogonal failure sometimes to encourage escape\n                        pop_sigma[parent_i] = max(1e-12, sigma * 0.98)\n\n            # Occasional Lévy-like jump to escape local basins (Cauchy heavy-tailed)\n            if remaining > 0 and rng.rand() < p_levy:\n                # sample heavy-tailed step via Cauchy (ratio of normals)\n                z = rng.standard_cauchy(self.dim)\n                # robust scale: mix individual's sigma and global spread\n                robust_scale = max(1e-12, sigma + 0.5 * np.mean(ub - lb) * rng.rand())\n                delta = z\n                # normalize heavy-tailed vector to avoid purely infinite leaps but keep tailness\n                md = np.median(np.abs(delta)) + 1e-12\n                delta = delta / md\n                x_jump = clip(x_parent + robust_scale * delta)\n                if remaining > 0:\n                    f_jump, x_jump = callf(x_jump)\n                    # if jump good, replace worst; else cool-down parent's sigma\n                    if f_jump < np.max(pop_f):\n                        worst_i = int(np.argmax(pop_f))\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    else:\n                        pop_sigma[parent_i] = max(1e-12, sigma * 0.6)\n                    continue\n\n            # Recombination exploitation: mix two best with small noise\n            if len(pop) >= 2:\n                best_idxs = np.argsort(pop_f)[:2]\n                a = pop[best_idxs[0]]\n                b = pop[best_idxs[1]]\n                child = clip(0.6 * a + 0.4 * b + (0.01 * (ub - lb)) * rng.randn(self.dim))\n                if remaining > 0:\n                    f_child, child = callf(child)\n                    # replace parent if improved, else replace worst if better\n                    if f_child < pop_f[parent_i]:\n                        pop[parent_i] = child\n                        pop_f[parent_i] = f_child\n                        pop_sigma[parent_i] = min(np.mean(ub - lb), sigma * grow_on_success)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_child < pop_f[worst_i]:\n                            pop[worst_i] = child\n                            pop_f[worst_i] = f_child\n                            pop_sigma[worst_i] = max(1e-12, sigma * 0.7)\n\n            # Adapt parent sigma on failure (if we reached here no improvement)\n            pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * decay_on_fail)\n\n            # Occasional population rejuvenation by replacing worst with a random sample\n            if remaining > 0 and rng.rand() < p_rejuvenate:\n                worst_i = int(np.argmax(pop_f))\n                x_new = rng.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * rng.rand())\n\n            # If population shrank due to budget exhaustion, continue random sampling until budget gone\n            # (This is handled by while loop)\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.408 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.06475345638962582, 0.16438993789216916, 0.7071218193378077, 0.9235150882564981, 0.24488608154036307, 0.9551995451064245, 0.2151739509746694, 0.40533488762650716, 0.23065118879945712, 0.17038036786219657]}, "task_prompt": ""}
{"id": "6fe28ad3-30db-473a-8808-a2a6e5c749cc", "fitness": 0.21816712157705626, "name": "ADLS", "description": "Population-based adaptive directional search mixing randomized directional local moves, orthogonal refinements and occasional Lévy jumps to balance robust local exploitation and heavy-tailed exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled if None)\n    - seed: RNG seed for reproducibility\n    One-line idea: adaptive directional local searches around a small population, orthogonal refinements and occasional heavy-tailed Lévy jumps to escape basins.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size_arg = pop_size\n        self.seed = seed\n        # RNG (RandomState is fine for portability)\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        # bounds handling: func.bounds.lb/ub may be scalars or sequences\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        # ensure shapes\n        lb = lb.ravel()[:self.dim].astype(float)\n        ub = ub.ravel()[:self.dim].astype(float)\n\n        # bookkeeping\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            \"\"\"Clip, evaluate (if budget remains), update best, and count eval.\"\"\"\n            x = np.asarray(x, dtype=float).ravel()[:self.dim]\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            if self.evals >= self.budget:\n                return np.inf\n            # evaluate\n            f = float(func(x))\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                # store a copy to avoid external mutation\n                self.x_opt = x.copy()\n            return f\n\n        # if budget is extremely tight, fallback to simple random search\n        if self.budget <= 2:\n            # sample randomly up to budget\n            for _ in range(self.budget):\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # determine population size adaptively but modest\n        if self.pop_size_arg is None:\n            # scale with dimensionality but leave most budget for iterations\n            pop_size = int(np.clip(4 + int(2 * np.sqrt(self.dim)), 4, 40))\n        else:\n            pop_size = int(max(1, self.pop_size_arg))\n\n        # ensure population does not consume too much of budget at init\n        max_init = max(1, self.budget // 6)\n        pop_size = min(pop_size, max_init)\n\n        # initialize population uniformly in bounds\n        pop = np.empty((pop_size, self.dim), dtype=float)\n        pop_f = np.empty(pop_size, dtype=float)\n        # initial step sizes: proportional to domain size\n        domain_scale = np.maximum(1e-12, (ub - lb).mean())\n        pop_sigma = np.empty(pop_size, dtype=float)\n\n        for i in range(pop_size):\n            x = lb + self.rng.rand(self.dim) * (ub - lb)\n            pop[i] = x\n            pop_sigma[i] = domain_scale * (0.25 + 0.5 * self.rng.rand())  # adaptive initial sigma\n            pop_f[i] = callf(x)\n            if pop_f[i] == np.inf:\n                # budget exhausted during initialization\n                return self.f_opt, self.x_opt\n\n        # If somehow no population (shouldn't happen), fallback to single point\n        if pop_size == 0:\n            return self.f_opt, self.x_opt\n\n        # Main loop: directional moves, orthogonal tweaks, Lévy jumps, recombination\n        while self.evals < self.budget:\n            # small tournament selection to pick parent (balance exploration/exploitation)\n            tour_k = min(3, pop.shape[0])\n            inds = self.rng.choice(pop.shape[0], tour_k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.normal(size=self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-16:\n                # degenerate draw, skip a bit\n                continue\n            d /= nd\n\n            # primary directional trial with a randomized step-length\n            step_scale = sigma * (0.4 + 1.2 * self.rng.rand())  # stochasticized length\n            x_try = x_parent + d * step_scale\n            f_try = callf(x_try)\n            if f_try == np.inf:\n                break\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = sigma * 1.10  # slightly increase sigma on success\n                continue  # next generation immediately\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for t in range(3):\n                small_factor = 0.5 ** (t + 1)\n                jitter = 0.6 + 0.8 * self.rng.rand()\n                x_try = x_parent + d * step_scale * small_factor * jitter\n                f_try = callf(x_try)\n                if f_try == np.inf:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * (1.04 - 0.01*t), 1e-12)\n                    improved = True\n                    break\n            if f_try == np.inf:\n                break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            v = self.rng.normal(size=self.dim)\n            # make orthogonal to d\n            v -= np.dot(v, d) * d\n            nv = np.linalg.norm(v)\n            if nv > 1e-12:\n                v /= nv\n                ortho_scale = sigma * (0.3 + 0.7 * self.rng.rand())\n                x_try = x_parent + v * ortho_scale\n                f_try = callf(x_try)\n                if f_try == np.inf:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = sigma * 1.05\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.07:\n                # Cauchy-like heavy-tailed vector\n                step = self.rng.standard_cauchy(size=self.dim)\n                # normalize by a robust scale (90th percentile) to avoid numerical explosions\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                levy_scale = sigma * (1.5 + 3.0 * self.rng.rand())\n                step = (step / denom) * levy_scale\n                x_try = x_parent + step\n                f_try = callf(x_try)\n                if f_try == np.inf:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    # replace the worst with this promising jump\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(sigma * 0.6, 1e-12)\n                    # continue main loop\n                    continue\n                # else keep trying next strategies\n\n            # recombination exploitation: mix two best and add small noise\n            if pop.shape[0] >= 2:\n                best_two = np.argsort(pop_f)[:2]\n                b1, b2 = best_two[0], best_two[1]\n                mix = 0.6 + 0.3 * self.rng.rand()\n                child = mix * pop[b1] + (1.0 - mix) * pop[b2] + self.rng.normal(scale=0.05 * sigma, size=self.dim)\n                f_child = callf(child)\n                if f_child == np.inf:\n                    break\n                if f_child < pop_f[parent_i]:\n                    pop[parent_i] = child\n                    pop_f[parent_i] = f_child\n                    pop_sigma[parent_i] = sigma * 1.07\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_child < pop_f[worst_i]:\n                        pop[worst_i] = child\n                        pop_f[worst_i] = f_child\n                        pop_sigma[worst_i] = max(sigma * 0.6, 1e-12)\n\n            # adapt parent sigma on failure (reduce to encourage local search)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < 0.02 and self.evals < self.budget:\n                worst_i = int(np.argmax(pop_f))\n                x_new = lb + self.rng.rand(self.dim) * (ub - lb)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = callf(x_new)\n                if pop_f[worst_i] == np.inf:\n                    break\n\n        # finished (budget exhausted or loop ended)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.218 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15580204021989807, 0.16786285120485644, 0.45843989470894575, 0.21335075407274007, 0.2546978116081575, 0.15898046990359294, 0.18698963567219617, 0.20471070007155967, 0.20802877774837847, 0.17280828056023745]}, "task_prompt": ""}
{"id": "7887c134-f8cc-4fab-bf68-f2371c4c5048", "fitness": 0.5477676714647746, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining randomized directional local searches, orthogonal refinements, adaptive per-individual step-sizes, occasional Lévy (Cauchy) escapes and light recombination to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: random seed for reproducibility\n\n    Usage: instantiate with budget and dim, then call with a blackbox 'func' that accepts a 1D numpy array.\n    The func is expected to provide bounds as func.bounds.lb and func.bounds.ub (scalars or arrays).\n    If bounds are not provided, [-5, 5] is assumed for all dimensions.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        if pop_size is None:\n            # moderate population scaling with dim\n            self.pop_size = max(4, min(20, int(2 * self.dim)))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        # parameters controlling behaviour\n        self.tournament_k = 3\n        self.sigma_init = 0.5  # initial step size\n        self.sigma_max = 5.0\n        self.sigma_min = 1e-8\n        self.levy_prob = 0.08\n        self.rejuv_prob = 0.03\n        self.orth_try_scale = 0.3\n        self.backtrack_tries = 3\n        self.recomb_prob = 0.12\n\n    def __call__(self, func):\n        # obtain bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # default bounds per problem statement\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # if provided bounds are scalars, expand\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()), dtype=float)\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # internal tracking\n        evals_used = 0\n        f_best = np.inf\n        x_best = None\n\n        # wrapper for calling func safely and counting budget\n        def callf(x):\n            nonlocal evals_used, f_best, x_best\n            if evals_used >= self.budget:\n                return None  # budget exhausted\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            evals_used += 1\n            if f is None:\n                # in case the blackbox returns None for some reason\n                return None\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x.copy()\n\n        # Fallback to simple random search if budget extremely small\n        if self.budget <= 4 or self.dim <= 0:\n            # very small budget -> random samples\n            while evals_used < self.budget:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return f_best, x_best\n\n        # Initialize population: positions and per-individual sigma\n        pop = []\n        for i in range(self.pop_size):\n            if evals_used >= self.budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            res = callf(x)\n            if res is None:\n                break\n            f, x_clipped = res\n            sigma = self.sigma_init * (1.0 + 0.2 * self.rng.randn())  # small randomization\n            sigma = float(np.clip(abs(sigma), 1e-6, self.sigma_max))\n            pop.append({'x': x_clipped, 'f': float(f), 'sigma': sigma})\n\n        # If population couldn't be initialized, do pure random search with remaining budget\n        if len(pop) == 0:\n            while evals_used < self.budget:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return f_best, x_best\n\n        # sort population by fitness ascending\n        pop.sort(key=lambda p: p['f'])\n\n        # helper to get worst index\n        def worst_idx():\n            worst = 0\n            worst_f = pop[0]['f']\n            for i, p in enumerate(pop):\n                if p['f'] > worst_f:\n                    worst_f = p['f']\n                    worst = i\n            return worst\n\n        # Main optimization loop\n        while evals_used < self.budget:\n            # pick a parent via small tournament (better individuals more likely)\n            k = min(self.tournament_k, len(pop))\n            candidates_idx = self.rng.choice(len(pop), size=k, replace=False)\n            # choose the best among sampled\n            parent_idx = min(candidates_idx, key=lambda i: pop[i]['f'])\n            parent = pop[parent_idx]\n            x_parent = parent['x'].copy()\n            sigma = float(parent['sigma'])\n\n            # sample a random search direction\n            v = self.rng.randn(self.dim)\n            v_norm = np.linalg.norm(v)\n            if v_norm == 0:\n                v = np.ones(self.dim)\n                v_norm = np.linalg.norm(v)\n            v = v / v_norm  # unit direction\n\n            # primary directional trial with stochasticized step-length\n            # multiplicative log-normal factor and uniform random magnitude to encourage variability\n            rand_scale = float(np.exp(0.5 * self.rng.randn()))\n            magnitude = float(sigma * rand_scale * (0.5 + self.rng.rand()))\n            x_trial = x_parent + magnitude * v\n            res = callf(x_trial)\n            if res is None:\n                break\n            f_trial, x_evaled = res\n\n            if f_trial < parent['f']:\n                # success: accept and slightly increase sigma\n                parent['x'] = x_evaled\n                parent['f'] = f_trial\n                parent['sigma'] = float(min(self.sigma_max, sigma * 1.06 + 1e-12))\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                improved = False\n                mag = magnitude\n                for bt in range(self.backtrack_tries):\n                    mag *= 0.5\n                    x_bt = x_parent + mag * v\n                    res2 = callf(x_bt)\n                    if res2 is None:\n                        improved = False\n                        break\n                    f_bt, x_evaled2 = res2\n                    if f_bt < parent['f']:\n                        parent['x'] = x_evaled2\n                        parent['f'] = f_bt\n                        parent['sigma'] = float(min(self.sigma_max, sigma * 1.03 + 1e-12))\n                        improved = True\n                        break\n                if not improved:\n                    # try an orthogonal perturbation for local diversification\n                    # create a random vector, project out v to make it (approximately) orthogonal\n                    r = self.rng.randn(self.dim)\n                    r -= np.dot(r, v) * v\n                    rn = np.linalg.norm(r)\n                    if rn < 1e-12:\n                        r = np.random.randn(self.dim)\n                        r -= np.dot(r, v) * v\n                        rn = np.linalg.norm(r) + 1e-12\n                    r = r / rn\n                    orth_step = sigma * self.orth_try_scale * (0.5 + self.rng.rand())\n                    x_orth = x_parent + orth_step * r\n                    res3 = callf(x_orth)\n                    if res3 is None:\n                        break\n                    f_orth, x_evaled3 = res3\n                    if f_orth < parent['f']:\n                        parent['x'] = x_evaled3\n                        parent['f'] = f_orth\n                        parent['sigma'] = float(min(self.sigma_max, sigma * 1.04 + 1e-12))\n                    else:\n                        # failure: adapt sigma downward slightly\n                        parent['sigma'] = float(max(self.sigma_min, sigma * 0.985))\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < self.levy_prob and evals_used < self.budget:\n                # sample a Cauchy (standard) for magnitude and random direction\n                u = self.rng.rand()\n                cauchy = np.tan(np.pi * (u - 0.5))\n                # robust scale: median(|pop_f - median|) style using positions' spread\n                pos_stack = np.vstack([p['x'] for p in pop])\n                spread = np.median(np.sqrt(np.sum((pos_stack - np.median(pos_stack, axis=0)) ** 2, axis=1))) + 1e-8\n                # create heavy-tailed vector with normalized direction\n                d = self.rng.randn(self.dim)\n                dn = np.linalg.norm(d)\n                if dn == 0:\n                    d = np.ones(self.dim)\n                    dn = np.linalg.norm(d)\n                d = d / dn\n                jump_magnitude = float(np.clip(abs(cauchy) * (spread + 1e-3), 1e-6, 10.0 * self.sigma_max))\n                x_jump = x_parent + jump_magnitude * d\n                resj = callf(x_jump)\n                if resj is None:\n                    break\n                f_jump, x_evaled_jump = resj\n                if f_jump < pop[-1]['f']:\n                    # replace the worst in population\n                    idx_w = worst_idx()\n                    pop[idx_w] = {'x': x_evaled_jump, 'f': f_jump, 'sigma': float(min(self.sigma_max, jump_magnitude * 0.5 + 1e-12))}\n                    pop.sort(key=lambda p: p['f'])\n                else:\n                    # keep as candidate: maybe insert replacing worst with probability proportional to improvement\n                    if self.rng.rand() < 0.1:\n                        idx_w = worst_idx()\n                        if f_jump < pop[idx_w]['f']:\n                            pop[idx_w] = {'x': x_evaled_jump, 'f': f_jump, 'sigma': float(min(self.sigma_max, jump_magnitude * 0.5 + 1e-12))}\n                            pop.sort(key=lambda p: p['f'])\n\n            # recombination exploitation: mix two best and small noise\n            if self.rng.rand() < self.recomb_prob and evals_used < self.budget and len(pop) >= 2:\n                parents_idx = [0, 1]  # best two\n                a = self.rng.rand()\n                x_mix = a * pop[parents_idx[0]]['x'] + (1 - a) * pop[parents_idx[1]]['x']\n                # add small gaussian noise relative to their sigmas\n                sigma_mix = 0.5 * (pop[parents_idx[0]]['sigma'] + pop[parents_idx[1]]['sigma'])\n                noise = self.rng.randn(self.dim) * (0.1 * sigma_mix)\n                x_mix += noise\n                resm = callf(x_mix)\n                if resm is None:\n                    break\n                f_mix, x_evaled_mix = resm\n                if f_mix < parent['f']:\n                    # replace parent\n                    parent['x'] = x_evaled_mix\n                    parent['f'] = f_mix\n                    parent['sigma'] = float(max(self.sigma_min, sigma_mix * 1.02))\n                else:\n                    # maybe replace worst\n                    idx_w = worst_idx()\n                    if f_mix < pop[idx_w]['f']:\n                        pop[idx_w] = {'x': x_evaled_mix, 'f': f_mix, 'sigma': float(sigma_mix)}\n                        pop.sort(key=lambda p: p['f'])\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < self.rejuv_prob and evals_used < self.budget:\n                x_new = self.rng.uniform(lb, ub)\n                resr = callf(x_new)\n                if resr is None:\n                    break\n                f_new, x_evaled_new = resr\n                idx_w = worst_idx()\n                if f_new < pop[idx_w]['f']:\n                    pop[idx_w] = {'x': x_evaled_new, 'f': f_new, 'sigma': float(self.sigma_init)}\n                    pop.sort(key=lambda p: p['f'])\n\n            # keep population sorted and enforce sigma bounds\n            for p in pop:\n                p['sigma'] = float(np.clip(p['sigma'], self.sigma_min, self.sigma_max))\n            pop.sort(key=lambda p: p['f'])\n\n            # small safeguard: if budget very low, break\n            if self.budget - evals_used <= 0:\n                break\n\n        # final return\n        return f_best, x_best", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.548 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14888696125314016, 0.15709749527882744, 0.8199175775963118, 0.908548139265499, 0.502947437110769, 0.8929922587630121, 0.27729986807132756, 0.7858895894082232, 0.8626864460960264, 0.1214109418046102]}, "task_prompt": ""}
{"id": "b2280024-3ae3-4baf-a12b-d661b13b1054", "fitness": 0.4033923946987258, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining adaptive per-agent step-sizes, randomized directional local searches, orthogonal refinements, occasional heavy-tailed Lévy-like jumps, and light recombination to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    High-level idea:\n    - Maintain a modest population of candidate solutions each with its own adaptive step-size (sigma).\n    - Repeatedly perform randomized directional local searches (primary trial + backtracking),\n      orthogonal refinements, occasional heavy-tailed Lévy-like jumps to escape basins,\n      and light recombination / population rejuvenation.\n    - Track remaining evaluation budget strictly and update global best.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # population scales with dimensionality but stays modest unless explicitly requested\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # prepare bounds as full-dimension arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # enforce shapes\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = max(0, int(self.budget))\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # safe evaluation wrapper: clips x, calls func, updates best and remaining budget\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x_clip = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x_clip))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x_clip.copy()\n            return f, x_clip\n\n        # if no budget, return immediately\n        if remaining <= 0:\n            return self.f_opt, (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (if budget too small, fallback to random search)\n        pop_n = min(self.pop_size, remaining)\n        pop = np.zeros((pop_n, self.dim), dtype=float)\n        pop_f = np.full(pop_n, np.inf, dtype=float)\n        pop_sigma = np.zeros(pop_n, dtype=float)\n\n        # initial base sigma is a fraction of the domain size\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n\n        for i in range(pop_n):\n            if remaining <= 0:\n                break\n            # draw uniformly within bounds\n            x0 = lb + self.rng.random(self.dim) * (ub - lb)\n            f0, x0 = callf(x0)\n            pop[i, :] = x0\n            pop_f[i] = f0\n            # diversify initial sigmas a bit\n            pop_sigma[i] = base_sigma * (0.8 + 0.4 * self.rng.random())\n\n        # if no population created (very small budget), do pure random sampling for remaining budget\n        if pop.shape[0] == 0:\n            while remaining > 0:\n                x = lb + self.rng.random(self.dim) * (ub - lb)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main optimization loop\n        while remaining > 0:\n            # small tournament selection to choose a parent index\n            tsize = min(3, pop.shape[0])\n            inds = self.rng.choice(pop.shape[0], size=tsize, replace=False)\n            best_local = inds[int(np.argmin(pop_f[inds]))]\n            parent_i = int(best_local)\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = self.rng.normal(size=self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                # if degenerate, sample a coordinate direction\n                d = np.zeros(self.dim)\n                d[self.rng.integers(self.dim)] = 1.0\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.normal())\n            alpha = float(np.clip(alpha, 1e-12, np.max(ub - lb)))  # keep within sensible scale\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n            else:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.normal(size=self.dim)\n            r = r - np.dot(r, d) * d  # make orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12 and remaining > 0:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.random() < 0.08 and remaining > 0:\n                step = self.rng.standard_cauchy(self.dim)\n                denom = float(np.percentile(np.abs(step), 90) + 1e-12)\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best individuals and add small noise\n            if pop.shape[0] >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = float(self.rng.random())\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # small gaussian nudging\n                mix = np.clip(mix + 0.05 * sigma * self.rng.normal(size=self.dim), lb, ub)\n                f_mix, x_mix = callf(mix)\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (slight contraction)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and self.rng.random() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = lb + self.rng.random(self.dim) * (ub - lb)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.random())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.403 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08719816642187683, 0.15707479016244996, 0.7998935825555753, 0.9762534910813042, 0.24559039622674084, 0.7925596726760102, 0.2252926820582234, 0.2751683199425313, 0.329534699662123, 0.14535814620042253]}, "task_prompt": ""}
{"id": "2d5ffa24-08d3-49bd-8b0e-ec8d3fa98f92", "fitness": 0.5243715215771295, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — combines randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy/Cauchy jumps with adaptive step-sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim, modest)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # sensible default population size: grows slowly with dim, bounded\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        # storage for best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds handling: func.bounds.lb / ub may be scalar or arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # evaluation wrapper to track remaining budget and best-so-far\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip before evaluating to ensure valid input\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # trivial case\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population with a few uniformly sampled points (budget permitting)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        # base sigma: fraction of domain\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(float(f0))\n            # initialize per-individual scale randomly around base_sigma\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # if we couldn't create any population (very tiny budget) do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # main optimization loop\n        while remaining > 0:\n            # small tournament selection to pick a parent\n            pop_size = len(pop)\n            k = min(max(2, int(2 + np.sqrt(self.dim) // 2)), pop_size)  # small tournament size\n            inds = np.random.choice(pop_size, k, replace=False)\n            # pick best among sampled indices\n            vals = np.array([pop_f[i] for i in inds])\n            parent_i = inds[int(np.argmin(vals))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = float(f_try)\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_local = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_local, x_local = callf(x_local)\n                except RuntimeError:\n                    break\n                if f_local < pop_f[parent_i]:\n                    pop[parent_i] = x_local.copy()\n                    pop_f[parent_i] = float(f_local)\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d by removing projection\n            proj = np.dot(r, d)\n            r = r - proj * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = float(f_try)\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like (heavy-tailed) jump to escape basins\n            if remaining > 0 and np.random.rand() < 0.5:\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization: avoid domination by single huge coordinate\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale per-dimension by fraction of domain\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # replace worst if this is better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = float(f_try)\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                        # accept that we mutated population; continue main loop\n                        continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # replace parent if improved, else possibly replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = float(f_try)\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try.copy()\n                            pop_f[worst_i] = float(f_try)\n                            pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (slight shrink)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.08:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = float(f_new)\n                pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n\n            # keep population size stable if budget allowed and some improvement happened\n            # occasionally add a fresh individual if budget still large and diversity low\n            if remaining > max(5, self.dim) and len(pop) < self.pop_size and np.random.rand() < 0.05:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop.append(x_new.copy())\n                pop_f.append(float(f_new))\n                pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n\n        # finished budget (or ran out)\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.524 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13158441436638968, 0.16757113370578658, 0.9121960971111168, 0.9364076256523789, 0.675086501034877, 0.7471605243340009, 0.26882131689859967, 0.4059739562173216, 0.8396749441689901, 0.15923870228183368]}, "task_prompt": ""}
{"id": "965a2bf4-99fc-4bb6-ab84-2f1f258a22c3", "fitness": 0.49190146289227094, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based heuristic combining directional local searches, orthogonal refinements, adaptive step-sizes and occasional heavy-tailed Lévy jumps to robustly explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by default)\n    - seed: optional RNG seed for reproducibility\n\n    The algorithm maintains a small population of solutions with adaptive step-sizes (sigma).\n    Each iteration performs a directional local search (with backtracking refinements),\n    an orthogonal perturbation, occasional Lévy-like jumps (Cauchy), and light recombination.\n    Successful moves slightly increase an individual's sigma, failures slightly decrease it.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimension\n            pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # normalize bounds to full arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        # enforce provided bounds shape\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # wrapper to evaluate while tracking budget and best\n        class BudgetExhausted(Exception):\n            pass\n\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise BudgetExhausted()\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))   # single evaluation\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # Quick fallback if budget is tiny\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # base scale relative to domain\n        domain_scale = np.maximum(1e-12, ub - lb)\n        base_sigma = max(1e-8, 0.25 * np.mean(domain_scale))  # initial scale\n\n        # initialize population\n        pop_n = min(self.pop_size, remaining)\n        pop_x = np.zeros((pop_n, self.dim), dtype=float)\n        pop_f = np.full(pop_n, np.inf)\n        pop_sigma = np.full(pop_n, base_sigma)\n        try:\n            for i in range(pop_n):\n                x0 = np.random.uniform(lb, ub)\n                f0, x0 = callf(x0)\n                pop_x[i, :] = x0\n                pop_f[i] = f0\n                # diversify sigmas a bit\n                pop_sigma[i] = base_sigma * (0.5 + np.random.rand())\n                if remaining <= 0:\n                    break\n        except BudgetExhausted:\n            # budget expired during initialization\n            return self.f_opt, self.x_opt\n\n        if pop_n == 0:\n            # no evaluations -> return\n            return self.f_opt, self.x_opt\n\n        # main optimization loop\n        try:\n            while remaining > 0:\n                # tournament selection (small) to pick parent index\n                k = min(3, pop_n)\n                inds = np.random.choice(pop_n, k, replace=False)\n                parent_i = inds[np.argmin(pop_f[inds])]\n                x_parent = pop_x[parent_i].copy()\n                sigma = pop_sigma[parent_i]\n\n                # sample a normalized random search direction\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    d = np.random.randn(self.dim)\n                    nd = np.linalg.norm(d) + 1e-12\n                d /= nd\n\n                # directional trial with stochasticized step length\n                alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n                x_try = np.clip(x_parent + alpha * d, lb, ub)\n                f_try, x_try = callf(x_try)\n\n                success = False\n                if f_try < pop_f[parent_i]:\n                    # accept improvement\n                    pop_x[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(np.mean(domain_scale), sigma * 1.15)\n                    success = True\n                else:\n                    # local backtracking / small-step refinement along same direction\n                    for frac in (0.5, 0.25, -0.5, -0.25):\n                        if remaining <= 0:\n                            break\n                        x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_i]:\n                            pop_x[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = min(np.mean(domain_scale), sigma * 1.12)\n                            success = True\n                            break\n\n                if success:\n                    # small exploitation: try a slightly longer step in same direction occasionally\n                    if remaining > 0 and np.random.rand() < 0.3:\n                        x_try = np.clip(pop_x[parent_i] + 1.3 * pop_sigma[parent_i] * d, lb, ub)\n                        f_try, x_try = callf(x_try)\n                        if f_try < pop_f[parent_i]:\n                            pop_x[parent_i] = x_try\n                            pop_f[parent_i] = f_try\n                            pop_sigma[parent_i] = min(np.mean(domain_scale), pop_sigma[parent_i] * 1.2)\n                    # continue to next iteration\n                    continue\n                else:\n                    # failure: slightly reduce sigma for parent\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.85)\n\n                # orthogonal perturbation for diversification\n                r = np.random.randn(self.dim)\n                # remove component along d to make it orthogonal\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop_x[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(np.mean(domain_scale), sigma * 1.1)\n                        continue\n                    else:\n                        pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.9)\n\n                # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n                if (np.random.rand() < 0.09) and (remaining > 0):\n                    # get a Cauchy-like vector but robustly scale it\n                    raw = np.random.standard_cauchy(self.dim)\n                    # cap extremes using percentiles and fallback\n                    cap = np.percentile(np.abs(raw), 90) + 1e-12\n                    step = raw / cap\n                    scale_vec = 0.2 * domain_scale\n                    x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    # replace worst if improvement or keep as new candidate with small sigma\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop_x[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    else:\n                        # possibly insert as a new exploratory individual replacing worst\n                        if np.random.rand() < 0.15:\n                            pop_x[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n                # recombination: mix two best and small noise\n                if pop_n >= 2 and remaining > 0:\n                    best_inds = np.argsort(pop_f)[:2]\n                    x_mix = 0.6 * pop_x[best_inds[0]] + 0.4 * pop_x[best_inds[1]]\n                    noise = (0.02 * domain_scale) * np.random.randn(self.dim)\n                    x_try = np.clip(x_mix + noise, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    # if improvement replace worst or replace parent\n                    if f_try < pop_f[parent_i]:\n                        pop_x[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.95)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop_x[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5\n\n                # occasional population rejuvenation by replacing the worst with a random or mutated sample\n                if remaining > 0 and np.random.rand() < 0.03:\n                    worst_i = int(np.argmax(pop_f))\n                    if np.random.rand() < 0.6:\n                        # pure random\n                        x_new = np.random.uniform(lb, ub)\n                        f_new, x_new = callf(x_new)\n                        pop_x[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                    else:\n                        # mutated from a random parent\n                        p = np.random.randint(pop_n)\n                        x_new = np.clip(pop_x[p] + 0.8 * base_sigma * np.random.randn(self.dim), lb, ub)\n                        f_new, x_new = callf(x_new)\n                        pop_x[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = pop_sigma[p] * (0.6 + 0.8 * np.random.rand())\n\n                # slight global population shrink/expand adaptation occasionally\n                if np.random.rand() < 0.02:\n                    # shrink high sigmas, increase low sigmas to keep diversity\n                    median_sigma = np.median(pop_sigma)\n                    for i in range(pop_n):\n                        if pop_sigma[i] > median_sigma:\n                            pop_sigma[i] = max(1e-12, pop_sigma[i] * 0.9)\n                        else:\n                            pop_sigma[i] = min(np.mean(domain_scale), pop_sigma[i] * 1.05)\n\n            # end while remaining > 0\n        except BudgetExhausted:\n            pass\n\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.492 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0907018766986325, 0.17156544624420755, 0.8682947876865746, 0.9457306342232729, 0.32483555271154974, 0.9477399269165172, 0.28603755386040963, 0.35800134433742825, 0.7630854674385382, 0.1630220388055793]}, "task_prompt": ""}
{"id": "a141baaf-8908-476f-adab-7aabb3ae5531", "fitness": 0.43127833027528395, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based optimizer combining randomized directional local searches, orthogonal refinements, adaptive per-individual step-sizes, recombination and occasional Lévy-like jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional initial population size (adaptive default based on dim)\n    - seed: optional RNG seed for reproducibility\n\n    The optimizer expects func to expose bounds as func.bounds.lb and func.bounds.ub\n    (as in the example). It will not call the function more times than `budget`.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaled by dim but not too large\n            self.pop_size = max(4, min(20, 4 + self.dim // 2))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(int(seed))\n\n    def __call__(self, func):\n        # extract bounds, allow scalar bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        # bookkeeping\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # call wrapper to ensure budget and track best\n        def callf(x):\n            nonlocal remaining, lb, ub\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small: pure random sampling\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n        if remaining < 5:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        # per-individual step sizes\n        base_sigma = 0.08 * (ub - lb)  # vector scale\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)  # cannot create more members than evaluations left\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # sigma is a scalar representing fraction of range (keep nonzero)\n            pop_sigma.append(max(1e-12, np.mean(base_sigma) * (0.5 + np.random.rand())))\n            if remaining <= 0:\n                break\n\n        # if population empty due to tiny budget, fallback to random search (already handled above)\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # Main loop\n        while remaining > 0:\n            # small tournament selection: pick k individuals and choose a parent (better is more likely)\n            k = min(max(2, int(2 + np.log1p(len(pop)))), len(pop))\n            inds = np.random.choice(len(pop), size=k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = int(inds[int(np.argmin(values))])  # pick best among tournament\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = max(np.linalg.norm(d), 1e-12)\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            # success\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                # increase sigma modestly (cap by domain)\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.2, np.mean(ub - lb))\n                continue  # continue main loop, using updated individual\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            proj = np.dot(r, d) * d\n            r_orth = r - proj\n            nr = np.linalg.norm(r_orth)\n            if nr > 1e-12:\n                r_orth = r_orth / nr\n                # scale orthogonal step to a fraction of sigma and domain\n                orth_step = max(1e-12, np.mean(ub - lb)) * 0.02 * (1.0 + np.random.rand()) * (pop_sigma[parent_i] / (np.mean(pop_sigma) + 1e-12))\n                x_try = np.clip(x_parent + orth_step * r_orth, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, np.mean(ub - lb))\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                step = np.random.standard_cauchy(size=self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace the worst if improved\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(np.mean(base_sigma) * 0.5, 1e-12)\n                # also possibly accept to parent if very good\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = 0.6 + 0.3 * (np.random.rand() - 0.5)  # bias toward slightly favoring best\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # add small gaussian noise proportional to average sigma\n                noise = np.random.randn(self.dim) * (0.2 * np.mean(pop_sigma))\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(np.mean(base_sigma) * 0.5, 1e-12)\n                else:\n                    # unsuccessful recombination slightly reduces parent's sigma (conservative)\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n\n            # adapt parent sigma on failure (mildly increase to encourage exploration)\n            pop_sigma[parent_i] = min(np.mean(ub - lb), pop_sigma[parent_i] * (1.02 + 0.02 * np.random.rand()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma.mean() * (0.7 + 0.6 * np.random.rand())\n\n            # if population lost members somehow (shouldn't happen), refill slightly\n            while len(pop) < self.pop_size and remaining > 0:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma.mean() * (0.5 + np.random.rand()))\n\n            # keep population reasonably sized (prune if necessary by keeping best)\n            if len(pop) > self.pop_size:\n                order = np.argsort(pop_f)[:self.pop_size]\n                pop = [pop[i] for i in order]\n                pop_f = [pop_f[i] for i in order]\n                pop_sigma = [pop_sigma[i] for i in order]\n\n        # finished budget or ran out of evaluations\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.431 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14736891363655313, 0.15958464048622512, 0.7361875543805497, 0.9503345981886432, 0.8951423916906017, 0.514849985804465, 0.26505269397098163, 0.2577395344953296, 0.23114859839038826, 0.15537439170910206]}, "task_prompt": ""}
{"id": "96d7db01-0c72-49ad-a390-de8df129ec7b", "fitness": 0.5588623746943661, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # scale modestly with dimensionality\n            self.pop_size = int(max(4, min(40, 6 + 2 * int(np.sqrt(self.dim)))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # get bounds from the provided function wrapper (following example style)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback to default [-5,5] if wrapper doesn't provide bounds\n            lb = -5.0 * np.ones(self.dim, dtype=float)\n            ub = 5.0 * np.ones(self.dim, dtype=float)\n\n        # ensure bounds are full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # safety: ensure shapes correct\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search (still via callf)\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # --- Initialize population ---\n        # Number of initial individuals limited by budget\n        n_init = min(self.pop_size, max(1, remaining))\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base sigma scale relative to bounds\n        base_scale = 0.05 * (ub - lb)          # vector scale\n        base_sigma_scalar = float(np.mean(ub - lb)) * 0.05\n\n        for i in range(n_init):\n            x0 = lb + np.random.rand(self.dim) * (ub - lb)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # per-individual scalar step size (positive)\n            pop_sigma.append(max(1e-12, base_sigma_scalar * (0.5 + np.random.rand())))\n\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            return self.f_opt, self.x_opt\n\n        # update actual population size\n        self.pop_size = len(pop)\n\n        # Main loop\n        # Heuristic control parameters\n        max_backtracks = 3\n        orth_prob = 0.25\n        jump_base_prob = 0.05\n        rejuvenation_prob = 0.02\n        recomb_prob = 0.2\n\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            # choose best among tournament\n            values = [pop_f[i] for i in inds]\n            parent_rel = int(np.argmin(values))\n            parent_i = inds[parent_rel]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d_unit = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d_unit, lb, ub)\n            success = False\n\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.25, np.mean(ub - lb))\n                continue  # move to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            for bt in range(max_backtracks):\n                frac = 0.5 ** (bt + 1)\n                x_try = np.clip(x_parent + alpha * frac * d_unit, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    remaining = 0\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                    success = True\n                    break\n            if success:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if np.random.rand() < orth_prob and remaining > 0:\n                # generate random orthogonal vector\n                r = np.random.randn(self.dim)\n                # make orthogonal to d_unit\n                r = r - np.dot(r, d_unit) * d_unit\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    r = np.random.randn(self.dim)\n                    r = r - np.dot(r, d_unit) * d_unit\n                    nr = np.linalg.norm(r) + 1e-12\n                r = r / nr\n                orth_step = sigma * 0.5 * (0.8 + 0.4 * np.random.rand())\n                x_try = np.clip(x_parent + orth_step * r, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            p_jump = jump_base_prob + 0.02 * (1.0 / max(1, remaining))\n            if np.random.rand() < p_jump and remaining > 0:\n                # generate Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(size=self.dim)\n                # robust scale normalization to avoid numerical blow-up\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale relative to domain\n                scale_vec = 0.15 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(pop_sigma[worst_i] * 0.6, 1e-12)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and np.random.rand() < recomb_prob and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                b0 = pop[best2[0]]\n                b1 = pop[best2[1]]\n                beta = np.random.rand()\n                child = np.clip(beta * b0 + (1 - beta) * b1 +\n                                (0.01 * (ub - lb)) * np.random.randn(self.dim), lb, ub)\n                try:\n                    f_child, x_child = callf(child)\n                except RuntimeError:\n                    break\n                # If it's better than parent, replace parent; else maybe replace worst\n                if f_child < pop_f[parent_i]:\n                    pop[parent_i] = x_child\n                    pop_f[parent_i] = f_child\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_child < pop_f[worst_i]:\n                        pop[worst_i] = x_child\n                        pop_f[worst_i] = f_child\n                        pop_sigma[worst_i] = max(0.5 * sigma, 1e-12)\n                continue\n\n            # adapt parent sigma on failure: shrink mildly\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < rejuvenation_prob and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = lb + np.random.rand(self.dim) * (ub - lb)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma_scalar * (0.6 + 0.8 * np.random.rand())\n\n        # finished budget or loop break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.559 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.131930775511221, 0.17146137524418936, 0.8146738632435127, 0.941395478849518, 0.8641000617528666, 0.9097371464449115, 0.23738564528334505, 0.5175447751833278, 0.855991882322415, 0.14440274310835421]}, "task_prompt": ""}
{"id": "d6c971b3-7792-407e-94f0-7ecaa3fc7c9c", "fitness": 0.4409694577933657, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — maintains a small population of solutions with adaptive local step-sizes, performs randomized directional local searches, orthogonal refinements, recombination and occasional Lévy-like jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest function of dim)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds as full-dim np arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # Ensure bounds length matches dim\n        assert lb.size == self.dim and ub.size == self.dim, \"Bounds must match dimensionality.\"\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # robust max scale for sigmas\n        global_scale = max(1e-12, 0.5 * np.max(ub - lb))\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return None, None\n            # ensure vector shape and clip to bounds\n            x = np.array(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, do random sampling\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n        if remaining < 10:\n            # simple random search for tiny budgets\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population (positions, fitnesses, sigmas)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            if f0 is None:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (1.0 + 0.5 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population (very small budget), fallback to random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        pop = np.array(pop)  # shape (n_pop, dim)\n        pop_f = np.array(pop_f)\n        pop_sigma = np.array(pop_sigma)\n\n        # Main loop\n        iter_since_improve = 0\n        max_iters_no_improve = 200\n        while remaining > 0:\n            n_pop = len(pop)\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, n_pop)\n            inds = np.random.choice(n_pop, k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            parent_x = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d)\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = parent_x + alpha * d\n            f_try, x_try = callf(x_try)\n            if f_try is None:\n                break\n\n            # success: accept and slightly increase sigma\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15, global_scale)\n                iter_since_improve = 0\n            else:\n                # local backtracking / small-step refinement along direction (few tries)\n                improved = False\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    if remaining <= 0:\n                        break\n                    x_try2 = parent_x + (alpha * frac) * d\n                    f_try2, x_try2 = callf(x_try2)\n                    if f_try2 is None:\n                        break\n                    if f_try2 < pop_f[parent_i]:\n                        pop[parent_i] = x_try2\n                        pop_f[parent_i] = f_try2\n                        pop_sigma[parent_i] = min(sigma * 1.07, global_scale)\n                        improved = True\n                        iter_since_improve = 0\n                        break\n                if not improved:\n                    # reduce sigma on failure (conservative)\n                    pop_sigma[parent_i] = max(sigma * 0.88, 1e-12)\n                    iter_since_improve += 1\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # orthogonalize to d\n                proj = np.dot(r, d) * d\n                r = r - proj\n                nr = np.linalg.norm(r)\n                if nr > 1e-16:\n                    r = r / nr\n                    x_try_o = pop[parent_i] + 0.6 * pop_sigma[parent_i] * r\n                    f_try_o, x_try_o = callf(x_try_o)\n                    if f_try_o is not None and f_try_o < pop_f[parent_i]:\n                        pop[parent_i] = x_try_o\n                        pop_f[parent_i] = f_try_o\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.05, global_scale)\n                        iter_since_improve = 0\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and np.random.rand() < 0.08:\n                # Cauchy-like heavy-tailed vector, but normalized so we don't exceed bounds too often\n                raw = np.random.standard_cauchy(size=self.dim)\n                # robust scale: combine sigma and global_scale\n                scale = max(1e-8, pop_sigma[parent_i]) * 6.0\n                levy_step = raw\n                # limit extreme values to avoid numeric blow-ups, while keeping heavy tails\n                levy_step = np.clip(levy_step, -50, 50)\n                levy_vec = levy_step / (np.linalg.norm(levy_step) + 1e-12)\n                x_try_l = pop[parent_i] + scale * levy_vec * (1.0 + np.random.rand())\n                f_try_l, x_try_l = callf(x_try_l)\n                if f_try_l is not None:\n                    # if it's good, replace worst; if very good, replace parent too\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try_l < pop_f[worst_i]:\n                        pop[worst_i] = x_try_l\n                        pop_f[worst_i] = f_try_l\n                        pop_sigma[worst_i] = pop_sigma[parent_i] * 0.8\n                    if f_try_l < pop_f[parent_i]:\n                        pop[parent_i] = x_try_l\n                        pop_f[parent_i] = f_try_l\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, global_scale)\n                        iter_since_improve = 0\n\n            # recombination exploitation: mix two best and add small noise\n            if remaining > 0 and n_pop >= 2 and np.random.rand() < 0.25:\n                best2 = np.argsort(pop_f)[:2]\n                b0, b1 = best2[0], best2[1]\n                beta = np.random.rand()\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_rec = beta * pop[b0] + (1.0 - beta) * pop[b1] + noise\n                f_rec, x_rec = callf(x_rec)\n                if f_rec is not None:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_rec < pop_f[worst_i]:\n                        pop[worst_i] = x_rec\n                        pop_f[worst_i] = f_rec\n                        pop_sigma[worst_i] = np.mean(pop_sigma) * 0.5\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and (np.random.rand() < 0.04 or iter_since_improve > max_iters_no_improve):\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new is not None:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (1.0 + 0.5 * np.random.rand())\n                iter_since_improve = 0\n\n            # Ensure sigma remains in reasonable bounds and update population arrays\n            pop_sigma = np.clip(pop_sigma, 1e-12, global_scale)\n            # If population size has shrunk (shouldn't normally happen), refill a bit\n            if remaining > 0 and len(pop) < self.pop_size and np.random.rand() < 0.2:\n                add = min(self.pop_size - len(pop), max(1, remaining // 20))\n                for _ in range(add):\n                    if remaining <= 0:\n                        break\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    if f_new is None:\n                        break\n                    pop = np.vstack([pop, x_new])\n                    pop_f = np.concatenate([pop_f, [f_new]])\n                    pop_sigma = np.concatenate([pop_sigma, [base_sigma * (1.0 + 0.5 * np.random.rand())]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.441 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15117538858547908, 0.1863488451509735, 0.932092208706199, 0.9737826529811333, 0.18119431670703157, 0.9638739454709943, 0.23417188373741826, 0.4813573930674997, 0.15738095645847205, 0.14831698706845653]}, "task_prompt": ""}
{"id": "5158d4d1-dba4-4f01-ab59-bdffe4a9a663", "fitness": 0.5752262769268867, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — combines directional local searches with orthogonal refinements, adaptive step-sizes, recombination and occasional Lévy-like jumps to robustly explore/exploit continuous search spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: number of candidate solutions kept (auto-scaled if None)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # heuristically scale population with dimensionality but keep modest\n        if pop_size is None:\n            self.pop_size = max(4, min(24, int(6 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds and make sure they are arrays of correct shape\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        # state\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper evaluating function while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return np.inf, x\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)  # clip\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is very small -> simple random search fallback\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = 0.15 * np.mean(ub - lb)  # initial scale relative to search range\n\n        for i in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x = self.rng.uniform(lb, ub)\n            f, x = callf(x)\n            pop.append(x)\n            pop_f.append(f)\n            # diversity in sigmas\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.rand()))\n\n        pop = list(pop)  # ensure list (we will mutate)\n        if len(pop) == 0:\n            # budget exhausted during init: return best found so far\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # main loop: repeat directional local searches, orthogonal tries, Levy jumps, recombination\n        it = 0\n        # tuning probabilities / parameters\n        levy_prob = 0.06\n        rejuvenate_prob = 0.02\n        recomb_every = 5\n        tournament_k = min(3, len(pop))\n        frac_list = (0.5, 0.25, -0.5, -0.25)\n\n        while remaining > 0:\n            it += 1\n            # choose parent by small tournament selection (prefer better)\n            inds = self.rng.choice(len(pop), size=tournament_k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            ss = np.exp(0.2 * self.rng.randn())  # multiplicative noise on step-length (~log-normal)\n            step = d * sigma * ss\n            x_try = np.clip(x_parent + step, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.12, np.mean(ub - lb))\n                continue  # continue to next iteration, successful exploitation\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in frac_list:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + d * sigma * frac, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.08, np.mean(ub - lb))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            # create a vector orthogonal to d by projecting random vector\n            r = self.rng.randn(self.dim)\n            # subtract projection on d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                orth_scale = 0.6 * sigma\n                x_try = np.clip(x_parent + orth_scale * r * (0.6 + 0.8 * self.rng.rand()), lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.08, np.mean(ub - lb))\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < levy_prob and remaining > 0:\n                # Cauchy-like heavy-tailed vector (standard Cauchy per component) with some gaussian smoothing\n                step_cauchy = self.rng.standard_cauchy(size=self.dim)\n                step_gauss = 0.3 * self.rng.randn(self.dim)\n                step_vec = step_cauchy * 0.6 + step_gauss * 0.4\n                # normalize heavy-tailed vector by robust scale to avoid extreme axis blow-ups\n                denom = np.percentile(np.abs(step_vec), 90) + 1e-12\n                step_vec = step_vec / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step_vec * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                # if it's good replace the worst in population, else maybe keep as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.8 + self.rng.rand() * 0.6)\n                # continue loop after jump attempt (no further local steps this iteration)\n                continue\n\n            # recombination exploitation: mix two best and small noise every few iterations\n            if (it % recomb_every) == 0 and remaining > 0 and len(pop) > 1:\n                # mix weighted average of two bests\n                sorted_idx = np.argsort(pop_f)\n                i1, i2 = sorted_idx[0], sorted_idx[1]\n                alpha = 0.6 + 0.4 * self.rng.rand()\n                mix = alpha * pop[i1] + (1.0 - alpha) * pop[i2]\n                noise = 0.02 * np.mean(ub - lb) * self.rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.06, np.mean(ub - lb))\n                else:\n                    # if it still is among the best, try to inject into worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.8 + 0.5 * self.rng.rand())\n\n            # adapt parent sigma on failure (shrink)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.86, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < rejuvenate_prob and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * self.rng.rand())\n\n            # if population got reduced due to exhausted budget during steps, break\n            if remaining <= 0:\n                break\n\n            # sanity: keep arrays consistent lengths\n            # (no explicit aging here; population stable size)\n            # if stagnation: small random injection (rare)\n            if it % 200 == 0 and self.rng.rand() < 0.5 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.clip(pop[worst_i] + 0.5 * base_sigma * self.rng.randn(self.dim), lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n\n        # finished budget or loop ended\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.575 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12110768548911699, 0.1630390968957377, 0.8876730566422877, 0.9506440183812251, 0.9344125034654431, 0.9447065320408321, 0.267242314728663, 0.41626227524629345, 0.9236638251475061, 0.14351146123176117]}, "task_prompt": ""}
{"id": "63ebea04-d134-476b-8518-718ab3ec88eb", "fitness": 0.313705620221604, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — combines normalized randomized directional local searches, small orthogonal refinements, population-level recombination, and occasional heavy-tailed (Cauchy) jumps with per-individual adaptive step sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (automatically scaled if None)\n    - seed: optional random seed for reproducibility\n\n    Main ideas:\n    - Maintain a small population of candidate points with per-individual adaptive sigma.\n    - Perform randomized directional local searches (normalized directions) with multiplicative step-length noise.\n    - Do a few backtracking/small-step refinements along successful directions.\n    - Try orthogonal small perturbations for local diversification.\n    - Occasionally perform heavy-tailed (Cauchy-like) jumps to escape basins and inject into the population.\n    - Recombine two best individuals with small noise to exploit promising regions.\n    - Replace the worst occasionally with random samples to maintain diversity.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population scaling with dimensionality\n            self.pop_size = max(6, min(40, int(4 + 3 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        # extract bounds; ensure full-dim arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            # helper that clips x, calls func, decrements remaining, and updates global best\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float)\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is tiny, perform pure random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n        if remaining < 5:\n            # a tiny random search\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # initialize population size limited by remaining budget\n        pop_n = min(self.pop_size, remaining)\n        pop = np.zeros((pop_n, self.dim), dtype=float)\n        pop_f = np.full(pop_n, np.inf, dtype=float)\n        pop_sigma = np.zeros(pop_n, dtype=float)\n\n        # base sigma scale relative to problem range\n        base_sigma = max(1e-12, 0.15 * np.mean(ub - lb))\n\n        # initialize population with random samples\n        for i in range(pop_n):\n            if remaining <= 0:\n                break\n            x = np.random.uniform(lb, ub)\n            f, x = callf(x)\n            pop[i, :] = x\n            pop_f[i] = f\n            pop_sigma[i] = base_sigma\n\n        # if nothing initialized (shouldn't happen), fallback\n        if pop.shape[0] == 0:\n            return self.f_opt, self.x_opt\n\n        # main loop\n        while remaining > 0:\n            # small tournament selection: bias to good parents but allow exploration\n            k = min(3, pop.shape[0])\n            inds = np.random.choice(pop.shape[0], k, replace=False)\n            # with probability 0.8 choose best from tournament, else random from tournament\n            if np.random.rand() < 0.8:\n                parent_local_idx = np.argmin(pop_f[inds])\n                parent_i = inds[parent_local_idx]\n            else:\n                parent_i = np.random.choice(inds)\n            x_parent = pop[parent_i].copy()\n            sigma = max(pop_sigma[parent_i], 1e-12)\n\n            # sample a random normalized direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 0:\n                continue\n            d = d / nd\n\n            # stochasticized step-length (multiplicative log-normal noise preserves positivity)\n            step_len = sigma * max(1e-15, np.exp(0.25 * np.random.randn()))\n            x_try = np.clip(x_parent + step_len * d, lb, ub)\n\n            # directional primary trial\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # accept and grow sigma slightly\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.18, max( (ub-lb).mean(), sigma*10 ))\n                improved = True\n\n                # local backtracking / small-step refinements along that direction\n                # try a few smaller steps to refine\n                for t in range(3):\n                    if remaining <= 0:\n                        break\n                    small = (0.6 * (0.5 ** t)) * pop_sigma[parent_i]\n                    x_small = np.clip(pop[parent_i] + small * d, lb, ub)\n                    f_small, x_small = callf(x_small)\n                    if f_small < pop_f[parent_i]:\n                        pop[parent_i] = x_small\n                        pop_f[parent_i] = f_small\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.05, 1e-12)\n                    else:\n                        # if no improvement with smaller step, slightly shrink sigma\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n                        break\n            else:\n                # failure: shrink sigma moderately\n                pop_sigma[parent_i] = max(sigma * 0.92, 1e-12)\n\n            # orthogonal perturbation for local diversification (most of the time)\n            if remaining > 0 and np.random.rand() < 0.7:\n                r = np.random.randn(self.dim)\n                # make orthogonal to d\n                proj = np.dot(r, d) * d\n                r = r - proj\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_ort = np.clip(x_parent + 0.6 * pop_sigma[parent_i] * r, lb, ub)\n                    if remaining > 0:\n                        f_ort, x_ort = callf(x_ort)\n                        if f_ort < pop_f[parent_i]:\n                            pop[parent_i] = x_ort\n                            pop_f[parent_i] = f_ort\n                            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n\n            # occasional Lévy-like heavy-tailed jump to escape local basins\n            if remaining > 0 and np.random.rand() < 0.03:\n                # Cauchy-like heavy-tailed vector, then robust-normalize to avoid numerical explosion\n                step = np.random.standard_cauchy(self.dim)\n                # robust scale normalization (MAD)\n                mad = np.median(np.abs(step - np.median(step))) + 1e-12\n                step = step / mad\n                # choose jump scale adaptively: sometimes moderate, sometimes large\n                scale = (ub - lb).mean() * (0.6 + 4.0 * np.random.rand())\n                # normalize direction of heavy-tailed vector to keep magnitude controlled\n                denom = np.linalg.norm(step)\n                if denom < 1e-12:\n                    denom = 1.0\n                x_jump = np.clip(x_parent + scale * step / denom, lb, ub)\n                if remaining > 0:\n                    f_jump, x_jump = callf(x_jump)\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        # inject into population by replacing worst\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = base_sigma * 0.6\n                    else:\n                        # maybe replace parent if it's better\n                        if f_jump < pop_f[parent_i]:\n                            pop[parent_i] = x_jump\n                            pop_f[parent_i] = f_jump\n                            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n\n            # recombination exploitation: mix two bests and add small noise\n            if remaining > 0 and np.random.rand() < 0.18 and pop.shape[0] >= 2:\n                best2 = np.argsort(pop_f)[:2]\n                a = pop[best2[0]]\n                b = pop[best2[1]]\n                w = np.random.rand()\n                mix = w * a + (1.0 - w) * b\n                noise = 0.3 * pop_sigma[best2[0]] * np.random.randn(self.dim)\n                x_mix = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    f_mix, x_mix = callf(x_mix)\n                    worst_i = int(np.argmax(pop_f))\n                    # try to replace parent if improved\n                    if f_mix < pop_f[parent_i]:\n                        pop[parent_i] = x_mix\n                        pop_f[parent_i] = f_mix\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                    # try to inject by replacing the worst\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # occasional population rejuvenation: replace worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, x_new = callf(x_new)\n                    if f_new < pop_f[worst_i]:\n                        pop[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = base_sigma\n\n            # safety: if population becomes degenerate (all identical or all large sigmas), inject random individual\n            if remaining > 0 and (np.max(pop_f) - np.min(pop_f)) < 1e-12:\n                idx = np.argmax(pop_f)\n                x_new = np.random.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, x_new = callf(x_new)\n                    pop[idx] = x_new\n                    pop_f[idx] = f_new\n                    pop_sigma[idx] = base_sigma\n\n            # keep population-level sigma bounded to sensible ranges\n            pop_sigma = np.clip(pop_sigma, 1e-12, max((ub - lb).mean() * 10.0, 1e-12))\n\n        return float(self.f_opt), np.asarray(self.x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.314 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12813605774027392, 0.16307705446573073, 0.6152169733805162, 0.16260941753779834, 0.3301849225540774, 0.7251244365801832, 0.2682866399292594, 0.37977892849759987, 0.22770531284668827, 0.1369364586839128]}, "task_prompt": ""}
{"id": "09aa2de8-7f37-4e63-bc22-66a78aa4ce85", "fitness": 0.2781754098495995, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional searches with orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps for robust exploration/exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to scaled by dim)\n    - seed: optional rng seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size if pop_size is not None else max(6, int(4 + 2 * self.dim))\n        self.seed = seed\n        if seed is None:\n            self.rng = np.random.default_rng()\n        else:\n            self.rng = np.random.default_rng(seed)\n        # will be set during run\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # establish bounds (many BBOB problems provide func.bounds.lb / ub)\n        if hasattr(func, \"bounds\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        else:\n            # default to [-5,5] per problem statement\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # ensure full-dim lb/ub\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub), dtype=float)\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # helper: remaining budget\n        self.evals = 0\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            # clip to bounds and ensure correct shape\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            if self.evals >= self.budget:\n                # no budget left; return worst possible to avoid being selected\n                return np.inf, x\n            f = func(x)\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, fall back to randomized search\n        if self.budget <= 0:\n            return np.inf, None\n        if self.budget < max(5, self.pop_size):\n            # simple random sampling until budget exhausted\n            self.f_opt = np.inf\n            self.x_opt = None\n            for _ in range(self.budget):\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population uniformly\n        pop = []\n        pop_f = []\n        pop_sigma = []  # per-individual step sizes\n        range_mean = float(np.mean(ub - lb))\n        base_sigma = max(1e-8, 0.25 * range_mean)\n        init_sigma = base_sigma\n\n        # Create initial population but avoid exceeding budget\n        for i in range(self.pop_size):\n            if self.evals >= self.budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f, x = callf(x)\n            pop.append(x)\n            pop_f.append(f)\n            # sigma initialized with some spread\n            s = init_sigma * (1.0 + 0.5 * self.rng.standard_normal())\n            s = max(1e-12, abs(s))\n            pop_sigma.append(s)\n\n        # if we couldn't create a population (budget too small), return best\n        if len(pop) == 0:\n            return self.f_opt, self.x_opt\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # helper to find worst/best indices\n        def best_index():\n            return int(np.argmin(pop_f))\n\n        def worst_index():\n            return int(np.argmax(pop_f))\n\n        # main loop: use remaining budget\n        # parameters\n        levy_prob = 0.07  # occasional levy jumps\n        levy_scale = 1.5\n        ortho_prob = 0.3\n        recomb_prob = 0.25\n        rejuvenate_prob = 0.03\n        backtrack_tries = 3\n\n        while self.evals < self.budget:\n            remaining = self.budget - self.evals\n            # small tournament to pick parent\n            k = min(3, len(pop))\n            cand_idx = self.rng.choice(len(pop), size=k, replace=False)\n            # choose best among candidates\n            parent_i = int(cand_idx[np.argmin([pop_f[i] for i in cand_idx])])\n            parent = pop[parent_i].copy()\n            parent_f = pop_f[parent_i]\n            parent_sigma = pop_sigma[parent_i]\n\n            # sample a random direction\n            d = self.rng.normal(size=self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # stochasticize step-length\n            sigma = parent_sigma * max(0.2, 1.0 + 0.3 * self.rng.standard_normal())\n            # primary directional trial with backtracking\n            improved = False\n            step_scale = 1.0\n            for bt in range(backtrack_tries):\n                if self.evals >= self.budget:\n                    break\n                x_try = parent + d * (sigma * step_scale)\n                f_try, x_try = callf(x_try)\n                if f_try < parent_f:\n                    # accept\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    # slightly expand sigma on success\n                    pop_sigma[parent_i] = min(range_mean * 2.0, parent_sigma * (1.0 + 0.08))\n                    improved = True\n                    break\n                else:\n                    # reduce step and try a smaller move\n                    step_scale *= 0.6\n\n            # local orthogonal refinement (small)\n            if not improved and self.evals < self.budget and self.rng.random() < ortho_prob:\n                # build orthogonal vector to d\n                v = self.rng.normal(size=self.dim)\n                # subtract projection on d\n                v = v - d * (np.dot(v, d))\n                nv = np.linalg.norm(v) + 1e-12\n                v = v / nv\n                small_step = 0.5 * parent_sigma\n                x_try = parent + v * small_step * (0.8 + 0.4 * self.rng.random())\n                f_try, x_try = callf(x_try)\n                if f_try < parent_f:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(range_mean * 2.0, parent_sigma * (1.0 + 0.05))\n                    improved = True\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed)\n            if (not improved) and (remaining > 1) and (self.rng.random() < levy_prob):\n                # Cauchy-like heavy-tailed vector\n                cauchy = self.rng.standard_cauchy(size=self.dim)\n                # robust scaling and clip extreme values to avoid NaNs\n                cauchy = np.tanh(cauchy / 5.0)  # preserve heavy tail but bounded\n                step = cauchy * (levy_scale * base_sigma * (0.5 + self.rng.random()))\n                # normalize to avoid extremely tiny/big magnitudes while preserving direction\n                step_norm = np.linalg.norm(step) + 1e-12\n                step = step * (0.8 * range_mean / step_norm)\n                x_try = parent + step\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[worst_index()]:\n                    # replace worst if jump produced something better\n                    w = worst_index()\n                    pop[w] = x_try\n                    pop_f[w] = f_try\n                    pop_sigma[w] = max(1e-12, base_sigma * (0.8 + 0.4 * self.rng.random()))\n                else:\n                    # consider keeping it by replacing the parent if better\n                    if f_try < parent_f:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(1e-12, base_sigma * (0.8 + 0.4 * self.rng.random()))\n                # continue main loop\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if (not improved) and (self.rng.random() < recomb_prob) and len(pop) >= 2:\n                # pick two best\n                sorted_idx = np.argsort(pop_f)\n                b1 = int(sorted_idx[0])\n                b2 = int(sorted_idx[1])\n                alpha = 0.2 + 0.6 * self.rng.random()\n                noise = 0.05 * range_mean * self.rng.normal(size=self.dim)\n                x_new = pop[b1] + alpha * (pop[b2] - pop[b1]) + noise\n                f_new, x_new = callf(x_new)\n                if f_new < parent_f:\n                    pop[parent_i] = x_new\n                    pop_f[parent_i] = f_new\n                    pop_sigma[parent_i] = max(1e-12, (pop_sigma[b1] + pop_sigma[b2]) / 2.0 * (1.0 + 0.05 * self.rng.random()))\n                    improved = True\n                else:\n                    # maybe replace worst\n                    w = worst_index()\n                    if f_new < pop_f[w]:\n                        pop[w] = x_new\n                        pop_f[w] = f_new\n                        pop_sigma[w] = max(1e-12, base_sigma * (0.5 + self.rng.random()))\n\n            # adapt parent sigma on failure\n            if not improved:\n                # shrink sigma moderately\n                old_sigma = pop_sigma[parent_i]\n                pop_sigma[parent_i] = max(1e-12, old_sigma * (0.85 - 0.1 * self.rng.random()))\n                # small local random probe replacing worst occasionally\n                if self.rng.random() < 0.1 and self.evals < self.budget:\n                    w = worst_index()\n                    x_rand = self.rng.uniform(lb, ub)\n                    f_rand, x_rand = callf(x_rand)\n                    if f_rand < pop_f[w]:\n                        pop[w] = x_rand\n                        pop_f[w] = f_rand\n                        pop_sigma[w] = max(1e-12, base_sigma * (0.7 + 0.6 * self.rng.random()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.random() < rejuvenate_prob and self.evals < self.budget:\n                w = worst_index()\n                x_rand = self.rng.uniform(lb, ub)\n                f_rand, x_rand = callf(x_rand)\n                if f_rand < pop_f[w]:\n                    pop[w] = x_rand\n                    pop_f[w] = f_rand\n                    pop_sigma[w] = max(1e-12, base_sigma * (0.7 + 0.6 * self.rng.random()))\n\n            # if population got too small (shouldn't happen), refill\n            if len(pop) < 2 and self.evals < self.budget:\n                x = self.rng.uniform(lb, ub)\n                f, x = callf(x)\n                pop.append(x); pop_f.append(f); pop_sigma.append(base_sigma)\n\n            # occasionally compact population by trimming extremely bad individuals if budget low\n            if (self.evals > 0.6 * self.budget) and len(pop) > 3:\n                # keep best 80% proportionally to remaining budget\n                keep = max(2, int(len(pop) * 0.9))\n                order = np.argsort(pop_f)\n                pop = [pop[int(i)].copy() for i in order[:keep]]\n                pop_f = [float(pop_f[int(i)]) for i in order[:keep]]\n                pop_sigma = [pop_sigma[int(i)] for i in order[:keep]]\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.278 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1677378437376088, 0.17788310264017726, 0.3448150646972259, 0.44085060844668666, 0.2748123505393538, 0.3716936642595491, 0.26376856149398087, 0.29989534111309024, 0.26068020895367516, 0.17961735261464706]}, "task_prompt": ""}
{"id": "f5a6a8df-2a87-4f25-883c-1633bbf7c4ee", "fitness": 0.2852400191499254, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — maintains a small population with adaptive per-individual step-sizes, performs randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy) jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: number of individuals in the population (optional)\n    - seed: RNG seed for reproducibility (optional)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population sizing heuristic: scale with dimension but limited by budget\n        if pop_size is None:\n            # ensure at least 4, but not too large relative to budget\n            pop_size = int(max(4, min(8 + self.dim // 2, max(4, self.budget // 20))))\n        self.pop_size = int(pop_size)\n\n        # to be filled at run time\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Set up bounds (robust to different func APIs). Default to [-5, 5].\n        try:\n            # common interface used in many benchmarks\n            lb = np.asarray(func.bounds.lb).astype(float)\n            ub = np.asarray(func.bounds.ub).astype(float)\n        except Exception:\n            # try other typical names\n            if hasattr(func, \"lower\") and hasattr(func, \"upper\"):\n                lb = np.asarray(func.lower).astype(float)\n                ub = np.asarray(func.upper).astype(float)\n            else:\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n\n        # Ensure bounds are full-dimension arrays\n        lb = lb.ravel()\n        ub = ub.ravel()\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # Clip bounds to length dim\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.resize(lb, self.dim)\n            ub = np.resize(ub, self.dim)\n\n        # Helper to evaluate while tracking budget and best\n        remaining = {'n': self.budget}  # use mutable to allow inner updates\n\n        def callf(x):\n            # ensure x is numpy array of correct shape\n            x = np.asarray(x, dtype=float).ravel()\n            if x.size != self.dim:\n                x = np.resize(x, self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n\n            if remaining['n'] <= 0:\n                # no budget: do not call\n                return np.inf, x\n\n            # evaluate\n            remaining['n'] -= 1\n            f = float(func(x))\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, fallback to random search\n        if remaining['n'] <= 0:\n            return self.f_opt, self.x_opt\n\n        if remaining['n'] < max(1, self.pop_size):\n            # not enough budget to build population: random sampling\n            while remaining['n'] > 0:\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        range_scale = ub - lb\n        # initial sigma per individual: fraction of search range\n        for i in range(self.pop_size):\n            if remaining['n'] <= 0:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            # sigma as scalar fraction of average range, randomized a bit\n            avg_range = float(np.mean(range_scale))\n            sigma0 = max(1e-6, avg_range * (0.15 * (1 + 0.6 * self.rng.rand())))\n            pop.append({'x': x0.copy(), 'f': f0, 'sigma': sigma0, 'age': 0})\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining['n'] > 0:\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Utility functions\n        def best_idx():\n            return int(np.argmin([p['f'] for p in pop]))\n\n        def worst_idx():\n            return int(np.argmax([p['f'] for p in pop]))\n\n        # Main loop using remaining budget\n        iter_since_improve = 0\n        iters = 0\n        # dynamics hyperparameters\n        p_levy = 0.03  # occasional heavy-tailed jumps\n        backtrack_steps = [0.5, 0.25, 0.1]  # local refinement scales\n        orth_scale = 0.6\n        recomb_noise = 1e-2\n        max_sigma = np.linalg.norm(range_scale) * 2.0\n        min_sigma = 1e-8\n\n        while remaining['n'] > 0:\n            iters += 1\n            # pick a parent via small tournament selection\n            tsize = min(3, len(pop))\n            tour = self.rng.choice(len(pop), size=tsize, replace=False)\n            parent_idx = tour[np.argmin([pop[i]['f'] for i in tour])]\n            parent = pop[parent_idx]\n\n            parent['age'] += 1\n\n            # sample a random search direction (normalized)\n            d = self.rng.normal(size=self.dim)\n            d_norm = np.linalg.norm(d)\n            if d_norm == 0:\n                d = self.rng.rand(self.dim) - 0.5\n                d_norm = np.linalg.norm(d) + 1e-12\n            d = d / d_norm\n\n            success = False\n\n            # primary directional trial with stochasticized step-length\n            # multiplicative noise on sigma (log-normal style)\n            factor = float(np.exp(self.rng.normal(loc=0.0, scale=0.35)))\n            step = parent['sigma'] * factor\n            x_trial = parent['x'] + step * d\n            x_trial = np.minimum(np.maximum(x_trial, lb), ub)\n            f_trial, x_trial = callf(x_trial)\n            if f_trial < parent['f']:\n                # accept and slightly increase sigma\n                parent['x'] = x_trial.copy()\n                parent['f'] = f_trial\n                parent['sigma'] = min(max_sigma, parent['sigma'] * (1.12 + 0.08 * self.rng.rand()))\n                parent['age'] = 0\n                success = True\n                iter_since_improve = 0\n            else:\n                iter_since_improve += 1\n\n            # local backtracking / small-step refinement along direction (few tries)\n            if (not success) and remaining['n'] > 0:\n                for bscale in backtrack_steps:\n                    if remaining['n'] <= 0:\n                        break\n                    stepb = parent['sigma'] * bscale\n                    # try both forward and backward small steps\n                    for sign in (+1.0, -1.0):\n                        x_b = parent['x'] + sign * stepb * d\n                        x_b = np.minimum(np.maximum(x_b, lb), ub)\n                        f_b, x_b = callf(x_b)\n                        if f_b < parent['f']:\n                            parent['x'] = x_b.copy()\n                            parent['f'] = f_b\n                            parent['sigma'] = min(max_sigma, parent['sigma'] * (1.08 + 0.06 * self.rng.rand()))\n                            parent['age'] = 0\n                            success = True\n                            iter_since_improve = 0\n                            break\n                    if success:\n                        break\n\n            # try an orthogonal perturbation for local diversification\n            if (not success) and remaining['n'] > 0:\n                r = self.rng.normal(size=self.dim)\n                # make r orthogonal to d\n                proj = np.dot(r, d) * d\n                ort = r - proj\n                n_ort = np.linalg.norm(ort)\n                if n_ort > 1e-12:\n                    ort = ort / n_ort\n                    x_o = parent['x'] + orth_scale * parent['sigma'] * ort\n                    x_o = np.minimum(np.maximum(x_o, lb), ub)\n                    f_o, x_o = callf(x_o)\n                    if f_o < parent['f']:\n                        parent['x'] = x_o.copy()\n                        parent['f'] = f_o\n                        parent['sigma'] = min(max_sigma, parent['sigma'] * (1.06 + 0.04 * self.rng.rand()))\n                        parent['age'] = 0\n                        success = True\n                        iter_since_improve = 0\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if (not success) and (remaining['n'] > 0) and (self.rng.rand() < p_levy):\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                s = self.rng.standard_cauchy(size=self.dim)\n                # robust scale: median absolute deviation\n                mad = np.median(np.abs(s - np.median(s)))\n                scale_norm = mad if (mad > 1e-12) else np.median(np.abs(s)) if np.median(np.abs(s)) > 1e-12 else 1.0\n                s = s / (scale_norm + 1e-12)\n                levy_scale = max(1.0, 2.0 * (1.0 + self.rng.rand()))  # random multiplier\n                x_jump = parent['x'] + parent['sigma'] * levy_scale * s\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                # cap by bounding box extent\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < parent['f']:\n                    # improved: replace worst in population or accept as parent\n                    wi = worst_idx()\n                    pop[wi] = {'x': x_jump.copy(), 'f': f_jump,\n                               'sigma': max(min_sigma, parent['sigma'] * 0.8), 'age': 0}\n                    success = True\n                    iter_since_improve = 0\n                else:\n                    # keep it as candidate occasionally (replace worst with some probability)\n                    if self.rng.rand() < 0.12:\n                        wi = worst_idx()\n                        pop[wi] = {'x': x_jump.copy(), 'f': f_jump,\n                                   'sigma': max(min_sigma, parent['sigma'] * 0.6), 'age': 0}\n\n            # recombination exploitation: mix two best and small noise\n            if remaining['n'] > 0:\n                # pick top two\n                if len(pop) >= 2:\n                    idx_sorted = np.argsort([p['f'] for p in pop])\n                    b1 = pop[int(idx_sorted[0])]\n                    b2 = pop[int(idx_sorted[1])]\n                    alpha = 0.6 + 0.2 * self.rng.rand()  # bias to best\n                    x_mix = alpha * b1['x'] + (1.0 - alpha) * b2['x']\n                    # small gaussian mutation scaled by average sigma\n                    avg_sigma = (b1['sigma'] + b2['sigma']) * 0.5\n                    noise = self.rng.normal(scale=recomb_noise * (avg_sigma + 1e-12), size=self.dim)\n                    x_mix = np.minimum(np.maximum(x_mix + noise, lb), ub)\n                    f_mix, x_mix = callf(x_mix)\n                    if f_mix < parent['f']:\n                        parent['x'] = x_mix.copy()\n                        parent['f'] = f_mix\n                        parent['sigma'] = min(max_sigma, avg_sigma * (1.0 + 0.1 * self.rng.rand()))\n                        parent['age'] = 0\n                        success = True\n                        iter_since_improve = 0\n                    else:\n                        # try to inject into population by replacing worst if better\n                        if f_mix < pop[worst_idx()]['f']:\n                            pop[worst_idx()] = {'x': x_mix.copy(), 'f': f_mix,\n                                                'sigma': max(min_sigma, avg_sigma * 0.7), 'age': 0}\n\n            # adapt parent sigma on failure\n            if not success:\n                # progressive shrinkage to focus search; occasional slight increase to escape\n                if self.rng.rand() < 0.04:\n                    parent['sigma'] = min(max_sigma, parent['sigma'] * (1.06 + 0.04 * self.rng.rand()))\n                else:\n                    parent['sigma'] = max(min_sigma, parent['sigma'] * (0.92 - 0.04 * self.rng.rand()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if (iters % max(20, 20 + self.dim)) == 0 and remaining['n'] > 0:\n                if self.rng.rand() < 0.6:\n                    wi = worst_idx()\n                    x_rand = self.rng.uniform(lb, ub)\n                    f_rand, x_rand = callf(x_rand)\n                    pop[wi] = {'x': x_rand.copy(), 'f': f_rand,\n                               'sigma': max(min_sigma, float(np.mean(range_scale)) * 0.12 * (1.0 + self.rng.rand())), 'age': 0}\n\n            # If parent's fitness improved, try to replace worst if it's better than worst\n            if success:\n                wi = worst_idx()\n                # sometimes inject improvements to population\n                if parent['f'] < pop[wi]['f']:\n                    pop[wi] = {'x': parent['x'].copy(), 'f': parent['f'],\n                               'sigma': max(min_sigma, parent['sigma'] * (0.7 + 0.3 * self.rng.rand())), 'age': 0}\n\n            # small housekeeping: ensure population size consistent\n            # remove overly old individuals with some probability (promotes turnover)\n            for i, p in enumerate(pop):\n                if p['age'] > 150 and self.rng.rand() < 0.08 and remaining['n'] > 0:\n                    x_new = self.rng.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop[i] = {'x': x_new.copy(), 'f': f_new,\n                              'sigma': max(min_sigma, float(np.mean(range_scale)) * 0.14 * (1.0 + self.rng.rand())), 'age': 0}\n\n            # ensure sigma remains in bounds\n            for p in pop:\n                if p['sigma'] < min_sigma:\n                    p['sigma'] = min_sigma\n                elif p['sigma'] > max_sigma:\n                    p['sigma'] = max_sigma\n\n            # stop early if perfect (optional), but we rely on budget mostly\n            # continue until budget exhausted\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.285 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09218992891363076, 0.15633044038040667, 0.9463340569253805, 0.2959994637588039, 0.21310098278018352, 0.2040863847000881, 0.19690121245831094, 0.3779496677302947, 0.23447006773903556, 0.13503798611311912]}, "task_prompt": ""}
{"id": "a79f692d-feb0-4265-a90a-a0c4e217e956", "fitness": 0.5011077453432137, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy-like jumps with adaptive step-sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by default)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds and ensure vector form\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # defensive: if bounds mismatch, fix length\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            \"\"\"Evaluate clipped x, update budget and best.\"\"\"\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # if no budget, immediate return\n        if remaining <= 0:\n            return self.f_opt, None if self.x_opt is None else self.x_opt.copy()\n\n        # initial population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))\n        for _ in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(float(f0))\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n\n        # if no initial population created, fallback to pure random until budget exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()\n\n        # main loop\n        while remaining > 0:\n            # pick a parent via small tournament (choose best among sampled)\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            # find the best among the chosen indices\n            parent_i = inds[int(np.argmin([pop_f[i] for i in inds]))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # adaptive directional magnitude: log-normal jitter around sigma\n            alpha = sigma * float(np.exp(np.random.randn() * 0.25))\n\n            # sample random direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial\n            if remaining <= 0:\n                break\n            x_try = x_parent + alpha * d\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # accept improvement\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = x_parent + alpha * frac * d\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # project out the component along d to make r orthogonal\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                if remaining <= 0:\n                    break\n                x_try = x_parent + 0.6 * sigma * r\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = x_parent + scale_vec * step\n                f_try, x_try = callf(x_try)\n                # If it's good, replace parent; else consider replacing worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.5)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = mix + noise\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (contraction)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # done\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.501 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13445035129794003, 0.17018048784961026, 0.5644031853453408, 0.972865088727862, 0.6678847243674748, 0.9334061624005097, 0.20983827251071396, 0.4486559543104358, 0.7650757492010204, 0.14431747742122836]}, "task_prompt": ""}
{"id": "ee7b7f5e-47f0-4414-a59c-927ed1bfb203", "fitness": 0.450560086169242, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a compact population-based continuous optimiser combining directional local searches, orthogonal refinements, adaptive step-sizes and occasional heavy-tailed Lévy-like jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by dim if None)\n    - seed: optional random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows slowly with dim\n            self.pop_size = max(4, int(4 + 0.5 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        # use new numpy Generator\n        self.rng = np.random.default_rng(seed)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # prepare bounds as arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # safety: ensure shapes\n        assert lb.shape == (self.dim,) and ub.shape == (self.dim,), \"Bounds must match dimension\"\n\n        remaining = int(self.budget)\n\n        # helper that evaluates, clips, updates remaining and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim,)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt or self.x_opt is None:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # quick fallback: if budget <=0\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (may be smaller than pop_size if budget is small)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # base sigma is proportional to the typical range of the search space\n        base_sigma = max(1e-12, 0.25 * float(np.mean(ub - lb)))\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize sigma around base_sigma with a bit of diversity\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * self.rng.random()))\n            if remaining <= 0:\n                break\n\n        # if no population could be created (very tiny budget), do pure random sampling\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                f, x = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop\n        while remaining > 0:\n            # compute some current indices\n            pop_n = len(pop)\n            worst_i = int(np.argmax(pop_f))\n            best_i = int(np.argmin(pop_f))\n\n            # small tournament selection\n            k = min(3, pop_n)\n            inds = self.rng.choice(pop_n, k, replace=False)\n            # select the best among the tournament (lower f better)\n            parent_i = int(inds[np.argmin([pop_f[i] for i in inds])])\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # normalized random direction\n            d = self.rng.standard_normal(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-16:\n                d = self.rng.standard_normal(self.dim)\n                nd = np.linalg.norm(d) + 1e-16\n            d = d / nd\n\n            # primary directional attempt\n            alpha = sigma * (1.0 + 0.5 * self.rng.normal())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # accept and slightly increase sigma (success)\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, np.mean(ub - lb))\n                continue\n\n            # backtracking / refinement: try fractional steps along direction (both signs)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation: build a random vector and remove its component along d\n            r = self.rng.standard_normal(self.dim)\n            nr = np.linalg.norm(r)\n            if nr > 1e-16:\n                # make r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                    if remaining <= 0:\n                        break\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.06, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.random() < 0.08 and remaining > 0:\n                step = self.rng.standard_cauchy(self.dim)\n                # robust normalization: scale by 90th percentile to keep heavy tail but avoid extreme blowups\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale vector proportional to current typical range\n                scale_vec = base_sigma * (1.0 + self.rng.random() * 4.0)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                # if it's good, replace worst; otherwise consider adding as new candidate (replace worst probabilistically)\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                else:\n                    # small chance to keep diverse candidate by replacing worst if not too bad\n                    if self.rng.random() < 0.02:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best with small noise\n            if pop_n >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = self.rng.random()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = self.rng.normal(scale=0.08 * sigma, size=self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                # if it's good, replace parent or worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt sigma of parent on failure (reduce slightly)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.88, 1e-12)\n\n            # occasional rejuvenation: replace worst with a fresh random sample\n            if remaining > 0 and self.rng.random() < 0.02:\n                x_new = self.rng.uniform(lb, ub)\n                if remaining <= 0:\n                    break\n                f_new, x_new = callf(x_new)\n                worst_i = int(np.argmax(pop_f))\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * self.rng.random())\n\n        # finished budget or early stop\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.451 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12067924907271033, 0.1580214663873809, 0.7088471471576121, 0.9293448815828004, 0.49012447426946026, 0.9732879295602572, 0.2338239777282336, 0.4627724928168868, 0.25400654648015886, 0.17469269663691933]}, "task_prompt": ""}
{"id": "834f0765-481e-44f9-997b-a6a83e2bb210", "fitness": 0.5479204649417377, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimiser combining randomized directional local searches, orthogonal refinements, adaptive step-sizes, recombination and occasional heavy-tailed Lévy jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_size: number of individuals in the population (default scales with dim)\n    - seed: RNG seed for reproducibility\n    - levy_prob: probability of performing a Lévy-like jump on an iteration\n    - rejuvenate_prob: probability to occasionally replace the worst with random sample\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None,\n                 levy_prob=0.08, rejuvenate_prob=0.04):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.levy_prob = float(levy_prob)\n        self.rejuvenate_prob = float(rejuvenate_prob)\n        self.rng = np.random.RandomState(seed)\n        # outputs set after running\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # obtain bounds and ensure they are arrays of length dim\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n        assert lb.size == self.dim and ub.size == self.dim, \"Bounds must match dim\"\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # safe normalization utility\n        def safe_norm(v):\n            n = np.linalg.norm(v)\n            if n <= 1e-12:\n                # generate a random unit vector if degenerate\n                v = self.rng.randn(self.dim)\n                n = np.linalg.norm(v) + 1e-12\n            return v / n\n\n        # call wrapper: clips, evaluates, updates best, decrements budget\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds defensively\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population uniformly in bounds\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale\n        for i in range(n_init):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # initialize sigma with small random multiplicative jitter\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n            if remaining <= 0:\n                break\n\n        # if we couldn't initialize any individual (budget too small), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # main loop\n        while remaining > 0:\n            # pick a parent via small tournament\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            d = safe_norm(d)\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * self.rng.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.12, max(1e-8, (ub - lb).max()))\n                continue  # move to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            frac_list = [0.5, 0.25, 0.125]\n            for frac in frac_list:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 1.05, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification (one or two tries)\n            r = self.rng.randn(self.dim)\n            # project r to orthogonal component relative to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                # try small and medium orthogonal steps\n                for scale_mul in (0.5, 1.0):\n                    if remaining <= 0:\n                        break\n                    scale = sigma * 0.6 * scale_mul\n                    x_try = np.clip(x_parent + scale * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 1.03, 1e-12)\n                        improved = True\n                        break\n                if improved:\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < self.levy_prob and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = self.rng.standard_cauchy(self.dim)\n                # robust denom: median absolute deviation + l2-norm clamp\n                mad = np.median(np.abs(step - np.median(step))) + 1e-12\n                denom = max(np.linalg.norm(step), mad, 1e-12)\n                step = step / denom\n                # scale by a vector proportional to search range with random multiplier\n                scale_vec = (ub - lb) * (0.2 + 2.0 * self.rng.rand(self.dim))\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace worst in population (exploit jump)\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = self.rng.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * self.rng.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n\n            # adapt parent sigma on failure (if no improvement happened in this loop)\n            # reduce sigma slowly to focus search\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.90, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < self.rejuvenate_prob and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.8 + 0.4 * self.rng.rand())\n\n            # ensure population size remains stable (if population is small because of initial budget)\n            # we may expand by injecting new random individuals occasionally\n            if len(pop) < self.pop_size and remaining > 0 and self.rng.rand() < 0.2:\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop.append(x_new.copy())\n                pop_f.append(f_new)\n                pop_sigma.append(base_sigma * (0.8 + 0.4 * self.rng.rand()))\n\n        # finished budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.548 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12473055697018254, 0.17004965857702414, 0.7935080187486339, 0.9390870178826507, 0.8935152476932796, 0.9285476415001591, 0.2748101786812738, 0.4367867962753257, 0.7910162439797654, 0.12715328910908286]}, "task_prompt": ""}
{"id": "d0315435-a826-47fc-bbd1-0dbed0fa2c1c", "fitness": 0.402885990717436, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — maintains a small adaptive population using randomized directional local searches, orthogonal refinements and occasional Lévy (Cauchy) jumps to balance robust exploration and focused exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults based on dim)\n    - seed: optional RNG seed\n    - init_sigma: initial relative step-size (fraction of search range)\n    - tournament_k: size of tournament for selecting parents\n    - levy_prob: probability per iteration to attempt a Lévy jump\n    - orthogonal_tries: how many orthogonal refinements to attempt\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None,\n                 init_sigma=0.2, tournament_k=None, levy_prob=0.08, orthogonal_tries=2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # scale population with dimensionality but keep modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed) if seed is not None else np.random\n        self.init_sigma = float(init_sigma)\n        self.tournament_k = int(tournament_k) if tournament_k is not None else max(2, min(4, int(np.sqrt(self.dim))))\n        self.levy_prob = float(levy_prob)\n        self.orthogonal_tries = int(orthogonal_tries)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Determine bounds. Accept either scalars, 1-d arrays or attributes func.bounds.lb/ub if present.\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            # fallback to usual BBOB assumption: [-5,5]\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # internal counters and state\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure x is numpy array of correct shape and clipped to bounds\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget extremely small, fallback to simple random sampling\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (as many as budget allows)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = self.init_sigma  # relative to (ub-lb)\n        max_initial = min(self.pop_size, remaining)\n        for i in range(max_initial):\n            x0 = self.rng.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize sigma per-individual with some randomization\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * self.rng.rand()))\n            if remaining <= 0:\n                break\n\n        # If no population created (very small budget), do pure random sampling until budget used\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Ensure lists are numpy-friendly\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # main loop\n        # we use iterations until budget exhausted, each iteration we perform a few local tries\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(self.tournament_k, len(pop))\n            inds = self.rng.choice(len(pop), size=k, replace=False)\n            parent_i = int(inds[np.argmin([pop_f[idx] for idx in inds])])\n            x_parent = pop[parent_i].copy()\n            f_parent = pop_f[parent_i]\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # stochasticized step-length scaled by sigma and search range\n            step_scale = sigma * (ub - lb)\n            alpha = self.rng.uniform(0.5, 1.5)  # randomize multiplier for adaptability\n\n            # primary directional trial\n            x_try = x_parent + alpha * step_scale * d\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < f_parent:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(1.0, sigma * (1.05 + 0.2 * self.rng.rand()))\n                continue  # go to next iteration\n\n            # local backtracking / small-step refinement along direction (a few fractional tries)\n            improved = False\n            for frac in (0.5, 0.25, 0.125):\n                if remaining <= 0:\n                    break\n                x_try = x_parent + alpha * frac * step_scale * d\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    remaining = 0\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * (0.9 - 0.2 * self.rng.rand()))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try orthogonal perturbations for local diversification\n            for t in range(self.orthogonal_tries):\n                if remaining <= 0:\n                    break\n                r = self.rng.randn(self.dim)\n                # make r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    continue\n                r = r / nr\n                # scale orthogonal step to be smaller than main direction\n                ortho_scale = 0.2 * sigma * (ub - lb)\n                x_try = x_parent + ortho_scale * r\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    remaining = 0\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.9)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < self.levy_prob and remaining > 0:\n                step = self.rng.standard_cauchy(self.dim)\n                # normalize deltas robustly to avoid huge steps while preserving heavy-tail\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # global jump magnitude proportional to search range\n                scale_vec = 0.25 * (ub - lb)\n                x_try = x_parent + step * scale_vec * (0.5 + self.rng.rand())\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else probabilistically keep\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                else:\n                    # keep as candidate with small probability\n                    if self.rng.rand() < 0.15:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.4 + 0.6 * self.rng.rand())\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                mix = 0.5 + 0.5 * self.rng.rand()\n                x_mix = mix * pop[a] + (1 - mix) * pop[b]\n                # small gaussian noise scaled to search range and sigmas\n                noise = (0.02 * (ub - lb)) * self.rng.randn(self.dim)\n                x_try = x_mix + noise\n                x_try = np.minimum(np.maximum(x_try, lb), ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # if better than parent, replace parent, else try to inject into worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.9)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * self.rng.rand())\n\n            # adapt parent sigma on failure (reduce to encourage finer search)\n            pop_sigma[parent_i] = max(1e-12, sigma * 0.85)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < 0.03 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * self.rng.rand())\n\n            # ensure population arrays consistent and keep population size stable\n            # (if budget exhausted during loop, we will exit next while check)\n            # occasionally shrink or grow sigma diversity\n            if self.rng.rand() < 0.02:\n                idx = self.rng.randint(len(pop))\n                pop_sigma[idx] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.403 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1359696118137772, 0.1603768100939157, 0.5061967409845668, 0.9159779720603451, 0.31395645802327954, 0.8197264408514007, 0.2564863971726441, 0.4144570347922546, 0.34866881715612563, 0.15704362422605045]}, "task_prompt": ""}
{"id": "e231afd1-da58-413b-ad16-8a9db52b3aba", "fitness": 0.44448140888149573, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining randomized directional local searches, orthogonal refinements, recombination, and occasional heavy-tailed Lévy-style jumps with adaptive step-sizes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with sqrt(dim))\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # determine bounds from func if available, else default to [-5,5]\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # expand scalar bounds to full dimension\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        # if bounds provided but wrong size, try to broadcast or fall back\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        # small safety: ensure lb < ub\n        rng_range = np.maximum(ub - lb, 1e-8)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure x is numpy array of correct shape and clipped to bounds\n            x = np.asarray(x, dtype=float).copy()\n            if x.size != self.dim:\n                x = np.resize(x, self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget is extremely small, fallback to random sampling within budget\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # base sigma scale relative to search range\n        base_sigma = 0.25 * np.mean(rng_range)\n\n        # Initialize population: uniform random sampling\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n        # if no population could be created (very small budget), return best found\n        if len(pop) == 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # bookkeeping for stagnation-based rejuvenation\n        no_improve = 0\n\n        # Main loop: directional local searches, orthogonal tries, Lévy jumps, recombination\n        while remaining > 0:\n            best_before = self.f_opt\n\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = int(inds[int(np.argmin(values))])\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (0.6 + 1.4 * np.random.rand())  # ~[0.6*sigma,2.0*sigma]\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(rng_range))\n                # continue to next iteration (exploit success)\n                no_improve = 0\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for factor in (0.5, 0.25, 0.1):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * factor * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                no_improve = 0\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 0:\n                    r = r / nr\n                    beta = sigma * (0.15 + 0.7 * np.random.rand())\n                    x_try = np.clip(x_parent + beta * r, lb, ub)\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(rng_range))\n                        no_improve = 0\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            p_jump = min(0.06, 3.0 / max(5.0, np.sqrt(self.dim)))  # adapt prob with dim\n            if remaining > 0 and np.random.rand() < p_jump:\n                # Cauchy-like heavy-tailed vector (tan(pi*(u-0.5))) normalized robustly\n                step = np.tan(np.pi * (np.random.rand(self.dim) - 0.5))\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale = np.mean(rng_range) * (0.8 + 2.5 * np.random.rand())\n                x_try = np.clip(x_parent + scale * step, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.9 * np.random.rand())\n                    no_improve = 0\n                    continue\n                # else we may keep it as a candidate (none implemented) and move on\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                mix = 0.6 * pop[a] + 0.4 * pop[b]\n                noise = (0.02 * (rng_range)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n                    no_improve = 0\n                    continue\n\n            # adapt parent sigma on failure (reduce step size)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0:\n                # If no improvement for a while, perform rejuvenation\n                if self.f_opt < best_before:\n                    # improved this iteration\n                    no_improve = 0\n                else:\n                    no_improve += 1\n\n                if no_improve >= 20:\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = np.random.uniform(lb, ub)\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        break\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                    no_improve = 0\n\n        # finished budget or exhausted evaluations\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.444 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13961482203832298, 0.15432563285478074, 0.4557165540912014, 0.9655410811784816, 0.30489144046705985, 0.9374576176234491, 0.29038971517613676, 0.3310944638871153, 0.7147867360066562, 0.15099602549175306]}, "task_prompt": ""}
{"id": "ca59bf40-1f3e-48e8-8b15-98709ef540d3", "fitness": 0.28094916856838587, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining directional local searches, orthogonal refinements, adaptive step-size per individual, occasional heavy-tailed Lévy-like escapes, and light recombination to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by default)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # best found (filled when __call__ runs)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # remaining evaluation budget\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # determine bounds: prefer func.bounds if available, otherwise use [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            # ensure they are full-dim arrays\n            if lb.shape == () or lb.size == 1:\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == () or ub.size == 1:\n                ub = np.full(self.dim, float(ub))\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        mean_span = float(np.mean(span))\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).ravel()\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # fallback to random search if budget extremely small\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop = []\n        pop_f = []\n        # initial sigma per individual (adaptive step-size)\n        base_sigma = max(1e-8, 0.25 * mean_span)\n        pop_sigma = []\n\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.5 + 0.5 * np.random.rand()))\n\n        pop = [np.asarray(p, dtype=float) for p in pop]\n        pop_f = np.asarray(pop_f, dtype=float) if len(pop_f) > 0 else np.array([])\n        pop_sigma = np.asarray(pop_sigma, dtype=float) if len(pop_sigma) > 0 else np.array([])\n\n        # if no population could be created (very small budget), do pure random sampling until exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # main loop\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), size=k, replace=False)\n            values = pop_f[inds]\n            parent_i = inds[int(np.argmin(values))]\n            parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / (nd + 1e-12)\n\n            # primary directional trial with stochastic step-length (Gaussian scaled)\n            step_scale = sigma * max(1e-12, np.abs(np.random.randn()) + 0.1)\n            x_try = parent + d * step_scale\n            # clip and evaluate\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    # accept and slightly increase sigma\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.2, mean_span)\n                    # continue to next iteration (exploit success)\n                    continue\n                else:\n                    # on failure, modestly shrink sigma for this parent\n                    pop_sigma[parent_i] = max(sigma * 0.92, 1e-12)\n\n            # local backtracking / small-step refinement along direction (geometric reductions)\n            improved = False\n            back_scale = 0.5\n            for bt in range(3):\n                sigma_bt = sigma * (back_scale ** (bt + 1))\n                x_try = parent + d * sigma_bt * (0.5 + 0.5 * np.random.rand())\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma_bt * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # subtract projection on d to make orthogonal\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr < 1e-12:\n                # degenerate: use random small gaussian\n                r = np.random.randn(self.dim)\n                nr = np.linalg.norm(r) + 1e-12\n            r = r / nr\n            x_try = parent + r * sigma * (0.3 + 0.7 * np.random.rand())\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, mean_span)\n                    continue\n                else:\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Use Cauchy (standard) to give heavy tail; normalize to a robust scale\n                v = np.random.standard_cauchy(size=self.dim)\n                # mitigate extreme outliers by scaling by median absolute value and clipping\n                med = np.median(np.abs(v)) + 1e-12\n                v = v / med\n                # limit its extreme amplitude\n                v = v * (0.5 * mean_span) * np.tanh(v / 10.0)\n                x_try = parent + v\n                # clip and eval\n                f_try, x_try = callf(x_try)\n                # if it's good, replace the worst in population\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try.copy()\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # do not continue; we allow further recombination attempts in same loop\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best_two = np.argsort(pop_f)[:2]\n                a, b = best_two[0], best_two[1]\n                # blend with random coefficient and small gaussian perturbation\n                alpha = 0.2 + 0.6 * np.random.rand()\n                x_try = alpha * pop[a] + (1 - alpha) * pop[b] + np.random.randn(self.dim) * (0.05 * mean_span)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # replace worst or parent upon improvement\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                    elif f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (small decay to encourage exploration elsewhere)\n            # If parent did not improve for a while, decay sigma slightly\n            # Implement a simple decay: 5% chance to nudge sigma down\n            if np.random.rand() < 0.05:\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.88, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_rand = np.random.uniform(lb, ub)\n                f_rand, x_rand = callf(x_rand)\n                pop[worst_i] = x_rand.copy()\n                pop_f[worst_i] = f_rand\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n            # make sure arrays remain consistent (in case any replacements happened)\n            # (pop is list of arrays; pop_f and pop_sigma are numpy arrays)\n            # if sizes changed unexpectedly, enforce consistency\n            # (no dynamic population size changes are performed here, so this is mostly a safety step)\n            if len(pop_f) != len(pop):\n                pop_f = np.array([float(func(p)) for p in pop])  # careful: this consumes budget if left; but should not happen\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.281 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11491954697748563, 0.27109854107342324, 0.3332901812406942, 0.4656637270917533, 0.2946273320750118, 0.34902847816658356, 0.2839138971272408, 0.2737413820186926, 0.2707219280456744, 0.15248667186729858]}, "task_prompt": ""}
{"id": "4f89d895-a007-4aec-b19c-a0909f15bd24", "fitness": 0.14775711782617296, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points that combine randomized directional local searches, orthogonal refinements, recombination and occasional Lévy-style jumps to robustly explore/exploit continuous black-box landscapes.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: size of population (optional)\n    - base_sigma: initial scale for step-length adaptation\n    - seed: RNG seed (optional)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, base_sigma=0.4, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that scales with dimensionality\n            self.pop_size = max(4, min(14, 4 + int(np.ceil(self.dim / 2))))\n        else:\n            self.pop_size = int(max(2, pop_size))\n        self.base_sigma = float(base_sigma)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # best-so-far\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds (func is expected to provide .bounds.lb / .bounds.ub)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # if scalars given, expand\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # sanity clamp\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        # local counters for remaining evaluations\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def ensure_array(x):\n            xa = np.asarray(x, dtype=float).reshape(self.dim)\n            return xa\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return np.inf, None\n            x = ensure_array(x)\n            x = np.clip(x, lb, ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        for i in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub, size=self.dim)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # randomized initial sigma per individual\n            pop_sigma.append(self.base_sigma * (0.7 + 0.6 * np.random.rand()))\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub, size=self.dim)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # If only one individual, try to create at least 2 for recombination\n        while len(pop) < 2 and remaining > 0:\n            x0 = np.random.uniform(lb, ub, size=self.dim)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(self.base_sigma * (0.7 + 0.6 * np.random.rand()))\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        while remaining > 0:\n            # maintain ordering info\n            idx_sorted = np.argsort(pop_f)\n            best_i = int(idx_sorted[0])\n            worst_i = int(idx_sorted[-1])\n            second_best_i = int(idx_sorted[1]) if len(idx_sorted) > 1 else best_i\n\n            # pick a parent via small tournament to balance exploration/exploitation\n            tour_k = min(3, len(pop))\n            candidates = np.random.choice(len(pop), size=tour_k, replace=False)\n            parent_i = int(sorted(candidates, key=lambda ii: pop_f[ii])[0])\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                continue\n            d /= nd\n\n            # scale base step length relative to search range\n            range_vec = ub - lb\n            # primary directional trial with stochasticized step-length\n            step_len = sigma * (0.5 + np.random.rand() * 1.5)  # varied multiplier\n            x_try = np.clip(x_parent + (step_len * d) * range_vec * 0.2, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(5.0, sigma * 1.12)\n            else:\n                # failure: reduce sigma moderately\n                pop_sigma[parent_i] = max(1e-12, sigma * 0.86)\n\n            # local backtracking / small-step refinement along direction (few tries)\n            if remaining > 0:\n                improved = False\n                fracs = [0.5, 0.25, 0.125]\n                for frac in fracs:\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + (frac * step_len * d) * range_vec * 0.2, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(5.0, pop_sigma[parent_i] * (1.05 + 0.05 * np.random.rand()))\n                        improved = True\n                        break\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make r orthogonal to d\n                proj = np.dot(r, d)\n                r = r - proj * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r /= nr\n                    ortho_scale = 0.08 + 0.12 * np.random.rand()\n                    x_try = np.clip(pop[parent_i] + ortho_scale * pop_sigma[parent_i] * r * range_vec, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(5.0, pop_sigma[parent_i] * 1.08)\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if remaining > 0 and np.random.rand() < 0.06:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(size=self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale vector adaptively using current parent's sigma and range\n                scale_vec = 0.3 * (ub - lb)\n                x_try = np.clip(x_parent + pop_sigma[parent_i] * scale_vec * step, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.6)\n                else:\n                    # if promising, replace the worst\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, pop_sigma[parent_i] * 0.5)\n\n            # recombination exploitation: mix two best and small noise\n            if remaining > 0 and len(pop) >= 2:\n                a, b = idx_sorted[0], idx_sorted[1]\n                if a == b and len(idx_sorted) > 2:\n                    b = idx_sorted[2]\n                beta = 0.3 + 0.4 * np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.03 * pop_sigma[a]) * (np.random.randn(self.dim) * (range_vec))\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    # replace parent if improved\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(1e-12, 0.9 * pop_sigma[parent_i])\n                    # try to inject into population by replacing the worst if it's better\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, 0.5 * pop_sigma[parent_i])\n\n            # adapt parent sigma on failure: small random walk for sigma\n            if remaining > 0:\n                # degrade slightly if many failures, occasional reset if stuck\n                idx_sorted = np.argsort(pop_f)\n                best_f = pop_f[int(idx_sorted[0])]\n                for ii in range(len(pop)):\n                    # small stochastic perturbation to sigma\n                    pop_sigma[ii] = max(1e-12, pop_sigma[ii] * (0.98 + 0.04 * np.random.rand()))\n                    # if an individual is much worse, increase sigma to encourage exploration\n                    if pop_f[ii] > best_f + 1e-6 and np.random.rand() < 0.05:\n                        pop_sigma[ii] = min(5.0, pop_sigma[ii] * (1.5 + 0.5 * np.random.rand()))\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                x_new = np.random.uniform(lb, ub, size=self.dim)\n                f_new, x_new = callf(x_new)\n                if x_new is not None:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = self.base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # small population maintenance: if some individual hasn't improved for long, nudge it\n            if remaining > 0 and np.random.rand() < 0.03:\n                i = np.random.randint(len(pop))\n                pop[i] = np.clip(pop[i] + 0.05 * pop_sigma[i] * np.random.randn(self.dim) * range_vec, lb, ub)\n                f_new, x_new = callf(pop[i])\n                if x_new is not None:\n                    pop[i] = x_new\n                    pop_f[i] = f_new\n\n        # finished budget\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.148 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13217166933764757, 0.15690568900127033, 0.19860703383679912, 0.11915146658535614, 0.11490384389655728, 0.1368120820341675, 0.17756953915131224, 0.154090081380597, 0.16281583783254405, 0.12454393520547835]}, "task_prompt": ""}
{"id": "558735e5-8b46-453e-9a23-e8c2f7d6d9d7", "fitness": 0.39725763421139637, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional searches with orthogonal refinements and occasional heavy-tailed Lévy jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to moderate function of dim)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimensionality but stays small\n            self.pop_size = max(4, min(40, int(4 + 1.5 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(int(seed))\n\n    def __call__(self, func):\n        # Prepare bounds: use func.bounds if available, otherwise default [-5,5]\n        if hasattr(func, \"bounds\"):\n            try:\n                lb = np.asarray(func.bounds.lb, dtype=float)\n                ub = np.asarray(func.bounds.ub, dtype=float)\n            except Exception:\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n        else:\n            # default BBOB search box\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure full-dim arrays\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub.item()))\n\n        # simple clamp function\n        def clamp(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # state for tracking budget and best solution\n        remaining = int(self.budget)\n        self.f_opt = float(\"inf\")\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = clamp(x)\n            # Use the provided function; decrement budget AFTER checking\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to purely random search\n        if self.budget < 6:\n            # simple random search until budget exhausted\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop_size = min(self.pop_size, max(2, remaining // 3))  # ensure we can evaluate a few rounds\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        # base scale for initial sigmas\n        base_sigma = max(1e-8, 0.25 * float(np.mean(ub - lb)))\n\n        # create initial population or fallback to random search if not enough budget\n        if remaining < pop_size:\n            # fallback random search to consume budget\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        for i in range(pop_size):\n            x = np.random.uniform(lb, ub)\n            f, x = callf(x)\n            pop.append(x.copy())\n            pop_f.append(f)\n            # initialize sigma with some diversity\n            pop_sigma.append(base_sigma * (0.8 + 0.6 * np.random.rand()))\n\n        pop = [np.asarray(x, dtype=float) for x in pop]\n        pop_f = np.asarray(pop_f, dtype=float)\n        pop_sigma = np.asarray(pop_sigma, dtype=float)\n\n        # main optimization loop\n        while remaining > 0:\n            # pick a parent via small tournament of size 3\n            k = min(3, len(pop))\n            ids = np.random.choice(len(pop), size=k, replace=False)\n            # choose the best of the tournament by fitness\n            parent_i = ids[int(np.argmin(pop_f[ids]))]\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            improved = False\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            x_try = clamp(x_parent + alpha * d)\n            if remaining > 0:\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    # accept improvement\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.15, base_sigma * 10.0)\n                    improved = True\n\n            # local backtracking / small-step refinement along direction (few tries)\n            if remaining > 0:\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    x_try = clamp(x_parent + alpha * frac * d)\n                    if remaining <= 0:\n                        break\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        # small reduction in sigma to focus search locally\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        improved = True\n\n            # try an orthogonal perturbation for local diversification\n            if remaining > 0:\n                r = np.random.randn(self.dim)\n                # make r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr < 1e-12:\n                    # if unlucky, pick a random orthonormal by swapping coordinate\n                    r = np.random.randn(self.dim)\n                    r = r - np.dot(r, d) * d\n                    nr = np.linalg.norm(r)\n                    if nr < 1e-12:\n                        r = np.ones(self.dim)\n                r = r / max(1e-12, nr)\n                x_try = clamp(x_parent + 0.6 * sigma * r)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                        improved = True\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(self.dim)\n                # robust scale: median absolute value\n                scale = float(np.median(np.abs(step)))\n                if scale < 1e-12:\n                    scale = 1.0\n                # normalize but keep heavy-tail shape\n                step = step / scale\n                # scale vector by local sigma blended with base\n                scale_vec = 1.8 * sigma + 0.6 * base_sigma\n                x_try = clamp(x_parent + step * (scale_vec * (0.5 + np.random.rand())))\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(1e-12, sigma * 0.5)\n                    else:\n                        # if not taken as parent, try to replace worst if it's better\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try.copy()\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                # pick best two\n                best_ids = np.argsort(pop_f)[:2]\n                a, b = best_ids[0], best_ids[1]\n                mix = 0.5 * (pop[a] + pop[b]) + 0.05 * base_sigma * np.random.randn(self.dim)\n                mix = clamp(mix)\n                if remaining > 0:\n                    f_mix, mix = callf(mix)\n                    worst_i = int(np.argmax(pop_f))\n                    # inject if better than worst\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = mix.copy()\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n            # adapt parent sigma on failure (if no improvement happened)\n            if not improved:\n                pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n            else:\n                # slight shrink to exploit around new good point\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.98, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.03 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                if remaining > 0:\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # preventive fitness array sanity (in case of race conditions)\n            # (no function calls; just numeric safety)\n            pop_f = np.asarray(pop_f, dtype=float)\n            pop_sigma = np.maximum(np.asarray(pop_sigma, dtype=float), 1e-12)\n\n            # If only a few evaluations remain, switch to greedy local sampling around best\n            if remaining <= 6 and remaining > 0:\n                best_i = int(np.argmin(pop_f))\n                base = pop[best_i].copy()\n                for _ in range(remaining):\n                    cand = clamp(base + 0.12 * base_sigma * np.random.randn(self.dim))\n                    callf(cand)\n                break\n\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.397 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13882991707073045, 0.1505104628344136, 0.4284327357271239, 0.8804301155657215, 0.5793932868039935, 0.5314502558615428, 0.26873478777457394, 0.38675655962733213, 0.4784297635080116, 0.12960845734051984]}, "task_prompt": ""}
{"id": "2d96ccbb-35cd-4c2a-9fba-e5ec892ec196", "fitness": "-inf", "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based directional local searches with adaptive step-sizes, orthogonal refinements and occasional heavy-tailed Lévy jumps to balance exploitation and escape.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.default_rng(self.seed)\n\n        # sensible default population size: grows slowly with dim but modest\n        if pop_size is None:\n            self.pop_size = max(6, min(40, int(6 + 2.0 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n\n        # internal trackers set on call\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize black-box `func` using at most self.budget function evaluations.\n        func(x) -> scalar; bounds available via func.bounds.lb and func.bounds.ub\n        Returns (f_best, x_best)\n        \"\"\"\n        # Prepare bounds (allow scalar or array)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # Clip safety\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        size_range = ub - lb\n        # Reset counters\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Helper to evaluate and track budget; raises to abort when budget used\n        class BudgetExhausted(Exception):\n            pass\n\n        def callf(x):\n            if self.evals >= self.budget:\n                raise BudgetExhausted()\n            x = np.array(x, dtype=float)\n            x = np.clip(x, lb, ub)\n            f = func(x)\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget is extremely small, perform simple random search\n        if self.budget <= 10:\n            for _ in range(self.budget):\n                x = self.rng.uniform(lb, ub)\n                f = func(x)\n                self.evals += 1\n                if f < self.f_opt:\n                    self.f_opt = float(f)\n                    self.x_opt = x.copy()\n            return self.f_opt, self.x_opt\n\n        # Initialize population (uniform)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        init_sigma_base = 0.2 * np.maximum(size_range, 1.0)  # robust per-dim base\n        # We'll use scalar sigma per individual measured relative to average scale\n        avg_scale = float(np.mean(size_range))\n        init_sigma_scalar = max(1e-6, 0.15 * avg_scale)\n\n        # try to fill population (stop if budget runs out)\n        try:\n            for i in range(self.pop_size):\n                if self.evals >= self.budget:\n                    break\n                x = self.rng.uniform(lb, ub)\n                f, x = callf(x)\n                pop.append(x)\n                pop_f.append(f)\n                # initialize per-individual sigma near init_sigma_scalar with noise\n                pop_sigma.append(init_sigma_scalar * float(1.0 + 0.5 * self.rng.normal()))\n        except BudgetExhausted:\n            pass\n\n        pop = np.array(pop) if len(pop) > 0 else np.zeros((0, self.dim))\n        pop_f = np.array(pop_f) if len(pop_f) > 0 else np.array([])\n        pop_sigma = np.array(pop_sigma) if len(pop_sigma) > 0 else np.array([])\n\n        # If no population created (very tiny budget), fallback to best-so-far\n        if pop.shape[0] == 0:\n            return self.f_opt, self.x_opt\n\n        # If budget left but population smaller than intended, allow reduced loops\n        n = pop.shape[0]\n\n        # Parameters\n        tournament_k = min(3, n)\n        orth_tries = 2\n        backtrack_tries = 3\n        backtrack_factor = 0.5\n        sigma_increase = 1.12\n        sigma_decrease = 0.92\n        min_sigma = 1e-12\n        levy_prob = 0.06\n        levy_scale_factor = 1.2  # multiplies sigma for heavy-tail jumps\n        recomb_prob = 0.12\n        rejuvenation_prob = 0.03\n\n        # Main loop until budget exhausted\n        try:\n            while self.evals < self.budget:\n                # pick a parent via small tournament\n                inds = self.rng.choice(n, size=tournament_k, replace=False)\n                parent_i = inds[np.argmin(pop_f[inds])]\n                x_parent = pop[parent_i].copy()\n                sigma_parent = max(min_sigma, abs(pop_sigma[parent_i]) if np.isfinite(pop_sigma[parent_i]) else init_sigma_scalar)\n\n                # sample a random search direction and normalize\n                d = self.rng.normal(size=self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    # degeneracy: pick axis-aligned direction\n                    axis = self.rng.integers(self.dim)\n                    d = np.zeros(self.dim)\n                    d[axis] = 1.0\n                    nd = 1.0\n                d = d / nd\n\n                # primary directional trial with stochasticized step-length\n                alpha = sigma_parent * (1.0 + 0.3 * self.rng.normal())\n                x_try = np.clip(x_parent + alpha * d, lb, ub)\n\n                f_try, x_try = callf(x_try)\n\n                improved = False\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(min_sigma, sigma_parent * sigma_increase)\n                    improved = True\n                else:\n                    # local backtracking / small-step refinement along direction\n                    s_try = alpha\n                    for bt in range(backtrack_tries):\n                        s_try = s_try * backtrack_factor\n                        if self.evals >= self.budget:\n                            break\n                        x_bt = np.clip(x_parent + s_try * d, lb, ub)\n                        f_bt, x_bt = callf(x_bt)\n                        if f_bt < pop_f[parent_i]:\n                            pop[parent_i] = x_bt\n                            pop_f[parent_i] = f_bt\n                            pop_sigma[parent_i] = max(min_sigma, sigma_parent * (1.0 + 0.5 * (1 - bt/backtrack_tries)))\n                            improved = True\n                            break\n\n                if improved:\n                    # move on to next iteration\n                    continue\n\n                # try an orthogonal perturbation for local diversification\n                for t in range(orth_tries):\n                    # generate vector orthogonal to d\n                    v = self.rng.normal(size=self.dim)\n                    v = v - np.dot(v, d) * d\n                    nv = np.linalg.norm(v)\n                    if nv < 1e-12:\n                        continue\n                    v = v / nv\n                    step = sigma_parent * 0.6 * (1.0 + 0.2 * self.rng.normal())\n                    x_orth = np.clip(x_parent + step * v, lb, ub)\n                    f_orth, x_orth = callf(x_orth)\n                    if f_orth < pop_f[parent_i]:\n                        pop[parent_i] = x_orth\n                        pop_f[parent_i] = f_orth\n                        pop_sigma[parent_i] = max(min_sigma, sigma_parent * 1.05)\n                        improved = True\n                        break\n                    # else try next orth vector\n\n                if improved:\n                    continue\n\n                # occasional Lévy-like jump to escape local basins (heavy-tailed)\n                if self.rng.random() < levy_prob and self.evals < self.budget:\n                    # sample Cauchy-like heavy tail vector\n                    cauch = self.rng.standard_cauchy(size=self.dim)\n                    # robust normalization to keep heavy-tail but avoid infinite scale\n                    med = float(np.median(np.abs(cauch))) + 1e-12\n                    scale_vec = cauch / med\n                    step = sigma_parent * levy_scale_factor\n                    x_jump = np.clip(x_parent + step * scale_vec, lb, ub)\n                    f_jump, x_jump = callf(x_jump)\n                    # replace worst if improves significantly else maybe keep as candidate\n                    worst_i = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_i]:\n                        pop[worst_i] = x_jump\n                        pop_f[worst_i] = f_jump\n                        pop_sigma[worst_i] = max(min_sigma, sigma_parent * 0.8)\n                    # continue main loop (do not attempt recombination this turn)\n                    continue\n\n                # recombination exploitation: mix two best and small noise\n                if self.rng.random() < recomb_prob and n >= 2 and self.evals < self.budget:\n                    best_i = int(np.argmin(pop_f))\n                    # choose another individual somewhat better-biased\n                    candidates = np.argsort(pop_f)[:max(2, min(6, n))]\n                    sec_i = int(self.rng.choice(candidates[candidates != best_i], size=1))\n                    # mix\n                    mix = 0.5 + 0.3 * self.rng.normal()\n                    noise = 0.05 * avg_scale * self.rng.normal(size=self.dim)\n                    x_recomb = np.clip(pop[best_i] * mix + pop[sec_i] * (1.0 - mix) + noise, lb, ub)\n                    f_recomb, x_recomb = callf(x_recomb)\n                    # replace worst or parent if improvement\n                    if f_recomb < pop_f[parent_i]:\n                        pop[parent_i] = x_recomb\n                        pop_f[parent_i] = f_recomb\n                        pop_sigma[parent_i] = max(min_sigma, sigma_parent * 0.9)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_recomb < pop_f[worst_i]:\n                            pop[worst_i] = x_recomb\n                            pop_f[worst_i] = f_recomb\n                            pop_sigma[worst_i] = max(min_sigma, sigma_parent * 0.8)\n                    # continue main loop\n                    continue\n\n                # no success: adapt parent sigma downwards slightly\n                pop_sigma[parent_i] = max(min_sigma, sigma_parent * sigma_decrease)\n\n                # occasional population rejuvenation by replacing the worst with a random sample\n                if self.rng.random() < rejuvenation_prob and self.evals < self.budget:\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = self.rng.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    if f_new < pop_f[worst_i]:\n                        pop[worst_i] = x_new\n                        pop_f[worst_i] = f_new\n                        pop_sigma[worst_i] = init_sigma_scalar\n\n                # loop continues until budget exhausted\n\n        except BudgetExhausted:\n            pass\n\n        return float(self.f_opt), (self.x_opt.copy() if self.x_opt is not None else None)", "configspace": "", "generation": 0, "feedback": "In the code, line 226, in __call__, the following error occurred:\nTypeError: only 0-dimensional arrays can be converted to Python scalars\nOn line: sec_i = int(self.rng.choice(candidates[candidates != best_i], size=1))", "error": "In the code, line 226, in __call__, the following error occurred:\nTypeError: only 0-dimensional arrays can be converted to Python scalars\nOn line: sec_i = int(self.rng.choice(candidates[candidates != best_i], size=1))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5d77a71d-5663-409d-915f-5fbeaa848cfd", "fitness": 0.5217076387513884, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a compact population-based continuous optimizer that blends randomized directional local searches, orthogonal refinements, recombination, adaptive per-individual step-sizes and occasional Cauchy/Lévy-like jumps to escape basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality (will be adapted from func.bounds if mismatched)\n    - pop_size: optional fixed population size; otherwise auto-scaled by dim/budget\n    - seed: RNG seed for reproducibility\n\n    Returns (f_opt, x_opt)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        rng = self.rng\n\n        # read bounds and normalize shapes to dim\n        lb = np.asarray(func.bounds.lb).ravel()\n        ub = np.asarray(func.bounds.ub).ravel()\n        # if bounds provide dimension, prefer that\n        if lb.size != self.dim:\n            self.dim = lb.size\n        dim = self.dim\n        lb = np.broadcast_to(lb, (dim,))\n        ub = np.broadcast_to(ub, (dim,))\n        rng = self.rng\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # helper to call func while respecting budget, clipping to bounds and tracking best\n        def callf(x):\n            nonlocal evals, f_opt, x_opt\n            if evals >= self.budget:\n                return np.inf\n            x = np.asarray(x).astype(float).ravel()\n            if x.size != dim:\n                x = x[:dim]\n            x = np.clip(x, lb, ub)\n            f = func(x)\n            evals += 1\n            if f < f_opt:\n                f_opt = float(f)\n                x_opt = x.copy()\n            return float(f)\n\n        # fallback: if budget too small just do random search\n        if self.budget < max(10, dim):\n            for _ in range(self.budget):\n                x = rng.uniform(lb, ub)\n                callf(x)\n            return f_opt, x_opt\n\n        # Determine population size: modestly scales with dim but bound by budget\n        if self.pop_size is None:\n            pop_size = int(min(max(4, dim * 4), max(4, self.budget // 10)))\n        else:\n            pop_size = int(self.pop_size)\n        pop = []\n\n        # initial scale: typical range-based sigma\n        global_scale = np.maximum(1e-9, (ub - lb).mean())\n        init_sigma_base = global_scale * 0.25\n\n        # initialize population\n        for i in range(pop_size):\n            if evals >= self.budget:\n                break\n            x = rng.uniform(lb, ub)\n            f = callf(x)\n            # per-individual step-size: slightly randomized\n            sigma = init_sigma_base * (0.5 + rng.rand())\n            pop.append({'x': x, 'f': f, 'sigma': float(sigma)})\n\n        # if no individuals could be evaluated (extremely small budget), pure random search\n        if len(pop) == 0:\n            while evals < self.budget:\n                x = rng.uniform(lb, ub)\n                callf(x)\n            return f_opt, x_opt\n\n        # handy accessors\n        def pop_matrix():\n            return np.vstack([p['x'] for p in pop])\n\n        def idx_worst():\n            return int(np.argmax([p['f'] for p in pop]))\n\n        def idx_best():\n            return int(np.argmin([p['f'] for p in pop]))\n\n        # main loop until budget exhausted\n        while evals < self.budget:\n            # small tournament selection for parent\n            tsize = min(3, len(pop))\n            t_idx = rng.choice(len(pop), tsize, replace=False)\n            p_idx = t_idx[np.argmin([pop[i]['f'] for i in t_idx])]\n            parent = pop[p_idx]\n\n            # random normalized direction\n            d = rng.normal(size=dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochastic step length\n            # step-length uses individual's sigma and a mild multiplicative noise\n            step_len = parent['sigma'] * (1.0 + 0.3 * rng.randn())\n            x_trial = parent['x'] + step_len * d\n            x_trial = np.clip(x_trial, lb, ub)\n            f_trial = callf(x_trial)\n            if f_trial < parent['f']:\n                parent['x'] = x_trial\n                parent['f'] = f_trial\n                parent['sigma'] *= 1.12  # success increase\n                # keep population entry updated\n                pop[p_idx] = parent\n                continue  # successful step -> continue main loop\n\n            # local backtracking / small-step refinement along direction (a few tries)\n            back_step = step_len\n            for back in range(3):\n                if evals >= self.budget:\n                    break\n                back_step *= 0.5\n                x_bt = parent['x'] + back_step * d\n                x_bt = np.clip(x_bt, lb, ub)\n                f_bt = callf(x_bt)\n                if f_bt < parent['f']:\n                    parent['x'] = x_bt\n                    parent['f'] = f_bt\n                    parent['sigma'] *= 1.06\n                    pop[p_idx] = parent\n                    break\n\n            # orthogonal perturbation for local diversification\n            if evals < self.budget:\n                v = rng.normal(size=dim)\n                # make orthogonal to d\n                v = v - np.dot(v, d) * d\n                nrm_v = np.linalg.norm(v)\n                if nrm_v > 1e-12:\n                    v /= nrm_v\n                    step = parent['sigma'] * 0.6 * (0.5 + rng.rand())\n                    x_o = parent['x'] + step * v\n                    x_o = np.clip(x_o, lb, ub)\n                    f_o = callf(x_o)\n                    if f_o < parent['f']:\n                        parent['x'] = x_o\n                        parent['f'] = f_o\n                        parent['sigma'] *= 1.05\n                        pop[p_idx] = parent\n\n            # occasional Lévy/Cauchy-like jump to escape basin\n            if evals < self.budget and rng.rand() < 0.06:\n                # Cauchy-like heavy-tailed vector\n                c = rng.standard_cauchy(size=dim)\n                # robust scale from population MAD\n                P = pop_matrix()\n                med = np.median(P, axis=0)\n                mad = np.median(np.abs(P - med), axis=0)\n                robust_scale = max(1e-9, np.mean(mad))\n                c_scaled = c * robust_scale\n                # normalize to avoid absolute extreme scaling but keep heavy-tail\n                denom = np.percentile(np.abs(c_scaled), 90) + 1e-12\n                c_scaled = c_scaled / denom\n                # scale relative to the bounds\n                jump = c_scaled * (ub - lb) * (0.6 + 0.8 * rng.rand())\n                x_jump = parent['x'] + jump\n                x_jump = np.clip(x_jump, lb, ub)\n                f_jump = callf(x_jump)\n                worst_i = idx_worst()\n                if f_jump < pop[worst_i]['f']:\n                    # replace worst if jump is beneficial\n                    pop[worst_i] = {'x': x_jump, 'f': f_jump, 'sigma': parent['sigma'] * 0.8}\n                else:\n                    # maybe keep as candidate by replacing parent if slightly better\n                    if f_jump < parent['f']:\n                        parent['x'] = x_jump\n                        parent['f'] = f_jump\n                        parent['sigma'] *= 1.02\n                        pop[p_idx] = parent\n                # continue main loop after jump attempt (counts as exploration)\n                continue\n\n            # recombination exploitation: mix two best with small noise\n            if evals < self.budget:\n                # pick two best and mix\n                sorted_idx = np.argsort([p['f'] for p in pop])\n                i1, i2 = sorted_idx[0], sorted_idx[1] if len(sorted_idx) > 1 else sorted_idx[0]\n                b1, b2 = pop[i1], pop[i2]\n                mix = 0.5 * (b1['x'] + b2['x'])\n                mix += parent['sigma'] * 0.12 * rng.randn(dim)\n                mix = np.clip(mix, lb, ub)\n                f_mix = callf(mix)\n                if f_mix < parent['f']:\n                    parent['x'] = mix\n                    parent['f'] = f_mix\n                    parent['sigma'] *= 1.08\n                    pop[p_idx] = parent\n                else:\n                    # possibly inject into population if better than worst\n                    worst_i = idx_worst()\n                    if f_mix < pop[worst_i]['f']:\n                        pop[worst_i] = {'x': mix, 'f': f_mix, 'sigma': parent['sigma'] * 0.6}\n                    else:\n                        # adapt parent sigma on failure\n                        parent['sigma'] *= 0.95\n                        pop[p_idx] = parent\n\n            # occasional population rejuvenation (replace worst with random sample)\n            if evals < self.budget and rng.rand() < max(0.01, 20.0 / max(1, self.budget)):\n                worst_i = idx_worst()\n                x_new = rng.uniform(lb, ub)\n                f_new = callf(x_new)\n                if f_new < pop[worst_i]['f']:\n                    pop[worst_i] = {'x': x_new, 'f': f_new, 'sigma': init_sigma_base}\n\n            # small safeguard to nudge sigmas if population collapses\n            if evals < self.budget and rng.rand() < 0.01:\n                # keep some diversity\n                for p in pop:\n                    p['sigma'] = max(p['sigma'], 1e-6 * global_scale)\n\n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.522 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15139386840183244, 0.19062893539555126, 0.9213847699720376, 0.9707886557154335, 0.9393169993120852, 0.9500647879734652, 0.24498433671735964, 0.40968842030260955, 0.26695140160380426, 0.17187421211970577]}, "task_prompt": ""}
{"id": "e2f1f61d-b7d0-47ba-a421-065f0e180dd5", "fitness": 0.4829623056761946, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional local searches with orthogonal refinements and rare Lévy jumps for robust exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n        # best seen\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds as numpy arrays of length dim\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # ensure bounds length matches dim\n        assert lb.size == self.dim and ub.size == self.dim\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # local helper to call func and track budget & best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds to be safe\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            # return scalar and the actually evaluated (clipped) point\n            return f, x.copy()\n\n        # trivial fallback if no budget\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population (n_init points or as many evaluations as budget allows)\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))  # initial step scale\n        n_init = min(self.pop_size, remaining)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        for _ in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize individual sigma with some diversity\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * np.random.rand()))\n\n        # If no population created (very small budget), do random sampling until exhausted\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # If we created fewer than desired population (budget limited), keep pop_size as created\n        # Main optimization loop\n        while remaining > 0:\n            # recompute some helpers: best indices and worst index\n            idx_sorted = np.argsort(pop_f)\n            best2 = idx_sorted[:2] if len(pop) >= 2 else idx_sorted[:1]\n            worst_i = int(idx_sorted[-1])\n\n            # select a parent via small tournament (choose k random and pick best among them)\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = int(inds[np.argmin(values)])\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining > 0:\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                    continue\n\n            # local backtracking / small-step refinement along direction (few fractional steps)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like (heavy-tailed) jump to escape local basins\n            if np.random.rand() < 0.08 and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization to avoid single huge coordinate but keep heavy-tail property\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # replace the worst if this is better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue to next iteration after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and remaining > 0:\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_try, x_try = callf(x_try)\n                    except RuntimeError:\n                        break\n                    # replace parent if improved, else try to inject into population by replacing worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (conservative shrink)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                if remaining > 0:\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        break\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget or early stop\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.483 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11193976478391099, 0.15852614383094088, 0.454701973304233, 0.9765939990927605, 0.5900684707061306, 0.8352808831349968, 0.23652187045278228, 0.3919255641860221, 0.9291942876049156, 0.14487009966525266]}, "task_prompt": ""}
{"id": "ef4887b7-aaf8-431d-9f23-d21b815c4aaa", "fitness": 0.4450608996525712, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional searches with orthogonal refinements and occasional Lévy jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional explicit population size (default scales with dim)\n    - seed: optional RNG seed for reproducibility\n\n    Main ideas:\n    - Maintain a small population of candidate solutions with individual adaptive sigmas.\n    - For each parent, perform randomized directional trials (primary step + backtracking fractions).\n    - Use orthogonal perturbations to diversify local moves.\n    - Occasionally perform heavy-tailed (Cauchy-like) Lévy jumps to escape basins.\n    - Recombine top individuals occasionally and rejuvenate the worst ones randomly.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # moderate-sized population that scales with dimensionality\n            self.pop_size = int(min(max(4, 2 * self.dim), 40))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds handling (allow scalar or vector bounds)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # initialize bookkeeping\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            # update best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search of remaining budget\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initial population\n        pop = []         # list of np arrays\n        pop_f = []       # list of floats\n        pop_sigma = []   # list of floats\n        # initial scale is a fraction of search range\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))\n\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # slightly heterogeneous sigmas\n            pop_sigma.append(base_sigma * (0.7 + 0.6 * np.random.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # main loop\n        while remaining > 0:\n            n_pop = len(pop)\n            # identify worst index for quick replacement decisions\n            worst_i = int(np.argmax(pop_f))\n            # tournament selection for parent (small tournament)\n            k = min(3, n_pop)\n            inds = np.random.choice(n_pop, size=k, replace=False)\n            # choose best among tournament\n            parent_i = int(inds[np.argmin([pop_f[idx] for idx in inds])])\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-16:\n                # fallback to coordinate direction if degenerate\n                d = np.zeros(self.dim)\n                d[np.random.randint(0, self.dim)] = 1.0\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            # success: accept and slightly increase sigma\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                # small chance to continue exploiting this parent immediately\n                if np.random.rand() < 0.6:\n                    continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_bt = np.clip(x_parent + (alpha * frac) * d, lb, ub)\n                try:\n                    f_bt, x_bt = callf(x_bt)\n                except RuntimeError:\n                    break\n                if f_bt < pop_f[parent_i]:\n                    pop[parent_i] = x_bt\n                    pop_f[parent_i] = f_bt\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                # orthogonal step magnitude based on sigma and a random factor\n                ortho_step = sigma * (0.5 + 0.5 * np.random.rand())\n                x_ort = np.clip(x_parent + ortho_step * r, lb, ub)\n                try:\n                    f_ort, x_ort = callf(x_ort)\n                except RuntimeError:\n                    break\n                if f_ort < pop_f[parent_i]:\n                    pop[parent_i] = x_ort\n                    pop_f[parent_i] = f_ort\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    continue\n                # small chance to accept as diversification into worst slot if somewhat competitive\n                if f_ort < pop_f[worst_i]:\n                    pop[worst_i] = x_ort\n                    pop_f[worst_i] = f_ort\n                    pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization to avoid extreme numeric values\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_lev = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_lev, x_lev = callf(x_lev)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population\n                if f_lev < pop_f[worst_i]:\n                    pop[worst_i] = x_lev\n                    pop_f[worst_i] = f_lev\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_mix = np.clip(mix + noise, lb, ub)\n                try:\n                    f_mix, x_mix = callf(x_mix)\n                except RuntimeError:\n                    break\n                # replace parent if improved, else possibly replace worst\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (conservative shrink)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n            # occasionally shrink population if one individual is very bad and budget is tight\n            if remaining < max(10, 0.05 * self.budget) and len(pop) > 4:\n                # keep top half\n                keep_k = max(4, len(pop) // 2)\n                order = np.argsort(pop_f)\n                new_pop = [pop[int(i)].copy() for i in order[:keep_k]]\n                new_pop_f = [float(pop_f[int(i)]) for i in order[:keep_k]]\n                new_pop_sigma = [float(pop_sigma[int(i)]) for i in order[:keep_k]]\n                pop = new_pop\n                pop_f = new_pop_f\n                pop_sigma = new_pop_sigma\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.445 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12173076154181162, 0.1603576332200276, 0.5936072696164189, 0.9640402713189771, 0.3776087345025305, 0.7678332602062725, 0.2557412051575437, 0.4354842536746232, 0.6341036725190314, 0.14010193476847577]}, "task_prompt": ""}
{"id": "14133347-6bf4-4a35-83f9-b2d0f9a3c293", "fitness": 0.5101308154170567, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points combining randomized directional local searches, orthogonal refinements, recombination and occasional Lévy-like jumps to efficiently explore/exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to a modest function of dim)\n    - seed: optional random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Bounds normalization to arrays of length dim\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # useful range vector\n        range_vec = ub - lb\n        mean_range = float(np.mean(range_vec))\n\n        # reset results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # internal evaluation counter and guarded function caller\n        eval_count = 0\n\n        def callf(x):\n            \"\"\"Evaluate x (clipped to bounds), increment eval_count, update best.\"\"\"\n            nonlocal eval_count\n            if eval_count >= self.budget:\n                return None  # indicate no evaluation performed\n            x = np.asarray(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            eval_count += 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is zero or negative simply return\n        if self.budget <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        base_sigma = max(1e-12, 0.2 * mean_range)  # initial typical step (~20% of mean range)\n\n        n_init = min(self.pop_size, self.budget)\n        for i in range(n_init):\n            if eval_count >= self.budget:\n                break\n            x0 = np.random.uniform(lb, ub)\n            res = callf(x0)\n            if res is None:\n                break\n            f0, x0 = res\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            # give each individual a slightly different sigma to promote diversity\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while eval_count < self.budget:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # main optimization loop\n        # Mix directional local search, orthogonal tries, Lévy jumps, recombination and rejuvenation\n        while eval_count < self.budget:\n            # pick a parent via small tournament\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            # choose the best among the sampled indices\n            best_local = inds[int(np.argmin([pop_f[j] for j in inds]))]\n            parent_i = int(best_local)\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-16:\n                continue\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if eval_count < self.budget:\n                res = callf(x_try)\n                if res is None:\n                    break\n                f_try, x_try = res\n            else:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.15, np.mean(range_vec))\n                # successful directional step; continue to next iteration\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if eval_count >= self.budget:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                res = callf(x_try)\n                if res is None:\n                    break\n                f_try, x_try = res\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.08, np.mean(range_vec))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d  # make orthogonal to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if eval_count < self.budget:\n                    res = callf(x_try)\n                    if res is None:\n                        break\n                    f_try, x_try = res\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(range_vec))\n                        # after orthogonal improvement, go to next iter\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < 0.08 and eval_count < self.budget:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = np.random.standard_cauchy(self.dim)\n                denom = float(np.percentile(np.abs(step), 90) + 1e-12)\n                step = step / denom\n                scale_vec = 0.2 * range_vec\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if eval_count < self.budget:\n                    res = callf(x_try)\n                    if res is None:\n                        break\n                    f_try, x_try = res\n                    # if it's good replace the worst in population\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                        # keep going after escape attempt\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and eval_count < self.budget:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * range_vec) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if eval_count < self.budget:\n                    res = callf(x_try)\n                    if res is None:\n                        break\n                    f_try, x_try = res\n                    # replace parent if improved, else possibly replace worst\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try.copy()\n                            pop_f[worst_i] = f_try\n\n            # adapt parent sigma on failure (mild shrink)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.03 and eval_count < self.budget:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                if eval_count < self.budget:\n                    res = callf(x_new)\n                    if res is None:\n                        break\n                    f_new, x_new = res\n                    pop[worst_i] = x_new.copy()\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget or early stop\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.510 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1701356730204795, 0.17098894759742367, 0.8971049903031314, 0.18050752742742648, 0.9211165178664844, 0.9516210022812464, 0.2527132560185098, 0.5313547199353464, 0.8720777823952925, 0.15368773732522722]}, "task_prompt": ""}
{"id": "4dbba879-b551-4a39-91af-297449132331", "fitness": 0.5239573278211652, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional local searches with orthogonal refinements and occasional Lévy jumps for robust global/local trade-off.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults adaptively with dim)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = int(max(4, min(40, 6 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # get bounds, be robust if func.bounds.* are scalars or missing\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            # default BBOB bounds\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure lb/ub are full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                # signal to outer loop that budget exhausted\n                raise StopIteration()\n            x = np.asarray(x, dtype=float)\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x.copy()\n\n        # Initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale\n\n        try:\n            for i in range(n_init):\n                x0 = np.random.uniform(lb, ub)\n                f0, x0 = callf(x0)\n                pop.append(x0)\n                pop_f.append(f0)\n                # individual sigma initialized from base_sigma with random jitter\n                pop_sigma.append(max(1e-12, base_sigma * (0.5 + np.random.rand())))\n        except StopIteration:\n            # budget exhausted during initialization\n            if self.x_opt is None:\n                return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n            return float(self.f_opt), self.x_opt.copy()\n\n        if len(pop) == 0:\n            # nothing evaluated\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = [np.asarray(x, dtype=float) for x in pop]\n        pop_f = [float(v) for v in pop_f]\n        pop_sigma = [float(s) for s in pop_sigma]\n\n        # Main search loop\n        try:\n            while remaining > 0:\n                # pick a parent via small tournament to balance exploration/exploitation\n                tsize = min(3, len(pop))\n                inds = np.random.choice(len(pop), tsize, replace=False)\n                values = [pop_f[i] for i in inds]\n                parent_i = int(inds[int(np.argmin(values))])\n                x_parent = pop[parent_i].copy()\n                sigma = pop_sigma[parent_i]\n\n                # sample a random search direction (normalized)\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    # skip this iteration and slightly perturb sigma\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    continue\n                d = d / nd\n\n                # primary directional trial with stochasticized step-length\n                alpha = sigma * (1.0 + 0.5 * np.random.randn())\n                x_try = x_parent + alpha * d\n                f_try, x_try = callf(x_try)\n\n                if f_try < pop_f[parent_i]:\n                    # success: accept and slightly increase sigma\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                    continue\n\n                # local backtracking / small-step refinement along direction (few tries)\n                improved = False\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    x_try = x_parent + alpha * frac * d\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        improved = True\n                        break\n                if improved:\n                    continue\n\n                # try an orthogonal perturbation for local diversification\n                r = np.random.randn(self.dim)\n                # make r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    x_try = x_parent + 0.6 * sigma * r\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n                # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n                if np.random.rand() < 0.08:\n                    step = np.random.standard_cauchy(self.dim)\n                    denom = np.percentile(np.abs(step), 90) + 1e-12\n                    step = step / denom  # normalize heavy tail\n                    scale_vec = 0.2 * (ub - lb)\n                    x_try = x_parent + step * scale_vec\n                    f_try, x_try = callf(x_try)\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    # continue to next iteration (do not leave parent unchanged yet)\n                    continue\n\n                # recombination exploitation: mix two best and small noise\n                if len(pop) >= 2 and np.random.rand() < 0.05:\n                    best2 = np.argsort(pop_f)[:2]\n                    a, b = int(best2[0]), int(best2[1])\n                    beta = np.random.rand()\n                    mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                    noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                    x_try = mix + noise\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    else:\n                        worst_i = int(np.argmax(pop_f))\n                        if f_try < pop_f[worst_i]:\n                            pop[worst_i] = x_try\n                            pop_f[worst_i] = f_try\n                            pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    continue\n\n                # failed directional attempts: adapt parent sigma on failure\n                pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n                # occasional population rejuvenation by replacing the worst with a random sample\n                if np.random.rand() < 0.02 and remaining > 0:\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    worst_i = int(np.argmax(pop_f))\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = max(1e-12, base_sigma * (0.5 + np.random.rand()))\n\n                # occasionally add a new exploratory individual if budget allows and population is not full\n                if len(pop) < self.pop_size and np.random.rand() < 0.03 and remaining > 0:\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop.append(x_new)\n                    pop_f.append(f_new)\n                    pop_sigma.append(max(1e-12, base_sigma * (0.4 + np.random.rand())))\n\n        except StopIteration:\n            # budget exhausted, break out\n            pass\n\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.524 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15001986445097992, 0.1632321779366479, 0.7266641638612219, 0.921304409248889, 0.7394673942454207, 0.8413486357497196, 0.31216492088681935, 0.45886979403472705, 0.7645715136876359, 0.16193040410959048]}, "task_prompt": ""}
{"id": "acac0441-94dd-4aed-83c4-167bf85ea372", "fitness": 0.5708165496982056, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based heuristic combining adaptive directional local searches, orthogonal refinements, Lévy-like heavy-tailed jumps and light recombination to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by dim if None)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # best-so-far\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # get bounds from func if provided, else default to [-5,5]\n        if hasattr(func, \"bounds\"):\n            try:\n                lb = np.array(func.bounds.lb, dtype=float)\n                ub = np.array(func.bounds.ub, dtype=float)\n            except Exception:\n                # fallback\n                lb = np.full(self.dim, -5.0)\n                ub = np.full(self.dim, 5.0)\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # clamp dims if mismatch\n        if lb.size != self.dim or ub.size != self.dim:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # remaining evaluations\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.array(x, dtype=float)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            nonlocal_f = f  # no-op just clarity\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x.copy()\n\n        # Quick random-search fallback if budget is tiny\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n        if remaining < max(5, self.pop_size // 2):\n            # do pure random sampling for tiny budgets\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # initialize population\n        pop_n = min(self.pop_size, max(2, remaining // 4))  # ensure some evaluations left for search\n        pop = np.zeros((pop_n, self.dim), dtype=float)\n        pop_f = np.full(pop_n, np.inf, dtype=float)\n        pop_sigma = np.zeros(pop_n, dtype=float)\n\n        base_sigma = max(1e-12, 0.25 * np.mean(ub - lb))  # initial scale\n        # attempt to create initial population\n        for i in range(pop_n):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop[i, :] = x0\n            pop_f[i] = f0\n            # initialize individual sigma with some spread\n            pop_sigma[i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n        # If we couldn't fill any population (extremely small budget), return best\n        if pop_f.size == 0 or np.isinf(pop_f).all():\n            return self.f_opt, self.x_opt\n\n        # main loop\n        # algorithm hyper-parameters\n        p_jump = 0.12\n        p_recomb = 0.18\n        recomb_beta = 0.6\n        frac_backtrack = [0.5, 0.25, 0.125]\n        rejuvenation_freq = 30  # every N iterations try rejuvenation\n        iter_count = 0\n\n        # helper to get indices of best and worst\n        def best_indices():\n            idx = np.argsort(pop_f)\n            return idx\n\n        # optimization iterations until budget exhausted\n        while remaining > 0:\n            iter_count += 1\n\n            # pick a parent via small tournament selection (choose k random and take best)\n            k = min(3, pop.shape[0])\n            cand_idx = np.random.choice(pop.shape[0], size=k, replace=False)\n            parent_i = cand_idx[np.argmin(pop_f[cand_idx])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, np.mean(ub - lb))\n                continue  # move to next iteration\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in frac_backtrack:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.08, np.mean(ub - lb))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if remaining <= 0:\n                break\n            r = np.random.randn(self.dim)\n            # make r orthogonal to d\n            r = r - np.dot(r, d) * d\n            rn = np.linalg.norm(r) + 1e-12\n            r = r / rn\n            x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, np.mean(ub - lb))\n                continue\n\n            # occasional Lévy-like (Cauchy) jump to escape local basins\n            if np.random.rand() < p_jump and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                # robust normalization: scale by median absolute value, fallback to 1.0\n                med = np.median(np.abs(step)) + 1e-12\n                step = step / med\n                scale_vec = 0.2 * (ub - lb)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    worst_i = int(np.argmax(pop_f))\n                    # if it's good replace the worst in population\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best with small noise\n            if pop.shape[0] >= 2 and np.random.rand() < p_recomb and remaining > 0:\n                best2 = best_indices()[:2]\n                a, b = best2[0], best2[1]\n                beta = recomb_beta * (0.8 + 0.4 * np.random.rand())  # small variability\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # replace worst if better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n                continue\n\n            # try injecting primary directional trial into population by replacing the worst if it's better\n            worst_i = int(np.argmax(pop_f))\n            if f_try < pop_f[worst_i]:\n                pop[worst_i] = x_try\n                pop_f[worst_i] = f_try\n                pop_sigma[worst_i] = base_sigma * (0.6 + 0.6 * np.random.rand())\n                continue\n\n            # adapt parent sigma on failure (shrink)\n            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if iter_count % rejuvenation_freq == 0 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget\n        # ensure x_opt and f_opt are set (if not, pick best from population)\n        if self.x_opt is None and pop.shape[0] > 0:\n            best_i = int(np.argmin(pop_f))\n            self.x_opt = pop[best_i].copy()\n            self.f_opt = float(pop_f[best_i])\n\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()\n\n\n# Example usage:\n# adls = ADLS(budget=1000, dim=5, seed=42)\n# fbest, xbest = adls(some_blackbox_function)", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.571 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17595243212460843, 0.15712699292383026, 0.8522666608426801, 0.9526342840761143, 0.8772478307208313, 0.9097754451270708, 0.26308771585676616, 0.4767622063739543, 0.8767459969673076, 0.1665659319688927]}, "task_prompt": ""}
{"id": "ad164193-852a-4798-bb00-ee00f6e2369a", "fitness": 0.5097460186436574, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based search combining normalized directional local searches with small-step backtracking, orthogonal refinements, occasional Cauchy/Lévy-like escapes and population rejuvenation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_size: population size (defaults to moderate, scales with dim)\n    - seed: RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimension but stays manageable\n            self.pop_size = int(min(30, max(4, 2 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.RandomState(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds (support both scalars and arrays)\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            # default bounds (problem statement: [-5, 5])\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # Ensure lb/ub are full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # safety cropping\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # remaining budget\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            # do not call if no budget\n            if remaining <= 0:\n                raise RuntimeError(\"Budget exhausted\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n\n        # base scale for sigma (fraction of domain)\n        base_sigma = 0.2 * np.mean(ub - lb)\n        min_sigma = 1e-12\n\n        # try to initialize as many individuals as budget allows or until pop_size\n        while len(pop) < self.pop_size and remaining > 0:\n            x0 = self.rng.uniform(lb, ub)\n            try:\n                f0, x0 = callf(x0)\n            except RuntimeError:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # initialize sigma with some diversity (scalar)\n            pop_sigma.append(base_sigma * (0.5 + self.rng.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                try:\n                    callf(x)\n                except RuntimeError:\n                    break\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # main loop\n        while remaining > 0:\n            # update best/worst indices\n            best_idx = int(np.argmin(pop_f))\n            worst_idx = int(np.argmax(pop_f))\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = self.rng.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            scale_factors = [0.5, 0.25, 0.125]\n            for sf in scale_factors:\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + (alpha * sf) * d, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * (1.0 + 0.1 * sf), min_sigma)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            # build an orthogonal vector to d by subtracting projection from random vector\n            r = self.rng.randn(self.dim)\n            r = r - np.dot(r, d) * d\n            rr = np.linalg.norm(r) + 1e-12\n            r = r / rr\n            # try few orthogonal magnitudes\n            for factor in (0.6, 0.3):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + factor * sigma * r, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.rand() < 0.08 and remaining > 0:\n                # sample a Cauchy-like heavy-tailed vector\n                u = self.rng.rand(self.dim)\n                cauchy = np.tan(np.pi * (u - 0.5))  # standard Cauchy entries\n                # normalize deltas to avoid extreme scale while keeping heavy-tail properties\n                denom = np.median(np.abs(cauchy)) + 1e-12\n                step = cauchy / denom\n                scale_vec = 0.2 * (ub - lb)  # per-dim scale\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_try, x_try = callf(x_try)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                if f_try < pop_f[worst_idx]:\n                    pop[worst_idx] = x_try\n                    pop_f[worst_idx] = f_try\n                    pop_sigma[worst_idx] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n                    continue\n                # else maybe keep as a temporary outsider with small chance to replace parent\n                if f_try < pop_f[parent_i] and self.rng.rand() < 0.3:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = base_sigma * (0.5 + self.rng.rand())\n                    continue\n                # otherwise no replacement\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                # find two best\n                sorted_idx = np.argsort(pop_f)\n                a, b = int(sorted_idx[0]), int(sorted_idx[1])\n                beta = self.rng.rand()\n                x_recomb = np.clip(beta * pop[a] + (1.0 - beta) * pop[b] + 0.02 * base_sigma * self.rng.randn(self.dim), lb, ub)\n                try:\n                    f_recomb, x_recomb = callf(x_recomb)\n                except RuntimeError:\n                    break\n                # if improved over parent, replace parent; else maybe replace worst\n                if f_recomb < pop_f[parent_i]:\n                    pop[parent_i] = x_recomb\n                    pop_f[parent_i] = f_recomb\n                    pop_sigma[parent_i] = max(sigma * 0.9, min_sigma)\n                    continue\n                else:\n                    if f_recomb < pop_f[worst_idx]:\n                        pop[worst_idx] = x_recomb\n                        pop_f[worst_idx] = f_recomb\n                        pop_sigma[worst_idx] = base_sigma * (0.6 + 0.6 * self.rng.rand())\n\n            # adapt parent sigma on failure (reduce)\n            pop_sigma[parent_i] = max(sigma * 0.85, min_sigma)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < 0.06 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * self.rng.rand())\n\n        # finished budget or exhausted\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.510 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1372463718386766, 0.16192547084919873, 0.8220787290404825, 0.96045484509805, 0.28395006689258373, 0.9181581691710132, 0.310337698431418, 0.46490614191649193, 0.8830912046400491, 0.15531148855861]}, "task_prompt": ""}
{"id": "6445323d-c265-435d-b7fc-e526084266d7", "fitness": 0.2579567230413826, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based optimizer combining directional local searches with orthogonal refinements, recombination and occasional heavy-tailed Lévy jumps; individual step-sizes adapt based on success.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest size based on dim)\n    - seed: optional random seed for reproducibility\n\n    The search space is assumed to be [-5, 5]^dim as per the Many BBOB tasks.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(int(seed))\n        # default pop size scales slowly with dim but stays modest\n        if pop_size is None:\n            self.pop_size = max(4, min(40, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(max(2, pop_size))\n\n        # bounds (given by problem statement)\n        self.lb = -5.0 * np.ones(self.dim)\n        self.ub =  5.0 * np.ones(self.dim)\n\n    def __call__(self, func):\n        # Remaining budget\n        remaining = int(self.budget)\n\n        # helpers to keep track of best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def eval_point(x):\n            nonlocal remaining\n            # clip to bounds\n            x = np.clip(np.asarray(x, dtype=float), self.lb, self.ub)\n            if remaining <= 0:\n                # Out of budget — return current best without calling func\n                return self.f_opt, (self.x_opt.copy() if self.x_opt is not None else x)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If extremely small budget, do pure random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n        if remaining < 10:\n            # quick random sampling\n            for _ in range(remaining):\n                x = np.random.uniform(self.lb, self.ub)\n                f, x = eval_point(x)\n            return self.f_opt, self.x_opt\n\n        # initialize population: ensure we don't evaluate more than budget\n        init_pop_size = min(self.pop_size, max(2, remaining // 2))  # leave room for search\n        pop = []\n        pop_f = []\n        for _ in range(init_pop_size):\n            x0 = np.random.uniform(self.lb, self.ub)\n            f0, x0 = eval_point(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n\n        # If we couldn't create enough population because budget low, fallback to random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(self.lb, self.ub)\n                eval_point(x)\n            return self.f_opt, self.x_opt\n\n        pop = np.array(pop)\n        pop_f = np.array(pop_f)\n\n        # per-individual adaptive sigma (step sizes), start proportional to domain\n        domain_scale = np.mean(self.ub - self.lb)\n        base_sigma = max(1e-8, 0.25 * domain_scale)\n        pop_sigma = np.full(len(pop), base_sigma)\n\n        # counters for stagnation detection\n        fail_count = np.zeros(len(pop), dtype=int)\n\n        # main loop\n        while remaining > 0:\n            # pick a parent by small tournament selection\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random normalized direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.ones(self.dim)\n                nd = np.linalg.norm(d)\n            d = d / nd\n\n            # primary directional trial: stochastic step length (log-normal scaling around sigma)\n            step_len = sigma * np.exp(0.5 * np.random.randn())  # log-normal multiplicative jitter\n            x_try = np.clip(x_parent + d * step_len, self.lb, self.ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = eval_point(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(domain_scale, sigma * (1.15 + 0.05 * np.random.randn()))\n                fail_count[parent_i] = 0\n                continue\n            else:\n                # failure: attempt local backtracking (reduce step) few times\n                accepted = False\n                for back in range(3):\n                    if remaining <= 0:\n                        break\n                    step_len_b = step_len * (0.5 ** (back + 1))\n                    x_b = np.clip(x_parent + d * step_len_b, self.lb, self.ub)\n                    f_b, x_b = eval_point(x_b)\n                    if f_b < pop_f[parent_i]:\n                        pop[parent_i] = x_b\n                        pop_f[parent_i] = f_b\n                        pop_sigma[parent_i] = max(1e-12, sigma * 0.9)\n                        fail_count[parent_i] = 0\n                        accepted = True\n                        break\n                if accepted:\n                    continue\n\n            # orthogonal refinement: small step in direction orthogonal to d\n            r = np.random.randn(self.dim)\n            # remove directional component\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                ortho_scale = sigma * 0.5\n                x_o = np.clip(x_parent + r * ortho_scale * np.random.rand(), self.lb, self.ub)\n                if remaining <= 0:\n                    break\n                f_o, x_o = eval_point(x_o)\n                if f_o < pop_f[parent_i]:\n                    pop[parent_i] = x_o\n                    pop_f[parent_i] = f_o\n                    pop_sigma[parent_i] = max(1e-12, sigma * 0.95)\n                    fail_count[parent_i] = 0\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed Cauchy)\n            if np.random.rand() < 0.06:\n                # generate heavy-tailed vector (Cauchy-like) but normalize by robust scale\n                step = np.random.standard_cauchy(size=self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                # scale vector by a larger scale to attempt basin escape\n                scale_vec = sigma * (5.0 + 5.0 * np.random.rand())\n                x_jump = np.clip(x_parent + step * scale_vec, self.lb, self.ub)\n                if remaining <= 0:\n                    break\n                f_jump, x_jump = eval_point(x_jump)\n                # if jump improved, replace the worst; otherwise keep as candidate with small chance\n                worst_i = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_i]:\n                    pop[worst_i] = x_jump\n                    pop_f[worst_i] = f_jump\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                else:\n                    # sometimes insert as exploration node replacing a random poor individual\n                    if np.random.rand() < 0.2:\n                        rpl = worst_i\n                        pop[rpl] = x_jump\n                        pop_f[rpl] = f_jump\n                        pop_sigma[rpl] = max(1e-12, sigma * 0.5)\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and np.random.rand() < 0.3:\n                # pick two best distinct indices\n                sorted_idx = np.argsort(pop_f)\n                a, b = sorted_idx[0], sorted_idx[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # inject small gaussian noise scaled by average sigma\n                avg_sigma = np.mean(pop_sigma)\n                mix_try = np.clip(mix + np.random.randn(self.dim) * (0.3 * avg_sigma), self.lb, self.ub)\n                if remaining <= 0:\n                    break\n                f_mix, mix_try = eval_point(mix_try)\n                worst_i = int(np.argmax(pop_f))\n                if f_mix < pop_f[worst_i]:\n                    pop[worst_i] = mix_try\n                    pop_f[worst_i] = f_mix\n                    pop_sigma[worst_i] = max(1e-12, avg_sigma * 0.7)\n                continue\n\n            # if reached here, the directional attempts all failed; adapt sigma downwards for parent\n            fail_count[parent_i] += 1\n            pop_sigma[parent_i] = max(1e-12, sigma * (0.85 if fail_count[parent_i] < 5 else 0.6))\n\n            # occasional replacement (rejuvenation) of the worst with a random sample\n            if np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(self.lb, self.ub)\n                if remaining <= 0:\n                    break\n                f_new, x_new = eval_point(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma\n\n            # small population maintenance: kill duplicates / collapse and re-seed\n            if np.random.rand() < 0.01:\n                # find pairwise distances and replace any too-close points\n                if len(pop) > 1:\n                    dists = np.linalg.norm(pop[:, None, :] - pop[None, :, :], axis=2)\n                    np.fill_diagonal(dists, np.inf)\n                    i_min, j_min = np.unravel_index(np.argmin(dists), dists.shape)\n                    if dists[i_min, j_min] < 1e-6:\n                        # reseed j_min\n                        x_new = np.random.uniform(self.lb, self.ub)\n                        if remaining <= 0:\n                            break\n                        f_new, x_new = eval_point(x_new)\n                        pop[j_min] = x_new\n                        pop_f[j_min] = f_new\n                        pop_sigma[j_min] = base_sigma\n\n        # finished budget or loop exit\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.258 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12981480644490917, 0.17187478009685286, 0.45406289255413257, 0.4194610587798637, 0.21527079495408064, 0.33063903071094736, 0.2345988224994643, 0.2476797004581709, 0.22590250450487936, 0.1502628394105251]}, "task_prompt": ""}
{"id": "ec383d7b-ff83-47e7-a733-52c86cdc2772", "fitness": 0.40201331055083783, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — a population-based continuous optimizer combining directional local searches, orthogonal refinements, adaptive step-sizes, occasional heavy-tailed (Lévy/Cauchy-like) jumps and light recombination to robustly explore/exploit bounded search spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    Main idea: maintain a modest population of candidates with per-individual adaptive step-sizes.\n    For each iteration pick a parent by a small tournament, do a directional local search along a\n    randomized normalized direction, attempt small backtracks and orthogonal refinements, occasionally\n    perform heavy-tailed Lévy-like jumps to escape basins, and perform light recombination/rejuvenation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n    def __call__(self, func):\n        # Determine bounds (many BBOB wrappers provide func.bounds.lb / ub as arrays)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            # Default to [-5,5] per problem statement\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Ensure lb/ub are full-dim\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # State\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Helper function to call objective while tracking budget and best\n        def callf(x):\n            nonlocal remaining, lb, ub\n            if remaining <= 0:\n                raise StopIteration(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget too small, fallback to pure random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n        if remaining < 10:\n            # random sampling until budget exhausted\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population size modestly with relation to dim and budget\n        pop_size = max(4, min(40, int(4 * self.dim)))  # modest population\n        pop_size = min(pop_size, max(2, self.budget // 8))  # ensure affordable initial sampling\n        pop_size = int(pop_size)\n\n        # Initialize population arrays\n        pop_x = np.zeros((pop_size, self.dim), dtype=float)\n        pop_f = np.full(pop_size, np.inf, dtype=float)\n        pop_sigma = np.zeros(pop_size, dtype=float)  # per-individual scale (absolute)\n        pop_age = np.zeros(pop_size, dtype=int)\n\n        # Robust initial sigma scale based on bound size\n        bound_scale = np.maximum(ub - lb, 1e-12)\n        sigma0 = 0.1 * np.linalg.norm(bound_scale) / np.sqrt(self.dim)  # global initial scale\n\n        # Fill initial population (random points)\n        try:\n            for i in range(pop_size):\n                x0 = np.random.uniform(lb, ub)\n                f0, x0 = callf(x0)\n                pop_x[i] = x0\n                pop_f[i] = f0\n                # init sigma with some diversity\n                pop_sigma[i] = sigma0 * (0.5 + np.random.rand())\n                pop_age[i] = 0\n        except StopIteration:\n            # budget exhausted during init\n            return self.f_opt, self.x_opt\n\n        # Ensure we have at least one best candidate recorded\n        best_i = int(np.argmin(pop_f))\n        if self.x_opt is None:\n            self.f_opt = float(pop_f[best_i])\n            self.x_opt = pop_x[best_i].copy()\n\n        # Main optimization loop\n        # Heuristic parameters\n        tournament_k = 3\n        backtrack_fracs = (0.5, 0.25, -0.5, -0.25)\n        levy_prob = 0.08\n        rejuvenation_prob = 0.02\n        recomb_prob = 0.06\n        max_iter_without_improve = 50\n\n        iter_since_improve = 0\n\n        try:\n            while remaining > 0:\n                # pick a parent via small tournament (lower f wins)\n                inds = np.random.choice(pop_size, size=min(tournament_k, pop_size), replace=False)\n                values = pop_f[inds]\n                parent_idx = int(inds[int(np.argmin(values))])\n                parent_x = pop_x[parent_idx].copy()\n                parent_f = pop_f[parent_idx]\n                sigma = float(pop_sigma[parent_idx])\n\n                # sample a normalized random search direction\n                d = np.random.randn(self.dim)\n                dn = np.linalg.norm(d)\n                if dn < 1e-12:\n                    d = np.ones(self.dim) / np.sqrt(self.dim)\n                    dn = 1.0\n                d /= dn\n\n                # primary directional trial with stochasticized step-length\n                # step drawn around sigma, positive, using lognormal-ish variation for adaptivity\n                step = sigma * max(1e-16, (1.0 + 0.5 * np.random.randn()))\n                x_try = np.clip(parent_x + step * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < parent_f:\n                    # accept and slightly increase sigma (successful exploitation)\n                    pop_x[parent_idx] = x_try\n                    pop_f[parent_idx] = f_try\n                    pop_sigma[parent_idx] = sigma * (1.15 + 0.02 * np.random.randn())\n                    pop_age[parent_idx] = 0\n                    iter_since_improve = 0\n                    continue\n                else:\n                    # failed: try local backtracking / small-step refinement along direction (few tries)\n                    improved = False\n                    for frac in backtrack_fracs:\n                        if remaining <= 0:\n                            break\n                        x_bt = np.clip(parent_x + frac * step * d, lb, ub)\n                        f_bt, x_bt = callf(x_bt)\n                        if f_bt < pop_f[parent_idx]:\n                            pop_x[parent_idx] = x_bt\n                            pop_f[parent_idx] = f_bt\n                            pop_sigma[parent_idx] = max(sigma * 0.95, 1e-12)\n                            pop_age[parent_idx] = 0\n                            improved = True\n                            iter_since_improve = 0\n                            break\n                    if improved:\n                        continue\n\n                # try an orthogonal perturbation for local diversification\n                r = np.random.randn(self.dim)\n                # remove projection on d to make r orthogonal\n                r = r - np.dot(r, d) * d\n                rn = np.linalg.norm(r)\n                if rn > 1e-12:\n                    r /= rn\n                    ortho_step = sigma * 0.6 * (0.5 + 0.5 * np.random.rand())\n                    x_try = np.clip(parent_x + ortho_step * r, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_idx]:\n                        pop_x[parent_idx] = x_try\n                        pop_f[parent_idx] = f_try\n                        pop_sigma[parent_idx] = max(sigma * 0.95, 1e-12)\n                        pop_age[parent_idx] = 0\n                        iter_since_improve = 0\n                        continue\n\n                # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n                if np.random.rand() < levy_prob and remaining > 0:\n                    # Cauchy-like heavy-tailed scalar\n                    c = np.tan(np.pi * (np.random.rand() - 0.5))  # Cauchy(0,1)\n                    # scale vector by per-dimension robust scale\n                    scale_vec = (0.2 * bound_scale) * (1.0 + 0.5 * np.random.randn(self.dim))\n                    # normalize scale_vec and apply cauchy multiplier with moderate clipping\n                    scale_norm = np.linalg.norm(scale_vec)\n                    if scale_norm < 1e-12:\n                        scale_vec = np.ones(self.dim) * 1e-3\n                        scale_norm = np.linalg.norm(scale_vec)\n                    scale_vec = scale_vec / scale_norm\n                    step_vec = (sigma * max(-100.0, min(100.0, c))) * scale_vec\n                    x_jump = np.clip(parent_x + step_vec, lb, ub)\n                    f_jump, x_jump = callf(x_jump)\n                    # If good, replace the worst individual; if best, also keep\n                    worst_idx = int(np.argmax(pop_f))\n                    if f_jump < pop_f[worst_idx]:\n                        pop_x[worst_idx] = x_jump\n                        pop_f[worst_idx] = f_jump\n                        pop_sigma[worst_idx] = max(sigma * (0.8 + 0.4 * np.random.rand()), 1e-12)\n                        pop_age[worst_idx] = 0\n                        iter_since_improve = 0\n                        # possibly also replace parent if it was worse\n                        if f_jump < pop_f[parent_idx]:\n                            parent_idx = worst_idx\n                    else:\n                        # maybe keep as candidate: replace parent if slightly better\n                        if f_jump < pop_f[parent_idx]:\n                            pop_x[parent_idx] = x_jump\n                            pop_f[parent_idx] = f_jump\n                            pop_sigma[parent_idx] = max(sigma * 0.9, 1e-12)\n                            pop_age[parent_idx] = 0\n                    # continue main loop\n                    continue\n\n                # recombination exploitation: mix two best and small noise\n                if np.random.rand() < recomb_prob and remaining > 0:\n                    best_two_idx = np.argsort(pop_f)[:2]\n                    a, b = best_two_idx[0], best_two_idx[1] if best_two_idx.size > 1 else best_two_idx[0]\n                    # biased blend toward the best\n                    alpha = 0.6 + 0.2 * np.random.rand()\n                    child = np.clip(alpha * pop_x[a] + (1.0 - alpha) * pop_x[b] +\n                                    0.01 * (ub - lb) * np.random.randn(self.dim), lb, ub)\n                    f_child, child = callf(child)\n                    worst_idx = int(np.argmax(pop_f))\n                    if f_child < pop_f[worst_idx]:\n                        pop_x[worst_idx] = child\n                        pop_f[worst_idx] = f_child\n                        pop_sigma[worst_idx] = max(0.8 * np.mean(pop_sigma), 1e-12)\n                        pop_age[worst_idx] = 0\n                        iter_since_improve = 0\n                    continue\n\n                # adapt parent sigma on failure (shrink) and age increment\n                pop_sigma[parent_idx] = max(sigma * 0.92, 1e-12)\n                pop_age[parent_idx] += 1\n                iter_since_improve += 1\n\n                # occasional population rejuvenation by replacing the worst with a random sample\n                if remaining > 0 and np.random.rand() < rejuvenation_prob:\n                    worst_idx = int(np.argmax(pop_f))\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop_x[worst_idx] = x_new\n                    pop_f[worst_idx] = f_new\n                    pop_sigma[worst_idx] = sigma0 * (0.5 + np.random.rand())\n                    pop_age[worst_idx] = 0\n                    iter_since_improve = 0\n\n                # If stagnant for a while, intensify exploration: replace oldest or worst\n                if iter_since_improve > max_iter_without_improve and remaining > 0:\n                    # replace the oldest or the worst with a random sample and a large sigma\n                    oldest_idx = int(np.argmax(pop_age))\n                    worst_idx = int(np.argmax(pop_f))\n                    replace_idx = oldest_idx if pop_age[oldest_idx] > pop_age[worst_idx] else worst_idx\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop_x[replace_idx] = x_new\n                    pop_f[replace_idx] = f_new\n                    pop_sigma[replace_idx] = sigma0 * (1.0 + np.random.rand())\n                    pop_age[replace_idx] = 0\n                    iter_since_improve = 0\n\n                # small housekeeping: keep sigma bounded relative to problem scale\n                pop_sigma = np.maximum(pop_sigma, 1e-12)\n                max_sigma = 10.0 * np.linalg.norm(bound_scale)\n                pop_sigma = np.minimum(pop_sigma, max_sigma)\n\n        except StopIteration:\n            # budget exhausted, return current best\n            pass\n\n        return float(self.f_opt), (self.x_opt.copy() if self.x_opt is not None else None)", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.402 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10810118886597975, 0.15934240420852164, 0.6992949842831666, 0.8612154013809572, 0.2622526276125111, 0.8604147563301465, 0.24094288737020386, 0.46283111389719545, 0.2248263490230361, 0.14091139253666018]}, "task_prompt": ""}
{"id": "6b710b78-6be3-449b-ad69-5616efdfd3bf", "fitness": "-inf", "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size points performing randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy jumps to balance robust exploration and focused exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget : total number of function evaluations allowed\n    - dim    : problem dimensionality\n    - pop_size (optional): override default population scaling\n    - seed (optional): RNG seed\n    Notes:\n      - Assumes func(x) accepts a 1D numpy array of length dim and returns a scalar.\n      - Tries to use func.bounds.lb / func.bounds.ub if available; otherwise uses [-5,5]^dim.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # sensible default population sizing (keeps population modest for high dims)\n        if pop_size is None:\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds if available\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n\n        # clamp bounds\n        lb = lb.ravel()[:self.dim]\n        ub = ub.ravel()[:self.dim]\n        span = ub - lb\n        span[span == 0] = 1.0  # avoid zero\n\n        # evaluation counter\n        evals = 0\n\n        def callf(x):\n            nonlocal evals\n            if evals >= self.budget:\n                # budget exhausted; should not happen if loop respects budget\n                return np.inf\n            x = np.asarray(x, dtype=float).ravel()[:self.dim]\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            evals += 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # trivial cases\n        if self.budget <= 0:\n            return self.f_opt, self.x_opt\n\n        # initialize population (may be reduced if budget is small)\n        pop_n = min(self.pop_size, max(1, self.budget // 3))  # ensure some evals for initialization\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # initial sigma proportional to span\n        init_sigma = 0.2 * np.mean(span)\n\n        for i in range(pop_n):\n            if evals >= self.budget:\n                break\n            x = lb + self.rng.rand(self.dim) * span\n            f = callf(x)\n            pop.append(x)\n            pop_f.append(f)\n            pop_sigma.append(init_sigma * (1.0 + 0.5 * self.rng.rand()))\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # if no population (extremely small budget), fallback to random search with remaining budget\n        if len(pop) == 0:\n            while evals < self.budget:\n                x = lb + self.rng.rand(self.dim) * span\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # helper to get worst/best indices\n        def best_indices():\n            return np.argsort(pop_f)\n\n        # main optimization loop\n        # parameters\n        p_jump = 0.06  # base probability of a Lévy-like jump\n        rejuvenation_interval = max(20, 5 * self.dim)\n        iter_since_rejuv = 0\n\n        # safety: cap number of iterations (each iteration usually uses 1-3 evals)\n        while evals < self.budget:\n            iter_since_rejuv += 1\n\n            # small tournament selection for parent (encourages moderate selection pressure)\n            k = min(3, len(pop))\n            cand_idx = self.rng.choice(len(pop), k, replace=False)\n            parent_i = int(cand_idx[np.argmin([pop_f[i] for i in cand_idx])])\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random normalized direction\n            r = self.rng.randn(self.dim)\n            rn = np.linalg.norm(r)\n            if rn == 0:\n                r = self.rng.randn(self.dim)\n                rn = np.linalg.norm(r) + 1e-12\n            r = r / rn\n\n            # stochasticize step length (log-normal factor) to vary step sizes\n            step_len = sigma * math_safe_randn(self.rng) * 0.5 + sigma\n            # ensure positive\n            step_len = max(1e-12, abs(step_len))\n\n            # Primary directional trial\n            x_try = x_parent + step_len * r\n            x_try = np.minimum(np.maximum(x_try, lb), ub)\n            f_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(np.max(span), sigma * 1.12)\n                # small local refinement along same direction (backtracking)\n                back_factor = 0.6\n                for bt in range(3):\n                    if evals >= self.budget:\n                        break\n                    step_len *= back_factor\n                    x_bt = pop[parent_i] + step_len * r\n                    x_bt = np.minimum(np.maximum(x_bt, lb), ub)\n                    f_bt = callf(x_bt)\n                    if f_bt < pop_f[parent_i]:\n                        pop[parent_i] = x_bt\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = min(np.max(span), pop_sigma[parent_i] * 1.05)\n                    else:\n                        pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.98)\n                continue  # continue main loop after successful directional improvement\n\n            else:\n                # failure on primary direction: slightly reduce sigma\n                pop_sigma[parent_i] = max(1e-12, sigma * 0.92)\n\n            # Orthogonal refinement: try a perturbation orthogonal to r for local diversification\n            ort = self.rng.randn(self.dim)\n            # make orthogonal to r\n            ort -= r * np.dot(ort, r)\n            norm_ort = np.linalg.norm(ort)\n            if norm_ort < 1e-12:\n                ort = self.rng.randn(self.dim)\n                norm_ort = np.linalg.norm(ort) + 1e-12\n            ort = ort / norm_ort\n            ort_step = 0.6 * pop_sigma[parent_i]\n            x_ort = x_parent + ort_step * ort\n            x_ort = np.minimum(np.maximum(x_ort, lb), ub)\n            f_ort = callf(x_ort)\n            if f_ort < pop_f[parent_i]:\n                pop[parent_i] = x_ort\n                pop_f[parent_i] = f_ort\n                pop_sigma[parent_i] = min(np.max(span), pop_sigma[parent_i] * 1.08)\n                continue\n            else:\n                pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 0.96)\n\n            # Occasional Lévy-like heavy-tailed jump to escape basins\n            # Probability grows slightly with stagnation / age (we use a simple counter based signal)\n            if self.rng.rand() < p_jump:\n                # sample Cauchy-like vector (standard Cauchy per component) for heavy tails\n                step = self.rng.standard_cauchy(self.dim)\n                # robust normalization: scale by median absolute deviation like factor to avoid extremes\n                denom = np.median(np.abs(step)) + 1e-12\n                step = step / denom\n                # scale vector relative to bounds to make jump meaningful\n                scale_vec = 0.2 * span * (0.5 + self.rng.rand(self.dim) * 1.0)\n                jump = step * scale_vec\n                # limit extreme values by overall norm\n                max_norm = 5.0 * np.mean(span)\n                jnorm = np.linalg.norm(jump)\n                if jnorm > max_norm:\n                    jump = jump * (max_norm / (jnorm + 1e-12))\n                x_jump = x_parent + jump\n                x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                f_jump = callf(x_jump)\n                # if jump is good, replace worst; else maybe keep as candidate (rare)\n                worst_idx = int(np.argmax(pop_f))\n                if f_jump < pop_f[worst_idx]:\n                    pop[worst_idx] = x_jump\n                    pop_f[worst_idx] = f_jump\n                    pop_sigma[worst_idx] = init_sigma\n                # continue after jump attempt\n                continue\n\n            # Recombination exploitation: mix two best with mild noise\n            if len(pop) >= 2:\n                idxs = best_indices()\n                a, b = int(idxs[0]), int(idxs[1])\n                beta = self.rng.beta(1.5, 1.5)\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # small additive noise scaled to bounds and local sigma\n                noise = 0.08 * span * self.rng.randn(self.dim) * (0.5 + 0.5 * self.rng.rand())\n                x_mix = mix + noise\n                x_mix = np.minimum(np.maximum(x_mix, lb), ub)\n                f_mix = callf(x_mix)\n                worst_idx = int(np.argmax(pop_f))\n                if f_mix < pop_f[worst_idx]:\n                    pop[worst_idx] = x_mix\n                    pop_f[worst_idx] = f_mix\n                    pop_sigma[worst_idx] = init_sigma * 0.8\n                # also try replacing parent if better\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 1.05)\n\n            # adapt parent sigma on failure a bit more (encourage exploration)\n            pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * (0.98 + 0.04 * self.rng.rand()))\n\n            # occasional rejuvenation: replace worst with random sample if stagnating\n            if iter_since_rejuv >= rejuvenation_interval:\n                iter_since_rejuv = 0\n                worst_idx = int(np.argmax(pop_f))\n                if evals < self.budget:\n                    x_rand = lb + self.rng.rand(self.dim) * span\n                    f_rand = callf(x_rand)\n                    if f_rand < pop_f[worst_idx]:\n                        pop[worst_idx] = x_rand\n                        pop_f[worst_idx] = f_rand\n                        pop_sigma[worst_idx] = init_sigma * (0.5 + self.rng.rand())\n                    else:\n                        # sometimes accept even if worse to maintain diversity\n                        if self.rng.rand() < 0.05:\n                            pop[worst_idx] = x_rand\n                            pop_f[worst_idx] = f_rand\n                            pop_sigma[worst_idx] = init_sigma * (0.5 + self.rng.rand())\n\n        return float(self.f_opt), None if self.x_opt is None else self.x_opt.copy()\n\n\n# small helper: safe gaussian perturbation (returns positive-ish number)\ndef math_safe_randn(rng):\n    # a symmetric small random factor centered ~1.0 with limited tails\n    g = rng.randn()\n    # compress extremes\n    g = np.tanh(g)  # in (-1,1)\n    return 1.0 + 0.3 * g", "configspace": "", "generation": 0, "feedback": "In the code, line 128, in __call__, the following error occurred:\nNameError: name 'math_safe_randn' is not defined\nOn line: step_len = sigma * math_safe_randn(self.rng) * 0.5 + sigma", "error": "In the code, line 128, in __call__, the following error occurred:\nNameError: name 'math_safe_randn' is not defined\nOn line: step_len = sigma * math_safe_randn(self.rng) * 0.5 + sigma", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "fc49fd48-598a-4e7b-8fba-2849e9b5cea8", "fitness": 0.5899699493646389, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements and occasional heavy-tailed Lévy jumps to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: population size (optional; default scales with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population size modestly scales with dimension\n        if pop_size is None:\n            self.pop_size = min(24, max(6, 2 * self.dim))\n        else:\n            self.pop_size = int(max(2, pop_size))\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # results placeholders\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # ensure bounds are full-dim numpy arrays\n        lb = np.asarray(func.bounds.lb)\n        ub = np.asarray(func.bounds.ub)\n        # allow scalar bounds\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n\n        # helper to clip and call while tracking budget and best\n        evals = 0\n        def callf(x):\n            nonlocal evals\n            x = np.asarray(x, dtype=float).reshape(-1)\n            # clip\n            x = np.minimum(np.maximum(x, lb), ub)\n            if evals >= self.budget:\n                # Should never happen if code correctly checks budget\n                return np.inf, x\n            f = func(x)\n            evals += 1\n            nonlocal_best_update(f, x)\n            return f, x\n\n        # update global best\n        def nonlocal_best_update(f, x):\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = np.array(x, copy=True)\n\n        # short-budgets fallback to pure random search\n        if self.budget < 10:\n            self.f_opt = np.inf\n            self.x_opt = None\n            for _ in range(self.budget):\n                x = self.rng.uniform(lb, ub)\n                f, _ = callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population (points + sigmas + fitness)\n        pop_size = min(self.pop_size, max(2, self.budget // 3))  # ensure feasible\n        pop_x = np.empty((pop_size, self.dim), dtype=float)\n        pop_f = np.empty(pop_size, dtype=float)\n        pop_sigma = np.empty(pop_size, dtype=float)\n\n        # initial step scale relative to bounds\n        span = ub - lb\n        default_sigma = 0.2 * np.linalg.norm(span) / np.sqrt(self.dim)  # global typical step\n        # fill population\n        for i in range(pop_size):\n            x = self.rng.uniform(lb, ub)\n            f, x = callf(x)\n            pop_x[i, :] = x\n            pop_f[i] = f\n            pop_sigma[i] = default_sigma * (0.5 + self.rng.rand())  # slight per-individual variation\n\n        # If somehow budget consumed during initialization, return\n        if evals >= self.budget:\n            return self.f_opt, self.x_opt\n\n        # helper indices\n        def best_idx():\n            return int(np.argmin(pop_f))\n\n        def worst_idx():\n            return int(np.argmax(pop_f))\n\n        # main loop: use remaining budget\n        p_jump = 0.03  # chance of Lévy jump each iteration\n        p_rejuvenate = 0.02\n        backtrack_tries = 3\n        orthogonal_tries = 2\n        recombine_tries = 1\n\n        # loop until budget exhausted\n        while evals < self.budget:\n            # small tournament selection to pick a parent\n            tour_k = min(3, pop_size)\n            inds = self.rng.choice(pop_size, tour_k, replace=False)\n            # choose the best among sampled tournament\n            parent_idx = inds[np.argmin(pop_f[inds])]\n            parent_x = pop_x[parent_idx].copy()\n            parent_f = pop_f[parent_idx]\n            sigma = float(pop_sigma[parent_idx])\n\n            # sample a random direction\n            v = self.rng.randn(self.dim)\n            nv = np.linalg.norm(v)\n            if nv == 0:\n                v = self.rng.randn(self.dim)\n                nv = np.linalg.norm(v) + 1e-12\n            v = v / nv\n\n            # primary directional trial with stochasticized step-length\n            step_len = sigma * max(1e-8, 1.0 + 0.4 * self.rng.randn())\n            x_try = parent_x + step_len * v\n            f_try, x_try = callf(x_try)\n            if f_try < parent_f:\n                # accept\n                pop_x[parent_idx] = x_try\n                pop_f[parent_idx] = f_try\n                pop_sigma[parent_idx] = min(np.linalg.norm(span), sigma * 1.12 + 1e-12)\n                # slight elite injection: if new is globally best, update stored best performed by callf\n                continue  # move to next iteration\n            else:\n                # backtracking / small-step refinement\n                improved = False\n                s = step_len\n                for t in range(backtrack_tries):\n                    s = s * 0.5\n                    if evals >= self.budget:\n                        break\n                    x_try = parent_x + s * v\n                    f_try, x_try = callf(x_try)\n                    if f_try < parent_f:\n                        pop_x[parent_idx] = x_try\n                        pop_f[parent_idx] = f_try\n                        pop_sigma[parent_idx] = max(1e-12, sigma * (1.0 + 0.08 * (0.5 ** t)))\n                        improved = True\n                        break\n                if improved:\n                    continue\n\n            # try orthogonal perturbations to diversify locally\n            for k in range(orthogonal_tries):\n                # random vector orthogonal to v\n                u = self.rng.randn(self.dim)\n                # orthogonalize: subtract projection on v\n                u = u - np.dot(u, v) * v\n                nu = np.linalg.norm(u)\n                if nu < 1e-12:\n                    u = self.rng.randn(self.dim)\n                    u = u - np.dot(u, v) * v\n                    nu = np.linalg.norm(u) + 1e-12\n                u = u / nu\n                mag = sigma * (0.3 + 0.7 * self.rng.rand())\n                x_try = parent_x + mag * u\n                f_try, x_try = callf(x_try)\n                if f_try < parent_f:\n                    pop_x[parent_idx] = x_try\n                    pop_f[parent_idx] = f_try\n                    pop_sigma[parent_idx] = min(np.linalg.norm(span), sigma * 1.08)\n                    break  # go to next main iteration\n                if evals >= self.budget:\n                    break\n            if evals >= self.budget:\n                break\n\n            # occasional Lévy-like heavy-tailed jump to escape local minima\n            if self.rng.rand() < p_jump and evals < self.budget:\n                # generate Cauchy-like vector (standard Cauchy) -> heavy tails\n                deltas = self.rng.standard_cauchy(self.dim)\n                # robust scale: median absolute deviation\n                mad = np.median(np.abs(deltas - np.median(deltas))) + 1e-9\n                # normalize while preserving heavy-tail profile\n                deltas = deltas / mad\n                levy_scale = sigma * (2.0 + 3.0 * self.rng.rand())  # variable jump magnitude\n                deltas = deltas * levy_scale\n                # avoid astronomic extremes: cap norm relative to problem span\n                max_norm = 5.0 * np.linalg.norm(span)\n                norm_d = np.linalg.norm(deltas)\n                if norm_d > max_norm:\n                    deltas = deltas / norm_d * max_norm\n                x_jump = parent_x + deltas\n                f_jump, x_jump = callf(x_jump)\n                if f_jump < pop_f[parent_idx]:\n                    # replace parent\n                    pop_x[parent_idx] = x_jump\n                    pop_f[parent_idx] = f_jump\n                    pop_sigma[parent_idx] = max(1e-12, sigma * 1.5)\n                else:\n                    # possibly replace the worst if it's better\n                    wi = worst_idx()\n                    if f_jump < pop_f[wi]:\n                        pop_x[wi] = x_jump\n                        pop_f[wi] = f_jump\n                        pop_sigma[wi] = max(1e-12, sigma * 0.8)\n                # continue loop after jump attempt\n                if evals >= self.budget:\n                    break\n                else:\n                    continue\n\n            # recombination exploitation: mix two best and small noise\n            if evals < self.budget:\n                # take two best individuals\n                sorted_idx = np.argsort(pop_f)\n                i1 = sorted_idx[0]\n                i2 = sorted_idx[1] if pop_size > 1 else sorted_idx[0]\n                alpha = self.rng.beta(2.0, 2.0)\n                child = alpha * pop_x[i1] + (1 - alpha) * pop_x[i2]\n                child += self.rng.randn(self.dim) * (0.08 * sigma)\n                child = np.minimum(np.maximum(child, lb), ub)\n                f_child, child = callf(child)\n                # if improved over parent, replace parent; else maybe replace worst\n                if f_child < pop_f[parent_idx]:\n                    pop_x[parent_idx] = child\n                    pop_f[parent_idx] = f_child\n                    pop_sigma[parent_idx] = max(1e-12, sigma * 1.05)\n                else:\n                    wi = worst_idx()\n                    if f_child < pop_f[wi]:\n                        pop_x[wi] = child\n                        pop_f[wi] = f_child\n                        pop_sigma[wi] = max(1e-12, sigma * 0.9)\n\n            # adapt sigma on failures: if parent hasn't improved recently, shrink it a bit\n            # (we detect failure with no replacement in pop for parent_idx)\n            # small probabilistic adaptation to avoid stagnation\n            if self.rng.rand() < 0.25:\n                pop_sigma[parent_idx] = max(1e-12, pop_sigma[parent_idx] * 0.94)\n            else:\n                pop_sigma[parent_idx] = max(1e-12, pop_sigma[parent_idx] * 0.98)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.rand() < p_rejuvenate and evals < self.budget:\n                wi = worst_idx()\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop_x[wi] = x_new\n                pop_f[wi] = f_new\n                pop_sigma[wi] = default_sigma * (0.5 + self.rng.rand())\n\n        # finished budget\n        # ensure best stored in self.f_opt/self.x_opt (callf updated it)\n        return float(self.f_opt), np.array(self.x_opt, copy=True)", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.590 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15512354113246296, 0.1574963035649325, 0.8159789496196452, 0.9536324780892322, 0.8413727303060371, 0.8813973298156623, 0.2787356139929472, 0.8112959334209187, 0.8396489499469735, 0.16501766375757643]}, "task_prompt": ""}
{"id": "8f290df8-30db-4367-b5c2-38cfdef79475", "fitness": 0.532332261068684, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based directional local search with adaptive step-sizes, orthogonal refinements and occasional Lévy jumps for robust global exploration and local exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-scaled by default)\n    - seed: optional RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but remains modest\n            self.pop_size = max(4, min(40, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        # best solution trackers\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # determine bounds from func if present, else default to [-5,5]\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # expand scalar bounds if necessary\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # ensure shapes match dim\n        if lb.size != self.dim:\n            lb = np.resize(lb, self.dim)\n        if ub.size != self.dim:\n            ub = np.resize(ub, self.dim)\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = float(func(x))\n            remaining -= 1\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return f, x\n\n        # early exit if no budget\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        n_init = min(self.pop_size, remaining)\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial step scale\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.5 + 0.5 * np.random.rand()))\n\n        # if couldn't initialize population, fallback to random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n\n        # main loop\n        while remaining > 0:\n            # small tournament selection\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n            improved = False\n\n            # sample a random direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = np.random.randn(self.dim)\n                nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                continue  # good move, pick new parent\n\n            # local backtracking / small-step refinement along direction\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_bt = np.clip(x_parent + alpha * frac * d, lb, ub)\n                try:\n                    f_bt, x_bt = callf(x_bt)\n                except RuntimeError:\n                    break\n                if f_bt < pop_f[parent_i]:\n                    pop[parent_i] = x_bt\n                    pop_f[parent_i] = f_bt\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # orthogonal perturbation for local diversification\n            r = np.random.randn(self.dim)\n            r = r - np.dot(r, d) * d  # orthogonalize to d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_ort = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    try:\n                        f_ort, x_ort = callf(x_ort)\n                    except RuntimeError:\n                        break\n                    if f_ort < pop_f[parent_i]:\n                        pop[parent_i] = x_ort\n                        pop_f[parent_i] = f_ort\n                        pop_sigma[parent_i] = min(sigma * 1.1, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed)\n            if np.random.rand() < 0.08 and remaining > 0:\n                step = np.random.standard_cauchy(self.dim)\n                # normalize heavy-tail vector to avoid explosion but keep heavy-tailness\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                x_levy = np.clip(x_parent + step * scale_vec, lb, ub)\n                try:\n                    f_levy, x_levy = callf(x_levy)\n                except RuntimeError:\n                    break\n                worst_i = int(np.argmax(pop_f))\n                if f_levy < pop_f[worst_i]:\n                    pop[worst_i] = x_levy\n                    pop_f[worst_i] = f_levy\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump\n                continue\n\n            # recombination exploitation: mix two best and add small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_mix = np.clip(mix + noise, lb, ub)\n                try:\n                    f_mix, x_mix = callf(x_mix)\n                except RuntimeError:\n                    break\n                if f_mix < pop_f[parent_i]:\n                    pop[parent_i] = x_mix\n                    pop_f[parent_i] = f_mix\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    continue\n                else:\n                    # try to inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_mix < pop_f[worst_i]:\n                        pop[worst_i] = x_mix\n                        pop_f[worst_i] = f_mix\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (shrink or jitter)\n            # small probabilistic increase for exploration occasionally\n            if remaining > 0:\n                if np.random.rand() < 0.15:\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.05, np.mean(ub - lb))\n                else:\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.96, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.05 and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget or exhausted by exceptions\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.532 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11182957907430735, 0.16713470145243559, 0.8061755529507117, 0.9609876705603808, 0.5680354920285056, 0.957565740101487, 0.23947104302302247, 0.5281131414969713, 0.841804605853613, 0.142205084145404]}, "task_prompt": ""}
{"id": "23f6330e-df52-4415-92bf-1c4bd024ec4e", "fitness": 0.39575217249043215, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, recombination and occasional heavy-tailed Lévy (Cauchy) jumps to robustly explore and exploit continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that grows with dimensionality\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Prepare bounds (expand scalars to arrays if necessary)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # Ensure bounds have correct shape\n        assert lb.shape == (self.dim,) and ub.shape == (self.dim,), \"Bounds must match dim\"\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Wrapper to call func while tracking budget and best-so-far\n        def callf(x):\n            nonlocal remaining\n            x = np.asarray(x, dtype=float)\n            # clip to bounds before evaluating\n            x = np.minimum(np.maximum(x, lb), ub)\n            if remaining <= 0:\n                # Do not call func anymore; indicate no evaluation left\n                return np.inf, x\n            # perform one function evaluation\n            f = float(func(x))\n            remaining -= 1\n            # update best\n            if f < self.f_opt:\n                self.f_opt = f\n                # store a copy\n                self.x_opt = x.copy()\n            return f, x\n\n        # If no budget, return immediately\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialization\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale relative to search range\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            if np.isinf(f0) and remaining <= 0:\n                break\n            pop.append(x0)\n            pop_f.append(f0)\n            # jitter initial sigmas to encourage diversity\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n            if remaining <= 0:\n                break\n\n        # If budget too small to fill any population, fallback to random sampling with remaining budget\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Main loop: use remaining budget for adaptive searches\n        # Stopping when no remaining evaluations\n        while remaining > 0:\n            # select a parent via small tournament\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                # degenerate draw; replace with small nonzero vector\n                d = np.ones(self.dim)\n                nd = np.linalg.norm(d)\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # accept improvement and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.15 + 1e-12, max(1e-2 * np.mean(ub - lb), pop_sigma[parent_i] * 2.0))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.25, -0.5):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            # produce a vector r and orthogonalize it against d\n            r = np.random.randn(self.dim)\n            # subtract projection to make orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                        continue\n\n            # occasional Lévy-like (Cauchy) jump to escape basins\n            if np.random.rand() < 0.08 and remaining > 0:\n                # Cauchy-like heavy-tailed vector\n                step = np.random.standard_cauchy(self.dim)\n                # robust denom to avoid infinite scaling in rare cases\n                denom = np.median(np.abs(step)) + 1e-6\n                step = step / denom\n                # scale by problem range and current sigma to keep steps meaningful\n                scale_vec = 0.5 * sigma * (ub - lb) / (np.mean(ub - lb) + 1e-12)\n                x_try = np.clip(x_parent + step * scale_vec, lb, ub)\n                if remaining > 0:\n                    f_try, x_try = callf(x_try)\n                    # if it's good replace the worst in population\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    # inject into population by replacing the worst if it's better\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure (shrink to encourage exploitation)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < 0.02:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.5 + 0.8 * np.random.rand())\n\n            # if population has become too small (e.g., some degenerate case), replenish\n            if remaining > 0 and len(pop) < self.pop_size and np.random.rand() < 0.1:\n                # add a new random individual\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                if not np.isinf(f_new):\n                    pop.append(x_new)\n                    pop_f.append(f_new)\n                    pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n\n        # finished or exhausted budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.396 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12556607162608013, 0.15344411761119192, 0.4071826362353609, 0.9531816888062322, 0.24681607263111272, 0.7372138596996252, 0.23038987510766518, 0.4310232052117484, 0.5495431385448616, 0.12316105943044342]}, "task_prompt": ""}
{"id": "f3da7962-787f-47a5-bb6a-1b924522c932", "fitness": 0.28414961997649746, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional searches with orthogonal refinements and occasional heavy-tailed Lévy jumps for robust exploration/exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (if None, auto-scaled by dim)\n    - seed: optional random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales modestly with dimensionality\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        if seed is not None:\n            np.random.seed(seed)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds and ensure full-dim arrays\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        # defensive: ensure shapes match dim\n        if lb.size != self.dim or ub.size != self.dim:\n            raise ValueError(\"Bounds dimension mismatch with self.dim\")\n\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds before evaluating (safe-guard)\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                # keep a copy to avoid accidental outside modification\n                self.x_opt = x.copy()\n            return f, x.copy()\n\n        # fallback if no budget\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initialize population\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        base_sigma = max(1e-8, 0.25 * np.mean(ub - lb))  # initial scale relative to domain\n        # choose initial size considering budget (do not exhaust budget at init)\n        n_init = min(self.pop_size, max(1, remaining // 10))\n        # make sure at least 2 individuals if budget allows\n        n_init = max(1, min(n_init, self.pop_size))\n        for i in range(n_init):\n            if remaining <= 0:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma * (0.8 + 0.4 * np.random.rand()))\n\n        # If no population could be created (very small budget), do pure random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                f, x = callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # main optimization loop\n        # parameters controlling behavior\n        levy_prob = 0.08\n        orth_prob = 0.6\n        recomb_prob = 0.4\n        rejuvenate_prob = 0.02\n\n        while remaining > 0:\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(3, len(pop))\n            inds = np.random.choice(len(pop), k, replace=False)\n            values = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(values))]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * np.random.randn())\n            if remaining <= 0:\n                break\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try, x_try = callf(x_try)\n\n            success = False\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2, np.mean(ub - lb))\n                success = True\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if np.random.rand() < orth_prob and remaining > 0:\n                r = np.random.randn(self.dim)\n                # remove component along d to make it orthogonal\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    step_scale = sigma * (0.3 + 0.7 * np.random.rand())\n                    x_try = np.clip(x_parent + r * step_scale, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = min(sigma * 1.15, np.mean(ub - lb))\n                        continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < levy_prob and remaining > 0:\n                # Cauchy-like heavy-tailed vector (tangent transform)\n                raw = np.tan(np.pi * (np.random.rand(self.dim) - 0.5))\n                # robust normalization but keep heavy-tail sign/ratio\n                raw_norm = np.linalg.norm(raw) + 1e-12\n                raw = raw / raw_norm\n                # scale per-dimension by domain size and a random magnitude\n                step_mag = 0.05 + 0.45 * np.random.rand()\n                scale_vec = step_mag * (ub - lb)\n                x_try = np.clip(x_parent + raw * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                # if it's good replace the worst in population, else maybe keep it as candidate by replacing a random one sometimes\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                # continue loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and np.random.rand() < recomb_prob and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = 0.7 * (0.6 + 0.8 * np.random.rand())  # bias towards the best\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                noise = (0.01 * (ub - lb)) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                f_try, x_try = callf(x_try)\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                else:\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n\n            # adapt parent sigma on failure\n            if not success:\n                pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if remaining > 0 and np.random.rand() < rejuvenate_prob:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.284 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.112644298985214, 0.1460460781465931, 0.6053246940486756, 0.17421045101283683, 0.1760966498462445, 0.7107867539272718, 0.22852383562648904, 0.3245388479106268, 0.21794925415441668, 0.1453753361066067]}, "task_prompt": ""}
{"id": "553be98d-ce46-4a9f-b2d7-78eb9ab98ccd", "fitness": 0.5201273514515993, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based search combining randomized directional local searches, orthogonal refinements, occasional Lévy jumps and simple recombination to robustly explore/exploit bounded continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to modest scaling with dim)\n    - seed: optional random seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population scales with dimensionality but stays modest\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # prepare bounds (func.bounds.lb / ub can be scalars or arrays)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item())\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item())\n\n        # safety: enforce provided problem dim\n        if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n            # try to adapt but keep consistent shape\n            lb = np.resize(lb, self.dim).astype(float)\n            ub = np.resize(ub, self.dim).astype(float)\n\n        # counters and defaults\n        remaining = int(self.budget)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            # ensure x is numpy array of correct shape and clipped to bounds\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            remaining -= 1\n            f = float(func(x))\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If no budget, return default\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Base sigma scale (initial step-size)\n        base_sigma = max(1e-12, 0.25 * float(np.mean(ub - lb)))\n\n        # Initialize population by sampling uniformly until pop_size or budget runs out\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        for _ in range(self.pop_size):\n            if remaining <= 0:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0.copy())\n            pop_f.append(f0)\n            pop_sigma.append(base_sigma)\n        # If we couldn't create a meaningful population (very small budget), fallback to random search\n        if len(pop) == 0:\n            while remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                callf(x)\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        # control parameters\n        levy_prob = 0.08\n        rejuvenate_prob = 0.03\n        tournament_k = min(3, len(pop))\n        recomb_prob = 0.15\n        iter_count = 0\n\n        while remaining > 0:\n            iter_count += 1\n            # pick a parent via small tournament (take best among k random)\n            inds = self.rng.choice(len(pop), size=tournament_k, replace=False)\n            parent_i = int(min(inds, key=lambda ii: pop_f[ii]))  # index with smallest f among inds\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = self.rng.normal(size=self.dim)\n            nd = np.linalg.norm(d)\n            if nd < 1e-12:\n                d = self.rng.normal(size=self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    d = np.ones(self.dim)\n                    nd = np.linalg.norm(d)\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * (1.0 + 0.5 * self.rng.normal())\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try.copy()\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.2 + 1e-12, np.mean(ub - lb))\n                continue\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, -0.5, -0.25):\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + alpha * frac * d, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            r = self.rng.normal(size=self.dim)\n            # project r to be orthogonal to d\n            r = r - np.dot(r, d) * d\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                if remaining <= 0:\n                    break\n                x_try = np.clip(x_parent + 0.6 * sigma * r, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.95, 1e-12)\n                    continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if self.rng.random() < levy_prob and remaining > 0:\n                # Cauchy-like heavy-tailed vector normalized by a robust scale\n                step = self.rng.standard_cauchy(self.dim)\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                step = step / denom\n                scale_vec = 0.2 * (ub - lb)\n                # random global center near parent with heavy-tail\n                x_try = np.clip(x_parent + scale_vec * step, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try.copy()\n                    pop_f[worst_i] = f_try\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.5)\n                    continue  # keep exploring from new pop\n                else:\n                    # small chance to accept as parent to diversify\n                    if self.rng.random() < 0.08:\n                        pop[parent_i] = x_try.copy()\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(1e-12, sigma * 0.7)\n                        continue\n\n            # recombination exploitation: mix two best and small noise\n            if len(pop) >= 2 and self.rng.random() < recomb_prob and remaining > 0:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = int(best2[0]), int(best2[1])\n                beta = self.rng.uniform(0.2, 0.8)\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # small gaussian noise proportional to sigma\n                mix = np.clip(mix + 0.05 * sigma * self.rng.normal(size=self.dim), lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(mix)\n                worst_i = int(np.argmax(pop_f))\n                # replace parent if improved, else possibly replace worst\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try.copy()\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, 1e-12)\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                    continue\n                else:\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try.copy()\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * 0.5\n                        continue\n\n            # adapt parent sigma on failure (shrink moderately)\n            pop_sigma[parent_i] = max(sigma * 0.85, 1e-12)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if self.rng.random() < rejuvenate_prob and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = self.rng.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop[worst_i] = x_new.copy()\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma\n\n            # small housekeeping: keep arrays consistent length (if budget forced early exit)\n            if len(pop) != len(pop_f) or len(pop) != len(pop_sigma):\n                # truncate to min length\n                m = min(len(pop), len(pop_f), len(pop_sigma))\n                pop = pop[:m]\n                pop_f = pop_f[:m]\n                pop_sigma = pop_sigma[:m]\n\n        # finished budget\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.520 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11017737998213939, 0.15467100618601048, 0.8410080285566424, 0.928812096952151, 0.5700771840850987, 0.9484810131965341, 0.2571713356630868, 0.5091483193944797, 0.7216706347813774, 0.16005651571847312]}, "task_prompt": ""}
{"id": "89600222-1617-4d66-ad4f-98c3d99a3c7f", "fitness": 0.5608575913806038, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based, directional local searches with orthogonal refinements and occasional Lévy-like jumps; sigmas adapt per-individual and the worst members are rejuvenated to balance exploration/exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional random seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        # population size: modest but grows with dim\n        if pop_size is None:\n            self.pop_size = max(6, min(30, 3 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds handling (func.bounds.lb/ub may be scalar or arrays)\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        # safety: ensure shapes\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        # evaluation budget remaining\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                # budget exhausted; return current best without calling func\n                return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n            x = np.asarray(x, dtype=float)\n            # clip\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            nonlocal_best = False\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n                nonlocal_best = True\n            return float(f), x.copy()\n\n        # quick fallback if no budget\n        if remaining <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # initial base sigma expressed as fraction of range per-dim (0..1)\n        range_vec = ub - lb\n        range_mean = float(np.maximum(1e-12, range_vec.mean()))\n        base_sigma = 0.08  # fraction of range used as typical sigma\n        # population structures\n        pop = []\n        pop_f = []\n        pop_sigma = []  # fraction of range (0..something)\n        pop_stagnant = []  # counters for stagnation\n\n        # initialize population (use up to remaining budget)\n        n_init = min(self.pop_size, remaining)\n        for i in range(n_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(float(f0))\n            # initialize sigma with some randomization\n            pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n            pop_stagnant.append(0)\n\n        # If we couldn't create any members (very tiny budget), return best\n        if len(pop) == 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # convenience conversions\n        pop = list(pop)\n        pop_f = list(pop_f)\n        pop_sigma = list(pop_sigma)\n        pop_stagnant = list(pop_stagnant)\n\n        # main loop\n        while remaining > 0:\n            # tournament selection for parent (balance exploration/exploitation)\n            k = min(3, len(pop))\n            cand_inds = np.random.choice(len(pop), k, replace=False)\n            # choose best among candidates (minimization)\n            parent_i = int(cand_inds[int(np.argmin([pop_f[i] for i in cand_inds]))])\n            x_parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n\n            # generate a normalized random direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd <= 1e-12:\n                # fallback direction\n                d = np.zeros(self.dim)\n                d[np.random.randint(self.dim)] = 1.0\n                nd = 1.0\n            d = d / nd\n\n            # stochastic multiplicative step factor (log-normal style) to create varied step-lengths\n            step_factor = np.exp(np.random.normal(loc=0.0, scale=0.6))\n            # occasional heavy tail inflation\n            if np.random.rand() < 0.06:\n                step_factor *= (1.0 + abs(np.random.standard_cauchy()) / 4.0)\n            # scale vector (per-dim)\n            scale_vec = sigma * range_vec\n\n            # primary directional try\n            s = step_factor  # relative multiplier\n            x_try = np.clip(x_parent + d * s * scale_vec, lb, ub)\n            if remaining <= 0:\n                break\n            f_try, x_try = callf(x_try)\n\n            improved = False\n            if f_try < pop_f[parent_i]:\n                # accept and expand sigma a bit\n                pop[parent_i] = x_try\n                pop_f[parent_i] = float(f_try)\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.15 + 1e-12, 1.0)\n                pop_stagnant[parent_i] = 0\n                improved = True\n                # continue to next iteration (exploitation)\n                continue\n            else:\n                # small backtracking along direction (try smaller fractions)\n                for frac in (0.5, 0.25, -0.5, -0.25):\n                    if remaining <= 0:\n                        break\n                    x_try = np.clip(x_parent + d * s * frac * scale_vec, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = float(f_try)\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n                        pop_stagnant[parent_i] = 0\n                        improved = True\n                        break\n                # if still not improved, mark a failure for this parent\n                if not improved:\n                    pop_stagnant[parent_i] += 1\n                    # contract sigma slightly due to failure\n                    pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n\n            # orthogonal refinement: try a perturbation orthogonal to the chosen direction\n            if remaining <= 0:\n                break\n            r = np.random.randn(self.dim)\n            # remove the component along d to make r orthogonal-ish\n            proj = np.dot(r, d) * d\n            r = r - proj\n            nr = np.linalg.norm(r)\n            if nr > 1e-12:\n                r = r / nr\n                ortho_scale = 0.4  # relative smaller moves\n                x_try = np.clip(x_parent + r * ortho_scale * s * scale_vec, lb, ub)\n                f_try, x_try = callf(x_try)\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = float(f_try)\n                    pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.08, 1.0)\n                    pop_stagnant[parent_i] = 0\n\n            # occasional Lévy-like jump to escape local minima\n            if remaining > 0 and np.random.rand() < 0.08:\n                # Cauchy vector with robust normalization to keep heavy tail but cap extremes\n                step = np.random.standard_cauchy(size=self.dim)\n                # cap extremes to avoid numerical blow-ups, but keep tail behavior\n                step = np.clip(step, -50.0, 50.0)\n                denom = np.linalg.norm(step)\n                if denom <= 1e-12:\n                    step = np.random.randn(self.dim)\n                    denom = np.linalg.norm(step)\n                step = step / denom\n                # scale jump: often large compared to sigma\n                jump_mult = 4.0 + 6.0 * np.random.rand()\n                x_try = np.clip(x_parent + step * jump_mult * sigma * range_vec, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                if f_try < max(pop_f):\n                    # replace worst with jump if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = float(f_try)\n                    pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                    pop_stagnant[worst_i] = 0\n                else:\n                    # maybe keep jump as new candidate by replacing the worst with some small prob\n                    if np.random.rand() < 0.02:\n                        worst_i = int(np.argmax(pop_f))\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = float(f_try)\n                        pop_sigma[worst_i] = max(1e-12, sigma * 0.6)\n                        pop_stagnant[worst_i] = 0\n\n            # recombination exploitation: mix two best and inject small noise\n            if remaining > 0 and len(pop) >= 2 and np.random.rand() < 0.15:\n                # pick two best\n                sorted_inds = np.argsort(pop_f)\n                a, b = int(sorted_inds[0]), int(sorted_inds[1])\n                alpha = 0.55 + 0.1 * (np.random.rand() - 0.5)  # slight bias\n                mix = alpha * pop[a] + (1.0 - alpha) * pop[b]\n                noise = (0.02 * range_vec) * np.random.randn(self.dim)\n                x_try = np.clip(mix + noise, lb, ub)\n                if remaining <= 0:\n                    break\n                f_try, x_try = callf(x_try)\n                # inject if better than worst\n                worst_i = int(np.argmax(pop_f))\n                if f_try < pop_f[worst_i]:\n                    pop[worst_i] = x_try\n                    pop_f[worst_i] = float(f_try)\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n                    pop_stagnant[worst_i] = 0\n\n            # adapt sigma globally a bit based on stagnation: rejuvenate worst occasionally\n            if remaining > 0 and np.random.rand() < 0.04:\n                worst_i = int(np.argmax(pop_f))\n                # if worst has been stagnant for a while or by chance, replace with random\n                if pop_stagnant[worst_i] > 6 or np.random.rand() < 0.6:\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = float(f_new)\n                    pop_sigma[worst_i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n                    pop_stagnant[worst_i] = 0\n\n            # ensure sigma values remain in reasonable bounds\n            for i in range(len(pop_sigma)):\n                if pop_sigma[i] <= 0 or not np.isfinite(pop_sigma[i]):\n                    pop_sigma[i] = base_sigma * (0.6 + 0.8 * np.random.rand())\n\n            # small population maintenance: if we have room (budget left and small population), expand\n            if remaining > 0 and len(pop) < self.pop_size and np.random.rand() < 0.05:\n                x_new = np.random.uniform(lb, ub)\n                f_new, x_new = callf(x_new)\n                pop.append(x_new)\n                pop_f.append(float(f_new))\n                pop_sigma.append(base_sigma * (0.6 + 0.8 * np.random.rand()))\n                pop_stagnant.append(0)\n\n            # If budget is very low, do a few random local samples around the best to polish\n            if remaining > 0 and remaining < max(5, int(0.01 * self.budget)):\n                # polish around current best\n                if self.x_opt is not None:\n                    perturb = (0.01 * range_vec) * np.random.randn(self.dim)\n                    x_try = np.clip(self.x_opt + perturb, lb, ub)\n                    f_try, x_try = callf(x_try)\n                    # optionally replace worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = float(f_try)\n\n        # finished budget or loop exit\n        return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.561 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16498314187119012, 0.16924322046485263, 0.735497472098777, 0.9331763503131794, 0.8697815758880574, 0.8906720310853566, 0.28201699635945976, 0.5591661787825369, 0.8473608210467857, 0.15667812589584196]}, "task_prompt": ""}
{"id": "2586bc4c-540c-4746-9257-54055b03f502", "fitness": 0.504003453233016, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional searches with orthogonal refinements and occasional heavy-tailed Lévy jumps to escape local basins.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (defaults to moderate scaling with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # moderate population scaling: keep modest even for higher dims\n            self.pop_size = int(max(4, min(40, 4 * self.dim)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # read bounds and ensure arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # if bounds are scalars or shape mismatch, broadcast\n        if lb.shape == () or lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == () or ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # evaluation bookkeeping\n        eval_count = 0\n        budget = int(self.budget)\n\n        # local best trackers\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        def eval_wrapper(x):\n            nonlocal eval_count\n            # do not exceed budget\n            if eval_count >= budget:\n                raise RuntimeError(\"Evaluation budget exceeded.\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.clip(x, lb, ub)\n            f = float(func(x))\n            eval_count += 1\n            # update best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            return f, x\n\n        # If budget is extremely small, fallback to random search\n        if budget <= 0:\n            return self.f_opt, self.x_opt\n\n        # base sigma scale (relative to search range)\n        range_scale = np.mean(ub - lb)\n        base_sigma = max(1e-9, 0.2 * range_scale)  # default global step scale\n        max_sigma = np.mean(ub - lb)  # do not grow beyond range\n        min_sigma = 1e-12\n\n        # initialize population (may use up budget)\n        pop_size = min(self.pop_size, budget)  # cannot start more individuals than budget\n        pop = np.zeros((pop_size, self.dim), dtype=float)\n        pop_f = np.full(pop_size, np.inf, dtype=float)\n        pop_sigma = np.zeros(pop_size, dtype=float)\n\n        for i in range(pop_size):\n            if eval_count >= budget:\n                break\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = eval_wrapper(x0)\n            pop[i, :] = x0\n            pop_f[i] = f0\n            pop_sigma[i] = base_sigma * (0.4 + 0.6 * np.random.rand())\n\n        # if no population created (very small budget), do pure random search with remaining budget\n        if pop.shape[0] == 0 or eval_count >= budget:\n            # random search remaining budget\n            while eval_count < budget:\n                x = np.random.uniform(lb, ub)\n                eval_wrapper(x)\n            return self.f_opt, self.x_opt\n\n        # main loop: use remaining budget\n        no_improve_counter = np.zeros(pop_size, dtype=int)\n        it = 0\n        while eval_count < budget:\n            it += 1\n            remaining = budget - eval_count\n\n            # pick a parent via small tournament to balance exploration/exploitation\n            tour_k = min(3, pop.shape[0])\n            candidates = np.random.choice(pop.shape[0], size=tour_k, replace=False)\n            parent_i = candidates[np.argmin(pop_f[candidates])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n            # clamp sigma to reasonable range\n            sigma = float(np.clip(sigma, min_sigma, max_sigma))\n\n            # sample normalized direction\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-20\n            d = d / nd\n\n            # stochasticized step-length\n            step_scale = sigma * (0.6 + 0.8 * np.random.rand())  # mix of exploitation/exploration\n            x_try = np.clip(x_parent + step_scale * d, lb, ub)\n            try:\n                f_try, x_try = eval_wrapper(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(sigma * 1.25, max_sigma)\n                no_improve_counter[parent_i] = 0\n                continue\n            else:\n                no_improve_counter[parent_i] += 1\n\n            # local backtracking / small-step refinement along direction (few tries)\n            improved = False\n            for frac in (0.5, 0.25, 0.125):\n                if eval_count >= budget:\n                    break\n                x_try = np.clip(x_parent + step_scale * frac * d, lb, ub)\n                try:\n                    f_try, x_try = eval_wrapper(x_try)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.15, max_sigma)\n                    no_improve_counter[parent_i] = 0\n                    improved = True\n                    break\n            if improved:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            if eval_count < budget:\n                r = np.random.randn(self.dim)\n                # make r orthogonal to d\n                r = r - (r.dot(d)) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-20:\n                    r = r / nr\n                    orth_scale = 0.6 * sigma * (0.6 + 0.8 * np.random.rand())\n                    x_try = np.clip(x_parent + orth_scale * r, lb, ub)\n                    try:\n                        f_try, x_try = eval_wrapper(x_try)\n                    except RuntimeError:\n                        break\n                    if f_try < pop_f[parent_i]:\n                        pop[parent_i] = x_try\n                        pop_f[parent_i] = f_try\n                        pop_sigma[parent_i] = max(sigma * 0.9, min_sigma)\n                        no_improve_counter[parent_i] = 0\n                        continue\n\n            # occasional Lévy-like jump to escape local basins\n            p_jump = 0.06 + 0.04 * (no_improve_counter[parent_i] / (5 + no_improve_counter[parent_i]))\n            if np.random.rand() < p_jump and eval_count < budget:\n                # sample heavy-tailed vector (Cauchy-like)\n                step = np.random.standard_cauchy(size=self.dim)\n                # robust normalization to avoid exceedingly huge steps\n                denom = np.percentile(np.abs(step), 90) + 1e-12\n                levy_scale = sigma * (2.0 + 3.0 * np.random.rand())\n                step = step * (levy_scale / denom)\n                x_try = np.clip(x_parent + step, lb, ub)\n                try:\n                    f_try, x_try = eval_wrapper(x_try)\n                except RuntimeError:\n                    break\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.8, min_sigma)\n                    no_improve_counter[parent_i] = 0\n                    continue\n                else:\n                    # try replacing worst if better than worst\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n                        continue\n\n            # recombination exploitation: mix two best and small noise\n            if pop.shape[0] >= 2 and eval_count < budget and np.random.rand() < 0.25:\n                best2 = np.argsort(pop_f)[:2]\n                a, b = best2[0], best2[1]\n                beta = np.random.rand()\n                mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                # small gaussian perturbation relative to average sigma\n                avg_sigma = (pop_sigma[a] + pop_sigma[b]) * 0.5\n                mix = np.clip(mix + 0.08 * avg_sigma * np.random.randn(self.dim), lb, ub)\n                try:\n                    f_try, x_try = eval_wrapper(mix)\n                except RuntimeError:\n                    break\n                if f_try < pop_f[parent_i]:\n                    pop[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = max(sigma * 0.9, min_sigma)\n                    no_improve_counter[parent_i] = 0\n                else:\n                    # maybe inject into population by replacing the worst if it's better\n                    worst_i = int(np.argmax(pop_f))\n                    if f_try < pop_f[worst_i]:\n                        pop[worst_i] = x_try\n                        pop_f[worst_i] = f_try\n                        pop_sigma[worst_i] = base_sigma * (0.5 + 0.5 * np.random.rand())\n\n            # adapt parent sigma on failure (no improvement within several tries)\n            if no_improve_counter[parent_i] > 3:\n                pop_sigma[parent_i] = max(sigma * 0.85, min_sigma)\n            elif no_improve_counter[parent_i] == 0:\n                pop_sigma[parent_i] = min(sigma * 1.05, max_sigma)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < 0.02 and eval_count < budget:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                try:\n                    f_new, x_new = eval_wrapper(x_new)\n                except RuntimeError:\n                    break\n                pop[worst_i] = x_new\n                pop_f[worst_i] = f_new\n                pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n                no_improve_counter[worst_i] = 0\n\n            # small safeguard: if population collapsed (all equal), inject random\n            if np.allclose(pop, pop[0], atol=1e-12):\n                worst_i = int(np.argmax(pop_f))\n                if eval_count < budget:\n                    x_new = np.random.uniform(lb, ub)\n                    try:\n                        f_new, x_new = eval_wrapper(x_new)\n                    except RuntimeError:\n                        break\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = base_sigma * (0.7 + 0.6 * np.random.rand())\n\n        # finished\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.504 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16477785614071472, 0.17094763619463627, 0.645955716965008, 0.8793849639569498, 0.6782351079052115, 0.7514362290066627, 0.29845952321657143, 0.5495363072932004, 0.6836237066047721, 0.21767748504643314]}, "task_prompt": ""}
{"id": "f7d2d400-cb0e-4543-bb23-dfb2326f1ba0", "fitness": 0.5137579398924397, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population of adaptive step-size walkers that combine randomized directional local searches, orthogonal refinements, recombination and occasional heavy‑tailed Lévy/Cauchy jumps to robustly explore continuous spaces.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size (optional): desired population size (ADLS adapts if budget is small)\n    - seed (optional): RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # sensible default population size: scales with dim, but limited by budget\n        if pop_size is None:\n            self.pop_size = max(4, min(12, int(2 * self.dim)))\n        else:\n            self.pop_size = int(max(2, pop_size))\n\n        # internal best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Determine bounds; Many BBOB uses func.bounds.lb / ub, but fallback to [-5,5]\n        if hasattr(func, \"bounds\") and getattr(func.bounds, \"lb\", None) is not None:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            # if scalars, expand\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n        else:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        if np.any(span <= 0):\n            raise ValueError(\"Invalid bounds provided\")\n\n        remaining = int(self.budget)\n\n        # Helper evaluate function which clips and tracks budget & best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                raise RuntimeError(\"No evaluations remaining\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            # clip to bounds\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            remaining -= 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f), x\n\n        # If budget extremely small, do simple random sampling until budget used\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n\n        # Initialize population (positions, values, adaptive sigmas)\n        pop = []\n        pop_f = []\n        pop_sigma = []\n        # initial sigma per individual: fraction of domain size\n        base_sigma = np.maximum(span * 0.2, 1e-8)  # vector scale; we'll use mean for scalar sigma\n        # number of individuals we can afford to initialize (leave some budget for search)\n        max_init = min(self.pop_size, max(1, remaining))\n        for i in range(max_init):\n            x0 = np.random.uniform(lb, ub)\n            f0, x0 = callf(x0)\n            pop.append(x0)\n            pop_f.append(f0)\n            # sigma scalar per individual: use mean span scaled by randomness\n            pop_sigma.append(float(np.mean(base_sigma) * (0.5 + np.random.rand())))\n\n        # If we couldn't create any population (budget exhausted), return\n        if len(pop) == 0:\n            return self.f_opt, self.x_opt\n\n        # Main optimization loop\n        # Heuristic control params\n        p_levy = 0.08   # probability for heavy-tailed jump\n        p_rejuv = 0.02  # probability to replace worst by random\n        max_backtracks = 4\n        stagnation_counter = np.zeros(len(pop), dtype=int)\n\n        while remaining > 0:\n            n_pop = len(pop)\n            # small tournament selection to pick parent\n            k = min(3, n_pop)\n            inds = np.random.choice(n_pop, size=k, replace=False)\n            vals = [pop_f[i] for i in inds]\n            parent_i = inds[int(np.argmin(vals))]\n            parent = pop[parent_i].copy()\n            sigma = float(pop_sigma[parent_i])\n            sigma = max(sigma, 1e-12)\n\n            # sample direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d) + 1e-12\n            d = d / nd\n\n            # stochasticized step-length (log-normal jitter around sigma)\n            alpha = sigma * np.exp(0.5 * np.random.randn()) * np.random.randn()\n            x_try = parent + alpha * d\n            # bound and evaluate\n            try:\n                f_try, x_try = callf(x_try)\n            except RuntimeError:\n                break\n\n            if f_try < pop_f[parent_i]:\n                # successful directional move: accept and increase sigma slightly\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.12, np.mean(span))\n                stagnation_counter[parent_i] = 0\n                continue\n            else:\n                # local backtracking along direction: reduce step and try few times\n                improved = False\n                alpha_bt = alpha\n                for bt in range(max_backtracks):\n                    alpha_bt = alpha_bt * 0.5\n                    x_bt = parent + alpha_bt * d\n                    try:\n                        f_bt, x_bt = callf(x_bt)\n                    except RuntimeError:\n                        f_bt = np.inf\n                        x_bt = parent\n                    if f_bt < pop_f[parent_i]:\n                        pop[parent_i] = x_bt\n                        pop_f[parent_i] = f_bt\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                        stagnation_counter[parent_i] = 0\n                        improved = True\n                        break\n                if improved:\n                    continue\n                # else directional fails: try orthogonal small perturbations\n                # create orthogonal vector to d\n                r = np.random.randn(self.dim)\n                # project out component along d to make orthogonal\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    o = r / nr\n                    # try a couple of orthogonal probes\n                    for _ in range(2):\n                        alpha_o = sigma * 0.6 * np.random.randn()\n                        x_o = parent + alpha_o * o\n                        try:\n                            f_o, x_o = callf(x_o)\n                        except RuntimeError:\n                            f_o = np.inf\n                            x_o = parent\n                        if f_o < pop_f[parent_i]:\n                            pop[parent_i] = x_o\n                            pop_f[parent_i] = f_o\n                            pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.1, np.mean(span))\n                            stagnation_counter[parent_i] = 0\n                            improved = True\n                            break\n                if improved:\n                    continue\n\n                # occasional Lévy/Cauchy jump to escape basins\n                if np.random.rand() < p_levy and remaining > 0:\n                    # Cauchy heavy-tailed vector, robustly scaled\n                    cauchy_vec = np.random.standard_cauchy(self.dim)\n                    # normalize to unit direction and scale by a heavy multiplier\n                    med = np.median(np.abs(cauchy_vec)) + 1e-9\n                    cvec = cauchy_vec / (med)\n                    # randomly choose a jump scale: mixture of big and moderate\n                    jump_scale = np.mean(span) * (2.0 ** np.random.randn())  # log-scale variation\n                    step = 0.5 + np.abs(np.random.standard_cauchy())  # positive heavy-tailed magnitude\n                    x_jump = parent + (cvec / (np.linalg.norm(cvec) + 1e-12)) * jump_scale * step\n                    # clip and evaluate\n                    try:\n                        f_j, x_j = callf(x_jump)\n                    except RuntimeError:\n                        f_j = np.inf\n                        x_j = parent\n                    if f_j < pop_f[parent_i]:\n                        pop[parent_i] = x_j\n                        pop_f[parent_i] = f_j\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 1.3, 1e-12)\n                        stagnation_counter[parent_i] = 0\n                    else:\n                        # if jump is good compared to worst, replace worst\n                        worst_i = int(np.argmax(pop_f))\n                        if f_j < pop_f[worst_i]:\n                            pop[worst_i] = x_j\n                            pop_f[worst_i] = f_j\n                            pop_sigma[worst_i] = max(pop_sigma[worst_i] * 0.9, 1e-12)\n                    # continue main loop\n                    continue\n\n                # recombination exploitation: mix two best\n                if n_pop >= 2 and remaining > 0:\n                    # pick two distinct indices among the best quarter (if small pop, pick best two)\n                    ordered = np.argsort(pop_f)\n                    a = ordered[0]\n                    b = ordered[1] if len(ordered) > 1 else ordered[0]\n                    beta = np.random.rand()\n                    mix = beta * pop[a] + (1.0 - beta) * pop[b]\n                    # add small gaussian noise proportional to average sigma\n                    mean_sigma = max(1e-12, np.mean(pop_sigma))\n                    mix += 0.08 * mean_sigma * np.random.randn(self.dim)\n                    try:\n                        f_mix, x_mix = callf(mix)\n                    except RuntimeError:\n                        f_mix = np.inf\n                        x_mix = mix\n                    if f_mix < pop_f[parent_i]:\n                        pop[parent_i] = x_mix\n                        pop_f[parent_i] = f_mix\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.95, 1e-12)\n                        stagnation_counter[parent_i] = 0\n                        continue\n                    else:\n                        # maybe replace worst if better\n                        worst_i = int(np.argmax(pop_f))\n                        if f_mix < pop_f[worst_i]:\n                            pop[worst_i] = x_mix\n                            pop_f[worst_i] = f_mix\n                            pop_sigma[worst_i] = max(pop_sigma[worst_i] * 0.9, 1e-12)\n                            continue\n\n                # adapt parent sigma on failure (decrease), increment stagnation counter\n                pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.92, 1e-12)\n                stagnation_counter[parent_i] += 1\n\n                # occasional population rejuvenation: replace worst with fresh random\n                if remaining > 0 and np.random.rand() < p_rejuv:\n                    worst_i = int(np.argmax(pop_f))\n                    x_new = np.random.uniform(lb, ub)\n                    try:\n                        f_new, x_new = callf(x_new)\n                    except RuntimeError:\n                        f_new = np.inf\n                        x_new = x_new\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = float(np.mean(base_sigma) * (0.3 + 0.7 * np.random.rand()))\n                    stagnation_counter[worst_i] = 0\n\n            # occasionally prune/expand population if budget allows and stagnation observed\n            if remaining > 2 and len(pop) < self.pop_size and np.random.rand() < 0.05:\n                # add a mutated copy of a good individual\n                best_i = int(np.argmin(pop_f))\n                x_new = pop[best_i] + 0.2 * np.mean(pop_sigma) * np.random.randn(self.dim)\n                try:\n                    f_new, x_new = callf(x_new)\n                except RuntimeError:\n                    f_new = np.inf\n                    x_new = x_new\n                pop.append(x_new)\n                pop_f.append(f_new)\n                pop_sigma.append(float(np.mean(base_sigma) * (0.3 + 0.7 * np.random.rand())))\n                stagnation_counter = np.append(stagnation_counter, 0)\n\n            # if some individuals stagnate too long, nudge them\n            for i in range(len(pop)):\n                if stagnation_counter[i] > 12 and remaining > 0 and np.random.rand() < 0.3:\n                    # small random perturbation\n                    x_pert = pop[i] + 0.5 * pop_sigma[i] * np.random.randn(self.dim)\n                    try:\n                        f_pert, x_pert = callf(x_pert)\n                    except RuntimeError:\n                        f_pert = np.inf\n                        x_pert = x_pert\n                    if f_pert < pop_f[i]:\n                        pop[i] = x_pert\n                        pop_f[i] = f_pert\n                        pop_sigma[i] = max(pop_sigma[i] * 0.9, 1e-12)\n                        stagnation_counter[i] = 0\n                    else:\n                        # if still bad, increase its sigma to encourage escape\n                        pop_sigma[i] = min(pop_sigma[i] * 1.5, np.mean(span))\n\n            # trim population if budget tight\n            if remaining < max(1, self.budget // 20) and len(pop) > 4:\n                # keep the best 4\n                ordered = np.argsort(pop_f)\n                keep = ordered[:4]\n                pop = [pop[i] for i in keep]\n                pop_f = [pop_f[i] for i in keep]\n                pop_sigma = [pop_sigma[i] for i in keep]\n                stagnation_counter = stagnation_counter[keep]\n\n        # return best found\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.514 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13776902100267352, 0.17225549929009865, 0.7817754126234384, 0.9584153605794007, 0.9133410890700244, 0.9416911390367547, 0.2400388985455253, 0.5422635349227151, 0.29598595081764445, 0.15404349303612075]}, "task_prompt": ""}
{"id": "045e0a33-535e-40b6-a711-7a903a11534a", "fitness": 0.3567527068858665, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — maintains a modest population of points with adaptive step-sizes, performs randomized directional local searches with orthogonal refinements, occasional Lévy (Cauchy) escapes and recombination to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (default scales with dim)\n    - seed: optional RNG seed\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # population grows modestly with dimensionality but stays small\n            self.pop_size = max(4, min(30, int(4 + 2 * np.sqrt(self.dim))))\n        else:\n            self.pop_size = max(2, int(pop_size))\n        if seed is not None:\n            np.random.seed(seed)\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # determine bounds (Many BBOB style: func.bounds.lb / ub) or fallback to [-5,5]\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        # ensure lb/ub are full-dim arrays\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n\n        # remaining evaluations\n        remaining = int(self.budget)\n\n        # helper to evaluate while tracking budget and best\n        def callf(x):\n            nonlocal remaining\n            if remaining <= 0:\n                return np.inf\n            # ensure x is numpy array of correct shape and clipped\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.clip(x, lb, ub)\n            remaining -= 1\n            f = func(x)\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            return float(f)\n\n        # If budget is extremely small, fallback to simple random search\n        if remaining <= 0:\n            return self.f_opt, self.x_opt\n        if remaining < 5:\n            # just random samples until budget is spent\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Initialize population (use as many as budget allows but <= pop_size)\n        n_init = min(self.pop_size, remaining)\n        pop = np.random.uniform(lb, ub, size=(n_init, self.dim))\n        pop_sigma = np.maximum(1e-8, 0.08 * (ub - lb).mean()) * np.ones(n_init)  # individual step sizes\n        pop_f = np.full(n_init, np.inf)\n        for i in range(n_init):\n            pop_f[i] = callf(pop[i])\n            # if we used up budget mid-init, break\n            if remaining <= 0:\n                break\n\n        # If no population could be created (very small budget), do pure random search\n        if np.isinf(pop_f).all():\n            while remaining > 0:\n                x = np.random.uniform(lb, ub)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # Main loop: use remaining budget for directional local searches, orthogonal tries, and Lévy jumps\n        # Control params\n        tour_size = 3\n        backtrack_tries = 3\n        orthogonal_tries = 1\n        levy_prob = 0.08\n        recomb_prob = 0.12\n        rejuvenate_prob = 0.02\n\n        # ensure arrays consistent sizes\n        pop = pop[:len(pop_f)]\n        pop_sigma = pop_sigma[:len(pop_f)]\n\n        while remaining > 0:\n            n_pop = len(pop_f)\n            # pick a parent via small tournament to balance exploration/exploitation\n            k = min(tour_size, n_pop)\n            inds = np.random.choice(n_pop, k, replace=False)\n            parent_i = inds[np.argmin(pop_f[inds])]\n            x_parent = pop[parent_i].copy()\n            sigma = pop_sigma[parent_i]\n\n            # sample a random search direction (normalized)\n            d = np.random.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                d = np.ones(self.dim) / np.sqrt(self.dim)\n            else:\n                d = d / nd\n\n            # primary directional trial with stochasticized step-length\n            alpha = sigma * max(1e-12, (1.0 + 0.5 * np.random.randn()))\n            x_try = np.clip(x_parent + alpha * d, lb, ub)\n            f_try = callf(x_try)\n            if f_try < pop_f[parent_i]:\n                # success: accept and slightly increase sigma\n                pop[parent_i] = x_try\n                pop_f[parent_i] = f_try\n                pop_sigma[parent_i] = min((ub - lb).mean(), sigma * 1.12)\n                continue  # continue main loop\n\n            # local backtracking / small-step refinement along direction (few tries)\n            accepted = False\n            for bt in range(backtrack_tries):\n                if remaining <= 0:\n                    break\n                alpha_bt = alpha * (0.5 ** (bt + 1))\n                x_bt = np.clip(x_parent + alpha_bt * d, lb, ub)\n                f_bt = callf(x_bt)\n                if f_bt < pop_f[parent_i]:\n                    pop[parent_i] = x_bt\n                    pop_f[parent_i] = f_bt\n                    pop_sigma[parent_i] = max(1e-12, sigma * 1.08)\n                    accepted = True\n                    break\n            if accepted:\n                continue\n\n            # try an orthogonal perturbation for local diversification\n            for ot in range(orthogonal_tries):\n                if remaining <= 0:\n                    break\n                r = np.random.randn(self.dim)\n                # project r orthogonal to d\n                r = r - np.dot(r, d) * d\n                nr = np.linalg.norm(r)\n                if nr == 0:\n                    continue\n                r = r / nr\n                # small orthogonal step\n                orth_scale = sigma * 0.6 * (0.8 + 0.4 * np.random.rand())\n                x_o = np.clip(x_parent + orth_scale * r, lb, ub)\n                f_o = callf(x_o)\n                if f_o < pop_f[parent_i]:\n                    pop[parent_i] = x_o\n                    pop_f[parent_i] = f_o\n                    pop_sigma[parent_i] = max(1e-12, sigma * 1.05)\n                    accepted = True\n                    break\n            if accepted:\n                continue\n\n            # occasional Lévy-like jump to escape local basins (heavy-tailed steps)\n            if np.random.rand() < levy_prob and remaining > 0:\n                # Cauchy-like heavy-tailed vector (standard_cauchy), robust scale using median sigma\n                step = np.random.standard_cauchy(self.dim)\n                # limit extremes: scale by median sigma and by bound range\n                med_sigma = np.median(pop_sigma)\n                scale = max(1e-12, med_sigma * 1.5)\n                # normalize heavy tail but preserve tail shape\n                step = step / (np.median(np.abs(step)) + 1e-12) * scale\n                x_levy = np.clip(x_parent + step, lb, ub)\n                f_levy = callf(x_levy)\n                # if it's good replace the worst in population, else maybe keep it as candidate\n                worst_i = int(np.argmax(pop_f))\n                if f_levy < pop_f[worst_i]:\n                    pop[worst_i] = x_levy\n                    pop_f[worst_i] = f_levy\n                    pop_sigma[worst_i] = max(1e-12, scale)\n                # continue main loop after jump attempt\n                continue\n\n            # recombination exploitation: mix two best and small noise\n            if np.random.rand() < recomb_prob and remaining > 0:\n                best_inds = np.argsort(pop_f)[:2]\n                a, b = best_inds[0], best_inds[1] if best_inds.size > 1 else best_inds[0]\n                beta = np.random.rand()\n                x_rec = beta * pop[a] + (1 - beta) * pop[b]\n                # add small gaussian perturbation proportional to med sigma\n                med_sigma = np.median(pop_sigma)\n                noise = np.random.randn(self.dim) * (med_sigma * 0.3)\n                x_rec = np.clip(x_rec + noise, lb, ub)\n                f_rec = callf(x_rec)\n                worst_i = int(np.argmax(pop_f))\n                if f_rec < pop_f[worst_i]:\n                    pop[worst_i] = x_rec\n                    pop_f[worst_i] = f_rec\n                    pop_sigma[worst_i] = max(1e-12, med_sigma * 0.8)\n                # if it improves parent, also consider replacing\n                if f_rec < pop_f[parent_i]:\n                    pop[parent_i] = x_rec\n                    pop_f[parent_i] = f_rec\n                    pop_sigma[parent_i] = max(1e-12, pop_sigma[parent_i] * 1.05)\n                continue\n\n            # adapt parent sigma on failure (shrinking to encourage finer search)\n            pop_sigma[parent_i] = max(1e-12, sigma * 0.85)\n\n            # occasional population rejuvenation by replacing the worst with a random sample\n            if np.random.rand() < rejuvenate_prob and remaining > 0:\n                worst_i = int(np.argmax(pop_f))\n                x_new = np.random.uniform(lb, ub)\n                f_new = callf(x_new)\n                if f_new < pop_f[worst_i]:\n                    pop[worst_i] = x_new\n                    pop_f[worst_i] = f_new\n                    pop_sigma[worst_i] = 0.08 * (ub - lb).mean()\n\n            # loop until budget exhausted\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.357 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08768540124488777, 0.14625397679470353, 0.8795584500362162, 0.5206102916740791, 0.23604278286056002, 0.9406254576117097, 0.2219673045001771, 0.24228490940807224, 0.1796147948563971, 0.11288369987186275]}, "task_prompt": ""}
{"id": "94da2886-29cc-4e64-bcdc-dbb3b1636e87", "fitness": 0.36152653513657396, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size directional local searches with orthogonal refinements and occasional Lévy (Cauchy) jumps for robust exploration/exploitation.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n\n    Main idea:\n      Maintain a modest population of candidate points with adaptive step sizes (sigmas).\n      Repeatedly perform randomized directional local searches (with stochastic step-length),\n      small backtracking refinements, orthogonal perturbations for local diversification,\n      occasional heavy-tailed (Cauchy-like) jumps to escape basins, and simple recombination.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # scale pop modestly with dimension but cap to keep evaluations reasonable\n            self.pop_size = max(6, min(40, 3 * self.dim))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = None if seed is None else int(seed)\n        if self.seed is not None:\n            np.random.seed(self.seed)\n\n        # will be set during run\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # setup bounds (func may provide bounds via func.bounds.lb / func.bounds.ub)\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            # default to typical BBOB search range if not provided\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        # Ensure lb/ub broadcast to dim\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb.item()))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub.item()))\n\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        if np.any(span <= 0):\n            raise ValueError(\"Invalid bounds: ub must be > lb elementwise.\")\n\n        # internal helper to evaluate while tracking budget and best\n        def callf(x):\n            # x: array-like\n            if self.eval_count >= self.budget:\n                raise StopIteration  # budget exhausted\n            x_arr = np.asarray(x, dtype=float).reshape(self.dim)\n            x_clipped = np.clip(x_arr, lb, ub)\n            f = func(x_clipped)\n            self.eval_count += 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x_clipped.copy()\n            return float(f), x_clipped\n\n        # handle extremely small budgets: pure random search\n        if self.budget <= 0:\n            return float(self.f_opt), (None if self.x_opt is None else self.x_opt.copy())\n\n        # Build initial population (if budget allows); otherwise do random sampling until budget exhausted\n        pop_size = min(self.pop_size, max(1, self.budget))  # cannot exceed budget\n        pop_x = []\n        pop_f = []\n        pop_sigma = []\n\n        try:\n            # initialize population uniformly in bounds\n            for i in range(pop_size):\n                x0 = np.random.uniform(lb, ub)\n                f0, x0c = callf(x0)\n                pop_x.append(x0c)\n                pop_f.append(f0)\n                # initial sigma proportional to problem scale, randomized a bit\n                init_sigma = 0.2 * np.linalg.norm(span) / np.sqrt(self.dim) * (1.0 + 0.5 * np.random.rand())\n                pop_sigma.append(max(init_sigma, 1e-12))\n        except StopIteration:\n            # budget exhausted during initialization -> return best so far\n            if self.x_opt is None:\n                return float(np.inf), None\n            return float(self.f_opt), self.x_opt.copy()\n\n        pop_x = np.array(pop_x).reshape(len(pop_f), self.dim)\n        pop_f = np.array(pop_f, dtype=float)\n        pop_sigma = np.array(pop_sigma, dtype=float)\n        N = pop_x.shape[0]\n\n        # helper to get worst index\n        def worst_index():\n            return int(np.argmax(pop_f))\n\n        # main loop: continue until budget is exhausted\n        try:\n            while self.eval_count < self.budget:\n                remaining = self.budget - self.eval_count\n                # small tournament (size 3) selection for parent\n                tour = np.random.choice(N, size=min(3, N), replace=False)\n                parent_i = int(tour[np.argmin(pop_f[tour])])\n                x_parent = pop_x[parent_i].copy()\n                f_parent = float(pop_f[parent_i])\n                sigma = float(pop_sigma[parent_i])\n\n                # sample a random direction (normalized)\n                d = np.random.normal(size=self.dim)\n                nd = np.linalg.norm(d)\n                if nd < 1e-12:\n                    d = np.random.uniform(-1, 1, size=self.dim)\n                    nd = np.linalg.norm(d)\n                    if nd < 1e-12:\n                        d = np.ones(self.dim)\n                        nd = np.linalg.norm(d)\n                d = d / nd\n\n                # stochasticized step length: log-normal style multiplicative noise\n                step_len = sigma * np.exp(0.0 + 0.3 * np.random.randn())\n                x_try = x_parent + step_len * d\n                x_try = np.clip(x_try, lb, ub)\n\n                # primary directional trial\n                f_try, x_try = callf(x_try)\n                success = False\n                if f_try < f_parent:\n                    # accept improvement\n                    pop_x[parent_i] = x_try\n                    pop_f[parent_i] = f_try\n                    pop_sigma[parent_i] = min(sigma * 1.15, np.linalg.norm(span) + 1e-12)\n                    success = True\n                else:\n                    # slight sigma shrink\n                    pop_sigma[parent_i] = max(sigma * 0.92, 1e-12)\n\n                    # local backtracking/small-step refinement along direction (few tries)\n                    backtries = 3\n                    back_factor = 0.5\n                    s = step_len\n                    for bt in range(backtries):\n                        s *= back_factor\n                        x_bt = x_parent + s * d\n                        x_bt = np.clip(x_bt, lb, ub)\n                        f_bt, x_bt = callf(x_bt)\n                        if f_bt < pop_f[parent_i]:\n                            pop_x[parent_i] = x_bt\n                            pop_f[parent_i] = f_bt\n                            pop_sigma[parent_i] = max(sigma * (1.0 + 0.05 * np.random.randn()), 1e-12)\n                            success = True\n                            break\n\n                # orthogonal perturbation\n                # build a random vector and make orth to d\n                if self.eval_count < self.budget:\n                    v = np.random.normal(size=self.dim)\n                    # Gram-Schmidt to remove projection on d\n                    v = v - np.dot(v, d) * d\n                    nv = np.linalg.norm(v)\n                    if nv > 1e-12:\n                        v = v / nv\n                        ortho_step = pop_sigma[parent_i] * 0.6 * (0.8 + 0.4 * np.random.rand())\n                        x_ort = pop_x[parent_i] + ortho_step * v\n                        x_ort = np.clip(x_ort, lb, ub)\n                        f_ort, x_ort = callf(x_ort)\n                        if f_ort < pop_f[parent_i]:\n                            pop_x[parent_i] = x_ort\n                            pop_f[parent_i] = f_ort\n                            pop_sigma[parent_i] = min(pop_sigma[parent_i] * 1.08, np.linalg.norm(span) + 1e-12)\n                            success = True\n\n                # occasional Lévy-like (Cauchy) jump to escape local basins\n                if (np.random.rand() < 0.06) and (self.eval_count < self.budget):\n                    # heavy-tailed vector (standard Cauchy components)\n                    c = np.tan(np.pi * (np.random.rand(self.dim) - 0.5))  # Cauchy\n                    # robust scale: median sigma of population or parent sigma\n                    median_sigma = max(np.median(pop_sigma), 1e-12)\n                    # normalize to avoid crazy huge steps but preserve heavy tail\n                    c_norm = np.linalg.norm(c)\n                    if c_norm < 1e-12:\n                        c = np.random.normal(size=self.dim)\n                        c_norm = np.linalg.norm(c)\n                    # scale Cauchy so typical jump is a few times median_sigma times problem scale\n                    scale = median_sigma * (1.0 + 2.0 * np.random.rand())\n                    # cap maximum norm to avoid leaving bounds completely\n                    max_allowed = np.linalg.norm(span) * 2.0\n                    delta = (c / c_norm) * (scale * (1.0 + np.abs(c).mean()))\n                    if np.linalg.norm(delta) > max_allowed:\n                        delta = delta / np.linalg.norm(delta) * max_allowed\n                    x_jump = pop_x[parent_i] + delta\n                    x_jump = np.clip(x_jump, lb, ub)\n                    f_jump, x_jump = callf(x_jump)\n                    if f_jump < pop_f[parent_i]:\n                        pop_x[parent_i] = x_jump\n                        pop_f[parent_i] = f_jump\n                        pop_sigma[parent_i] = max(median_sigma * 0.7, 1e-12)\n                    else:\n                        # maybe replace the worst with this candidate if it's better\n                        wi = worst_index()\n                        if f_jump < pop_f[wi]:\n                            pop_x[wi] = x_jump\n                            pop_f[wi] = f_jump\n                            pop_sigma[wi] = max(median_sigma * 0.5, 1e-12)\n\n                # recombination exploitation: mix two best + small gaussian noise\n                if self.eval_count < self.budget:\n                    best_idx = int(np.argmin(pop_f))\n                    # pick second best different from best\n                    candidates = [i for i in range(N) if i != best_idx]\n                    if candidates:\n                        second_idx = int(min(candidates, key=lambda i: pop_f[i]))\n                    else:\n                        second_idx = best_idx\n                    w = np.random.rand()\n                    child = w * pop_x[best_idx] + (1 - w) * pop_x[second_idx]\n                    child += 0.02 * np.linalg.norm(span) * np.random.randn(self.dim)  # small noise\n                    child = np.clip(child, lb, ub)\n                    f_child, child = callf(child)\n                    if f_child < pop_f[parent_i]:\n                        pop_x[parent_i] = child\n                        pop_f[parent_i] = f_child\n                        pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.9, 1e-12)\n                    else:\n                        # occasionally inject into the population by replacing worst if it's better\n                        wi = worst_index()\n                        if f_child < pop_f[wi]:\n                            pop_x[wi] = child\n                            pop_f[wi] = f_child\n                            pop_sigma[wi] = max(np.median(pop_sigma) * 0.6, 1e-12)\n                        else:\n                            # failure: adapt parent sigma down a bit\n                            pop_sigma[parent_i] = max(pop_sigma[parent_i] * 0.96, 1e-12)\n\n                # occasional population rejuvenation: replace worst with random sample\n                if (np.random.rand() < 0.02) and (self.eval_count < self.budget):\n                    wi = worst_index()\n                    x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = callf(x_new)\n                    if f_new < pop_f[wi]:\n                        pop_x[wi] = x_new\n                        pop_f[wi] = f_new\n                        pop_sigma[wi] = max(0.2 * np.linalg.norm(span) / np.sqrt(self.dim), 1e-12)\n\n                # small safeguard: keep sigma within reasonable bounds\n                pop_sigma = np.clip(pop_sigma, 1e-12, np.linalg.norm(span) * 2.0)\n\n                # optional short-circuit: if we hit very good solution (close to 0), we might break early\n                # but we don't assume known target; leave loop to exhaust budget or natural termination\n\n            # end while\n        except StopIteration:\n            # budget exhausted inside loop, normal finish\n            pass\n\n        # return best found\n        if self.x_opt is None:\n            return float(np.inf), None\n        return float(self.f_opt), self.x_opt.copy()", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.362 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1418519521830216, 0.21480786044033928, 0.7548358492471813, 0.3722911323728638, 0.2574255778041664, 0.798938994661004, 0.2816898691927048, 0.33987871311208173, 0.2858380398218794, 0.16770736253049767]}, "task_prompt": ""}
{"id": "63ebc2c1-8431-4049-a00d-4b27bf77d1f3", "fitness": 0.5526267832893591, "name": "ADLS", "description": "Adaptive Directional Lévy Search (ADLS) — population-based adaptive step-size search combining randomized directional local searches, orthogonal refinements, and occasional Lévy-like jumps to balance local exploitation and global exploration.", "code": "import numpy as np\n\nclass ADLS:\n    \"\"\"\n    Adaptive Directional Lévy Search (ADLS)\n    - budget: total number of function evaluations allowed\n    - dim: problem dimensionality\n    - pop_size: optional population size (auto-chosen if None)\n    - seed: optional RNG seed\n    Notes:\n      - func must expose func.bounds.lb and func.bounds.ub (arrays or scalars).\n      - All evaluations are clipped to bounds before calling func.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_size=None, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if pop_size is None:\n            # modest population that scales with dimension but stays reasonable\n            self.pop_size = max(4, min(20, 4 + int(self.dim * 0.8)))\n        else:\n            self.pop_size = int(pop_size)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        # internal bookkeeping\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        # ensure bounds are numpy arrays of correct dim\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            # default bounds for BBOB-like tasks\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        if lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        assert lb.size == self.dim and ub.size == self.dim, \"Bounds must match dimension\"\n\n        # helper to call func while tracking budget and best\n        def callf(x):\n            # x numpy array shape (dim,)\n            x_clip = np.minimum(np.maximum(x, lb), ub)\n            if self.evals >= self.budget:\n                raise RuntimeError(\"Budget exhausted\")\n            f = func(x_clip)\n            self.evals += 1\n            # update global best\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x_clip.copy()\n            return float(f), x_clip\n\n        # fallback to pure random search if budget too small to initialize population\n        if self.budget <= 2:\n            # trivial random search\n            while self.evals < self.budget:\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # initialize population: sample uniformly across bounds\n        pop = []\n        init_pop_size = min(self.pop_size, max(1, self.budget // 3))\n        for _ in range(init_pop_size):\n            if self.evals >= self.budget:\n                break\n            x0 = lb + self.rng.rand(self.dim) * (ub - lb)\n            f0, x0 = callf(x0)\n            # initialize step-size sigma relative to bound size and dimension\n            base_sigma = 0.1 * np.linalg.norm(ub - lb) / np.sqrt(self.dim)\n            sigma = base_sigma * (0.5 + self.rng.rand())\n            pop.append({'x': x0, 'f': f0, 'sigma': float(sigma)})\n\n        # if no population created (very tiny budget), continue random search\n        if len(pop) == 0:\n            while self.evals < self.budget:\n                x = lb + self.rng.rand(self.dim) * (ub - lb)\n                callf(x)\n            return self.f_opt, self.x_opt\n\n        # ensure we have at least two individuals for recombination\n        while len(pop) < 2 and self.evals < self.budget:\n            x0 = lb + self.rng.rand(self.dim) * (ub - lb)\n            f0, x0 = callf(x0)\n            sigma = 0.1 * np.linalg.norm(ub - lb) / np.sqrt(self.dim)\n            pop.append({'x': x0, 'f': f0, 'sigma': float(sigma)})\n\n        # helper to get index of worst and best\n        def idx_best():\n            return int(np.argmin([p['f'] for p in pop]))\n        def idx_worst():\n            return int(np.argmax([p['f'] for p in pop]))\n\n        # main loop\n        # hyperparameters\n        tourn_size = 3\n        backtrack_tries = 3\n        orthogonal_scale = 0.6\n        levy_prob = 0.03  # chance of performing a Lévy jump each iteration\n        levy_scale_factor = 3.0\n        sigma_increase = 1.12\n        sigma_decrease = 0.9\n        sigma_min = 1e-8\n        sigma_max = (np.linalg.norm(ub - lb) + 1e-9)\n\n        # small bank to detect stagnation: track best improvements over last K evals\n        recent_best = [self.f_opt]\n        recent_len = 20\n\n        # loop while budget remains\n        while self.evals < self.budget:\n            # pick parent via small tournament\n            t_inds = self.rng.randint(0, len(pop), size=min(tourn_size, len(pop)))\n            parent_idx = t_inds[np.argmin([pop[i]['f'] for i in t_inds])]\n            parent = pop[parent_idx]\n            x_parent = parent['x']\n            sigma = parent['sigma']\n\n            # sample a random search direction (normalized)\n            d = self.rng.randn(self.dim)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                d = np.ones(self.dim)\n                nd = np.linalg.norm(d)\n            d = d / nd\n\n            # primary directional trial with stochasticized step-length (multiplicative noise)\n            # ensure step positive\n            step = sigma * np.exp(0.2 * self.rng.randn())\n            x_trial = x_parent + step * d\n            if self.evals < self.budget:\n                f_trial, x_trial = callf(x_trial)\n            else:\n                break\n\n            improved = False\n            if f_trial < parent['f']:\n                # accept\n                parent['x'] = x_trial\n                parent['f'] = f_trial\n                parent['sigma'] = min(sigma_max, sigma * sigma_increase)\n                improved = True\n            else:\n                # small backtracking / refinement: reduce step size a few times\n                reduced_step = step\n                for bt in range(backtrack_tries):\n                    reduced_step *= 0.5\n                    x_bt = x_parent + reduced_step * d\n                    if self.evals >= self.budget:\n                        break\n                    f_bt, x_bt = callf(x_bt)\n                    if f_bt < parent['f']:\n                        parent['x'] = x_bt\n                        parent['f'] = f_bt\n                        parent['sigma'] = min(sigma_max, sigma * (1.0 + 0.5*(bt+1)/backtrack_tries))\n                        improved = True\n                        break\n\n            # orthogonal perturbation for local diversification\n            if self.evals < self.budget:\n                # generate random vector and make orthogonal to d\n                r = self.rng.randn(self.dim)\n                r -= d * np.dot(r, d)  # remove component along d\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    r = r / nr\n                    orth_step = sigma * orthogonal_scale * (0.5 + self.rng.rand())\n                    x_orth = parent['x'] + orth_step * r\n                    f_orth, x_orth = callf(x_orth)\n                    if f_orth < parent['f']:\n                        parent['x'] = x_orth\n                        parent['f'] = f_orth\n                        parent['sigma'] = min(sigma_max, parent['sigma'] * (1.0 + 0.3))\n                        improved = True\n\n            # occasional Lévy-like jump to escape basins\n            if self.rng.rand() < levy_prob and self.evals < self.budget:\n                # multivariate Cauchy-like: sample Cauchy per-dimension, normalize to unit vector\n                c = self.rng.standard_cauchy(size=self.dim)\n                # robust scaling: divide by median absolute value to avoid one extreme dimension dominating\n                mad = np.median(np.abs(c)) + 1e-9\n                c = c / mad\n                nc = np.linalg.norm(c)\n                if nc == 0:\n                    c = self.rng.randn(self.dim)\n                    nc = np.linalg.norm(c)\n                c = c / nc\n                # heavy-tailed magnitude\n                magnitude = levy_scale_factor * sigma * (1.0 + abs(self.rng.standard_cauchy()))\n                magnitude = min(magnitude, 10.0 * sigma_max)\n                x_levy = parent['x'] + magnitude * c\n                # normalize/clip and evaluate\n                if self.evals < self.budget:\n                    f_levy, x_levy = callf(x_levy)\n                    if f_levy < parent['f']:\n                        # replace the worst in population if promising\n                        widx = idx_worst()\n                        if f_levy < pop[widx]['f']:\n                            pop[widx] = {'x': x_levy.copy(), 'f': f_levy, 'sigma': max(sigma_min, sigma * 0.8)}\n                        else:\n                            # keep as a candidate by replacing parent\n                            parent['x'] = x_levy\n                            parent['f'] = f_levy\n                            parent['sigma'] = max(sigma_min, sigma * 0.9)\n                            improved = True\n                    # continue main loop\n                    continue\n\n            # recombination exploitation: mix two best with small noise\n            if len(pop) >= 2 and self.evals < self.budget:\n                # pick two from top fraction\n                sorted_idx = np.argsort([p['f'] for p in pop])\n                topk = max(2, min(len(pop), int(0.3 * len(pop)) + 1))\n                a_idx = sorted_idx[0]\n                b_idx = sorted_idx[min(1, topk-1)]\n                a = pop[a_idx]\n                b = pop[b_idx]\n                alpha = 0.5 + 0.2 * (self.rng.rand() - 0.5)\n                child = alpha * a['x'] + (1.0 - alpha) * b['x']\n                # small Gaussian perturbation scaled by mean sigma\n                mean_sigma = 0.5 * (a['sigma'] + b['sigma'])\n                child += mean_sigma * 0.3 * self.rng.randn(self.dim)\n                if self.evals < self.budget:\n                    f_child, child = callf(child)\n                    if f_child < parent['f']:\n                        # replace parent\n                        parent['x'] = child\n                        parent['f'] = f_child\n                        parent['sigma'] = min(sigma_max, mean_sigma * 1.0)\n                        improved = True\n                    else:\n                        # maybe replace worst in population\n                        widx = idx_worst()\n                        if f_child < pop[widx]['f']:\n                            pop[widx] = {'x': child.copy(), 'f': f_child, 'sigma': max(sigma_min, mean_sigma * 0.8)}\n\n            # adapt parent sigma on failure\n            if not improved:\n                parent['sigma'] = max(sigma_min, parent['sigma'] * sigma_decrease)\n\n            # occasional population rejuvenation: replace worst with a fresh random sample\n            if self.rng.rand() < 0.02 and self.evals < self.budget:\n                widx = idx_worst()\n                x_new = lb + self.rng.rand(self.dim) * (ub - lb)\n                f_new, x_new = callf(x_new)\n                pop[widx] = {'x': x_new.copy(), 'f': f_new, 'sigma': 0.1 * np.linalg.norm(ub - lb) / np.sqrt(self.dim)}\n\n            # keep track of recent improvement to possibly increase levy probability on stagnation\n            recent_best.append(self.f_opt)\n            if len(recent_best) > recent_len:\n                recent_best.pop(0)\n            # simple stagnation measure: if no improvement in recent window, slightly increase levy chance\n            if len(recent_best) == recent_len and min(recent_best) >= recent_best[0]:\n                levy_prob = min(0.2, levy_prob * 1.05)\n            else:\n                # slowly decay to default\n                levy_prob = max(0.01, levy_prob * 0.995)\n\n            # ensure population remains consistent if parent was modified\n            pop[parent_idx] = parent\n\n        # finished budget\n        # final best is tracked in self.f_opt and self.x_opt\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADLS scored 0.553 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1378821855953547, 0.15866138281363762, 0.7445332094161623, 0.8769296342532447, 0.787326056009121, 0.8227638605204474, 0.2921918921483624, 0.761676852253094, 0.8026516139940195, 0.14165114589014838]}, "task_prompt": ""}

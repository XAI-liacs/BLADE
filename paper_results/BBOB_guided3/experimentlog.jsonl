{"method_name": "ES-guided-new", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided-new", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true, "feature_guided_mutation": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "adbb429f-776c-49b4-91f7-89b67d549230", "fitness": 0.7817199299013535, "name": "ESEADL_DenseGraph", "description": "ESEADL-DenseGraph — an entropy‑scaffolded eccentric Lévy search that actively learns and densifies a coordinate-interaction graph to propose connected subspace probes, adjacency-aware recombination and spectral subgraph moves for richer multi-variable exploration.", "code": "import numpy as np\n\nclass ESEADL_DenseGraph:\n    \"\"\"\n    ESEADL-DenseGraph\n\n    Enhances ESEADL by maintaining and densifying a learned coordinate-interaction\n    adjacency matrix (edge-density boosting). Uses that graph to sample connected\n    subspaces (random-walk/subgraph), adjacency-aware recombination, and spectral\n    subgraph eigenmoves, while keeping the original eccentricity/degree-entropy\n    scaffolding and diversified operator ensemble.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_frac=0.12, base_scale=0.12, max_subspace=8,\n                 ecc_power=1.6, deg_entropy_window=300, entropy_influence=2.0,\n                 min_step=1e-10,\n                 graph_update_freq=20, graph_smooth=0.25, graph_target_density=0.35,\n                 graph_add_frac=0.02, edge_sample_window=300):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.init_frac = float(init_frac)\n        self.base_scale = float(base_scale)\n        self.max_subspace = min(int(max_subspace), self.dim)\n        self.ecc_power = float(ecc_power)\n        self.deg_entropy_window = int(deg_entropy_window)\n        self.entropy_influence = float(entropy_influence)\n        self.min_step = float(min_step)\n\n        # graph parameters\n        self.graph_update_freq = int(graph_update_freq)\n        self.graph_smooth = float(graph_smooth)\n        self.graph_target_density = float(graph_target_density)\n        self.graph_add_frac = float(graph_add_frac)\n        self.edge_sample_window = int(edge_sample_window)\n\n        # best found\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        if hasattr(func, \"bounds\"):\n            try:\n                lb = np.asarray(func.bounds.lb, dtype=float)\n                ub = np.asarray(func.bounds.ub, dtype=float)\n                if lb.size == 1:\n                    lb = np.full(self.dim, float(lb))\n                    ub = np.full(self.dim, float(ub))\n                return lb.reshape(self.dim), ub.reshape(self.dim)\n            except Exception:\n                pass\n        return np.full(self.dim, -5.0), np.full(self.dim, 5.0)\n\n    def _clip(self, x, lb, ub):\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def _lhs(self, n, lb, ub):\n        d = self.dim\n        rng = self.rng\n        cut = np.linspace(0, 1, n + 1)\n        u = rng.random((n, d))\n        pts = np.zeros((n, d))\n        for j in range(d):\n            a = cut[:n]\n            b = cut[1:n+1]\n            pts[:, j] = a + (b - a) * u[:, j]\n            rng.shuffle(pts[:, j])\n        return lb + pts * (ub - lb)\n\n    def _eval(self, x, func, lb, ub, evals, archive_x, archive_f):\n        if evals >= self.budget:\n            return evals, False, None, None\n        x = np.array(x, dtype=float)\n        x = self._clip(x, lb, ub)\n        f = float(func(x))\n        evals += 1\n        archive_x.append(x.copy())\n        archive_f.append(f)\n        improved = False\n        if f < self.f_opt:\n            self.f_opt = f\n            self.x_opt = x.copy()\n            improved = True\n        return evals, improved, x.copy(), f\n\n    def _eccentricity(self, archive_x, archive_f):\n        d = self.dim\n        if len(archive_x) < max(4, d//2):\n            return np.ones(d) / d\n        X = np.vstack(archive_x)\n        farr = np.asarray(archive_f)\n        # focus on best portion but ensure enough samples\n        k = min(len(farr), max(6, d + 2))\n        idx = np.argsort(farr)[:k]\n        Xk = X[idx]\n        med = np.median(Xk, axis=0)\n        mad = np.median(np.abs(Xk - med), axis=0) + 1e-12\n        nad = np.mean(np.abs(Xk - med), axis=0) / mad\n        q75 = np.percentile(Xk, 75, axis=0)\n        q25 = np.percentile(Xk, 25, axis=0)\n        iqr = (q75 - q25) + 1e-12\n        tail = np.mean(np.abs(Xk - med) > 1.5 * iqr, axis=0)\n        ecc = nad + 0.9 * tail\n        ecc = np.maximum(ecc, 1e-12)\n        ecc = ecc / np.mean(ecc)\n        ecc = ecc ** self.ecc_power\n        ecc = ecc / ecc.sum()\n        return ecc\n\n    def _select_coords(self, probs, k):\n        k = max(1, min(self.dim, int(k)))\n        if k >= self.dim:\n            return np.arange(self.dim)\n        return self.rng.choice(self.dim, size=k, replace=False, p=probs)\n\n    def _fit_quadratic(self, center, U, local_points, local_f, ridge=1e-8):\n        s = U.shape[1]\n        if s == 0:\n            return None\n        Z = (local_points - center) @ U\n        n = Z.shape[0]\n        needed = 1 + s + s + (s*(s-1))//2\n        if n < max(needed, s+2):\n            return None\n        feats = []\n        for i in range(n):\n            zi = Z[i]\n            row = [1.0]\n            row.extend(zi.tolist())\n            row.extend((zi**2).tolist())\n            for a in range(s):\n                for b in range(a+1, s):\n                    row.append(zi[a]*zi[b])\n            feats.append(row)\n        A = np.asarray(feats, dtype=float)\n        y = np.asarray(local_f, dtype=float).reshape(-1, 1)\n        ATA = A.T @ A + ridge * np.eye(A.shape[1])\n        ATy = A.T @ y\n        try:\n            coeffs = np.linalg.solve(ATA, ATy).flatten()\n        except np.linalg.LinAlgError:\n            return None\n        c = coeffs[0]\n        linear = coeffs[1:1+s]\n        sq = coeffs[1+s:1+2*s]\n        cross = coeffs[1+2*s:]\n        H = np.zeros((s, s))\n        for i in range(s):\n            H[i,i] = 2.0 * sq[i]\n        idx = 0\n        for a in range(s):\n            for b in range(a+1, s):\n                H[a,b] = cross[idx]\n                H[b,a] = cross[idx]\n                idx += 1\n        g = linear.copy()\n        return {'c': c, 'g': g, 'H': H, 'U': U, 'center': center}\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        domain_width = ub - lb\n        self.max_step = 0.6 * np.linalg.norm(domain_width)\n        budget = self.budget\n        evals = 0\n\n        # Initialization: LHS\n        n_init = max(8, int(min(max(2*self.dim, 8), max(1, int(budget * self.init_frac)))))\n        n_init = min(n_init, max(1, budget // 6))\n        archive_x = []\n        archive_f = []\n        for x in self._lhs(n_init, lb, ub):\n            if evals >= budget:\n                break\n            f = float(func(x))\n            evals += 1\n            archive_x.append(np.array(x, dtype=float))\n            archive_f.append(f)\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = np.array(x, dtype=float)\n        if len(archive_x) == 0 and evals < budget:\n            x0 = self.rng.uniform(lb, ub)\n            f0 = float(func(x0))\n            evals += 1\n            archive_x.append(x0.copy())\n            archive_f.append(f0)\n            self.f_opt = f0\n            self.x_opt = x0.copy()\n\n        x_center = self.x_opt.copy()\n        mean_domain = np.mean(domain_width)\n        per_dim_trust = np.full(self.dim, self.base_scale * mean_domain)\n        per_dim_trust = np.clip(per_dim_trust, self.min_step, self.max_step)\n\n        # operators: reuse ESEADL ops + graph-aware: 8 subgraph recomb, 9 spectral-subgraph move, 10 densify-edge exploration\n        n_ops = 11\n        op_success = np.ones(n_ops) * 1e-3\n        op_tries = np.ones(n_ops) * 1e-3\n\n        # degree stats\n        max_deg = min(self.max_subspace, self.dim)\n        deg_counts = np.zeros(self.dim + 1, dtype=float)\n        recent_degs = []\n        deg_bonus = np.ones(self.dim + 1, dtype=float)\n\n        no_improve = 0\n        max_no_improve = max(12, 8 + self.dim // 2)\n\n        # adjacency / graph learning\n        adj = np.zeros((self.dim, self.dim), dtype=float)  # symmetric weights\n        edge_counts = np.zeros((self.dim, self.dim), dtype=float)\n        graph_updates = 0\n\n        # helper: record degree with decaying bonus\n        def record_degree(k):\n            k = int(max(1, min(self.dim, k)))\n            deg_counts[k] += 1.0\n            recent_degs.append(k)\n            if len(recent_degs) > self.deg_entropy_window:\n                old = recent_degs.pop(0)\n                deg_counts[old] = max(0.0, deg_counts[old] - 1.0)\n            # refresh per-degree bonus (UCB-style + recency)\n            total = max(1.0, deg_counts[1:max_deg+1].sum())\n            for d in range(1, max_deg+1):\n                freq = deg_counts[d] + 1e-6\n                self_age = min(1.0, (1.0 + (total / freq))**0.5)\n                deg_bonus[d] = (1.0 / freq) ** (0.6) * self_age\n\n        # degree selection probabilities\n        def degree_probs():\n            counts = deg_counts[1:max_deg+1] + 1e-6\n            inv = (1.0 / counts)\n            raw = (inv ** (0.6)) * deg_bonus[1:max_deg+1]\n            p = raw / raw.sum()\n            return p\n\n        # Graph update: infer coordinate interactions from top samples and recent deltas\n        def _update_graph(archive_x, archive_f):\n            nonlocal adj, graph_updates, edge_counts\n            graph_updates += 1\n            X = np.vstack(archive_x)\n            farr = np.asarray(archive_f)\n            n = X.shape[0]\n            # use top-k and also recent pairs for edge signal\n            k = min(max(6, self.dim+2), n)\n            idx_top = np.argsort(farr)[:k]\n            Xk = X[idx_top]\n            # compute absolute Pearson correlation among columns\n            try:\n                C = np.corrcoef(Xk, rowvar=False)\n                C = np.nan_to_num(C)\n                Anew = np.abs(C)\n            except Exception:\n                Anew = np.abs(np.cov(Xk.T))\n                Anew = Anew / (np.max(Anew) + 1e-12)\n            # additionally use co-variation from recent differences\n            if n >= 3:\n                m = min(n-1, 50)\n                diffs = X[-m:] - X[-m-1:-1]\n                if diffs.shape[0] >= 2:\n                    co = np.abs(np.corrcoef(diffs, rowvar=False))\n                    co = np.nan_to_num(co)\n                    Anew = 0.6*Anew + 0.4*co\n            # normalize\n            Anew = Anew - np.diag(np.diag(Anew))\n            maxv = np.max(Anew) + 1e-12\n            Anew = Anew / maxv\n            # smooth into adj\n            adj = (1.0 - self.graph_smooth) * adj + self.graph_smooth * Anew\n            # symmetrize\n            adj = 0.5 * (adj + adj.T)\n            # increase edge counts when edges used in recent archive moves (heuristic)\n            # detect coordinate pairs that moved together often: use sign concordance of diffs\n            if n >= 4:\n                recent = X[-min(n, self.edge_sample_window):]\n                rd = np.diff(recent, axis=0)\n                sign = np.sign(rd)\n                concord = np.abs(sign.T @ sign)  # high when signs align often\n                concord = concord / (np.max(concord) + 1e-12)\n                edge_counts = 0.9*edge_counts + 0.1*concord\n            # densify towards target density by adding top missing edges gradually\n            current_density = (np.sum(adj > 1e-3) - self.dim) / (self.dim*(self.dim-1))\n            target = self.graph_target_density\n            if current_density < target:\n                # add some highest-weight but missing edges\n                tri_idx = np.triu_indices(self.dim, k=1)\n                weights = adj[tri_idx]\n                order = np.argsort(weights)[::-1]\n                to_add = max(1, int(self.graph_add_frac * len(order)))\n                add_idx = order[:to_add]\n                for ai in add_idx:\n                    i = tri_idx[0][ai]; j = tri_idx[1][ai]\n                    # nudge the weight upward proportionally to combined edge_counts\n                    boost = 0.02 + 0.08 * (edge_counts[i,j] if edge_counts[i,j]>0 else 1.0)\n                    adj[i,j] = max(adj[i,j], min(1.0, adj[i,j] + boost * (1.0 - adj[i,j])))\n                    adj[j,i] = adj[i,j]\n            # ensure no negatives and diag zero\n            adj = np.clip(adj, 0.0, 1.0)\n            np.fill_diagonal(adj, 0.0)\n\n        def sample_connected_subgraph(seed=None, size=3):\n            # sample a connected subgraph via weighted random walk on adj\n            if size >= self.dim:\n                return np.arange(self.dim)\n            if seed is None:\n                seed = int(self.rng.integers(0, self.dim))\n            nodes = [seed]\n            frontier = set([seed])\n            while len(nodes) < size:\n                cur = nodes[-1]\n                neigh_weights = adj[cur].copy()\n                # ensure exploration possibility: if sparse, allow random pick\n                if np.sum(neigh_weights) < 1e-6:\n                    cand = int(self.rng.integers(0, self.dim))\n                    if cand not in nodes:\n                        nodes.append(cand)\n                    else:\n                        # random neighbor by probability proportional to eccentricity fallback\n                        cand = int(self.rng.integers(0, self.dim))\n                        if cand not in nodes:\n                            nodes.append(cand)\n                    continue\n                neigh_weights = neigh_weights / (neigh_weights.sum() + 1e-12)\n                # sample a neighbor not already in nodes with some attempts\n                attempts = 0\n                pick = None\n                while attempts < 5:\n                    pick = int(self.rng.choice(self.dim, p=neigh_weights))\n                    if pick not in nodes:\n                        break\n                    attempts += 1\n                    neigh_weights[pick] = 0.0\n                    s = neigh_weights.sum()\n                    if s < 1e-8:\n                        break\n                    neigh_weights = neigh_weights / (s + 1e-12)\n                if pick is None or pick in nodes:\n                    # fallback: pick random unused node\n                    choices = [i for i in range(self.dim) if i not in nodes]\n                    if not choices:\n                        break\n                    nodes.append(int(self.rng.choice(choices)))\n                else:\n                    nodes.append(pick)\n            return np.array(nodes, dtype=int)\n\n        # main loop\n        loop_iter = 0\n        while evals < budget:\n            loop_iter += 1\n            ecc = self._eccentricity(archive_x, archive_f)\n            X = np.vstack(archive_x)\n            farr = np.asarray(archive_f)\n            topk = min(len(farr), max(self.dim + 1, 4))\n            top_idx = np.argsort(farr)[:topk]\n            Xk = X[top_idx]\n            mean_top = np.mean(Xk, axis=0)\n            W = np.sqrt(ecc + 1e-12)\n            Xw = (Xk - mean_top) * W[np.newaxis, :]\n            cov = np.cov(Xw.T) + 1e-8 * np.eye(self.dim)\n            try:\n                eigvals, eigvecs = np.linalg.eigh(cov)\n                order = np.argsort(eigvals)[::-1]\n                eigvals = eigvals[order]\n                eigvecs = eigvecs[:, order]\n            except Exception:\n                Q, _ = np.linalg.qr(self.rng.normal(size=(self.dim, self.dim)))\n                eigvecs = Q\n                eigvals = np.ones(self.dim)\n\n            # update graph periodically\n            if (loop_iter % max(1, self.graph_update_freq)) == 0:\n                try:\n                    _update_graph(archive_x, archive_f)\n                except Exception:\n                    pass\n\n            # degree distribution for sampling\n            p_deg = degree_probs()\n\n            # operator selection using success ratios + entropy-driven bonus\n            op_rate = (op_success + 1e-8) / (op_tries + 1e-8)\n            if recent_degs:\n                hist = np.bincount(np.array(recent_degs), minlength=self.dim+1)[1:max_deg+1]\n                p_hist = (hist + 1e-6) / (hist + 1e-6).sum()\n                deg_entropy = -np.sum(p_hist * np.log(p_hist + 1e-12))\n                entropy_gap = max(0.0, np.log(max_deg) - deg_entropy)\n            else:\n                entropy_gap = 0.0\n            op_bonus = np.ones(n_ops)\n            # give graph-aware ops a bonus proportional to missing density\n            current_density = (np.sum(adj > 1e-3) - self.dim) / (self.dim*(self.dim-1) + 1e-12)\n            missing = max(0.0, self.graph_target_density - current_density)\n            # ops that touch adjacency: indices 8,9,10\n            op_bonus[[8,9,10]] += 1.6 * (missing)\n            # also keep entropy influenced ops as before\n            op_bonus[[0,3,5,6,7]] += self.entropy_influence * (entropy_gap / (1.0 + np.log(1+max_deg)))\n            ucb = np.sqrt(np.log(1.0 + op_tries.sum()) / (op_tries + 1e-12))\n            scores = op_rate * op_bonus + 0.12 * ucb\n            probs = np.maximum(scores, 1e-12)\n            probs = probs / probs.sum()\n            op = int(self.rng.choice(np.arange(n_ops), p=probs))\n\n            improved = False\n\n            # 0: sparse parabolic probes (same idea)\n            if op == 0:\n                deg_i = int(self.rng.choice(np.arange(1, max_deg+1), p=p_deg))\n                record_degree(deg_i)\n                # pick coords by eccentricity\n                ecc_probs = ecc / ecc.sum()\n                coords = self._select_coords(ecc_probs, deg_i)\n                for c in coords:\n                    if evals >= budget:\n                        break\n                    s0 = per_dim_trust[c] * (0.9 + 0.6 * self.rng.random())\n                    probes = []\n                    for mult in (0.6, 1.0, 1.6):\n                        if evals >= budget:\n                            break\n                        s = s0 * mult\n                        x_p = x_center.copy(); x_p[c] += s\n                        evals, imp, xp, fp = self._eval(x_p, func, lb, ub, evals, archive_x, archive_f)\n                        if imp:\n                            x_center = xp.copy(); per_dim_trust[c] = min(self.max_step, per_dim_trust[c]*1.18); improved=True; break\n                        probes.append((s, fp))\n                        if evals >= budget:\n                            break\n                        x_m = x_center.copy(); x_m[c] -= s\n                        evals, imp2, xp2, fp2 = self._eval(x_m, func, lb, ub, evals, archive_x, archive_f)\n                        if imp2:\n                            x_center = xp2.copy(); per_dim_trust[c] = min(self.max_step, per_dim_trust[c]*1.18); improved=True; break\n                        probes.append((-s, fp2))\n                    if (not improved) and len(probes) >= 3 and evals < budget:\n                        try:\n                            xs = np.array([p[0] for p in probes[:3]])\n                            ys = np.array([p[1] for p in probes[:3]])\n                            A = np.vstack([xs**2, xs, np.ones_like(xs)]).T\n                            coeffs = np.linalg.lstsq(A, ys, rcond=None)[0]\n                            a, b, _ = coeffs\n                            if abs(a) > 1e-16:\n                                opt = -b / (2*a)\n                                opt = np.clip(opt, -3*s0, 3*s0)\n                                x_try = x_center.copy(); x_try[c] += opt\n                                evals, imp3, xp3, fp3 = self._eval(x_try, func, lb, ub, evals, archive_x, archive_f)\n                                if imp3:\n                                    x_center = xp3.copy(); per_dim_trust[c] = min(self.max_step, per_dim_trust[c]*1.22); improved=True\n                        except Exception:\n                            pass\n                if (not improved) and len(coords) > 1 and evals < budget:\n                    avg_tr = np.mean(per_dim_trust[coords])\n                    s = avg_tr * (0.6 + self.rng.random()*1.2)\n                    for sign in (+1.0, -1.0):\n                        x_d = x_center.copy(); x_d[coords] += sign * s\n                        evals, imp4, xp4, fp4 = self._eval(x_d, func, lb, ub, evals, archive_x, archive_f)\n                        if imp4:\n                            x_center = xp4.copy(); per_dim_trust[coords] = np.minimum(self.max_step, per_dim_trust[coords]*1.14); improved=True; break\n\n            # 1: eigen-direction probes\n            elif op == 1:\n                n_dirs = min(self.dim, max(1, int(np.sum(eigvals > 1e-12))))\n                n_try_dirs = min(5, n_dirs)\n                for k_dir in range(n_try_dirs):\n                    if evals >= budget:\n                        break\n                    v = eigvecs[:, k_dir]\n                    v = v / (np.linalg.norm(v) + 1e-12)\n                    weight = max(0.4, np.dot(np.abs(v), ecc))\n                    base = np.mean(per_dim_trust)*(0.8 + 1.4*(eigvals[k_dir]/(eigvals[0]+1e-12)))\n                    for mult in (0.5, 1.0, 1.6):\n                        if evals >= budget:\n                            break\n                        step = base * mult * (0.8 + 0.6*weight)\n                        for sign in (+1.0, -1.0):\n                            x_try = x_center + sign * step * v\n                            evals, imp, xp, fp = self._eval(x_try, func, lb, ub, evals, archive_x, archive_f)\n                            if imp:\n                                x_center = xp.copy(); per_dim_trust = np.minimum(self.max_step, per_dim_trust*1.08); improved=True; break\n                        if improved:\n                            break\n                    if improved:\n                        break\n\n            # 2: differential recombination (adjacency aware)\n            elif op == 2:\n                if len(archive_x) >= 4:\n                    m = min(len(archive_x), 6)\n                    ids = self.rng.choice(len(archive_x), size=m, replace=False)\n                    elites = np.vstack([archive_x[i] for i in ids])\n                    diffs = elites - elites.mean(axis=0)\n                    # adjacency-aware mask: sample connected small subgraph and apply mask\n                    seed = int(self.rng.integers(0, self.dim))\n                    deg = int(self.rng.choice(np.arange(1, max_deg+1), p=p_deg))\n                    mask_nodes = sample_connected_subgraph(seed=seed, size=deg)\n                    mask_prob = np.zeros(self.dim)\n                    mask_prob[mask_nodes] = 1.0\n                    mask = mask_prob\n                    F = 0.35 + 0.9 * self.rng.random()\n                    cand = x_center.copy()\n                    for i in range(len(diffs)-1):\n                        w = self.rng.random()\n                        d = diffs[i] - diffs[(i+1) % len(diffs)]\n                        cand += F * w * d * (0.1 + 1.9 * mask)\n                    cand = self._clip(cand, lb, ub)\n                    evals, imp, xp, fp = self._eval(cand, func, lb, ub, evals, archive_x, archive_f)\n                    if imp:\n                        x_center = xp.copy(); per_dim_trust = np.minimum(self.max_step, per_dim_trust*1.10); improved=True\n                    else:\n                        noise = self.rng.normal(scale=0.25, size=self.dim) * (0.2 + 2.0*ecc)\n                        x_try = self._clip(x_center + noise * per_dim_trust, lb, ub)\n                        if evals < budget:\n                            evals, imp2, xp2, fp2 = self._eval(x_try, func, lb, ub, evals, archive_x, archive_f)\n                            if imp2:\n                                x_center = xp2.copy(); per_dim_trust = np.minimum(self.max_step, per_dim_trust*1.06); improved=True\n                else:\n                    noise = self.rng.normal(scale=0.25, size=self.dim) * (0.2 + 2.0*ecc)\n                    x_try = self._clip(x_center + noise * per_dim_trust, lb, ub)\n                    evals, imp, xp, fp = self._eval(x_try, func, lb, ub, evals, archive_x, archive_f)\n                    if imp:\n                        x_center = xp.copy(); improved=True\n\n            # 3: quadratic subspace fit\n            elif op == 3:\n                deg_i = int(self.rng.choice(np.arange(1, max_deg+1), p=p_deg))\n                record_degree(deg_i)\n                sorted_idx = np.argsort(-ecc)\n                # prefer coordinates that are strongly connected in adj (edge density)\n                conn_strength = np.sum(adj, axis=0)\n                # combine eccentricity and connectivity\n                score = 0.6*ecc + 0.4*(conn_strength / (conn_strength.sum()+1e-12))\n                coords = np.argsort(-score)[:deg_i]\n                U = np.zeros((self.dim, len(coords)))\n                for i,c in enumerate(coords):\n                    U[c,i] = 1.0\n                Xall = np.vstack(archive_x)\n                dists = np.linalg.norm(Xall - x_center, axis=1)\n                order = np.argsort(dists)\n                n_needed = max(6, 2*len(coords)+3)\n                sel = order[:min(len(order), max(n_needed, len(coords)+4))]\n                local_points = Xall[sel]\n                local_f = [archive_f[i] for i in sel]\n                model = self._fit_quadratic(x_center, U, local_points, local_f)\n                if model is not None:\n                    H = model['H']; g = model['g']\n                    try:\n                        H_reg = H + 1e-6 * np.eye(H.shape[0])\n                        z_star = -np.linalg.solve(H_reg, g)\n                        proj_trust = np.linalg.norm(per_dim_trust[coords])\n                        max_norm = min(self.max_step, 3.0 * proj_trust)\n                        if np.linalg.norm(z_star) > max_norm:\n                            z_star = z_star / (np.linalg.norm(z_star) + 1e-12) * max_norm\n                        x_pred = model['center'] + model['U'] @ z_star\n                        evals, imp, xp, fp = self._eval(x_pred, func, lb, ub, evals, archive_x, archive_f)\n                        if imp:\n                            x_center = xp.copy(); per_dim_trust[coords] = np.minimum(self.max_step, per_dim_trust[coords]*1.17); improved=True\n                    except Exception:\n                        pass\n                if (not improved) and evals < budget:\n                    tries = min(3, budget - evals)\n                    for _ in range(tries):\n                        noise = self.rng.normal(scale=0.34, size=self.dim) * (0.18 + 2.2*ecc)\n                        x_try = self._clip(x_center + noise * per_dim_trust, lb, ub)\n                        evals, imp2, xp2, fp2 = self._eval(x_try, func, lb, ub, evals, archive_x, archive_f)\n                        if imp2:\n                            x_center = xp2.copy(); per_dim_trust = np.minimum(self.max_step, per_dim_trust*1.06); improved=True; break\n\n            # 4: anisotropic Lévy jump\n            elif op == 4:\n                direction = self.rng.normal(size=self.dim) * (0.35 + 3.2 * ecc)\n                norm = np.linalg.norm(direction)\n                if norm < 1e-12:\n                    direction = self.rng.normal(size=self.dim); norm = np.linalg.norm(direction)\n                direction = direction / (norm + 1e-12)\n                try:\n                    c = float(self.rng.standard_cauchy())\n                except Exception:\n                    c = float(self.rng.normal())\n                levy_scale = np.clip(np.mean(per_dim_trust) * (1.0 + 1.0 * abs(c)), self.min_step, self.max_step)\n                x_jump = x_center + levy_scale * direction * (0.35 + 2.2 * ecc)\n                x_jump = self._clip(x_jump, lb, ub)\n                evals, imp, xp, fp = self._eval(x_jump, func, lb, ub, evals, archive_x, archive_f)\n                if imp:\n                    x_center = xp.copy(); per_dim_trust = np.minimum(self.max_step, per_dim_trust*1.28); improved=True\n                else:\n                    coords = np.argsort(-ecc)[:max(1, min(4, self.dim))]\n                    for _ in range(min(2, budget - evals)):\n                        x_r = x_center.copy()\n                        for c in coords:\n                            x_r[c] = self.rng.uniform(lb[c], ub[c])\n                        evals, imp2, xp2, fp2 = self._eval(x_r, func, lb, ub, evals, archive_x, archive_f)\n                        if imp2:\n                            x_center = xp2.copy(); per_dim_trust[coords] = np.minimum(self.max_step, per_dim_trust[coords]*1.16); improved=True; break\n                    if not improved:\n                        per_dim_trust = np.maximum(self.min_step, per_dim_trust * 0.88)\n\n            # 5: degree-mixture batch\n            elif op == 5:\n                max_batch = min(6, budget - evals)\n                if max_batch > 0:\n                    degs = self.rng.choice(np.arange(1, max_deg+1), size=max_batch, p=p_deg)\n                    for k in degs:\n                        if evals >= budget:\n                            break\n                        record_degree(k)\n                        ecc_probs = ecc / ecc.sum()\n                        coords = self._select_coords(ecc_probs, k)\n                        noise = self.rng.normal(scale=0.18, size=self.dim) * (0.18 + 2.2*ecc)\n                        step_vec = noise * per_dim_trust\n                        mask = np.zeros(self.dim); mask[coords] = 1.0\n                        x_try = self._clip(x_center + (0.15 * mask) * step_vec + 0.02 * (1.0-mask) * step_vec, lb, ub)\n                        evals, imp, xp, fp = self._eval(x_try, func, lb, ub, evals, archive_x, archive_f)\n                        if imp:\n                            x_center = xp.copy(); per_dim_trust[coords] = np.minimum(self.max_step, per_dim_trust[coords]*1.12); improved=True\n                            if evals < budget:\n                                x_try2 = self._clip(x_center + 0.6 * step_vec * mask, lb, ub)\n                                evals, imp2, xp2, fp2 = self._eval(x_try2, func, lb, ub, evals, archive_x, archive_f)\n                                if imp2:\n                                    x_center = xp2.copy(); per_dim_trust[coords] = np.minimum(self.max_step, per_dim_trust[coords]*1.18); improved=True\n                                    break\n\n            # 6: scaffolded mixed-degree\n            elif op == 6:\n                core_deg = int(self.rng.choice(np.arange(1, max_deg+1), p=p_deg))\n                comp_lim = min(core_deg, self.dim - core_deg)\n                comp_deg = self.rng.integers(0, comp_lim+1) if comp_lim > 0 else 0\n                record_degree(core_deg + comp_deg)\n                ecc_probs = ecc / ecc.sum()\n                coords_core = self._select_coords(ecc_probs, core_deg)\n                rem = [i for i in range(self.dim) if i not in coords_core]\n                if len(rem) == 0:\n                    comp_coords = np.array([], dtype=int)\n                else:\n                    comp_coords = self.rng.choice(rem, size=comp_deg, replace=False) if comp_deg>0 else np.array([], dtype=int)\n                x_sc = x_center.copy()\n                mode = self.rng.choice(['best','median','random'], p=[0.4,0.4,0.2])\n                if mode == 'best':\n                    idx_best = np.argmin(np.asarray(archive_f))\n                    fill = archive_x[idx_best]\n                elif mode == 'median':\n                    fill = np.median(np.vstack(archive_x), axis=0)\n                else:\n                    fill = self.rng.uniform(lb, ub)\n                core_step = per_dim_trust[coords_core] * (0.8 + 1.6 * self.rng.random(len(coords_core)))\n                for i, c in enumerate(coords_core):\n                    x_sc[c] += np.sign(self.rng.normal()) * core_step[i] * (0.7 + 0.8 * ecc[c])\n                for c in comp_coords:\n                    x_sc[c] = x_center[c] + 0.4*(fill[c] - x_center[c]) + self.rng.normal(scale=0.08)*(per_dim_trust[c]*(0.4+ecc[c]))\n                x_sc = self._clip(x_sc, lb, ub)\n                evals, imp, xp, fp = self._eval(x_sc, func, lb, ub, evals, archive_x, archive_f)\n                if imp:\n                    x_center = xp.copy()\n                    per_dim_trust[coords_core] = np.minimum(self.max_step, per_dim_trust[coords_core]*1.15)\n                    if len(comp_coords)>0:\n                        per_dim_trust[comp_coords] = np.minimum(self.max_step, per_dim_trust[comp_coords]*1.08)\n                    improved = True\n                else:\n                    if comp_deg > 0 and evals < budget:\n                        x_sc2 = x_center.copy()\n                        for c in comp_coords:\n                            x_sc2[c] += np.sign(self.rng.normal()) * per_dim_trust[c] * (0.6 + 1.0*ecc[c])\n                        evals, imp2, xp2, fp2 = self._eval(x_sc2, func, lb, ub, evals, archive_x, archive_f)\n                        if imp2:\n                            x_center = xp2.copy(); improved=True\n\n            # 7: degree-swap neighborhood\n            elif op == 7:\n                deg = int(self.rng.choice(np.arange(1, max_deg+1), p=p_deg))\n                record_degree(deg)\n                active = np.argsort(-ecc)[:deg]\n                outside = [i for i in range(self.dim) if i not in active]\n                if outside:\n                    sw = int(self.rng.choice(outside))\n                    for c in active:\n                        if evals >= budget:\n                            break\n                        x_new = x_center.copy()\n                        x_new[sw] = x_center[sw] + np.sign(self.rng.normal()) * per_dim_trust[sw] * (0.9 + 1.1*ecc[sw])\n                        x_new[c] = x_center[c] * 0.6 + 0.4 * np.median(np.vstack(archive_x), axis=0)[c]\n                        x_new = self._clip(x_new, lb, ub)\n                        evals, imp, xp, fp = self._eval(x_new, func, lb, ub, evals, archive_x, archive_f)\n                        if imp:\n                            x_center = xp.copy(); per_dim_trust[[sw,c]] = np.minimum(self.max_step, per_dim_trust[[sw,c]]*1.10); improved=True; break\n\n            # 8: subgraph-recombination (graph-aware recombine of coords in a connected subgraph)\n            elif op == 8:\n                # pick subgraph size biased by under-sampled degrees\n                deg_i = int(self.rng.choice(np.arange(1, max_deg+1), p=p_deg))\n                record_degree(deg_i)\n                # seed biased by eccentricity and small degree centrality\n                degree_centrality = np.sum(adj, axis=0)\n                prob_seed = (ecc * (1.0 + 0.5*(1.0 - degree_centrality/ (np.max(degree_centrality)+1e-12))))\n                prob_seed = prob_seed / (prob_seed.sum() + 1e-12)\n                seed = int(self.rng.choice(np.arange(self.dim), p=prob_seed))\n                nodes = sample_connected_subgraph(seed=seed, size=deg_i)\n                # recombine using neighbors in nodes: take convex combination of best archived points restricted to nodes\n                if len(nodes) > 0:\n                    # pick a few elites and average on subgraph coords\n                    n_el = min(4, len(archive_x))\n                    ids = np.argsort(archive_f)[:n_el]\n                    elites = np.vstack([archive_x[i] for i in ids])\n                    coeffs = self.rng.random(n_el)\n                    coeffs = coeffs / coeffs.sum()\n                    cand = x_center.copy()\n                    # mix only on nodes\n                    cand[nodes] = (coeffs @ elites[:, nodes])\n                    # small jitter on non-nodes\n                    other = [i for i in range(self.dim) if i not in nodes]\n                    if other:\n                        cand[other] = x_center[other] + 0.08 * per_dim_trust[other] * self.rng.normal(size=len(other))\n                    cand = self._clip(cand, lb, ub)\n                    evals, imp, xp, fp = self._eval(cand, func, lb, ub, evals, archive_x, archive_f)\n                    if imp:\n                        x_center = xp.copy(); per_dim_trust[nodes] = np.minimum(self.max_step, per_dim_trust[nodes]*1.12); improved=True\n                    else:\n                        # try small perturbations along subgraph principal direction\n                        if len(nodes) > 1:\n                            U = np.zeros((self.dim, len(nodes)))\n                            for i, c in enumerate(nodes):\n                                U[c, i] = 1.0\n                            local_points = np.vstack(archive_x)[-min(30, len(archive_x)):]\n                            model = self._fit_quadratic(x_center, U, local_points, archive_f[-min(30, len(archive_f)):])\n                            if model is not None:\n                                try:\n                                    H_reg = model['H'] + 1e-6*np.eye(model['H'].shape[0])\n                                    g = model['g']\n                                    z = -np.linalg.solve(H_reg, g)\n                                    max_norm = min(self.max_step, 2.0*np.linalg.norm(per_dim_trust[nodes]))\n                                    if np.linalg.norm(z) > max_norm:\n                                        z = z / (np.linalg.norm(z)+1e-12) * max_norm\n                                    cand2 = model['center'] + model['U'] @ z\n                                    cand2 = self._clip(cand2, lb, ub)\n                                    if evals < budget:\n                                        evals, imp2, xp2, fp2 = self._eval(cand2, func, lb, ub, evals, archive_x, archive_f)\n                                        if imp2:\n                                            x_center = xp2.copy(); per_dim_trust[nodes] = np.minimum(self.max_step, per_dim_trust[nodes]*1.14); improved=True\n                                except Exception:\n                                    pass\n\n            # 9: spectral-subgraph move (use adjacency spectral clustering / eigenvectors)\n            elif op == 9:\n                # compute leading eigenvector of adjacency to find a large coordinated block\n                try:\n                    vals, vecs = np.linalg.eigh(adj + 1e-12*np.eye(self.dim))\n                    leading = vecs[:, np.argmax(vals)]\n                    # choose nodes with large absolute components\n                    idxs = np.argsort(-np.abs(leading))[:min(max_deg, self.dim)]\n                    size = max(1, min(len(idxs), int(self.rng.choice(np.arange(1, max_deg+1), p=p_deg))))\n                    nodes = idxs[:size]\n                    # propose a move along leading eigenvector restricted to nodes\n                    v = np.zeros(self.dim); v[nodes] = leading[nodes]\n                    v = v / (np.linalg.norm(v) + 1e-12)\n                    scale = np.mean(per_dim_trust) * (0.8 + 1.8*self.rng.random())\n                    for mult in (0.6, 1.0, 1.6):\n                        if evals >= budget:\n                            break\n                        step = scale * mult\n                        for sign in (+1.0, -1.0):\n                            x_try = x_center + sign * step * v\n                            evals, imp, xp, fp = self._eval(x_try, func, lb, ub, evals, archive_x, archive_f)\n                            if imp:\n                                x_center = xp.copy(); per_dim_trust[nodes] = np.minimum(self.max_step, per_dim_trust[nodes]*1.12); improved=True; break\n                        if improved:\n                            break\n                except Exception:\n                    pass\n\n            # 10: edge-density densify exploration — force probes focused on weak edges to increase sampling\n            elif op == 10:\n                # find lowest-weight edges with some weight but not zero and probe their coordinates jointly\n                tri_i, tri_j = np.triu_indices(self.dim, k=1)\n                weights = adj[tri_i, tri_j]\n                # pick some small set of low-weight but informative edges\n                order = np.argsort(weights)\n                if len(order) == 0:\n                    # fallback random small gaussian\n                    x_try = self._clip(x_center + self.rng.normal(scale=0.2, size=self.dim) * per_dim_trust, lb, ub)\n                    evals, imp, xp, fp = self._eval(x_try, func, lb, ub, evals, archive_x, archive_f)\n                    if imp:\n                        x_center = xp.copy(); improved=True\n                else:\n                    picks = order[:min( max(1, int(0.05*len(order))), 6)]\n                    # construct a probe that perturbs both ends of selected edges\n                    mask = np.zeros(self.dim)\n                    for pi in picks:\n                        i = tri_i[pi]; j = tri_j[pi]\n                        mask[i] = 1.0; mask[j] = 1.0\n                    idxs = np.where(mask > 0)[0]\n                    if len(idxs) == 0:\n                        idxs = np.array([int(self.rng.integers(0,self.dim))])\n                    noise = self.rng.normal(scale=0.28, size=len(idxs))\n                    x_try = x_center.copy()\n                    x_try[idxs] += noise * per_dim_trust[idxs] * (0.6 + 1.6*ecc[idxs])\n                    x_try = self._clip(x_try, lb, ub)\n                    evals, imp, xp, fp = self._eval(x_try, func, lb, ub, evals, archive_x, archive_f)\n                    if imp:\n                        x_center = xp.copy()\n                        per_dim_trust[idxs] = np.minimum(self.max_step, per_dim_trust[idxs]*1.12)\n                        improved=True\n                    else:\n                        # light densify: nudge adjacency for those edges so they become preferred in future subgraph sampling\n                        for pi in picks:\n                            i = tri_i[pi]; j = tri_j[pi]\n                            adj[i,j] = min(1.0, adj[i,j] + 0.03)\n                            adj[j,i] = adj[i,j]\n\n            # update operator stats\n            op_tries[op] += 1.0\n            if improved:\n                op_success[op] += 1.0\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # manage archive size\n            max_archive = max(600, 60*self.dim)\n            if len(archive_x) > max_archive:\n                af = np.asarray(archive_f)\n                idx_sorted = np.argsort(af)\n                keep_best = idx_sorted[:min(200, len(idx_sorted))]\n                rem = max_archive - len(keep_best)\n                recent_items = np.arange(len(archive_x) - rem, len(archive_x))\n                keep_idx = np.unique(np.concatenate([keep_best, recent_items]))\n                new_archive_x = [archive_x[i] for i in keep_idx]\n                new_archive_f = [archive_f[i] for i in keep_idx]\n                archive_x = new_archive_x\n                archive_f = new_archive_f\n\n            # stagnation handling: escalate graph densification and heavy moves\n            if no_improve > max_no_improve:\n                no_improve = 0\n                # boost deg bonuses for under-sampled degrees temporarily\n                for d in range(1, max_deg+1):\n                    if deg_counts[d] < (len(recent_degs)/max(1, max_deg)):\n                        deg_bonus[d] *= 1.18\n                # densify: increase adj weights for top co-varying pairs observed in recent archive\n                try:\n                    _update_graph(archive_x, archive_f)\n                    # then perform a big anisotropic graph-guided move\n                    top_coords = np.argsort(-np.sum(adj, axis=0))[:max(1, min(6, self.dim))]\n                    direction = self.rng.normal(size=self.dim) * (0.8 + 4.2 * ecc)\n                    # reinforce top coords\n                    for c in top_coords:\n                        direction[c] *= 1.8\n                    direction = direction / (np.linalg.norm(direction) + 1e-12)\n                    big = np.clip(2.4 * np.mean(per_dim_trust), self.min_step, self.max_step)\n                    x_big = self._clip(x_center + big * direction, lb, ub)\n                    evals, imp, xp, fp = self._eval(x_big, func, lb, ub, evals, archive_x, archive_f)\n                    if imp:\n                        x_center = xp.copy()\n                        per_dim_trust = np.minimum(self.max_step, per_dim_trust * 1.28)\n                    else:\n                        x_touch = x_center.copy()\n                        for c in top_coords:\n                            x_touch[c] = self.rng.uniform(lb[c], ub[c])\n                        evals, imp2, xp2, fp2 = self._eval(x_touch, func, lb, ub, evals, archive_x, archive_f)\n                        if imp2:\n                            x_center = xp2.copy()\n                            per_dim_trust[top_coords] = np.minimum(self.max_step, per_dim_trust[top_coords] * 1.20)\n                except Exception:\n                    pass\n\n            # gentle trust adaptation\n            if no_improve > (max_no_improve // 2):\n                per_dim_trust = np.maximum(self.min_step, per_dim_trust * 0.90)\n            else:\n                per_dim_trust = np.minimum(self.max_step, per_dim_trust * 1.006)\n\n            # occasional decay of deg_bonus and adjacency smoothing\n            if self.rng.random() < 0.02:\n                deg_bonus[1:max_deg+1] = deg_bonus[1:max_deg+1] * 0.92\n            if self.rng.random() < 0.03:\n                adj = np.clip(adj * 0.96, 0.0, 1.0)\n\n        return float(self.f_opt), np.array(self.x_opt, dtype=float)", "configspace": "", "generation": 22, "feedback": "The algorithm ESEADL_DenseGraph scored 0.782 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7e5ae53c-4f03-4d2c-9835-e6e81be8a619"], "operator": null, "metadata": {"ast_features": {"Nodes": 10140.0, "Edges": 10139.0, "Max Degree": 38.0, "Min Degree": 1.0, "Mean Degree": 1.9998027613412228, "Degree Variance": 2.1351084423592392, "Transitivity": 0.0, "Max Depth": 27.0, "Min Depth": 2.0, "Mean Depth": 11.633720930232558, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3384378864476427, "Depth Entropy": 2.8825719975885438, "Assortativity": 1.2720562261200347e-08, "Average Eccentricity": 28.55611439842209, "Diameter": 38.0, "Radius": 19.0, "Edge Density": 9.861932938856015e-05, "Average Shortest Path": 15.086974078066357, "mean_complexity": 14.846153846153847, "total_complexity": 193.0, "mean_token_count": 701.5384615384615, "total_token_count": 9120.0, "mean_parameter_count": 4.076923076923077, "total_parameter_count": 53.0}, "feature_guidance_action": "increase", "feature_guidance_feature_name": "Edge Density", "performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.8851354767610382}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9113641295529556}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.858897018981587}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.7431589822658431}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.7364420097056201}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.7633445442460721}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.7887486645909713}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.7265357244545443}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.1635140626103121}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.7067899968746925}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.7701044140825055}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.7160500997195931}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9829567773246959}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9891206849514047}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9836363623984669}], "aucs": [0.8851354767610382, 0.9113641295529556, 0.858897018981587, 0.7431589822658431, 0.7364420097056201, 0.7633445442460721, 0.7887486645909713, 0.7265357244545443, 0.1635140626103121, 0.7067899968746925, 0.7701044140825055, 0.7160500997195931, 0.9829567773246959, 0.9891206849514047, 0.9836363623984669]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-new-BBOB-2", "seed": 2}
{"method_name": "ES-guided-new", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided-new", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true, "feature_guided_mutation": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "9fc85c14-e6c3-49db-978d-398212f8651e", "fitness": 0.6124134978595523, "name": "HubMagnetizedAssortativeDQN", "description": "Hub-Magnetized Assortative Diagonal Quasi-Newton (HMA-DQN) — actively \"magnetizes\" and grows high-degree hubs using targeted hub seeding, soft-cloning, dense spoke-fans and bridge-reinforcement while keeping diagonal quasi-Newton preconditioning and momentum-chained line-search for reliable exploitation.", "code": "import numpy as np\n\nclass HubMagnetizedAssortativeDQN:\n    \"\"\"\n    Hub-Magnetized Assortative Diagonal Quasi-Newton (HMA-DQN)\n    - budget: total function evaluations allowed\n    - dim: dimensionality of the search space\n\n    Key ideas:\n      * Explicit \"hub magnetization\" operators that place points intentionally designed to be within\n        a small distance of many archive points (increasing node degree).\n      * Soft-cloning: create slight perturbations (micro-clones) around a candidate hub so multiple\n        archive points end up mutually adjacent under the adaptive threshold.\n      * Fan-spoke amplification: many short spokes toward diverse archive points (short t in interpolation)\n        to attach many edges to the hub; vary t so some spokes bridge clusters.\n      * Bridge-reinforcement: multi-frac interpolation between far pairs combined with small jitter to create\n        connecting nodes.\n      * Degree-aware preferential sampling: direct budget to operations that increase max degree when the\n        observed max-degree is below a target.\n      * Retains diagonal quasi-Newton-esque probing, Adagrad preconditioning and momentum-chained line-search.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, archive_size=80, init_samples=None,\n                 rng_seed=None, sigma_init=0.45,\n                 success_inc=1.12, failure_dec=0.72,\n                 max_sigma=5.0, min_sigma=1e-8,\n                 max_chain_len=6, hub_base_prob=0.22, hub_aggressive_prob=0.12,\n                 hub_clone_factor=0.06, hub_spoke_factor=0.26,\n                 bridge_base_prob=0.14, pref_attach_power=1.8,\n                 min_init_samples=8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.archive_size = int(archive_size)\n        self.rng = np.random.default_rng(rng_seed)\n\n        if init_samples is None:\n            self.init_samples = int(min(max(min_init_samples, 4 * self.dim), max(8, self.budget // 80)))\n        else:\n            self.init_samples = int(init_samples)\n\n        self.sigma = float(sigma_init)\n        self.success_inc = float(success_inc)\n        self.failure_dec = float(failure_dec)\n        self.max_sigma = float(max_sigma)\n        self.min_sigma = float(min_sigma)\n\n        self.max_chain_len = int(max_chain_len)\n        self.momentum_gain = 1.4\n\n        # Adagrad-like accumulator\n        self.grad_sq_accum = np.ones(self.dim, dtype=float) * 1e-8\n\n        # hub/bridge params\n        self.hub_base_prob = float(hub_base_prob)\n        self.hub_aggressive_prob = float(hub_aggressive_prob)\n        self.hub_clone_factor = float(hub_clone_factor)   # fraction of archive used to create clones\n        self.hub_spoke_factor = float(hub_spoke_factor)   # fraction of archive to shoot spokes at\n        self.bridge_base_prob = float(bridge_base_prob)\n        self.pref_attach_power = float(pref_attach_power)\n\n        # small ridge for linear solves\n        self.ridge = 1e-8\n\n        # tracking\n        self.max_observed_degree = 0\n\n    def __call__(self, func):\n        # read bounds if provided; else default\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        lb = lb.ravel()[:self.dim]\n        ub = ub.ravel()[:self.dim]\n\n        evals = 0\n        archive = []  # list of (f, x) sorted ascending by f\n        f_best = np.inf\n        x_best = None\n\n        def record(x, f):\n            nonlocal f_best, x_best, archive\n            f = float(f)\n            x = np.array(x, dtype=float)\n            if f < f_best:\n                f_best = f\n                x_best = x.copy()\n            archive.append((f, x.copy()))\n            archive.sort(key=lambda t: t[0])\n            if len(archive) > self.archive_size:\n                del archive[self.archive_size:]\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                raise StopIteration()\n            x_clipped = np.clip(np.array(x, dtype=float), lb, ub)\n            f = float(func(x_clipped))\n            evals += 1\n            record(x_clipped, f)\n            return f, x_clipped\n\n        # robust degree computation: use k-th nearest neighbor distances to set adaptive threshold\n        def compute_degrees(archive_local):\n            n = len(archive_local)\n            if n <= 1:\n                return np.zeros(n, dtype=int), np.linalg.norm(ub - lb) * 0.5\n            pts = np.vstack([t[1] for t in archive_local])\n            dmat = np.linalg.norm(pts[:, None, :] - pts[None, :, :], axis=2)\n            # for each point, compute distance to k-th neighbor (k ~ min(8,n-1))\n            k = min(8, max(1, n - 1))\n            # set diagonal to large to ignore\n            np.fill_diagonal(dmat, np.inf)\n            kth = np.partition(dmat, kth=k-1, axis=1)[:, k-1]\n            # global threshold: median of kth-distances times small factor that increases with dimension\n            median_kth = np.median(kth[np.isfinite(kth)])\n            factor = 1.0 + 0.10 * (self.dim / 10.0)\n            thresh = max(1e-12, median_kth * factor)\n            adj = (dmat <= thresh).astype(int)\n            np.fill_diagonal(adj, 0)\n            degs = adj.sum(axis=1)\n            return degs.astype(int), float(thresh)\n\n        # initial sampling\n        for _ in range(max(1, self.init_samples)):\n            if evals >= self.budget:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            try:\n                safe_eval(x0)\n            except StopIteration:\n                break\n\n        if evals == 0:\n            try:\n                safe_eval(self.rng.uniform(lb, ub))\n            except StopIteration:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        prev_success_dir = None\n        no_improve = 0\n        iteration = 0\n        eye = np.eye(self.dim)\n\n        def pref_attach_probs(degs):\n            base = np.maximum(degs, 0).astype(float) + 1.0\n            powed = base ** self.pref_attach_power\n            s = np.sum(powed)\n            if s <= 0:\n                return np.ones_like(powed) / len(powed)\n            return powed / s\n\n        # Hub magnetization: create a hub designed to be near many points.\n        def hub_magnetize(archive_local, degrees_local, remaining_budget, aggressive=False):\n            \"\"\"\n            Create 1) a central hub point intended to be within threshold of many archive points\n            and 2) a set of micro-clones around it to amplify observed degree (multiple nearby archive nodes).\n            Also shoot short spokes toward selected targets to attach edges.\n            \"\"\"\n            if remaining_budget <= 0 or len(archive_local) < 3:\n                return 0\n            done = 0\n            pts = np.vstack([t[1] for t in archive_local])\n            fvals = np.array([t[0] for t in archive_local])\n            n = len(pts)\n            # choose seed: either best centroid or high-degree node (biased)\n            if (self.rng.random() < 0.65) and np.any(degrees_local > 0):\n                probs = pref_attach_probs(degrees_local)\n                seed_idx = int(self.rng.choice(n, p=probs))\n                seed = pts[seed_idx]\n            else:\n                # centroid of top quarter elites\n                topk = max(2, int(max(3, n * 0.15)))\n                centroid = np.mean(pts[:topk], axis=0)\n                seed = 0.6 * centroid + 0.4 * pts[0]  # favor best slightly\n\n            # decide hub radius: use current threshold scaled smaller for magnetization\n            _, thr = compute_degrees(archive_local)\n            hub_radius = max(1e-12, thr * (0.55 if aggressive else 0.7))\n            # place hub slightly nudged toward dense direction: compute vector to weighted mean of distant & good points\n            dists = np.linalg.norm(pts - seed.reshape(1, -1), axis=1)\n            quality = (np.max(fvals) - fvals) / (np.max(fvals) - np.min(fvals) + 1e-12)\n            score = 0.5 * (dists / (np.max(dists) + 1e-12)) + 0.5 * quality\n            targets_order = np.argsort(-score)\n            # target set size: want a hub that connects to many, but budget-limited\n            target_count = min(max(6, int(n * (self.hub_spoke_factor * (1.4 if aggressive else 1.0)))), n - 1)\n            if target_count <= 0:\n                return 0\n            targets = targets_order[:target_count]\n\n            # compute a hub candidate as weighted average of several targets but pulled toward seed\n            heavy_targets = pts[targets[:max(3, min(6, len(targets)))]]\n            hub = 0.5 * seed + 0.5 * np.mean(heavy_targets, axis=0)\n            # ensure hub inside bounds\n            hub = np.clip(hub, lb, ub)\n\n            # eval hub (this first evaluation is critical)\n            if evals >= self.budget:\n                return done\n            try:\n                safe_eval(hub)\n                done += 1\n            except StopIteration:\n                return done\n\n            # micro-cloning: create several tiny perturbations around hub so many archive nodes become mutually adjacent\n            clone_frac = max(1, int(min(8, max(2, int(self.hub_clone_factor * n * (1.5 if aggressive else 1.0))))))\n            # small jitter scale relative to hub_radius (make clones within hub_radius/2)\n            clone_scale = hub_radius * 0.35 + 1e-8\n            for _c in range(clone_frac):\n                if evals >= self.budget:\n                    break\n                cand = np.clip(hub + self.rng.normal(scale=clone_scale, size=self.dim), lb, ub)\n                try:\n                    safe_eval(cand)\n                    done += 1\n                except StopIteration:\n                    break\n\n            # fan-spokes: short segments from hub toward many targets (short t to keep points close to hub)\n            # ensure at least some longer spokes to bridge clusters\n            short_t_low, short_t_high = (0.12, 0.45) if not aggressive else (0.08, 0.35)\n            long_t_low, long_t_high = (0.55, 0.9)\n            # alternate short and occasional long spokes\n            for i, tidx in enumerate(targets):\n                if evals >= self.budget:\n                    break\n                target = pts[tidx]\n                t = self.rng.uniform(short_t_low, short_t_high) if (i % 5 != 0) else self.rng.uniform(long_t_low, long_t_high)\n                # jitter scale small relative to hub radius\n                jitter = hub_radius * (0.08 + 0.06 * self.rng.random())\n                cand = np.clip(hub + t * (target - hub) + self.rng.normal(scale=jitter, size=self.dim), lb, ub)\n                try:\n                    safe_eval(cand)\n                    done += 1\n                except StopIteration:\n                    break\n\n            # optionally add a local mini-cloud around the hub (dense evaluations near hub)\n            if aggressive:\n                cloud_n = min(6, max(2, int(n * 0.02)))\n                cloud_scale = hub_radius * 0.15\n                for _ in range(cloud_n):\n                    if evals >= self.budget:\n                        break\n                    cand = np.clip(hub + self.rng.normal(scale=cloud_scale, size=self.dim), lb, ub)\n                    try:\n                        safe_eval(cand)\n                        done += 1\n                    except StopIteration:\n                        break\n\n            return done\n\n        # Bridge operator: evaluate several connecting points between far pairs\n        def bridge_reinforce(archive_local, remaining_budget):\n            if remaining_budget <= 0 or len(archive_local) < 3:\n                return 0\n            pts = np.vstack([t[1] for t in archive_local])\n            n = len(pts)\n            dmat = np.linalg.norm(pts[:, None, :] - pts[None, :, :], axis=2)\n            np.fill_diagonal(dmat, -1.0)\n            i, j = np.unravel_index(np.argmax(dmat), dmat.shape)\n            if dmat[i, j] <= 0:\n                return 0\n            done = 0\n            a, b = pts[i], pts[j]\n            # sample multiple fractions biased toward mid but with jitter to hit clusters\n            fracs = [0.5, 0.33, 0.66, 0.2, 0.8]\n            self.rng.shuffle(fracs)\n            _, thr = compute_degrees(archive_local)\n            jitter_scale = max(1e-12, thr * 0.05)\n            for f in fracs:\n                if evals >= self.budget:\n                    break\n                cand = np.clip(f * a + (1.0 - f) * b + self.rng.normal(scale=jitter_scale, size=self.dim), lb, ub)\n                try:\n                    safe_eval(cand)\n                    done += 1\n                    if done >= 4:\n                        break\n                except StopIteration:\n                    break\n            return done\n\n        # directional probing and local quasi-newton-like step helper\n        def directional_probes_and_step(center, remaining):\n            # build candidate directions: momentum, axes, PCA from archive, inter-archive directions\n            cand_dirs = []\n            if prev_success_dir is not None:\n                cand_dirs.append(prev_success_dir)\n            for i in range(self.dim):\n                cand_dirs.append(eye[i])\n            if len(archive) >= 4:\n                topk = min(len(archive), max(6, self.dim))\n                pts = np.vstack([t[1] for t in archive[:topk]])\n                for i in range(min(6, pts.shape[0])):\n                    vec = pts[i] - center\n                    nrm = np.linalg.norm(vec)\n                    if nrm > 0:\n                        cand_dirs.append(vec / nrm)\n            # PCA direction\n            if len(archive) >= min(4, self.dim):\n                top_pts = np.vstack([t[1] for t in archive[: min(len(archive), max(6, self.dim))]])\n                Xc = top_pts - center.reshape(1, -1)\n                try:\n                    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                    pc = Vt[0]\n                    if np.linalg.norm(pc) > 0:\n                        cand_dirs.append(pc / (np.linalg.norm(pc) + 1e-20))\n                except Exception:\n                    pass\n            # random extras to ensure coverage\n            extra_needed = max(0, min(6, self.dim) - len(cand_dirs))\n            if extra_needed > 0:\n                rand_mat = self.rng.normal(size=(self.dim, extra_needed))\n                try:\n                    q, _ = np.linalg.qr(rand_mat)\n                    for j in range(min(extra_needed, q.shape[1])):\n                        cand_dirs.append(q[:, j])\n                except Exception:\n                    for _ in range(extra_needed):\n                        v = self.rng.normal(size=self.dim)\n                        v /= (np.linalg.norm(v) + 1e-20)\n                        cand_dirs.append(v)\n\n            domain_scale = np.maximum(ub - lb, 1e-9)\n            alpha = np.clip(self.sigma * np.mean(domain_scale), 1e-12, np.max(domain_scale) * 0.9)\n            # allocate number of probes conservatively: leave budget for hub ops\n            reserve = 6\n            max_probes = max(1, min(len(cand_dirs), int(max(1, (remaining - reserve) // 2))))\n            directions = np.array(cand_dirs[:max_probes])\n\n            grad_est = np.zeros(self.dim, dtype=float)\n            diag_h = np.zeros(self.dim, dtype=float)\n            n_probed = 0\n            f0_local = f_best\n            center_local = center\n            for dvec in directions:\n                if evals >= self.budget:\n                    break\n                xp = np.clip(center_local + alpha * dvec, lb, ub)\n                xm = np.clip(center_local - alpha * dvec, lb, ub)\n                try:\n                    fp, xp = safe_eval(xp)\n                except StopIteration:\n                    break\n                fm = None\n                if evals < self.budget:\n                    try:\n                        fm, xm = safe_eval(xm)\n                    except StopIteration:\n                        fm = None\n                if fm is None:\n                    deriv = (fp - f0_local) / (alpha + 1e-20)\n                    grad_est += deriv * dvec\n                else:\n                    deriv = (fp - fm) / (2.0 * alpha)\n                    curv = (fp + fm - 2.0 * f0_local) / (alpha * alpha + 1e-20)\n                    grad_est += deriv * dvec\n                    diag_h += curv * (dvec ** 2)\n                n_probed += 1\n\n            if n_probed == 0:\n                return None  # signal no progress\n\n            grad_est /= float(n_probed)\n            self.grad_sq_accum += grad_est ** 2\n            precond = 1.0 / (np.sqrt(self.grad_sq_accum) + 1e-8)\n            pos_h = np.where(diag_h > 1e-12, diag_h, 0.0)\n            if np.any(pos_h > 0):\n                precond = precond / (1.0 + np.minimum(pos_h, 1e6))\n\n            raw_step = - (precond * grad_est)\n            # local diagonal fit if archive large\n            if len(archive) >= min(8, self.dim + 3):\n                pts = np.vstack([t[1] for t in archive])\n                fs = np.array([t[0] for t in archive])\n                disps = pts - center_local.reshape(1, -1)\n                radii = np.linalg.norm(disps, axis=1)\n                M = min(len(archive), max(8, 4 * self.dim))\n                idxs = np.argsort(radii)[:M]\n                X = disps[idxs]\n                y = fs[idxs] - f0_local\n                A = np.hstack([X, 0.5 * (X ** 2)])\n                try:\n                    ATA = A.T @ A + self.ridge * np.eye(A.shape[1])\n                    sol = np.linalg.solve(ATA, A.T @ y)\n                    g_fit = sol[:self.dim]\n                    h_fit = sol[self.dim:]\n                    if np.all(np.isfinite(g_fit)):\n                        w = min(0.85, float(len(idxs)) / (len(idxs) + 6.0))\n                        combined_grad = (1.0 - w) * grad_est + w * g_fit\n                        denom = np.abs(h_fit) + 1e-6\n                        newton_step = - combined_grad / denom\n                        nrm_raw = np.linalg.norm(raw_step) + 1e-20\n                        nrm_newt = np.linalg.norm(newton_step) + 1e-20\n                        newton_step = newton_step * (nrm_raw / nrm_newt)\n                        raw_step = 0.35 * raw_step + 0.65 * newton_step\n                except Exception:\n                    pass\n\n            # scale step amplitude\n            max_step = np.max(domain_scale) * 0.6\n            step_norm = np.linalg.norm(raw_step)\n            if step_norm > 0:\n                step = raw_step * (self.sigma * max_step / (step_norm + 1e-20))\n            else:\n                step = self.rng.normal(scale=self.sigma * 0.1 * domain_scale, size=self.dim)\n\n            # clip to not jump out of bounds\n            step = np.clip(step, -0.9 * (center_local - lb), 0.9 * (ub - center_local))\n\n            # perform line-search with momentum chaining\n            try:\n                f1, x1 = safe_eval(center_local + 1.0 * step)\n            except StopIteration:\n                return None\n\n            accepted = False\n            used_chain = 0\n            f_line = None\n            x_line = None\n            if f1 + 1e-12 < f0_local:\n                t_prev = 1.0\n                f_prev = f1\n                x_prev = x1.copy()\n                for chain in range(self.max_chain_len):\n                    if evals >= self.budget:\n                        break\n                    t_try = t_prev * self.momentum_gain\n                    if np.abs(t_try) > 1e6:\n                        break\n                    try:\n                        f_try, x_try = safe_eval(center_local + t_try * step)\n                    except StopIteration:\n                        break\n                    if f_try + 1e-12 < f_prev:\n                        t_prev = t_try\n                        f_prev = f_try\n                        x_prev = x_try.copy()\n                        used_chain += 1\n                        continue\n                    else:\n                        # parabolic refine\n                        try:\n                            A = np.vstack([np.array([t_prev**2, t_prev, 1.0]),\n                                           np.array([t_try**2, t_try, 1.0]),\n                                           np.array([0.0, 0.0, 1.0])])\n                            bvec = np.array([f_prev, f_try, f0_local])\n                            coeff = np.linalg.lstsq(A, bvec, rcond=None)[0]\n                            a, bcoef, _ = coeff\n                            if a < 0:\n                                t_opt = -bcoef / (2.0 * a)\n                                if np.isfinite(t_opt) and 0.0 < t_opt < max(t_prev, t_try) * 1.5:\n                                    if evals < self.budget:\n                                        try:\n                                            f_opt, x_opt = safe_eval(center_local + t_opt * step)\n                                            if f_opt + 1e-12 < f_prev:\n                                                f_prev = f_opt\n                                                x_prev = x_opt.copy()\n                                        except StopIteration:\n                                            pass\n                        except Exception:\n                            pass\n                        break\n                if f_prev + 1e-12 < f0_local:\n                    accepted = True\n                    f_line = f_prev\n                    x_line = x_prev.copy()\n                else:\n                    accepted = True\n                    f_line = f1\n                    x_line = x1.copy()\n            else:\n                # backtracking exploration\n                t_back = 0.5\n                best_f = f1\n                best_x = x1.copy()\n                improved = False\n                while evals < self.budget and t_back > 1e-5:\n                    try:\n                        f_back, x_back = safe_eval(center_local + t_back * step)\n                    except StopIteration:\n                        break\n                    if f_back + 1e-12 < f0_local:\n                        improved = True\n                        best_f = f_back\n                        best_x = x_back.copy()\n                        # quick local extrapolation\n                        t_try = min(1.0, t_back * 1.5)\n                        if evals < self.budget:\n                            try:\n                                f_try, x_try = safe_eval(center_local + t_try * step)\n                                if f_try + 1e-12 < best_f:\n                                    best_f = f_try\n                                    best_x = x_try.copy()\n                            except StopIteration:\n                                pass\n                        break\n                    t_back *= 0.5\n                if improved:\n                    accepted = True\n                    f_line = best_f\n                    x_line = best_x.copy()\n                else:\n                    accepted = False\n\n            return {\n                'accepted': accepted,\n                'f_line': f_line,\n                'x_line': x_line\n            }\n\n        # main loop\n        try:\n            while evals < self.budget:\n                iteration += 1\n                remaining = self.budget - evals\n                center = x_best.copy()\n                f0 = f_best\n\n                # compute degrees\n                if len(archive) >= 3:\n                    degrees, thresh = compute_degrees(archive)\n                    max_degree = int(degrees.max())\n                    mean_degree = float(degrees.mean())\n                else:\n                    degrees = np.array([0])\n                    max_degree = 0\n                    mean_degree = 0.0\n                    thresh = np.linalg.norm(ub - lb) * 0.25\n\n                # adapt hub probability: more hub ops when max_degree is below target\n                target_degree = max(5, int(self.dim / 3))\n                hub_prob = self.hub_base_prob * (1.0 + max(0.0, (target_degree - max_degree)) * 0.22)\n                hub_aggressive_prob = self.hub_aggressive_prob * (1.0 + max(0.0, (target_degree - max_degree)) * 0.25)\n                bridge_prob = self.bridge_base_prob * (1.0 + max(0.0, (3 - max_degree)) * 0.14)\n\n                # small preferential attach sampling (jittered near high-degree)\n                if len(archive) >= 4 and remaining >= 1 and self.rng.random() < 0.10:\n                    degs_all, _ = compute_degrees(archive)\n                    probs = pref_attach_probs(degs_all)\n                    idx = int(self.rng.choice(len(archive), p=probs))\n                    hub_pt = archive[idx][1]\n                    if evals < self.budget:\n                        try:\n                            safe_eval(np.clip(hub_pt + self.rng.normal(scale=self.sigma * 0.09 * (ub-lb), size=self.dim), lb, ub))\n                        except StopIteration:\n                            pass\n\n                # possibly run hub magnetization\n                if len(archive) >= 5 and remaining >= 3 and self.rng.random() < hub_prob:\n                    aggressive = (max_degree < max(3, int(self.dim/4))) and (self.rng.random() < hub_aggressive_prob)\n                    try:\n                        hub_magnetize(archive, degrees, self.budget - evals, aggressive=aggressive)\n                    except StopIteration:\n                        pass\n\n                # possibly run bridge reinforcement\n                if len(archive) >= 5 and remaining >= 2 and self.rng.random() < bridge_prob:\n                    try:\n                        bridge_reinforce(archive, self.budget - evals)\n                    except StopIteration:\n                        pass\n\n                # recompute remaining\n                remaining = self.budget - evals\n                if remaining <= 0:\n                    break\n\n                # build and execute directional probes + step\n                result = directional_probes_and_step(center, remaining)\n                if result is None:\n                    # nothing probed or budget exhausted -> break\n                    break\n                if result['accepted']:\n                    f_line = result['f_line']\n                    x_line = result['x_line']\n                    if f_line is not None and f_line + 1e-12 < f_best:\n                        f_best = float(f_line)\n                        x_best = x_line.copy()\n                    # success: expand sigma slightly\n                    self.sigma = min(self.sigma * self.success_inc, self.max_sigma)\n                    dir_vec = x_line - center\n                    nrm = np.linalg.norm(dir_vec)\n                    prev_success_dir = (dir_vec / (nrm + 1e-20)) if nrm > 0 else None\n                    no_improve = 0\n                else:\n                    # failure\n                    self.sigma = max(self.sigma * self.failure_dec, self.min_sigma)\n                    prev_success_dir = None\n                    no_improve += 1\n\n                # occasional random local sampling near best for diversity\n                if (iteration % max(1, self.dim // 2) == 0) and evals < self.budget:\n                    r_alloc = min(max(1, (self.budget - evals) // 20), 6)\n                    domain_scale = np.maximum(ub - lb, 1e-9)\n                    for _ in range(r_alloc):\n                        if evals >= self.budget:\n                            break\n                        samp = np.clip(x_best + self.rng.normal(scale=self.sigma * 0.7 * domain_scale, size=self.dim), lb, ub)\n                        try:\n                            safe_eval(samp)\n                        except StopIteration:\n                            break\n\n                # intensified hub burst on stagnation\n                if no_improve >= max(7, 2 * self.dim):\n                    no_improve = 0\n                    # if max degree low, perform an aggressive magnetization burst\n                    if max_degree < max(4, int(self.dim / 3)):\n                        anchors = min(6, max(3, len(archive)//5))\n                        for _a in range(anchors):\n                            if evals >= self.budget:\n                                break\n                            # pick a pool biased to elites\n                            pool = archive[: max(1, len(archive)//2)] if self.rng.random() < 0.8 else archive\n                            pts_pool = np.vstack([t[1] for t in pool])\n                            anchor_idx = int(self.rng.integers(0, len(pts_pool)))\n                            dists = np.linalg.norm(pts_pool - pts_pool[anchor_idx].reshape(1, -1), axis=1)\n                            k = max(3, int(len(pts_pool) * (self.hub_clone_factor * 2.0)))\n                            idxs = np.argsort(dists)[:k]\n                            centroid = np.mean(pts_pool[idxs], axis=0)\n                            candidate_x = np.clip(centroid + self.rng.normal(scale=0.03 * (ub-lb), size=self.dim), lb, ub)\n                            try:\n                                safe_eval(candidate_x)\n                            except StopIteration:\n                                break\n                            # follow-up magnetize on this small neighborhood\n                            try:\n                                local_archive = [(archive[i][0], archive[i][1]) for i in idxs if i < len(archive)]\n                                # degrees_local won't matter much here\n                                hub_magnetize(local_archive, np.zeros(len(local_archive), dtype=int), self.budget - evals, aggressive=True)\n                            except StopIteration:\n                                pass\n                    else:\n                        # moderate restart to diversify and then allow hubs next iterations\n                        if len(archive) > 0 and self.rng.random() < 0.9:\n                            top_k = min(len(archive), max(6, self.archive_size))\n                            pts = np.vstack([t[1] for t in archive[:top_k]])\n                            degs_small, _ = compute_degrees(archive[:top_k])\n                            scores = degs_small / (1.0 + np.linalg.norm(pts - x_best.reshape(1, -1), axis=1))\n                            idx = int(np.argmax(scores))\n                            chosen = pts[idx]\n                        else:\n                            chosen = self.rng.uniform(lb, ub)\n                        try:\n                            safe_eval(np.clip(chosen + self.rng.normal(scale=self.sigma * (ub-lb), size=self.dim), lb, ub))\n                        except StopIteration:\n                            pass\n                    # encourage exploration after burst\n                    self.sigma = min(self.sigma * 2.0, self.max_sigma)\n                    prev_success_dir = None\n\n                # update max observed degree\n                if len(archive) >= 3:\n                    degs, _ = compute_degrees(archive)\n                    self.max_observed_degree = max(self.max_observed_degree, int(degs.max()))\n\n            # end main while\n        except StopIteration:\n            pass\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 24, "feedback": "The algorithm HubMagnetizedAssortativeDQN scored 0.612 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["edc0d2b8-758b-4395-9c25-9a95e04d9139"], "operator": null, "metadata": {"ast_features": {"Nodes": 5650.0, "Edges": 5649.0, "Max Degree": 39.0, "Min Degree": 1.0, "Mean Degree": 1.999646017699115, "Degree Variance": 2.2895573968204244, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 8.906697638404955, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3262650086864392, "Depth Entropy": 2.268341716942399, "Assortativity": 0.0, "Average Eccentricity": 22.05716814159292, "Diameter": 31.0, "Radius": 16.0, "Edge Density": 0.00017699115044247788, "Average Shortest Path": 11.91928695970937, "mean_complexity": 16.444444444444443, "total_complexity": 148.0, "mean_token_count": 566.5555555555555, "total_token_count": 5099.0, "mean_parameter_count": 3.7777777777777777, "total_parameter_count": 34.0}, "feature_guidance_action": "increase", "feature_guidance_feature_name": "Max Degree", "performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9256835607269733}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.88042709183351}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.8877668338281192}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.6505399094809644}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.7141313313020907}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.7570340362313941}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.643332080154157}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.13361353884754767}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.14529588309241048}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.12783472098337256}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.1820234782901059}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.15903873301667026}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9942399053995452}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.990791917394836}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9944494473115886}], "aucs": [0.9256835607269733, 0.88042709183351, 0.8877668338281192, 0.6505399094809644, 0.7141313313020907, 0.7570340362313941, 0.643332080154157, 0.13361353884754767, 0.14529588309241048, 0.12783472098337256, 0.1820234782901059, 0.15903873301667026, 0.9942399053995452, 0.990791917394836, 0.9944494473115886]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-new-BBOB-5", "seed": 5}
{"method_name": "ES-guided-new", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided-new", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true, "feature_guided_mutation": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "a58dde24-e11e-468e-94ac-b35c87be45f4", "fitness": 0.6683316767299671, "name": "ProgressiveHighDegreeInteractionAmplifierPathStitchingV2", "description": "Progressive High-Degree Interaction Amplifier with Enhanced Mask Ensembles, Adaptive Trust, Low-Rank Covariance Synthesis and Smarter Path Stitching — progressively raises interaction degree, evolves and recombines sparse masks with per-mask trust and momentum, synthesizes anisotropic low-rank covariances from successful displacements, uses mask-guided quasi-gradient and heavy-tailed probes, and builds/stitches piecewise paths between elites with backtracking and smoothing to accelerate exploitation.", "code": "import numpy as np\n\nclass ProgressiveHighDegreeInteractionAmplifierPathStitchingV2:\n    \"\"\"\n    PHIA-PS-v2: Enhanced variant with many tunable parameters.\n      - Progressive phases increasing allowed interaction degree.\n      - Mask ensemble with per-mask trust, wins/trials, step, momentum; occasional recombination (union/intersection).\n      - Per-dimension importance weights updated from successful displacements.\n      - Low-rank covariance synthesis from displacement memory (shrinkage + PCA).\n      - Subspace quasi-gradient proposals (use disp_mem/elite points to infer promising subspace direction).\n      - Waypoint/edge graph for path stitching: includes backtracking, smoothing, and UCB-like selection.\n      - Final exploitation mixes anisotropic probes, targeted subspace quasi-Newton steps, heavy-tailed jumps,\n        and occasional mini-BFGS-like local refinement on small subspaces (budget permitting).\n    One-line: \"Evolves high-degree sparse coordinate masks with trust and recombination, synthesizes anisotropic low-rank covariances from successful displacements, and stitches/optimizes piecewise paths between elites for aggressive exploitation.\"\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, rng=None,\n                 init_samples=64, phase_count=8, max_degree=None,\n                 mem_size=512, seed_step=0.22, max_waypoints=400,\n                 mask_exploit_prob=0.62, mask_recomb_prob=0.12, mask_mutate_prob=0.07,\n                 mask_degree_bias=2.2, mask_decay=0.86, mask_boost=1.12,\n                 cov_probe_count=8, cov_shrink=0.25, cov_pow=0.5,\n                 heavy_tail_prob=0.18, heavy_tail_scale=0.22,\n                 follow_edge_p=0.66, ucb_c=1.0,\n                 elite_fraction=0.06, local_refine_prob=0.12,\n                 verbose=False):\n        # required\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.init_samples = int(init_samples)\n        self.phase_count = max(1, int(phase_count))\n        self.max_degree = int(max_degree) if max_degree is not None else min(self.dim, 14)\n        self.mem_size = int(mem_size)\n        self.seed_step = float(seed_step)\n        self.max_waypoints = int(max_waypoints)\n\n        # tuning parameters (increase parameter count as requested)\n        self.mask_exploit_prob = float(mask_exploit_prob)\n        self.mask_recomb_prob = float(mask_recomb_prob)\n        self.mask_mutate_prob = float(mask_mutate_prob)\n        self.mask_degree_bias = float(mask_degree_bias)\n        self.mask_decay = float(mask_decay)\n        self.mask_boost = float(mask_boost)\n\n        self.cov_probe_count = int(cov_probe_count)\n        self.cov_shrink = float(cov_shrink)\n        self.cov_pow = float(cov_pow)\n\n        self.heavy_tail_prob = float(heavy_tail_prob)\n        self.heavy_tail_scale = float(heavy_tail_scale)\n\n        self.follow_edge_p = float(follow_edge_p)\n        self.ucb_c = float(ucb_c)\n\n        self.elite_fraction = float(elite_fraction)\n        self.local_refine_prob = float(local_refine_prob)\n\n        self.verbose = bool(verbose)\n\n        if isinstance(rng, np.random.Generator):\n            self.rng = rng\n        else:\n            self.rng = np.random.default_rng(rng)\n\n    def __call__(self, func):\n        # read bounds if provided, else default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n            if ub.shape == ():\n                ub = np.full(self.dim, float(ub))\n            lb = lb.reshape(self.dim)\n            ub = ub.reshape(self.dim)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        box_range = ub - lb\n        max_range = float(np.max(box_range))\n\n        def project(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        budget_remaining = int(self.budget)\n        f_best = np.inf\n        x_best = None\n\n        # evaluation wrapper that strictly enforces budget\n        def evalf(x):\n            nonlocal budget_remaining, f_best, x_best\n            if budget_remaining <= 0:\n                return None\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            budget_remaining -= 1\n            fx = func(x)\n            if fx is not None and fx < f_best:\n                f_best = float(fx)\n                x_best = x.copy()\n            return fx\n\n        if budget_remaining <= 0:\n            return f_best, x_best\n\n        # initial random seeding\n        n_init = min(self.init_samples, budget_remaining)\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            fx = evalf(x)\n            if fx is None:\n                break\n\n        # ensure at least one evaluated point exists\n        if x_best is None:\n            if budget_remaining > 0:\n                x = self.rng.uniform(lb, ub)\n                if evalf(x) is None:\n                    return f_best, x_best\n            else:\n                return f_best, x_best\n\n        # repositories\n        disp_mem = []           # list of recent displacement vectors (x_new - x_old)\n        disp_from = []          # origins corresponding to each displacement\n        mask_stats = {}         # mask tuple -> {'wins','trials','step','trust','mom'}\n        per_dim_weight = np.ones(self.dim, dtype=float)  # importance per-dimension\n\n        base_step = self.seed_step * max(1.0, max_range)\n\n        # waypoint graph\n        waypoints = []  # nodes: {'x','f','age'}\n        edge_stats = {}  # (i,j) -> {'wins','trials'}\n        def add_waypoint(x, f):\n            nonlocal waypoints, edge_stats\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            if len(waypoints) == 0:\n                waypoints.append({'x': x.copy(), 'f': float(f), 'age': 0})\n                return 0\n            pts = np.stack([w['x'] for w in waypoints], axis=0)\n            dists = np.linalg.norm(pts - x.reshape(1, -1), axis=1)\n            i = int(np.argmin(dists))\n            if dists[i] < 1e-8 + 1e-3 * max_range:\n                if f < waypoints[i]['f']:\n                    waypoints[i]['f'] = float(f)\n                    waypoints[i]['x'] = x.copy()\n                waypoints[i]['age'] = 0\n                return i\n            if len(waypoints) >= self.max_waypoints:\n                # replace worst occasionally\n                worst_idx = int(np.argmax([w['f'] for w in waypoints]))\n                waypoints[worst_idx] = {'x': x.copy(), 'f': float(f), 'age': 0}\n                # drop incident edges\n                keys_to_drop = [k for k in edge_stats.keys() if k[0] == worst_idx or k[1] == worst_idx]\n                for k in keys_to_drop:\n                    edge_stats.pop(k, None)\n                return worst_idx\n            waypoints.append({'x': x.copy(), 'f': float(f), 'age': 0})\n            return len(waypoints) - 1\n\n        # initialize waypoint with current best\n        add_waypoint(x_best.copy(), f_best)\n\n        # helper: sample a new mask (sparse coordinate set)\n        def sample_new_mask(max_deg):\n            # sample degree biased to higher degrees via power-law\n            degrees = np.arange(1, max_deg + 1)\n            weights = degrees.astype(float) ** self.mask_degree_bias\n            weights /= weights.sum()\n            d = int(self.rng.choice(degrees, p=weights))\n            if d >= self.dim:\n                return tuple(range(self.dim))\n            return tuple(sorted(self.rng.choice(self.dim, size=d, replace=False)))\n\n        # pick existing mask or generate new, with occasional recombination/mutation\n        def pick_mask(cur_max_deg):\n            nonlocal mask_stats\n            # with probability exploit known masks\n            if len(mask_stats) > 0 and self.rng.random() < self.mask_exploit_prob:\n                keys = list(mask_stats.keys())\n                wins = np.array([mask_stats[k]['wins'] + 1e-9 for k in keys], dtype=float)\n                trials = np.array([mask_stats[k]['trials'] + 1e-9 for k in keys], dtype=float)\n                score = (wins / trials) * (trials ** 0.2)\n                score = score + 1e-6\n                probs = score / score.sum()\n                sel = self.rng.choice(len(keys), p=probs)\n                mask = tuple(keys[sel])\n                # occasional mutation\n                if self.rng.random() < self.mask_mutate_prob:\n                    # add or drop a coordinate\n                    mask_set = set(mask)\n                    if self.rng.random() < 0.5 and len(mask_set) < self.dim:\n                        # add\n                        picks = list(set(range(self.dim)) - mask_set)\n                        mask_set.add(self.rng.choice(picks))\n                    elif len(mask_set) > 1:\n                        # drop\n                        mask_set.remove(self.rng.choice(list(mask_set)))\n                    mask = tuple(sorted(mask_set))\n                # occasional recombine\n                if self.rng.random() < self.mask_recomb_prob and len(mask_stats) > 1:\n                    other = tuple(mask_stats.keys())[self.rng.integers(len(mask_stats))]\n                    if self.rng.random() < 0.5:\n                        mask = tuple(sorted(set(mask) | set(other)))  # union\n                    else:\n                        mask = tuple(sorted(set(mask) & set(other)))  # intersection (could be empty)\n                        if len(mask) == 0:\n                            # fall back to single random coordinate\n                            mask = (self.rng.integers(self.dim),)\n                # ensure not exceed cur_max_deg\n                if len(mask) > cur_max_deg:\n                    mask = tuple(sorted(mask)[:cur_max_deg])\n                return mask\n            else:\n                return sample_new_mask(cur_max_deg)\n\n        # try to stitch path i -> j with backtracking and smoothing\n        def try_path_stitch(i, j, max_steps=40, relax=0.2):\n            nonlocal disp_mem, disp_from, edge_stats, per_dim_weight\n            if budget_remaining <= 0:\n                return False\n            xi = waypoints[i]['x'].copy()\n            xj = waypoints[j]['x'].copy()\n            fi = waypoints[i]['f']\n            dist = np.linalg.norm(xj - xi)\n            if dist <= 0:\n                return False\n            # step_count roughly proportional to distance / (base small step)\n            small_step = base_step * (0.36 + 0.22 * self.rng.random())\n            n_steps = int(min(max_steps, max(2, np.ceil(dist / (small_step * (0.6 + 0.8 * self.rng.random()))))))\n            ts = np.linspace(0.0, 1.0, n_steps + 1)[1:]\n            last_f = fi\n            cur = xi.copy()\n            success = False\n            created_nodes = []\n            for t in ts:\n                if budget_remaining <= 0:\n                    break\n                # allow slight curved interpolation by adding small PCA-guided perturbation\n                pt = (1 - t) * xi + t * xj\n                if len(disp_mem) >= 6 and self.rng.random() < 0.36:\n                    # low-rank perturbation in dominant displacement directions\n                    D = np.stack(disp_mem[-min(len(disp_mem), self.mem_size):], axis=0)\n                    try:\n                        cov = np.cov(D.T, bias=True)\n                        vals, vecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                        top = np.argsort(-vals)[:min(3, len(vals))]\n                        noise = vecs[:, top] @ (0.04 * max_range * self.rng.normal(size=len(top)))\n                        pt = pt + relax * (1 - t) * noise\n                    except Exception:\n                        pass\n                x_try = project(pt)\n                fx = evalf(x_try)\n                if fx is None:\n                    break\n                # accept if improves relative to last accepted waypoint along path\n                if fx <= last_f + 1e-12:\n                    idx = add_waypoint(x_try.copy(), fx)\n                    created_nodes.append(idx)\n                    success = True\n                    # store displacement\n                    disp = (x_try - cur).copy()\n                    disp_mem.append(disp)\n                    disp_from.append(cur.copy())\n                    if len(disp_mem) > self.mem_size:\n                        disp_mem.pop(0); disp_from.pop(0)\n                    # update per-dimension weights increase for coordinates that moved\n                    moved = np.abs(disp) > (1e-12 + 1e-5 * max_range)\n                    per_dim_weight[moved] += 0.02\n                    last_f = fx\n                    cur = x_try.copy()\n                else:\n                    # small chance to accept mild uphill for exploration (simulated annealing tiny)\n                    if fx <= last_f + 1e-6 and self.rng.random() < 0.025:\n                        idx = add_waypoint(x_try.copy(), fx)\n                        created_nodes.append(idx)\n                        cur = x_try.copy()\n                        last_f = fx\n                    else:\n                        break\n            # update edge stats\n            key = (i, j)\n            if key not in edge_stats:\n                edge_stats[key] = {'wins': 0.0, 'trials': 0.0}\n            edge_stats[key]['trials'] += 1.0\n            if success:\n                edge_stats[key]['wins'] += 1.0\n            return success\n\n        # utility: synthesize low-rank covariance with shrinkage\n        def synthesize_cov():\n            if len(disp_mem) < 2:\n                return None\n            D = np.stack(disp_mem[-min(len(disp_mem), self.mem_size):], axis=0)\n            # weighted centering to prefer recent\n            n = D.shape[0]\n            weights = np.linspace(1.0, 1.0 + self.cov_pow, n)\n            wmean = (weights[:, None] * D).sum(axis=0) / weights.sum()\n            D0 = D - wmean\n            C = (D0.T * weights) @ D0 / max(1.0, n)\n            # shrink toward diagonal scaled by per_dim_weight\n            shrink = np.diag(np.diag(C)) * (1.0 - self.cov_shrink) + self.cov_shrink * np.eye(self.dim) * (np.mean(np.diag(C)) + 1e-12)\n            C = (1.0 - self.cov_shrink) * C + self.cov_shrink * shrink\n            # regularize\n            C += 1e-10 * np.eye(self.dim)\n            return C\n\n        # main progressive phases\n        phases_left = self.phase_count\n        for phase in range(self.phase_count):\n            if budget_remaining <= 0:\n                break\n            phases_left -= 1\n            if self.phase_count == 1:\n                cur_max_deg = min(self.max_degree, self.dim)\n            else:\n                cur_max_deg = 1 + int((phase / max(1, self.phase_count - 1)) * (self.max_degree - 1))\n                cur_max_deg = max(1, min(cur_max_deg, min(self.max_degree, self.dim)))\n\n            # allocate phase budget\n            if budget_remaining < 16:\n                iters = budget_remaining\n            else:\n                share = 0.30 if phase < self.phase_count - 1 else 0.18\n                iters = max(1, int(min(max(12, budget_remaining * share / (phases_left + 1)),\n                                       max(16, budget_remaining // (phases_left + 1)))))\n            iters = min(iters, budget_remaining)\n\n            # scale step down as phases progress\n            phase_scale = 0.66 ** phase\n            phase_base_step = base_step * (0.38 + 0.62 * phase_scale)\n\n            for it in range(iters):\n                if budget_remaining <= 0:\n                    break\n                # pick mask\n                mask = pick_mask(cur_max_deg)\n                mask_t = tuple(sorted(int(m) for m in mask))\n                # init mask stats\n                if mask_t not in mask_stats:\n                    # initial step scaled by sqrt of mask size (smaller masks -> larger per-coord)\n                    init_step = phase_base_step / max(1.0, np.sqrt(max(1, len(mask_t))))\n                    mask_stats[mask_t] = {'wins': 0.0, 'trials': 0.0,\n                                          'step': init_step,\n                                          'trust': 1.0, 'mom': 0.0}\n                st = mask_stats[mask_t]\n                st['trials'] += 1.0\n\n                # derive a recommended direction using disp_mem & weights if available\n                d = np.zeros(self.dim, dtype=float)\n                if len(disp_mem) >= 4:\n                    # gather displacements that overlap mask\n                    idxs = []\n                    for k,disp in enumerate(disp_mem):\n                        if np.any(np.abs(disp[list(mask_t)]) > 1e-12):\n                            idxs.append(k)\n                    if len(idxs) >= 2:\n                        Dsel = np.stack([disp_mem[k] for k in idxs], axis=0)\n                        try:\n                            cov = np.cov(Dsel.T, bias=True)\n                            vals, vecs = np.linalg.eigh(cov + 1e-12 * np.eye(self.dim))\n                            top = np.argsort(-vals)[:min(2, len(vals))]\n                            coeffs = self.rng.normal(scale=np.sqrt(np.maximum(vals[top], 1e-12)))\n                            delta = vecs[:, top] @ coeffs\n                            delta = delta / (np.linalg.norm(delta[list(mask_t)]) + 1e-12)\n                            d[list(mask_t)] = delta[list(mask_t)]\n                        except Exception:\n                            # fallback random on mask\n                            dm = self.rng.normal(size=len(mask_t))\n                            d[list(mask_t)] = dm / max(1e-12, np.linalg.norm(dm))\n                    else:\n                        dm = self.rng.normal(size=len(mask_t))\n                        d[list(mask_t)] = dm / max(1e-12, np.linalg.norm(dm))\n                else:\n                    dm = self.rng.normal(size=len(mask_t))\n                    d[list(mask_t)] = dm / max(1e-12, np.linalg.norm(dm))\n\n                # incorporate per-dimension weights to bias direction\n                d = d * (1.0 + 0.2 * (per_dim_weight / (per_dim_weight.mean() + 1e-12) - 1.0))\n                nd = np.linalg.norm(d)\n                if nd == 0:\n                    continue\n                d = d / nd\n\n                # effective step: mask-stored + phase scaling + trust\n                step = st['step'] * (0.9 + 0.3 * st['trust']) * (0.8 + 0.4 * phase_scale)\n                # with heavytail occasionally amplify\n                if self.rng.random() < self.heavy_tail_prob:\n                    tail = np.tan((self.rng.random() - 0.5) * np.pi * 0.9)\n                    step_eff = step * (1.0 + abs(tail) * self.rng.random())\n                else:\n                    step_eff = step\n\n                improved = False\n                candidate_points = []\n                # try in both signs\n                for sign in (+1.0, -1.0):\n                    if budget_remaining <= 0:\n                        break\n                    x_cand = project(x_best + sign * step_eff * d)\n                    fx = evalf(x_cand)\n                    if fx is None:\n                        break\n                    candidate_points.append((x_cand, fx))\n                    if fx < f_best - 1e-12:\n                        improved = True\n                        # record displacement and update mask stats\n                        disp = (x_cand - x_best).copy()\n                        disp_mem.append(disp); disp_from.append(x_best.copy())\n                        if len(disp_mem) > self.mem_size:\n                            disp_mem.pop(0); disp_from.pop(0)\n                        st['wins'] += 1.0\n                        st['trust'] = min(3.0, st['trust'] * 1.08 + 0.04)\n                        st['step'] = min(max_range * 1.5, st['step'] * self.mask_boost)\n                        # increment per-dimension weight\n                        moved = np.abs(disp) > (1e-12 + 1e-6 * max_range)\n                        per_dim_weight[moved] += 0.03\n                        # opportunistic geometric/extrapolation line search along same direction\n                        mult = 1.6\n                        for r in range(3):\n                            if budget_remaining <= 0:\n                                break\n                            x_try = project(x_best + sign * step_eff * mult * d)\n                            fx2 = evalf(x_try)\n                            if fx2 is None:\n                                break\n                            if fx2 < f_best - 1e-12:\n                                disp2 = (x_try - x_best).copy()\n                                disp_mem.append(disp2); disp_from.append(x_best.copy())\n                                if len(disp_mem) > self.mem_size:\n                                    disp_mem.pop(0); disp_from.pop(0)\n                                st['wins'] += 0.6\n                                st['step'] *= 1.06\n                                per_dim_weight[np.abs(disp2) > 1e-12] += 0.015\n                                mult *= 1.9\n                            else:\n                                break\n                        break  # stop signs loop when improvement found\n\n                if improved:\n                    # add waypoint for current best (promote graph)\n                    idx_new = add_waypoint(x_best.copy(), f_best)\n                    # attempt to connect new node to a promising other node\n                    if len(waypoints) > 1 and self.rng.random() < 0.66:\n                        cand_idxs = [j for j in range(len(waypoints)) if j != idx_new]\n                        scores = []\n                        for j in cand_idxs:\n                            dist = np.linalg.norm(waypoints[j]['x'] - waypoints[idx_new]['x'])\n                            key = (idx_new, j)\n                            es = edge_stats.get(key, {'wins': 0.0, 'trials': 0.0})\n                            score = dist * (1.0 + 1.0 / (1.0 + es['trials'])) * (1.0 + 0.4 * np.sqrt(es['wins'] + 1e-9))\n                            scores.append(score)\n                        if len(scores) > 0:\n                            probs = np.array(scores, dtype=float)\n                            probs = probs / probs.sum()\n                            tgt = self.rng.choice(cand_idxs, p=probs)\n                            try_path_stitch(idx_new, tgt, max_steps=28)\n                    continue\n                else:\n                    # failing: penalize and decay trust/step\n                    st['step'] *= self.mask_decay\n                    st['wins'] = max(0.0, st['wins'] - 0.02)\n                    st['trust'] = max(0.4, st['trust'] * 0.94)\n\n            # end-phase anisotropic synthesis probes\n            if budget_remaining <= 0:\n                break\n            C = synthesize_cov()\n            if C is not None and budget_remaining > 0:\n                # low-rank approximation\n                try:\n                    vals, vecs = np.linalg.eigh(C)\n                    vals = np.maximum(vals, 1e-12)\n                    order = np.argsort(-vals)\n                    topk = order[:min(self.dim, 6)]\n                    n_probes = min(self.cov_probe_count, budget_remaining)\n                    for _ in range(n_probes):\n                        if budget_remaining <= 0:\n                            break\n                        coeffs = self.rng.normal(scale=np.sqrt(vals[topk]), size=topk.size)\n                        delta = vecs[:, topk] @ coeffs\n                        # slight isotropic jitter\n                        delta += 0.03 * max_range * self.rng.normal(size=self.dim)\n                        x_try = project(x_best + 0.28 * delta)\n                        fx = evalf(x_try)\n                        if fx is None:\n                            break\n                        if fx < f_best - 1e-12:\n                            add_waypoint(x_try.copy(), fx)\n                            disp_mem.append((x_try - x_best).copy()); disp_from.append(x_best.copy())\n                            if len(disp_mem) > self.mem_size:\n                                disp_mem.pop(0); disp_from.pop(0)\n                except Exception:\n                    # fallback random local jitter\n                    for _ in range(min(4, budget_remaining)):\n                        x_try = project(x_best + 0.06 * max_range * self.rng.normal(size=self.dim))\n                        fx = evalf(x_try)\n                        if fx is None:\n                            break\n                        if fx < f_best - 1e-12:\n                            add_waypoint(x_try.copy(), fx)\n                            disp_mem.append((x_try - x_best).copy()); disp_from.append(x_best.copy())\n                            if len(disp_mem) > self.mem_size:\n                                disp_mem.pop(0); disp_from.pop(0)\n            # age waypoints\n            for w in waypoints:\n                w['age'] += 1\n\n        # final exploitation phase: graph-guided + anisotropic + quasi-local refinement\n        while budget_remaining > 0:\n            performed = False\n            # locate best node\n            if len(waypoints) > 0:\n                pts = np.stack([w['x'] for w in waypoints], axis=0)\n                dists = np.linalg.norm(pts - x_best.reshape(1, -1), axis=1)\n                node_best_idx = int(np.argmin(dists))\n                waypoints[node_best_idx]['f'] = f_best\n                waypoints[node_best_idx]['x'] = x_best.copy()\n                waypoints[node_best_idx]['age'] = 0\n\n            # sometimes follow a promising edge\n            if len(waypoints) > 1 and self.rng.random() < self.follow_edge_p and budget_remaining > 0:\n                cand = [j for j in range(len(waypoints)) if j != node_best_idx]\n                if len(cand) > 0:\n                    scores = []\n                    total_trials = sum(e.get('trials', 0) for e in edge_stats.values()) + 1e-12\n                    for j in cand:\n                        dist = np.linalg.norm(waypoints[j]['x'] - waypoints[node_best_idx]['x'])\n                        key = (node_best_idx, j)\n                        es = edge_stats.get(key, {'wins': 0.0, 'trials': 0.0})\n                        win_ratio = (es['wins'] / es['trials']) if es['trials'] > 0 else 0.5\n                        exploration = self.ucb_c * np.sqrt(np.log(1 + total_trials) / (1.0 + es['trials']))\n                        score = 0.75 * win_ratio + 0.25 * exploration + 0.0006 * dist\n                        scores.append(max(1e-12, score))\n                    probs = np.array(scores) / np.sum(scores)\n                    tgt = self.rng.choice(cand, p=probs)\n                    success = try_path_stitch(node_best_idx, tgt, max_steps=36)\n                    performed = True\n                    if not success and self.rng.random() < 0.22:\n                        try_path_stitch(tgt, node_best_idx, max_steps=20)\n            if performed:\n                continue\n\n            # else sample anisotropic or heavy-tailed jump\n            C = synthesize_cov()\n            if C is not None and self.rng.random() < 0.72:\n                try:\n                    z = self.rng.multivariate_normal(np.zeros(self.dim), C)\n                except Exception:\n                    z = self.rng.normal(size=self.dim) * 0.06 * max_range\n                x_try = project(x_best + 0.24 * z)\n            else:\n                # heavy tail; use t-like via normal / sqrt(U)\n                U = self.rng.random(size=self.dim) * 0.98 + 0.01\n                t = self.rng.normal(size=self.dim) / np.sqrt(U)\n                x_try = project(x_best + self.heavy_tail_scale * max_range * t)\n\n            fx = evalf(x_try)\n            if fx is None:\n                break\n            if fx < f_best - 1e-12:\n                idx_new = add_waypoint(x_try.copy(), fx)\n                # aggressive stitching both directions\n                if self.rng.random() < 0.92:\n                    try_path_stitch(node_best_idx, idx_new, max_steps=28)\n                if self.rng.random() < 0.46:\n                    try_path_stitch(idx_new, node_best_idx, max_steps=16)\n                disp_mem.append((x_try - x_best).copy()); disp_from.append(x_best.copy())\n                if len(disp_mem) > self.mem_size:\n                    disp_mem.pop(0); disp_from.pop(0)\n                continue\n            else:\n                # occasional local quasi-Newton-like refine on small subspace\n                if self.rng.random() < self.local_refine_prob and len(disp_mem) >= 4 and budget_remaining > 3:\n                    # build local linear model in top-K important dims\n                    k = min(max(2, int(self.dim * 0.12)), 8)\n                    top_dims = np.argsort(-per_dim_weight)[:k]\n                    # gather recent points near x_best (origins + endpoints)\n                    X = []\n                    Y = []\n                    for a,b in zip(disp_from[::-1], disp_mem[::-1]):\n                        if len(X) >= 12:\n                            break\n                        if np.linalg.norm(a - x_best) < 1.5 * max_range:\n                            X.append(a[top_dims])\n                            Y.append((a + b)[top_dims] - a[top_dims])\n                    if len(X) >= 3:\n                        X = np.array(X)\n                        Y = np.array(Y)\n                        # solve for linear mapping approximating local best step (ordinary least squares)\n                        try:\n                            # aim for average delta that reduced f in those moves: weight recent more\n                            wts = np.linspace(1.0, 2.4, X.shape[0])\n                            A = np.linalg.lstsq((X * wts[:, None]), (Y * wts[:, None]), rcond=None)[0]\n                            # estimate gradient-like direction by applying A to current local point\n                            loc = x_best[top_dims]\n                            delta_pred = A.T @ loc\n                            # normalize and attempt small step\n                            if np.linalg.norm(delta_pred) > 1e-12:\n                                delta_full = np.zeros(self.dim)\n                                delta_full[top_dims] = delta_pred / (np.linalg.norm(delta_pred) + 1e-12)\n                                xt = project(x_best + 0.06 * max_range * delta_full)\n                                fr = evalf(xt)\n                                if fr is None:\n                                    break\n                                if fr < f_best - 1e-12:\n                                    add_waypoint(xt.copy(), fr)\n                                    disp_mem.append((xt - x_best).copy()); disp_from.append(x_best.copy())\n                                    if len(disp_mem) > self.mem_size:\n                                        disp_mem.pop(0); disp_from.pop(0)\n                        except Exception:\n                            pass\n\n                # small randomized line search occasionally\n                if self.rng.random() < 0.18 and budget_remaining > 0:\n                    # pick mask from mask_stats favoring wins\n                    if len(mask_stats) > 0:\n                        keys = list(mask_stats.keys())\n                        wins = np.array([mask_stats[k]['wins'] + 1e-9 for k in keys], dtype=float)\n                        probs = wins / wins.sum()\n                        mask_t = keys[self.rng.choice(len(keys), p=probs)]\n                    else:\n                        mask_t = tuple([self.rng.integers(self.dim)])\n                    dm = self.rng.normal(size=len(mask_t))\n                    dm = dm / (np.linalg.norm(dm) + 1e-12)\n                    for m in (0.6, 1.2, 2.4):\n                        if budget_remaining <= 0:\n                            break\n                        xt = project(x_best + 0.08 * max_range * m * np.repeat(0.0, self.dim))\n                        # place delta only on mask dims\n                        delta = np.zeros(self.dim)\n                        delta[list(mask_t)] = dm\n                        xt = project(x_best + 0.08 * max_range * m * delta)\n                        fr = evalf(xt)\n                        if fr is None:\n                            break\n                        if fr < f_best - 1e-12:\n                            add_waypoint(xt.copy(), fr)\n                            disp_mem.append((xt - x_best).copy()); disp_from.append(x_best.copy())\n                            if len(disp_mem) > self.mem_size:\n                                disp_mem.pop(0); disp_from.pop(0)\n                            break\n\n        # final return\n        return f_best, x_best", "configspace": "", "generation": 21, "feedback": "The algorithm ProgressiveHighDegreeInteractionAmplifierPathStitchingV2 scored 0.668 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0d54d3eb-3d08-4a18-b13d-18a157b0be35"], "operator": null, "metadata": {"ast_features": {"Nodes": 6056.0, "Edges": 6055.0, "Max Degree": 52.0, "Min Degree": 1.0, "Mean Degree": 1.999669749009247, "Degree Variance": 2.245376377063742, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 9.324157913799349, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3228888532958991, "Depth Entropy": 2.3153981563657764, "Assortativity": 0.0, "Average Eccentricity": 21.63672391017173, "Diameter": 29.0, "Radius": 15.0, "Edge Density": 0.00016512549537648613, "Average Shortest Path": 12.820508504712963, "mean_complexity": 17.11111111111111, "total_complexity": 154.0, "mean_token_count": 624.1111111111111, "total_token_count": 5617.0, "mean_parameter_count": 4.222222222222222, "total_parameter_count": 38.0}, "feature_guidance_action": "increase", "feature_guidance_feature_name": "total_parameter_count", "performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9578275420604093}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9600174437361922}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9561301579482745}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.868230722212557}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8130966454263432}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.7830346349726374}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.9001630218321499}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.1688363227129337}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.16360209928147396}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.16177029372058682}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.1460403457605125}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.1830882837793032}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9874774050468443}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9876586114391152}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9880016210201735}], "aucs": [0.9578275420604093, 0.9600174437361922, 0.9561301579482745, 0.868230722212557, 0.8130966454263432, 0.7830346349726374, 0.9001630218321499, 0.1688363227129337, 0.16360209928147396, 0.16177029372058682, 0.1460403457605125, 0.1830882837793032, 0.9874774050468443, 0.9876586114391152, 0.9880016210201735]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-new-BBOB-4", "seed": 4}
{"method_name": "ES-guided-new", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided-new", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true, "feature_guided_mutation": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "558073b7-fa84-4c76-9baf-5ab82465e3ce", "fitness": 0.6553982675070219, "name": "ALPSPlusPlusRefinedV4", "description": "Ensemble-Trust ALPS++ V4 — an enhanced budget-aware adaptive multi-scale subspace optimizer that blends ensemble local surrogates (weighted quadratic + RBF), surrogate-guided Expected-Improvement proposals in adaptive active subspaces, lightweight subspace L-BFGS refinement, covariance-informed heavy-tailed restarts and richer tunable hyperparameters for higher expressive capacity.", "code": "import numpy as np\n\nclass ALPSPlusPlusRefinedV4:\n    \"\"\"\n    ALPS++ Refined V4\n\n    Key improvements over V3:\n    - More tunable parameters (increasing mean parameter count for model selection/ensembling).\n    - Ensemble surrogate: weighted quadratic in low-rank subspace + local RBF (Gaussian kernel) regression.\n      Each surrogate returns mean and an error estimate (used as pseudo-std). They are combined adaptively\n      using residual-based weights to form an ensemble prediction and an Expected Improvement (EI) metric.\n    - Surrogate-guided EI maximization with cheap sampling in adaptive active subspaces (PCA + gradient SVD).\n    - Lightweight subspace L-BFGS refinement using s/y pairs collected from successful steps (no extra evals).\n    - Adaptive covariance update (rank-μ style) built from elites and successful displacements,\n      used to sample proposals with correlated noise.\n    - Refined budget-aware proposal scheduling and dynamic evaluation allocation.\n    - Additional restart heuristics and richer annealing schedule.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 initial_step_factor=0.18,\n                 min_step_ratio=1e-6,\n                 levy_base_prob=0.06,\n                 memory_size=24,\n                 elite_size=None,\n                 init_samples=None,\n                 surrogate_window=16,\n                 rng=None,\n                 max_surrogate_dim=6,\n                 verbose=False,\n                 # Additional hyperparameters (increase parameterization)\n                 lb=-5.0, ub=5.0,\n                 rbf_kernel_scale=0.6,\n                 rbf_reg=1e-6,\n                 quad_reg=1e-6,\n                 ei_candidates=64,\n                 ei_top=6,\n                 trust_increase=1.25,\n                 trust_decrease=0.6,\n                 lbfgs_mem=10,\n                 cov_shrink=0.4,\n                 restart_cooldown=30):\n        \"\"\"\n        Args: (primary ones required per specification)\n            budget, dim: required.\n            Many other hyperparameters have sensible defaults but can be tuned externally.\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.initial_step_factor = float(initial_step_factor)\n        self.min_step_ratio = float(min_step_ratio)\n        self.levy_base_prob = float(levy_base_prob)\n        self.memory_size = int(memory_size)\n        self.elite_size = int(elite_size) if elite_size is not None else max(6, int(1.5 * dim))\n        self.init_samples = init_samples\n        self.surrogate_window = int(max(8, surrogate_window))\n        self.rng = rng if rng is not None else np.random.default_rng()\n        self.max_surrogate_dim = int(max(1, min(max_surrogate_dim, self.dim)))\n        self.verbose = bool(verbose)\n\n        # extra tunables (increase mean parameter count)\n        self.lb = lb\n        self.ub = ub\n        self.rbf_kernel_scale = float(rbf_kernel_scale)\n        self.rbf_reg = float(rbf_reg)\n        self.quad_reg = float(quad_reg)\n        self.ei_candidates = int(ei_candidates)\n        self.ei_top = int(ei_top)\n        self.trust_increase = float(trust_increase)\n        self.trust_decrease = float(trust_decrease)\n        self.lbfgs_mem = int(lbfgs_mem)\n        self.cov_shrink = float(cov_shrink)\n        self.restart_cooldown = int(restart_cooldown)\n\n    def __call__(self, func):\n        # Try to extract bounds if available; otherwise use provided lb/ub or default [-5,5]\n        try:\n            lb_try = np.asarray(func.bounds.lb, dtype=float)\n            ub_try = np.asarray(func.bounds.ub, dtype=float)\n            if lb_try.shape == () or lb_try.size == 1:\n                lb = np.full(self.dim, float(lb_try), dtype=float)\n            else:\n                lb = np.broadcast_to(lb_try, (self.dim,)).astype(float)\n            if ub_try.shape == () or ub_try.size == 1:\n                ub = np.full(self.dim, float(ub_try), dtype=float)\n            else:\n                ub = np.broadcast_to(ub_try, (self.dim,)).astype(float)\n        except Exception:\n            lb = np.full(self.dim, float(self.lb), dtype=float)\n            ub = np.full(self.dim, float(self.ub), dtype=float)\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # Budget-safe evaluation\n        calls = 0\n\n        def safe_eval(x):\n            nonlocal calls\n            if calls >= self.budget:\n                return None\n            calls += 1\n            x = np.asarray(x, dtype=float).copy()\n            try:\n                fx = float(func(x))\n            except Exception:\n                fx = float(np.inf)\n            return fx\n\n        # initial sampling\n        if self.init_samples is None:\n            n_init = int(max(8, min(6 * self.dim, self.budget // 12)))\n        else:\n            n_init = int(max(1, min(self.init_samples, self.budget - 1)))\n\n        history = []  # (f, x)\n        # seed midpoint\n        mid = (lb + ub) / 2.0\n        fm = safe_eval(mid)\n        if fm is None:\n            return float(np.inf), None\n        history.append((fm, mid.copy()))\n\n        # random initial spread with stratified intuition\n        n_init = min(n_init, max(1, self.budget - calls - 1))\n        for i in range(n_init):\n            if calls >= self.budget:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            f0 = safe_eval(x0)\n            if f0 is None:\n                break\n            history.append((f0, x0.copy()))\n\n        # small directional probes\n        probe_dirs = []\n        for i in range(self.dim):\n            e = np.zeros(self.dim)\n            e[i] = 1.0\n            probe_dirs.append(e)\n            probe_dirs.append(-e)\n        for d in probe_dirs[:min(len(probe_dirs), max(4, self.dim))]:\n            if calls >= self.budget:\n                break\n            x = clip(mid + 0.06 * (ub - lb) * d)\n            f = safe_eval(x)\n            if f is None:\n                break\n            history.append((f, x.copy()))\n\n        history.sort(key=lambda t: t[0])\n        elites = history[:self.elite_size].copy()\n        f_best, x_best = elites[0][0], elites[0][1].copy()\n\n        span = ub - lb\n        range_norm = max(1e-12, np.linalg.norm(span))\n        step = max(self.initial_step_factor * range_norm, 1e-12)\n        min_step = max(self.min_step_ratio * range_norm, 1e-12)\n\n        # memory and lbfgs s/y pairs\n        memory = []\n        lbfgs_s = []\n        lbfgs_y = []\n        last_improve_call = 0\n\n        # helpers\n        def insert_point(fv, xv):\n            nonlocal elites, f_best, x_best, history, last_improve_call\n            xv = np.asarray(xv, dtype=float).copy()\n            fv = float(fv)\n            history.append((fv, xv))\n            # limit history\n            if len(history) > 10000:\n                history[:] = history[-10000:]\n            # maintain elites\n            inserted = False\n            for i, (fe, xe) in enumerate(elites):\n                if fv < fe:\n                    elites.insert(i, (fv, xv.copy()))\n                    inserted = True\n                    break\n            if not inserted:\n                elites.append((fv, xv.copy()))\n            elites = elites[:self.elite_size]\n            if fv < f_best:\n                f_best = fv\n                x_best = xv.copy()\n                last_improve_call = calls\n\n        # orthonormalize\n        def orthonorm(vecs, max_dirs=128, tol=1e-12):\n            out = []\n            for v in vecs:\n                v = np.asarray(v, dtype=float).ravel()\n                nrm = np.linalg.norm(v)\n                if nrm < tol:\n                    continue\n                v = v / nrm\n                for u in out:\n                    proj = float(np.dot(u, v))\n                    v = v - proj * u\n                    if np.linalg.norm(v) < tol:\n                        break\n                nrm2 = np.linalg.norm(v)\n                if nrm2 >= tol:\n                    out.append(v / nrm2)\n                if len(out) >= max_dirs:\n                    break\n            return out\n\n        # weighted PCA for active subspace\n        def weighted_pca(elite_list, nvec):\n            if len(elite_list) < 2:\n                return [self.rng.normal(size=self.dim) for _ in range(nvec)]\n            pts = np.vstack([x for _, x in elite_list])\n            fs = np.array([f for f, _ in elite_list], dtype=float)\n            ranks = np.argsort(fs)\n            w = np.linspace(1.0, 0.4, len(fs))[np.argsort(ranks)]\n            w = np.maximum(w, 1e-6)\n            mean = (w[:, None] * pts).sum(axis=0) / max(1e-12, w.sum())\n            A = (pts - mean) * np.sqrt(w[:, None])\n            try:\n                U, S, Vt = np.linalg.svd(A, full_matrices=False)\n                vecs = [v / max(1e-12, np.linalg.norm(v)) for v in Vt[:nvec]]\n                return vecs\n            except Exception:\n                return [self.rng.normal(size=self.dim) for _ in range(nvec)]\n\n        # elite covariance (shrinkage)\n        def elite_cov(elite_list, shrink=None):\n            if shrink is None:\n                shrink = self.cov_shrink\n            if len(elite_list) < 2:\n                return np.eye(self.dim) * (1e-8)\n            pts = np.vstack([x for _, x in elite_list])\n            mean = pts.mean(axis=0)\n            A = pts - mean\n            cov = np.cov(A, rowvar=False, bias=False)\n            diag = np.diag(np.diag(cov))\n            cov = shrink * diag + (1 - shrink) * cov\n            cov += np.eye(self.dim) * (1e-10 + 1e-8 * np.trace(cov) / max(1, self.dim))\n            return cov\n\n        # estimate gradient from local history (weighted least squares)\n        def estimate_gradient(xc, k=None):\n            if k is None:\n                k = min(max(self.dim + 3, self.surrogate_window), len(history))\n            if len(history) < 4:\n                return None\n            rec = history[-min(len(history), max(12, 4 * k)):]\n            X = np.vstack([x for _, x in rec])\n            F = np.array([f for f, _ in rec], dtype=float)\n            dists = np.linalg.norm(X - xc, axis=1)\n            w = 1.0 / (1e-8 + dists)\n            w *= (np.max(F) - F + 1e-8)\n            idx = np.argsort(-w)[:min(len(w), max(2 * k, 6 * k))]\n            Xs = X[idx]\n            Fs = F[idx]\n            y = Fs - np.min(Fs)\n            D = Xs - xc\n            try:\n                W = np.sqrt(w[idx])[:, None]\n                Dw = D * W\n                yw = y * np.sqrt(w[idx])\n                DtD = Dw.T @ Dw\n                DtY = Dw.T @ yw\n                reg = np.eye(self.dim) * (1e-8 + 1e-6 * np.trace(DtD) / max(1, self.dim))\n                g = np.linalg.solve(DtD + reg, DtY)\n                if np.any(np.isnan(g)) or np.any(np.isinf(g)):\n                    return None\n                return g\n            except Exception:\n                return None\n\n        # fit low-rank quadratic surrogate (improved regularization)\n        def fit_quadratic_subspace(xc, max_dim=None):\n            if max_dim is None:\n                max_dim = min(self.max_surrogate_dim, self.dim)\n            n_pca = min(max_dim, max(1, len(elites)))\n            pcas = weighted_pca(elites, n_pca)\n            mix = pcas.copy()\n            for v in memory[:max(0, max_dim - len(mix))]:\n                mix.append(v)\n            while len(mix) < max_dim:\n                mix.append(self.rng.normal(size=self.dim))\n            Ulist = orthonorm(mix, max_dirs=max_dim)\n            if len(Ulist) == 0:\n                return None\n            U = np.vstack(Ulist[:max_dim])\n            k = U.shape[0]\n            cand = history[-min(len(history), 8 * max(8, self.surrogate_window)):]\n            Y = []\n            F = []\n            weights = []\n            for idx, (fv, x) in enumerate(cand):\n                y = U @ (x - xc)\n                Y.append(y)\n                F.append(float(fv))\n                dist = np.linalg.norm(x - xc)\n                w = 1.0 / (1e-8 + dist)\n                w *= (1.0 + 0.02 * idx)\n                weights.append(w)\n            Ymat = np.vstack(Y)\n            Fvec = np.array(F, dtype=float)\n            m = Ymat.shape[0]\n            # if too few points, fallback\n            if m < (1 + k):\n                return None\n            # build polynomial design: 1 + k + upper triangular quadratic\n            cols = 1 + k + k * (k + 1) // 2\n            G = np.zeros((m, cols), dtype=float)\n            G[:, 0] = 1.0\n            G[:, 1:1 + k] = Ymat\n            col = 1 + k\n            for i in range(k):\n                for j in range(i, k):\n                    G[:, col] = Ymat[:, i] * Ymat[:, j]\n                    col += 1\n            W = np.diag(weights)\n            try:\n                reg = np.eye(cols) * max(self.quad_reg, 1e-9)\n                beta = np.linalg.solve(G.T @ W @ G + reg, G.T @ W @ Fvec)\n                a0 = float(beta[0])\n                g_sub = beta[1:1 + k]\n                H_sub = np.zeros((k, k), dtype=float)\n                col = 1 + k\n                for i in range(k):\n                    for j in range(i, k):\n                        H_sub[i, j] = beta[col]\n                        H_sub[j, i] = H_sub[i, j]\n                        col += 1\n                g_full = U.T @ g_sub\n                H_full = U.T @ H_sub @ U\n                # compute residuals -> pseudo-std\n                pred = G @ beta\n                resid = Fvec - pred\n                sigma = max(1e-8, np.std(resid))\n                return {\n                    \"type\": \"quad\",\n                    \"U\": U, \"k\": k, \"a0\": a0, \"g_sub\": g_sub,\n                    \"H_sub\": H_sub, \"g_full\": g_full, \"H_full\": H_full,\n                    \"center\": xc.copy(), \"sigma\": sigma\n                }\n            except Exception:\n                return None\n\n        # fit local RBF (Gaussian kernel) surrogate: returns mean and pseudo-std\n        def fit_rbf_local(xc, max_pts=120):\n            if len(history) < 6:\n                return None\n            rec = history[-min(len(history), max_pts):]\n            X = np.vstack([x for _, x in rec])\n            F = np.array([f for f, _ in rec], dtype=float)\n            dists = np.linalg.norm(X - xc, axis=1)\n            # use local subset\n            idx = np.argsort(dists)[:min(len(dists), max(60, 8 * self.dim))]\n            Xs = X[idx]\n            Fs = F[idx]\n            # kernel width scaled by median dist\n            med = max(1e-6, np.median(np.linalg.norm(Xs - xc, axis=1)))\n            scale = self.rbf_kernel_scale * (med + 1e-8)\n            # build kernel matrix K_ij = exp(-||xi-xj||^2 / (2*scale^2))\n            XX = Xs\n            m = XX.shape[0]\n            if m < 4:\n                return None\n            D2 = np.sum((XX[:, None, :] - XX[None, :, :])**2, axis=2)\n            K = np.exp(-0.5 * D2 / (scale * scale))\n            reg = self.rbf_reg * np.eye(m)\n            try:\n                alpha = np.linalg.solve(K + reg, Fs)\n            except Exception:\n                try:\n                    alpha, *_ = np.linalg.lstsq(K + reg, Fs, rcond=None)\n                except Exception:\n                    return None\n            # predict at xc and estimate local variance by residuals\n            kx = np.exp(-0.5 * np.sum((XX - xc)**2, axis=1) / (scale * scale))\n            mean = float(np.dot(kx, alpha))\n            pred = K @ alpha\n            resid = Fs - pred\n            sigma = max(1e-8, np.std(resid))\n            return {\"type\": \"rbf\", \"alpha\": alpha, \"XX\": XX, \"scale\": scale, \"mean\": mean, \"sigma\": sigma, \"center\": xc.copy()}\n\n        # ensemble surrogate: fit both and combine by inverse-sigma weighting\n        def ensemble_predict(xc):\n            q = fit_quadratic_subspace(xc, max_dim=min(self.max_surrogate_dim, max(1, len(elites))))\n            r = fit_rbf_local(xc)\n            parts = []\n            if q is not None:\n                # predict at xc is a0\n                parts.append((q[\"a0\"], max(1e-8, q.get(\"sigma\", 1e-6))))\n            if r is not None:\n                parts.append((r[\"mean\"], max(1e-8, r.get(\"sigma\", 1e-6))))\n            if len(parts) == 0:\n                return None\n            vals = np.array([p[0] for p in parts])\n            sigs = np.array([p[1] for p in parts])\n            weights = 1.0 / (sigs + 1e-12)\n            w = weights / weights.sum()\n            mu = float(np.dot(w, vals))\n            # ensemble sigma: weighted plus variance of predictions\n            sigma = float(np.dot(w, sigs) + np.sqrt(max(0.0, np.var(vals))) + 1e-12)\n            return {\"mu\": mu, \"sigma\": sigma, \"quad\": q, \"rbf\": r}\n\n        # expected improvement (using pseudo-normal)\n        def expected_improvement(mu, sigma, fmin):\n            if sigma <= 0:\n                return max(0.0, fmin - mu)\n            z = (fmin - mu) / sigma\n            # use standard normal cdf/ pdf\n            # approximate cdf and pdf with numpy\n            from math import erf, sqrt, pi, exp\n            cdf = 0.5 * (1.0 + erf(z / sqrt(2.0)))\n            pdf = exp(-0.5 * z * z) / sqrt(2.0 * pi)\n            ei = sigma * (z * cdf + pdf)\n            return max(0.0, ei)\n\n        # propose candidates by sampling in adaptive active subspace and ranking by EI\n        def propose_ei_candidates(center, ncand=None):\n            if ncand is None:\n                ncand = max(8, min(self.ei_candidates, (self.budget - calls) // 2))\n            if ncand <= 0:\n                return []\n            # choose active subspace: combine PCA and gradient-based directions\n            pcas = weighted_pca(elites, min(4, self.dim))\n            g = estimate_gradient(center)\n            gdirs = []\n            if g is not None:\n                # top directions from gradient components\n                gdirs.append(g / (np.linalg.norm(g) + 1e-12))\n            mix = pcas + gdirs\n            while len(mix) < min(6, self.dim):\n                mix.append(self.rng.normal(size=self.dim))\n            U = np.vstack(orthonorm(mix, max_dirs=min(self.dim, len(mix))))\n            k = min(max(1, U.shape[0]), self.max_surrogate_dim)\n            U = U[:k]\n            candidates = []\n            # sample in subspace coordinates\n            for _ in range(ncand * 2):\n                alpha = self.rng.normal(scale=1.0, size=k)\n                # scale by trust radius and current step\n                scale_mult = self.rng.uniform(0.2, 1.6)\n                delta = (U.T @ alpha) * (step * scale_mult / max(1e-12, np.linalg.norm(alpha)))\n                x = clip(center + delta)\n                if np.allclose(x, center, atol=1e-14):\n                    continue\n                sp = ensemble_predict(x)\n                if sp is None:\n                    continue\n                ei = expected_improvement(sp[\"mu\"], sp[\"sigma\"], f_best)\n                candidates.append((ei, sp[\"mu\"], sp[\"sigma\"], x))\n                if len(candidates) >= ncand:\n                    break\n            candidates.sort(key=lambda t: -t[0])\n            return candidates[:min(len(candidates), self.ei_top)]\n\n        # lightweight L-BFGS two-loop using stored s/y (subspace)\n        def lbfgs_direction(g_full):\n            if len(lbfgs_s) == 0:\n                return -g_full\n            q = g_full.copy()\n            alphas = []\n            rhos = []\n            for s, y in zip(reversed(lbfgs_s), reversed(lbfgs_y)):\n                rho = 1.0 / (np.dot(y, s) + 1e-12)\n                rhos.append(rho)\n                alpha = rho * np.dot(s, q)\n                alphas.append(alpha)\n                q = q - alpha * y\n            # initial H0 scaling by last ys slope\n            if len(lbfgs_y) > 0:\n                ys = np.dot(lbfgs_y[-1], lbfgs_s[-1])\n                gamma = ys / (np.dot(lbfgs_y[-1], lbfgs_y[-1]) + 1e-12)\n            else:\n                gamma = 1.0\n            r = gamma * q\n            for s, y, rho, alpha in zip(lbfgs_s, lbfgs_y, reversed(rhos), reversed(alphas)):\n                beta = rho * np.dot(y, r)\n                r = r + s * (alpha - beta)\n            return -r\n\n        # simple line/backtracking respecting budget\n        def line_search(x0, d, s0, max_back=6):\n            if calls >= self.budget:\n                return None, None\n            s = float(s0)\n            best_local_f = None\n            best_local_x = None\n            tries = 0\n            while tries < max_back and calls < self.budget:\n                x_try = clip(x0 + s * d)\n                if np.allclose(x_try, x0, atol=1e-14):\n                    s *= 0.5\n                    tries += 1\n                    continue\n                f_try = safe_eval(x_try)\n                if f_try is None:\n                    break\n                insert_point(f_try, x_try)\n                if best_local_f is None or f_try < best_local_f:\n                    best_local_f = float(f_try)\n                    best_local_x = x_try.copy()\n                if f_try < f_best:\n                    return f_try, x_try.copy()\n                s *= 0.5\n                tries += 1\n            return best_local_f, best_local_x\n\n        # main loop\n        iter_count = 0\n        successes = []\n        stagnation = 0\n        trust_radius = step * 1.8\n        last_restart = -9999\n\n        while calls < self.budget and step >= min_step:\n            iter_count += 1\n            improved = False\n\n            # direction pool: coordinates, memory, PCA, randoms\n            dirs = []\n            for i in range(self.dim):\n                e = np.zeros(self.dim); e[i] = 1.0\n                dirs.append(e); dirs.append(-e)\n            for d in memory:\n                dirs.append(d); dirs.append(-d)\n            pcas = weighted_pca(elites, min(6, self.dim))\n            for v in pcas:\n                dirs.append(v); dirs.append(-v)\n            cov = elite_cov(elites)\n            try:\n                eigvals, eigvecs = np.linalg.eigh(cov)\n                order = np.argsort(-np.abs(eigvals))\n                for j in range(min(self.dim, 4)):\n                    v = eigvecs[:, order[j]]\n                    dirs.append(v); dirs.append(-v)\n            except Exception:\n                pass\n            n_rand = min(18 + self.dim, 48)\n            for _ in range(n_rand // 2):\n                v = self.rng.normal(size=self.dim)\n                dirs.append(v); dirs.append(-v)\n            dirs = orthonorm(dirs, max_dirs=min(6 * (self.dim + 2), 6 * self.dim + 20))\n\n            # probing\n            probes_per_dir_budget = max(1, (self.budget - calls) // (3 + len(dirs)))\n            for d in dirs:\n                if calls >= self.budget:\n                    break\n                d = np.asarray(d, dtype=float)\n                nd = np.linalg.norm(d)\n                if nd < 1e-14:\n                    continue\n                d /= nd\n                scales = [1.0, 0.6, -0.6, 1.3, -1.0]\n                max_probes_per_dir = min(len(scales), probes_per_dir_budget)\n                probes_done = 0\n                for s in scales:\n                    if probes_done >= max_probes_per_dir or calls >= self.budget:\n                        break\n                    t = float(s) * step\n                    x_try = clip(x_best + t * d)\n                    if np.allclose(x_try, x_best, atol=1e-14):\n                        continue\n                    f_try = safe_eval(x_try)\n                    if f_try is None:\n                        break\n                    insert_point(f_try, x_try)\n                    probes_done += 1\n                    if f_try < f_best:\n                        improved = True\n                        memory.insert(0, d.copy())\n                        if len(memory) > self.memory_size:\n                            memory.pop()\n                        # store lbfgs s/y: s = step*d, y=delta f gradient approx (finite diff)\n                        if calls > 2:\n                            # approximate gradient direction y via central differences on stored history if present\n                            # here we approximate y vector for curvature as difference in gradient proxies\n                            # simple: y = -(f_try - f_best) * d normalized (cheap heuristic)\n                            svec = (x_try - x_best)\n                            yvec = - (f_try - f_best) * d\n                            if np.linalg.norm(svec) > 1e-12 and np.linalg.norm(yvec) > 1e-12:\n                                lbfgs_s.append(svec.copy())\n                                lbfgs_y.append(yvec.copy())\n                                if len(lbfgs_s) > self.lbfgs_mem:\n                                    lbfgs_s.pop(0); lbfgs_y.pop(0)\n                        # opportunistic extrapolate\n                        if calls < self.budget:\n                            x_far = clip(x_try + 1.6 * t * d)\n                            if not np.allclose(x_far, x_try, atol=1e-14):\n                                f_far = safe_eval(x_far)\n                                if f_far is None:\n                                    break\n                                insert_point(f_far, x_far)\n                                if f_far < f_try:\n                                    memory.insert(0, d.copy())\n                                    if len(memory) > self.memory_size:\n                                        memory.pop()\n                        break\n\n                # parabolic polishing\n                if probes_done >= 3 and calls < self.budget:\n                    recent = history[-min(len(history), 80):]\n                    dir_pts = []\n                    for fv, x in recent:\n                        vec = x - x_best\n                        if np.linalg.norm(vec) < 1e-12:\n                            continue\n                        proj = float(np.dot(vec, d))\n                        if abs(proj) > 0:\n                            dir_pts.append((proj, x.copy(), fv))\n                    if len(dir_pts) >= 3:\n                        dir_pts_sorted = sorted(dir_pts, key=lambda t: t[2])\n                        chosen = []\n                        seen_proj = set()\n                        for pr, xx, ff in dir_pts_sorted:\n                            key = round(pr / (step * 0.1 + 1e-12), 4)\n                            if key in seen_proj:\n                                continue\n                            chosen.append((pr, xx, ff))\n                            seen_proj.add(key)\n                            if len(chosen) >= 3:\n                                break\n                        if len(chosen) >= 3:\n                            (ta, xa, fa), (tb, xb, fb), (tc, xc, fc) = chosen[:3]\n                            denom = (ta - tb) * (ta - tc) * (tb - tc)\n                            if abs(denom) > 1e-16:\n                                A = (ta * (fc - fb) + tb * (fa - fc) + tc * (fb - fa)) / denom\n                                B = (ta**2 * (fb - fc) + tb**2 * (fc - fa) + tc**2 * (fa - fb)) / denom\n                                if A != 0:\n                                    t_opt = -B / (2 * A)\n                                    t_opt = float(np.clip(t_opt, -4 * step, 4 * step))\n                                    x_opt = clip(x_best + t_opt * d)\n                                    if not np.allclose(x_opt, x_best, atol=1e-14) and calls < self.budget:\n                                        f_opt = safe_eval(x_opt)\n                                        if f_opt is None:\n                                            break\n                                        insert_point(f_opt, x_opt)\n                                        if f_opt < f_best:\n                                            improved = True\n                                            memory.insert(0, d.copy())\n                                            if len(memory) > self.memory_size:\n                                                memory.pop()\n\n            # covariance-informed proposals\n            if calls < self.budget:\n                cov = elite_cov(elites)\n                try:\n                    Uc, Svals, Vt = np.linalg.svd(cov)\n                    sqrt_cov = (Uc * np.sqrt(Svals)).dot(Vt)\n                except Exception:\n                    sqrt_cov = np.eye(self.dim)\n                ntries = min(6, max(1, (self.budget - calls) // 12))\n                for _ in range(ntries):\n                    if calls >= self.budget:\n                        break\n                    z = self.rng.normal(size=self.dim)\n                    vec = sqrt_cov @ z\n                    nrm = np.linalg.norm(vec)\n                    if nrm < 1e-12:\n                        continue\n                    dirv = vec / nrm\n                    scale = step * self.rng.uniform(0.4, 1.6)\n                    x_cov = clip(x_best + scale * dirv)\n                    if np.allclose(x_cov, x_best, atol=1e-14):\n                        continue\n                    f_cov = safe_eval(x_cov)\n                    if f_cov is None:\n                        break\n                    insert_point(f_cov, x_cov)\n                    if f_cov < f_best:\n                        improved = True\n                        memory.insert(0, dirv.copy())\n                        if len(memory) > self.memory_size:\n                            memory.pop()\n                        break\n\n            # surrogate-guided EI proposals (novel)\n            if calls < self.budget and len(history) >= max(8, self.surrogate_window // 2):\n                candidates = propose_ei_candidates(x_best)\n                for ei, mu, sigma, xcand in candidates:\n                    if calls >= self.budget:\n                        break\n                    # only evaluate top-k candidates but budget aware\n                    f_c = safe_eval(xcand)\n                    if f_c is None:\n                        break\n                    insert_point(f_c, xcand)\n                    if f_c < f_best:\n                        improved = True\n                        v = xcand - x_best\n                        nrm = np.linalg.norm(v)\n                        if nrm > 1e-12:\n                            memory.insert(0, v / nrm)\n                            if len(memory) > self.memory_size:\n                                memory.pop()\n                        # store lbfgs pair\n                        if np.linalg.norm(v) > 1e-12:\n                            svec = v\n                            yvec = np.zeros_like(v)\n                            # cheap approximate gradient difference: use predicted ensemble gradients if available\n                            g1 = estimate_gradient(x_best)\n                            g2 = estimate_gradient(xcand)\n                            if g1 is not None and g2 is not None:\n                                yvec = g2 - g1\n                            else:\n                                yvec = -(f_c - f_best) * (v / (np.linalg.norm(v) + 1e-12))\n                            lbfgs_s.append(svec)\n                            lbfgs_y.append(yvec)\n                            if len(lbfgs_s) > self.lbfgs_mem:\n                                lbfgs_s.pop(0); lbfgs_y.pop(0)\n\n            # surrogate-assisted subspace L-BFGS refinement\n            if calls < self.budget:\n                g_est = estimate_gradient(x_best)\n                if g_est is not None and np.linalg.norm(g_est) > 1e-12:\n                    dir_full = lbfgs_direction(g_est)\n                    dn = np.linalg.norm(dir_full)\n                    if dn > 1e-12:\n                        direction = dir_full / dn\n                        for alpha in [1.4, 0.7, 0.35]:\n                            if calls >= self.budget:\n                                break\n                            sc = step * alpha\n                            x_prop = clip(x_best + sc * direction)\n                            if np.allclose(x_prop, x_best, atol=1e-14):\n                                continue\n                            f_prop = safe_eval(x_prop)\n                            if f_prop is None:\n                                break\n                            insert_point(f_prop, x_prop)\n                            if f_prop < f_best:\n                                improved = True\n                                v = x_prop - x_best\n                                if np.linalg.norm(v) > 1e-12:\n                                    memory.insert(0, v / np.linalg.norm(v))\n                                    if len(memory) > self.memory_size:\n                                        memory.pop()\n                                break\n\n            # adapt step based on success rate and remaining budget\n            successes.append(1 if improved else 0)\n            if len(successes) > 60 + 6 * self.dim:\n                successes.pop(0)\n            succ_rate = sum(successes) / max(1, len(successes))\n            if succ_rate > 0.46:\n                step = min(step * 1.10, range_norm)\n                trust_radius = min(trust_radius * self.trust_increase, range_norm)\n            elif succ_rate < 0.07:\n                step = max(step * 0.72, min_step / 6.0)\n                trust_radius = max(trust_radius * self.trust_decrease, min_step)\n            # anneal with budget\n            remaining_ratio = max(0.02, float(self.budget - calls) / max(1, self.budget))\n            step = max(min(step, range_norm * remaining_ratio * 2.8), min_step)\n\n            if improved:\n                stagnation = 0\n            else:\n                stagnation += 1\n\n            # adaptive heavy-tailed restarts\n            levy_prob = self.levy_base_prob + 0.0014 * stagnation\n            do_jump = (stagnation > max(11, 3 * self.dim) and self.rng.random() < 0.9) or (self.rng.random() < levy_prob)\n            if do_jump and calls < self.budget and (iter_count - last_restart) > (self.restart_cooldown // 3):\n                last_restart = iter_count\n                ntries = min(10, max(1, (self.budget - calls) // 6))\n                accepted = False\n                for _ in range(ntries):\n                    if calls >= self.budget:\n                        break\n                    c = float(np.clip(self.rng.standard_cauchy(), -6.0, 6.0))\n                    dirv = self.rng.normal(size=self.dim)\n                    dirv /= max(1e-12, np.linalg.norm(dirv))\n                    jump_scale = step * (0.5 + 2.8 * self.rng.random())\n                    xj = clip(x_best + jump_scale * c * dirv)\n                    if np.allclose(xj, x_best, atol=1e-14):\n                        continue\n                    fj = safe_eval(xj)\n                    if fj is None:\n                        break\n                    insert_point(fj, xj)\n                    if fj < f_best:\n                        improved = True\n                        memory.insert(0, dirv.copy())\n                        if len(memory) > self.memory_size:\n                            memory.pop()\n                        accepted = True\n                        break\n                if not accepted and calls < self.budget:\n                    # local exploitation around random elite\n                    if len(elites) > 0:\n                        pick = elites[self.rng.integers(0, len(elites))][1]\n                        tries2 = min(8, max(1, (self.budget - calls) // 8))\n                        for _ in range(tries2):\n                            if calls >= self.budget:\n                                break\n                            noise = self.rng.normal(scale=step * self.rng.uniform(0.2, 1.2), size=self.dim)\n                            xr = clip(pick + noise)\n                            if np.allclose(xr, pick, atol=1e-14):\n                                continue\n                            fr = safe_eval(xr)\n                            if fr is None:\n                                break\n                            insert_point(fr, xr)\n                            if fr < f_best:\n                                improved = True\n                                vmem = xr - pick\n                                nvm = np.linalg.norm(vmem)\n                                if nvm > 1e-12:\n                                    memory.insert(0, vmem / nvm)\n                                    if len(memory) > self.memory_size:\n                                        memory.pop()\n                                break\n                    if not improved:\n                        step *= 0.8\n                        stagnation = 0\n\n            # moderate random restart if really stuck\n            if stagnation > max(24, 12 * self.dim) and calls < self.budget:\n                nrand = min(12, max(1, (self.budget - calls) // 4))\n                for _ in range(nrand):\n                    xr = self.rng.uniform(lb, ub)\n                    fr = safe_eval(xr)\n                    if fr is None:\n                        break\n                    insert_point(fr, xr)\n                step *= 0.55\n                stagnation = 0\n                successes = []\n\n            # progress print\n            if self.verbose and iter_count % 11 == 0:\n                print(f\"[ALPS++V4] iter {iter_count} calls {calls}/{self.budget} best {f_best:.6g} step {step:.3g} stagn {stagnation}\")\n\n        # final polishing: local gaussian probes + parabolic polishing from memory\n        if calls < self.budget:\n            local_tries = min(32, max(1, (self.budget - calls)))\n            for k_ in range(local_tries):\n                rscale = max(1e-14, 0.05 * step * (0.85 ** (k_ // 6)))\n                r = self.rng.normal(scale=rscale, size=self.dim)\n                xloc = clip(x_best + r)\n                if np.allclose(xloc, x_best, atol=1e-14):\n                    continue\n                floc = safe_eval(xloc)\n                if floc is None:\n                    break\n                insert_point(floc, xloc)\n\n            # parabolic polishing using memory directions\n            if len(memory) >= 2 and calls < self.budget:\n                for d in memory[:min(8, len(memory))]:\n                    if calls >= self.budget:\n                        break\n                    s = 0.5 * step\n                    pts = []\n                    for t in [-s, 0.0, s]:\n                        xq = clip(x_best + t * d)\n                        if np.allclose(xq, x_best, atol=1e-14):\n                            continue\n                        fq = safe_eval(xq)\n                        if fq is None:\n                            break\n                        insert_point(fq, xq)\n                        pts.append((t, xq.copy(), float(fq)))\n                    if len(pts) >= 3:\n                        (ta, xa, fa), (tb, xb, fb), (tc, xc, fc) = pts[:3]\n                        denom = (ta - tb) * (ta - tc) * (tb - tc)\n                        if abs(denom) > 1e-16:\n                            A = (ta * (fc - fb) + tb * (fa - fc) + tc * (fb - fa)) / denom\n                            B = (ta**2 * (fb - fc) + tb**2 * (fc - fa) + tc**2 * (fa - fb)) / denom\n                            if A != 0:\n                                t_opt = -B / (2 * A)\n                                t_opt = float(np.clip(t_opt, -2 * s, 2 * s))\n                                x_opt = clip(x_best + t_opt * d)\n                                if not np.allclose(x_opt, x_best, atol=1e-14) and calls < self.budget:\n                                    f_opt = safe_eval(x_opt)\n                                    if f_opt is None:\n                                        break\n                                    insert_point(f_opt, x_opt)\n\n        # finalize best\n        if len(elites) > 0:\n            elites.sort(key=lambda t: t[0])\n            f_best, x_best = float(elites[0][0]), elites[0][1].copy()\n\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 13, "feedback": "The algorithm ALPSPlusPlusRefinedV4 scored 0.655 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6b7f9f9a-102f-46e1-b81a-e7d81d9f879d"], "operator": null, "metadata": {"ast_features": {"Nodes": 8290.0, "Edges": 8289.0, "Max Degree": 50.0, "Min Degree": 1.0, "Mean Degree": 1.9997587454764776, "Degree Variance": 2.4388419200833478, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.743160690571049, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.33040363244536, "Depth Entropy": 2.2380036409217796, "Assortativity": 0.0, "Average Eccentricity": 20.382750301568155, "Diameter": 28.0, "Radius": 14.0, "Edge Density": 0.00012062726176115802, "Average Shortest Path": 11.784208874202312, "mean_complexity": 14.5, "total_complexity": 232.0, "mean_token_count": 457.0, "total_token_count": 7312.0, "mean_parameter_count": 3.4375, "total_parameter_count": 55.0}, "feature_guidance_action": "increase", "feature_guidance_feature_name": "mean_parameter_count", "performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.971061409486164}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9726745406943895}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9531430758861799}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.7757084007269415}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.7973716627652698}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.6957851461070181}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.16589700846680022}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.1944494962930854}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.19424581705342459}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.1477668965476605}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.19392334087629937}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.7852637752928073}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9947499238160437}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9944905589329168}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9944429596603263}], "aucs": [0.971061409486164, 0.9726745406943895, 0.9531430758861799, 0.7757084007269415, 0.7973716627652698, 0.6957851461070181, 0.16589700846680022, 0.1944494962930854, 0.19424581705342459, 0.1477668965476605, 0.19392334087629937, 0.7852637752928073, 0.9947499238160437, 0.9944905589329168, 0.9944429596603263]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-new-BBOB-1", "seed": 1}
{"method_name": "ES-guided-new", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided-new", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true, "feature_guided_mutation": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "068e2dfc-7577-4a91-8757-77f99bf5a0d3", "fitness": 0.6529143659052397, "name": "AdaptiveQuadraticSearch", "description": "Adaptive Quasi-Newton with local quadratic modeling and stochastic directional search — fit a local quadratic to nearby samples, take its minimizer as a trust-region step, and recover with adaptive random directional sampling when the model fails.", "code": "import numpy as np\n\nclass AdaptiveQuadraticSearch:\n    \"\"\"\n    Adaptive Quasi-Newtonian heuristic optimizer for box [-5,5]^d problems.\n    Main idea: build a local quadratic model from nearby evaluated points, take its minimizer\n    as a trust-region step (regularized), and when the model fails, perform adaptive\n    directional/stochastic sampling with decreasing variance. Budget-aware.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, init_samples=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # initial random samples (will be clipped to budget)\n        if init_samples is None:\n            self.init_samples = min(max(10 * self.dim, 20), max(5, self.budget // 10))\n        else:\n            self.init_samples = int(init_samples)\n        # internal state\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _build_quadratic_design(self, Z):\n        # Z: (m, d) matrix of z = x - x0\n        m, d = Z.shape\n        # number of terms: 1 (const) + d (linear) + d*(d+1)//2 (unique quadratic)\n        nquad = 1 + d + d * (d + 1) // 2\n        A = np.empty((m, nquad), dtype=float)\n        A[:, 0] = 1.0\n        A[:, 1:1 + d] = Z\n        col = 1 + d\n        # diag squared terms z_i^2\n        for i in range(d):\n            A[:, col] = Z[:, i] * Z[:, i]\n            col += 1\n        # off-diagonal z_i*z_j (i<j)\n        for i in range(d):\n            for j in range(i + 1, d):\n                A[:, col] = Z[:, i] * Z[:, j]\n                col += 1\n        return A\n\n    def _extract_H_and_g(self, coeffs, d):\n        # coeffs: length 1 + d + d(d+1)/2\n        # gradient b: linear coefficients\n        b = coeffs[1:1 + d].copy()\n        # build H from quadratic coefficients: recall model uses 0.5*z^T H z representation\n        H = np.zeros((d, d), dtype=float)\n        col = 1 + d\n        # diagonal entries: coeff corresponds to z_i^2 which equals 0.5*H_ii\n        for i in range(d):\n            gamma = coeffs[col]\n            H[i, i] = 2.0 * gamma\n            col += 1\n        # off-diagonals: coeff corresponds to H_ij (since z_i*z_j term equals H_ij * z_i*z_j)\n        for i in range(d):\n            for j in range(i + 1, d):\n                gamma = coeffs[col]\n                H[i, j] = gamma\n                H[j, i] = gamma\n                col += 1\n        return H, b\n\n    def __call__(self, func):\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        d = self.dim\n        # safety if bounds not provided or single values\n        if lb.size == 1:\n            lb = np.full(d, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(d, ub.item(), dtype=float)\n\n        # storage of evaluated points and values\n        X_hist = []\n        F_hist = []\n\n        evals = 0\n\n        # initial random sampling\n        n_init = min(self.init_samples, max(1, self.budget - 1))\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X_hist.append(x.copy())\n            F_hist.append(float(f))\n            if f < self.f_opt:\n                self.f_opt = float(f)\n                self.x_opt = x.copy()\n            if evals >= self.budget:\n                return self.f_opt, self.x_opt\n\n        # trust-region and sampling parameters\n        span = ub - lb\n        initial_trust = max(1e-3, 0.25 * np.linalg.norm(span))  # initial trust radius\n        trust_radius = initial_trust\n        sigma = 0.5 * np.mean(span)  # sampling std for stochastic moves\n        min_sigma = 1e-8 * np.mean(span)\n        max_sigma = np.max(span)\n        stagnation_counter = 0\n\n        # main loop\n        while evals < self.budget:\n            # choose nearby points around current best\n            X_arr = np.asarray(X_hist)\n            F_arr = np.asarray(F_hist)\n            if X_arr.shape[0] == 0:\n                # should not happen as we sampled initial points\n                x_center = self.rng.uniform(lb, ub)\n            else:\n                x_center = self.x_opt.copy()\n\n            # compute distances to center and pick nearest samples (at least enough for linear/quadratic)\n            dists = np.linalg.norm(X_arr - x_center, axis=1)\n            idx_sorted = np.argsort(dists)\n            # attempt quadratic if enough points\n            n_terms_quad = 1 + d + d * (d + 1) // 2\n            m_quad = min(X_arr.shape[0], max(n_terms_quad, 2 * n_terms_quad))\n            idx = idx_sorted[:m_quad]\n            X_sel = X_arr[idx]\n            F_sel = F_arr[idx]\n\n            # build Z = X_sel - x_center\n            Z = X_sel - x_center[None, :]\n            # weight points by distance to prioritize near ones\n            r_scale = max(np.median(dists[idx]) if idx.size > 0 else trust_radius, 1e-12)\n            weights = np.exp(-0.5 * (np.linalg.norm(Z, axis=1) / (r_scale + 1e-12)) ** 2)\n            weights /= (np.sum(weights) + 1e-12)\n\n            # Fit a quadratic (or fallback to linear) via weighted least squares\n            try:\n                A = self._build_quadratic_design(Z)\n                # apply weights\n                W_sqrt = np.sqrt(weights)[:, None]\n                Aw = A * W_sqrt\n                yw = (F_sel - np.mean(F_sel)) * W_sqrt.ravel()  # center y slightly for stability\n                coeffs, *_ = np.linalg.lstsq(Aw, yw, rcond=None)\n                # After lstsq we fitted to y - mean(F_sel); need to re-introduce offset into constant term\n                coeffs = coeffs.copy()\n                coeffs[0] += np.mean(F_sel)\n                H_est, g_est = self._extract_H_and_g(coeffs, d)\n                # ensure H_est is symmetric (numerical)\n                H_est = 0.5 * (H_est + H_est.T)\n                # regularize H to be invertible\n                reg = 1e-6 * (np.trace(H_est) / max(1.0, d))\n                H_reg = H_est + (reg + 1e-8) * np.eye(d)\n                # try solving Newton step: step = -H^{-1} g\n                try:\n                    step = -np.linalg.solve(H_reg, g_est)\n                except np.linalg.LinAlgError:\n                    # fallback to pseudo-inverse\n                    step = -np.linalg.pinv(H_reg) @ g_est\n                # scale step to trust radius\n                step_norm = np.linalg.norm(step)\n                if step_norm > 0:\n                    if step_norm > trust_radius:\n                        step = step * (trust_radius / step_norm)\n                # propose candidate\n                x_trial = np.clip(x_center + step, lb, ub)\n                # if candidate identical to center (numerical), nudge slightly\n                if np.allclose(x_trial, x_center):\n                    x_trial = np.clip(x_center + self.rng.normal(0, 1e-6, size=d), lb, ub)\n\n                f_trial = func(x_trial)\n                evals += 1\n                X_hist.append(x_trial.copy())\n                F_hist.append(float(f_trial))\n                improved = False\n                if f_trial < self.f_opt - 1e-12:\n                    self.f_opt = float(f_trial)\n                    self.x_opt = x_trial.copy()\n                    improved = True\n                    trust_radius = min(trust_radius * 1.5, max(np.linalg.norm(span), initial_trust))\n                    sigma = max(min_sigma, sigma * 0.8)\n                    stagnation_counter = 0\n                else:\n                    # if quadratic step didn't improve, shrink trust and try stochastic probing\n                    trust_radius *= 0.6\n                    sigma = min(max_sigma, sigma * 1.05)\n                    stagnation_counter += 1\n\n            except Exception:\n                # any fitting error -> fallback to random directional probing\n                improved = False\n                stagnation_counter += 1\n\n            # If no improvement from quadratic or after a few steps, perform directional stochastic sampling\n            if (not improved) and evals < self.budget:\n                # try a small batch of directional samples (budget-aware)\n                remaining = self.budget - evals\n                # we'll evaluate up to k candidates but not more than remaining\n                k = min(max(4, d + 2), remaining)\n                # propose k candidates: mix of axis probes and gaussian around best\n                candidates = []\n                for i in range(k):\n                    if (i % 3 == 0) and d <= 50:\n                        # axis-aligned probe: pick a coordinate and step +/- trust_radius\n                        coord = self.rng.integers(0, d)\n                        sign = 1 if self.rng.random() < 0.5 else -1\n                        step_vec = np.zeros(d)\n                        step_vec[coord] = sign * trust_radius * (0.5 + self.rng.random() * 0.5)\n                        cand = np.clip(self.x_opt + step_vec, lb, ub)\n                    else:\n                        # gaussian directional with anisotropy from H_est eigenvectors if available\n                        if 'H_reg' in locals():\n                            try:\n                                # create anisotropic covariance from H_est inverse (approx)\n                                eigvals, eigvecs = np.linalg.eigh(H_reg)\n                                # avoid tiny or negative eigenvalues\n                                eigvals = np.clip(eigvals, 1e-6, None)\n                                # sample in principal components with scales ~ 1/sqrt(eig)\n                                scales = 1.0 / np.sqrt(eigvals)\n                                z = self.rng.normal(size=d) * scales\n                                step_vec = eigvecs @ z\n                                step_vec = step_vec / (np.linalg.norm(step_vec) + 1e-12) * (self.rng.random() * trust_radius)\n                                cand = np.clip(self.x_opt + step_vec, lb, ub)\n                            except Exception:\n                                cand = np.clip(self.x_opt + self.rng.normal(0, sigma, size=d), lb, ub)\n                        else:\n                            cand = np.clip(self.x_opt + self.rng.normal(0, sigma, size=d), lb, ub)\n                    candidates.append(cand)\n                # evaluate candidates sequentially, opportunistic accept best\n                best_local_f = np.inf\n                best_local_x = None\n                for cand in candidates:\n                    if evals >= self.budget:\n                        break\n                    f_c = func(cand)\n                    evals += 1\n                    X_hist.append(cand.copy())\n                    F_hist.append(float(f_c))\n                    if f_c < best_local_f:\n                        best_local_f = float(f_c)\n                        best_local_x = cand.copy()\n                    if f_c < self.f_opt - 1e-12:\n                        self.f_opt = float(f_c)\n                        self.x_opt = cand.copy()\n                        # immediate accepted improvement -> decrease sigma and grow trust a bit\n                        sigma = max(min_sigma, sigma * 0.75)\n                        trust_radius = min(max(np.linalg.norm(span), trust_radius * 1.2), max(np.linalg.norm(span), initial_trust))\n                        stagnation_counter = 0\n                # if none improved but a local best is better than center of this probing, maybe update center for next model\n                if (best_local_x is not None) and (best_local_f < (F_arr[idx[0]] if idx.size > 0 else np.inf)):\n                    # adopt best_local as new center for subsequent modeling (but do not mark as global opt unless it's best)\n                    x_center = best_local_x.copy()\n\n            # adapt sigma conservatively on long stagnation\n            if stagnation_counter > 7:\n                sigma = min(max_sigma, sigma * 1.5)\n                trust_radius = max(1e-6, trust_radius * 0.9)\n                stagnation_counter = 0\n\n            # safety guards\n            sigma = np.clip(sigma, min_sigma, max_sigma)\n            trust_radius = max(trust_radius, 1e-8 * np.linalg.norm(span))\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveQuadraticSearch scored 0.653 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9951974949419887}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9951004597756534}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9951671754835536}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.572589946866149}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.5162750039975013}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.6112086253798967}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.2432360347945961}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.6898880155613084}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.44699022044940095}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.318016272685558}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.16923375656183126}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.2563846094771842}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9948199541275223}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9947866766058552}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9948212418705965}], "aucs": [0.9951974949419887, 0.9951004597756534, 0.9951671754835536, 0.572589946866149, 0.5162750039975013, 0.6112086253798967, 0.2432360347945961, 0.6898880155613084, 0.44699022044940095, 0.318016272685558, 0.16923375656183126, 0.2563846094771842, 0.9948199541275223, 0.9947866766058552, 0.9948212418705965], "ast_features": {"Nodes": 2136.0, "Edges": 2135.0, "Max Degree": 24.0, "Min Degree": 1.0, "Mean Degree": 1.999063670411985, "Degree Variance": 1.9644185989423333, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.279141104294478, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.327877136849546, "Depth Entropy": 2.217938367677785, "Assortativity": 0.0, "Average Eccentricity": 19.04681647940075, "Diameter": 25.0, "Radius": 13.0, "Edge Density": 0.00046816479400749064, "Average Shortest Path": 10.66129735371769, "mean_complexity": 10.25, "total_complexity": 41.0, "mean_token_count": 461.0, "total_token_count": 1844.0, "mean_parameter_count": 3.0, "total_parameter_count": 12.0}}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-new-BBOB-3", "seed": 3}

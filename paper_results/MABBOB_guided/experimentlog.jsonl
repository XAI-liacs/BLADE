{"method_name": "ES", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 4, "n_offspring": 16, "elitism": true}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "1075fb19-2d1a-4413-9005-a84684f12f29", "fitness": 0.8386488982250416, "name": "DARESv2_Plus", "description": "Hybrid low-rank directional ES with budget-aware trust-region quadratic surrogate, anisotropic step scaling, mirrored sampling, Lévy escapes and smart restarts — combines directional memory + cheap local quadratic modeling in a low-rank subspace to accelerate exploitation while keeping robust global exploration.", "code": "import numpy as np\n\nclass DARESv2_Plus:\n    \"\"\"\n    DARESv2_Plus\n\n    Improvements over the provided DARESv2:\n    - Explicit trust-region style local quadratic surrogate in a small directional subspace\n      (spanned by directional memory + principal components of recent elites). The surrogate\n      is a lightweight diagonal-quadratic + linear model in that subspace, fit with regularized\n      least-squares and used to propose a budget-aware model step.\n    - Adaptive anisotropic per-dimension scaling (cov_diag) with stabilized updates.\n    - Budget-aware usage of parabolic probes and surrogate evaluations (never exceed budget).\n    - Lévy-style occasional escape jumps that scale with stagnation to leave deceptive basins.\n    - Improved restart policy that preserves dominant memory directions and shrinks covariance.\n    - Careful mirrored/antithetic sampling and momentum-smoothed sigma adaptation.\n    \"\"\"\n\n    def __init__(self, budget, dim, population=None, init_sigma=0.2, mem_size=6,\n                 seed=None, max_elite=80, model_min_points=10):\n        \"\"\"\n        Required:\n        - budget: total function evaluations allowed\n        - dim: problem dimensionality\n\n        Optional:\n        - population: initial population size heuristic override\n        - init_sigma: initial step-size fraction of domain mean span\n        - mem_size: number of directional memory vectors to keep (low-rank)\n        - seed: RNG seed\n        - max_elite: number of elite points to keep for surrogate fitting\n        - model_min_points: minimum elite points to attempt surrogate fit\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.population = population\n        self.init_sigma = float(init_sigma)\n        self.mem_size = int(mem_size)\n        self.seed = seed\n        self.max_elite = int(max_elite)\n        self.model_min_points = max(6, int(model_min_points))\n        self.rng = np.random.RandomState(seed)\n\n    def _get_bounds(self, func):\n        try:\n            lb = np.array(func.bounds.lb, dtype=float)\n            ub = np.array(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        return lb, ub\n\n    def _orthonormalize_memory(self, M):\n        # Orthonormalize rows of M via Gram-Schmidt -> return rows as orthonormal basis\n        if len(M) == 0:\n            return np.zeros((0, self.dim))\n        A = np.array(M, dtype=float)\n        Q = []\n        for i in range(A.shape[0]):\n            v = A[i].copy()\n            for q in Q:\n                v -= np.dot(q, v) * q\n            nrm = np.linalg.norm(v)\n            if nrm > 1e-12:\n                Q.append(v / nrm)\n        if len(Q) == 0:\n            return np.zeros((0, self.dim))\n        return np.vstack(Q)\n\n    def _levy_jump(self, scale):\n        # Simple heavy-tailed jump using Cauchy scaled by provided scale\n        return self.rng.standard_cauchy(self.dim) * scale\n\n    def __call__(self, func):\n        evals = 0\n        budget = int(self.budget)\n        dim = int(self.dim)\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span_mean = float(np.mean(np.abs(span)))\n        span_max = float(np.max(np.abs(span)))\n        span_mean = max(span_mean, 1e-12)\n\n        # safe evaluator that enforces budget\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return None\n            # ensure contiguous float64\n            x = np.asarray(x, dtype=float)\n            val = float(func(x))\n            evals += 1\n            return val\n\n        # Initialize a small random sample to seed mean and best\n        init_n = min(max(10, 2 * dim), max(1, budget // 12))\n        f_opt = np.inf\n        x_opt = None\n        samples = []\n        for _ in range(init_n):\n            if evals >= budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = safe_eval(x)\n            if f is None:\n                break\n            samples.append((x.copy(), f))\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n\n        # If nothing evaluated yet (very small budget), return trivial\n        if evals >= budget:\n            return f_opt, x_opt\n\n        if x_opt is None:\n            mean = self.rng.uniform(lb, ub)\n            f_mean = safe_eval(mean)\n            if f_mean is None:\n                return f_opt, x_opt\n            f_opt = f_mean\n            x_opt = mean.copy()\n        else:\n            mean = x_opt.copy()\n            # ensure f_mean known\n            f_mean = f_opt\n\n        # population settings\n        if self.population is None:\n            lam = int(max(4, 4 + int(3 * np.log(max(1, dim)))))\n        else:\n            lam = int(self.population)\n        lam = min(lam, max(2, budget // 20))\n        if lam % 2 == 1:\n            lam += 1\n        lam_min = 2\n        lam_max = max(4, min(300, budget // 4))\n\n        # recombination weights\n        mu = max(1, lam // 2)\n        def compute_weights(mu):\n            ranks = np.arange(1, mu + 1)\n            w = np.log(mu + 0.5) - np.log(ranks)\n            w = np.maximum(w, 0.0)\n            if w.sum() <= 0:\n                w = np.ones(mu)\n            return w / w.sum()\n        weights = compute_weights(mu)\n\n        # strategy state\n        sigma = max(1e-12, self.init_sigma * span_mean)\n        sigma_min = 1e-12\n        sigma_max = span_max * 4.0 if span_max > 0 else 1.0\n\n        cov_diag = np.ones(dim)\n        cov_eps = 1e-12\n        cov_beta = 0.06  # EMA for covariance diag\n        rank_beta = 0.5\n        momentum = 0.65\n\n        success_target = 0.2\n        success_increase = 1.18\n        success_decrease = 0.86\n\n        # memory and archive\n        mem = []\n        mem_capacity = max(1, min(self.mem_size, dim))\n        elite_X = [x.copy() for x, f in samples]  # historical elites (positions)\n        elite_F = [f for x, f in samples]\n        # keep recent samples separately for PCA\n        recent_X = [x.copy() for x, f in samples]\n        recent_F = [f for x, f in samples]\n\n        stagnation_limit = max(8, int(25 + dim / 2))\n        no_improve = 0\n        prev_best = f_opt\n\n        # trust-region parameters for surrogate proposals\n        trust_radius = sigma * (1.0 + np.sqrt(dim))\n        trust_shrink = 0.7\n        trust_expand = 1.2\n        trust_min = 1e-8 * span_mean\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n\n            # dynamic lam adaptation: expand population if stagnating, shrink when progressing\n            if no_improve > 0 and no_improve % max(1, (stagnation_limit // 3)) == 0:\n                lam = min(lam * 2, lam_max)\n            else:\n                lam = max(lam_min, int(lam * (0.999 if no_improve == 0 else 1.0)))\n            if lam % 2 == 1:\n                lam += 1\n            lam = max(2, min(lam, remaining))  # cannot exceed remaining evals\n            mu = max(1, lam // 2)\n            weights = compute_weights(mu)\n\n            half = lam // 2\n\n            # generate mirrored samples with memory bias\n            if len(mem) > 0:\n                Z_core = self.rng.randn(half, dim)\n                bias_strength = 0.6 / max(1.0, len(mem))\n                mem_mat = np.array(mem)  # (k, dim)\n                coeff = self.rng.randn(half, mem_mat.shape[0]) * bias_strength\n                bias_part = coeff.dot(mem_mat)\n                Z = Z_core + bias_part\n            else:\n                Z = self.rng.randn(half, dim)\n\n            scales = sigma * np.sqrt(cov_diag + cov_eps)\n            X = mean + np.vstack([Z, -Z]) * scales[np.newaxis, :]\n\n            # clip\n            np.clip(X, lb, ub, out=X)\n\n            lam_eff = X.shape[0]\n            fs = np.full(lam_eff, np.inf)\n            X_evaluated = []\n            Z_evaluated = []\n\n            # sequentially evaluate candidates until budget\n            for i in range(lam_eff):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = safe_eval(xi)\n                if fi is None:\n                    break\n                fs[i] = fi\n                X_evaluated.append(xi.copy())\n                Z_evaluated.append((Z[i % half].copy() if i < half else (-Z[i - half].copy())))\n                # improvement found -> cheap parabolic probe (budget-aware)\n                if fi < f_opt - 1e-16:\n                    f_opt = fi\n                    x_opt = xi.copy()\n                    elite_X.append(xi.copy())\n                    elite_F.append(fi)\n                    recent_X.append(xi.copy())\n                    recent_F.append(fi)\n                    # parabolic probe along xi - mean direction (small exploitation)\n                    dir_vec = xi - mean\n                    dnorm = np.linalg.norm(dir_vec)\n                    if dnorm > 1e-12 and evals < budget:\n                        dir_unit = dir_vec / dnorm\n                        # probe further and closer points, but budget-aware: only one more probe if room\n                        # choose a farther probe factor adaptively\n                        probe_factor = 1.4 if remaining > 3 else 1.2\n                        probe = xi + dir_unit * (dnorm * (probe_factor - 1.0))\n                        np.clip(probe, lb, ub, out=probe)\n                        fp = safe_eval(probe)\n                        if fp is not None:\n                            # simple quadratic fit using points (0,f_mean),(s,fi),(s2,fp)\n                            s1 = dnorm\n                            s2 = dnorm * probe_factor\n                            y0 = f_mean\n                            y1 = fi\n                            y2 = fp\n                            # build system as in previous algorithm with regularization guard\n                            try:\n                                A = np.array([[s1 * s1, s1, 1.0],\n                                              [s2 * s2, s2, 1.0],\n                                              [0.0, 0.0, 1.0]])\n                                b = np.array([y1, y2, y0])\n                                coeffs = np.linalg.solve(A, b)\n                                a = coeffs[0]\n                                b_lin = coeffs[1]\n                                if abs(a) > 1e-16:\n                                    s_opt = -b_lin / (2.0 * a)\n                                    if 0.0 < s_opt < s2 * 1.2 and evals < budget:\n                                        trial = mean + dir_unit * s_opt\n                                        np.clip(trial, lb, ub, out=trial)\n                                        ft = safe_eval(trial)\n                                        if ft is not None and ft < f_opt:\n                                            f_opt = ft\n                                            x_opt = trial.copy()\n                                            elite_X.append(trial.copy())\n                                            elite_F.append(ft)\n                            except Exception:\n                                pass\n\n                    # deposit direction into memory\n                    if dnorm > 1e-12:\n                        mem.append((xi - mean) / (dnorm))\n                        mem = list(self._orthonormalize_memory(mem))\n                        if len(mem) > mem_capacity:\n                            mem = list(mem[:mem_capacity])\n\n            if len(X_evaluated) == 0:\n                break\n\n            fs_arr = np.array([fs[i] for i in range(len(fs))])\n            idx_sorted = np.argsort(fs_arr)[:len(X_evaluated)]\n            top_idx = idx_sorted[:mu]\n            X_top = np.array([X_evaluated[i] for i in top_idx])\n            Z_top = np.array([Z_evaluated[i] for i in top_idx])\n            f_top = fs_arr[top_idx]\n\n            # ensure f_mean known\n            if f_mean is None or not np.isfinite(f_mean):\n                f_mean = safe_eval(mean)\n                if f_mean is None:\n                    f_mean = f_opt\n\n            # recombine\n            if mu == 1:\n                new_mean = X_top[0].copy()\n            else:\n                new_mean = np.sum((weights[:, np.newaxis] * X_top), axis=0)\n\n            # success measurement\n            success_count = np.sum(f_top < f_mean - 1e-12)\n            success_rate = float(success_count) / float(mu)\n\n            # sigma adaptation with momentum smoothing\n            adapt_factor = (success_increase if success_rate > success_target else success_decrease)\n            sigma_new = sigma * adapt_factor\n            sigma = float(max(sigma_min, min(sigma_max, momentum * sigma + (1.0 - momentum) * sigma_new)))\n\n            # update diag covariance via mean step and memory contributions\n            mean_step = new_mean - mean\n            if np.any(np.abs(mean_step) > 0):\n                normed = (mean_step / max(1e-12, sigma)) ** 2\n                rank_contrib = np.zeros(dim)\n                if len(mem) > 0:\n                    for i_m, d in enumerate(mem):\n                        w = rank_beta * (1.0 / (1 + i_m))\n                        rank_contrib += w * (d ** 2)\n                    # scale rank_contrib to sensible magnitude\n                    if np.sum(rank_contrib) > 0:\n                        rank_contrib = rank_contrib / np.sum(rank_contrib) * np.mean(rank_contrib + 1e-12) * len(mem)\n                cov_diag = (1 - cov_beta) * cov_diag + cov_beta * (normed + rank_contrib + 1e-6)\n                cov_diag = np.maximum(cov_diag, 1e-8)\n\n            # update mean and evaluate it if budget allows (keep consistency)\n            prev_mean = mean.copy()\n            prev_f_mean = f_mean\n            mean = new_mean.copy()\n            if evals < budget:\n                f_mean_new = safe_eval(mean)\n                if f_mean_new is not None:\n                    f_mean = f_mean_new\n                    # record if improved\n                    if f_mean < f_opt:\n                        f_opt = f_mean\n                        x_opt = mean.copy()\n                        elite_X.append(mean.copy())\n                        elite_F.append(f_mean)\n                        recent_X.append(mean.copy())\n                        recent_F.append(f_mean)\n            else:\n                f_mean = min(f_mean, f_opt)\n\n            # If recombination improved mean, add to memory\n            if f_mean < prev_f_mean - 1e-12:\n                step = mean - prev_mean\n                s_norm = np.linalg.norm(step)\n                if s_norm > 1e-12:\n                    mem.append(step / s_norm)\n                    mem = list(self._orthonormalize_memory(mem))\n                    if len(mem) > mem_capacity:\n                        mem = list(mem[:mem_capacity])\n\n            # maintain elite and recent pools\n            if len(elite_F) == 0 or (x_opt is not None and f_opt < np.inf):\n                if x_opt is not None and (len(elite_X) == 0 or not any(np.allclose(x_opt, ex) for ex in elite_X)):\n                    elite_X.append(x_opt.copy())\n                    elite_F.append(f_opt)\n            # trim elite\n            if len(elite_F) > self.max_elite:\n                idxs = np.argsort(elite_F)[:self.max_elite // 2]\n                elite_X = [elite_X[i] for i in idxs]\n                elite_F = [elite_F[i] for i in idxs]\n            # recent buffer trim\n            if len(recent_F) > max(2 * dim, 100):\n                recent_X = recent_X[-100:]\n                recent_F = recent_F[-100:]\n\n            # surrogate modeling in a low-rank subspace\n            # proceed only if enough elite points and some budget left for trial\n            attempt_model = (len(elite_F) >= self.model_min_points and evals + 2 < budget)\n            if attempt_model:\n                # Build subspace basis B: prioritize mem directions, then PCA on recent samples residuals\n                B_rows = []\n                mem_orth = self._orthonormalize_memory(mem)\n                for r in mem_orth:\n                    B_rows.append(r.copy())\n                # add PCA components if space left\n                k_needed = min(dim, max(1, mem_capacity + 2))\n                if len(B_rows) < k_needed:\n                    # center recent samples at current mean\n                    Xmat = np.array(recent_X)\n                    if Xmat.shape[0] >= 2:\n                        diffs = (Xmat - mean)\n                        # PCA via SVD of diffs\n                        try:\n                            U, S, Vt = np.linalg.svd(diffs, full_matrices=False)\n                            comps = Vt[:max(0, k_needed - len(B_rows))]\n                            for row in comps:\n                                # orthonormalize new comp against existing rows\n                                v = row.copy()\n                                for q in B_rows:\n                                    v -= np.dot(q, v) * q\n                                nrm = np.linalg.norm(v)\n                                if nrm > 1e-12:\n                                    B_rows.append(v / nrm)\n                                if len(B_rows) >= k_needed:\n                                    break\n                        except Exception:\n                            pass\n                # build B (k x dim), want orthonormal rows; if none, fallback to standard basis directions subset\n                if len(B_rows) == 0:\n                    # choose top coordinate directions by cov_diag\n                    idxs = np.argsort(-cov_diag)[:min(k_needed, dim)]\n                    for i in idxs:\n                        e = np.zeros(dim)\n                        e[i] = 1.0\n                        B_rows.append(e)\n                # final orthonormalize to be safe\n                B = self._orthonormalize_memory(B_rows)\n                k = B.shape[0]\n                if k > 0 and len(elite_F) >= max(self.model_min_points, k + 3):\n                    # prepare design matrix for diagonal-quadratic model in subspace:\n                    # f = a + b^T s + 0.5 * sum(c_i * s_i^2) + noise\n                    # For each elite point compute s = B.dot(x - mean)\n                    Xe = np.array(elite_X)\n                    Fe = np.array(elite_F)\n                    S = (B.dot((Xe - mean).T)).T  # shape (n_points, k)\n                    n_pts = S.shape[0]\n                    # Build design: [1, s1,...,sk, 0.5*s1^2,...,0.5*sk^2]\n                    A = np.zeros((n_pts, 1 + k + k))\n                    A[:, 0] = 1.0\n                    A[:, 1:1 + k] = S\n                    A[:, 1 + k:] = 0.5 * (S ** 2)\n                    # regularized least-squares solve\n                    lambda_reg = 1e-6 * (1.0 + np.var(Fe))\n                    try:\n                        ATA = A.T.dot(A) + np.eye(A.shape[1]) * lambda_reg\n                        ATb = A.T.dot(Fe)\n                        coeffs = np.linalg.solve(ATA, ATb)\n                        a0 = coeffs[0]\n                        b_vec = coeffs[1:1 + k]\n                        c_vec = coeffs[1 + k:]\n                        # propose optimum in subspace: solve gradient = b + c * s = 0 -> s = -b / c (elementwise)\n                        # handle near-zero c by damping\n                        denom = c_vec.copy()\n                        small_mask = np.abs(denom) < 1e-12\n                        denom[small_mask] = np.sign(denom[small_mask]) * 1e-12 + 1e-12\n                        s_opt = -b_vec / denom\n                        # limit step by trust radius in ambient space\n                        x_candidate = mean + (B.T.dot(s_opt))\n                        step_norm = np.linalg.norm(x_candidate - mean)\n                        # scale-back if too large\n                        tr = max(trust_min, trust_radius)\n                        if step_norm > tr:\n                            s_opt = s_opt * (tr / max(1e-12, step_norm))\n                            x_candidate = mean + B.T.dot(s_opt)\n                        # ensure within bounds\n                        np.clip(x_candidate, lb, ub, out=x_candidate)\n                        # evaluate candidate if we have budget\n                        if evals < budget:\n                            f_cand = safe_eval(x_candidate)\n                            if f_cand is not None:\n                                # if promising, accept and update trust radius\n                                if f_cand < f_opt:\n                                    f_opt = f_cand\n                                    x_opt = x_candidate.copy()\n                                    elite_X.append(x_candidate.copy())\n                                    elite_F.append(f_cand)\n                                    recent_X.append(x_candidate.copy())\n                                    recent_F.append(f_cand)\n                                    trust_radius = min(sigma * (1.0 + np.sqrt(dim)) * 4.0, trust_radius * trust_expand)\n                                else:\n                                    # if not improving, shrink trust region slightly\n                                    trust_radius = max(trust_min, trust_radius * trust_shrink)\n                    except Exception:\n                        # if model fails, continue\n                        pass\n\n            # stagnation book-keeping\n            if f_opt < prev_best - 1e-12:\n                no_improve = 0\n                prev_best = f_opt\n                # gentle shrink of sigma on success\n                sigma = max(sigma_min, sigma * 0.98)\n                trust_radius = max(trust_min, trust_radius * 1.05)\n            else:\n                no_improve += 1\n                # occasional Lévy jump when stagnating but we have budget\n                if no_improve > (stagnation_limit // 3) and (self.rng.rand() < min(0.25, 0.02 * no_improve)) and evals + 1 < budget:\n                    scale = span_mean * (1.0 + 0.5 * (no_improve / max(1, stagnation_limit)))\n                    jump = self._levy_jump(scale)\n                    candidate = mean + jump\n                    np.clip(candidate, lb, ub, out=candidate)\n                    fc = safe_eval(candidate)\n                    if fc is not None and fc < f_opt:\n                        f_opt = fc\n                        x_opt = candidate.copy()\n                        elite_X.append(candidate.copy())\n                        elite_F.append(fc)\n                        recent_X.append(candidate.copy())\n                        recent_F.append(fc)\n                        # moderate sigma increase to explore newly found basin\n                        sigma = min(sigma_max, sigma * 2.0)\n\n            # adaptive restart on long stagnation\n            if no_improve >= stagnation_limit:\n                no_improve = 0\n                # choose new center from elites or random\n                if len(elite_X) > 0:\n                    pick = self.rng.randint(len(elite_X))\n                    mean = elite_X[pick].copy() + self.rng.randn(dim) * 0.2 * span_mean\n                elif x_opt is not None:\n                    mean = x_opt.copy() + self.rng.randn(dim) * 0.2 * span_mean\n                else:\n                    mean = self.rng.uniform(lb, ub)\n                np.clip(mean, lb, ub, out=mean)\n                if evals < budget:\n                    fm = safe_eval(mean)\n                    if fm is not None:\n                        f_mean = fm\n                        if fm < f_opt:\n                            f_opt = fm\n                            x_opt = mean.copy()\n                            elite_X.append(mean.copy())\n                            elite_F.append(fm)\n                # increase sigma and reset some covariance memory but keep primary directions\n                sigma = min(sigma_max, max(sigma * 2.5, self.init_sigma * span_mean))\n                cov_diag = cov_diag * 0.6 + 0.4 * np.ones_like(cov_diag)\n                # preserve primary mem directions only\n                if len(mem) > 0:\n                    mem = list(self._orthonormalize_memory(mem)[:max(1, len(mem)//2)])\n                else:\n                    mem = []\n                # widen population to explore after restart\n                lam = min(lam_max, max(lam, 2 * lam))\n                if lam % 2 == 1:\n                    lam += 1\n                # reset trust radius\n                trust_radius = sigma * (1.0 + np.sqrt(dim))\n\n            # enforce numeric bounds on sigma and cov_diag\n            sigma = max(sigma_min, min(sigma, sigma_max))\n            cov_diag = np.clip(cov_diag, 1e-8, 1e8)\n\n            # final guard\n            if evals >= budget:\n                break\n\n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm DARESv2_Plus scored 0.839 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2b286bc1-c404-4680-9bdf-a8bb544023df"], "operator": null, "metadata": {"aucs": [0.3420057974784517, 0.9715308690175147, 0.9307225035276213, 0.9855528254054122, 0.9756889686916869, 0.9823118342189068, 0.29553427755682116, 0.950340087090205, 0.9790469211554996, 0.9737548981082974]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-MA_BBOB-2", "seed": 2}
{"method_name": "ES", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 4, "n_offspring": 16, "elitism": true}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "97b09f41-16dd-4b97-b046-9dcd1f56a34c", "fitness": 0.7436660854740809, "name": "AHLRDS_plus_v2", "description": "Multi-scale Ensemble-Surrogate Low-Rank Sampler — combines mirrored multi-scale low-rank sampling, a lightweight ensemble surrogate (ridge-linear + diagonal-quadratic) with uncertainty-aware trust-region proposals, momentum-adaptive per-eigen scaling, and clustered elite-aware covariance restarts for robust affine-robust continuous optimization.", "code": "import numpy as np\n\nclass AHLRDS_plus_v2:\n    \"\"\"\n    AHLRDS_plus_v2\n    - Multi-scale mirrored low-rank exploration (different effective sigmas per generation).\n    - Ensemble surrogate: ridge linear + diagonal-quadratic approximation built from an elite neighborhood.\n    - Uncertainty-aware trust-region proposals: surrogate proposals only evaluated when predicted improvement\n      exceeds uncertainty-scaled threshold.\n    - Momentum-adaptive per-eigen scaling to keep directions \"warm\" across iterations.\n    - Elite clustering and covariance-aware informed restarts (sample around several elites).\n    - Budget-respecting: never exceeds allowed function evaluations.\n    \"\"\"\n    def __init__(self, budget, dim, pop=None, sigma0=None, seed=None,\n                 mem_size=None, rank_max=None, levy_prob=0.03):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop = None if pop is None else int(pop)\n        self.sigma0 = None if sigma0 is None else float(sigma0)\n        self.seed = None if seed is None else int(seed)\n        self.rank_max = None if rank_max is None else int(rank_max)\n        self.levy_prob = float(levy_prob)\n        self.mem_size = mem_size if mem_size is not None else min(300, 30 + 12 * self.dim)\n\n        # best seen\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.RandomState(None if self.seed is None else (self.seed + 13579))\n\n        # attempt to read bounds, else default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0\n            ub = 5.0\n        if lb.shape == ():\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == ():\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        rng_uniform = rng.uniform\n        search_range = ub - lb\n        avg_range = float(np.mean(np.abs(search_range)) + 1e-12)\n\n        def reflect(x):\n            # sawtooth reflection to avoid boundary bias\n            r = ub - lb\n            r_safe = np.where(r == 0.0, 1.0, r)\n            x_rel = (x - lb) / r_safe\n            x_rel = np.mod(x_rel, 2.0)\n            mask = x_rel > 1.0\n            x_rel[mask] = 2.0 - x_rel[mask]\n            return lb + x_rel * r_safe\n\n        # initial sigma\n        if self.sigma0 is None:\n            sigma = 0.2 * max(1e-12, avg_range)\n        else:\n            sigma = float(self.sigma0) * max(1e-12, avg_range)\n\n        # initialize center\n        evals = 0\n        center = rng_uniform(lb, ub)\n        # safe evaluation wrapper\n        def eval_point(x):\n            nonlocal evals, center\n            if evals >= self.budget:\n                return None\n            xr = reflect(x)\n            fx = func(xr.copy()); evals += 1\n            # update global best\n            if fx < self.f_opt:\n                self.f_opt = float(fx)\n                self.x_opt = xr.copy()\n            return xr, float(fx)\n\n        res = eval_point(center)\n        if res is None:\n            # nothing to do\n            if self.x_opt is None:\n                return float(np.inf), np.zeros(self.dim)\n            return float(self.f_opt), self.x_opt.copy()\n        center, f_center = res\n        self.f_opt = float(f_center); self.x_opt = center.copy()\n\n        # dynamic population\n        default_lam = max(8, int(6 + np.floor(4 * np.log(max(1, self.dim)))))\n        if self.pop is None:\n            lam = min(default_lam + int(np.sqrt(self.budget) / 12), max(default_lam, 3 * default_lam))\n        else:\n            lam = max(4, int(self.pop))\n        if lam % 2 == 1:\n            lam += 1\n        lam = max(4, lam)\n        mu = max(1, lam // 2)\n\n        # weights\n        ranks = np.arange(1, mu + 1)\n        weights = np.log(mu + 0.5) - np.log(ranks)\n        weights = np.maximum(weights, 0.0)\n        if np.sum(weights) <= 0:\n            weights = np.ones(mu) / mu\n        else:\n            weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # adaptation hyperparams\n        c_s = 0.3 * min(1.0, mu_eff / (self.dim + 1.0))\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (self.dim + 1.0)) - 1.0)\n        target_ps = np.sqrt(self.dim) * (1.0 - 1.0 / (4.0 * self.dim) + 1.0 / (21.0 * (self.dim ** 2)))\n\n        # z-space covariance components: symmetric positive definite\n        Cz = np.eye(self.dim)\n        Cdiag = np.ones(self.dim)\n        B = np.eye(self.dim)\n        D = np.ones(self.dim)\n\n        eps = 1e-12 * (avg_range ** 2 + 1e-12)\n        ps = np.zeros(self.dim)\n        pc = np.zeros(self.dim)\n\n        # memory for surrogate & elites\n        mem_X = []\n        mem_F = []\n        mem_Z = []\n\n        # elite archive (keeps top-K)\n        eliteK = min(8 + self.dim // 8, 20)\n        elites = []  # list of tuples (f,x,Cz_snapshot)\n\n        # per-eigen momentum multipliers\n        eigen_scale = np.ones(self.dim)\n        eigen_momentum = np.ones(self.dim) * 0.6\n\n        # scheduling\n        last_eig = 0\n        eigen_update_period = max(6, int(4 + np.sqrt(self.dim)))\n        rank_cap = min(self.dim, max(1, int(self.dim // 4))) if self.rank_max is None else min(self.dim, int(self.rank_max))\n\n        # trust region parameters for surrogate proposals\n        trust_base = 1.2\n        trust_scale = 1.0\n        trust_decay = 0.95\n        min_trust = 0.2\n\n        # helper to update memories and elites safely\n        def record_evaluation(xr, fx, z_approx=None):\n            # xr: reflected x\n            nonlocal mem_X, mem_F, mem_Z, elites\n            mem_X.append(xr.copy())\n            mem_F.append(float(fx))\n            if z_approx is not None:\n                mem_Z.append(z_approx.copy())\n            else:\n                mem_Z.append(np.zeros(self.dim))\n            # maintain memory cap\n            if len(mem_X) > self.mem_size:\n                excess = len(mem_X) - self.mem_size\n                mem_X = mem_X[excess:]\n                mem_F = mem_F[excess:]\n                mem_Z = mem_Z[excess:]\n            # update elites (small archive)\n            elites.append((float(fx), xr.copy(), None))\n            elites = sorted(elites, key=lambda t: t[0])[:eliteK]\n\n        # initial record center\n        record_evaluation(center.copy(), f_center, z_approx=np.zeros(self.dim))\n\n        # multi-scale factors for sampling (minor, main, exploratory)\n        scale_factors = np.array([0.5, 1.0, 2.2])\n        scale_weights = np.array([0.25, 0.6, 0.15])\n        scale_weights = scale_weights / np.sum(scale_weights)\n\n        stall = 0\n        max_stall = max(12, 10 + self.dim // 3)\n        restart_count = 0\n        max_restarts = 8\n\n        # main optimization loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            # adapt lambda with remaining budget\n            cur_lambda = min(lam, remaining)\n            if cur_lambda % 2 == 1:\n                cur_lambda = max(2, cur_lambda - 1)\n            if cur_lambda <= 0:\n                break\n\n            # periodic eigendecomposition to set B,D and adaptive rank\n            if (evals - last_eig) >= max(1, eigen_update_period) or last_eig == 0:\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(Cz)\n                except np.linalg.LinAlgError:\n                    Cz = 0.5 * (Cz + Cz.T) + np.eye(self.dim) * eps\n                    eigvals, eigvecs = np.linalg.eigh(Cz)\n                order = np.argsort(-eigvals)\n                eigvals = np.maximum(eigvals[order], eps)\n                eigvecs = eigvecs[:, order]\n                # normalize to mean 1\n                mean_e = max(np.mean(eigvals), eps)\n                eig_norm = eigvals / mean_e\n                # adaptive rank by explained variance\n                cum = np.cumsum(eig_norm)\n                total = np.sum(eig_norm)\n                var_threshold = 0.88\n                r = int(np.searchsorted(cum, var_threshold * total) + 1)\n                r = min(max(1, r), rank_cap)\n                # build D (sqrt of eigen-norm for top r, rest set to 1)\n                top_vals = eig_norm.copy()\n                if len(top_vals) < self.dim:\n                    top_vals = np.pad(top_vals, (0, self.dim - len(top_vals)), 'constant', constant_values=1.0)\n                D = np.sqrt(np.maximum(top_vals, 1e-16))\n                B = eigvecs.copy()\n                # mild shrink to diagonal to avoid overfitting low-rank\n                diag_shrink = 0.12 + 0.5 * (1.0 / (1.0 + restart_count))\n                Cz = (B * (D ** 2)[None, :]) @ B.T\n                Cz = (1.0 - diag_shrink) * Cz + diag_shrink * np.diag(np.maximum(np.diag(Cz), 1e-16))\n                Cz = 0.5 * (Cz + Cz.T) + np.eye(self.dim) * eps\n                last_eig = evals\n                # adapt eigen_scale with momentum toward sqrt(eigvals)\n                new_scale = 1.0 + 0.6 * ((D - np.mean(D)) / (np.mean(D) + 1e-16))\n                eigen_scale = eigen_momentum * eigen_scale + (1.0 - eigen_momentum) * new_scale\n\n            # sample proportionally across scales, produce mirrored pairs\n            # compute per-scale counts (at least 2 per scale)\n            counts = np.maximum(2, np.floor(scale_weights * cur_lambda).astype(int))\n            # adjust to exactly cur_lambda\n            s = np.sum(counts)\n            i = 0\n            while s < cur_lambda:\n                counts[i % len(counts)] += 2  # keep mirrored parity\n                s = np.sum(counts)\n                i += 1\n            while s > cur_lambda:\n                for j in range(len(counts)):\n                    if counts[j] > 2 and s > cur_lambda:\n                        counts[j] -= 2\n                        s = np.sum(counts)\n            # build batch\n            Z_list = []\n            Y_list = []\n            offspring = []\n            scale_used = []\n            for sf_i, cnt in enumerate(counts):\n                sf = scale_factors[sf_i]\n                half = cnt // 2\n                # occasionally use orthonormal probing for condition detection\n                if rng.rand() < 0.06:\n                    Q, _ = np.linalg.qr(rng.normal(size=(self.dim, self.dim)))\n                    Z_half = rng.normal(size=(half, self.dim))\n                    for k in range(min(2, half)):\n                        Z_half[k] = (rng.normal(size=(self.dim,)) @ Q)\n                else:\n                    Z_half = rng.normal(size=(half, self.dim))\n                Z_block = np.vstack([Z_half, -Z_half])\n                # incorporate eigen_scale as pre-scaling in z-space\n                Z_scaled = Z_block * np.sqrt(np.maximum(eigen_scale[None, :], 1e-8))\n                Y_block = (Z_scaled * D[None, :]) @ B.T  # transform to real-space directions\n                X_block = center[None, :] + (sigma * sf) * Y_block\n                for k in range(X_block.shape[0]):\n                    Z_list.append(Z_block[k])\n                    Y_list.append(Y_block[k])\n                    offspring.append(X_block[k])\n                    scale_used.append(sf)\n            Z = np.array(Z_list)\n            Y = np.array(Y_list)\n            offspring = np.array(offspring)\n            scale_used = np.array(scale_used)\n            batch_n = offspring.shape[0]\n            # evaluate offspring (respect budget)\n            f_off = np.empty(batch_n, dtype=float)\n            actual = batch_n\n            for i in range(batch_n):\n                if evals >= self.budget:\n                    actual = i\n                    break\n                res = eval_point(offspring[i])\n                if res is None:\n                    actual = i\n                    break\n                x_ref, fx = res\n                offspring[i] = x_ref\n                f_off[i] = fx\n            if actual == 0:\n                break\n            if actual < batch_n:\n                Z = Z[:actual]\n                Y = Y[:actual]\n                offspring = offspring[:actual]\n                f_off = f_off[:actual]\n                scale_used = scale_used[:actual]\n                batch_n = actual\n\n            # sort by fitness\n            idx_sorted = np.argsort(f_off)\n            best_idx = idx_sorted[0]\n            f_best = f_off[best_idx]\n            x_best = offspring[best_idx].copy()\n\n            # success stats\n            success_mask = f_off < f_center\n            success_count = int(np.sum(success_mask))\n            success_rate = success_count / max(1, batch_n)\n\n            # recombination in z-space: weighted average of top mu z's\n            actual_mu = min(mu, batch_n)\n            top_idx = idx_sorted[:actual_mu]\n            w = weights[:actual_mu].copy()\n            if np.sum(w) <= 0:\n                w = np.ones(actual_mu) / actual_mu\n            else:\n                w = w / np.sum(w)\n            z_top = Z[top_idx]\n            z_mean = np.dot(w, z_top)\n            y_mean = (z_mean * D) @ B.T\n            cand_mean = center + sigma * y_mean\n            cand_mean = reflect(cand_mean)\n\n            improved = False\n\n            # greedy accept best offspring if improved\n            if f_best < f_center:\n                center = x_best.copy()\n                f_center = f_best\n                improved = True\n                stall = 0\n            else:\n                # evaluate recombination candidate but only if predicted to be promising by simple check\n                if evals < self.budget:\n                    re = eval_point(cand_mean)\n                    if re is not None:\n                        _, f_cand = re\n                        if f_cand < f_center:\n                            center = cand_mean.copy()\n                            f_center = f_cand\n                            improved = True\n                            stall = 0\n                        else:\n                            stall += 1\n                    else:\n                        stall += 1\n\n            if f_center < self.f_opt:\n                self.f_opt = float(f_center); self.x_opt = center.copy()\n\n            # update evolution path and sigma (momentum-smoothed CSA)\n            ps = (1.0 - c_s) * ps + np.sqrt(c_s * (2.0 - c_s) * mu_eff) * z_mean\n            ps_norm = np.linalg.norm(ps)\n            # adapt sigma with softened factor and momentum\n            rate_factor = 1.0 + 0.9 * (success_rate - 0.2)\n            if ps_norm > target_ps:\n                sigma *= np.exp((ps_norm / target_ps - 1.0) * 0.45 / (damps * rate_factor))\n            else:\n                sigma *= np.exp((ps_norm / target_ps - 1.0) * 0.22 / (damps * rate_factor))\n            sigma = float(np.clip(sigma, 1e-14 * max(1.0, avg_range), 10.0 * max(1.0, avg_range)))\n\n            # covariance update in z-space: rank-mu + diag RMS from successful samples\n            rank_mu_z = np.zeros((self.dim, self.dim))\n            for k in range(actual_mu):\n                zk = z_top[k].reshape(self.dim, 1)\n                rank_mu_z += w[k] * (zk @ zk.T)\n            # update diagonal part from RMS of recent successful z's\n            if success_count > 0:\n                z_succ = Z[success_mask]\n                if z_succ.ndim == 1:\n                    z_succ = z_succ.reshape(1, -1)\n                rms = np.sqrt(np.mean(z_succ ** 2, axis=0) + 1e-16)\n                target_diag = rms / (np.mean(rms) + 1e-16)\n                Cdiag = 0.85 * Cdiag + 0.15 * target_diag\n                Cdiag = Cdiag / (np.mean(Cdiag) + 1e-16)\n            # conservative blending\n            c_cov = min(0.5, 4.0 / (self.dim + 4.0))\n            Cz = (1.0 - c_cov) * Cz + c_cov * rank_mu_z + np.diag(0.3 * (Cdiag - np.diag(Cz)))\n            # slight shrink-to-identity regularization\n            shrink_to_id = 0.02 + 0.025 * min(1.0, restart_count / max(1, max_restarts))\n            Cz = (1.0 - shrink_to_id) * Cz + shrink_to_id * np.eye(self.dim)\n            Cz = 0.5 * (Cz + Cz.T) + np.eye(self.dim) * eps\n\n            # small covariance path update\n            pc = 0.9 * pc + 0.1 * z_mean\n\n            # approximate z for memory: map dx back to z approx\n            for i in range(batch_n):\n                dx = (offspring[i] - center) / (sigma + 1e-16)\n                z_approx = (dx @ B) / (D + 1e-16)\n                mem_Z.append(z_approx.copy())\n                if len(mem_Z) > self.mem_size:\n                    del mem_Z[0]\n            # record offspring evaluations to mem arrays\n            for i in range(batch_n):\n                record_evaluation(offspring[i].copy(), float(f_off[i]), z_approx=None)\n\n            # Lightweight ensemble surrogate when enough memory: linear ridge + diagonal quadratic\n            surrogate_available = len(mem_X) >= min(self.dim + 4, 3 * self.dim // 2)\n            surrogate_model = None\n            if surrogate_available:\n                try:\n                    Xm = np.array(mem_X)\n                    Fm = np.array(mem_F)\n                    # center data\n                    xm_mean = Xm.mean(axis=0)\n                    fm_mean = Fm.mean()\n                    Xc = Xm - xm_mean\n                    Fc = Fm - fm_mean\n                    # linear ridge\n                    lam_ridge = 1e-6 * (1.0 + np.var(Fm))\n                    XtX = Xc.T @ Xc\n                    rhs = Xc.T @ Fc\n                    g_lin = np.linalg.solve(XtX + lam_ridge * np.eye(self.dim), rhs)\n                    # diagonal quadratic estimation: fit residuals to squared features diag only\n                    resid = Fc - Xc @ g_lin\n                    # form squared features normalized\n                    Sq = Xc ** 2\n                    # regularized solve for diag entries\n                    lam_q = 1e-6 * (1.0 + np.var(resid))\n                    H_diag = np.linalg.solve(Sq.T @ Sq + lam_q * np.eye(self.dim), Sq.T @ resid)\n                    # combine: predicted f(x) = fm_mean + g_lin^T (x-xm_mean) + 0.5 * sum(H_diag * (x-xm_mean)^2)\n                    surrogate_model = (xm_mean, fm_mean, g_lin, H_diag)\n                except Exception:\n                    surrogate_model = None\n\n            # build surrogate-based trust-region proposals (one per loop when available)\n            if surrogate_model is not None and evals < self.budget:\n                xm_mean, fm_mean, g_lin, H_diag = surrogate_model\n                # build local preconditioner from Cz diagonal -> prefer directions with small variance (more certain)\n                pre = 1.0 / (np.sqrt(np.abs(np.diag(Cz))) + 1e-8)\n                # compute Newton-like step with diagonal Hessian (H_diag can be noisy)\n                H_diag_pos = np.abs(H_diag) + 1e-8\n                # step = - inv(H_diag + lambda diag) * g_lin with preconditioning\n                lam_diag = 0.5 * (np.mean(H_diag_pos) + 1e-6)\n                denom = H_diag_pos + lam_diag\n                step_dir = - (g_lin / (denom + 1e-16))\n                # precondition and scale by trust radius\n                sdir = pre * step_dir\n                s_norm = np.linalg.norm(sdir)\n                if s_norm > 1e-16:\n                    # trust radius computed from sigma and trust_scale\n                    trust_radius = trust_base * trust_scale * sigma\n                    step = sdir * (trust_radius / (s_norm + 1e-16))\n                    candidate = center + step\n                    # uncertainty estimate: distance to nearest mem point normalized by avg_range\n                    Xm = np.array(mem_X)\n                    dists = np.linalg.norm((Xm - candidate), axis=1)\n                    uncert = np.min(dists) / (avg_range + 1e-12)\n                    # predicted improvement from surrogate\n                    dx = candidate - xm_mean\n                    pred = fm_mean + np.dot(g_lin, dx) + 0.5 * np.dot(H_diag, dx * dx)\n                    # if predicted improvement is good proportional to uncertainty, evaluate\n                    improv = fm_mean - pred\n                    if improv > 0 and improv > 0.5 * uncert * (abs(fm_mean) + 1e-12):\n                        # evaluate candidate under budget\n                        if evals < self.budget:\n                            re = eval_point(candidate)\n                            if re is not None:\n                                _, f_cand = re\n                                if f_cand < f_center:\n                                    center = candidate.copy(); f_center = f_cand; improved = True; stall = 0\n                                    # increase trust if surrogate succeeded\n                                    trust_scale = min(3.0, trust_scale * 1.12)\n                                else:\n                                    stall += 1\n                                    trust_scale = max(min_trust, trust_scale * trust_decay)\n                    else:\n                        # if not promising, slightly reduce trust for future proposals\n                        trust_scale = max(min_trust, trust_scale * trust_decay)\n\n            # directional quasi-Newton probing (few line-search probes) if improved or many successes\n            if evals < self.budget and (improved or success_rate > 0.25):\n                # prefer surrogate direction if available\n                if surrogate_model is not None:\n                    dir_vec = -g_lin.copy()\n                else:\n                    dir_vec = -y_mean.copy()\n                nrm = np.linalg.norm(dir_vec)\n                if nrm > 1e-16:\n                    dvec = dir_vec / nrm\n                    # try few scaled probes (scale by sigma and eigen_scale averaged)\n                    dir_scale = sigma * (1.0 + 0.5 * (np.mean(eigen_scale) - 1.0))\n                    fractions = [0.95, 0.45, 0.18]\n                    for sf in fractions:\n                        if evals >= self.budget:\n                            break\n                        s = sf * dir_scale\n                        x_try = center + s * dvec\n                        re = eval_point(x_try)\n                        if re is None:\n                            break\n                        x_r, f_try = re\n                        if f_try < f_center:\n                            center = x_r.copy(); f_center = f_try; improved = True; stall = 0\n                            # on success increase sigma slightly\n                            sigma = min(10.0 * avg_range, sigma * 1.08)\n                            break\n                        else:\n                            stall += 1\n\n            # stagnation => informed multi-elite restarts or Levy exploration\n            if stall >= max_stall and restart_count < max_restarts:\n                stall = 0\n                restart_count += 1\n                # maintain multiple elites and sample around top ones\n                if len(elites) > 0:\n                    # pick an elite by roulette favoring lower f\n                    fs = np.array([e[0] for e in elites])\n                    probs = np.max(fs) - fs + 1e-12\n                    probs = probs / np.sum(probs)\n                    idx = rng.choice(len(elites), p=probs)\n                    chosen = elites[idx]\n                    base = chosen[1]\n                    # heavy-tail jump with decreasing scale over restarts\n                    tail_scale = avg_range * (0.9 ** restart_count)\n                    if rng.rand() < self.levy_prob:\n                        step = tail_scale * rng.standard_cauchy(size=self.dim)\n                    else:\n                        step = rng.normal(size=self.dim) * (tail_scale * (rng.rand() ** (-0.6)))\n                    center = reflect(base + step)\n                    res = eval_point(center)\n                    if res is not None:\n                        _, f_center = res\n                        if f_center < self.f_opt:\n                            self.f_opt = float(f_center); self.x_opt = center.copy()\n                else:\n                    # vanilla random restart\n                    center = rng_uniform(lb, ub)\n                    res = eval_point(center)\n                    if res is not None:\n                        _, f_center = res\n                # inflate covariance and sigma moderately\n                Cz = np.eye(self.dim) * (1.0 + 0.8 * (restart_count / max(1, max_restarts)))\n                Cdiag = np.ones(self.dim)\n                B = np.eye(self.dim); D = np.ones(self.dim)\n                ps.fill(0.0); pc.fill(0.0)\n                sigma = max(sigma * 1.6, 0.25 * avg_range)\n                trust_scale = 1.0\n                last_eig = evals\n\n            # localized final exploitation when near budget: directional greedy around best\n            if (self.budget - evals) <= max(8, self.dim):\n                try:\n                    eigvals_f, eigvecs_f = np.linalg.eigh(Cz)\n                    order_f = np.argsort(-eigvals_f)\n                    dirs = eigvecs_f[:, order_f].T\n                except Exception:\n                    dirs = np.eye(self.dim)\n                steps = [0.45, 0.12, 0.035]\n                for step_frac in steps:\n                    if evals >= self.budget:\n                        break\n                    step_size = step_frac * max(sigma, 0.02 * avg_range)\n                    for dvec in dirs:\n                        if evals >= self.budget:\n                            break\n                        for sgn in (+1, -1):\n                            if evals >= self.budget:\n                                break\n                            x_try = center + sgn * step_size * dvec\n                            re = eval_point(x_try)\n                            if re is None:\n                                break\n                            x_r, f_try = re\n                            if f_try < f_center:\n                                old_center = center.copy()\n                                center = x_r.copy(); f_center = f_try\n                                if f_center < self.f_opt:\n                                    self.f_opt = float(f_center); self.x_opt = center.copy()\n                                # update Cz from this small move\n                                y = (center - old_center) / max(1e-16, sigma)\n                                z_approx = ((y @ B) / (D + 1e-16))\n                                Cz = (1.0 - 0.6 * c_cov) * Cz + (0.6 * c_cov) * np.outer(z_approx, z_approx)\n                                Cz = 0.5 * (Cz + Cz.T) + np.eye(self.dim) * eps\n\n            # always keep Cz symmetric positive definite\n            Cz = 0.5 * (Cz + Cz.T) + np.eye(self.dim) * eps\n\n        # fallback\n        if self.x_opt is None:\n            self.x_opt = center.copy()\n            self.f_opt = float(f_center)\n\n        return float(self.f_opt), self.x_opt.copy()", "configspace": "", "generation": 13, "feedback": "The algorithm AHLRDS_plus_v2 scored 0.744 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["da6b7611-2c9b-4e79-a084-2c869b8a826a"], "operator": null, "metadata": {"aucs": [0.22022570752038806, 0.1756651675991635, 0.9390246666504092, 0.9802215736294123, 0.9644214798913718, 0.9770749378202762, 0.30690185297833816, 0.9614936573756669, 0.9651340161402348, 0.9464977951355467]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-MA_BBOB-1", "seed": 1}
{"method_name": "ES", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 4, "n_offspring": 16, "elitism": true}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "008715f8-d903-40a3-b331-ffbbeff335d1", "fitness": 0.6633387165602562, "name": "AdaptiveLowRankOrthogonalSecant", "description": "Adaptive Low-Rank Orthogonal Secant with Trust-Weighted Momentum (ALOST-M) — build cheap orthonormal directional secants, form a regularized subspace Newton step plus AdaGrad-style preconditioning, refine promising direction minima by parabolic interpolation, and use trust-weighted momentum + low-rank echo memory to accelerate convergence while budget-aware probing controls evaluations.", "code": "import numpy as np\n\nclass AdaptiveLowRankOrthogonalSecant:\n    \"\"\"\n    Adaptive Low-Rank Orthogonal Secant with Trust-Weighted Momentum (ALOST-M)\n\n    Key ideas / refinements over OSAS:\n      - Adaptive subspace size controlled by remaining budget and recent success.\n      - Better parabolic interpolation: compute central finite differences (f+, f-, f0)\n        and use the derived parabola to propose an in-subspace minimizer; evaluate\n        those proposals only for the most promising directions (budget-aware).\n      - Trust-weighted momentum: keep a smoothed step (momentum) but scale its magnitude\n        by recent success (trust). This reduces oscillations and amplifies consistent descent.\n      - Low-rank echo memory (orthonormalized) is used to bias subspace generation and\n        to create a small corrective low-rank term for preconditioning (soft shaping).\n      - More conservative and robust curvature handling (Tikhonov regularization),\n        clipping of proposed steps, and a budget-aware short parabolic line-refinement\n        along the combined direction.\n      - All function calls are guarded by a strict budget-aware wrapper.\n    \"\"\"\n\n    def __init__(self, budget, dim, seed=None,\n                 pop_dir=None, sigma0=None,\n                 mem_size=None,\n                 ls_budget=6,\n                 levy_prob=0.03,\n                 levy_scale=2.0,\n                 echo_blend=0.45,\n                 diag_lr=0.8,\n                 damping=1e-4,\n                 momentum=0.75,\n                 trust_decay=0.95,\n                 restart_patience=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.sigma0 = float(sigma0) if sigma0 is not None else None\n        # number of orthonormal probe directions; default scales with dim\n        self.pop_dir = int(pop_dir) if pop_dir is not None else max(4, min(32, int(4 + np.sqrt(dim) * 1.6)))\n        self.mem_size = int(mem_size) if mem_size is not None else max(6, 4 + int(np.log1p(dim) * 2))\n        self.ls_budget = int(ls_budget)\n        self.levy_prob = float(levy_prob)\n        self.levy_scale = float(levy_scale)\n        self.echo_blend = float(echo_blend)\n        self.diag_lr = float(diag_lr)\n        self.damping = float(damping)\n        self.momentum = float(momentum)\n        self.trust_decay = float(trust_decay)\n        self.restart_patience = restart_patience if restart_patience is not None else max(30, 8 * dim)\n\n        # internal memory\n        self.echo = []           # list of orthonormal directions (unit)\n        self.step_archive = []   # recent successful steps for low-rank shaping\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def _get_bounds(self, func):\n        try:\n            lb = np.atleast_1d(func.bounds.lb)\n            ub = np.atleast_1d(func.bounds.ub)\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item())\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        return lb.astype(float), ub.astype(float)\n\n    def _reflect_bounds(self, x, lb, ub):\n        # reflect back into bounds (keeps search inside feasible box)\n        x = np.array(x, dtype=float)\n        for i in range(x.size):\n            li, ui = lb[i], ub[i]\n            if li >= ui:\n                x[i] = li\n                continue\n            it = 0\n            while (x[i] < li) or (x[i] > ui):\n                if x[i] < li:\n                    x[i] = li + (li - x[i])\n                elif x[i] > ui:\n                    x[i] = ui - (x[i] - ui)\n                it += 1\n                if it > 12:\n                    x[i] = np.clip(x[i], li, ui)\n                    break\n        return x\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n        budget = max(0, int(self.budget))\n        rng = self.rng\n\n        # initial mean & sigma\n        mean = rng.uniform(lb, ub)\n        if self.sigma0 is None:\n            domain_scale = float(np.mean(ub - lb))\n            sigma = 0.18 * domain_scale\n        else:\n            sigma = float(self.sigma0)\n\n        # AdaGrad-like accumulator\n        acc_sq = np.ones(dim) * 1e-6\n\n        # momentum and trust\n        momentum_vec = np.zeros(dim)\n        trust = 0.5\n\n        evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # safe eval wrapper that enforces budget\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                raise StopIteration(\"budget exhausted\")\n            x = self._reflect_bounds(x, lb, ub)\n            try:\n                v = float(func(x))\n            except Exception:\n                v = float(func(np.asarray(x)))\n            evals += 1\n            return v, x\n\n        # initial evaluation\n        try:\n            fmean, mean = safe_eval(mean)\n        except StopIteration:\n            fmean = np.inf\n\n        self.f_opt = float(fmean)\n        self.x_opt = mean.copy()\n        archive = [(self.f_opt, self.x_opt.copy())] if evals > 0 else []\n\n        gen = 0\n        gens_since_improve = 0\n        recent_success = 0  # counts improvements in a sliding window\n\n        # helper to form an orthonormal basis of k vectors biased by echo memory (returns k x dim)\n        def make_basis(k):\n            # start with gaussian directions\n            A = rng.randn(k, dim)\n            # overlay echo memory on top rows for bias\n            if len(self.echo) > 0:\n                m = min(len(self.echo), k)\n                chosen = np.array(self.echo[-m:])  # last m orthonormal directions\n                for i in range(m):\n                    if rng.rand() < self.echo_blend:\n                        # blend: emphasize echo vector\n                        A[i] = (A[i] * (1.0 - self.echo_blend) + chosen[i] * (self.echo_blend * 2.5))\n            # orthonormalize rows via QR of A^T -> Q has orthonormal columns\n            try:\n                Q, _ = np.linalg.qr(A.T)  # dim x dim\n                B = Q.T[:k]  # rows are orthonormal\n            except Exception:\n                # fallback Gram-Schmidt\n                Blist = []\n                for row in A:\n                    v = row.copy().astype(float)\n                    for q in Blist:\n                        v -= np.dot(q, v) * q\n                    n = np.linalg.norm(v)\n                    if n > 1e-12:\n                        Blist.append(v / n)\n                    if len(Blist) >= k:\n                        break\n                while len(Blist) < k:\n                    v = rng.randn(dim)\n                    for q in Blist:\n                        v -= np.dot(q, v) * q\n                    n = np.linalg.norm(v)\n                    if n > 1e-12:\n                        Blist.append(v / n)\n                B = np.array(Blist[:k])\n            return B\n\n        # small helper: orthonormalize list of vectors\n        def orthonormalize_list(vecs, keep=0):\n            E = []\n            for v in vecs:\n                w = v.copy().astype(float)\n                for q in E:\n                    w -= np.dot(q, w) * q\n                n = np.linalg.norm(w)\n                if n > 1e-12:\n                    E.append(w / n)\n                if keep and len(E) >= keep:\n                    break\n            return E\n\n        # main loop\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n\n            # adapt k by remaining budget and recent success\n            max_k_by_budget = max(1, remaining // 6)  # heuristic: cost per direction ~6 evals including refinements\n            success_factor = 1.0 + min(3.0, recent_success / 3.0)  # increase k when recent successes\n            k = int(np.clip(self.pop_dir * success_factor, 1, min(dim, max_k_by_budget)))\n            k = max(1, k)\n\n            # build orthonormal basis\n            B = make_basis(k)  # shape k x dim\n\n            # central-difference probes: f+, f-, and use f0 = fmean\n            step = sigma\n            derivs = np.full(k, np.nan)\n            curv = np.full(k, np.nan)\n            f_plus_list = [None] * k\n            f_minus_list = [None] * k\n            xm_plus = [None] * k\n            xm_minus = [None] * k\n\n            # do mirrored probes first (consume 2*k evals or as many as budget permits)\n            for i in range(k):\n                if evals >= budget:\n                    break\n                d = B[i]\n                xplus = mean + step * d\n                try:\n                    f_plus, xp = safe_eval(xplus)\n                except StopIteration:\n                    break\n                f_plus_list[i] = f_plus\n                xm_plus[i] = xp.copy()\n\n                if evals >= budget:\n                    # can't do minus, use symmetric assumption\n                    f_minus_list[i] = f_plus\n                    xm_minus[i] = xp.copy()\n                    break\n\n                xminus = mean - step * d\n                try:\n                    f_minus, xm = safe_eval(xminus)\n                except StopIteration:\n                    # if cannot evaluate, reuse plus\n                    f_minus = f_plus\n                    xm = xp.copy()\n                f_minus_list[i] = f_minus\n                xm_minus[i] = xm.copy()\n\n                # compute central derivative and curvature\n                g_dir = (f_plus - f_minus) / (2.0 * step)\n                curvature = (f_plus + f_minus - 2.0 * fmean) / (step ** 2 + 1e-16)\n                derivs[i] = g_dir\n                curv[i] = curvature\n\n            if np.all(np.isnan(derivs)):\n                break\n\n            # build full-space gradient estimate\n            g_est = np.zeros(dim)\n            for i in range(k):\n                if np.isfinite(derivs[i]):\n                    g_est += derivs[i] * B[i]\n            g_norm = np.linalg.norm(g_est) + 1e-12\n\n            # update accumulator\n            acc_sq += (g_est ** 2)\n            diag_scale = 1.0 / (np.sqrt(acc_sq) + 1e-12)\n            diag_step = - self.diag_lr * (diag_scale * g_est)\n\n            # compute subspace Newton-like step with robust regularization and clipping\n            sub_step = np.zeros(dim)\n            # proposals for parabolic minima along each direction\n            proposals = []\n            for i in range(k):\n                if not np.isfinite(derivs[i]):\n                    continue\n                gi = derivs[i]\n                ci = curv[i]\n                # robust regularization for curvature\n                if not np.isfinite(ci) or ci <= self.damping:\n                    ci_eff = max(self.damping, abs(ci)) + 1e-6\n                else:\n                    ci_eff = ci\n                # classical Newton scalar = -gi / ci_eff\n                s_scalar = - (gi / (ci_eff + 1e-16))\n                # clip proposed scalar to avoid huge moves (relative to probe step)\n                max_scale = 4.0 * step  # can go beyond probe but limited\n                s_scalar = np.clip(s_scalar, -max_scale, max_scale)\n                proposals.append((i, s_scalar, gi, ci_eff))\n                sub_step += s_scalar * B[i]\n\n            # budget-aware refinement: pick top promising proposals by predicted relative improvement\n            # predicted improvement by parabola: delta = gi * s + 0.5 * ci * s^2 ~ -gi^2/(2*ci)\n            predicted = []\n            for (i, s_scalar, gi, ci_eff) in proposals:\n                if ci_eff > 0:\n                    pred = - (gi ** 2) / (2.0 * ci_eff)\n                else:\n                    pred = -abs(gi) * abs(s_scalar) * 0.1\n                predicted.append((pred, i, s_scalar))\n            predicted.sort(key=lambda t: t[0])  # ascending (more negative => larger decrease)\n            # evaluate parabolic minimizers for a small number of top directions, budget permitting\n            max_refinements = max(1, min(3, remaining // 8))  # conservative\n            refinements = 0\n            for tup in predicted:\n                if refinements >= max_refinements or evals >= budget:\n                    break\n                pred, i, s_scalar = tup\n                if pred >= -1e-12:\n                    continue\n                # candidate from parabola: s* = -gi / curv (equivalent to above but re-evaluated)\n                # evaluate at mean + s_scalar * B[i]\n                x_cand = mean + s_scalar * B[i]\n                try:\n                    f_cand, x_c = safe_eval(x_cand)\n                except StopIteration:\n                    break\n                refinements += 1\n                archive.append((f_cand, x_c.copy()))\n                # accept if better than fmean\n                if f_cand < fmean:\n                    # record improvement\n                    old_mean = mean.copy()\n                    mean = x_c.copy()\n                    fmean = f_cand\n                    improved = True\n                    # store successful step\n                    svec = mean - old_mean\n                    norm_s = np.linalg.norm(svec)\n                    if norm_s > 1e-12:\n                        self.step_archive.append(svec / norm_s)\n                        self.echo.append((svec / norm_s).copy())\n                else:\n                    improved = False\n\n            # after directional refinements, recompute g_est if we moved\n            # (for simplicity, we recompute only if we improved; otherwise keep current estimate)\n            # if improved above, update gradient estimate via finite differences reuse is complex; we keep g_est\n\n            # combine steps: precondition diag_step and low-rank correction from echoed directions\n            # simple low-rank shaping: if we have echo vectors, slightly project the diag_step into echo span and amplify\n            lowrank_corr = np.zeros(dim)\n            if len(self.step_archive) > 0:\n                # orthonormalize recent step archive (small)\n                U = orthonormalize_list(self.step_archive[-self.mem_size:], keep=min(self.mem_size, len(self.step_archive)))\n                if len(U) > 0:\n                    # project g_est onto U and build corrective step akin to limited-memory secant\n                    Gproj = np.array([np.dot(u, g_est) for u in U])  # coefficients\n                    # heuristic small correction in U: scale by diag_scale average\n                    lam = 0.6 * trust  # strength\n                    for j, u in enumerate(U):\n                        lowrank_corr += - lam * (Gproj[j]) * u\n\n            # core combined direction\n            combined = 0.6 * sub_step + 0.4 * diag_step + lowrank_corr\n\n            # momentum update (trust-weighted)\n            momentum_vec = self.momentum * momentum_vec + (1.0 - self.momentum) * combined\n            step_vec = (0.9 * trust) * momentum_vec + 0.1 * combined\n\n            # if step_vec is tiny, add small exploration\n            if np.linalg.norm(step_vec) < 1e-12:\n                step_vec = sigma * 0.08 * rng.randn(dim)\n\n            # normalize direction and pick length alpha initial\n            p_dir = step_vec / (np.linalg.norm(step_vec) + 1e-12)\n            alpha0 = sigma * (1.2 + 0.5 * (trust - 0.5))  # trust increases initial length modestly\n\n            # budget-aware parabolic line refinement: evaluate a few alphas using fmean as f(0)\n            best_ls_val = fmean\n            best_ls_x = mean.copy()\n            used_ls = 0\n            alphas = [alpha0, 0.5 * alpha0, 0.25 * alpha0, -0.6 * alpha0]  # include negative attempt\n            for a in alphas:\n                if evals >= budget or used_ls >= self.ls_budget:\n                    break\n                x_try = mean + a * p_dir\n                try:\n                    f_try, x_t = safe_eval(x_try)\n                except StopIteration:\n                    break\n                used_ls += 1\n                archive.append((f_try, x_t.copy()))\n                if f_try < best_ls_val:\n                    best_ls_val = f_try\n                    best_ls_x = x_t.copy()\n\n            # optionally parabolic interpolation between best three points if we have evaluations (f0, f1, f2)\n            # cheap attempt: if we found an improved alpha in the ls samples, try a local quadratic fit using (0,fmean), (a1,f1), (a2,f2)\n            # gather last few distinct evaluations along p_dir for interpolation\n            if evals < budget:\n                # collect distinct along-direction points from archive tail\n                tail = archive[-(used_ls + 3):] if len(archive) >= (used_ls + 1) else archive[:]\n                # construct arrays of scalars t and values y for points near the line (distance projection)\n                pts = []\n                for (fv, xv) in tail:\n                    # projection scalar along direction from mean\n                    t = np.dot(xv - mean, p_dir)\n                    # keep only small neighbourhood\n                    if abs(np.linalg.norm(xv - (mean + t * p_dir))) < 1e-6:\n                        pts.append((t, fv, xv.copy()))\n                # require at least two non-zero offsets to fit parabola\n                if len(pts) >= 2:\n                    # include (0, fmean)\n                    pts2 = [(0.0, fmean, mean.copy())] + sorted(pts, key=lambda x: abs(x[0]))[:3]\n                    if len(pts2) >= 3:\n                        # fit parabola using three points (t,y)\n                        t0, y0, x0 = pts2[0]\n                        t1, y1, x1 = pts2[1]\n                        t2, y2, x2 = pts2[2]\n                        denom = (t0 - t1) * (t0 - t2) * (t1 - t2) + 1e-20\n                        # solve for parabola coefficients if well-posed\n                        try:\n                            A = ( (y0*(t1 - t2) + y1*(t2 - t0) + y2*(t0 - t1)) / (denom) )\n                            # vertex at -B/(2A) but we can compute directly:\n                            # Using Lagrange form gives vertex t* = ...\n                            # Simpler: compute derivative coefficients via finite differences\n                            # We'll compute derivative at 0: B = (y1 - y2)/(t1 - t2) - 2*A*(t1 + t2)/2 etc. Simpler approach below:\n                            # Solve quadratic fit c2*t^2 + c1*t + c0 = y\n                            M = np.array([[t0*t0, t0, 1.0],\n                                          [t1*t1, t1, 1.0],\n                                          [t2*t2, t2, 1.0]])\n                            Y = np.array([y0, y1, y2])\n                            coeffs = np.linalg.solve(M, Y)\n                            a_coef, b_coef, c_coef = coeffs\n                            if abs(a_coef) > 1e-12:\n                                t_star = - b_coef / (2.0 * a_coef)\n                                # clip t_star within a reasonable range\n                                t_star = float(np.clip(t_star, -3.0 * alpha0, 3.0 * alpha0))\n                                x_star = mean + t_star * p_dir\n                                if evals < budget:\n                                    try:\n                                        f_star, x_st = safe_eval(x_star)\n                                    except StopIteration:\n                                        f_star = None\n                                        x_st = None\n                                    if f_star is not None:\n                                        archive.append((f_star, x_st.copy()))\n                                        if f_star < best_ls_val:\n                                            best_ls_val = f_star\n                                            best_ls_x = x_st.copy()\n                        except Exception:\n                            pass\n\n            # accept best line-search point\n            improved = False\n            if best_ls_val < fmean:\n                old_mean = mean.copy()\n                mean = best_ls_x.copy()\n                fmean = best_ls_val\n                improved = True\n                # push normalized step into echo and low-rank archive\n                s = mean - old_mean\n                ns = np.linalg.norm(s)\n                if ns > 1e-12:\n                    sn = s / ns\n                    self.echo.append(sn.copy())\n                    self.step_archive.append(sn.copy())\n            else:\n                # shrink sigma moderately and small jitter exploration (budget permitting)\n                sigma *= 0.80\n                if evals < budget:\n                    jitter = 0.15 * sigma * rng.randn(dim)\n                    try:\n                        f_j, x_j = safe_eval(mean + jitter)\n                    except StopIteration:\n                        f_j = None\n                        x_j = None\n                    if f_j is not None:\n                        archive.append((f_j, x_j.copy()))\n                        if f_j < fmean:\n                            mean = x_j.copy()\n                            fmean = f_j\n                            improved = True\n                            s = mean - (mean - jitter)\n                            ns = np.linalg.norm(s)\n                            if ns > 1e-12:\n                                sn = s / ns\n                                self.echo.append(sn.copy())\n                                self.step_archive.append(sn.copy())\n\n            # update global best\n            if fmean < self.f_opt:\n                self.f_opt = float(fmean)\n                self.x_opt = mean.copy()\n                gens_since_improve = 0\n                recent_success = recent_success + 1\n                trust = min(1.0, trust * 1.06 + 0.01)\n            else:\n                gens_since_improve += 1\n                recent_success = max(0, int(recent_success * 0.9))\n                trust = max(0.05, trust * self.trust_decay)\n\n            # keep echo and step_archive sizes bounded and orthonormalize occasionally\n            if len(self.echo) > self.mem_size:\n                self.echo = self.echo[-self.mem_size:]\n            if len(self.step_archive) > 4 * self.mem_size:\n                self.step_archive = self.step_archive[-(4 * self.mem_size):]\n            if len(self.echo) > 1 and (gen % 13 == 0):\n                self.echo = orthonormalize_list(self.echo, keep=self.mem_size)\n\n            # occasional Levy jump to escape traps (budget-aware)\n            levy_chance = self.levy_prob * (1.0 + 0.5 * (gens_since_improve / float(max(1, self.restart_patience))))\n            if rng.rand() < levy_chance and evals < budget:\n                # sample heavy-tailed magnitude (Pareto-like)\n                u = max(1e-6, rng.rand())\n                levy_mag = (1.0 / (u ** (1.0 / 1.5))) * sigma * self.levy_scale\n                v = rng.randn(dim)\n                v /= (np.linalg.norm(v) + 1e-12)\n                x_jump = mean + levy_mag * v\n                try:\n                    f_jump, x_j = safe_eval(x_jump)\n                except StopIteration:\n                    f_jump = None\n                    x_j = None\n                if f_jump is not None:\n                    archive.append((f_jump, x_j.copy()))\n                    if f_jump < fmean:\n                        mean = x_j.copy()\n                        fmean = f_jump\n                        if f_jump < self.f_opt:\n                            self.f_opt = f_jump\n                            self.x_opt = x_j.copy()\n                        s = x_j - mean\n                        ns = np.linalg.norm(s)\n                        if ns > 1e-12:\n                            sn = s / ns\n                            self.echo.append(sn.copy())\n                            self.step_archive.append(sn.copy())\n\n            # adapt sigma: expand on success, shrink on failure moderately\n            if improved:\n                sigma *= 1.08 + 0.02 * trust\n            else:\n                sigma *= 0.94\n            domain_scale = float(np.mean(ub - lb))\n            sigma = float(np.clip(sigma, 1e-8, max(1e-3, domain_scale * 2.0)))\n\n            # occasional soft restart if stagnating\n            if gens_since_improve >= self.restart_patience and evals < budget:\n                widen = 1.0 + rng.rand() * 2.5\n                sigma = max(sigma * widen, 0.05 * domain_scale)\n                if self.x_opt is not None:\n                    mean = self.x_opt + 0.6 * sigma * rng.randn(dim)\n                    mean = self._reflect_bounds(mean, lb, ub)\n                    try:\n                        f_try, x_try = safe_eval(mean)\n                    except StopIteration:\n                        break\n                    fmean = f_try\n                    archive.append((f_try, x_try.copy()))\n                    if f_try < self.f_opt:\n                        self.f_opt = f_try\n                        self.x_opt = x_try.copy()\n                        gens_since_improve = 0\n                else:\n                    mean = rng.uniform(lb, ub)\n                    try:\n                        f_try, x_try = safe_eval(mean)\n                    except StopIteration:\n                        break\n                    fmean = f_try\n                    archive.append((f_try, x_try.copy()))\n                    if f_try < self.f_opt:\n                        self.f_opt = f_try\n                        self.x_opt = x_try.copy()\n                        gens_since_improve = 0\n                # keep only strongest echoes\n                if len(self.echo) > 0:\n                    self.echo = self.echo[-max(1, len(self.echo)//2):]\n                gens_since_improve = 0\n                trust = 0.5\n                momentum_vec *= 0.2\n                recent_success = 0\n\n            # maintain archive size\n            max_archive = max(300, 10 * dim)\n            if len(archive) > max_archive:\n                archive.sort(key=lambda t: t[0])\n                archive = archive[:max_archive]\n\n        # final fallback random sampling if nothing found\n        if self.x_opt is None:\n            while evals < budget:\n                x = rng.uniform(lb, ub)\n                try:\n                    f, xs = safe_eval(x)\n                except StopIteration:\n                    break\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = xs.copy()\n\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveLowRankOrthogonalSecant scored 0.663 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3a4239fb-234a-42d2-bbb8-4bf22278e612"], "operator": null, "metadata": {"aucs": [0.14539624043371357, 0.1444531571034875, 0.9729094064564398, 0.9885524792297478, 0.9863235042191412, 0.9913268324213094, 0.28678453865950404, 0.9832339579638586, 0.987729660678979, 0.146677388436381]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-MA_BBOB-5", "seed": 5}
{"method_name": "ES", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 4, "n_offspring": 16, "elitism": true}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "2a2b9226-c20f-44ac-95be-b81a5536bba3", "fitness": 0.6695404830523822, "name": "EnsembleAdaptiveLowRankTrustBandit", "description": "Ensemble Adaptive Low-Rank Trust & Bandit Search (EALR-TS) — combine per-anchor low-rank quadratic surrogates with bandit-driven anchor allocation, cross-validated model trust, adaptive trust-radii, and a lightweight global low-rank covariance + heavy-tailed sampling to robustly mix local quasi-Newton steps and global exploration.", "code": "import numpy as np\n\nclass EnsembleAdaptiveLowRankTrustBandit:\n    \"\"\"\n    Ensemble Adaptive Low-Rank Trust & Bandit Search (EALR-TS)\n\n    Key ideas:\n      - Maintain a compact ensemble of anchors, each with small archive, diffs and bandit stats.\n      - Build per-anchor low-rank orthonormal subspace via weighted SVD of recent displacements + archive deltas.\n      - Fit quadratic surrogate in that subspace via ridge regression; estimate model reliability via simple holdout (CV).\n      - Use a multi-armed-bandit (UCB-like) to allocate evaluation attempts to anchors with promising improvement per eval and uncertainty.\n      - Propose Newton-like steps in trusted subspaces; perform small budget-aware line/quadratic refinements.\n      - Maintain low-rank global covariance from successful steps and add occasional Student-t global jumps.\n      - Strict budget enforcement and caching for function evaluations.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 pop_size=None, base_rank_factor=1.0,\n                 init_radius_frac=0.12, min_radius=1e-8, max_radius=8.0,\n                 de_prob=0.16, heavy_tail_prob=0.08, archive_limit=200,\n                 ucb_coef=1.2, model_holdout_frac=0.25):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        if pop_size is None:\n            pop_size = max(6, int(np.clip(self.budget * 0.0035, 6, 36)))\n        self.pop_size = int(pop_size)\n        self.base_rank_factor = float(base_rank_factor)\n        self.init_radius_frac = float(init_radius_frac)\n        self.min_radius = float(min_radius)\n        self.max_radius = float(max_radius)\n        self.de_prob = float(de_prob)\n        self.heavy_tail_prob = float(heavy_tail_prob)\n        self.archive_limit = int(archive_limit)\n        self.ucb_coef = float(ucb_coef)\n        self.model_holdout_frac = float(model_holdout_frac)\n\n        self.best_f = np.inf\n        self.best_x = None\n\n    def __call__(self, func):\n        # get bounds if provided\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n        if lb.shape == ():\n            lb = np.full(self.dim, lb, dtype=float)\n        if ub.shape == ():\n            ub = np.full(self.dim, ub, dtype=float)\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n\n        def clamp(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        domain_scale = ub - lb\n        domain_mean = float(np.mean(np.abs(domain_scale)))\n        init_radius = max(1e-12, self.init_radius_frac * (domain_mean + 1e-12))\n\n        evals = 0\n        history = {}\n\n        def eval_point(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None\n            x_c = clamp(np.asarray(x, dtype=float))\n            key = tuple(np.round(x_c, 12))\n            if key in history:\n                return x_c.copy(), float(history[key])\n            try:\n                f = float(func(x_c))\n            except Exception:\n                f = float(np.inf)\n            evals += 1\n            history[key] = f\n            return x_c.copy(), float(f)\n\n        # Latin-hypercube-like init\n        m = self.pop_size\n        samples = np.zeros((m, self.dim), dtype=float)\n        strata = np.linspace(0.0, 1.0, m + 1)\n        for d in range(self.dim):\n            picks = self.rng.uniform(strata[:-1], strata[1:])\n            self.rng.shuffle(picks)\n            samples[:, d] = picks\n        samples = lb + samples * (ub - lb)\n\n        anchors = []\n        for i in range(m):\n            if evals >= self.budget:\n                break\n            r = eval_point(samples[i])\n            if r is None:\n                break\n            x_c, f = r\n            a = {\n                'x': x_c.copy(),\n                'f': float(f),\n                'radius': init_radius,\n                'age': 0,\n                'succ': 0,\n                'archive': [(x_c.copy(), float(f))],\n                'diffs': [],  # normalized recent successful steps\n                # bandit stats\n                'pulls': 1,\n                'cum_imp': 0.0,  # cumulative improvement achieved when pulling this anchor\n            }\n            anchors.append(a)\n            if f < self.best_f:\n                self.best_f = float(f)\n                self.best_x = x_c.copy()\n\n        # fill remainder randomly if budget allows\n        while len(anchors) < m and evals < self.budget:\n            r = eval_point(self.rng.uniform(lb, ub))\n            if r is None:\n                break\n            x_c, f = r\n            a = {'x': x_c.copy(), 'f': float(f), 'radius': init_radius, 'age': 0, 'succ': 0, 'archive': [(x_c.copy(), float(f))], 'diffs': [], 'pulls': 1, 'cum_imp': 0.0}\n            anchors.append(a)\n            if f < self.best_f:\n                self.best_f = float(f)\n                self.best_x = x_c.copy()\n\n        # temperature for uphill acceptance\n        temp = max(1e-9, 0.5 * (1.0 + abs(self.best_f)))\n        min_temp = 1e-9\n        temp_decay = (min_temp / temp) ** (1.0 / max(1, self.budget // 400))\n\n        stagn = 0\n        total_pulls = sum(a['pulls'] for a in anchors) if anchors else 1\n\n        def build_basis(anchor, max_rank):\n            # weighted SVD of diffs and archive deltas + small global randomization\n            cand = []\n            weights = []\n            # recent diffs (higher weight)\n            for i, d in enumerate(reversed(anchor['diffs'][-(8 * max_rank):])):\n                cand.append(d.copy())\n                weights.append(2.0 * (1.0 / (1 + i)))\n            base = anchor['x']\n            pts = [p for p, _ in anchor['archive'][-(10 * max_rank):]]\n            for i, p in enumerate(pts):\n                v = p - base\n                n = np.linalg.norm(v)\n                if n > 1e-12:\n                    cand.append(v / n)\n                    weights.append(1.0)\n            # if too few, add global covariance directions from other anchors\n            if len(cand) < max(3, max_rank):\n                all_d = []\n                for a in anchors:\n                    all_d.extend(a['diffs'][-4:])\n                if len(all_d) >= 2:\n                    D = np.vstack(all_d)\n                    try:\n                        U, S, Vt = np.linalg.svd(D, full_matrices=False)\n                        for j in range(min(Vt.shape[0], max_rank)):\n                            v = Vt[j]\n                            cand.append(v.copy())\n                            weights.append(0.8 * (0.5 + 0.5 * (S[j] / (S[0] + 1e-12))))\n                    except Exception:\n                        pass\n            # pad with random orthonormal directions\n            while len(cand) < max(3, max_rank):\n                v = self.rng.randn(self.dim)\n                n = np.linalg.norm(v)\n                if n > 1e-12:\n                    cand.append(v / n)\n                    weights.append(0.3)\n            M = np.vstack(cand)\n            W = np.sqrt(np.maximum(1e-12, np.asarray(weights, dtype=float)))[:, None]\n            A = W * M\n            try:\n                U, S, Vt = np.linalg.svd(A, full_matrices=False)\n                k = min(max_rank, Vt.shape[0])\n                basis = Vt[:k].T\n            except Exception:\n                Q, _ = np.linalg.qr(self.rng.randn(self.dim, max_rank))\n                basis = Q[:, :max_rank]\n            if basis.shape[1] == 0:\n                Q, _ = np.linalg.qr(self.rng.randn(self.dim, 1))\n                basis = Q[:, :1]\n            return basis\n\n        def fit_quadratic_with_cv(anchor, basis):\n            # Fit quadratic in subspace using ridge and evaluate simple holdout error to judge reliability\n            k = basis.shape[1]\n            params_count = k + (k * (k + 1)) // 2 + 1\n            samples = anchor['archive'][-max(12, 4 * params_count):]\n            if len(samples) < max(8, int(1.4 * params_count)):\n                return None\n            x0 = anchor['x']\n            rows = []\n            y = []\n            for p, fp in samples:\n                s = basis.T.dot(p - x0)\n                row = list(s.copy())\n                for i in range(k):\n                    for j in range(i, k):\n                        row.append(s[i] * s[j])\n                row.append(1.0)\n                rows.append(row)\n                y.append(fp)\n            A = np.asarray(rows, dtype=float)\n            y = np.asarray(y, dtype=float)\n            # holdout indices\n            n = len(y)\n            h = max(1, int(n * self.model_holdout_frac))\n            idx = np.arange(n)\n            self.rng.shuffle(idx)\n            hold = idx[:h]\n            train = idx[h:]\n            A_train = A[train]\n            y_train = y[train]\n            A_hold = A[hold]\n            y_hold = y[hold]\n            lam = 1e-6 * (1.0 + abs(anchor['f'])) + 1e-8 * len(y_train)\n            try:\n                ATA = A_train.T.dot(A_train)\n                reg = lam * np.eye(ATA.shape[0])\n                params = np.linalg.solve(ATA + reg, A_train.T.dot(y_train))\n                g = params[:k].copy()\n                ptri = params[k:k + (k * (k + 1)) // 2].copy()\n                H = np.zeros((k, k), dtype=float)\n                idxp = 0\n                for i in range(k):\n                    for j in range(i, k):\n                        if i == j:\n                            H[i, j] = 2.0 * ptri[idxp]\n                        else:\n                            H[i, j] = ptri[idxp]\n                            H[j, i] = ptri[idxp]\n                        idxp += 1\n                c = float(params[-1])\n                # compute holdout RMSE\n                preds = A_hold.dot(params)\n                rmse = np.sqrt(np.mean((preds - y_hold) ** 2)) if len(preds) > 0 else np.inf\n                return {'g': g, 'H': H, 'c': c, 'rmse': float(rmse), 'n_train': len(train)}\n            except Exception:\n                return None\n\n        def predict_model(model, s):\n            return float(model['c'] + np.dot(model['g'], s) + 0.5 * float(s.T.dot(model['H']).dot(s)))\n\n        it = 0\n        while evals < self.budget:\n            it += 1\n            # sort anchors by fitness\n            anchors.sort(key=lambda a: a['f'])\n            kpop = len(anchors)\n            total_pulls = sum(max(1, a.get('pulls', 1)) for a in anchors)\n            # choose anchor via UCB: exploitation = avg improvement per pull, exploration term = sqrt(ln total / pulls)\n            ucb_scores = []\n            for a in anchors:\n                pulls = max(1, a.get('pulls', 1))\n                mean_imp = (a.get('cum_imp', 0.0) / pulls)\n                bonus = self.ucb_coef * np.sqrt(np.log(max(2, total_pulls)) / pulls)\n                # bias towards good fitness (lower f) by adding a scaled inverse rank term\n                fitness_bias = 0.0\n                ucb_scores.append(mean_imp + bonus - 0.005 * a['f'] + fitness_bias)\n            # if all zeros, pick random\n            if np.all(np.isclose(ucb_scores, 0.0)):\n                idx = self.rng.randint(kpop)\n            else:\n                probs = np.array(ucb_scores, dtype=float)\n                probs = np.maximum(probs - probs.min() + 1e-12, 1e-12)\n                probs /= probs.sum()\n                idx = int(self.rng.choice(kpop, p=probs))\n            anchor = anchors[idx]\n            x0 = anchor['x'].copy()\n            f0 = float(anchor['f'])\n            r0 = float(anchor['radius'])\n            anchor['age'] += 1\n            anchor['pulls'] = anchor.get('pulls', 0) + 1\n\n            # adapt subspace rank\n            base_k = max(1, int(np.clip(int(np.ceil(np.sqrt(self.dim) * self.base_rank_factor)), 1, self.dim)))\n            extra = int(min(4 + len(anchor['diffs']) // 2, self.dim - base_k))\n            k_sub = min(self.dim, base_k + extra)\n            k_sub = max(1, k_sub)\n\n            basis = build_basis(anchor, k_sub)\n            if basis.shape[1] > k_sub:\n                basis = basis[:, :k_sub]\n            k = basis.shape[1]\n\n            # fit model with CV estimate\n            model = fit_quadratic_with_cv(anchor, basis)\n\n            candidate_x = None\n            candidate_f = None\n            used_diff = None\n            predicted_improvement = 0.0\n\n            # If model is present and CV rmse is reasonable relative to function scale, try Newton-like step\n            if model is not None and evals < self.budget:\n                # accept model only if rmse small relative to local function range or training size adequate\n                scale_ok = (model['rmse'] < max(1e-6, 0.5 * (abs(f0) + 1.0))) or (model['n_train'] > 1.5 * (k + (k*(k+1))//2))\n                if scale_ok:\n                    g_sub = model['g']\n                    H_sub = model['H']\n                    try:\n                        eigs = np.linalg.eigvalsh(H_sub)\n                        maxeig = float(max(1e-8, np.max(np.abs(eigs))))\n                    except Exception:\n                        maxeig = 1e-6\n                    damping = max(1e-6, 0.4 * (1.0 + np.linalg.norm(g_sub) / (1.0 + maxeig)))\n                    H_reg = H_sub + damping * np.eye(k)\n                    try:\n                        s_sub = -np.linalg.solve(H_reg, g_sub)\n                    except Exception:\n                        s_sub = -g_sub / (np.diag(H_sub) + 1e-6)\n                    step_full = basis.dot(s_sub)\n                    norm_full = np.linalg.norm(step_full)\n                    # trust radius control\n                    max_step = max(1.4 * r0, domain_mean * 0.28 + 0.6 * r0)\n                    if norm_full > 1e-12 and norm_full > max_step:\n                        s_sub = s_sub * (max_step / (norm_full + 1e-16))\n                        step_full = basis.dot(s_sub)\n                        norm_full = np.linalg.norm(step_full)\n                    # refined trust scalar: scaled by model confidence (lower rmse -> higher trust)\n                    conf = 1.0 / (1.0 + model['rmse'])\n                    trust = np.clip(0.4 + 0.9 * min(1.0, conf), 0.25, 1.05) * (0.7 + 0.6 * self.rng.rand())\n                    trial = clamp(x0 + step_full * trust)\n                    r_trial = eval_point(trial)\n                    if r_trial is not None:\n                        tx, tf = r_trial\n                        candidate_x = tx.copy()\n                        candidate_f = float(tf)\n                        used_diff = (candidate_x - x0).copy()\n                        # compute predicted improvement using subspace model\n                        s_prop = s_sub * trust\n                        f_model0 = predict_model(model, np.zeros(k))\n                        f_model_pred = predict_model(model, s_prop)\n                        predicted_improvement = max(0.0, f_model0 - f_model_pred)\n                        delta_actual = f0 - candidate_f\n                        # simple acceptance\n                        accept = False\n                        if candidate_f < f0 - 1e-12:\n                            accept = True\n                        else:\n                            # allow uphill if within noise/confidence via temperature\n                            delta = candidate_f - f0\n                            if temp > 1e-12 and self.rng.rand() < np.exp(-max(0.0, delta) / (temp + 1e-12) * (0.8 + 0.4 * conf)):\n                                accept = True\n                        if accept:\n                            # budget-aware local refinement: try one more point along the direction (quadratic interpolation)\n                            if evals < self.budget:\n                                dir_full = clamp(candidate_x - x0)\n                                norm_dir = np.linalg.norm(dir_full)\n                                if norm_dir > 1e-12:\n                                    alpha_try = 0.6 + 0.9 * self.rng.rand()\n                                    pt = clamp(x0 + dir_full * alpha_try)\n                                    r2 = eval_point(pt)\n                                    if r2 is not None:\n                                        p2x, p2f = r2\n                                        if p2f < candidate_f - 1e-12:\n                                            candidate_x = p2x.copy()\n                                            candidate_f = float(p2f)\n                                            used_diff = (candidate_x - x0).copy()\n\n                        # update anchor radius based on (actual / predicted) ratio but guarded by model rmse\n                        if predicted_improvement > 1e-12:\n                            ratio = max(0.0, float((f0 - candidate_f) / (predicted_improvement + 1e-12)))\n                            if ratio > 0.85 and model['rmse'] < max(1e-3, abs(f0) * 0.15):\n                                anchor['radius'] = float(min(self.max_radius * domain_mean, anchor['radius'] * (1.2 + 0.12 * self.rng.rand())))\n                            elif ratio < 0.3:\n                                anchor['radius'] = float(max(self.min_radius, anchor['radius'] * (0.65 + 0.12 * self.rng.rand())))\n                            else:\n                                anchor['radius'] = float(max(self.min_radius, min(self.max_radius * domain_mean, anchor['radius'] * (0.95 + 0.10 * self.rng.rand()))))\n\n            # fallback directional probes if model failed or didn't improve much\n            if (candidate_x is None or candidate_f >= f0 - 1e-12) and evals < self.budget:\n                n_probe = min(k, 3)\n                best_probe = (np.inf, None, None)\n                for i in range(n_probe):\n                    if evals >= self.budget:\n                        break\n                    d = basis[:, i]\n                    s = r0 * (0.4 + 0.9 * self.rng.rand())\n                    xp = clamp(x0 + d * s)\n                    rp = eval_point(xp)\n                    if rp is None:\n                        break\n                    xp_c, fp = rp\n                    xm = clamp(x0 - d * s)\n                    rm = eval_point(xm)\n                    if rm is None:\n                        break\n                    xm_c, fm = rm\n                    if fp < best_probe[0]:\n                        best_probe = (fp, xp_c.copy(), (d, s))\n                    if fm < best_probe[0]:\n                        best_probe = (fm, xm_c.copy(), (d, s))\n                if best_probe[1] is not None:\n                    pf, px, info = best_probe\n                    if pf < candidate_f if candidate_x is not None else True:\n                        candidate_x = px.copy()\n                        candidate_f = float(pf)\n                        used_diff = (candidate_x - x0).copy()\n\n            # occasional DE-style recombination using top anchors\n            if (candidate_x is None or candidate_f >= f0 - 1e-12) and self.rng.rand() < self.de_prob and evals < self.budget and kpop >= 3:\n                topk = min(6, kpop)\n                weights = np.array([1.0 / (1 + i) for i in range(topk)])\n                weights = weights / weights.sum()\n                mix = np.zeros(self.dim)\n                for i, w in enumerate(weights):\n                    mix += w * anchors[i]['x']\n                a, b = self.rng.choice(range(kpop), size=2, replace=False)\n                mutant = anchors[a]['x'] + 0.7 * (anchors[b]['x'] - anchors[a]['x'])\n                CR = 0.18 + 0.6 * self.rng.rand()\n                mask = self.rng.rand(self.dim) < CR\n                if not np.any(mask):\n                    mask[self.rng.randint(self.dim)] = True\n                trial = clamp(mix)\n                trial[mask] = mutant[mask]\n                r_tr = eval_point(trial)\n                if r_tr is not None:\n                    tx, tf = r_tr\n                    if candidate_x is None or tf < candidate_f:\n                        candidate_x = tx.copy()\n                        candidate_f = float(tf)\n                        used_diff = (candidate_x - x0).copy()\n\n            # small gaussian jitter last resort\n            if candidate_x is None and evals < self.budget:\n                d = self.rng.randn(self.dim)\n                d /= (np.linalg.norm(d) + 1e-16)\n                s = r0 * (0.16 + 0.9 * self.rng.rand())\n                trial = clamp(x0 + d * s)\n                r_trial = eval_point(trial)\n                if r_trial is not None:\n                    tx, tf = r_trial\n                    candidate_x = tx.copy()\n                    candidate_f = float(tf)\n                    used_diff = (candidate_x - x0).copy()\n\n            # acceptance decision: greedy with temperature and model confidence\n            accepted = False\n            if candidate_x is not None:\n                if candidate_f < f0 - 1e-12:\n                    accepted = True\n                else:\n                    delta = candidate_f - f0\n                    if temp > 1e-12 and self.rng.rand() < np.exp(-max(0.0, delta) / (temp + 1e-12)):\n                        accepted = True\n\n            # apply update and bandit reward bookkeeping\n            if accepted and candidate_x is not None:\n                old_r = anchor['radius']\n                anchor['radius'] = float(min(self.max_radius * domain_mean, max(self.min_radius, old_r * (1.06 + 0.22 * self.rng.rand()))))\n                imp = max(0.0, anchor['f'] - candidate_f)\n                anchor['cum_imp'] = anchor.get('cum_imp', 0.0) + imp\n                anchor['x'] = candidate_x.copy()\n                anchor['f'] = float(candidate_f)\n                anchor['age'] = 0\n                anchor['succ'] += 1\n                anchor['archive'].append((candidate_x.copy(), float(candidate_f)))\n                if len(anchor['archive']) > self.archive_limit:\n                    anchor['archive'] = anchor['archive'][-self.archive_limit:]\n                if used_diff is not None:\n                    dn = np.linalg.norm(used_diff)\n                    if dn > 1e-12:\n                        anchor['diffs'].append(used_diff / dn)\n                        anchor['diffs'] = anchor['diffs'][-(8 * self.dim):]\n                # update global best\n                if candidate_f < self.best_f:\n                    self.best_f = float(candidate_f)\n                    self.best_x = candidate_x.copy()\n                    stagn = 0\n                else:\n                    stagn += 1\n            else:\n                # rejection: shrink radius modestly and age\n                anchor['radius'] = float(max(self.min_radius, anchor['radius'] * (0.78 + 0.16 * self.rng.rand())))\n                anchor['age'] += 1\n                stagn += 1\n\n            # replace worst if new point is better than worst anchor\n            anchors.sort(key=lambda a: a['f'])\n            if candidate_x is not None and candidate_f is not None and candidate_f < anchors[-1]['f']:\n                anchors[-1] = {'x': candidate_x.copy(), 'f': float(candidate_f), 'radius': max(self.min_radius, min(self.max_radius * domain_mean, anchor.get('radius', init_radius))), 'age': 0, 'succ': 0, 'archive': [(candidate_x.copy(), float(candidate_f))], 'diffs': [], 'pulls': 1, 'cum_imp': 0.0}\n\n            # occasional heavy-tailed global probe with low-rank covariance from diffs\n            if evals < self.budget and self.rng.rand() < self.heavy_tail_prob:\n                if self.best_x is not None and len(anchors) > 1:\n                    all_diffs = []\n                    for a in anchors:\n                        all_diffs.extend(a['diffs'][-(4 * k_sub):])\n                    if len(all_diffs) >= 2:\n                        D = np.vstack(all_diffs)\n                        try:\n                            U, S, Vt = np.linalg.svd(D, full_matrices=False)\n                            rank = min(max(1, min(4, Vt.shape[0])), Vt.shape[0])\n                            cov_basis = Vt[:rank].T\n                            scales = (np.maximum(1e-3, np.array(S[:rank]) / (np.sum(S[:rank]) + 1e-12)))\n                            # Student-t flavored jump in subspace\n                            nu = 3.0\n                            z = self.rng.standard_normal(rank)\n                            t = z / (np.sqrt(self.rng.chisquare(int(max(1, nu))) / nu) + 1e-12)\n                            jump = cov_basis.dot(np.diag(0.9 + 0.9 * scales)).dot(t)\n                            jump = jump + 0.2 * self.rng.randn(self.dim)\n                        except Exception:\n                            jump = self.rng.standard_normal(self.dim)\n                    else:\n                        u = self.rng.rand(self.dim)\n                        jump = np.tan(np.pi * (u - 0.5))\n                    scale = max(domain_mean * 0.6, 0.8 * np.mean([a['radius'] for a in anchors]))\n                    probe = clamp(self.best_x + jump * scale * (0.5 + 1.0 * self.rng.rand()))\n                else:\n                    probe = self.rng.uniform(lb, ub)\n                r_probe = eval_point(probe)\n                if r_probe is None:\n                    break\n                px, pf = r_probe\n                if pf < self.best_f:\n                    self.best_f = float(pf)\n                    self.best_x = px.copy()\n                anchors.sort(key=lambda a: a['f'])\n                if pf < anchors[-1]['f']:\n                    anchors[-1] = {'x': px.copy(), 'f': float(pf), 'radius': init_radius, 'age': 0, 'succ': 0, 'archive': [(px.copy(), float(pf))], 'diffs': [], 'pulls': 1, 'cum_imp': 0.0}\n\n            # stagnation-driven refresh: re-seed some worst anchors around best with adaptive spread\n            if stagn > max(18, 6 + self.dim // 2):\n                anchors.sort(key=lambda a: a['f'])\n                num_reset = max(1, min(len(anchors) // 2, 4))\n                spread = domain_mean * (0.35 + 0.6 * self.rng.rand())\n                for j in range(num_reset):\n                    if evals >= self.budget:\n                        break\n                    center = anchors[0]['x']\n                    newx = clamp(center + self.rng.randn(self.dim) * spread * (0.6 + 0.8 * self.rng.rand()))\n                    r_new = eval_point(newx)\n                    if r_new is None:\n                        break\n                    nx, nf = r_new\n                    anchors[-1 - j] = {'x': nx.copy(), 'f': float(nf), 'radius': init_radius, 'age': 0, 'succ': 0, 'archive': [(nx.copy(), float(nf))], 'diffs': [], 'pulls': 1, 'cum_imp': 0.0}\n                    if nf < self.best_f:\n                        self.best_f = float(nf)\n                        self.best_x = nx.copy()\n                stagn = 0\n                # clear diffs to encourage new directions\n                for a in anchors:\n                    a['diffs'].clear()\n\n            # minor diversification: random nudge of a worse anchor occasionally\n            if self.rng.rand() < 0.03 and evals < self.budget and len(anchors) >= 2:\n                ia, ib = self.rng.choice(len(anchors), size=2, replace=False)\n                worse = ib if anchors[ib]['f'] > anchors[ia]['f'] else ia\n                nudge = clamp(anchors[0]['x'] + 0.6 * anchors[worse]['radius'] * self.rng.randn(self.dim))\n                r_n = eval_point(nudge)\n                if r_n is None:\n                    break\n                nx, nf = r_n\n                if nf < anchors[worse]['f']:\n                    anchors[worse] = {'x': nx.copy(), 'f': float(nf), 'radius': init_radius, 'age': 0, 'succ': 0, 'archive': [(nx.copy(), float(nf))], 'diffs': [], 'pulls': 1, 'cum_imp': 0.0}\n                    if nf < self.best_f:\n                        self.best_f = float(nf)\n                        self.best_x = nx.copy()\n\n            # cap archives\n            for a in anchors:\n                if len(a['archive']) > self.archive_limit:\n                    a['archive'] = a['archive'][-self.archive_limit:]\n\n            # temperature update\n            temp = max(min_temp, temp * temp_decay)\n\n        return float(self.best_f), (self.best_x.copy() if self.best_x is not None else None)", "configspace": "", "generation": 11, "feedback": "The algorithm EnsembleAdaptiveLowRankTrustBandit scored 0.670 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["59fd3423-3e5f-48ee-b039-ea15bdbc40be"], "operator": null, "metadata": {"aucs": [0.26374692543736555, 0.2737622306670999, 0.8008064582962512, 0.9250691455777665, 0.8371672311944856, 0.8499391594676904, 0.2924989437356489, 0.796469771701949, 0.8271796668969057, 0.8287652975486596]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-MA_BBOB-3", "seed": 3}
{"method_name": "ES", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 4, "n_offspring": 16, "elitism": true}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "79bbe510-8d01-4e86-bb38-258c33bcbd1d", "fitness": 0.763909211278287, "name": "ACDMS", "description": "Adaptive Covariance and Directional Memory Search (ACDMS) — a compact CMA-like population search with lightweight covariance adaptation, adaptive step-size and a small directional memory of successful steps for efficient multi-scale continuous optimization.", "code": "import numpy as np\n\nclass ACDMS:\n    \"\"\"\n    Adaptive Covariance and Directional Memory Search (ACDMS)\n\n    - budget: number of function evaluations allowed\n    - dim: problem dimensionality\n    Optional:\n    - pop_factor: multiplier controlling population size (larger -> more global sampling)\n    - seed: RNG seed for reproducibility\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, pop_factor=1.5, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_factor = float(pop_factor)\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # Allow scalar bounds too\n        if lb.shape == ():\n            lb = np.full(self.dim, lb)\n        if ub.shape == ():\n            ub = np.full(self.dim, ub)\n\n        # Basic checks\n        assert lb.shape == (self.dim,)\n        assert ub.shape == (self.dim,)\n\n        # scale and initialization\n        range_scale = (ub - lb)\n        # initial mean uniformly in bounds\n        mean = lb + self.rng.rand(self.dim) * range_scale\n        # initial step-size (relative to range)\n        sigma = 0.3 * np.maximum(1e-12, range_scale).mean()\n\n        # population size dependent on dimension and budget\n        lam = max(4, int(round(self.pop_factor * (4 + 3 * np.log(max(2, self.dim))))))\n        # avoid too large lambda when budget is small\n        lam = min(lam, max(4, self.budget // 5))\n        mu = max(1, lam // 2)\n\n        # weights: log-weights like CMA\n        ranks = np.arange(1, mu + 1)\n        w = np.log(mu + 0.5) - np.log(ranks)\n        w = w / np.sum(w)\n        mu_eff = 1.0 / np.sum(w**2)\n\n        # covariance matrix\n        C = np.eye(self.dim)\n        # small regularization\n        eps = 1e-8 * (range_scale.mean() + 1e-8)\n\n        # adaptation constants\n        c_cov = (2.0 / ((self.dim + 1.3)**2 + mu_eff))  # small covariance learning rate\n        c_sigma_inc = 1.05\n        c_sigma_dec = 0.85\n        sigma_min = 1e-12 * (range_scale.mean() + 1e-12)\n        sigma_max = 0.5 * range_scale.mean()\n\n        # directional memory: store recent successful displacement vectors (normalized by sigma)\n        mem_size = max(3, int(np.sqrt(self.dim)))\n        memory = []\n\n        # counters and best\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial population seed: evaluate at a few random points (at least one generation)\n        # We will proceed generation by generation: each generation draws up to lam candidates\n        # and uses those evaluations to update mean/C/sigma.\n        generation = 0\n        # To avoid numerical issues use Cholesky when sampling; compute cholesky of C when needed\n        def sample_multivariate(n):\n            # sample n vectors from N(0, C)\n            # attempt cholesky; fallback to eigh\n            try:\n                L = np.linalg.cholesky(C + eps * np.eye(self.dim))\n                Z = self.rng.randn(n, self.dim).dot(L.T)\n            except Exception:\n                # use eigh\n                D, B = np.linalg.eigh(C)\n                D = np.clip(D, 0, None)\n                sqrtD = np.sqrt(D + eps)\n                Z = self.rng.randn(n, self.dim).dot((B * sqrtD).T)\n            return Z\n\n        # Main loop: generation by generation\n        while evals < self.budget:\n            generation += 1\n            # adjust lambda if close to budget\n            remaining = self.budget - evals\n            cur_lam = min(lam, remaining)\n            # Sample candidates: x = mean + sigma * z  (z ~ N(0, C))\n            Z = sample_multivariate(cur_lam)\n            X = mean + sigma * Z\n            # clip to bounds\n            X = np.minimum(np.maximum(X, lb), ub)\n\n            # Evaluate candidates (one by one to ensure not exceeding budget)\n            Fs = np.empty(cur_lam)\n            for i in range(cur_lam):\n                if evals >= self.budget:\n                    Fs[i:] = np.inf\n                    break\n                x = X[i]\n                f = func(x)\n                evals += 1\n                Fs[i] = f\n                if f < f_opt:\n                    # record improvement and save directional memory\n                    if x_opt is not None:\n                        disp = (x - x_opt) / max(sigma, 1e-16)\n                        # push normalized displacement\n                        if np.linalg.norm(disp) > 0:\n                            memory.append(disp)\n                            if len(memory) > mem_size:\n                                memory.pop(0)\n                    f_opt = f\n                    x_opt = x.copy()\n\n            # If no evaluations done (shouldn't happen) break\n            if cur_lam == 0:\n                break\n\n            # Selection: sort by fitness\n            idx = np.argsort(Fs)\n            # compute new mean using top mu (but ensure mu <= actual available)\n            mu_actual = min(mu, cur_lam)\n            selected = X[idx[:mu_actual]]\n            selected_Z = Z[idx[:mu_actual]]  # corresponding z vectors\n            # weighted recombination (if mu reduced, re-weight)\n            w_act = w.copy()\n            if mu_actual != mu:\n                # re-normalize weights for current actual mu\n                w_act = w[:mu_actual] / np.sum(w[:mu_actual])\n\n            mean_old = mean.copy()\n            # new mean\n            mean = np.sum(selected.T * w_act, axis=1)\n\n            # compute y_w (weighted step in z-space)\n            y_w = np.sum(selected_Z.T * w_act, axis=1)\n\n            # covariance update: rank-mu like update with the selected Z vectors\n            S = np.zeros((self.dim, self.dim))\n            for k in range(mu_actual):\n                zk = selected_Z[k]\n                S += w_act[k] * np.outer(zk, zk)\n            # blend into C\n            C = (1.0 - c_cov) * C + c_cov * S\n            # regularize\n            # (ensure symmetry)\n            C = (C + C.T) / 2.0\n            # add small eps to diagonal for numerical stability\n            C += eps * np.eye(self.dim)\n\n            # step-size control: if best in this generation improved -> inflate sigma, else shrink\n            improved = (Fs.min() < f_opt + 1e-300)  # improvement accounted earlier; keep check safe\n            # Actually we want to detect improvement relative to mean_old evaluation; use whether best improved during this generation\n            # Use memory: if last appended in this generation then improved True else False is ok\n            # Simpler: if any candidate improved previous global we already set x_opt; check if that happened this generation:\n            # Determine improvement by checking if minimum of Fs is strictly less than previous generation's best (we can approximate)\n            # For stability use last few memory entries: above we appended on improvement, so check memory length change is not straightforward.\n            # We'll use a heuristic: if the best among candidates is within top and better than previous mean projected then consider improvement.\n            # For simplicity: if the minimum Fs equals f_opt (which we set during eval) mark improved True\n            gen_best_f = Fs.min()\n            if gen_best_f < f_opt + 1e-16:\n                # If it is exactly equal to f_opt, it may be the same one; we treat it as improvement if it changed x_opt (we updated earlier)\n                improved_flag = True\n            else:\n                improved_flag = False\n\n            # to be more pragmatic: if any candidate is better than mean evaluated as proxy, we use improvement check:\n            # Apply multiplicative adaptation\n            if gen_best_f < f_opt:\n                sigma *= c_sigma_inc\n            else:\n                sigma *= c_sigma_dec ** (1.0 / max(1.0, np.sqrt(self.dim)))\n            # enforce bounds on sigma\n            sigma = float(np.clip(sigma, sigma_min, sigma_max))\n\n            # directional exploitation: if we have memory, occasionally try linear combinations of memory directions\n            # Use a small probability proportional to memory and generation count\n            if memory and evals < self.budget:\n                # try up to few direction-guided samples (but don't exceed remaining budget)\n                tries = min(len(memory), (self.budget - evals))\n                for t in range(tries):\n                    # combine few memory vectors with random coefficients\n                    k = min(len(memory), max(1, int(1 + self.rng.rand() * len(memory))))\n                    coeffs = self.rng.randn(k)\n                    coeffs = coeffs / (np.linalg.norm(coeffs) + 1e-12)\n                    directions = np.array(memory[-k:])\n                    d = coeffs.dot(directions)\n                    # normalize and scale by sigma * small factor\n                    if np.linalg.norm(d) == 0:\n                        continue\n                    d = d / np.linalg.norm(d)\n                    step = 0.5 * sigma * d  # smaller step for directional exploitation\n                    x_try = x_opt + step if x_opt is not None else mean + step\n                    x_try = np.minimum(np.maximum(x_try, lb), ub)\n                    f_try = func(x_try)\n                    evals += 1\n                    if f_try < f_opt:\n                        # record improvement\n                        if x_opt is not None:\n                            disp = (x_try - x_opt) / max(sigma, 1e-16)\n                            if np.linalg.norm(disp) > 0:\n                                memory.append(disp)\n                                if len(memory) > mem_size:\n                                    memory.pop(0)\n                        f_opt = f_try\n                        x_opt = x_try.copy()\n                    if evals >= self.budget:\n                        break\n\n            # small local refinement: try a few gaussian samples around the current best with reduced sigma\n            if evals < self.budget and x_opt is not None:\n                local_tries = min(3, self.budget - evals)\n                for _ in range(local_tries):\n                    z = self.rng.randn(self.dim)\n                    x_local = x_opt + 0.3 * sigma * z\n                    x_local = np.minimum(np.maximum(x_local, lb), ub)\n                    f_local = func(x_local)\n                    evals += 1\n                    if f_local < f_opt:\n                        # store directional memory\n                        disp = (x_local - x_opt) / max(sigma, 1e-16)\n                        if np.linalg.norm(disp) > 0:\n                            memory.append(disp)\n                            if len(memory) > mem_size:\n                                memory.pop(0)\n                        f_opt = f_local\n                        x_opt = x_local.copy()\n                    if evals >= self.budget:\n                        break\n\n            # if budget nearly exhausted, we may break\n            # (loop will exit naturally because evals updated)\n            # small safeguard to keep C well-conditioned: apply small shrink if diagonals dominate\n            diag = np.diag(C)\n            if np.any(diag <= 0):\n                C += (eps + 1e-12) * np.eye(self.dim)\n\n        # Final return\n        # if never evaluated (shouldn't happen) evaluate mean once\n        if x_opt is None:\n            x_try = np.minimum(np.maximum(mean, lb), ub)\n            f_opt = func(x_try)\n            x_opt = x_try\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ACDMS scored 0.764 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.26960043360498764, 0.3596825661784553, 0.9469826646824658, 0.9760865041843781, 0.9516291768663281, 0.9583578786073246, 0.33826110896469685, 0.9395546885123103, 0.9509623711239887, 0.9479747200579345]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-MA_BBOB-4", "seed": 4}
{"method_name": "ES-guided", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 4, "n_offspring": 16, "elitism": true, "feature_guided_mutation": true, "parent_selection": "tournament", "tournament_size": 2}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "5278cbfd-fb64-4efa-af15-db9cea001d36", "fitness": 0.7979541123378766, "name": "EntropyOrthogonalEnsembleSearch", "description": "Entropy-Orthogonal Ensemble Search (EOES) — maintain a Gaussian search with an adaptively rotated orthogonal inflation schedule that targets under-explored eigen-directions (via recent-step coverage and directional entropy), mixes heavy-tailed jumps and archive-guided relinking, and uses mirrored sampling + orthogonal repulsion to maximize directional (degree) variance of solutions.", "code": "import numpy as np\n\nclass EntropyOrthogonalEnsembleSearch:\n    \"\"\"\n    Entropy-Orthogonal Ensemble Search (EOES)\n\n    Main idea:\n    Maintain a CMA-like Gaussian search distribution but periodically compute\n    directional coverage from recent steps (archive). Inflate/rotate variance\n    preferentially along under-explored eigen-directions (orthogonal inflation),\n    occasionally perform heavy-tailed jumps, and use mirrored sampling +\n    orthogonal repulsion to increase solution-degree variance robustly.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # Extract bounds\n        lb = np.asarray(func.bounds.lb)\n        ub = np.asarray(func.bounds.ub)\n        if lb.shape == (): lb = np.full(self.dim, float(lb))\n        if ub.shape == (): ub = np.full(self.dim, float(ub))\n        lb = lb.astype(float)\n        ub = ub.astype(float)\n        if lb.size != self.dim: lb = np.resize(lb, (self.dim,))\n        if ub.size != self.dim: ub = np.resize(ub, (self.dim,))\n        if np.any(ub <= lb):\n            raise ValueError(\"Invalid bounds\")\n\n        range_bound = ub - lb\n        budget = max(1, int(self.budget))\n        evals = 0\n\n        # INITIALIZATION: sample a small initial set to set mean and best\n        init_n = min(max(4, self.dim), budget, 2 * int(np.sqrt(self.dim) + 4))\n        X0 = self.rng.uniform(lb, ub, size=(init_n, self.dim))\n        f0 = np.full(init_n, np.inf)\n        for i in range(init_n):\n            if evals >= budget: break\n            f0[i] = float(func(X0[i]))\n            evals += 1\n            if f0[i] < self.f_opt:\n                self.f_opt = f0[i]\n                self.x_opt = X0[i].copy()\n        if evals == 0:\n            # no budget - return trivial\n            return float(self.f_opt), np.asarray(self.x_opt)\n\n        best_idx = int(np.argmin(f0[:init_n]))\n        m = X0[best_idx].copy()\n        f_m = float(f0[best_idx])\n\n        # initial covariance and step-size\n        C = np.eye(self.dim) * (0.2 * np.mean(range_bound)) ** 2\n        sigma = 0.25 * float(np.mean(range_bound))\n        sigma_min = 1e-12\n        sigma_max = 1.0 * float(np.max(range_bound))\n\n        # population size heuristic\n        lam_base = max(6, int(4 + np.floor(2.5 * np.log(max(2, self.dim)))))\n        lam = min(lam_base, max(2, budget // max(8, self.dim)))\n        lam_max = max(lam, min(6 * lam_base, max(8, budget // max(3, self.dim))))\n        mu = max(1, lam // 2)\n\n        # weights for recombination (logarithmic)\n        def weights_for(mu_local):\n            ranks = np.arange(1, mu_local + 1)\n            w = np.log(mu_local + 0.5) - np.log(ranks)\n            w = np.maximum(w, 0.0)\n            if w.sum() == 0:\n                return np.ones(mu_local) / mu_local\n            return w / w.sum()\n\n        # adaptation params\n        cc = 4.0 / (self.dim + 4)\n        cs = (self.dim + 2) / (self.dim + 5)\n        damper = 1.0 + 2.0 * max(0.0, np.sqrt((self.dim - 1) / (self.dim + 1)) - 1) + cs\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        c1 = 2.0 / ((self.dim + 1.3) ** 2 + mu)\n        cmu = min(1 - c1, 2.0 * (mu - 2 + 1.0 / mu) / ((self.dim + 2) ** 2 + mu))\n        c_cov = c1 + cmu\n\n        # archive of recent points and steps (for directional coverage)\n        archive = []\n        max_archive = max(20, min(200, budget // 2))\n\n        # store recent step vectors for directional coverage calculations\n        recent_steps = []\n        max_steps = 8 * self.dim if (8 * self.dim) < 200 else 200\n\n        generation = 0\n        stagn_count = 0\n        last_improvement = evals\n\n        rng = self.rng  # alias\n\n        # helper: mirrored sampler given transformation A (dim x dim)\n        def mirrored_samples(curr_lam, A):\n            half = curr_lam // 2\n            Z = rng.randn(self.dim, half)\n            if curr_lam % 2 == 0:\n                Zfull = np.hstack([Z, -Z])\n            else:\n                extra = rng.randn(self.dim, 1)\n                Zfull = np.hstack([Z, -Z, extra])\n            X = m.reshape(self.dim, 1) + sigma * (A @ Zfull)\n            return X.T, Zfull\n\n        # helper: compute directional coverage from recent_steps onto eigenvectors\n        def directional_coverage(evecs, steps):\n            if len(steps) == 0:\n                return np.ones(self.dim) / self.dim\n            S = np.vstack(steps)  # shape (k, dim)\n            # project steps onto basis\n            proj = S @ evecs  # (k, dim)\n            # mean squared projection per direction\n            msq = np.mean(proj ** 2, axis=0)\n            # avoid zeros and normalize\n            msq += 1e-20\n            msq /= msq.sum()\n            return msq  # sum to 1\n\n        while evals < budget:\n            remaining = budget - evals\n\n            # adapt lam with budget and stagnation pressure\n            if stagn_count > 2 and lam < lam_max:\n                lam = min(lam_max, lam + max(1, stagn_count // 2))\n            lam = int(min(lam, max(2, remaining)))\n            mu = max(1, lam // 2)\n            weights = weights_for(mu)\n\n            # symmetrize C, eigh\n            C = 0.5 * (C + C.T)\n            try:\n                eigvals, eigvecs = np.linalg.eigh(C)\n                eigvals = np.maximum(eigvals, 1e-16)\n            except np.linalg.LinAlgError:\n                eigvals = np.ones(self.dim)\n                eigvecs = np.eye(self.dim)\n\n            # compute directional entropy and coverage\n            p = eigvals / np.sum(eigvals)\n            entropy = -np.sum(p * np.log(np.clip(p, 1e-20, None))) / np.log(self.dim)\n            coverage = directional_coverage(eigvecs, recent_steps)  # low => under-explored\n\n            # design inflation factors: target low-coverage directions (we want to explore them),\n            # but rotate focal directions periodically to increase degree variance across runs\n            # strength depends on how concentrated current eigen-spectrum is and coverage imbalance\n            coverage_inv = 1.0 / (coverage + 1e-12)\n            coverage_inv /= np.sum(coverage_inv)\n            # measure concentration of coverage (higher => some directions underused)\n            coverage_gini = 1.0 - (np.sum(coverage ** 2) * self.dim)\n            # base inflation parameter\n            base_alpha = 0.8 * (1.0 - entropy) + 0.6 * coverage_gini\n            base_alpha = np.clip(base_alpha, 0.05, 3.0)\n            # rotate target: choose a small random orthonormal rotation every few generations\n            if generation % max(1, 6 - int(np.log2(max(2, lam)))) == 0:\n                # small random rotation applied to selected eigenvectors\n                rot_noise = rng.randn(self.dim, self.dim) * 0.02\n                # re-orthonormalize\n                try:\n                    q, _ = np.linalg.qr(np.eye(self.dim) + rot_noise)\n                except np.linalg.LinAlgError:\n                    q = np.eye(self.dim)\n                eigvecs = eigvecs @ q\n\n            # inflation factors favoring under-covered directions\n            infl = 1.0 + base_alpha * (coverage_inv)\n            infl = np.clip(infl, 0.3, 8.0)\n            # build transformation A = eigvecs * diag(sqrt(eigvals * infl))\n            A = eigvecs @ np.diag(np.sqrt(eigvals * infl))\n\n            # sample offspring (mirrored)\n            curr_lam = int(min(lam, remaining))\n            X_off, Z = mirrored_samples(curr_lam, A)\n            X_off = np.clip(X_off, lb, ub)\n\n            # occasionally mix a few heavy-tailed samples to increase degree variance\n            heavy_frac = 0.12 + 0.6 * (1.0 - entropy)  # more heavy tails when directions concentrated\n            heavy_count = int(np.ceil(heavy_frac * curr_lam))\n            heavy_idx = rng.choice(curr_lam, size=heavy_count, replace=False) if heavy_count > 0 else []\n\n            fvals = np.full(curr_lam, np.inf)\n            for k in range(curr_lam):\n                if evals >= budget: break\n                xk = X_off[k].copy()\n                if k in heavy_idx:\n                    # Cauchy-like heavy tail centered at mean with directionally-biased scale\n                    # scale along A's columns: sample t and transform\n                    t = rng.standard_cauchy(self.dim) * 0.4 * np.mean(range_bound)\n                    xk = np.clip(m + t, lb, ub)\n                fk = float(func(xk))\n                fvals[k] = fk\n                evals += 1\n                # update global best\n                if fk < self.f_opt - 1e-18:\n                    self.f_opt = fk\n                    self.x_opt = xk.copy()\n                    last_improvement = evals\n\n            # if none evaluated, break\n            if np.all(np.isinf(fvals)):\n                break\n\n            # sort offspring\n            idx = np.argsort(fvals)\n            X_sorted = X_off[idx]\n            f_sorted = fvals[idx]\n\n            # update archive with top few\n            top_keep = min(5, len(X_sorted))\n            for i_a in range(top_keep):\n                if len(archive) < max_archive:\n                    archive.append((float(f_sorted[i_a]), X_sorted[i_a].copy()))\n                else:\n                    # replace worst if improved\n                    worst_idx = int(np.argmax([f for f, _ in archive]))\n                    if f_sorted[i_a] < archive[worst_idx][0]:\n                        archive[worst_idx] = (float(f_sorted[i_a]), X_sorted[i_a].copy())\n\n            # compute parents and weighted mean update\n            mu_par = max(1, mu)\n            parents = X_sorted[:mu_par]\n            m_new = np.zeros_like(m)\n            ws = weights_for(mu_par)\n            for j in range(mu_par):\n                m_new += ws[j] * parents[j]\n\n            # compute Ys in sample space (normalized by sigma and transformed)\n            Ys = (parents - m.reshape(1, -1)) / max(sigma, 1e-24)  # shape mu_par x dim\n\n            # update evolution paths (in basis of eigvecs)\n            try:\n                invsqrt = eigvecs @ np.diag(1.0 / np.sqrt(eigvals + 1e-20)) @ eigvecs.T\n                y_w = np.sum((ws.reshape(-1, 1) * Ys), axis=0)\n                ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_par) * (invsqrt @ y_w)\n            except Exception:\n                y_w = np.sum((ws.reshape(-1, 1) * Ys), axis=0)\n                ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_par) * y_w\n\n            norm_ps = np.linalg.norm(ps)\n            expected_norm = np.sqrt(self.dim) * (1 - 1.0 / (4 * self.dim) + 1.0 / (21 * (self.dim ** 2)))\n            sigma *= np.exp((cs / damper) * (min(1.0, norm_ps / (expected_norm + 1e-24)) - 1.0))\n            sigma = float(np.clip(sigma, sigma_min, sigma_max))\n\n            # rank-mu covariance update but with orthogonal inflation bias to under-covered directions\n            rank_mu = np.zeros((self.dim, self.dim))\n            for j in range(mu_par):\n                y = Ys[j].reshape(self.dim, 1)\n                rank_mu += ws[j] * (y @ y.T)\n\n            # compute a mild repulsion term: if many offspring cluster, push pc along orthogonal low-coverage axes\n            cluster_center = np.mean(X_off, axis=0)\n            cluster_disp = (m - cluster_center)\n            # project cluster_disp into eigvecs basis, amplify under-covered directions\n            proj = eigvecs.T @ cluster_disp\n            repulsion_vec = eigvecs @ (proj * (0.5 * (coverage_inv / (coverage_inv.max() + 1e-12))))\n            repulsion = np.outer(repulsion_vec / max(np.linalg.norm(repulsion_vec), 1e-12), repulsion_vec / max(np.linalg.norm(repulsion_vec), 1e-12))\n            # combine updates\n            C = (1 - c_cov) * C + c_cov * (rank_mu - 0.25 * repulsion)\n            # regularize and ensure symmetry\n            C = 0.5 * (C + C.T) + 1e-12 * np.eye(self.dim)\n\n            # momentum on mean\n            m = m_new.copy()\n            f_m = float(f_sorted[0])\n\n            # update recent_steps: store steps relative to mean for coverage estimation\n            for j in range(min(len(parents), 4)):\n                step_vec = (parents[j] - m)\n                recent_steps.append(step_vec)\n            # trim\n            if len(recent_steps) > max_steps:\n                recent_steps = recent_steps[-max_steps:]\n\n            # stagnation detection\n            if self.f_opt < np.inf and evals - last_improvement > max(1, 8 * self.dim // max(1, lam)):\n                stagn_count += 1\n            else:\n                stagn_count = 0\n\n            # when stagnating: targeted archive-guided orthogonal relinking and strong directional injection\n            if stagn_count > 0 and evals < budget:\n                # choose how many injections\n                inj = min(max(1, stagn_count), max(1, self.dim // 4))\n                for _ in range(inj):\n                    if evals >= budget: break\n                    if len(archive) >= 2 and rng.rand() < 0.7:\n                        # pick one best and another random; do orthogonal perturbation relative to best\n                        a_idx = int(np.argmin([f for f, _ in archive]))\n                        b_idx = rng.randint(0, len(archive))\n                        x_best = archive[a_idx][1]\n                        x_other = archive[b_idx][1]\n                        direction = x_other - x_best\n                        # orthonormalize direction against top eigvecs and amplify orthogonal part\n                        for _ in range(1):\n                            direction -= eigvecs[:, :min(3, self.dim)].dot(eigvecs[:, :min(3, self.dim)].T.dot(direction))\n                        step = rng.randn(self.dim) * 0.25 * np.mean(range_bound) + 0.6 * direction\n                        xin = np.clip(x_best + step, lb, ub)\n                    else:\n                        # heavy orthogonal jump: random vector projected to low-coverage subspace\n                        # build subspace basis from smallest coverage directions\n                        cov_order = np.argsort(coverage)  # ascending: lowest coverage first\n                        ksub = min(max(1, int(np.sqrt(self.dim))), self.dim)\n                        basis = eigvecs[:, cov_order[:ksub]]\n                        coeff = rng.randn(ksub) * 0.6 * np.mean(range_bound)\n                        xin = np.clip(m + (basis @ coeff), lb, ub)\n                    fin = float(func(xin))\n                    evals += 1\n                    if fin < self.f_opt:\n                        self.f_opt = fin\n                        self.x_opt = xin.copy()\n                        last_improvement = evals\n                        stagn_count = 0\n                        # nudge mean\n                        m = 0.6 * m + 0.4 * xin\n\n            # periodic path-relinking among archive members to boost degree variance\n            if generation % 14 == 0 and len(archive) >= 3 and evals < budget:\n                ids = rng.choice(len(archive), size=3, replace=False)\n                xA = archive[ids[0]][1]\n                xB = archive[ids[1]][1]\n                xC = archive[ids[2]][1]\n                # build a small simplex of relinks\n                for alpha in [0.2, 0.5, 0.8]:\n                    if evals >= budget: break\n                    xr = np.clip(alpha * xA + (1 - alpha) * xB + rng.randn(self.dim) * 0.02 * np.mean(range_bound) + 0.05 * (xC - m), lb, ub)\n                    fr = float(func(xr))\n                    evals += 1\n                    if fr < self.f_opt:\n                        self.f_opt = fr\n                        self.x_opt = xr.copy()\n                        last_improvement = evals\n\n            # reduce lam when making steady progress\n            if evals - last_improvement <= max(1, 2 * self.dim // max(1, lam)):\n                if lam > lam_base:\n                    lam = max(lam_base, lam - 1)\n                stagn_count = max(0, stagn_count - 1)\n\n            generation += 1\n\n        # final ensure best candidate\n        if self.x_opt is None:\n            try:\n                fm = float(func(m))\n                if evals < budget:\n                    evals += 1\n                self.x_opt = m.copy()\n                self.f_opt = fm\n            except Exception:\n                xrand = rng.uniform(lb, ub)\n                self.x_opt = xrand\n                if evals < budget:\n                    self.f_opt = float(func(xrand))\n                    evals += 1\n                else:\n                    self.f_opt = np.inf\n\n        return float(self.f_opt), np.asarray(self.x_opt)", "configspace": "", "generation": 10, "feedback": "The algorithm EntropyOrthogonalEnsembleSearch scored 0.798 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a0cd859e-26de-43f5-92de-e84b7818d095"], "operator": null, "metadata": {"ast_features": {"Nodes": 3521.0, "Edges": 3520.0, "Max Degree": 64.0, "Min Degree": 1.0, "Mean Degree": 1.9994319795512638, "Degree Variance": 3.0911669593749234, "Transitivity": 0.0, "Max Depth": 19.0, "Min Depth": 2.0, "Mean Depth": 8.246457178065311, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3110564265155913, "Depth Entropy": 2.1250921705988235, "Assortativity": 0.0, "Average Eccentricity": 20.946038057370064, "Diameter": 29.0, "Radius": 15.0, "Edge Density": 0.00028401022436807724, "Average Shortest Path": 10.172012406082983, "mean_complexity": 12.4, "total_complexity": 62.0, "mean_token_count": 604.2, "total_token_count": 3021.0, "mean_parameter_count": 2.2, "total_parameter_count": 11.0}, "feature_guidance_action": "increase", "feature_guidance_feature_name": "Degree Variance", "aucs": [0.537154770521038, 0.46939004268347984, 0.8965552604179136, 0.9774697205348192, 0.9666287287357915, 0.967393319362954, 0.3027004039520875, 0.9445141321344812, 0.9678691572226712, 0.94986558781353]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-MA_BBOB-1", "seed": 1}
{"method_name": "ES-guided", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 4, "n_offspring": 16, "elitism": true, "feature_guided_mutation": true, "parent_selection": "tournament", "tournament_size": 2}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "4aa17339-d3bb-4e02-99f6-121388fc405d", "fitness": 0.8552607688180913, "name": "HAPSO_P", "description": "HAPSO-P — Higher-degree Adaptive Polynomial Surrogate with Projection and CMA-like sampling: fits adaptive-degree polynomials (up to degree 5 in low dims or in a learned low-dimensional subspace), uses surrogate-minimizer proposals inside a trust-region, and blends with covariance-guided sampling and heavy-tailed exploration for robust multi-scale search.", "code": "import numpy as np\nfrom math import comb\n\nclass HAPSO_P:\n    \"\"\"\n    HAPSO-P: Higher-degree Adaptive Polynomial Surrogate with Projection\n\n    Key ideas:\n    - Maintain CMA-like covariance and step-size control for affine-invariant exploration.\n    - Keep an archive of evaluated points; build local polynomial surrogates on a low-dimensional\n      projected subspace (PCA/top-eigvecs). Allow adaptive polynomial degree up to max_deg\n      (up to 5 in small dims), selected by simple cross-validation.\n    - Use analytic surrogate predictions (and gradients) to propose trust-region steps by solving\n      quadratic surrogate when possible or by cheap stochastic/minibatch local search on surrogate.\n    - Blend proposals from Gaussian sampling, surrogate optimization, mirrored and heavy-tailed sampling.\n    - Trust-region acceptance compares predicted vs actual improvement to adapt trust radius and surrogate usage.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False, max_deg=5):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.verbose = verbose\n        # Cap max degree sensibly by dimension; we'll still allow high degree in a low proj-subspace\n        self.user_max_deg = int(max_deg)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        if lb.shape == () or lb.size == 1:\n            lb = np.full(self.dim, float(lb))\n        if ub.shape == () or ub.size == 1:\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        span_norm = np.linalg.norm(span) if np.linalg.norm(span) > 0 else 1.0\n\n        budget = self.budget\n        eval_count = 0\n\n        best_holder = {'x': None, 'f': np.inf}\n\n        def safe_eval(x):\n            nonlocal eval_count\n            if eval_count >= budget:\n                raise StopIteration()\n            x = np.asarray(x, dtype=float)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            eval_count += 1\n            if f < best_holder['f']:\n                best_holder['f'] = float(f)\n                best_holder['x'] = x.copy()\n            return float(f), x.copy()\n\n        # initial stratified sampling\n        n_init = int(min(max(12, 6 + self.dim), max(1, budget // 10)))\n        samples = np.empty((n_init, self.dim))\n        for d in range(self.dim):\n            perm = self.rng.permutation(n_init)\n            strata = (perm + self.rng.rand(n_init)) / n_init\n            samples[:, d] = lb[d] + strata * span[d]\n        f_init = []\n        for i in range(n_init):\n            if eval_count >= budget:\n                break\n            try:\n                f, x = safe_eval(samples[i])\n            except StopIteration:\n                break\n            f_init.append((f, x))\n        if len(f_init) == 0:\n            center = lb + 0.5 * span\n            try:\n                f, x = safe_eval(center)\n            except StopIteration:\n                return best_holder['f'], best_holder['x']\n            f_init.append((f, x))\n\n        f_init.sort(key=lambda t: t[0])\n        mu = f_init[0][1].copy()\n        mu_f = float(f_init[0][0])\n        best_holder['x'] = mu.copy()\n        best_holder['f'] = mu_f\n\n        # strategy params\n        lam_base = int(max(6, min(100, 6 + int(np.sqrt(self.dim) * 8))))\n        lam = lam_base\n        sigma = 0.3 * span_norm / max(1.0, np.sqrt(self.dim))\n        if sigma <= 0: sigma = 0.1\n        C = np.eye(self.dim)\n        ps = np.zeros(self.dim)\n        pc = np.zeros(self.dim)\n\n        # CMA-style rates\n        mu_eff = max(1.0, lam / 2.0)\n        c_sigma = (mu_eff + 2) / (self.dim + mu_eff + 5)\n        d_sigma = 1 + 2 * max(0.0, np.sqrt((mu_eff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_c = (4 + mu_eff / self.dim) / (self.dim + 4 + 2 * mu_eff / self.dim)\n        c1 = 2.0 / ((self.dim + 1.3) ** 2 + mu_eff)\n        cmu = min(1 - c1, 2 * (mu_eff - 2 + 1 / mu_eff) / ((self.dim + 2) ** 2 + mu_eff))\n        eps = 1e-12\n\n        # Archive\n        archive_X = [mu.copy()]\n        archive_f = [mu_f]\n        archive_size = min(300, max(40, int(10 + 10 * np.log1p(self.dim))))\n\n        # trust-region and surrogate use\n        trust_radius = 0.5 * span_norm\n        trust_min = 1e-6\n        trust_max = 2.0 * span_norm\n        surrogate_prob = 0.25  # base probability to use surrogate proposals\n        heavy_prob = 0.05\n        bridge_prob = 0.04\n\n        stagn = 0\n        stagn_limit_local = 6 + int(self.dim / 3)\n        stagn_limit_restart = 60 + int(self.dim / 2)\n\n        iters = 0\n        restart_count = 0\n\n        # polynomial helpers: generate exponent multi-indices for m dims up to degree d (total degree)\n        from functools import lru_cache\n\n        @lru_cache(maxsize=128)\n        def multi_indices(m, d):\n            # returns list of tuples of length m with non-negative ints summing <= d\n            # we include all degrees 0..d\n            res = []\n            def rec(idx, sofar, left):\n                if idx == m:\n                    res.append(tuple(sofar))\n                    return\n                # allocate 0..left to position idx, but we still allow remaining dims to use up to left\n                for k in range(left + 1):\n                    sofar.append(k)\n                    rec(idx+1, sofar, left - k)\n                    sofar.pop()\n            rec(0, [], d)\n            return res\n\n        def build_design(Xproj, degree, reg=1e-8):\n            # Xproj shape (n, m)\n            n, m = Xproj.shape\n            idxs = multi_indices(m, degree)\n            # remove trailing high-degree combos where sum>degree? multi_indices already ensures sum<=d\n            P = np.empty((n, len(idxs)))\n            for j, exps in enumerate(idxs):\n                # compute prod x_i^exp_i\n                col = np.ones(n)\n                for k, e in enumerate(exps):\n                    if e == 0: continue\n                    col *= Xproj[:, k] ** e\n                P[:, j] = col\n            # fit using ridge: coeff = (P^T P + reg I)^-1 P^T y\n            return P, idxs\n\n        def fit_poly(Xproj, y, degree, reg=1e-8):\n            P, idxs = build_design(Xproj, degree)\n            # quick Tikhonov\n            try:\n                A = P.T.dot(P)\n                A += reg * np.eye(A.shape[0])\n                b = P.T.dot(y)\n                coeff = np.linalg.solve(A, b)\n                return coeff, idxs\n            except np.linalg.LinAlgError:\n                # fallback pseudo-inv\n                try:\n                    coeff = np.linalg.pinv(P).dot(y)\n                    return coeff, idxs\n                except Exception:\n                    return None, None\n\n        def predict_poly(xproj, coeff, idxs):\n            # xproj can be (m,) or (k,m)\n            x = np.asarray(xproj)\n            single = (x.ndim == 1)\n            if single:\n                x = x.reshape(1, -1)\n            n = x.shape[0]\n            P = np.empty((n, len(idxs)))\n            for j, exps in enumerate(idxs):\n                col = np.ones(n)\n                for k, e in enumerate(exps):\n                    if e == 0: continue\n                    col *= x[:, k] ** e\n                P[:, j] = col\n            pred = P.dot(coeff)\n            return pred[0] if single else pred\n\n        def grad_poly(xproj, coeff, idxs):\n            # gradient in projected coordinates (m,)\n            x = np.asarray(xproj).reshape(-1)\n            m = x.size\n            g = np.zeros(m)\n            for j, exps in enumerate(idxs):\n                c = coeff[j]\n                for k in range(m):\n                    e_k = exps[k]\n                    if e_k == 0:\n                        continue\n                    # compute product of x_i^e_i with exponent decremented at k\n                    val = c * e_k\n                    for t, e_t in enumerate(exps):\n                        if t == k:\n                            if e_k - 1 > 0:\n                                val *= x[t] ** (e_k - 1)\n                        else:\n                            if e_t > 0:\n                                val *= x[t] ** e_t\n                    g[k] += val\n            return g\n\n        def choose_degree_and_fit(X_near, f_near, m):\n            # choose degree from 1..max by CV (simple holdout) on the projected points X_near (n,m)\n            n = X_near.shape[0]\n            max_deg_allowed = min(self.user_max_deg, 5)\n            # if projection dim is large, reduce degree\n            if m >= 8:\n                max_deg_allowed = min(max_deg_allowed, 2)\n            elif m >= 6:\n                max_deg_allowed = min(max_deg_allowed, 3)\n            degrees = list(range(1, max_deg_allowed + 1))\n            best_score = np.inf\n            best_model = (None, None, 1)\n            # require at least (m+1) points to fit degree 1; else fallback degree 1\n            if n < max(4, m + 1):\n                deg = 1\n                coeff, idxs = fit_poly(X_near, f_near, deg, reg=1e-6)\n                return coeff, idxs, deg\n            # simple CV: hold 20% or at least 3 points as val\n            k_val = max(3, int(0.2 * n))\n            idxs_perm = np.arange(n)\n            self.rng.shuffle(idxs_perm)\n            val_idx = idxs_perm[:k_val]\n            train_idx = idxs_perm[k_val:]\n            Xtr, ftr = X_near[train_idx], np.asarray(f_near)[train_idx]\n            Xval, fval = X_near[val_idx], np.asarray(f_near)[val_idx]\n            for deg in degrees:\n                coeff, idxs = fit_poly(Xtr, ftr, deg, reg=1e-8)\n                if coeff is None:\n                    continue\n                pred = predict_poly(Xval, coeff, idxs)\n                mse = np.mean((pred - fval) ** 2)\n                # penalize by model complexity lightly\n                complexity = 0.001 * len(idxs)\n                score = mse + complexity\n                if score < best_score:\n                    best_score = score\n                    best_model = (coeff, idxs, deg)\n            if best_model[0] is None:\n                # fallback linear fit on whole set\n                coeff, idxs = fit_poly(X_near, f_near, 1, reg=1e-6)\n                return coeff, idxs, 1\n            return best_model\n\n        def proj_subspace(center, Xs, m_proj):\n            # compute top-m_proj eigenvectors of covariance of Xs around center\n            X = np.asarray(Xs)\n            if X.shape[0] <= 1:\n                return np.eye(self.dim)[:, :m_proj]\n            Xm = X - center[np.newaxis, :]\n            # small sample fallback\n            try:\n                S = Xm.T.dot(Xm) / max(1, X.shape[0] - 1)\n                vals, vecs = np.linalg.eigh(S)\n                idx = np.argsort(-vals)\n                vecs = vecs[:, idx]\n                # ensure orthonormal\n                return vecs[:, :m_proj]\n            except Exception:\n                Q = np.eye(self.dim)\n                return Q[:, :m_proj]\n\n        # main loop\n        try:\n            while eval_count < budget:\n                remaining = budget - eval_count\n                lam = min(lam_base + restart_count * 2, max(4, remaining))\n                lam = min(lam, remaining, 300)\n                # recompute weights\n                ranks = np.arange(1, lam + 1)\n                w_raw = np.log((lam + 1) / 2.0) - np.log(ranks)\n                w_raw = np.maximum(0.0, w_raw)\n                if w_raw.sum() <= 0:\n                    weights = np.ones(lam) / lam\n                else:\n                    weights = w_raw / w_raw.sum()\n                mu_eff = 1.0 / np.sum(weights ** 2)\n\n                # sqrtC via eig\n                try:\n                    valsC, vecsC = np.linalg.eigh(C + eps * np.eye(self.dim))\n                    valsC = np.clip(valsC, 1e-16, None)\n                    sqrtC = (vecsC * np.sqrt(valsC)) @ vecsC.T\n                    inv_sqrtC = (vecsC * (1.0/np.sqrt(valsC))) @ vecsC.T\n                except Exception:\n                    sqrtC = np.eye(self.dim)\n                    inv_sqrtC = np.eye(self.dim)\n\n                # build local dataset (nearest to mu)\n                if len(archive_X) > 5:\n                    arr = np.stack(archive_X[-min(len(archive_X), archive_size):], axis=0)\n                    arr_f = np.array(archive_f[-min(len(archive_f), archive_size):])\n                    dists = np.linalg.norm(arr - mu[np.newaxis, :], axis=1)\n                    order = np.argsort(dists)\n                    # pick up to N_near\n                    N_near = min(arr.shape[0], max(20, 6 * self.dim))\n                    near_idx = order[:N_near]\n                    X_near = arr[near_idx]\n                    f_near = arr_f[near_idx]\n                else:\n                    X_near = np.array(archive_X)\n                    f_near = np.array(archive_f)\n\n                # choose projection dimension\n                m_proj = min(self.dim, max(1, 6 if self.dim <= 10 else (4 if self.dim <= 30 else 2)))\n                # compute projection matrix\n                Pmat = proj_subspace(mu, X_near, m_proj)  # (dim, m_proj)\n                X_near_proj = (X_near - mu[np.newaxis, :]).dot(Pmat)  # center and project\n\n                # adapt max degree: increase if more data and dims small\n                dynamic_max_deg = min(self.user_max_deg, 5)\n                if m_proj >= 6:\n                    dynamic_max_deg = min(dynamic_max_deg, 4)\n                if len(X_near_proj) < 30:\n                    dynamic_max_deg = min(dynamic_max_deg, 3)\n\n                # fit polynomial surrogate in projected coords\n                coeff, idxs_poly, deg_chosen = choose_degree_and_fit(X_near_proj, f_near, m_proj)\n\n                # Candidate generation: mix of gaussian, surrogate-minimizer, heavy-tailed, bridge\n                offspring = np.empty((lam, self.dim))\n                f_off = np.empty(lam)\n                for i in range(lam):\n                    if eval_count >= budget:\n                        raise StopIteration()\n                    r = self.rng.rand()\n                    prob_sur = surrogate_prob + 0.2 * min(1.0, stagn / max(1, stagn_limit_local))\n                    prob_heavy = heavy_prob\n                    prob_br = bridge_prob\n                    if r < prob_sur and coeff is not None and eval_count + 1 < budget:\n                        # build surrogate minimizer in projected space within trust radius\n                        # if quadratic (deg 2) we can solve stationary point directly\n                        if deg_chosen == 2:\n                            # assemble quadratic forms from coeff and idxs_poly\n                            # find gradient: H x + g = 0 => x* = -H^{-1} g\n                            # build H and g in projected coords\n                            m = m_proj\n                            H = np.zeros((m, m))\n                            gvec = np.zeros(m)\n                            const = 0.0\n                            for j, exps in enumerate(idxs_poly):\n                                c = coeff[j]\n                                deg_sum = sum(exps)\n                                if deg_sum == 0:\n                                    const += c\n                                else:\n                                    # linear term\n                                    for k in range(m):\n                                        if exps[k] == 1 and deg_sum == 1:\n                                            gvec[k] += c\n                                    # quadratic terms\n                                    for p in range(m):\n                                        for q in range(m):\n                                            # coefficient for x_p x_q: if exps[p]>=1 and exps[q]>=1 and deg_sum==2 with exps[p]+exps[q]==2\n                                            # More general form: derivative yields terms; we compute H by second derivatives\n                                            e_p = exps[p]\n                                            e_q = exps[q]\n                                            if e_p >= 1 and e_q >= 1:\n                                                # second derivative contribution: c * e_p * e_q * x^(exps - unit_p - unit_q)\n                                                # For quadratic surrogate, exps sum==2 so remaining powers are zero -> constant\n                                                if deg_sum == 2:\n                                                    # contribution to H\n                                                    H[p, q] += c * e_p * e_q\n                            # make H symmetric\n                            H = 0.5 * (H + H.T)\n                            # solve\n                            try:\n                                # regularize\n                                H_reg = H + 1e-8 * np.eye(m)\n                                xstar_proj = -np.linalg.solve(H_reg, gvec)\n                            except Exception:\n                                xstar_proj = np.zeros(m)\n                        else:\n                            # for higher degree, sample many points in projected ball and take best predicted, then refine by gradient descent on surrogate\n                            m = m_proj\n                            n_trials = min(120, 40 + 8 * m)\n                            # sample around 0 projected center within trust_radius\n                            cand_proj = (self.rng.randn(n_trials, m) * (trust_radius / max(1.0, np.sqrt(m))) )\n                            preds = predict_poly(cand_proj, coeff, idxs_poly)\n                            best_idx = int(np.argmin(preds))\n                            xstar_proj = cand_proj[best_idx]\n                            # small gradient-based refinement (few steps)\n                            for _gd in range(6):\n                                g = grad_poly(xstar_proj, coeff, idxs_poly)\n                                # step\n                                step = -0.25 * (trust_radius / max(1.0, np.linalg.norm(g) + 1e-12)) * g\n                                xstar_proj = xstar_proj + step\n                        # map back to original space\n                        candidate = mu + Pmat.dot(xstar_proj)\n                        # add tiny perturbation\n                        candidate = candidate + 0.03 * sigma * self.rng.randn(self.dim)\n                    elif r < prob_sur + prob_br:\n                        # bridge between best and a far archive point\n                        if len(archive_X) > 1 and self.rng.rand() < 0.9:\n                            a = best_holder['x']\n                            b = archive_X[self.rng.randint(len(archive_X))]\n                            t = self.rng.rand()\n                            base = a * t + b * (1 - t)\n                        else:\n                            base = mu\n                        # Cauchy-like\n                        u = self.rng.randn(self.dim)\n                        v = self.rng.randn(self.dim)\n                        step = u / (v + 1e-12)\n                        candidate = base + 1.6 * sigma * step\n                    elif r < prob_sur + prob_br + prob_heavy:\n                        # heavy-tailed around mu\n                        u = self.rng.randn(self.dim)\n                        v = self.rng.randn(self.dim)\n                        step = u / (v + 1e-12)\n                        candidate = mu + 1.3 * sigma * step\n                    else:\n                        # gaussian sample with covariance\n                        z = self.rng.randn(self.dim)\n                        step = sqrtC.dot(z)\n                        candidate = mu + sigma * step\n                    candidate = np.minimum(np.maximum(candidate, lb), ub)\n                    try:\n                        f_c, x_c = safe_eval(candidate)\n                    except StopIteration:\n                        raise\n                    offspring[i] = x_c\n                    f_off[i] = f_c\n\n                # sort\n                idx = np.argsort(f_off)\n                offspring = offspring[idx]\n                f_off = f_off[idx]\n\n                # recombination\n                k = len(f_off)\n                w_k = np.log((k + 1) / 2.0) - np.log(np.arange(1, k + 1))\n                w_k = np.maximum(0.0, w_k)\n                if w_k.sum() <= 0:\n                    w_k = np.ones(k) / k\n                else:\n                    w_k = w_k / w_k.sum()\n                new_mu = np.sum(offspring * w_k[:, None], axis=0)\n\n                # sigma control path\n                y = (new_mu - mu) / (sigma + 1e-16)\n                ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * mu_eff) * (inv_sqrtC.dot(y))\n                norm_ps = np.linalg.norm(ps)\n                expected_norm = np.sqrt(self.dim) * (1 - 1/(4*self.dim) + 1/(21*(self.dim**2)))\n                sigma *= np.exp((c_sigma / d_sigma) * (norm_ps / (expected_norm + 1e-12) - 1))\n\n                # covariance path\n                h_sigma_cond = (norm_ps / np.sqrt(1 - (1 - c_sigma) ** (2 * (iters + 1)))) < (1.4 + 2 / (self.dim + 1)) * expected_norm\n                h_sigma = 1.0 if h_sigma_cond else 0.0\n                pc = (1 - c_c) * pc + h_sigma * np.sqrt(c_c * (2 - c_c) * mu_eff) * y\n\n                diffs = (offspring - new_mu) / (sigma + 1e-16)\n                rank_mu_cov = np.zeros((self.dim, self.dim))\n                for wi, d in zip(w_k, diffs):\n                    rank_mu_cov += wi * np.outer(d, d)\n\n                C = (1 - c1 - cmu) * C + c1 * np.outer(pc, pc) + cmu * rank_mu_cov\n                C = 0.5 * (C + C.T) + eps * np.eye(self.dim)\n\n                # update mu, mu_f\n                best_off_f = float(f_off[0])\n                best_off_x = offspring[0].copy()\n                improved = False\n                if best_off_f < mu_f - 1e-12:\n                    mu_f = best_off_f\n                    improved = True\n                    stagn = 0\n                else:\n                    stagn += 1\n                mu_old = mu.copy()\n                mu = new_mu.copy()\n\n                # update archive with good points\n                for i in range(min(6, len(f_off))):\n                    if f_off[i] <= best_holder['f'] * 1.000001:\n                        archive_X.append(offspring[i].copy())\n                        archive_f.append(float(f_off[i]))\n                if len(archive_X) > archive_size:\n                    archive_X = archive_X[-archive_size:]\n                    archive_f = archive_f[-archive_size:]\n\n                # surrogate verification: try one surrogate-refined trust proposal per iteration if confident\n                if coeff is not None and eval_count < budget and len(X_near_proj) >= max(6, m_proj+1):\n                    # propose surrogate minimizer again (conservative scale)\n                    try:\n                        # reuse earlier procedure: find xstar_proj\n                        if deg_chosen == 2:\n                            # build H and g for solved quadratic using previously defined logic\n                            m = m_proj\n                            H = np.zeros((m, m))\n                            gvec = np.zeros(m)\n                            for j, exps in enumerate(idxs_poly):\n                                c = coeff[j]\n                                deg_sum = sum(exps)\n                                if deg_sum == 0:\n                                    continue\n                                for k in range(m):\n                                    if exps[k] == 1 and deg_sum == 1:\n                                        gvec[k] += c\n                                for p in range(m):\n                                    for q in range(m):\n                                        e_p = exps[p]; e_q = exps[q]\n                                        if e_p >= 1 and e_q >= 1 and deg_sum == 2:\n                                            H[p, q] += c * e_p * e_q\n                            H = 0.5 * (H + H.T)\n                            try:\n                                H_reg = H + 1e-8 * np.eye(m)\n                                xstar_proj = -np.linalg.solve(H_reg, gvec)\n                            except Exception:\n                                xstar_proj = np.zeros(m)\n                        else:\n                            # pick best among many predicted samples\n                            trials = min(160, 60 + 10 * m_proj)\n                            cand_proj = (self.rng.randn(trials, m_proj) * (0.8 * trust_radius / max(1.0, np.sqrt(m_proj))))\n                            preds = predict_poly(cand_proj, coeff, idxs_poly)\n                            best_idx = int(np.argmin(preds))\n                            xstar_proj = cand_proj[best_idx]\n                            # refine small GD steps\n                            for _gd in range(4):\n                                g = grad_poly(xstar_proj, coeff, idxs_poly)\n                                step = -0.15 * (trust_radius / max(1.0, np.linalg.norm(g) + 1e-12)) * g\n                                xstar_proj += step\n                        candidate = mu + Pmat.dot(xstar_proj)\n                        candidate = np.minimum(np.maximum(candidate, lb), ub)\n                        f_c, x_c = safe_eval(candidate)\n                    except StopIteration:\n                        raise\n                    # predicted improvement\n                    pred = predict_poly(xstar_proj.reshape(1, -1), coeff, idxs_poly)[0]\n                    actual = mu_f - f_c\n                    # compute baseline surrogate at projected 0 (mu)\n                    baseline = predict_poly(np.zeros((1, m_proj)), coeff, idxs_poly)[0]\n                    predicted_impr = baseline - pred\n                    ratio = (actual) / (predicted_impr + 1e-12) if predicted_impr != 0 else (1.0 if actual > 0 else -1.0)\n                    if ratio > 0.6:\n                        # accept\n                        mu = x_c.copy()\n                        mu_f = float(f_c)\n                        stagn = 0\n                        trust_radius = min(trust_radius * 1.4, trust_max)\n                        surrogate_prob = min(0.9, surrogate_prob * 1.08)\n                    elif ratio > 0.15:\n                        mu = x_c.copy()\n                        mu_f = float(f_c)\n                        stagn = 0\n                        trust_radius = min(trust_radius * 1.12, trust_max)\n                    else:\n                        trust_radius = max(trust_radius * 0.6, trust_min)\n                        surrogate_prob = max(0.02, surrogate_prob * 0.85)\n                    archive_X.append(x_c.copy()); archive_f.append(float(f_c))\n                    if len(archive_X) > archive_size:\n                        archive_X = archive_X[-archive_size:]; archive_f = archive_f[-archive_size:]\n\n                # local probing when stagnating moderately\n                if stagn >= stagn_limit_local and eval_count < budget:\n                    n_probes = min(10, max(3, self.dim // 2))\n                    try:\n                        vals, vecs = np.linalg.eigh(C)\n                    except Exception:\n                        vals = np.ones(self.dim); vecs = np.eye(self.dim)\n                    order = np.argsort(-vals)\n                    dirs = [vecs[:, order[i]] for i in range(min(self.dim, n_probes))]\n                    while len(dirs) < n_probes:\n                        v = self.rng.randn(self.dim)\n                        for d in dirs:\n                            v -= np.dot(v, d) * d\n                        nrm = np.linalg.norm(v) + 1e-12\n                        dirs.append(v / nrm)\n                    sigma_local = sigma * 0.28\n                    for d in dirs:\n                        if eval_count >= budget: break\n                        x = mu + sigma_local * sqrtC.dot(d)\n                        x = np.minimum(np.maximum(x, lb), ub)\n                        try:\n                            f_x, x_c = safe_eval(x)\n                        except StopIteration:\n                            raise\n                        if f_x < mu_f - 1e-12:\n                            mu_f = f_x; mu = x_c.copy(); stagn = 0\n                    sigma *= 1.05\n\n                # soft restart on severe stagnation\n                if stagn >= stagn_limit_restart and eval_count < budget:\n                    restart_count += 1\n                    stagn = 0\n                    if best_holder['x'] is not None:\n                        perturb_scale = 0.25 * span_norm / max(1.0, np.sqrt(self.dim)) * (1.0 + 0.6 * restart_count)\n                        mu = best_holder['x'].copy() + perturb_scale * self.rng.randn(self.dim)\n                        mu = np.minimum(np.maximum(mu, lb), ub)\n                        try:\n                            mu_f, mu = safe_eval(mu)\n                        except StopIteration:\n                            raise\n                    else:\n                        mu = lb + self.rng.rand(self.dim) * span\n                        try:\n                            mu_f, mu = safe_eval(mu)\n                        except StopIteration:\n                            raise\n                    ps *= 0.3; pc *= 0.0\n                    sigma = max(sigma, 0.2 * span_norm / max(1.0, np.sqrt(self.dim))) * (1.3 + 0.12 * restart_count)\n                    lam_base = min(400, lam_base + 6)\n                    trust_radius = max(trust_radius * 0.8, 1e-6)\n\n                iters += 1\n\n        except StopIteration:\n            pass\n\n        self.x_opt = best_holder['x']\n        self.f_opt = best_holder['f']\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm HAPSO_P scored 0.855 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["83ec50cf-a260-47cd-b3ec-276ab86138ae"], "operator": null, "metadata": {"ast_features": {"Nodes": 5725.0, "Edges": 5724.0, "Max Degree": 64.0, "Min Degree": 1.0, "Mean Degree": 1.999650655021834, "Degree Variance": 2.6508295722812307, "Transitivity": 0.0, "Max Depth": 19.0, "Min Depth": 2.0, "Mean Depth": 9.178720181749338, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3229044354541462, "Depth Entropy": 2.3075266744326406, "Assortativity": 0.0, "Average Eccentricity": 21.134323144104805, "Diameter": 30.0, "Radius": 15.0, "Edge Density": 0.00017467248908296942, "Average Shortest Path": 11.392444529888708, "mean_complexity": 11.818181818181818, "total_complexity": 130.0, "mean_token_count": 445.8181818181818, "total_token_count": 4904.0, "mean_parameter_count": 3.0, "total_parameter_count": 33.0}, "feature_guidance_action": "increase", "feature_guidance_feature_name": "Max Degree", "aucs": [0.31163092930585223, 0.8098598363812652, 0.9356814636021908, 0.9720341543373674, 0.9544106970930246, 0.886006443623115, 0.928355017855006, 0.8918505408554808, 0.9462808103466642, 0.9164977947809466]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-MA_BBOB-2", "seed": 2}
{"method_name": "ES-guided", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 4, "n_offspring": 16, "elitism": true, "feature_guided_mutation": true, "parent_selection": "tournament", "tournament_size": 2}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "1a4a20f6-6ca9-420b-9212-c77274d42715", "fitness": 0.7037561928606373, "name": "BRIDAS_XT", "description": "BRIDAS-XT — an expanded Bridge-Directed Adaptive Subspace Explorer with layered moves, richer bandit arbitration, per-dimension adaptation, incremental low-rank covariance/PCA, path-relink & ranked-PCA sampling, and budget-aware local trust batches (aimed at stronger exploration-exploitation balance).", "code": "import numpy as np\n\nclass BRIDAS_XT:\n    \"\"\"\n    BRIDAS-XT: Extended Bridge-Directed Adaptive Subspace Explorer\n    - __init__(budget, dim, ...) many tunable hyper-parameters (to raise mean parameter count)\n    - __call__(func) runs optimization using at most self.budget evaluations\n    Returns (best_f, best_x_array)\n    \"\"\"\n\n    def __init__(self, budget, dim, seed=None,\n                 pop_size=None,\n                 init_sigma=0.25,\n                 elite_frac=0.2,\n                 mem_size=None,\n                 levy_alpha=1.4,\n                 cov_rank=6,\n                 cov_decay=0.95,\n                 bandit_beta=1.0,\n                 bandit_temp=0.9,\n                 bandit_thompson_prob=0.15,\n                 local_batch=6,\n                 trust_init=0.45,\n                 trust_shrink=0.75,\n                 trust_expand=1.2,\n                 max_clusters=4,\n                 surrogate_points_mult=4,\n                 surrogate_ridge=1e-6,\n                 per_dim_adapt=0.6,\n                 ranked_pca_frac=0.6,\n                 path_relink_prob=0.12,\n                 bridge_bias=0.55,\n                 extra_noise=0.02,\n                 verbose=False):\n        # basic\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed) if seed is not None else np.random\n\n        # population and memory\n        self.pop_size = pop_size\n        self.init_sigma = float(init_sigma)\n        self.elite_frac = float(elite_frac)\n        self.mem_size = int(mem_size) if mem_size is not None else max(80, 12 * self.dim)\n        self.levy_alpha = float(levy_alpha)\n\n        # covariance / PCA\n        self.cov_rank = int(max(1, min(self.dim, cov_rank)))\n        self.cov_decay = float(cov_decay)\n\n        # bandit controller\n        self.bandit_beta = float(bandit_beta)\n        self.bandit_temp = float(bandit_temp)\n        self.bandit_thompson_prob = float(bandit_thompson_prob)\n\n        # trust/local\n        self.local_batch = int(max(1, local_batch))\n        self.trust_init = float(trust_init)\n        self.trust_shrink = float(trust_shrink)\n        self.trust_expand = float(trust_expand)\n        self.max_clusters = int(max_clusters)\n\n        # surrogate and heuristics\n        self.surrogate_points_mult = int(surrogate_points_mult)\n        self.surrogate_ridge = float(surrogate_ridge)\n        self.per_dim_adapt = float(per_dim_adapt)\n        self.ranked_pca_frac = float(ranked_pca_frac)\n        self.path_relink_prob = float(path_relink_prob)\n        self.bridge_bias = float(bridge_bias)\n        self.extra_noise = float(extra_noise)\n\n        self.verbose = bool(verbose)\n\n        # many extra hyperparameters (increase mean parameter count)\n        self._hp = {\n            'levy_big_prob': 0.18,\n            'levy_scales': (0.6, 3.5),\n            'iso_scale': 0.7,\n            'pc_noise_scale': 0.22,\n            'recomb_scale': 0.95,\n            'cov_sample_scale': 1.0,\n            'min_move_prob': 0.02,\n            'surrogate_min_points': max(10, min(120, self.dim * self.surrogate_points_mult)),\n            'bridge_ortho_noise': 0.22,\n            'path_relink_steps': max(3, int(0.12 * self.budget)),\n            'ranked_pca_topk': max(2, int(0.2 * self.dim))\n        }\n\n    def _get_bounds(self, func):\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            lb = np.broadcast_to(lb, (self.dim,))\n            ub = np.broadcast_to(ub, (self.dim,))\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n        return lb, ub\n\n    def _reflect(self, x, lb, ub):\n        x = x.copy()\n        span = ub - lb\n        for i in range(len(x)):\n            if span[i] <= 0:\n                x[i] = lb[i]\n                continue\n            v = (x[i] - lb[i]) / span[i]\n            if v < 0 or v > 1:\n                k = np.floor(v)\n                frac = v - k\n                if int(k) % 2 == 0:\n                    x[i] = lb[i] + frac * span[i]\n                else:\n                    x[i] = ub[i] - frac * span[i]\n        return x\n\n    def _levy(self, alpha, dim):\n        # Mantegna-like elementwise approx (stabilized)\n        u = self.rng.normal(0, 1, size=dim)\n        v = self.rng.normal(0, 1, size=dim)\n        v = np.where(np.abs(v) < 1e-12, 1e-12, v)\n        step = u / (np.abs(v) ** (1.0 / alpha))\n        denom = np.median(np.abs(step)) + 1e-12\n        return step / denom\n\n    def _sample_multiscale_levy(self, dim):\n        s0, s1 = self._hp['levy_scales']\n        if self.rng.rand() < (1.0 - self._hp['levy_big_prob']):\n            return self._levy(self.levy_alpha, dim) * s0\n        else:\n            return self._levy(max(1.05, 0.8 * self.levy_alpha), dim) * s1\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        dim = self.dim\n\n        # determine population size\n        default_pop = max(8, int(8 + np.sqrt(dim) * 2.0))\n        if self.pop_size is None:\n            pop_size = min(default_pop, max(6, self.budget // 6))\n        else:\n            pop_size = max(4, min(self.pop_size, max(4, self.budget // 6)))\n\n        elite_size = max(1, int(np.ceil(self.elite_frac * pop_size)))\n        range_mean = (ub - lb).mean()\n        sigma_global = max(1e-9, self.init_sigma * range_mean)\n\n        eval_count = 0\n\n        def eval_point(x):\n            nonlocal eval_count\n            if eval_count >= self.budget:\n                return None\n            xr = self._reflect(np.asarray(x, dtype=float), lb, ub)\n            f = float(func(xr))\n            eval_count += 1\n            return xr, f\n\n        # archive and memories\n        archive = []  # dicts: {'x','f','age','cluster'}\n        dir_memory = []  # recent normalized displacement vectors\n        cov_components = []  # (vec, weight)\n        diag_scale = np.ones(dim)  # per-dimension adaptation (multiplicative)\n        pca_buffer = []  # store recent improvements for incremental PCA\n        # expanded move set\n        move_names = [\n            'levy', 'recomb', 'pc', 'iso', 'cov', 'trust', 'bridge',\n            'surrogate', 'path_relink', 'ranked_pca', 'gauss_mix'\n        ]\n        n_moves = len(move_names)\n        move_tries = np.ones(n_moves)\n        move_success = np.ones(n_moves)\n\n        clusters = []\n\n        # initialization: sample initial points across domain but not exhaust budget\n        initial = int(min(pop_size, max(8, int(self.budget * 0.06))))\n        for _ in range(initial):\n            x0 = self.rng.uniform(lb, ub)\n            res = eval_point(x0)\n            if res is None:\n                break\n            x_eval, f_eval = res\n            archive.append({'x': x_eval.copy(), 'f': f_eval, 'age': 0, 'cluster': -1})\n\n        if len(archive) == 0:\n            res = eval_point(self.rng.uniform(lb, ub))\n            if res is None:\n                return float('inf'), np.zeros(dim)\n            x_eval, f_eval = res\n            archive.append({'x': x_eval.copy(), 'f': f_eval, 'age': 0, 'cluster': -1})\n\n        archive.sort(key=lambda e: e['f'])\n        best = archive[0]['x'].copy()\n        f_best = float(archive[0]['f'])\n\n        def rebuild_clusters():\n            nonlocal clusters\n            clusters = []\n            k = min(self.max_clusters, max(1, elite_size))\n            elites = archive[:k]\n            for e in elites:\n                clusters.append({'center': e['x'].copy(),\n                                 'best_f': e['f'],\n                                 'radius': self.trust_init * range_mean,\n                                 'no_improve': 0})\n            # assign\n            if len(clusters) > 0:\n                centers = np.vstack([c['center'] for c in clusters])\n                for a in archive:\n                    d = np.linalg.norm(centers - a['x'], axis=1)\n                    a['cluster'] = int(np.argmin(d))\n            else:\n                for a in archive:\n                    a['cluster'] = -1\n\n        rebuild_clusters()\n        recent_success = []\n\n        # simple surrogate: weighted quadratic fit f ≈ a + b^T z + c*||z||^2 (centered)\n        def fit_simple_quadratic(points):\n            m = len(points)\n            if m < 4:\n                return None\n            X = np.vstack([p['x'] for p in points])\n            y = np.array([p['f'] for p in points])\n            x0 = X.mean(axis=0)\n            Z = X - x0\n            Z2 = np.sum(Z * Z, axis=1).reshape(-1, 1)\n            A = np.hstack([np.ones((m, 1)), Z, Z2])\n            lam = self.surrogate_ridge\n            try:\n                ATA = A.T.dot(A) + lam * np.eye(A.shape[1])\n                coeff = np.linalg.solve(ATA, A.T.dot(y))\n            except Exception:\n                try:\n                    coeff, *_ = np.linalg.lstsq(A, y, rcond=None)\n                except Exception:\n                    return None\n            a = coeff[0]\n            b = coeff[1:1 + dim]\n            c = float(coeff[-1])\n            return {'a': a, 'b': b, 'c': c, 'x0': x0}\n\n        # incremental PCA helper using pca_buffer to compute principal axes\n        def compute_incremental_pca(k):\n            if len(pca_buffer) < 3:\n                return None, None\n            X = np.vstack(pca_buffer[-min(len(pca_buffer), 4 * self.dim):])\n            if X.shape[0] <= 1:\n                return None, None\n            Xc = X - X.mean(axis=0)\n            try:\n                U, s, Vt = np.linalg.svd(Xc, full_matrices=False)\n                pcs = Vt[:k]\n                svals = s[:k]\n                return pcs, svals\n            except Exception:\n                return None, None\n\n        # main loop\n        while eval_count < self.budget:\n            remaining = self.budget - eval_count\n            t = 1.0 - eval_count / max(1.0, self.budget)\n            sigma = sigma_global * (0.5 + 0.5 * t)\n\n            # Bandit arbitration: UCB + occasional Thompson-sampling mix\n            total_tries = move_tries.sum()\n            avg_success = move_success / (move_tries + 1e-12)\n            ucb = avg_success + self.bandit_beta * np.sqrt(np.log(np.maximum(1.0, total_tries)) / (move_tries + 1e-12))\n            ucb = np.maximum(ucb, 1e-8)\n            probs = np.exp(self.bandit_temp * (ucb - ucb.max()))\n            probs = probs / (probs.sum() + 1e-12)\n            probs = np.maximum(probs, self._hp['min_move_prob'])\n            probs = probs / probs.sum()\n            # occasionally replace with Thompson-like sampling\n            if self.rng.rand() < self.bandit_thompson_prob:\n                samples = self.rng.gamma(shape=move_success + 1e-6, scale=1.0)\n                probs = np.maximum(samples, 1e-12)\n                probs = probs / probs.sum()\n            mv = self.rng.choice(n_moves, p=probs)\n            mv_descr = move_names[mv]\n\n            x_candidate = None\n\n            if mv_descr == 'levy':\n                step = self._sample_multiscale_levy(dim)\n                x_candidate = best + step * sigma * (0.4 + 0.6 * self.rng.rand()) * diag_scale\n\n            elif mv_descr == 'recomb':\n                pool = archive[:max(elite_size, 4)]\n                if len(pool) < 3:\n                    pool = archive\n                if len(pool) >= 3:\n                    idxs = self.rng.choice(len(pool), 3, replace=False)\n                else:\n                    idxs = self.rng.choice(len(archive), 3, replace=False)\n                xa = pool[idxs[0]]['x']\n                xb = pool[idxs[1]]['x']\n                xc = pool[idxs[2]]['x']\n                F = self.rng.uniform(0.2, 1.0) * self._hp['recomb_scale']\n                noise = self.rng.normal(0, 0.22 * sigma, size=dim) * diag_scale\n                x_candidate = xa + F * (xb - xc) + noise\n\n            elif mv_descr == 'pc':\n                if len(dir_memory) >= 8:\n                    M = np.vstack(dir_memory[-min(len(dir_memory), self.mem_size):])\n                    try:\n                        U, svals, Vt = np.linalg.svd(M, full_matrices=False)\n                        D = min(self.cov_rank, Vt.shape[0])\n                        pcs = Vt[:D]\n                        coeffs = self.rng.normal(0, 1.0, size=D) * (svals[:D] / (svals[0] + 1e-12))\n                        dir_part = np.dot(coeffs, pcs)\n                        ort = self.rng.normal(0, self._hp['pc_noise_scale'] * sigma, size=dim) * diag_scale\n                        x_candidate = best + dir_part * sigma * 0.9 + ort\n                    except Exception:\n                        x_candidate = best + self.rng.normal(0, 0.9 * sigma, size=dim) * diag_scale\n                else:\n                    x_candidate = best + self.rng.normal(0, 0.9 * sigma, size=dim) * diag_scale\n\n            elif mv_descr == 'iso':\n                # sample around a biased center (top elites more likely)\n                weights = np.linspace(1.5, 0.3, len(archive))\n                weights = np.maximum(weights, 0.01)\n                weights = weights / weights.sum()\n                idx = self.rng.choice(len(archive), p=weights)\n                center = archive[idx]['x']\n                x_candidate = center + self.rng.normal(0, self._hp['iso_scale'] * sigma, size=dim) * diag_scale\n\n            elif mv_descr == 'cov':\n                if len(cov_components) >= 1:\n                    take_k = min(len(cov_components), self.cov_rank)\n                    z = self.rng.normal(0, 1.0, size=take_k)\n                    vec = np.zeros(dim)\n                    for i, (v, w) in enumerate(cov_components[:take_k]):\n                        vec += z[i] * np.sqrt(max(w, 1e-12)) * v\n                    vec = vec * self._hp['cov_sample_scale']\n                    x_candidate = best + vec * sigma + self.rng.normal(0, 0.18 * sigma, size=dim) * diag_scale\n                else:\n                    x_candidate = best + self.rng.normal(0, sigma, size=dim) * diag_scale\n\n            elif mv_descr == 'trust':\n                if len(clusters) == 0:\n                    x_candidate = best + self.rng.normal(0, 0.6 * sigma, size=dim) * diag_scale\n                else:\n                    cluster_scores = np.array([max(1e-6, (c['best_f'] - f_best) + 1e-6) for c in clusters])\n                    cluster_probs = np.exp(-cluster_scores / (cluster_scores.mean() + 1e-12))\n                    cluster_probs = cluster_probs / cluster_probs.sum()\n                    cid = self.rng.choice(len(clusters), p=cluster_probs)\n                    cl = clusters[cid]\n                    r = cl['radius']\n                    # smaller draw around center biased to center if center is good\n                    x_candidate = cl['center'] + self.rng.normal(0, 0.55 * r, size=dim) * diag_scale\n                    if self.rng.rand() < 0.25:\n                        x_candidate = cl['center'] + self.rng.normal(0, 0.35 * r, size=dim) * diag_scale\n\n            elif mv_descr == 'bridge':\n                # Bridge between distant archive points (biased toward elites)\n                if len(archive) >= 2:\n                    top_k = min(len(archive), max(4, elite_size))\n                    p = np.zeros(len(archive))\n                    p[:top_k] = (1.0 + np.arange(top_k)[::-1])\n                    p[top_k:] = 1.0\n                    p = p / p.sum()\n                    idx_a = self.rng.choice(len(archive), p=p)\n                    a = archive[idx_a]['x']\n                    dists = np.linalg.norm(np.vstack([e['x'] for e in archive]) - a, axis=1)\n                    far_threshold = np.percentile(dists, 70)\n                    far_indices = np.where(dists >= far_threshold)[0]\n                    if len(far_indices) == 0:\n                        idx_b = self.rng.choice(len(archive))\n                    else:\n                        if self.rng.rand() < self.bridge_bias:\n                            idx_b = far_indices[self.rng.randint(0, len(far_indices))]\n                        else:\n                            idx_b = self.rng.choice(len(archive))\n                    b = archive[idx_b]['x']\n                    frac = self.rng.normal(0.5, 0.18)\n                    frac = np.clip(frac, -0.25, 1.25)\n                    line_point = a + frac * (b - a)\n                    ort = self.rng.normal(0, self._hp['bridge_ortho_noise'] * sigma, size=dim) * diag_scale\n                    line_levy = self._levy(self.levy_alpha, dim)\n                    line_dir = (b - a)\n                    if np.linalg.norm(line_dir) > 1e-12:\n                        line_dir = line_dir / (np.linalg.norm(line_dir))\n                    else:\n                        line_dir = self.rng.normal(0, 1.0, size=dim)\n                        line_dir = line_dir / (np.linalg.norm(line_dir) + 1e-12)\n                    jump = (np.median(np.abs(line_levy)) + 1e-12) * (0.45 + 0.55 * self.rng.rand())\n                    x_candidate = line_point + line_dir * (0.6 * sigma) * jump + ort\n                else:\n                    x_candidate = best + self.rng.normal(0, sigma, size=dim) * diag_scale\n\n            elif mv_descr == 'surrogate':\n                # Fit surrogate on top points and propose minimizer or descent\n                mreq = self._hp['surrogate_min_points']\n                m = min(len(archive), max(8, mreq))\n                points = archive[:m]\n                model = fit_simple_quadratic(points)\n                if model is not None:\n                    bvec = model['b']\n                    c = model['c']\n                    x0 = model['x0']\n                    if abs(c) > 1e-12 and c > 0:\n                        x_pred = x0 - bvec / (2.0 * c)\n                        x_candidate = best + 0.75 * (x_pred - best) + self.rng.normal(0, 0.24 * sigma, size=dim) * diag_scale\n                    else:\n                        if np.linalg.norm(bvec) > 1e-12:\n                            dirp = -bvec / (np.linalg.norm(bvec) + 1e-12)\n                            step_len = sigma * (0.5 + 0.7 * self.rng.rand())\n                            x_candidate = best + dirp * step_len + self.rng.normal(0, 0.24 * sigma, size=dim) * diag_scale\n                        else:\n                            x_candidate = best + self.rng.normal(0, 0.6 * sigma, size=dim) * diag_scale\n                else:\n                    x_candidate = best + self.rng.normal(0, 0.6 * sigma, size=dim) * diag_scale\n\n            elif mv_descr == 'path_relink':\n                # Choose two elites and interpolate along the better-to-worse path, sampling several points and picking a candidate\n                if len(archive) >= 2 and self.rng.rand() < self.path_relink_prob:\n                    top_k = max(2, min(len(archive), elite_size))\n                    i, j = self.rng.choice(top_k, 2, replace=False)\n                    a = archive[i]['x']\n                    b = archive[j]['x']\n                    better = a if archive[i]['f'] <= archive[j]['f'] else b\n                    worse = b if archive[i]['f'] <= archive[j]['f'] else a\n                    steps = min(self._hp['path_relink_steps'], max(2, int(3 + self.dim // 4)))\n                    fractions = np.linspace(0.1, 0.9, steps)\n                    candidate_pool = []\n                    for frac in fractions:\n                        p = better + frac * (worse - better) + self.rng.normal(0, 0.25 * sigma, size=dim) * diag_scale\n                        p = self._reflect(p, lb, ub)\n                        candidate_pool.append(p)\n                    # evaluate greedily until budget allows to pick best; but do not exceed remaining too much\n                    best_local_x = None\n                    best_local_f = None\n                    for p in candidate_pool:\n                        if eval_count >= self.budget:\n                            break\n                        res = eval_point(p)\n                        if res is None:\n                            break\n                        px, pf = res\n                        if best_local_f is None or pf < best_local_f:\n                            best_local_f = pf\n                            best_local_x = px.copy()\n                    if best_local_x is not None:\n                        x_candidate = best_local_x.copy()\n                    else:\n                        x_candidate = best + self.rng.normal(0, sigma, size=dim) * diag_scale\n                else:\n                    x_candidate = best + self.rng.normal(0, sigma, size=dim) * diag_scale\n\n            elif mv_descr == 'ranked_pca':\n                # sample along top principal components computed from good archive members\n                top_m = max(4, min(len(archive), int(self.ranked_pca_frac * len(archive))))\n                X = np.vstack([e['x'] for e in archive[:top_m]])\n                if X.shape[0] > 1:\n                    Xc = X - X.mean(axis=0)\n                    try:\n                        U, svals, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        k = min(self._hp['ranked_pca_topk'], Vt.shape[0])\n                        pcs = Vt[:k]\n                        z = self.rng.normal(0, 1.0, size=k) * (svals[:k] / (svals[0] + 1e-12))\n                        vec = np.dot(z, pcs)\n                        x_candidate = best + vec * sigma * 0.9 + self.rng.normal(0, 0.18 * sigma, size=dim) * diag_scale\n                    except Exception:\n                        x_candidate = best + self.rng.normal(0, sigma, size=dim) * diag_scale\n                else:\n                    x_candidate = best + self.rng.normal(0, sigma, size=dim) * diag_scale\n\n            elif mv_descr == 'gauss_mix':\n                # sample from mixture: global gaussian + local elite gaussian\n                if len(archive) >= 3:\n                    if self.rng.rand() < 0.6:\n                        # local around random elite\n                        idx = self.rng.randint(0, min(len(archive), max(3, elite_size)))\n                        center = archive[idx]['x']\n                        x_candidate = center + self.rng.normal(0, 0.35 * sigma, size=dim) * diag_scale\n                    else:\n                        # global gaussian\n                        means = np.vstack([e['x'] for e in archive[:max(3, elite_size)]])\n                        gmean = means.mean(axis=0)\n                        x_candidate = gmean + self.rng.normal(0, 0.8 * sigma, size=dim) * diag_scale\n                else:\n                    x_candidate = best + self.rng.normal(0, sigma, size=dim) * diag_scale\n\n            else:\n                x_candidate = best + self.rng.normal(0, sigma, size=dim) * diag_scale\n\n            # small jitter and reflect\n            x_candidate = x_candidate + self.rng.normal(0, self.extra_noise * sigma, size=dim) * diag_scale\n            x_candidate = self._reflect(x_candidate, lb, ub)\n\n            # evaluate candidate\n            res = eval_point(x_candidate)\n            if res is None:\n                break\n            x_eval, f_eval = res\n\n            move_tries[mv] += 1.0\n\n            # archive insertion: soft-elitist but maintain diversity\n            if len(archive) < pop_size:\n                archive.append({'x': x_eval.copy(), 'f': f_eval, 'age': 0, 'cluster': -1})\n            else:\n                worst_idx = int(np.argmax([e['f'] for e in archive]))\n                ages = np.array([e['age'] for e in archive], dtype=float)\n                oldest_idx = int(np.argmax(ages))\n                replace_idx = worst_idx if self.rng.rand() < 0.85 else oldest_idx\n                if f_eval < archive[replace_idx]['f'] or self.rng.rand() < 0.01:\n                    archive[replace_idx] = {'x': x_eval.copy(), 'f': f_eval, 'age': 0, 'cluster': -1}\n\n            # increment ages\n            for e in archive:\n                e['age'] += 1\n\n            # enforce archive size cap for memory efficiency\n            archive.sort(key=lambda e: e['f'])\n            if len(archive) > 5 * pop_size:\n                archive = archive[:5 * pop_size]\n            if len(archive) > pop_size:\n                archive = archive[:pop_size]\n\n            # update best, memories and covariance\n            prev_best = f_best\n            improved = False\n            if f_eval < f_best - 1e-15:\n                disp = x_eval - best\n                norm = np.linalg.norm(disp)\n                if norm > 1e-12:\n                    vec = disp / norm\n                    dir_memory.append(vec.copy())\n                    if len(dir_memory) > self.mem_size:\n                        dir_memory.pop(0)\n                    imp = max(1e-8, prev_best - f_eval)\n                    cov_components.insert(0, (vec.copy(), imp))\n                    # prune and decay cov_components\n                    if len(cov_components) > max(self.cov_rank * 10, self.mem_size):\n                        cov_components = cov_components[:self.mem_size]\n                    for i in range(len(cov_components)):\n                        v, w = cov_components[i]\n                        cov_components[i] = (v, w * self.cov_decay)\n                    total_w = sum(w for (_, w) in cov_components) + 1e-12\n                    cov_components = [(v, w / total_w) for (v, w) in cov_components]\n                    # update PCA buffer for ranked_pca and incremental PCA\n                    pca_buffer.append(x_eval.copy())\n                    if len(pca_buffer) > max(200, 8 * self.dim):\n                        pca_buffer.pop(0)\n                    # adjust diag_scale adaptively on success\n                    diag_scale *= (1.0 + self.per_dim_adapt * (np.abs(vec) * 0.5 + 0.5 * self.rng.rand(dim)))\n                    # clamp\n                    diag_scale = np.clip(diag_scale, 0.1, 6.0)\n                best = x_eval.copy()\n                f_best = float(f_eval)\n                improved = True\n                recent_success.append(1)\n            else:\n                recent_success.append(0)\n\n            # update move success stats\n            if improved:\n                reward = 1.0 + max(0.0, (prev_best - f_eval) / (abs(prev_best) + 1e-12))\n                move_success[mv] += reward\n            else:\n                move_success[mv] *= 0.998\n\n            # periodic clusters and trust-region update\n            if eval_count % max(8, dim) == 0:\n                rebuild_clusters()\n                for i, cl in enumerate(clusters):\n                    members = [a for a in archive if a.get('cluster', -1) == i]\n                    if len(members) > 0:\n                        best_member = min(members, key=lambda e: e['f'])\n                        if best_member['f'] < cl['best_f'] - 1e-12:\n                            cl['best_f'] = best_member['f']\n                            cl['center'] = best_member['x'].copy()\n                            cl['no_improve'] = 0\n                            cl['radius'] *= self.trust_expand\n                        else:\n                            cl['no_improve'] += 1\n                            if cl['no_improve'] > max(6, int(0.6 * self.dim)):\n                                cl['radius'] *= self.trust_shrink\n                                cl['no_improve'] = 0\n                        cl['radius'] = np.clip(cl['radius'], 1e-9 * range_mean, 2.5 * range_mean)\n\n            # adaptive sigma_global using recent success window (budget-aware)\n            if len(recent_success) >= max(25, 6 * dim):\n                sr = np.mean(recent_success)\n                if sr > 0.24:\n                    sigma_global *= 1.07\n                elif sr < 0.16:\n                    sigma_global *= 0.92\n                sigma_global = np.clip(sigma_global, 1e-9 * range_mean, 4.0 * range_mean)\n                recent_success = []\n\n            # budget-aware local exploitation when improvement observed: layered local batches + mini line-search on surrogate\n            if improved and remaining >= self.local_batch:\n                # create directional local probes using last direction and local PCA\n                probes = []\n                if len(dir_memory) >= 1:\n                    dvec = dir_memory[-1]\n                else:\n                    dvec = self.rng.normal(0, 1.0, size=dim)\n                    dvec /= (np.linalg.norm(dvec) + 1e-12)\n                # use incremental PCA to propose stronger directions\n                pcs, svals = compute_incremental_pca(min(self.cov_rank, max(1, self.dim // 6)))\n                for k in range(self.local_batch):\n                    alpha = (k + 1) * 0.18\n                    base = best + dvec * alpha * sigma * 0.6\n                    if pcs is not None and self.rng.rand() < 0.6:\n                        # perturb along top PC\n                        pc = pcs[self.rng.randint(0, pcs.shape[0])]\n                        mag = (0.4 + 0.8 * self.rng.rand()) * sigma * (0.8 if k == 0 else 0.5)\n                        cand = base + pc * mag + self.rng.normal(0, 0.22 * sigma, size=dim) * diag_scale\n                    else:\n                        cand = base + self.rng.normal(0, 0.22 * sigma, size=dim) * diag_scale\n                    cand = self._reflect(cand, lb, ub)\n                    probes.append(cand)\n                # evaluate probes but ensure budget not exceeded\n                best_child_f = None\n                best_child_x = None\n                for p in probes:\n                    if eval_count >= self.budget:\n                        break\n                    res = eval_point(p)\n                    if res is None:\n                        break\n                    px, pf = res\n                    # add to archive softly\n                    if len(archive) < pop_size:\n                        archive.append({'x': px.copy(), 'f': pf, 'age': 0, 'cluster': -1})\n                    else:\n                        worst_idx = int(np.argmax([e['f'] for e in archive]))\n                        if pf < archive[worst_idx]['f']:\n                            archive[worst_idx] = {'x': px.copy(), 'f': pf, 'age': 0, 'cluster': -1}\n                    if best_child_f is None or pf < best_child_f:\n                        best_child_f = pf\n                        best_child_x = px.copy()\n                if best_child_f is not None and best_child_f < f_best - 1e-15:\n                    prev_best = f_best\n                    best = best_child_x.copy()\n                    f_best = float(best_child_f)\n                    # strengthen diag_scale slightly along direction\n                    if len(dir_memory) > 0:\n                        diag_scale *= (1.0 + 0.02)\n                        diag_scale = np.clip(diag_scale, 0.08, 8.0)\n                    # reward the trust-like moves moderately\n                    if 'trust' in move_names:\n                        move_success[move_names.index('trust')] += 0.85\n\n            # periodic decay/normalization for stability\n            if eval_count % max(60, 8 * dim) == 0:\n                move_success = np.maximum(move_success * 0.985, 1e-6)\n                move_tries = np.maximum(move_tries * 0.995, 1e-6)\n                # slight decay of diag_scale to avoid runaway\n                diag_scale = np.clip(diag_scale * 0.995 + 0.001, 0.05, 10.0)\n\n            if eval_count >= self.budget:\n                break\n\n        archive.sort(key=lambda e: e['f'])\n        final_best_x = best.copy()\n        final_best_f = float(f_best)\n        return final_best_f, np.asarray(final_best_x)", "configspace": "", "generation": 13, "feedback": "The algorithm BRIDAS_XT scored 0.704 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["18d0bc7d-f63a-41e3-a7ec-e6ad7d107e59"], "operator": null, "metadata": {"ast_features": {"Nodes": 6548.0, "Edges": 6547.0, "Max Degree": 52.0, "Min Degree": 1.0, "Mean Degree": 1.9996945632254124, "Degree Variance": 2.514355435113997, "Transitivity": 0.0, "Max Depth": 25.0, "Min Depth": 2.0, "Mean Depth": 11.137178233952428, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3312138822477013, "Depth Entropy": 2.810781453976893, "Assortativity": 0.0, "Average Eccentricity": 27.075442883323152, "Diameter": 35.0, "Radius": 18.0, "Edge Density": 0.00015271838729383018, "Average Shortest Path": 14.699429686513728, "mean_complexity": 13.6, "total_complexity": 136.0, "mean_token_count": 604.3, "total_token_count": 6043.0, "mean_parameter_count": 4.3, "total_parameter_count": 43.0}, "feature_guidance_action": "increase", "feature_guidance_feature_name": "mean_parameter_count", "aucs": [0.2575760646031826, 0.16925949867558432, 0.8953318615259862, 0.9467646295184715, 0.8987557704698442, 0.9110682514849385, 0.29234208822956065, 0.8779387417067319, 0.8974989475417997, 0.8910260748502739]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-MA_BBOB-3-1", "seed": 3}
{"method_name": "ES-guided", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 4, "n_offspring": 16, "elitism": true, "feature_guided_mutation": true, "parent_selection": "tournament", "tournament_size": 2}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "009a9d03-b9e3-4bbd-971f-7c685cdaf148", "fitness": 0.8564919137251016, "name": "EntropyBoostedSubspaceAdaptiveSearch", "description": "Entropy-Boosted Subspace Adaptive Search (EBSAS) — adaptively balances focused subspace exploitation and broad orthogonal exploration by monitoring eigenvalue entropy of recent successful steps, dynamically tuning sampling modes, multi-scale mirrored sampling, and lightweight quasi-Newton probes to robustly solve rotated/affine continuous problems.", "code": "import numpy as np\n\nclass EntropyBoostedSubspaceAdaptiveSearch:\n    \"\"\"\n    Entropy-Boosted Subspace Adaptive Search (EBSAS)\n\n    Main ideas and novelties:\n    - Monitor the eigenvalue (variance) spectrum entropy from a recent-step archive to measure\n      directional diversity (\"degree entropy\"). Use this to adaptively tune sampling strategy:\n        * Low entropy -> concentrate sampling in a smaller subspace (exploit)\n        * High entropy -> broaden sampling (explore)\n    - Multi-mode mirrored sampling: subspace-focused, orthogonal-only, global isotropic,\n      coordinate/top-eigen perturbations, and amplified long jumps. Mode probabilities adapt\n      based on measured entropy and stagnation to keep a high effective behavioral entropy.\n    - Rank-mu covariance adaptation with importance-sampling correction and CMA-like paths.\n    - Lightweight quasi-Newton / linear-probe steps in the learned subspace to accelerate local improvement.\n    - Budget-aware population sizing and occasional small restarts when stagnation is detected.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_sigma=None,\n                 lambda_base=None,          # base population size\n                 p_random=0.02,             # occasional global random probe\n                 cov_eps=1e-8,\n                 memory_size=120,\n                 target_entropy=0.7,        # desired normalized entropy (0..1)\n                 entropy_smooth=0.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.init_sigma = init_sigma\n\n        if lambda_base is None:\n            # small to moderate base population; will be scaled with dimension\n            lambda_base = int(max(6, np.floor(4 + 3.0 * np.log(max(2, dim)))))\n        self.lambda_base = max(4, int(lambda_base))\n\n        self.p_random = float(p_random)\n        self.cov_eps = float(cov_eps)\n        self.memory_size = int(max(40, memory_size))\n\n        self.target_entropy = float(np.clip(target_entropy, 0.01, 0.99))\n        self.entropy_smooth = float(np.clip(entropy_smooth, 0.0, 1.0))\n\n        # internal best\n        self.f_opt = None\n        self.x_opt = None\n\n    def __call__(self, func):\n        # expect func to have bounds .lb and .ub like in the framework\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        assert lb.shape[0] == self.dim\n        assert ub.shape[0] == self.dim\n        dim = self.dim\n\n        # initialize center randomly and evaluate\n        x_mean = self.rng.uniform(lb, ub)\n        f_mean = float(func(x_mean))\n        evals = 1\n\n        # best overall\n        x_best = x_mean.copy()\n        f_best = f_mean\n\n        # initialize sigma\n        span = np.max(ub - lb)\n        sigma = self.init_sigma if self.init_sigma is not None else 0.25 * np.mean(ub - lb)\n        sigma = float(max(sigma, 1e-12))\n\n        # initialize covariance\n        C = np.eye(dim)\n        def decompose_cov(Cmat):\n            # robust eigen-decomposition ensuring positive definiteness\n            vals, vecs = np.linalg.eigh((Cmat + Cmat.T) * 0.5)\n            vals = np.where(vals > 1e-16, vals, 1e-16)\n            sqrtvals = np.sqrt(vals)\n            inv_sqrtvals = 1.0 / sqrtvals\n            B = (vecs * sqrtvals).dot(vecs.T)          # sqrt(C)\n            invB = (vecs * inv_sqrtvals).dot(vecs.T)  # inv_sqrt(C)\n            return B, invB, vals, vecs\n\n        B, invB, eigvals, eigvecs = decompose_cov(C)\n\n        # population weighting (rank-mu like)\n        lam = max(4, self.lambda_base)\n        # ensure even for mirrored sampling\n        if lam % 2 == 1:\n            lam += 1\n        mu = max(1, lam // 2)\n        ranks = np.arange(1, mu + 1)\n        weights = np.log(mu + 0.5) - np.log(ranks)\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # CMA-like adaptation constants\n        c_sigma = (mu_eff + 2.0) / (dim + mu_eff + 5.0)\n        d_sigma = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (dim + 1.0)) - 1.0) + c_sigma\n        c_c = (4.0 + mu_eff / dim) / (dim + 4.0 + 2.0 * mu_eff / dim)\n        c1 = 2.0 / ((dim + 1.3) ** 2 + mu_eff)\n        cmu = min(1.0 - c1, 2.0 * (mu_eff - 2.0 + 1.0 / mu_eff) / ((dim + 2.0) ** 2 + mu_eff))\n        c1 = max(c1, 1e-6)\n        cmu = max(cmu, 1e-6)\n\n        # evolution paths\n        p_sigma = np.zeros(dim)\n        p_c = np.zeros(dim)\n\n        # archive for PCA, step statistics, and probes\n        archive_X = [x_mean.copy()]\n        archive_f = [f_mean]\n        step_archive = []  # store recent successful step vectors (x_new - x_old)\n\n        generation = 0\n        stagnation = 0\n        stagnation_limit = max(30, 6 * dim)\n\n        # expected length of N(0,I)\n        expected_norm = np.sqrt(dim) * (1.0 - 1.0 / (4.0 * dim) + 1.0 / (21.0 * dim ** 2))\n\n        # initialize entropy estimate\n        entropy_est = 1.0  # normalized (0..1)\n\n        # helper: compute normalized eigenvalue entropy from an empirical covariance\n        def normalized_entropy(eigvals_arr):\n            vals = np.array(eigvals_arr, dtype=float)\n            vals = np.where(vals > 0, vals, 1e-16)\n            p = vals / np.sum(vals)\n            H = -np.sum(p * np.log(p + 1e-16)) / np.log(len(p) + 1e-16)\n            return float(np.clip(H, 0.0, 1.0))\n\n        # main optimization loop\n        while evals < self.budget:\n            generation += 1\n\n            # occasional purely random global probe (forced exploration)\n            if self.rng.rand() < self.p_random and evals < self.budget:\n                x_probe = self.rng.uniform(lb, ub)\n                f_probe = float(func(x_probe))\n                evals += 1\n                archive_X.append(x_probe.copy()); archive_f.append(f_probe)\n                if len(archive_X) > self.memory_size:\n                    archive_X.pop(0); archive_f.pop(0)\n                if f_probe < f_best:\n                    x_best, f_best = x_probe.copy(), f_probe\n                # maybe re-center to a good probe if it improves mean\n                if f_probe < f_mean:\n                    x_mean, f_mean = x_probe.copy(), f_probe\n                # continue but still allow regular generation below\n\n            # Build adaptive subspace from successful steps in step_archive and archive\n            use_for_pca = min(len(archive_X), self.memory_size)\n            if use_for_pca >= 2:\n                Xmat = np.array(archive_X[-use_for_pca:])\n                centered = (Xmat - np.mean(Xmat, axis=0))\n                # compute empirical covariance (robust)\n                cov_est = np.cov(centered, rowvar=False) + 1e-12 * np.eye(dim)\n                vvals, vvecs = np.linalg.eigh(cov_est)\n                idx = np.argsort(vvals)[::-1]\n                vvals = vvals[idx]; vvecs = vvecs[:, idx]\n                # normalized eigenvalue entropy\n                H = normalized_entropy(vvals)\n                # smooth entropy estimate\n                entropy_est = (1.0 - self.entropy_smooth) * entropy_est + self.entropy_smooth * H\n                # choose subspace dimension k based on entropy: lower entropy -> smaller k\n                # map entropy in [0,1] to k in [1, dim]\n                k = int(np.clip(np.round((0.5 + 0.8 * entropy_est) * dim), 1, dim))\n                # we bias to capture at least top variance directions but allow adaptive k\n                sub_basis = vvecs[:, :k]\n            else:\n                # not enough data: default small subspace\n                entropy_est = 1.0\n                k = max(1, int(0.25 * dim))\n                sub_basis = np.eye(dim)[:, :k]\n\n            # adapt sampling mode probabilities to encourage maintenance of entropy\n            # Modes: 'sub' (focused), 'orth' (complement), 'global', 'coord' (top-eigen axes), 'jump' (amplified)\n            # If entropy_est < target, encourage orth/global to increase entropy; otherwise encourage subspace exploitation.\n            p_target = self.target_entropy\n            # base probabilities\n            p_sub = 0.35\n            p_orth = 0.20\n            p_global = 0.15\n            p_coord = 0.15\n            p_jump = 0.15\n            # adjust based on difference\n            diff = entropy_est - p_target\n            # if entropy below target, increase orth/global/jump to inject diversity\n            if diff < 0:\n                boost = min(0.5, -diff)\n                p_orth += 0.4 * boost\n                p_global += 0.3 * boost\n                p_jump += 0.3 * boost\n                p_sub = max(0.05, p_sub * (1 - 0.8 * boost))\n            else:\n                # entropy high -> concentrate more in subspace\n                boost = min(0.6, diff)\n                p_sub = min(0.9, p_sub + 0.6 * boost)\n                p_coord = max(0.05, p_coord * (1 - 0.5 * boost))\n            # normalize probabilities\n            probs = np.array([p_sub, p_orth, p_global, p_coord, p_jump], dtype=float)\n            probs = np.maximum(probs, 1e-6)\n            probs = probs / np.sum(probs)\n\n            # population size scaling with remaining budget & dimension (budget-aware)\n            remaining = max(1, self.budget - evals)\n            lam = self.lambda_base\n            # scale up lambda moderately if many evaluations remain and dim smallish\n            if remaining > 10 * lam and dim <= 50:\n                lam = int(min(4 * self.lambda_base, lam + remaining // 200))\n            if lam % 2 == 1:\n                lam += 1\n            half = lam // 2\n\n            # generate half independent z then mirror; but we might need fewer candidates due to budget\n            z_list = [self.rng.normal(size=dim) for _ in range(half)]\n            cand_list = []\n            z_store = []\n            modes_store = []\n\n            for z in z_list:\n                mode = self.rng.choice(5, p=probs)\n                # transform base z via B (sqrt(C)) later; we adjust z components\n                if mode == 0:  # subspace-focused\n                    proj = sub_basis.dot(sub_basis.T.dot(z))\n                    off = z - proj\n                    # amplify subspace, shrink orthogonal\n                    z_new = proj * (1.8 + 0.6 * self.rng.rand()) + off * (0.3 + 0.4 * self.rng.rand())\n                elif mode == 1:  # orthogonal exploration\n                    proj = sub_basis.dot(sub_basis.T.dot(z))\n                    off = z - proj\n                    # emphasize orthogonal component\n                    if np.linalg.norm(off) < 1e-12:\n                        # fallback: perturb random orth direction\n                        off = z.copy()\n                    z_new = off * (1.6 + 0.8 * self.rng.rand()) + proj * (0.2 + 0.3 * self.rng.rand())\n                elif mode == 2:  # global isotropic\n                    z_new = z * (0.8 + 0.8 * self.rng.rand())\n                elif mode == 3:  # coordinate / eigen-axis perturbation (sparse)\n                    # perturb only along one or a few top eigenvectors of current C\n                    ntop = max(1, min(k, int(1 + self.rng.randint(0, min(4, k)))))\n                    axes = eigvecs[:, :k]\n                    coeffs = np.zeros(k)\n                    idxs = self.rng.choice(k, size=ntop, replace=False)\n                    coeffs[idxs] = self.rng.normal(scale=2.0, size=ntop)\n                    z_new = axes.dot(coeffs)\n                    # add small noise in orth directions\n                    z_new = z_new + 0.1 * z\n                else:  # jump (long-range)\n                    # amplify whole vector, occasionally flip direction to push far\n                    z_new = z * (2.5 + 2.0 * self.rng.rand())\n                    if self.rng.rand() < 0.1:\n                        z_new = -z_new\n\n                # whiten (approximately) using invB to keep sampling scale consistent\n                # We'll apply B later when generating actual candidate\n                y = B.dot(z_new)\n                cand_list.append(x_mean + sigma * y)\n                z_store.append(z_new)\n                modes_store.append(mode)\n                # mirrored\n                cand_list.append(x_mean - sigma * y)\n                z_store.append(-z_new)\n                modes_store.append(mode)\n\n                if len(cand_list) >= remaining:\n                    break\n\n            # trim to budget\n            to_eval = min(len(cand_list), remaining)\n            cand_list = cand_list[:to_eval]\n            z_store = z_store[:to_eval]\n            modes_store = modes_store[:to_eval]\n\n            # Evaluate candidates in an order that prioritizes diversity: interleave modes\n            # Create indices grouped per mode and sample one from each group in round-robin\n            mode_indices = {}\n            for i, m in enumerate(modes_store):\n                mode_indices.setdefault(m, []).append(i)\n            interleaved = []\n            pointers = {m: 0 for m in mode_indices}\n            while len(interleaved) < len(cand_list):\n                for m in list(mode_indices.keys()):\n                    p = pointers[m]\n                    if p < len(mode_indices[m]):\n                        interleaved.append(mode_indices[m][p])\n                        pointers[m] = p + 1\n                    if len(interleaved) >= len(cand_list):\n                        break\n            # Evaluate in interleaved order\n            cand_X = []\n            cand_f = []\n            for idx in interleaved:\n                x_cand = cand_list[idx]\n                x_c = np.minimum(np.maximum(x_cand, lb), ub)\n                f_c = float(func(x_c))\n                evals += 1\n                cand_X.append(x_c.copy()); cand_f.append(f_c)\n                archive_X.append(x_c.copy()); archive_f.append(f_c)\n                if len(archive_X) > self.memory_size:\n                    archive_X.pop(0); archive_f.pop(0)\n                # store step vectors for successful improvements relative to mean\n                step = x_c - x_mean\n                step_archive.append(step.copy())\n                if len(step_archive) > self.memory_size:\n                    step_archive.pop(0)\n                if f_c < f_best:\n                    # accept best immediately\n                    x_best, f_best = x_c.copy(), f_c\n                if evals >= self.budget:\n                    break\n\n            # selection: rank by fitness among candidates\n            if len(cand_f) == 0:\n                break\n            idx_sorted = np.argsort(cand_f)\n            top_count = min(mu, len(idx_sorted))\n            top_idx = idx_sorted[:top_count]\n\n            # importance-sampling weights: prefer diversity-weighted contributions\n            ys = []\n            weighted_shift = np.zeros(dim)\n            is_weights = []\n            for rank_j, jj in enumerate(top_idx):\n                # base recombination weight (shape)\n                w = weights[rank_j] if rank_j < len(weights) else 0.0\n                # diversity bonus: penalize candidates too close to mean to encourage exploration\n                dist = np.linalg.norm((cand_X[jj] - x_mean) / (sigma + 1e-12))\n                diversity_bonus = 1.0 + 0.3 * np.tanh(dist / (1.0 + 0.5 * dim ** 0.5))\n                w_eff = w * diversity_bonus\n                y = (cand_X[jj] - x_mean) / (sigma + 1e-16)\n                ys.append((w_eff, y))\n                weighted_shift += w_eff * (cand_X[jj] - x_mean)\n                is_weights.append(w_eff)\n            wsum = sum(is_weights) if len(is_weights) > 0 else 0.0\n            if wsum > 0:\n                delta_mean = weighted_shift / (wsum + 1e-16)\n            else:\n                delta_mean = np.zeros(dim)\n\n            # update mean and approximate mean fitness conservatively\n            x_mean = x_mean + delta_mean\n            f_mean = min([cand_f[jj] for jj in top_idx]) if len(top_idx) > 0 else f_mean\n\n            # evolution path and sigma adaptation\n            z_for_ps = invB.dot(delta_mean / (sigma + 1e-16))\n            p_sigma = (1 - c_sigma) * p_sigma + np.sqrt(c_sigma * (2 - c_sigma) * mu_eff) * z_for_ps\n            norm_p_sigma = np.linalg.norm(p_sigma)\n            sigma *= np.exp((c_sigma / d_sigma) * (norm_p_sigma / (expected_norm + 1e-16) - 1.0))\n\n            # conjugate path for covariance\n            denom = np.sqrt(1.0 - (1.0 - c_sigma) ** (2 * generation))\n            if denom <= 0:\n                denom = 1e-8\n            h_sig = 1.0 if (norm_p_sigma / denom) < (1.4 + 2.0 / (dim + 1.0)) * expected_norm else 0.0\n            p_c = (1 - c_c) * p_c + h_sig * np.sqrt(c_c * (2 - c_c) * mu_eff) * (delta_mean / (sigma + 1e-16))\n\n            # compute rank-mu update with importance-sampling normalization\n            rank_mu = np.zeros((dim, dim))\n            # normalize importance weights so that sum(weights) == 1 for covariance update\n            if len(ys) > 0:\n                wsum_norm = sum([abs(w) for w, _ in ys]) + 1e-16\n                for w, y in ys:\n                    w_norm = w / wsum_norm\n                    rank_mu += w_norm * np.outer(y, y)\n\n            # covariance update with small damping when h_sig==0 (to handle bad signals)\n            damp = 0.5 if h_sig < 0.5 else 1.0\n            C = (1 - c1 - cmu) * C + c1 * np.outer(p_c, p_c) * damp + cmu * rank_mu\n            # regularize\n            C += self.cov_eps * np.eye(dim)\n\n            # re-decompose every generation (cheap for modest dims)\n            B, invB, eigvals, eigvecs = decompose_cov(C)\n\n            # update eigenvalue-based entropy estimate and influence sampling next loop done above\n\n            # local quasi-Newton / gradient probe every few gens if budget allows\n            if generation % max(3, int(5 + dim / 10)) == 0 and evals < self.budget and len(archive_X) >= min(8, dim):\n                # build linear model in current subspace using recent archive\n                recent_k = min(len(archive_X), self.memory_size)\n                Xr = np.array(archive_X[-recent_k:])\n                fr = np.array(archive_f[-recent_k:])\n                Xc = Xr - np.mean(Xr, axis=0)\n                fr_c = fr - np.mean(fr)\n                # project to subspace\n                Z = Xc.dot(sub_basis)  # Nxk\n                if Z.shape[1] >= 1 and Z.shape[0] >= Z.shape[1]:\n                    # ridge least squares to get gradient approx in subspace\n                    reg = 1e-6 * max(1.0, np.linalg.norm(fr_c))\n                    try:\n                        A = Z.T.dot(Z) + reg * np.eye(Z.shape[1])\n                        b = Z.T.dot(fr_c)\n                        sol = np.linalg.solve(A, b)\n                        grad_sub = sub_basis.dot(sol)  # approximate gradient in original space\n                        gnorm = np.linalg.norm(grad_sub)\n                        if gnorm > 1e-12:\n                            step_dir = -grad_sub / gnorm\n                            step_size = 0.9 * sigma * np.sqrt(dim) * (1.0 + 0.5 * self.rng.rand())\n                            probe = x_mean + step_dir * step_size\n                            probe = np.minimum(np.maximum(probe, lb), ub)\n                            # evaluate probe\n                            f_probe = float(func(probe))\n                            evals += 1\n                            archive_X.append(probe.copy()); archive_f.append(f_probe)\n                            if len(archive_X) > self.memory_size:\n                                archive_X.pop(0); archive_f.pop(0)\n                            # incorporate successful probe\n                            if f_probe < f_best:\n                                step_vec = probe - x_mean\n                                s2 = np.dot(step_vec, step_vec)\n                                if s2 > 1e-16:\n                                    rank_one = np.outer(step_vec, step_vec) / (s2 + 1e-16)\n                                    C = (1 - 0.5 * cmu) * C + (0.5 * cmu) * rank_one\n                                    B, invB, eigvals, eigvecs = decompose_cov(C)\n                                x_best, f_best = probe.copy(), f_probe\n                            # small chance to re-center mean if probe is good\n                            if f_probe < f_mean:\n                                x_mean, f_mean = probe.copy(), f_probe\n                    except Exception:\n                        pass\n\n            # stagnation handling\n            if f_best < f_mean:\n                stagnation = 0\n            else:\n                stagnation += 1\n            if stagnation > stagnation_limit:\n                stagnation = 0\n                # diversify by re-centering around a good archived point or making a randomized restart\n                if len(archive_X) > 0 and self.rng.rand() < 0.8:\n                    # pick among top archived points\n                    top_count = min(len(archive_f), 10)\n                    idxs = np.argsort(archive_f)[:top_count]\n                    pick = int(self.rng.choice(idxs))\n                    x_mean = archive_X[pick].copy()\n                    f_mean = archive_f[pick]\n                else:\n                    x_mean = self.rng.uniform(lb, ub)\n                    f_mean = float(func(x_mean))\n                    evals += 1\n                    archive_X.append(x_mean.copy()); archive_f.append(f_mean)\n                    if f_mean < f_best:\n                        x_best, f_best = x_mean.copy(), f_mean\n                # inflate sigma to encourage exploration\n                sigma = min(0.8 * span, sigma * (1.5 + 0.5 * self.rng.rand()))\n                # slightly isotropize covariance to avoid becoming too narrow\n                C = 0.5 * C + 0.5 * np.eye(dim)\n                B, invB, eigvals, eigvecs = decompose_cov(C)\n\n            # track global best\n            self.f_opt = float(f_best)\n            self.x_opt = x_best.copy()\n\n        return float(self.f_opt), self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm EntropyBoostedSubspaceAdaptiveSearch scored 0.856 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["64fd218a-75e4-48f0-a80b-bf2f7e0990dc"], "operator": null, "metadata": {"ast_features": {"Nodes": 4016.0, "Edges": 4015.0, "Max Degree": 70.0, "Min Degree": 1.0, "Mean Degree": 1.9995019920318724, "Degree Variance": 3.1867527400358733, "Transitivity": 0.0, "Max Depth": 18.0, "Min Depth": 2.0, "Mean Depth": 8.490514905149052, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3061515684391187, "Depth Entropy": 2.229614048929992, "Assortativity": 0.0, "Average Eccentricity": 20.2554780876494, "Diameter": 29.0, "Radius": 15.0, "Edge Density": 0.000249003984063745, "Average Shortest Path": 10.410974284679464, "mean_complexity": 16.75, "total_complexity": 67.0, "mean_token_count": 882.75, "total_token_count": 3531.0, "mean_parameter_count": 3.75, "total_parameter_count": 15.0}, "feature_guidance_action": "increase", "feature_guidance_feature_name": "Degree Entropy", "aucs": [0.7738158324746235, 0.8549110460674754, 0.9326375987676142, 0.9706584851803858, 0.9478452964488366, 0.9540847681400152, 0.3070231111736572, 0.9340653845933483, 0.9475115895974793, 0.9423660248075796]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-MA_BBOB-5", "seed": 5}
{"method_name": "ES-guided", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES-guided", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 4, "n_offspring": 16, "elitism": true, "feature_guided_mutation": true, "parent_selection": "tournament", "tournament_size": 2}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "d9bc213f-1f8e-49dd-b0f5-bbc0ad8de445", "fitness": 0.6273890018509147, "name": "DenseGraphFusionExplorer", "description": "Graph-Fusion with Multi-Scale Dense Edge Augmentation — a direction-graph optimizer that actively densifies its edge fabric using co-occurrence cliques, spectral smoothing and \"supernodes\" to promote high-degree mixed directions for exploring elongated affine landscapes.", "code": "import numpy as np\n\nclass DenseGraphFusionExplorer:\n    \"\"\"\n    Dense Graph-Fusion Directional Explorer (DGFE)\n    - budget: maximum number of function evaluations\n    - dim: problem dimensionality\n\n    API:\n        solver = DenseGraphFusionExplorer(budget=10000, dim=10)\n        fbest, xbest = solver(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 seed=None,\n                 init_frac=0.06,\n                 nodes_mult=4.0,\n                 max_nodes=160,\n                 base_pop=14,\n                 walk_heavy_tail_p=0.22,\n                 edge_init_scale=0.28,\n                 edge_reinforce=1.05,\n                 sigma0_frac=0.14,\n                 elite_size=None,\n                 stagnation_restart_frac=0.015,\n                 supernode_frac=0.12,\n                 cooc_decay=0.92,\n                 spectral_blend=0.18):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.init_frac = float(init_frac)\n        self.nodes_mult = float(nodes_mult)\n        self.max_nodes = int(max_nodes)\n        self.base_pop = int(base_pop)\n        self.walk_heavy_tail_p = float(walk_heavy_tail_p)\n        self.edge_init_scale = float(edge_init_scale)\n        self.edge_reinforce = float(edge_reinforce)\n        self.sigma0_frac = float(sigma0_frac)\n        self.elite_size = int(elite_size) if elite_size is not None else max(6, min(48, 6 + 3 * self.dim))\n        self.stagnation_restart = max(12, int(stagnation_restart_frac * max(1, self.budget)))\n        self.supernode_frac = float(supernode_frac)\n        self.cooc_decay = float(cooc_decay)\n        self.spectral_blend = float(spectral_blend)\n\n    def __call__(self, func):\n        # Bounds handling (fallback to [-5,5])\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n                ub = np.full(self.dim, float(ub))\n            else:\n                if lb.size != self.dim:\n                    lb = np.broadcast_to(lb, (self.dim,))\n                if ub.size != self.dim:\n                    ub = np.broadcast_to(ub, (self.dim,))\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n\n        range_vec = ub - lb\n        range_mean = float(np.mean(range_vec)) + 1e-12\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # initial sampling: spread more to seed elite with variety\n        n_init = int(max(14, min(self.budget // 7, max(12 * self.dim, int(self.init_frac * self.budget)))))\n        elite = []\n        for i in range(min(n_init, self.budget)):\n            x = np.random.uniform(lb, ub)\n            f = func(x); evals += 1\n            if f < f_best:\n                f_best = f; x_best = x.copy()\n            elite.append((f, x.copy()))\n        elite.sort(key=lambda t: t[0])\n        elite = elite[:self.elite_size]\n\n        if evals >= self.budget:\n            return f_best, x_best\n\n        # initial center and sigma\n        center = x_best.copy() if x_best is not None else np.random.uniform(lb, ub)\n        sigma = max(1e-12, self.sigma0_frac * range_mean)\n\n        # Graph of direction nodes\n        n_nodes = int(min(self.max_nodes, max(8, int(np.ceil(self.nodes_mult * self.dim)))))\n        # initialize nodes as random orthonormal-ish directions distributed across space\n        nodes = np.zeros((self.dim, n_nodes))\n        remaining = n_nodes\n        idx = 0\n        block = min(self.dim, max(2, int(np.ceil(n_nodes / max(1, int(np.ceil(n_nodes / self.dim)))))))\n        while remaining > 0:\n            bsz = min(remaining, block)\n            A = np.random.randn(self.dim, bsz)\n            try:\n                Q, _ = np.linalg.qr(A)\n                Q = Q[:, :bsz]\n            except Exception:\n                Q = A / (np.linalg.norm(A, axis=0, keepdims=True) + 1e-12)\n            nodes[:, idx:idx + bsz] = Q\n            idx += bsz\n            remaining -= bsz\n        norms = np.linalg.norm(nodes, axis=0, keepdims=True) + 1e-12\n        nodes = nodes / norms\n\n        # adjacency (symmetric) - start denser intentionally\n        adj = np.abs(np.random.randn(n_nodes, n_nodes)) * self.edge_init_scale\n        adj = (adj + adj.T) / 2.0\n        np.fill_diagonal(adj, 0.0)\n\n        # co-occurrence memory matrix (encourages clique formation)\n        cooc = np.zeros_like(adj)\n\n        # supernodes: virtual nodes that connect to many physical nodes to encourage high-degree mixes\n        n_super = max(1, int(np.ceil(self.supernode_frac * n_nodes)))\n        super_adj = np.abs(np.random.randn(n_super, n_nodes)) * (self.edge_init_scale * 0.9)\n        # normalize super_adj rows\n        super_adj = super_adj / (super_adj.sum(axis=1, keepdims=True) + 1e-12)\n\n        # normalize rows to make transition probabilities\n        def row_normed(matrix):\n            rsum = matrix.sum(axis=1, keepdims=True) + 1e-12\n            return matrix / rsum\n\n        trans = row_normed(adj)\n        node_usage = np.ones(n_nodes)\n        edge_usage = np.zeros_like(adj)\n\n        recent_steps = []  # store recent successful steps (vector, weight)\n        recent_limit = max(40, 12 * self.dim)\n\n        # helper: pick start node combining rarity and alignment\n        def pick_start_node(center_vec=None):\n            invu = (1.0 / (node_usage + 1e-12)) ** 1.35\n            probs = invu / (invu.sum() + 1e-12)\n            # sometimes use supernode as start (teleport to many)\n            if np.random.rand() < 0.06 and n_super > 0:\n                sidx = np.random.choice(n_super)\n                # sample a physical node from super adjacency\n                p = super_adj[sidx]\n                p = p / (p.sum() + 1e-12)\n                return np.random.choice(n_nodes, p=p)\n            if center_vec is not None and np.random.rand() < 0.44:\n                centroid = np.mean(np.array([x for (_, x) in elite]), axis=0) if elite else center_vec\n                align = np.abs(nodes.T @ (centroid - center_vec))\n                align = align / (align.sum() + 1e-12)\n                mix = 0.55 * probs + 0.45 * align\n                mix = mix / (mix.sum() + 1e-12)\n                return np.random.choice(n_nodes, p=mix)\n            return np.random.choice(n_nodes, p=probs)\n\n        # random walk that supports multi-scale mixing using supernodes & powered transitions\n        def random_walk_direction(start_idx, L):\n            idxs = [start_idx]\n            cur = start_idx\n            for _ in range(L - 1):\n                if np.random.rand() < 0.10:\n                    # teleport or supernode-driven jump - promotes mixing\n                    if np.random.rand() < 0.35 and n_super > 0:\n                        # pick supernode, then sample a node proportionally\n                        sidx = np.random.randint(0, n_super)\n                        probs = super_adj[sidx]\n                        nxt = np.random.choice(n_nodes, p=probs)\n                    else:\n                        nxt = np.random.randint(0, n_nodes)\n                else:\n                    probs = trans[cur]\n                    if np.all(probs == 0):\n                        nxt = np.random.randint(0, n_nodes)\n                    else:\n                        # allow higher-order mixing: sample from trans^k by blending neighbors\n                        if np.random.rand() < 0.22:\n                            # sample neighbor then neighbor-of-neighbor (2-step)\n                            neigh = np.random.choice(n_nodes, p=probs)\n                            probs2 = trans[neigh]\n                            if np.all(probs2 == 0):\n                                nxt = neigh\n                            else:\n                                nxt = np.random.choice(n_nodes, p=probs2)\n                        else:\n                            nxt = np.random.choice(n_nodes, p=probs)\n                idxs.append(nxt)\n                cur = nxt\n            # heavy-tailed coefficients for high-degree combination\n            coefs = np.random.standard_t(2.2, size=len(idxs))\n            if np.random.rand() < 0.48:\n                coefs = np.sign(coefs) * (np.abs(coefs) ** (1.0 + np.random.rand() * 2.2))\n            coefs = coefs / (np.linalg.norm(coefs) + 1e-12)\n            vec = nodes[:, idxs] @ coefs\n            vec = vec / (np.linalg.norm(vec) + 1e-12)\n            return vec, idxs, coefs\n\n        # reinforce edges and co-occurrence memory\n        def reinforce_path(idxs, coefs, weight):\n            nonlocal adj, cooc, edge_usage, super_adj\n            for i in range(len(idxs) - 1):\n                a = idxs[i]; b = idxs[i + 1]\n                delta = self.edge_reinforce * weight * (abs(coefs[i]) + abs(coefs[i + 1])) * 0.6\n                adj[a, b] += delta\n                adj[b, a] += delta\n                edge_usage[a, b] += delta\n                edge_usage[b, a] += delta\n            # co-occurrence boost: connect all nodes in path into clique with smaller weight\n            if len(idxs) > 1:\n                for i in range(len(idxs)):\n                    for j in range(i + 1, len(idxs)):\n                        a = idxs[i]; b = idxs[j]\n                        delta2 = 0.24 * self.edge_reinforce * weight * (abs(coefs[i]) + abs(coefs[j])) * 0.15\n                        adj[a, b] += delta2\n                        adj[b, a] += delta2\n                        cooc[a, b] += delta2\n                        cooc[b, a] += delta2\n            # slightly reinforce connections to supernodes proportional to participation\n            if n_super > 0:\n                participation = np.bincount(idxs, minlength=n_nodes).astype(float)\n                top_nodes = np.argsort(participation)[-min(6, self.dim):]\n                for si in range(n_super):\n                    # small adaptive link: more to supernodes that align with this path centroid\n                    aset = np.abs(super_adj[si] @ participation)\n                    add = 0.02 * weight * (1.0 + aset)\n                    super_adj[si, top_nodes] += add\n                # renormalize super_adj rows for stability\n                super_adj[:] = super_adj / (super_adj.sum(axis=1, keepdims=True) + 1e-12)\n\n        # cooccurrence-driven edge augmentation (periodic)\n        def augment_edges_from_cooc():\n            nonlocal adj, cooc, trans\n            if cooc.sum() <= 0:\n                return\n            # take top co-occurring pairs and form cliques\n            flat = cooc.flatten()\n            k = min( (n_nodes * 3), max(40, int(0.015 * flat.size)) )\n            # pick indices of largest cooc entries\n            top_idxs = np.argpartition(-flat, k)[:k]\n            for idx_flat in top_idxs:\n                a = idx_flat // n_nodes\n                b = idx_flat % n_nodes\n                if a == b:\n                    continue\n                add = 0.35 * (cooc[a, b] ** 0.6 + 1e-6)\n                adj[a, b] += add\n                adj[b, a] += add\n            # spectral smoothing: blend with node similarity to spread density\n            sim = np.abs(nodes.T @ nodes)\n            np.fill_diagonal(sim, 0.0)\n            adj = (1.0 - self.spectral_blend) * adj + self.spectral_blend * (sim * (0.6 * self.edge_init_scale + 0.9))\n            # regularize cooc decay\n            cooc[:] = self.cooc_decay * cooc\n            trans = row_normed(adj)\n\n        # periodic node re-alignment using recent successful steps (PCA-like)\n        def realign_nodes_from_recent():\n            nonlocal nodes, adj, trans, super_adj\n            if len(recent_steps) < 6:\n                # jitter for diversity\n                jitter = 0.034 * np.random.randn(*nodes.shape)\n                nodes += jitter\n                norms = np.linalg.norm(nodes, axis=0, keepdims=True) + 1e-12\n                nodes[:] = nodes / norms\n                return\n            S = np.zeros((self.dim, self.dim))\n            totw = 0.0\n            for svec, w in recent_steps[-min(len(recent_steps), recent_limit):]:\n                S += w * np.outer(svec, svec)\n                totw += w\n            if totw <= 0:\n                return\n            S /= totw\n            try:\n                eigvals, eigvecs = np.linalg.eigh(S)\n            except Exception:\n                return\n            idxs = np.argsort(eigvals)[::-1]\n            eigvecs = eigvecs[:, idxs]\n            topk = min(self.dim, eigvecs.shape[1])\n            rep = np.zeros_like(nodes)\n            for i in range(nodes.shape[1]):\n                mixk = max(1, int(np.random.exponential(scale=1.6)))\n                mixk = min(mixk, topk)\n                picks = np.random.choice(topk, size=mixk, replace=False)\n                coefs = np.random.randn(mixk)\n                vec = eigvecs[:, picks] @ coefs\n                vec += 0.065 * np.random.randn(self.dim)\n                rep[:, i] = vec / (np.linalg.norm(vec) + 1e-12)\n            nodes[:] = rep\n            norms = np.linalg.norm(nodes, axis=0, keepdims=True) + 1e-12\n            nodes[:] = nodes / norms\n            # recompute a dense-ish adj from similarity but keep memory via blending\n            sim = np.abs(nodes.T @ nodes)\n            np.fill_diagonal(sim, 0.0)\n            adj[:] = 0.55 * adj + 0.45 * (sim * (self.edge_init_scale * 1.9))\n            # renormalize super_adj by projecting supernodes onto new node basis\n            if n_super > 0:\n                # small random reassignments to keep supernodes relevant\n                super_adj[:] = np.abs(super_adj + 0.02 * np.random.randn(*super_adj.shape))\n                super_adj[:] = super_adj / (super_adj.sum(axis=1, keepdims=True) + 1e-12)\n            trans[:] = row_normed(adj)\n\n        # helper to update elite\n        def update_elite(x, f):\n            nonlocal elite\n            elite.append((f, x.copy()))\n            elite.sort(key=lambda t: t[0])\n            elite = elite[:self.elite_size]\n\n        stagnation = 0\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            pop = min(remaining, max(8, int(self.base_pop + 2.6 * np.sqrt(self.dim))))\n            successes = 0\n            gen_best_improv = 0.0\n            gen_best_step = None\n\n            centroid = np.mean(np.array([x for (_, x) in elite]), axis=0) if elite else center\n            trans = row_normed(adj)\n            total_nodes = nodes.shape[1]\n\n            for _ in range(pop):\n                if evals >= self.budget:\n                    break\n\n                # walk length heavy-tailed\n                if np.random.rand() < self.walk_heavy_tail_p:\n                    L = min(total_nodes, 1 + int(np.random.exponential(scale=max(2.2, self.dim * 0.5))))\n                else:\n                    L = 1 + np.random.randint(0, max(1, int(np.ceil(0.65 * max(1, self.dim / 3.0)))))\n                L = max(1, min(total_nodes, L))\n\n                start = pick_start_node(center_vec=center)\n                vec, path_idxs, coefs = random_walk_direction(start, L)\n\n                inv_usage = (1.0 / (node_usage[path_idxs] + 1e-12))\n                inv_usage = inv_usage / (np.mean(inv_usage) + 1e-12)\n                scale = sigma * (0.88 + 0.72 * np.mean(inv_usage)) * (0.85 + np.random.rand() * 1.6)\n\n                step = vec * scale\n\n                if np.random.rand() < 0.52:\n                    noise = 0.22 * sigma * np.random.randn(self.dim)\n                    noise = noise - vec * (vec @ noise)\n                    step += 0.22 * noise\n\n                if np.random.rand() < 0.18:\n                    # more aggressive global heavy jump (but rarer)\n                    d = np.random.randn(self.dim)\n                    if elite and np.random.rand() < 0.92:\n                        pick = elite[np.random.randint(0, len(elite))][1]\n                        v = pick - center\n                        if np.linalg.norm(v) > 1e-12:\n                            d = v\n                    d = d / (np.linalg.norm(d) + 1e-12)\n                    rad = range_mean * (0.45 + np.abs(np.random.standard_t(2.0)) * 1.0)\n                    step = 0.35 * step + rad * d * (0.6 + np.random.rand() * 1.6)\n\n                # clamp and candidate\n                max_coord = 6.0 * range_vec\n                step = np.minimum(np.maximum(step, -max_coord), max_coord)\n                x_plus = center + step\n                x_plus = np.minimum(np.maximum(x_plus, lb), ub)\n\n                # mirrored evaluation when feasible\n                if evals + 2 <= self.budget:\n                    x_minus = center - step\n                    x_minus = np.minimum(np.maximum(x_minus, lb), ub)\n                    f_plus = func(x_plus); evals += 1\n                    f_minus = func(x_minus); evals += 1\n                    if f_plus <= f_minus:\n                        f_cand = f_plus; x_cand = x_plus; s = x_plus - center\n                    else:\n                        f_cand = f_minus; x_cand = x_minus; s = x_minus - center\n                else:\n                    f_cand = func(x_plus); evals += 1\n                    x_cand = x_plus; s = x_plus - center\n\n                # update\n                if f_cand + 1e-16 < f_best:\n                    improv = f_best - f_cand if f_best < np.inf else 0.0\n                    f_best = f_cand\n                    x_best = x_cand.copy()\n                    center = x_cand.copy()\n                    update_elite(x_cand, f_cand)\n                    successes += 1\n                    stagnation = 0\n                    weight = (abs(improv) + 1e-10) * (1.0 + np.linalg.norm(s) / (range_mean + 1e-12))\n                    reinforce_path(path_idxs, coefs, weight)\n                    node_usage[path_idxs] += 0.45 * weight\n                    recent_steps.append((s.copy(), max(1e-8, weight)))\n                    if improv > gen_best_improv:\n                        gen_best_improv = improv\n                        gen_best_step = s.copy()\n                else:\n                    node_usage[path_idxs] += 0.06\n                    recent_steps.append((s.copy(), 0.07 * (1.0 + np.linalg.norm(s) / (range_mean + 1e-12))))\n                    stagnation += 1\n                    proj = np.abs(nodes.T @ s)\n                    if proj.sum() > 0:\n                        best_idx = int(np.argmax(proj))\n                        node_usage[best_idx] += 0.09\n                        adj[start, best_idx] += 0.06\n                        adj[best_idx, start] += 0.06\n\n                # bounded recent list\n                if len(recent_steps) > recent_limit:\n                    recent_steps = recent_steps[-recent_limit:]\n\n                # stabilization: clip and normalize\n                adj = np.clip(adj, 0.0, 1e7)\n                # small global diffusion of adj into neighbors to keep density spreading\n                if np.random.rand() < 0.12:\n                    adj = 0.98 * adj + 0.02 * (adj @ adj) / (np.maximum(adj.sum(axis=1, keepdims=True), 1e-12))\n                trans = row_normed(adj)\n\n                if evals >= self.budget:\n                    break\n\n            # adapt sigma based on success rate\n            success_rate = successes / max(1, pop)\n            target = 0.25\n            lr = 0.18\n            if success_rate > target:\n                sigma *= np.exp(lr * (success_rate - target))\n            else:\n                sigma *= np.exp(-lr * (target - success_rate))\n            sigma = float(np.clip(sigma, 1e-12, 6.0 * range_mean))\n\n            # progressive densification based on stagnation or randomly\n            if stagnation > max(8, self.dim) or np.random.rand() < 0.14:\n                # use recent cooc to build cliques\n                if len(recent_steps) > 0:\n                    recent_sorted = sorted(recent_steps, key=lambda t: -t[1])[:min(16, len(recent_steps))]\n                    for svec, w in recent_sorted:\n                        proj = np.abs(nodes.T @ svec)\n                        topk = min(10, nodes.shape[1])\n                        idxs = np.argsort(proj)[-topk:]\n                        for i in range(len(idxs)):\n                            for j in range(i + 1, len(idxs)):\n                                a = idxs[i]; b = idxs[j]\n                                delta = 0.26 * w * (1.0 + np.random.rand() * 0.6)\n                                adj[a, b] += delta\n                                adj[b, a] += delta\n                                cooc[a, b] += delta\n                                cooc[b, a] += delta\n                # random small edges added across the graph to increase connectivity\n                additions = max(1, int(0.03 * adj.size))\n                for _e in range(additions):\n                    a = np.random.randint(0, n_nodes); b = np.random.randint(0, n_nodes)\n                    if a != b:\n                        val = 0.06 * (1.0 + np.random.rand())\n                        adj[a, b] += val\n                        adj[b, a] += val\n                # strengthen randomly some supernode links to spread mixing mass\n                if n_super > 0:\n                    for si in range(n_super):\n                        picks = np.random.choice(n_nodes, size=min(6, n_nodes), replace=False)\n                        super_adj[si, picks] += 0.06 * (1.0 + np.random.rand(len(picks)))\n                    super_adj[:] = super_adj / (super_adj.sum(axis=1, keepdims=True) + 1e-12)\n                trans = row_normed(adj)\n\n            # structural realignment and augmentation occasionally or on heavy stagnation\n            if np.random.rand() < 0.20 or (stagnation > self.stagnation_restart and len(recent_steps) > 3):\n                adj = np.maximum(adj * 0.94, 0.0)\n                augment_edges_from_cooc()\n                realign_nodes_from_recent()\n                trans = row_normed(adj)\n                node_usage = 0.92 * node_usage + 0.08\n\n            # reward nodes from generation best\n            if gen_best_step is not None:\n                proj = np.abs(nodes.T @ gen_best_step)\n                if proj.sum() > 0:\n                    best_idx = int(np.argmax(proj))\n                    node_usage[best_idx] += 0.9 * (gen_best_improv ** 0.33 + 1e-9)\n                    neighbors = np.argsort(adj[best_idx])[-min(8, n_nodes):]\n                    for nb in neighbors:\n                        adj[best_idx, nb] += 0.22 * (gen_best_improv ** 0.2 + 1e-9)\n                        adj[nb, best_idx] += 0.22 * (gen_best_improv ** 0.2 + 1e-9)\n                    # slightly link best node to supernodes to spread influence\n                    if n_super > 0:\n                        for si in range(n_super):\n                            super_adj[si, best_idx] += 0.06 * (gen_best_improv ** 0.15 + 1e-9)\n                        super_adj[:] = super_adj / (super_adj.sum(axis=1, keepdims=True) + 1e-12)\n                trans = row_normed(adj)\n\n            # stagnation-driven dense surge (more aggressive: create temporary high-degree mixing using supernodes)\n            if stagnation > self.stagnation_restart:\n                stagnation = 0\n                if elite and np.random.rand() < 0.9:\n                    pick = elite[np.random.randint(0, len(elite))][1]\n                    base = pick.copy()\n                else:\n                    base = np.random.uniform(lb, ub)\n                tries = min(self.budget - evals, max(8, 8 * self.dim))\n                saved_adj = adj.copy()\n                # densify by blending with strong co-occurrence and supernode influence\n                adj = 0.6 * adj + 0.4 * (cooc + 0.2 * (super_adj.sum(axis=0, keepdims=True).T @ np.ones((1, n_nodes))))\n                # ensure some randomness\n                adj += 0.3 * np.abs(np.random.randn(*adj.shape)) * self.edge_init_scale\n                trans = row_normed(adj)\n                for _ in range(tries):\n                    if evals >= self.budget:\n                        break\n                    deg = min(total_nodes, max(3, int(np.random.randint(max(3, int(0.5 * total_nodes)), total_nodes + 1))))\n                    invu = (1.0 / (node_usage + 1e-12)) ** 1.4\n                    prob = invu / (invu.sum() + 1e-12)\n                    idxs = np.random.choice(total_nodes, size=deg, replace=False, p=prob)\n                    coefs = np.random.standard_t(2.0, size=deg)\n                    coefs = np.sign(coefs) * (np.abs(coefs) ** (1.0 + 0.9 * np.random.rand()))\n                    coefs = coefs / (np.linalg.norm(coefs) + 1e-12)\n                    step = nodes[:, idxs] @ coefs\n                    step = step / (np.linalg.norm(step) + 1e-12)\n                    step = step * range_mean * (1.0 + 1.1 * np.random.rand()) * (1.0 + 0.9 * np.mean(1.0 / (node_usage[idxs] + 1e-12)))\n                    step += 0.18 * range_mean * np.random.randn(self.dim) * np.random.rand()\n                    x = np.minimum(np.maximum(base + step, lb), ub)\n                    f = func(x); evals += 1\n                    if f + 1e-16 < f_best:\n                        f_best = f; x_best = x.copy(); center = x.copy()\n                        update_elite(x, f)\n                # soften-restore adj so that densification leaves memory but does not dominate forever\n                adj = 0.55 * saved_adj + 0.45 * adj\n                trans = row_normed(adj)\n                recent_steps = recent_steps[-recent_limit:]\n                realign_nodes_from_recent()\n\n            # occasional small jitter to center\n            if np.random.rand() < 0.07:\n                center = np.minimum(np.maximum(center + 0.035 * sigma * np.random.randn(self.dim), lb), ub)\n\n        return f_best, x_best", "configspace": "", "generation": 7, "feedback": "The algorithm DenseGraphFusionExplorer scored 0.627 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["76901bbf-f8f5-4160-aafc-bd2333b311cd"], "operator": null, "metadata": {"ast_features": {"Nodes": 5520.0, "Edges": 5519.0, "Max Degree": 46.0, "Min Degree": 1.0, "Mean Degree": 1.9996376811594203, "Degree Variance": 2.2789853759714345, "Transitivity": 0.0, "Max Depth": 17.0, "Min Depth": 2.0, "Mean Depth": 8.698083691826358, "Max Clustering": 0.0, "Min Clustering": 0.0, "Mean Clustering": 0.0, "Clustering Variance": 0.0, "Degree Entropy": 1.3151724970686147, "Depth Entropy": 2.18990415059272, "Assortativity": 0.0, "Average Eccentricity": 20.158695652173915, "Diameter": 28.0, "Radius": 14.0, "Edge Density": 0.00018115942028985507, "Average Shortest Path": 11.525578436547264, "mean_complexity": 10.666666666666666, "total_complexity": 96.0, "mean_token_count": 558.0, "total_token_count": 5022.0, "mean_parameter_count": 3.111111111111111, "total_parameter_count": 28.0}, "feature_guidance_action": "increase", "feature_guidance_feature_name": "Edge Density", "aucs": [0.08843081211209725, 0.3126774421774312, 0.8306749896881753, 0.8974988960380965, 0.8341077900969779, 0.8496947102463082, 0.2982330225762091, 0.5036494470352897, 0.832719304130979, 0.8262036044075832]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-guided-MA_BBOB-4", "seed": 4}

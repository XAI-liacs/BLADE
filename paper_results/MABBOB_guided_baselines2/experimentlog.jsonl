{"method_name": "MCTS_AHD", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "MCTS_AHD", "budget": 200, "kwargs": {"lambda_0": 0.1, "alpha": 0.5, "maximisation": true, "max_children": 5, "expansion_factor": 2}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "396683fe-ad29-4f0c-b4bb-bd141f4a9657", "fitness": 0.8368902145110958, "name": "MirrorAdaptiveDECovInject", "description": "Hybridizes mirrored Gaussian sampling and adaptive Differential Evolution (rand/1/bin) under a learned covariance: mirrored pairs around a weighted exponential-rank mean reduce variance while an eigendecomposed covariance (updated with a relatively slow cov_lr=0.10) shapes proposals. DE trials use adaptive per-trial F sampled from a heavy-tailed Cauchy (centered at F_base=0.6) and CR from a normal (CR_base=0.9), with archive-biased selection and a larger archive_size to preserve diversity. Step-size sigma adapts softly (sigma_adapt_rate=0.15) toward a slightly higher success target (0.25), and eigenvalue clamping / condition-number limits plus covariance injection keep the search numerically stable. On stagnation the algorithm rarely (levy_prob=0.10) performs heavy-tailed multivariate‑t probes or eigen-inflation and mild restarts to escape local traps, while all sampling and updates respect problem bounds.", "code": "import numpy as np\n\nclass MirrorAdaptiveDECovInject:\n    \"\"\"\n    MirrorAdaptiveDECovInject:\n    - Hybrid of mirrored Gaussian sampling and adaptive Differential Evolution (rand/1/bin).\n    - Maintains a covariance matrix to shape Gaussian proposals and updates it from selected elites.\n    - Uses exponential rank weights (different from log-weights in the reference).\n    - Adaptive F and CR per trial (sampled from heavy-tailed / normal priors).\n    - Eigenvalue clamping + covariance injection instead of plain floor blend; opportunistic eigen-inflation\n      / small multivariate-t probes on stagnation (different equations and parameter settings).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, pop_base=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        # Algorithmic knobs (deliberately different choices from the provided HybridCMADE)\n        self.pop_base = pop_base  # if None, computed below (uses sqrt scaling instead of log)\n        self.cov_lr = 0.10               # slower covariance learning than reference's 0.25\n        self.sigma_adapt_rate = 0.15     # milder step-size adaptation\n        self.success_target = 0.25       # slightly higher target success rate\n        self.archive_size = min(50, max(5, 6 * self.dim))  # larger archive scaling\n        self.stagnation_iters = max(15, int(0.03 * self.budget))  # different stagnation trigger\n        self.levy_prob = 0.10            # rarer heavy-tailed probes\n        self.F_base = 0.6                # base differential weight\n        self.CR_base = 0.9               # base crossover prob\n        self.max_eig_ratio = 1e6         # maximal condition number allowed\n        self.min_eig_scale = 1e-12       # absolute minimum eigenvalue floor\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # bounds (Many BBOB uses [-5,5], but honor func bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n        assert lb.shape[0] == self.dim and ub.shape[0] == self.dim\n        bounds_scale = (ub - lb)\n\n        # adaptive population size (different from original log formula)\n        if self.pop_base is None:\n            lam = max(8, int(6 + 2.0 * np.sqrt(max(1, self.dim))))\n        else:\n            lam = int(self.pop_base)\n        lam = min(lam, max(2, self.budget))\n\n        evals = 0\n        # initial uniform seeding (consume up to lam evaluations)\n        init_n = min(lam, self.budget - evals)\n        X0 = rng.uniform(lb, ub, size=(init_n, self.dim))\n        f0 = np.array([func(x) for x in X0])\n        evals += init_n\n\n        # best-known\n        best_idx = int(np.argmin(f0))\n        f_best = float(f0[best_idx])\n        x_best = X0[best_idx].copy()\n\n        # archive (store as arrays)\n        archive_X = X0.copy()\n        archive_f = f0.copy()\n\n        def archive_add(x, fx):\n            nonlocal archive_X, archive_f\n            if len(archive_f) < self.archive_size:\n                archive_X = np.vstack([archive_X, x.reshape(1, -1)])\n                archive_f = np.concatenate([archive_f, np.array([fx])])\n            else:\n                worst_idx = int(np.argmax(archive_f))\n                if fx < archive_f[worst_idx]:\n                    archive_X[worst_idx] = x\n                    archive_f[worst_idx] = fx\n\n        # initialize mean by exponential rank-weighted average of the top half\n        mu0 = max(1, init_n // 2)\n        order0 = np.argsort(f0)\n        elites0 = X0[order0[:mu0]]\n        # exponential weights (different equation)\n        ranks = np.arange(mu0)\n        weights0 = np.exp(-ranks / max(1.0, mu0 / 3.0))\n        weights0 = weights0 / weights0.sum()\n        m = (weights0.reshape(-1, 1) * elites0).sum(axis=0)\n\n        # initial covariance: isotropic scaled relative to bounds but different divisor\n        C = np.diag(((bounds_scale / 6.0) ** 2).clip(min=1e-16))\n        sigma = 0.20 * np.mean(bounds_scale)\n        sigma = max(sigma, 1e-12)\n\n        # strategy state\n        p_succ = self.success_target\n        stagn_iters = 0\n        iter_count = 0\n\n        # helper: safe decomposition to get transform A with A^T A = C\n        def chol_like(Cmat):\n            # prefer eigh and form sqrt(V D V^T) to allow rectangular A\n            vals, vecs = np.linalg.eigh(Cmat)\n            vals_clipped = np.clip(vals, self.min_eig_scale, None)\n            # enforce condition limit\n            if vals_clipped.max() / vals_clipped.min() > self.max_eig_ratio:\n                # rescale largest eigenvalues to maintain condition number\n                max_allowed = vals_clipped.min() * self.max_eig_ratio\n                vals_clipped = np.minimum(vals_clipped, max_allowed)\n            A = (vecs * np.sqrt(vals_clipped)).T  # A @ A.T = Cmat, A^T @ A = ...\n            return A\n\n        # main loop: generate up to lam candidates per generation\n        while evals < self.budget:\n            iter_count += 1\n            remaining = self.budget - evals\n            lam_iter = min(lam, remaining)\n            mu = max(1, lam_iter // 2)\n\n            # recompute exponential rank weights (different equation)\n            ranks = np.arange(mu)\n            weights = np.exp(-ranks / max(1.0, mu / 3.0))\n            weights = weights / np.sum(weights)\n\n            A = chol_like(C)\n\n            # candidate composition: half mirrored Gaussian, half adaptive DE\n            n_gauss = lam_iter // 2\n            n_de = lam_iter - n_gauss\n            Xcand = np.empty((lam_iter, self.dim), dtype=float)\n\n            # 1) mirrored Gaussian proposals: produce pairs (x, 2m - x) for variance reduction\n            if n_gauss > 0:\n                # generate n_gauss/2 unique normals and mirror them; if odd handle last separately\n                Z = rng.normal(size=(n_gauss, self.dim))\n                Y = Z @ (A.T)\n                Xg = m + sigma * Y\n                # mirrored\n                Xg_mirror = 2.0 * m - Xg\n                # mix sequence Xg and mirrors to fill n_gauss slots\n                for i in range(n_gauss):\n                    if i < len(Xg):\n                        cand = Xg[i]\n                    else:\n                        cand = Xg_mirror[i - len(Xg)]\n                    # clamp\n                    Xcand[i] = np.minimum(np.maximum(cand, lb), ub)\n\n            # 2) adaptive DE rand/1/bin proposals using archive\n            if n_de > 0:\n                # ensure sorted archive for selection bias towards elites\n                if len(archive_f) >= 3:\n                    idx_sort = np.argsort(archive_f)\n                    sorted_X = archive_X[idx_sort]\n                    sorted_f = archive_f[idx_sort]\n                else:\n                    sorted_X = archive_X\n                    sorted_f = archive_f\n\n                for i in range(n_de):\n                    # sample F from Cauchy centered at base, heavy-tailed but clipped\n                    F = rng.standard_cauchy() * 0.1 + self.F_base\n                    F = float(np.clip(F, 0.2, 1.0))\n                    # sample CR from normal near base\n                    CR = float(np.clip(rng.normal(loc=self.CR_base, scale=0.15), 0.0, 1.0))\n\n                    if len(sorted_X) < 3:\n                        # fallback gaussian\n                        z = rng.normal(size=self.dim)\n                        y = z @ (A.T)\n                        trial = m + sigma * y\n                    else:\n                        # pick three distinct indices for rand/1\n                        idxs = rng.choice(len(sorted_X), size=3, replace=False)\n                        xr = sorted_X[idxs[0]]\n                        xa = sorted_X[idxs[1]]\n                        xb = sorted_X[idxs[2]]\n                        mutant = xr + F * (xa - xb)\n                        # target vector chosen randomly between m and a random elite (introduce diversity)\n                        if rng.rand() < 0.5:\n                            target = m\n                        else:\n                            target = sorted_X[rng.randint(len(sorted_X))]\n                        # binomial crossover\n                        mask = rng.rand(self.dim) < CR\n                        if not np.any(mask):\n                            mask[rng.randint(self.dim)] = True\n                        trial = np.where(mask, mutant, target)\n                        # small Gaussian jitter scaled by sigma and relative to bounds\n                        trial += rng.normal(scale=0.3 * sigma, size=self.dim)\n\n                    # clamp\n                    Xcand[n_gauss + i] = np.minimum(np.maximum(trial, lb), ub)\n\n            # evaluate candidates (respect budget)\n            f_cand = np.empty(lam_iter, dtype=float)\n            for i in range(lam_iter):\n                if evals >= self.budget:\n                    # Should not happen as lam_iter chosen <= remaining, but guard anyway\n                    f_cand[i] = np.inf\n                    continue\n                f_cand[i] = float(func(Xcand[i]))\n            evals += lam_iter\n\n            # update archive\n            for i in range(lam_iter):\n                archive_add(Xcand[i].copy(), float(f_cand[i]))\n\n            # generation best\n            gen_best_idx = int(np.argmin(f_cand))\n            gen_best_f = float(f_cand[gen_best_idx])\n            gen_best_x = Xcand[gen_best_idx].copy()\n\n            improved = False\n            if gen_best_f < f_best:\n                f_best = gen_best_f\n                x_best = gen_best_x.copy()\n                improved = True\n                stagn_iters = 0\n            else:\n                stagn_iters += 1\n\n            # selection: choose top-mu candidates\n            order = np.argsort(f_cand)\n            X_mu = Xcand[order[:mu]]\n\n            # recompute weighted mean (exponential rank weights)\n            m_new = (weights.reshape(-1, 1) * X_mu).sum(axis=0)\n\n            # deltas normalized by sigma for covariance update\n            deltas = (X_mu - m) / (sigma + 1e-20)\n            W = weights.reshape(-1, 1)\n            weighted_cov = (deltas * W).T @ deltas  # (dim x dim)\n\n            # include small rank-one update from mean shift\n            mean_shift = ((m_new - m) / max(sigma, 1e-20)).reshape(-1, 1)\n            rank_one = (mean_shift @ mean_shift.T) * 0.5\n\n            # covariance update with injection term (different blend than reference)\n            C = (1.0 - self.cov_lr) * C + self.cov_lr * (weighted_cov + 0.5 * rank_one)\n\n            # eigen-regularize: clamp eigenvalues to reasonable range relative to bounds_scale\n            vals, vecs = np.linalg.eigh(C)\n            # set minimum eigenvalue relative to typical squared scale of bounds\n            min_eig = max(self.min_eig_scale, (np.mean(bounds_scale) * 1e-3) ** 2)\n            vals_clipped = np.clip(vals, min_eig, None)\n            # enforce max condition number\n            max_allowed = vals_clipped.min() * self.max_eig_ratio\n            vals_clipped = np.minimum(vals_clipped, max_allowed)\n            C = (vecs * vals_clipped) @ vecs.T\n\n            # update mean\n            m = m_new.copy()\n\n            # step-size adaptation via smoothed success-rate but with different smoothing\n            p_succ = 0.85 * p_succ + 0.15 * float(improved)\n            sigma *= np.exp(self.sigma_adapt_rate * (p_succ - self.success_target))\n            # clip sigma to sensible bounds (bounded by domain size)\n            sigma = float(np.clip(sigma, 1e-12, 1.5 * np.max(bounds_scale)))\n\n            # stagnation handling: covariance injection or small multivariate-t probes\n            if stagn_iters >= self.stagnation_iters and evals < self.budget:\n                if rng.rand() < self.levy_prob:\n                    # heavy-tailed multivariate-t probe centered at one of the top elites\n                    idx_sort = np.argsort(archive_f)\n                    topK = min(len(idx_sort), max(3, self.dim))\n                    anchor = archive_X[idx_sort[rng.randint(topK)]]\n                    # multivariate t-like by sampling normal and dividing by sqrt(gamma)\n                    # gamma drawn from chi2 with df small to create heavy tails\n                    df = 2.0\n                    gamma = max(1e-8, rng.chisquare(df))\n                    z = rng.normal(size=self.dim)\n                    scale = max(0.4 * np.mean(bounds_scale), sigma * 3.0)\n                    jump = z / np.sqrt(gamma / df) * scale\n                    x_jump = anchor + jump\n                    x_jump = np.minimum(np.maximum(x_jump, lb), ub)\n                    fj = float(func(x_jump))\n                    evals += 1\n                    archive_add(x_jump.copy(), fj)\n                    if fj < f_best:\n                        f_best = fj\n                        x_best = x_jump.copy()\n                        stagn_iters = 0\n                        m = x_jump.copy()\n                        sigma = max(sigma, 0.4 * np.mean(bounds_scale))\n                        C = np.diag(((bounds_scale / 5.0) ** 2).clip(min=1e-16))\n                    else:\n                        # perform eigen-inflation: increase small eigenvalues moderately to encourage new directions\n                        vals, vecs = np.linalg.eigh(C)\n                        vals = np.maximum(vals, min_eig)\n                        inflation = 1.5\n                        vals = vals * inflation\n                        C = (vecs * vals) @ vecs.T\n                        # nudge mean slightly toward the best\n                        m = 0.85 * m + 0.15 * x_best\n                        sigma = min(1.5 * np.max(bounds_scale), sigma * 1.4)\n                else:\n                    # milder restart around the best with small Gaussian jitter (not full reinitialization)\n                    jitter = rng.normal(scale=0.08 * np.maximum(bounds_scale, 1.0), size=self.dim)\n                    m = x_best + jitter\n                    m = np.minimum(np.maximum(m, lb), ub)\n                    C = np.diag(((bounds_scale / 7.0) ** 2).clip(min=1e-16))\n                    sigma = max(sigma, 0.25 * np.mean(bounds_scale))\n                stagn_iters = 0\n                p_succ = self.success_target  # reset smoothed success estimate\n\n        # finished budget\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm MirrorAdaptiveDECovInject scored 0.837 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "ad00285c-2740-4894-bdf7-843cbde9ae5f", "operator": null, "metadata": {"aucs": [0.47300416260066824, 0.6238677356342714, 0.9029142496152635, 0.9650200021339553, 0.9257517102557562, 0.9357207553435835, 0.7955137354389289, 0.901434392868441, 0.9202952522322716, 0.9253801489878187]}, "task_prompt": ""}, "log_dir": "run-MCTS_AHD-MA_BBOB-5", "seed": 5}
{"method_name": "MCTS_AHD", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "MCTS_AHD", "budget": 200, "kwargs": {"lambda_0": 0.1, "alpha": 0.5, "maximisation": true, "max_children": 5, "expansion_factor": 2}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "cdb41671-1583-49d0-ae74-4bd81b978104", "fitness": 0.7254473308775983, "name": "ARDCE", "description": "The ARDCE algorithm is a hybrid CMA-ES–style optimizer: it uses a population (lambda, mu chosen like CMA-ES), log-based recombination weights, evolution paths (ps, pc), rank-one and rank-mu covariance updates with learning rates (cc, cs, c1, cmu, damps) and sigma adaptation using the expected norm chi_n to control step-size. Sampling is performed by transforming standard normals with the eigendecomposition factors B and D (approximate sqrt(C)), with periodic eigen-recomputation (eig_every) and numerical safeguards (non-negative eigenvalues, LinAlgError fallback, lower bound on sigma). To boost exploration and affine robustness an archive of evaluated points is kept and occasional DE-style difference mutations (probability p_de, factor F_de) are added to offspring, and all candidates are clipped to user-specified bounds. Initialization choices (mean uniformly in bounds, sigma = 0.3·mean(range)), strict budget-aware evaluation counting, and maintaining the best-so-far (x_opt, f_opt) complete the design for reliable, bounded-budget continuous optimization.", "code": "import numpy as np\n\nclass ARDCE:\n    \"\"\"\n    Hybrid Adaptive Rotational Differential Covariance Estimation (ARDCE)\n    A CMA-ES style covariance adaptation with occasional differential-evolution\n    style difference mutations for added exploration robustness.\n    One-line: CMA-ES-like adaptive covariance + DE-inspired mutations for affine-robust continuous optimization.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # default population size similar to CMA-ES recommendation\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = self.budget\n\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # ensure shapes broadcast\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # Strategy parameters (CMA-ES inspired)\n        lam = self.lambda_\n        mu = self.mu\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # adaptation constants\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        c1 = 2 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1 - c1, 2 * (mu_eff - 2 + 1 / mu_eff) / ((n + 2) ** 2 + mu_eff))\n        damps = 1 + 2 * max(0, np.sqrt((mu_eff - 1) / (n + 1)) - 1) + cs\n\n        # expectation of ||N(0,I)||\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize dynamic state\n        m = np.random.uniform(lb, ub)  # initial mean in bounds\n        sigma = 0.3 * np.mean(ub - lb)  # initial step-size\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eigen_eval_counter = 0\n        eig_every = max(1, int(10 * n))  # recompute eigendecomposition occasionally\n\n        # maintain archive of evaluated points for DE-style differences\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of m (optional but useful)\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            if fm < f_opt:\n                f_opt = fm\n                x_opt = xm.copy()\n\n        # main loop\n        while evals < budget:\n            # decide how many offspring to generate this generation\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # generate candidates\n            arz = np.random.randn(current_lambda, n)  # standard normal\n            ary = np.dot(arz, (B * D).T)  # y = B*D*z  (approx sampling from N(0,C))\n            arx = m + sigma * ary\n            # occasionally apply DE-style difference mutation (keeps affine exploration)\n            p_de = 0.25\n            F_de = 0.8\n            for k in range(current_lambda):\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    # choose two random distinct archived vectors\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    arx[k] = arx[k] + de_mut\n                # clip to bounds\n                arx[k] = np.clip(arx[k], lb, ub)\n\n            # Evaluate candidates\n            arfit = np.zeros(current_lambda)\n            for k in range(current_lambda):\n                x = arx[k]\n                f = func(x)\n                evals += 1\n                arfit[k] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n                if evals >= budget:\n                    # we must stop after reaching budget\n                    break\n\n            # selection and recombination\n            idx = np.argsort(arfit)\n            # get top mu vectors\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)  # shape (mu, n)\n            m_old = m.copy()\n            m = np.sum(weights[:, np.newaxis] * x_sel, axis=0)\n\n            # update evolution paths\n            y_w = np.sum(weights[:, np.newaxis] * y_sel, axis=0)  # weighted mean step\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n            norm_ps = np.linalg.norm(ps)\n            # hsig\n            hsig = 1.0 if (norm_ps / np.sqrt(1 - (1 - cs) ** (2 * (evals / current_lambda + 1))) / chi_n) < (1.4 + 2 / (n + 1)) else 0.0\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n\n            # adapt covariance matrix C\n            # rank-one update\n            rank_one = np.outer(pc, pc)\n            # rank-mu update\n            rank_mu = np.zeros((n, n))\n            for i in range(mu):\n                yi = y_sel[i][:, None]\n                rank_mu += weights[i] * (yi @ yi.T)\n            C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n\n            # adapt step-size sigma\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1))\n\n            # recompute B and D from C every eig_every evaluations (approx)\n            eigen_eval_counter += current_lambda\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                # ensure symmetry\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    # numerical stability: non-negative\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    # fallback to identity\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # small safeguard: if sigma is too small, increase\n            if sigma < 1e-12:\n                sigma = 1e-12\n\n            # If budget exhausted, break (loop condition handles it)\n            if evals >= budget:\n                break\n\n        # final return best found\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ARDCE scored 0.725 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "80014b04-8d18-4853-8f88-dd939ade8f97", "operator": null, "metadata": {"aucs": [0.28826235218145924, 0.8282870727367804, 0.8098979762344413, 0.9875256728519688, 0.8744507448084525, 0.8212246218846051, 0.36654567678821526, 0.6375089156840716, 0.732487976806662, 0.9082822987993264]}, "task_prompt": ""}, "log_dir": "run-MCTS_AHD-MA_BBOB-3", "seed": 3}
{"method_name": "MCTS_AHD", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "MCTS_AHD", "budget": 200, "kwargs": {"lambda_0": 0.1, "alpha": 0.5, "maximisation": true, "max_children": 5, "expansion_factor": 2}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "e2d95911-5f91-443f-9adb-3d660bf8d4bc", "fitness": 0.5017152580189368, "name": "AdaptiveMomentumExplorer", "description": "The algorithm is archive-driven and elite-guided: it maintains all evaluated points, computes a rank-weighted mean and a robust PCA-shaped covariance from elites to shape anisotropic search directions and anchors. It learns a nonnegative coordinate momentum (boosted by successful probe movements and decayed multiplicatively) and mixes three probe families—PCA-rotated Laplace-like probes, momentum-biased coordinate probes, and isotropic Gaussian probes—with log-uniform multiscale step sizes plus occasional tempered heavy-tailed or Cauchy-like global jumps and anchors chosen from mean/best/random elite. A lightweight ridge linear surrogate fitted on elites provides a gradient-like descent nudging, while an adaptive trust/scale is updated from recent success history and momentum, and soft restarts/injections plus archive pruning guard against long stagnation. Numerous numerical safeguards and tuned hyperparameters (e.g. base_scale_mult≈0.45, heavy_tail_prob≈0.12, momentum_decay≈0.975, jump_prob≈0.05) promote robust multiscale exploration-exploitation within the [-5,5] bounds.", "code": "import numpy as np\n\nclass AdaptiveMomentumExplorer:\n    \"\"\"\n    Adaptive Momentum Explorer (AME)\n\n    Main concept:\n    - Maintain an archive of evaluated points and compute a PCA-shaped covariance.\n    - Use a learned nonnegative momentum vector to bias coordinate sampling (momentum is boosted\n      by successful directions and decays multiplicatively).\n    - Generate a mix of probes: PCA-rotated subspace probes (Laplace-like coeffs), momentum-biased\n      coordinate probes, isotropic Gaussian probes; step sizes are sampled log-uniformly\n      over a wide multiscale range and occasionally multiplied by tempered heavy-tailed samples.\n    - Use a lightweight ridge linear surrogate (on elites) to estimate a descent direction and\n      nudge the best point. When stuck for long, perform a soft restart of a portion of the archive.\n    - Many internal numerical safeguards and strict adherence to the provided function evaluation budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_frac=0.06, elite_frac=0.15, base_scale_mult=0.45,\n                 heavy_tail_prob=0.12, jump_prob=0.05, momentum_decay=0.975,\n                 stagnation_restart=600):\n        \"\"\"\n        budget: allowed function evaluations\n        dim: problem dimension\n        seed: RNG seed\n        init_frac: fraction of budget used for initial uniform sampling\n        elite_frac: fraction of archive considered as elites\n        base_scale_mult: baseline multiplier of box size for starting scale\n        heavy_tail_prob: probability to apply tempered heavy-tailed multiplier to a probe\n        jump_prob: probability for a rare global cauchy-like jump\n        momentum_decay: multiplicative decay per major iteration for coordinate momentum\n        stagnation_restart: number of evaluations without improvement after which to partially restart\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n        self.init_frac = float(max(0.005, min(0.25, init_frac)))\n        self.elite_frac = float(max(0.05, min(0.5, elite_frac)))\n        self.base_scale_mult = float(base_scale_mult)\n        self.heavy_tail_prob = float(max(0.0, min(0.5, heavy_tail_prob)))\n        self.jump_prob = float(max(0.0, min(0.5, jump_prob)))\n        self.momentum_decay = float(max(0.90, min(0.9999, momentum_decay)))\n        self.stagnation_restart = int(max(50, stagnation_restart))\n\n    def __call__(self, func):\n        # bounds and clipping\n        lb = np.array(func.bounds.lb, dtype=float)\n        ub = np.array(func.bounds.ub, dtype=float)\n        if lb.size == 1:\n            lb = np.full(self.dim, lb.item(), dtype=float)\n        if ub.size == 1:\n            ub = np.full(self.dim, ub.item(), dtype=float)\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # Archive containers\n        xs = []\n        fs = []\n        evals = 0\n\n        # initial sampling\n        n_init = int(max(4 * self.dim, self.init_frac * self.budget))\n        n_init = max(2 * self.dim, n_init) if self.budget >= 2 * self.dim else max(1, n_init)\n        n_init = min(n_init, self.budget)\n        for _ in range(n_init):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            xs.append(x.copy())\n            fs.append(float(f))\n            evals += 1\n\n        xs = np.array(xs, dtype=float)\n        fs = np.array(fs, dtype=float)\n        best_idx = int(np.argmin(fs))\n        f_best = float(fs[best_idx])\n        x_best = xs[best_idx].copy()\n\n        # baseline scales\n        box_scale = np.mean(ub - lb)\n        base_scale = max(1e-12, self.base_scale_mult * box_scale)\n        scale = float(base_scale)\n\n        # momentum vector (coordinate bias)\n        momentum = np.zeros(self.dim, dtype=float)\n\n        # recent success tracking\n        recent = []\n        recent_len = 40\n        last_improve_eval = evals\n\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            # elite selection\n            k = max(4, int(max(6, self.dim * self.elite_frac)))\n            k = min(k, len(fs))\n            elite_idx = np.argsort(fs)[:k]\n            elite_pts = xs[elite_idx]\n            elite_fs = fs[elite_idx]\n\n            # compute weighted mean (rank-based weights) and a robust covariance\n            ranks = np.argsort(np.argsort(elite_fs))\n            rank_weights = (k - ranks).astype(float) + 1.0\n            rank_weights /= np.sum(rank_weights)\n            mean_el = np.sum(elite_pts * rank_weights[:, None], axis=0)\n            centered = elite_pts - mean_el\n            # robust covariance estimate with extra regularization\n            cov = (centered * rank_weights[:, None]).T.dot(centered)\n            cov += 1e-8 * (np.trace(cov) / (self.dim + 1e-12) + 1e-12) * np.eye(self.dim)\n            cov = 0.5 * (cov + cov.T)\n\n            # eigen-decompose\n            try:\n                eigvals, eigvecs = np.linalg.eigh(cov)\n                order = np.argsort(eigvals)[::-1]\n                eigvals = np.maximum(eigvals[order], 1e-16)\n                eigvecs = eigvecs[:, order]\n            except np.linalg.LinAlgError:\n                eigvals = np.ones(self.dim)\n                eigvecs = np.eye(self.dim)\n\n            # trust radius (different exponent, includes momentum influence)\n            cov_scale = np.sqrt(np.mean(eigvals))\n            mom_infl = 1.0 + 0.6 * (np.linalg.norm(momentum) / (1.0 + np.linalg.norm(momentum)))\n            trust = scale * (1.0 + cov_scale) * mom_infl * ((1.0 - evals / max(1, self.budget)) ** 1.2)\n            trust = max(1e-12, trust)\n\n            # batch size\n            p_base = 4 + max(0, self.dim // 4)\n            p = int(min(p_base + 4, max(1, remaining // max(4, self.dim // 2))))\n            p = max(1, p)\n\n            probes = []\n            probe_meta = []  # store (anchor, v, s)\n\n            for _ in range(p):\n                r = self.rng.rand()\n                if r < 0.50:\n                    # PCA-rotated subspace probe using Laplace-like coeffs (sharper center)\n                    n_comp = 1 + self.rng.randint(0, min(4, self.dim))\n                    # laplace-like sampling: exponential mix\n                    lap = self.rng.laplace(scale=1.0, size=n_comp)\n                    coeffs = lap * np.sqrt(eigvals[:n_comp] + 1e-12)\n                    v = eigvecs[:, :n_comp].dot(coeffs)\n                elif r < 0.85:\n                    # momentum-biased coordinate probe: choose coordinate via softmax of powered momentum\n                    scores = (np.abs(momentum) + 1e-9) ** 1.6\n                    if np.sum(scores) <= 0:\n                        probs = np.ones(self.dim) / self.dim\n                    else:\n                        probs = scores / np.sum(scores)\n                    idx = self.rng.choice(self.dim, p=probs)\n                    sign = np.sign(momentum[idx]) if np.abs(momentum[idx]) > 1e-12 else (1.0 if self.rng.rand() < 0.5 else -1.0)\n                    v = np.zeros(self.dim)\n                    v[idx] = float(sign)\n                    # small jitter in other dims\n                    v += 0.02 * self.rng.normal(size=self.dim)\n                else:\n                    # isotropic Gaussian probe\n                    v = self.rng.normal(size=self.dim)\n\n                # normalize direction\n                v_norm = np.linalg.norm(v)\n                if v_norm < 1e-12:\n                    v = self.rng.normal(size=self.dim)\n                    v_norm = np.linalg.norm(v) + 1e-12\n                v = v / v_norm\n\n                # step size log-uniform: different bounds (wider top)\n                smin = trust * 0.03125   # trust/32\n                smax = trust * 16.0      # trust*16\n                u = self.rng.rand()\n                s = smin * (smax / smin) ** u\n\n                # tempered heavy tail occasionally using student-t(3) or tempered cauchy mix\n                if self.rng.rand() < self.heavy_tail_prob:\n                    if self.rng.rand() < 0.6:\n                        ht = self.rng.standard_t(df=3)\n                    else:\n                        ht = self.rng.standard_cauchy()\n                    # temper extremes\n                    ht = np.tanh(0.6 * ht)\n                    s *= (1.0 + 0.6 * ht)\n\n                # anchor: prefer mean_el, sometimes best, occasionally random elite\n                ar = self.rng.rand()\n                if ar < 0.60:\n                    anchor = clip(mean_el + 0.04 * trust * self.rng.normal(size=self.dim))\n                elif ar < 0.9:\n                    anchor = x_best\n                else:\n                    anchor = elite_pts[self.rng.randint(0, elite_pts.shape[0])]\n\n                x_p = clip(anchor + s * v)\n                probes.append(x_p)\n                probe_meta.append((anchor.copy(), v.copy(), float(s)))\n\n            # Evaluate probes sequentially\n            new_indices = []\n            improved = 0\n            for x_p in probes:\n                if evals >= self.budget:\n                    break\n                f_p = func(x_p)\n                evals += 1\n                xs = np.vstack([xs, x_p])\n                fs = np.concatenate([fs, [float(f_p)]])\n                new_indices.append(len(fs) - 1)\n                if f_p < f_best:\n                    f_best = float(f_p)\n                    x_best = x_p.copy()\n                    improved += 1\n                    last_improve_eval = evals\n\n            # Reward momentum from successful probes (use matching via closeness)\n            if len(new_indices) > 0:\n                # find top few newly evaluated probes\n                sorted_new = sorted(new_indices, key=lambda ii: fs[ii])[:min(3, len(new_indices))]\n                for ni in sorted_new:\n                    x_c = xs[ni]\n                    # find meta that matches x_c (cheap matching)\n                    matched = None\n                    for a, (anc, v, s) in enumerate(probe_meta):\n                        if np.allclose(clip(anc + s * v), x_c, atol=1e-9):\n                            matched = (anc, v, s)\n                            break\n                    if matched is not None:\n                        anc, v, s = matched\n                        # project movement direction to coordinates and update momentum\n                        mv = x_c - anc\n                        norm_mv = np.linalg.norm(mv)\n                        if norm_mv > 1e-12:\n                            dircoord = np.abs(mv / (norm_mv + 1e-12))\n                            # nonlinear reward via tanh and scaled by step\n                            reward = 0.5 * np.tanh(3.0 * (s / (trust + 1e-12)))\n                            momentum = momentum * self.momentum_decay + reward * dircoord\n                    else:\n                        # fallback: add vector to momentum as movement from best\n                        mv = x_c - x_best\n                        norm_mv = np.linalg.norm(mv)\n                        if norm_mv > 1e-12:\n                            dircoord = np.abs(mv / (norm_mv + 1e-12))\n                            momentum = momentum * self.momentum_decay + 0.25 * dircoord\n\n            # light linear surrogate on elites to estimate gradient-like direction (different lambda)\n            if elite_pts.shape[0] >= min(self.dim + 1, 6) and evals < self.budget:\n                X_design = np.hstack([elite_pts - mean_el, np.ones((elite_pts.shape[0], 1))])\n                y = (elite_fs - np.min(elite_fs)).reshape(-1, 1)\n                lam = 5e-3 + 1e-2 * np.var(elite_fs)\n                reg = lam * np.eye(X_design.shape[1])\n                try:\n                    beta = np.linalg.solve(X_design.T.dot(X_design) + reg, X_design.T.dot(y)).flatten()\n                    g_est = beta[:self.dim]\n                    gn = np.linalg.norm(g_est)\n                    if gn > 1e-12:\n                        # step opposite to gradient estimate with randomized magnitude\n                        alpha = trust * (0.25 + 0.9 * self.rng.rand())\n                        x_g = clip(x_best - alpha * (g_est / (gn + 1e-12)))\n                        if evals < self.budget:\n                            f_g = func(x_g)\n                            evals += 1\n                            xs = np.vstack([xs, x_g])\n                            fs = np.concatenate([fs, [float(f_g)]])\n                            if f_g < f_best:\n                                f_best = float(f_g)\n                                x_best = x_g.copy()\n                                improved += 1\n                                last_improve_eval = evals\n                                # update momentum aligned with the improvement\n                                momentum += 0.6 * np.abs(g_est / (gn + 1e-12))\n                except np.linalg.LinAlgError:\n                    pass\n\n            # occasional global tempered Cauchy-like jump\n            if self.rng.rand() < self.jump_prob and evals < self.budget:\n                u = self.rng.normal(size=self.dim)\n                v = self.rng.normal(size=self.dim)\n                jump = u / (np.abs(v) + 1e-12)\n                denom = (np.std(jump) + 1e-12)\n                jump_scale = trust * (0.8 + 4.0 * (1.0 - evals / max(1, self.budget)))\n                x_jump = clip(x_best + (jump_scale * jump / denom))\n                f_jump = func(x_jump)\n                evals += 1\n                xs = np.vstack([xs, x_jump])\n                fs = np.concatenate([fs, [float(f_jump)]])\n                if f_jump < f_best:\n                    f_best = float(f_jump)\n                    x_best = x_jump.copy()\n                    improved += 1\n                    last_improve_eval = evals\n                    # boost momentum along jump coordinates mildly\n                    momentum += 0.4 * np.abs(jump / (denom + 1e-12))\n\n            # decay momentum (ensure non-negativity)\n            momentum *= self.momentum_decay\n            momentum = np.maximum(momentum, 0.0)\n\n            # update success history and adapt scale (sigmoid-style adjustment)\n            recent.append(1 if improved > 0 else 0)\n            if len(recent) > recent_len:\n                recent.pop(0)\n            succ = sum(recent) / max(1, len(recent))\n\n            # adaptive scale update (different functional form)\n            if succ > 0.35:\n                # ramp up with logistic-like accentuation\n                scale *= (1.0 + 0.5 * (succ - 0.25) / 0.75 + 0.05)\n            elif succ < 0.05:\n                # stronger contraction when nearly stuck\n                scale *= 0.70\n            else:\n                # mild decay\n                scale *= 0.992\n\n            # clamp scale\n            scale = float(np.clip(scale, base_scale * 1e-6, base_scale * 12.0))\n            if scale < 1e-14:\n                scale = base_scale * 1e-4\n\n            # if no improvement for long, perform soft restart: inject random points and enlarge scale\n            if evals - last_improve_eval > self.stagnation_restart and evals < self.budget:\n                # inject up to min(20, remaining/2) random samples\n                n_restart = int(min(20, max(2, (self.stagnation_restart // 40))))\n                n_restart = min(n_restart, self.budget - evals)\n                for _ in range(n_restart):\n                    xr = self.rng.uniform(lb, ub)\n                    fr = func(xr)\n                    evals += 1\n                    xs = np.vstack([xs, xr])\n                    fs = np.concatenate([fs, [float(fr)]])\n                    if fr < f_best:\n                        f_best = float(fr)\n                        x_best = xr.copy()\n                        last_improve_eval = evals\n                # enlarge scale moderately to encourage exploration\n                scale *= 2.5\n                # nudge momentum down to encourage new directions\n                momentum *= 0.3\n\n            # prune archive if too large\n            if xs.shape[0] > 3000:\n                keep = np.argsort(fs)[:1200]\n                xs = xs[keep]\n                fs = fs[keep]\n\n        # finalize best result\n        self.f_opt = float(f_best)\n        self.x_opt = x_best.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveMomentumExplorer scored 0.502 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "b070723a-8511-4cf7-bd21-d516b8d5ff21", "operator": null, "metadata": {"aucs": [0.20018652929809833, 0.15590426672066582, 0.8091167133726984, 0.8730498013581769, 0.3464635890007287, 0.8676142430492948, 0.2473488856489625, 0.568108812421684, 0.7876926260753359, 0.16166711324372296]}, "task_prompt": ""}, "log_dir": "run-MCTS_AHD-MA_BBOB-4", "seed": 4}
{"method_name": "MCTS_AHD", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "MCTS_AHD", "budget": 200, "kwargs": {"lambda_0": 0.1, "alpha": 0.5, "maximisation": true, "max_children": 5, "expansion_factor": 2}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "c36f260b-549d-4cfa-b69b-f7f348fa6a9d", "fitness": 0.5749175928324202, "name": "HybridMASHLevy", "description": "HybridMASHLevy is a budget-aware hybrid optimizer that runs a small Latin‑Hypercube initial design and then mixes four specialized kernels (K=4) via an additive bandit-style softmax allocator with Dirichlet jitter, where kernel weights are adapted by fast score updates (score_lr=0.9) and log-domain step-size adaptation (per-kernel log_scales with bounds ~1e-5..30) tempered by a multiplicative temperature (cooling=0.995). The four kernels are: (0) heavy-tailed Lévy/Cauchy global jumps anchored to the current elite for long-range exploration, (1) PCA‑guided anisotropic local probes that use principal directions and eigenvalue scaling for focused search, (2) DE‑style differential recombination with stochastic F/cr and donor masking for population-driven moves, and (3) a diagonal quadratic/linear surrogate fit around a chosen center that proposes a Newton-like step plus optional 1D internal probes. Additional design features include strict evaluation accounting via an eval wrapper, archive-maintained history with novelty pushes to avoid duplicates, small stochastic jitter for exploration, batch evaluation control (batch ≈ max(4,2*dim)), smoothing/decay of scores and successes, and periodic archive pruning to keep best + diverse points.", "code": "import numpy as np\n\nclass HybridMASHLevy:\n    \"\"\"\n    HybridMASHLevy: A multi-kernel adaptive optimizer combining\n    MASH-style additive bandit scores and log-scale step adaptation\n    with No.1-inspired Lévy/Cauchy jumps, PCA-informed local probes\n    and differential-style recombination. Budget-aware, archive-based,\n    with multiplicative temperature cooling and strict evaluation accounting.\n\n    Usage:\n        opt = HybridMASHLevy(budget=10000, dim=10, seed=1)\n        fbest, xbest = opt(func)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.verbose = verbose\n\n        # outputs\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        # bounds support\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        # broadcast scalars if necessary\n        if lb.shape == () or ub.shape == ():\n            lb = np.full(self.dim, float(lb))\n            ub = np.full(self.dim, float(ub))\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        span[span == 0.0] = 1.0\n\n        evals_left = int(self.budget)\n        X_archive = []\n        F_archive = []\n        f_opt = np.inf\n        x_opt = None\n\n        # safe evaluator wrapper with strict budget accounting\n        def eval_point(x):\n            nonlocal evals_left, X_archive, F_archive, f_opt, x_opt\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            if evals_left <= 0:\n                raise RuntimeError(\"No evaluations left\")\n            f = float(func(x))\n            evals_left -= 1\n            X_archive.append(x.copy())\n            F_archive.append(float(f))\n            if f < f_opt:\n                f_opt = float(f)\n                x_opt = x.copy()\n            return float(f), x.copy()\n\n        # Latin Hypercube Sampling initializer\n        def lhs(n):\n            samples = np.empty((n, self.dim), dtype=float)\n            for j in range(self.dim):\n                perm = rng.permutation(n)\n                u = (perm + rng.rand(n)) / float(n)\n                samples[:, j] = lb[j] + u * span[j]\n            return samples\n\n        # initial design\n        if self.budget < 30:\n            n_init = min(self.budget, max(6, 2 * self.dim))\n        else:\n            n_init = min(max(10, 4 * self.dim), max(8, self.budget // 8))\n        pts = lhs(int(n_init))\n        for i in range(pts.shape[0]):\n            if evals_left <= 0:\n                break\n            eval_point(pts[i])\n        if evals_left <= 0:\n            self.f_opt = float(f_opt)\n            self.x_opt = np.asarray(x_opt) if x_opt is not None else np.zeros(self.dim)\n            return self.f_opt, self.x_opt\n\n        # helper to get archive arrays\n        def archive_arrays():\n            return (np.asarray(X_archive), np.asarray(F_archive))\n\n        # PCA builder for top points\n        def pca_top(k=None):\n            X, F = archive_arrays()\n            n = X.shape[0]\n            if n < 3:\n                return None, None, None\n            if k is None:\n                k = min(20 + self.dim, n)\n            top_k = min(max(3, k), n)\n            idx = np.argsort(F)[:top_k]\n            Xt = X[idx]\n            center = np.mean(Xt, axis=0)\n            C = Xt - center\n            try:\n                U_svd, s_svd, Vt = np.linalg.svd(C, full_matrices=False)\n                eigvals = (s_svd ** 2) / max(1, (Xt.shape[0] - 1))\n                basis = Vt.T\n                return basis, eigvals, center\n            except Exception:\n                return None, None, None\n\n        # lightweight quadratic/linear fit (diag quadratic fallback)\n        def fit_diag_quadratic(center_idx, neigh_idx):\n            X, F = archive_arrays()\n            if len(neigh_idx) == 0:\n                return None\n            x0 = X[center_idx]\n            Xn = X[neigh_idx]\n            Fn = F[neigh_idx]\n            dx = Xn - x0\n            n = dx.shape[0]\n            P = 1 + self.dim + self.dim  # constant + linear + diag-quadratic\n            ridge = 1e-8\n            if n < P:\n                # linear ridge fallback\n                A = np.concatenate([np.ones((n, 1)), dx], axis=1)\n                dnorm = np.linalg.norm(dx / (span + 1e-12), axis=1)\n                s0 = np.median(dnorm) + 1e-6\n                w = 1.0 / (1.0 + (dnorm / (s0 + 1e-12)) ** 2)\n                W = np.diag(w / (w.sum() + 1e-12))\n                try:\n                    theta = np.linalg.solve(A.T @ W @ A + ridge * np.eye(A.shape[1]), A.T @ W @ Fn)\n                    a = theta[0]; g = theta[1:]; h = np.ones(self.dim) * 1e-6\n                    res = Fn - A @ theta\n                    sigma = np.sqrt(np.sum(w * (res ** 2)) / (w.sum() + 1e-12) + 1e-12)\n                    return a, g, h, sigma\n                except Exception:\n                    return None\n            # quadratic diagonal terms\n            Phi = np.zeros((n, P))\n            Phi[:, 0] = 1.0\n            Phi[:, 1:1 + self.dim] = dx\n            Phi[:, 1 + self.dim:] = 0.5 * (dx ** 2)\n            dnorm = np.linalg.norm(dx / (span + 1e-12), axis=1)\n            s0 = max(1e-3, np.median(dnorm) + 1e-6)\n            w = 1.0 / (1.0 + (dnorm / (s0 + 1e-12)) ** 2)\n            W = np.diag(w / (w.sum() + 1e-12))\n            try:\n                theta = np.linalg.solve(Phi.T @ W @ Phi + ridge * np.eye(P), Phi.T @ W @ Fn)\n                a = theta[0]\n                g = theta[1:1 + self.dim]\n                h = theta[1 + self.dim:]\n                h = np.sign(h) * np.maximum(np.abs(h), 1e-7)\n                res = Fn - Phi @ theta\n                sigma = np.sqrt(np.sum(w * (res ** 2)) / (w.sum() + 1e-12) + 1e-12)\n                return a, g, h, sigma\n            except Exception:\n                return None\n\n        # algorithm configuration\n        K = 4  # kernels: 0=cauchy global,1=PCA local,2=DE-style,3=diag quad\n        scores = np.zeros(K, dtype=float)\n        baseline = np.zeros(K, dtype=float)\n        baseline_lr = 0.08\n        score_lr = 0.9\n        # log-scales initial (inspired by MASH and No.1)\n        log_scales = np.log(np.array([0.9, 0.16, 0.7, 0.35], dtype=float))\n        log_scale_min = np.log(1e-5)\n        log_scale_max = np.log(30.0)\n        logscale_lr = 0.4\n\n        # also maintain simple success/attempt counts for smoothing like No.1\n        kernel_success = np.ones(K, dtype=float) * 1.0\n        kernel_attempts = np.ones(K, dtype=float) * 1.0\n\n        # temperature cooling (multiplicative)\n        temp = 1.0\n        temp_min = 0.05\n        cooling = 0.995\n\n        batch = max(4, 2 * self.dim)\n        iter_no = 0\n        novelty_eps = 1e-9 * np.linalg.norm(span) + 1e-6 * np.mean(span)\n\n        # initial best\n        X, F = archive_arrays()\n        best_idx = int(np.argmin(F))\n        x_best = X[best_idx].copy()\n        f_best = float(F[best_idx])\n\n        # main loop\n        while evals_left > 0:\n            iter_no += 1\n            X, F = archive_arrays()\n            n_archive = X.shape[0]\n            if n_archive == 0:\n                break\n            # PCA info (for kernel 1)\n            basis, eigvals, center = pca_top(k=min(20 + self.dim, n_archive))\n\n            # compute probabilities via softmax of additive scores (tempered)\n            scaled = scores / (np.max(np.abs(scores)) + 1e-12)\n            tau = max(0.12, 0.22 * temp)\n            exp_s = np.exp(scaled / tau)\n            probs = exp_s / (exp_s.sum() + 1e-12)\n            # small Dirichlet jitter for exploration\n            jitter = rng.dirichlet(np.ones(K) * 0.06)\n            probs = 0.88 * probs + 0.12 * jitter\n            probs = probs / (probs.sum() + 1e-12)\n\n            bsize = min(batch, evals_left)\n            for _ in range(bsize):\n                if evals_left <= 0:\n                    break\n                # sample kernel according to probs\n                k = int(rng.choice(K, p=probs))\n                kernel_attempts[k] += 1.0\n\n                kscale = float(np.exp(log_scales[k])) * temp\n\n                x_cand = None\n                internal_candidates = []\n\n                # Kernel 0: Lévy/Cauchy heavy-tailed global jump anchored at best or random elite\n                if k == 0:\n                    if rng.rand() < 0.6:\n                        anchor = x_best.copy()\n                    else:\n                        topk = max(2, min(n_archive, 6))\n                        anchor = X[rng.choice(np.argsort(F)[:topk])]\n                    dirv = rng.randn(self.dim)\n                    dirv /= (np.linalg.norm(dirv) + 1e-12)\n                    step_raw = rng.standard_cauchy()\n                    step_raw = np.clip(step_raw, -10.0, 10.0)\n                    step_len = (0.6 + 1.2 * abs(step_raw)) * 0.5 * kscale\n                    x_cand = anchor + dirv * step_len * span\n\n                # Kernel 1: PCA-guided anisotropic local probe\n                elif k == 1:\n                    if basis is not None and eigvals is not None:\n                        scales_pc = np.power(np.maximum(eigvals, 1e-12), 0.75)\n                        if scales_pc.shape[0] < self.dim:\n                            pad = np.ones(self.dim - scales_pc.shape[0]) * np.mean(scales_pc + 1e-6)\n                            scales_full = np.concatenate([scales_pc, pad])\n                        else:\n                            scales_full = scales_pc[:self.dim]\n                        z = rng.randn(self.dim) * (0.08 * (1.0 + temp) * scales_full / (np.mean(scales_full) + 1e-12))\n                        pc_step = basis @ z\n                        anchor = 0.85 * x_best + 0.15 * (center if center is not None else x_best)\n                        x_cand = anchor + pc_step + rng.randn(self.dim) * (0.015 * span) * temp\n                    else:\n                        x_cand = x_best + rng.randn(self.dim) * (0.09 * span) * temp\n\n                # Kernel 2: DE-like recombination (differential-style moves)\n                elif k == 2:\n                    if n_archive >= 3:\n                        idxs = rng.choice(n_archive, size=3, replace=False)\n                        a, b, c = X[idxs[0]], X[idxs[1]], X[idxs[2]]\n                        # heavy-tailed factor with moderation\n                        step_raw = rng.standard_cauchy()\n                        step_raw = np.clip(step_raw, -8.0, 8.0)\n                        Fscale = 0.45 * (0.4 + rng.rand() * 0.8) * (0.6 + 0.4 * (1.0 / (1.0 + abs(step_raw))))\n                        donor = a + Fscale * (b - c)\n                        cr = 0.25 + 0.5 * rng.rand()\n                        mask = rng.rand(self.dim) < cr\n                        trial = x_best.copy()\n                        trial[mask] = donor[mask]\n                        trial += rng.randn(self.dim) * (0.02 * span) * temp\n                        x_cand = trial\n                    else:\n                        x_cand = x_best + rng.randn(self.dim) * (0.2 * span) * temp\n\n                # Kernel 3: Cauchy-weighted diagonal quadratic / linear surrogate descent\n                else:\n                    topk = min(max(4, 3 + self.dim), n_archive)\n                    top_idx = np.argsort(F)[:topk]\n                    if rng.rand() < 0.85:\n                        center_idx = int(top_idx[0])\n                    else:\n                        center_idx = int(top_idx[rng.randint(0, topk)])\n                    Xarr = X\n                    dnorm = np.linalg.norm((Xarr - Xarr[center_idx]) / (span + 1e-12), axis=1)\n                    neigh_idx = np.argsort(dnorm)[:min(max(6, 2 * self.dim), n_archive)]\n                    fit = fit_diag_quadratic(center_idx, neigh_idx)\n                    if fit is None:\n                        x_cand = Xarr[center_idx] + rng.randn(self.dim) * (0.07 * span) * temp\n                    else:\n                        a, g, h, sigma = fit\n                        dx_star = - g / (h + 1e-12)\n                        local_spread = np.mean(np.std(Xarr[neigh_idx], axis=0)) if len(neigh_idx) > 1 else np.mean(span) * 0.18\n                        max_step = local_spread * (1.35 + 0.5 * temp)\n                        dx_star = np.clip(dx_star, -max_step, max_step)\n                        base_prop = Xarr[center_idx] + dx_star * (0.95 * kscale)\n                        x_cand = base_prop + rng.randn(self.dim) * (0.011 * span) * (1.0 + sigma) * temp\n                        # internal 1D probes (at most 2 extra evals)\n                        if np.linalg.norm(dx_star) > 1e-9 and evals_left > 2:\n                            dir1 = dx_star / (np.linalg.norm(dx_star) + 1e-12)\n                            offs = np.array([-0.5, 0.5]) * np.linalg.norm(dx_star)\n                            for off in offs:\n                                if evals_left <= 0:\n                                    break\n                                xi = Xarr[center_idx] + dir1 * off\n                                internal_candidates.append(xi)\n\n                # occasional small jitter\n                if rng.rand() < 0.18:\n                    jitter = (rng.rand(self.dim) - 0.5) * 2.0 * (0.06 * (0.97 ** iter_no)) * span * (0.6 + 0.8 * temp)\n                    x_cand = x_cand + jitter\n\n                # novelty push when too close to archive\n                if len(X_archive) > 0:\n                    dists = np.linalg.norm(np.asarray(X_archive) - x_cand, axis=1)\n                    if dists.size > 0 and dists.min() < novelty_eps:\n                        idx_near = int(np.argmin(dists))\n                        push_dir = x_cand - X_archive[idx_near]\n                        if np.linalg.norm(push_dir) < 1e-12:\n                            push_dir = rng.randn(self.dim)\n                        push_dir /= (np.linalg.norm(push_dir) + 1e-12)\n                        x_cand = np.clip(x_cand + push_dir * (novelty_eps * 30.0), lb, ub)\n\n                # evaluate candidate\n                try:\n                    f_new, x_new = eval_point(x_cand)\n                except RuntimeError:\n                    break\n\n                # evaluate internal probes if any\n                internal_fs = []\n                for xi in internal_candidates:\n                    if evals_left <= 0:\n                        break\n                    xi = np.clip(np.asarray(xi, dtype=float).reshape(self.dim), lb, ub)\n                    fi, _ = eval_point(xi)\n                    internal_fs.append(fi)\n\n                # compute reward (improvement over current f_best at generation time)\n                reward = max(0.0, f_best - f_new) / (abs(f_best) + 1.0)\n                reward = min(reward, 1.0)\n\n                # update additive bandit score and baseline\n                delta = (reward - baseline[k]) * score_lr\n                scores[k] += delta\n                baseline[k] = (1.0 - baseline_lr) * baseline[k] + baseline_lr * reward\n\n                # update log-scale in log-domain (positive reward increases scale)\n                if reward > 0:\n                    log_scales[k] = min(log_scale_max, log_scales[k] + logscale_lr * reward)\n                    kernel_success[k] += 1.0\n                else:\n                    log_scales[k] = max(log_scale_min, log_scales[k] - 0.02 * logscale_lr)\n\n                # small smoothing/decay of scores\n                scores = 0.995 * scores\n\n                # update local best references\n                X, F = archive_arrays()\n                if X.shape[0] > 0:\n                    best_idx = int(np.argmin(F))\n                    x_best = X[best_idx].copy()\n                    f_best = float(F[best_idx])\n\n                if evals_left <= 0:\n                    break\n\n            # end batch: cooling and smoothing attempts\n            temp = max(temp_min, temp * cooling)\n            kernel_attempts = 0.96 * kernel_attempts + 0.04 * np.ones_like(kernel_attempts)\n            kernel_success = 0.92 * kernel_success + 0.08 * (kernel_attempts * (kernel_success / (kernel_attempts + 1e-12)))\n\n            # prune archive occasionally to control memory (keep best + diverse)\n            if len(X_archive) > max(400, 8 * max(40, 5 * self.dim)):\n                # greedy k-center preserving best half\n                max_keep = max(40, 5 * self.dim)\n                Xa = np.asarray(X_archive)\n                Fa = np.asarray(F_archive)\n                keep_top = int(0.5 * max_keep)\n                idx_best = np.argsort(Fa)[:keep_top].tolist()\n                remaining = list(set(range(Xa.shape[0])) - set(idx_best))\n                Xnorm = (Xa - lb) / (span + 1e-12)\n                selected = idx_best.copy()\n                if len(selected) == 0:\n                    selected = [int(np.argmin(Fa))]\n                rem = remaining[:]\n                while len(selected) < max_keep and len(rem) > 0:\n                    dmin = np.min(np.linalg.norm(Xnorm[rem][:, None, :] - Xnorm[selected][None, :, :], axis=2), axis=1)\n                    chosen = rem.pop(int(np.argmax(dmin)))\n                    selected.append(chosen)\n                keep_set = set(selected[:max_keep])\n                X_archive = [X_archive[i] for i in range(len(X_archive)) if i in keep_set]\n                F_archive = [F_archive[i] for i in range(len(F_archive)) if i in keep_set]\n\n        # finalize\n        if x_opt is None and len(F_archive) > 0:\n            idx = int(np.argmin(F_archive))\n            f_opt = float(F_archive[idx])\n            x_opt = np.asarray(X_archive[idx]).copy()\n\n        self.f_opt = float(f_opt)\n        self.x_opt = np.asarray(x_opt) if x_opt is not None else np.zeros(self.dim)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridMASHLevy scored 0.575 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "7c650f3e-923b-4fbe-b23d-92337d4aeb36", "operator": null, "metadata": {"aucs": [0.17210639128215977, 0.17197559677026342, 0.7780440675881395, 0.973770008008761, 0.9497305214029064, 0.9604293105973565, 0.3122392773836662, 0.5280610781976045, 0.7476858178591017, 0.15513385923424194]}, "task_prompt": ""}, "log_dir": "run-MCTS_AHD-MA_BBOB-2", "seed": 2}
{"method_name": "MCTS_AHD", "problem_name": "MA_BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "MCTS_AHD", "budget": 200, "kwargs": {"lambda_0": 0.1, "alpha": 0.5, "maximisation": true, "max_children": 5, "expansion_factor": 2}}, "problem": {"name": "MA_BBOB", "dims": [10], "training_instances": "range(0, 10)", "test_instances": "range(20, 120)", "budget_factor": 5000}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "b65d243f-8f7c-4374-b524-d99c0fff495c", "fitness": 0.5613549265503889, "name": "HybridDirectionalMultiTrustLevy", "description": "The algorithm uses a small Halton low-discrepancy initialization (init_frac=0.06, min_init≈10) and an adaptive ensemble of pop_regions centers (set by dimension) to cover the search space. Each center builds a local spherical sample cloud (cloud_min ≈ max(6,3*dim)) and fits a linear least-squares gradient to propose directional trust-region steps with rho-based radius adaptation (eta_step, r_min/r_max clamps) and lightweight regularization/fallbacks. To balance local exploitation and global structure it intermittently does coordinate polishing, principal-direction probes computed from the best samples (PCA-like) and assimilates good probes into the worst center. Robust stagnation escapes use heavy-tailed moves (Cauchy/Levy long jumps), targeted reseeding near the global best, and multiple budget-aware safeguards and clipping to respect bounds and the evaluation budget.", "code": "import numpy as np\n\nclass HybridDirectionalMultiTrustLevy:\n    \"\"\"\n    HybridDirectionalMultiTrustLevy (HD-MTL)\n\n    Multi-center directional trust-region ensemble:\n    - Halton-seeded initial coverage\n    - Per-center spherical-cloud sampling + linear least-squares gradient estimate\n    - Directional trust-region steps with rho-based radius adaptation\n    - Coordinate polishing, principal-direction global probes\n    - Coordinated heavy-tailed Cauchy/Levy restarts for stagnation escape\n    - Budget-aware, bound-clipped, robust fallbacks\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, pop_regions=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = None if seed is None else int(seed)\n        self.rng = np.random.RandomState(seed)\n        # number of concurrent trust regions\n        self.pop_regions = pop_regions if pop_regions is not None else max(3, min(8, self.dim // 2 + 1))\n        # internal hyperparameters\n        self.init_frac = 0.06\n        self.min_init = max(12, 8)\n        self.eta_step = 0.9\n        self.cloud_min = max(6, 3 * self.dim)\n        self.coord_period = 4\n        self.stagnation_thresh = 5\n\n    # --- Halton utilities for low-discrepancy init ---\n    def _van_der_corput(self, n, base):\n        v = 0.0\n        denom = 1.0\n        while n > 0:\n            n, rem = divmod(n, base)\n            denom *= base\n            v += rem / denom\n        return v\n\n    def _halton(self, idx):\n        primes = [2,3,5,7,11,13,17,19,23,29,31,37]\n        out = np.empty(self.dim, dtype=float)\n        for d in range(self.dim):\n            base = primes[d % len(primes)]\n            out[d] = self._van_der_corput(idx + 1, base)\n        return out\n\n    def _get_bounds(self, func):\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(self.dim, float(lb))\n                ub = np.full(self.dim, float(ub))\n        except Exception:\n            lb = -5.0 * np.ones(self.dim)\n            ub = 5.0 * np.ones(self.dim)\n        return lb, ub\n\n    def _clip(self, x, lb, ub):\n        return np.minimum(np.maximum(x, lb), ub)\n\n    def _levy_step(self, beta=1.5, scale=1.0):\n        # Mantegna's approximate Levy step\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = self.rng.randn(self.dim) * sigma_u\n        v = self.rng.randn(self.dim)\n        step = u / (np.abs(v) ** (1 / beta))\n        norm = np.linalg.norm(step) + 1e-12\n        return scale * step / (norm ** (1.0 / self.dim))\n\n    def __call__(self, func):\n        lb, ub = self._get_bounds(func)\n        span = ub - lb\n        span_mean = float(np.mean(span))\n        budget = int(self.budget)\n        evals = 0\n\n        # global archive\n        all_X = []\n        all_F = []\n\n        # best-so-far\n        f_opt = np.inf\n        x_opt = None\n\n        # --- Halton initial coverage (budget-aware) ---\n        n_init = int(min(max(self.min_init, int(self.init_frac * budget)), max(10, budget // 4)))\n        n_init = max(10, n_init)\n        n_init = min(n_init, budget)\n        for i in range(n_init):\n            h = self._halton(i)\n            x = lb + h * (ub - lb)\n            f = float(func(x))\n            evals += 1\n            all_X.append(x.copy()); all_F.append(f)\n            if f < f_opt:\n                f_opt = f; x_opt = x.copy()\n            if evals >= budget:\n                return f_opt, x_opt\n\n        # prepare centers from best initial points\n        arrX = np.array(all_X)\n        arrF = np.array(all_F)\n        order = np.argsort(arrF)\n        num_centers = min(self.pop_regions, len(order))\n        centers = []\n        for idx in order[:num_centers]:\n            x = arrX[idx].copy()\n            f = float(arrF[idx])\n            r0 = max(0.25 * span_mean, 0.2)\n            centers.append({'x': x.copy(), 'f': f, 'r': float(r0), 'age': 0, 'fail': 0, 'succ': 0})\n\n        # fill up remaining centers by perturbing global best\n        while len(centers) < self.pop_regions and evals < budget:\n            base = x_opt.copy() if x_opt is not None else lb + 0.5 * span\n            x = self._clip(base + 0.2 * span_mean * self.rng.randn(self.dim), lb, ub)\n            f = float(func(x)); evals += 1\n            all_X.append(x.copy()); all_F.append(f)\n            centers.append({'x': x.copy(), 'f': f, 'r': max(0.25 * span_mean, 0.1), 'age':0, 'fail':0, 'succ':0})\n            if f < f_opt:\n                f_opt = f; x_opt = x.copy()\n\n        # parameters\n        r_min = 1e-6 * max(1.0, span_mean)\n        r_max = max(span_mean * 2.0, 2.0)\n        eta = self.eta_step\n        coord_period = self.coord_period\n\n        iter_count = 0\n        global_stag = 0\n\n        # main loop: rotate through centers until budget exhausted\n        center_idx = 0\n        while evals < budget:\n            iter_count += 1\n            # pick center in round-robin with slight bias to better centers\n            center_idx = center_idx % len(centers)\n            c = centers[center_idx]\n            center_idx += 1\n            c['age'] += 1\n\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n\n            # choose cloud size proportional but budget aware\n            m = int(min(self.cloud_min, max(3, remaining - 1)))\n            if c['r'] <= r_min * 10:\n                m = max(3, min(m, 6))\n            m = min(m, remaining - 1) if remaining > 1 else 0\n            improved_this_center = False\n\n            # sample local spherical cloud for model\n            if m > 0:\n                # generate directions\n                D = self.rng.randn(m, self.dim)\n                norms = np.linalg.norm(D, axis=1, keepdims=True)\n                norms = np.maximum(norms, 1e-12)\n                D = D / norms\n                radii = (self.rng.rand(m,1) ** (1.0 / max(1.0, self.dim)))\n                D = D * radii * c['r']\n                Xs = c['x'].reshape(1,-1) + D\n                for i in range(m):\n                    Xs[i] = self._clip(Xs[i], lb, ub)\n\n                Fs = np.empty(m, dtype=float)\n                # evaluate cloud; use the center f before cloud evaluations for modeling\n                f_center_for_model = c['f']\n                for i in range(m):\n                    if evals >= budget:\n                        break\n                    xi = Xs[i].copy()\n                    fi = float(func(xi))\n                    Fs[i] = fi\n                    evals += 1\n                    all_X.append(xi.copy()); all_F.append(fi)\n                    # immediate accept if better than center\n                    if fi < c['f']:\n                        c['x'] = xi.copy(); c['f'] = fi\n                        c['succ'] += 1; c['fail'] = 0\n                        improved_this_center = True\n                        if fi < f_opt:\n                            f_opt = fi; x_opt = xi.copy()\n                    else:\n                        c['fail'] += 1\n                    if evals >= budget:\n                        break\n\n                if evals >= budget:\n                    break\n\n                # Build linear least-squares: A g = b where A = D and b = Fs - f_center_for_model\n                # Use only samples that were evaluated\n                try:\n                    b = Fs - f_center_for_model\n                    A = D\n                    # regularize slightly\n                    g, *_ = np.linalg.lstsq(A, b, rcond=None)\n                    g = g.flatten()\n                except Exception:\n                    slopes = (Fs - f_center_for_model) / (np.linalg.norm(D, axis=1) + 1e-12)\n                    g = -np.sum((D.T * slopes), axis=1)\n                # fallback if degenerate\n                gnorm = np.linalg.norm(g)\n                if gnorm < 1e-12 or np.isnan(gnorm):\n                    direction = self.rng.randn(self.dim)\n                    direction /= (np.linalg.norm(direction) + 1e-12)\n                else:\n                    direction = -g / (gnorm + 1e-12)\n\n                # propose trust-region trial along direction\n                step_len = eta * c['r']\n                s = step_len * direction\n                x_trial = self._clip(c['x'] + s, lb, ub)\n                if evals < budget:\n                    f_trial = float(func(x_trial)); evals += 1\n                    all_X.append(x_trial.copy()); all_F.append(f_trial)\n                    # predicted reduction and actual\n                    pred = np.dot(g, s) if gnorm > 1e-12 else -0.0\n                    actual = c['f'] - f_trial\n                    rho = None\n                    if pred < 0:\n                        rho = actual / (-pred)\n                    else:\n                        rho = 1.0 if actual > 0 else 0.0\n\n                    tol = 1e-12\n                    if f_trial < c['f'] - tol:\n                        # accept\n                        c['x'] = x_trial.copy(); c['f'] = f_trial\n                        c['succ'] += 1; c['fail'] = 0\n                        improved_this_center = True\n                        # radius adaptation (aggressive on good model)\n                        if rho is not None:\n                            if rho > 0.75:\n                                c['r'] = min(c['r'] * 1.6, r_max)\n                            elif rho > 0.25:\n                                c['r'] = min(c['r'] * 1.2, r_max)\n                            else:\n                                c['r'] = max(c['r'] * 0.9, r_min)\n                        else:\n                            c['r'] = min(c['r'] * 1.1, r_max)\n                    else:\n                        # reject\n                        c['fail'] += 1\n                        c['r'] = max(c['r'] * 0.6, r_min)\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial; x_opt = x_trial.copy()\n\n            # coordinate polishing occasionally for center\n            if (c['age'] % coord_period == 0) and (evals < budget):\n                sub_dim = min(self.dim, max(2, self.dim // 4))\n                idxs = self.rng.choice(self.dim, sub_dim, replace=False)\n                for idx in idxs:\n                    if evals >= budget:\n                        break\n                    for sign in (+1, -1):\n                        step = np.zeros(self.dim)\n                        step[idx] = 0.5 * c['r'] * sign\n                        x_cand = self._clip(c['x'] + step, lb, ub)\n                        f_cand = float(func(x_cand)); evals += 1\n                        all_X.append(x_cand.copy()); all_F.append(f_cand)\n                        if f_cand < c['f'] - 1e-12:\n                            c['x'] = x_cand.copy(); c['f'] = f_cand\n                            c['succ'] += 1; c['fail'] = 0\n                            c['r'] = min(c['r'] * 1.1, r_max)\n                            if f_cand < f_opt:\n                                f_opt = f_cand; x_opt = x_cand.copy()\n                        else:\n                            c['fail'] += 1\n                        if evals >= budget:\n                            break\n\n            # principal-direction global probes occasionally to exploit structure\n            if iter_count % max(5, 6 - self.dim // 10) == 0 and evals < budget and len(all_F) >= max(6, self.dim + 2):\n                arrX = np.array(all_X)\n                arrF = np.array(all_F)\n                topk = min(max(8, self.dim + 2), len(arrF))\n                idxs = np.argsort(arrF)[:topk]\n                subset = arrX[idxs]\n                mean = subset.mean(axis=0)\n                centered = subset - mean\n                try:\n                    cov = np.cov(centered, rowvar=False)\n                    w, v = np.linalg.eigh(cov + np.eye(self.dim) * 1e-8)\n                    dir_vec = v[:, np.argmax(w)]\n                    dir_vec = dir_vec / (np.linalg.norm(dir_vec) + 1e-12)\n                except Exception:\n                    dir_vec = self.rng.randn(self.dim)\n                    dir_vec /= (np.linalg.norm(dir_vec) + 1e-12)\n                base = x_opt.copy() if x_opt is not None else mean\n                scales = [0.5, 1.0, 2.0]\n                for sc in scales:\n                    if evals >= budget:\n                        break\n                    for sign in (+1, -1):\n                        x = base + sign * sc * (0.8 * span_mean) * dir_vec * (min(1.0, np.mean([cc['r'] for cc in centers]) / (span_mean + 1e-12)))\n                        x = self._clip(x, lb, ub)\n                        f = float(func(x)); evals += 1\n                        all_X.append(x.copy()); all_F.append(f)\n                        if f < f_opt:\n                            f_opt = f; x_opt = x.copy()\n                        # assimilate good probe into worst center\n                        worst = max(centers, key=lambda cc: cc['f'])\n                        if f < worst['f']:\n                            worst['x'] = x.copy(); worst['f'] = f; worst['r'] = max(worst['r'] * 0.6, r_min); worst['fail'] = 0\n                        if evals >= budget:\n                            break\n\n            # Cauchy/Levy long jumps for stagnating center\n            if c['fail'] >= self.stagnation_thresh and evals < budget:\n                n_jumps = int(min(6, max(2, (budget - evals) // 20)))\n                jumped = False\n                for _ in range(n_jumps):\n                    if evals >= budget:\n                        break\n                    cauchy = self.rng.standard_cauchy()\n                    direction = self.rng.randn(self.dim)\n                    direction /= (np.linalg.norm(direction) + 1e-12)\n                    jump_len = (1.5 + abs(cauchy)) * c['r'] * (1.0 + 0.5 * self.rng.rand())\n                    x_jump = self._clip(c['x'] + jump_len * direction, lb, ub)\n                    f_jump = float(func(x_jump)); evals += 1\n                    all_X.append(x_jump.copy()); all_F.append(f_jump)\n                    if f_jump < c['f'] - 1e-12:\n                        c['x'] = x_jump.copy(); c['f'] = f_jump; c['r'] = min(c['r'] * 1.2, r_max)\n                        c['fail'] = 0; c['succ'] += 1\n                        jumped = True\n                        if f_jump < f_opt:\n                            f_opt = f_jump; x_opt = x_jump.copy()\n                        break\n                if not jumped:\n                    # relax radius a bit to explore more\n                    c['r'] = min(c['r'] * 1.5, r_max)\n                    c['fail'] = 0\n\n            # if center stagnated for very long, reseed near global best\n            if c['fail'] >= 12 and evals < budget:\n                for _ in range(3):\n                    if evals >= budget:\n                        break\n                    scale = min(max(0.2 * (c['r'] / (span_mean + 1e-12)), 0.05), 0.5)\n                    x_new = (x_opt.copy() if x_opt is not None else lb + 0.5 * span) + scale * span_mean * self.rng.randn(self.dim)\n                    x_new = self._clip(x_new, lb, ub)\n                    f_new = float(func(x_new)); evals += 1\n                    all_X.append(x_new.copy()); all_F.append(f_new)\n                    if f_new < c['f']:\n                        c['x'] = x_new.copy(); c['f'] = f_new; c['fail'] = 0; c['succ'] = 0\n                    if f_new < f_opt:\n                        f_opt = f_new; x_opt = x_new.copy()\n                c['r'] = max(min(c['r'] * 0.8, r_max), r_min)\n                c['fail'] = 0\n\n            # small global Levy reboot if entire system stagnating\n            # measure global stagnation as many centers having recent failures\n            num_bad = sum(1 for cc in centers if cc['fail'] >= self.stagnation_thresh)\n            if num_bad >= max(1, len(centers) // 2) and evals < budget:\n                reboot_budget = min(6 + self.dim, max(4, (budget - evals) // 10))\n                for r in range(reboot_budget):\n                    if evals >= budget:\n                        break\n                    step = self._levy_step(beta=1.5, scale=min(r_max, 0.8 * span_mean * (1 + r * 0.5)))\n                    x = self._clip((x_opt if x_opt is not None else lb + 0.5 * span) + step, lb, ub)\n                    f = float(func(x)); evals += 1\n                    all_X.append(x.copy()); all_F.append(f)\n                    if f < f_opt:\n                        f_opt = f; x_opt = x.copy()\n                    # assimilate good reboot into worst center\n                    worst = max(centers, key=lambda cc: cc['f'])\n                    if f < worst['f']:\n                        worst['x'] = x.copy(); worst['f'] = f; worst['r'] = max(worst['r'] * 0.8, r_min); worst['fail'] = 0\n\n            # clamp radii\n            for cc in centers:\n                cc['r'] = float(np.clip(cc['r'], r_min, r_max))\n\n            # small check to increment global stagnation\n            prev_best = np.min(all_F) if len(all_F) else f_opt\n            if f_opt < prev_best - 1e-12:\n                global_stag = 0\n            else:\n                global_stag += 1\n\n        return float(f_opt), (None if x_opt is None else x_opt.copy())", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDirectionalMultiTrustLevy scored 0.561 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e0bdd62d-523a-4b1b-849e-5e5c4dd0f4dc", "operator": null, "metadata": {"aucs": [0.2172980707507245, 0.16875174539191273, 0.4354526702451528, 0.8783860488954328, 0.9019469878079355, 0.9075508257029429, 0.3045022673305632, 0.7434190891861001, 0.8859927813081592, 0.1702487788849656]}, "task_prompt": ""}, "log_dir": "run-MCTS_AHD-MA_BBOB-1", "seed": 1}

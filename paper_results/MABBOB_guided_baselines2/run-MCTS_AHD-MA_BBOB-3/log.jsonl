{"id": "cdb41671-1583-49d0-ae74-4bd81b978104", "fitness": 0.3254473308775983, "name": "ARDCE", "description": "The ARDCE algorithm is a hybrid CMA-ES–style optimizer: it uses a population (lambda, mu chosen like CMA-ES), log-based recombination weights, evolution paths (ps, pc), rank-one and rank-mu covariance updates with learning rates (cc, cs, c1, cmu, damps) and sigma adaptation using the expected norm chi_n to control step-size. Sampling is performed by transforming standard normals with the eigendecomposition factors B and D (approximate sqrt(C)), with periodic eigen-recomputation (eig_every) and numerical safeguards (non-negative eigenvalues, LinAlgError fallback, lower bound on sigma). To boost exploration and affine robustness an archive of evaluated points is kept and occasional DE-style difference mutations (probability p_de, factor F_de) are added to offspring, and all candidates are clipped to user-specified bounds. Initialization choices (mean uniformly in bounds, sigma = 0.3·mean(range)), strict budget-aware evaluation counting, and maintaining the best-so-far (x_opt, f_opt) complete the design for reliable, bounded-budget continuous optimization.", "code": "import numpy as np\n\nclass ARDCE:\n    \"\"\"\n    Hybrid Adaptive Rotational Differential Covariance Estimation (ARDCE)\n    A CMA-ES style covariance adaptation with occasional differential-evolution\n    style difference mutations for added exploration robustness.\n    One-line: CMA-ES-like adaptive covariance + DE-inspired mutations for affine-robust continuous optimization.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # default population size similar to CMA-ES recommendation\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = self.budget\n\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # ensure shapes broadcast\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # Strategy parameters (CMA-ES inspired)\n        lam = self.lambda_\n        mu = self.mu\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # adaptation constants\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        c1 = 2 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1 - c1, 2 * (mu_eff - 2 + 1 / mu_eff) / ((n + 2) ** 2 + mu_eff))\n        damps = 1 + 2 * max(0, np.sqrt((mu_eff - 1) / (n + 1)) - 1) + cs\n\n        # expectation of ||N(0,I)||\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize dynamic state\n        m = np.random.uniform(lb, ub)  # initial mean in bounds\n        sigma = 0.3 * np.mean(ub - lb)  # initial step-size\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eigen_eval_counter = 0\n        eig_every = max(1, int(10 * n))  # recompute eigendecomposition occasionally\n\n        # maintain archive of evaluated points for DE-style differences\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of m (optional but useful)\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            if fm < f_opt:\n                f_opt = fm\n                x_opt = xm.copy()\n\n        # main loop\n        while evals < budget:\n            # decide how many offspring to generate this generation\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # generate candidates\n            arz = np.random.randn(current_lambda, n)  # standard normal\n            ary = np.dot(arz, (B * D).T)  # y = B*D*z  (approx sampling from N(0,C))\n            arx = m + sigma * ary\n            # occasionally apply DE-style difference mutation (keeps affine exploration)\n            p_de = 0.25\n            F_de = 0.8\n            for k in range(current_lambda):\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    # choose two random distinct archived vectors\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    arx[k] = arx[k] + de_mut\n                # clip to bounds\n                arx[k] = np.clip(arx[k], lb, ub)\n\n            # Evaluate candidates\n            arfit = np.zeros(current_lambda)\n            for k in range(current_lambda):\n                x = arx[k]\n                f = func(x)\n                evals += 1\n                arfit[k] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n                if evals >= budget:\n                    # we must stop after reaching budget\n                    break\n\n            # selection and recombination\n            idx = np.argsort(arfit)\n            # get top mu vectors\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)  # shape (mu, n)\n            m_old = m.copy()\n            m = np.sum(weights[:, np.newaxis] * x_sel, axis=0)\n\n            # update evolution paths\n            y_w = np.sum(weights[:, np.newaxis] * y_sel, axis=0)  # weighted mean step\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n            norm_ps = np.linalg.norm(ps)\n            # hsig\n            hsig = 1.0 if (norm_ps / np.sqrt(1 - (1 - cs) ** (2 * (evals / current_lambda + 1))) / chi_n) < (1.4 + 2 / (n + 1)) else 0.0\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n\n            # adapt covariance matrix C\n            # rank-one update\n            rank_one = np.outer(pc, pc)\n            # rank-mu update\n            rank_mu = np.zeros((n, n))\n            for i in range(mu):\n                yi = y_sel[i][:, None]\n                rank_mu += weights[i] * (yi @ yi.T)\n            C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n\n            # adapt step-size sigma\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1))\n\n            # recompute B and D from C every eig_every evaluations (approx)\n            eigen_eval_counter += current_lambda\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                # ensure symmetry\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    # numerical stability: non-negative\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    # fallback to identity\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # small safeguard: if sigma is too small, increase\n            if sigma < 1e-12:\n                sigma = 1e-12\n\n            # If budget exhausted, break (loop condition handles it)\n            if evals >= budget:\n                break\n\n        # final return best found\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ARDCE scored 0.725 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "80014b04-8d18-4853-8f88-dd939ade8f97", "operator": null, "metadata": {"aucs": [0.28826235218145924, 0.8282870727367804, 0.8098979762344413, 0.9875256728519688, 0.8744507448084525, 0.8212246218846051, 0.36654567678821526, 0.6375089156840716, 0.732487976806662, 0.9082822987993264]}, "task_prompt": ""}
{"id": "1170b68b-fa6d-49c9-986c-c4c18a103838", "fitness": 0.38360476015012795, "name": "ARSS", "description": "The algorithm alternates global exploration in small random affine subspaces (subspace dimension k ≈ sqrt(n), built by QR orthonormalization) with intensification via short 1‑D line searches (golden‑section, limited to a few evals) along promising probe directions; each probe samples coefficients in the k‑dimensional subspace and scales moves by an adaptive step (initially 0.4·domain, min_step tiny). It keeps a small LRU memory of successful unit directions (memory_size, reused to bias future subspaces), performs occasional local polishing after improvements, and applies multiplicative step adaptation (grow ~1.12–1.15 on success, shrink 0.85 on failure) plus occasional focused short line searches. Budget and bounds are strictly enforced via safe_eval and clipping, and stagnation triggers randomized restarts around the best with enlarged step or final polishing when restarts are exhausted; probes per round are moderate (probes = max(4, 2·k)) and line searches are cheap (max_evals ~ 8–12) to preserve budget. Overall design trades concentrated local 1‑D optimization and memory‑guided searches inside small random subspaces for robust, budget‑aware global exploration.", "code": "import numpy as np\n\nclass ARSS:\n    \"\"\"\n    Adaptive Random Subspace Search (ARSS)\n    Main idea: iterate by optimizing along small random subspaces (k ≈ sqrt(n)),\n    performing a few cheap directional probes and short 1D line-searches along\n    promising directions. Adapt step-size depending on success, keep a small\n    memory of good directions, and do occasional restarts around the best point.\n    This structure is quite different from CMA-ES or DE: it alternates\n    subspace probing + short line-search + adaptive trust-region style radius.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, memory_size=8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # memory of successful directions (for guided sampling)\n        self.memory_size = int(memory_size)\n\n    def __call__(self, func):\n        n = self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initialize\n        evals = 0\n        # start at a random point inside bounds\n        x_cur = np.random.uniform(lb, ub)\n        f_cur = func(x_cur); evals += 1\n        x_best = x_cur.copy()\n        f_best = float(f_cur)\n\n        # initial step-size: sizeable fraction of domain\n        step = 0.4 * np.mean(ub - lb)\n        min_step = 1e-8 * np.maximum(1.0, np.mean(ub - lb))\n        # subspace dimension ~ sqrt(n), at least 1\n        k = max(1, int(np.ceil(np.sqrt(n))))\n        # number of random probe directions per subspace\n        probes = max(4, 2 * k)\n        # memory of good directions (unit vectors in R^n)\n        dir_memory = []\n\n        # stagnation control and restart parameters\n        no_improve = 0\n        stagnation_limit = max(10, int(10 * np.log(1 + n)))\n        max_restarts = 5\n        restarts = 0\n\n        # helper: clip to bounds\n        def clip_to_bounds(x):\n            return np.clip(x, lb, ub)\n\n        # helper: safe function evaluation that respects budget\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(x)\n            if evals >= self.budget:\n                # budget exhausted; return current best without calling func\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                # in case of evaluation error, return inf\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # small 1D golden-section search along direction d starting at x0\n        # uses only remaining budget-limited evaluations\n        def line_search(x0, f0, d, init_step=1.0, max_evals=12):\n            # normalize direction\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            # bracket search: try +init_step and -init_step\n            alpha0 = 0.0\n            fa = f0\n            remaining = max(0, self.budget - evals)\n            if remaining <= 0:\n                return None, None\n            # Evaluate positive direction\n            b_alpha = init_step\n            xb = clip_to_bounds(x0 + b_alpha * d)\n            fb, _ = safe_eval(xb)\n            if fb is None:\n                return None, None\n            remaining -= 1\n            # If worse, try negative direction\n            if fb >= fa:\n                b_alpha = -init_step\n                xb = clip_to_bounds(x0 + b_alpha * d)\n                fb2, _ = safe_eval(xb)\n                if fb2 is None:\n                    return None, None\n                remaining -= 1\n                if fb2 >= fa:\n                    # no improvement both sides\n                    return None, None\n                else:\n                    fb = fb2\n            # Now we have a bracket [alpha0, b_alpha] with fb < fa\n            # Expand bracket a bit to attempt to enclose a minima\n            expansion = 1.5\n            max_expand = 5\n            expand_count = 0\n            while remaining > 0 and expand_count < max_expand:\n                new_alpha = b_alpha * expansion\n                x_new = clip_to_bounds(x0 + new_alpha * d)\n                f_new, _ = safe_eval(x_new)\n                if f_new is None:\n                    return None, None\n                remaining -= 1\n                if f_new < fb:\n                    # move bracket outward\n                    b_alpha = new_alpha\n                    fb = f_new\n                    expand_count += 1\n                    continue\n                else:\n                    # bracket [alpha0, b_alpha] seems to contain minimum near b_alpha\n                    break\n\n            # golden section between 0 and b_alpha\n            # ensure we have at least 1 eval left\n            if remaining <= 0:\n                # return best seen along the bracket (either x0 or xb)\n                if fb < fa:\n                    return fb, clip_to_bounds(x0 + b_alpha * d)\n                return None, None\n\n            gr = (np.sqrt(5) - 1) / 2  # golden ratio fraction\n            a = alpha0\n            b = b_alpha\n            # initialize interior points\n            c = b - gr * (b - a)\n            d_alpha = a + gr * (b - a)\n            xc = clip_to_bounds(x0 + c * d)\n            dc_f, _ = safe_eval(xc)\n            if dc_f is None:\n                return None, None\n            remaining -= 1\n            xd = clip_to_bounds(x0 + d_alpha * d)\n            dd_f, _ = safe_eval(xd)\n            if dd_f is None:\n                return None, None\n            remaining -= 1\n\n            iters = 0\n            max_iters = max_evals\n            best_f = fa\n            best_x = x0.copy()\n            if dc_f < best_f:\n                best_f = dc_f; best_x = xc.copy()\n            if dd_f < best_f:\n                best_f = dd_f; best_x = xd.copy()\n\n            while remaining > 0 and iters < max_iters and abs(b - a) > 1e-12:\n                iters += 1\n                if dc_f < dd_f:\n                    b = d_alpha\n                    d_alpha = c\n                    dd_f = dc_f\n                    d_alpha = a + gr * (b - a)\n                    x_new = clip_to_bounds(x0 + d_alpha * d)\n                    dd_f, _ = safe_eval(x_new)\n                    if dd_f is None:\n                        break\n                    remaining -= 1\n                    if dd_f < best_f:\n                        best_f = dd_f; best_x = x_new.copy()\n                else:\n                    a = c\n                    c = d_alpha\n                    dc_f = dd_f\n                    c = b - gr * (b - a)\n                    x_new = clip_to_bounds(x0 + c * d)\n                    dc_f, _ = safe_eval(x_new)\n                    if dc_f is None:\n                        break\n                    remaining -= 1\n                    if dc_f < best_f:\n                        best_f = dc_f; best_x = x_new.copy()\n\n            if best_f < f0:\n                return best_f, best_x\n            return None, None\n\n        # Main loop\n        while evals < self.budget:\n            # adapt subspace dimension occasionally\n            k = max(1, int(np.clip(int(np.ceil(np.sqrt(n))), 1, n)))\n            probes = max(4, 2 * k)\n\n            improved = False\n\n            # Create a random orthonormal basis of size k embedded in R^n\n            # If we have helpful memory directions, reuse some\n            use_mem = min(len(dir_memory), k // 2)\n            basis = np.zeros((n, 0))\n            if use_mem > 0:\n                # pick top memory directions\n                mem_idx = np.argsort([-1.0 * i for i in range(len(dir_memory))])  # trivial order\n                mem_idx = np.arange(use_mem)\n                mem_sel = np.array(dir_memory)[mem_idx]\n                basis = np.column_stack((basis, mem_sel.T))\n\n            # supplement with random directions and orthonormalize\n            needed = k - basis.shape[1]\n            if needed > 0:\n                R = np.random.randn(n, needed)\n                if basis.size > 0:\n                    R = np.column_stack((basis, R))\n                # QR to orthonormalize first k columns\n                Q, _ = np.linalg.qr(R)\n                basis = Q[:, :k]\n            else:\n                # ensure orthonormal\n                Q, _ = np.linalg.qr(basis)\n                basis = Q[:, :k]\n\n            # Probing: sample random linear combinations within the subspace\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample a random direction inside subspace\n                coeffs = np.random.randn(k)\n                d = basis @ coeffs\n                # normalize and scale to current step\n                if np.linalg.norm(d) == 0:\n                    continue\n                d = d / np.linalg.norm(d)\n                alpha = np.random.uniform(-step, step)\n                x_try = clip_to_bounds(x_cur + alpha * d)\n                f_try, x_ret = safe_eval(x_try)\n                if f_try is None:\n                    break\n                # If improvement, accept and try a short line-search\n                if f_try < f_cur - 1e-12:\n                    # preferred direction (from current to x_try)\n                    dir_succ = (x_ret - x_cur)\n                    if np.linalg.norm(dir_succ) > 0:\n                        dir_succ = dir_succ / np.linalg.norm(dir_succ)\n                        # store in memory (LRU style)\n                        dir_memory.insert(0, dir_succ.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                    # perform short line search along this direction\n                    remaining_budget = max(0, self.budget - evals)\n                    line_maxevals = min(12, remaining_budget)\n                    f_line, x_line = line_search(x_cur, f_cur, dir_succ, init_step=alpha if alpha != 0 else step, max_evals=line_maxevals)\n                    if f_line is not None and f_line < f_try - 1e-12:\n                        f_try = f_line\n                        x_ret = x_line\n                    # accept\n                    x_cur = x_ret.copy()\n                    f_cur = f_try\n                    improved = True\n                    no_improve = 0\n                    # slightly increase step if improvement\n                    step = min(step * 1.15, 5.0 * np.mean(ub - lb))\n                    # update best stored inside safe_eval\n                else:\n                    # small chance to do a focused short line-search from current point using this direction\n                    if np.random.rand() < 0.05 and evals < self.budget:\n                        remaining_budget = max(0, self.budget - evals)\n                        if remaining_budget >= 3:\n                            line_maxevals = min(8, remaining_budget)\n                            f_line, x_line = line_search(x_cur, f_cur, d, init_step=step, max_evals=line_maxevals)\n                            if f_line is not None and f_line < f_cur - 1e-12:\n                                # success via line search\n                                x_cur = x_line.copy()\n                                f_cur = f_line\n                                improved = True\n                                no_improve = 0\n                                step = min(step * 1.12, 5.0 * np.mean(ub - lb))\n                                # store direction\n                                dir_succ = d.copy()\n                                dir_memory.insert(0, dir_succ)\n                                if len(dir_memory) > self.memory_size:\n                                    dir_memory.pop()\n\n            # After probing round, decide step-size adaptation\n            if not improved:\n                no_improve += 1\n                # shrink step-size moderately\n                step *= 0.85\n            # If improved, maybe explore a bit more around new x_cur (local intensification)\n            else:\n                # local randomized polishing (few additional probes)\n                extra = min(5, max(1, int(np.ceil(np.log(1 + n)))))\n                for _ in range(extra):\n                    if evals >= self.budget:\n                        break\n                    d = np.random.randn(n)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                    a = np.random.uniform(-0.6 * step, 0.6 * step)\n                    f_try, x_ret = safe_eval(clip_to_bounds(x_cur + a * d))\n                    if f_try is None:\n                        break\n                    if f_try < f_cur - 1e-12:\n                        x_cur = x_ret.copy()\n                        f_cur = f_try\n                        # store direction\n                        dir_succ = (x_ret - x_cur)\n                        if np.linalg.norm(dir_succ) > 0:\n                            dir_succ = dir_succ / np.linalg.norm(dir_succ)\n                            dir_memory.insert(0, dir_succ.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n\n            # stagnation handling: if too many iterations without improvement, restart around best\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # reduce step to try final polishing\n                    step = max(step * 0.5, min_step)\n                else:\n                    # restart sampled around x_best (diversify) with larger step\n                    perturb = np.random.randn(n) * (0.6 * np.mean(ub - lb))\n                    x_cur = clip_to_bounds(x_best + perturb)\n                    f_cur, _ = safe_eval(x_cur)\n                    if f_cur is None:\n                        break\n                    # reset some memory and enlarge step\n                    dir_memory = []\n                    step = min(1.0 * np.mean(ub - lb), step * 2.0)\n                    no_improve = 0\n\n            # safeguard step lower bound\n            if step < min_step:\n                step = min_step\n\n            # quick termination if best is extremely good (close to known optimum 0 for BBOB functions sometimes)\n            # (not assuming knowledge, but if f_best is very small, we can break)\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ARSS scored 0.384 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "80014b04-8d18-4853-8f88-dd939ade8f97", "operator": null, "metadata": {"aucs": [0.099097220182358, 0.14433807370358376, 0.4454852158401378, 0.9696549481524167, 0.22163270373724853, 0.9561618820198801, 0.247151723818818, 0.3232254256066671, 0.282654716368696, 0.1466456920714736]}, "task_prompt": ""}
{"id": "aa267358-ed7e-4f49-b4a7-35c55ce9ec9d", "fitness": 0.481069271857129, "name": "LevySurrogateDirectionalSearch", "description": "The algorithm first builds a small space‑filling archive (init_samples_ratio ≈ 0.12, min samples ∝ dim) and enforces strict budget/bound handling via a safe_eval wrapper, with per-dimension trust radii initialized as a fraction of the search range. It fits a local separable quadratic surrogate (diagonal Hessian + linear terms) by weighted least squares on the nearest neighbors (model_neighbor_multiplier = 8) with ridge regularization, computes a surrogate minimizer clamped by the trust region, and uses curvature regularization to ensure a well‑posed local model. When the surrogate does not help it performs budget‑limited multi‑scale directional probes and coordinate pattern steps (adaptive number of directions and subset of coords) and occasionally attempts heavy‑tailed Cauchy jumps (cauchy_jump_prob = 0.12, scale ~0.4·range) to escape basins. Successes expand the trust radius (success_expand = 1.5 or smaller boosts), failures shrink it (failure_shrink = 0.7), and the archive is pruned to retain best/diverse samples to keep modeling robust.", "code": "import numpy as np\n\nclass LevySurrogateDirectionalSearch:\n    \"\"\"\n    Surrogate-guided Lévy-boosted directional search (LSDS).\n\n    Main idea:\n    - Start with a small random design.\n    - Repeatedly fit a separable quadratic surrogate (diagonal Hessian + linear terms)\n      around the current best using nearby archived points.\n    - Propose the surrogate minimizer under a trust region; accept if improves.\n    - If surrogate fails, perform directional probes (multiple directions, multi-scale)\n      and coordinate pattern steps.\n    - Occasionally perform heavy-tailed (Cauchy) jumps to escape basins.\n    - Adapt a trust radius depending on success/failure. Always respect budget and bounds.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # algorithm hyperparameters (conservative defaults)\n        self.init_samples_ratio = 0.12         # fraction of budget to use for initialization (capped)\n        self.min_init = max(10, 2 * self.dim)\n        self.max_init = min(400, int(0.4 * self.budget))\n        self.model_neighbor_multiplier = 8    # neighbors = multiplier * dim\n        self.trust_init = 0.5                 # fraction of (ub-lb) to start trust region\n        self.trust_min = 1e-6\n        self.trust_max = 2.0\n        self.success_expand = 1.5\n        self.failure_shrink = 0.7\n        self.cauchy_jump_prob = 0.12\n        self.cauchy_scale_frac = 0.4          # fraction of range for typical Cauchy scale\n        self.max_eval_per_iter = 60           # limit evaluations per outer iteration (keeps control)\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        n = self.dim\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        # range\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archives\n        X = []\n        F = []\n\n        # best\n        f_best = np.inf\n        x_best = None\n\n        # Determine initial sampling budget\n        init_budget = int(np.clip(self.init_samples_ratio * budget,\n                                  self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n\n        # Initial random sampling (space-filling-ish)\n        for i in range(init_budget):\n            x = rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # trust region (per-dimension radius)\n        trust_radius = np.maximum(self.trust_init * rng_range, 1e-9)\n\n        # helper: safe evaluate (checks budget)\n        def safe_eval(x):\n            nonlocal evals, budget\n            if evals >= budget:\n                return None\n            x_clipped = np.clip(x, lb, ub)\n            f = func(x_clipped)\n            evals += 1\n            X.append(x_clipped.copy()); F.append(float(f))\n            return float(f), x_clipped\n\n        # main optimization loop\n        while evals < budget:\n            remaining = budget - evals\n            # limit work this outer iteration so we don't overshoot with many probes\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            improved_in_iter = False\n\n            # Attempt surrogate fit if enough nearby points\n            neighbors_needed = max(2 * n + 1, self.model_neighbor_multiplier * n)\n            if len(X) >= neighbors_needed:\n                # choose nearest neighbors to x_best\n                X_arr = np.asarray(X)\n                dists = np.linalg.norm(X_arr - x_best, axis=1)\n                idx_sorted = np.argsort(dists)[:min(len(X), neighbors_needed)]\n                X_nei = X_arr[idx_sorted]\n                F_nei = np.asarray(F)[idx_sorted]\n\n                # Build design matrix for separable quadratic around x_best:\n                # f ~ a + sum_i b_i * dx_i + 0.5 * sum_i h_i * dx_i^2\n                dx = X_nei - x_best  # shape (m, n)\n                M = np.ones((dx.shape[0], 1 + 2 * n))\n                # linear terms\n                M[:, 1:1 + n] = dx\n                # quadratic diagonal terms\n                M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n                y = F_nei\n\n                # weighted least squares: weight by proximity (closer more weight)\n                eps = 1e-12\n                w = 1.0 / (dists[idx_sorted] + 1e-12)\n                w = w / (np.max(w) + eps)\n                W = np.sqrt(w)[:, None]\n                A = W * M\n                b = W * y\n\n                # ridge for stability\n                ridge = 1e-6 * np.eye(M.shape[1])\n                try:\n                    params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ b, rcond=None)\n                    params = params.flatten()\n                    a = params[0]\n                    b_lin = params[1:1 + n]\n                    h_diag = params[1 + n:1 + 2 * n]\n                    # regularize small or negative curvature (we want convex surrogate for minimizer)\n                    # if h_diag <= 1e-8 or negative, push to small positive\n                    h_reg = np.copy(h_diag)\n                    h_reg[h_reg < 1e-8] = 1e-8\n\n                    # candidate delta = - b_lin / h_reg\n                    delta_model = -b_lin / (h_reg + 1e-20)\n                    # limit delta by trust region: per-dim clamp\n                    delta_limited = np.clip(delta_model, -trust_radius, trust_radius)\n                    x_model = x_best + delta_limited\n                    x_model = np.clip(x_model, lb, ub)\n\n                    # Evaluate surrogate candidate if distinct and we have budget\n                    if evals < budget:\n                        out = safe_eval(x_model)\n                        if out is not None:\n                            f_model, x_model = out\n                            work_allow -= 1\n                            if f_model < f_best - 1e-12:\n                                f_best = float(f_model); x_best = x_model.copy()\n                                improved_in_iter = True\n                                # expand trust if surrogate succeeded\n                                trust_radius = np.minimum(trust_radius * self.success_expand,\n                                                          self.trust_max * rng_range)\n                            else:\n                                # shrink trust region on failure\n                                trust_radius = np.maximum(trust_radius * self.failure_shrink,\n                                                          self.trust_min * rng_range)\n                except Exception:\n                    # regression failed: skip surrogate this time\n                    pass\n\n            # If surrogate didn't give improvement (or we skipped it), do directional multi-scale probes\n            if not improved_in_iter and evals < budget and work_allow > 0:\n                # number of directions adapt to dimension and remaining budget\n                num_directions = int(np.clip(4 + n // 2, 4, 12))\n                # base step-size proportional to trust radius magnitude (use L2)\n                base_step = np.linalg.norm(trust_radius) / np.sqrt(float(n))\n                # create direction unit vectors\n                directions = []\n                for _ in range(num_directions):\n                    v = rng.randn(n)\n                    nv = np.linalg.norm(v)\n                    if nv == 0:\n                        v = np.ones(n); nv = np.linalg.norm(v)\n                    directions.append(v / nv)\n\n                # multi-scale distances per direction (geometric)\n                scales = np.array([0.25, 0.5, 1.0, 2.0])  # relative to base_step\n                # Shuffle orders to vary exploration\n                rng.shuffle(directions)\n\n                for d in directions:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    # choose multi-scale sample points along +d and -d\n                    candidates = []\n                    for s in scales:\n                        step = s * base_step\n                        for sign in (+1.0, -1.0):\n                            x_try = x_best + sign * d * step\n                            x_try = np.clip(x_try, lb, ub)\n                            candidates.append(x_try)\n                    # Evaluate candidates until a success or tests exhausted, but respect work_allow\n                    # Evaluate in random order to avoid bias\n                    order = list(range(len(candidates)))\n                    rng.shuffle(order)\n                    for idx in order:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        x_try = candidates[idx]\n                        # small skip if identical to last archived point\n                        if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            f_best = float(f_try); x_best = x_try.copy()\n                            improved_in_iter = True\n                            # small trust increase on success\n                            trust_radius = np.minimum(trust_radius * 1.2, self.trust_max * rng_range)\n                            # break to rebuild surrogate next outer loop\n                            break\n                    # end candidates for this direction\n                # end directions\n\n            # coordinate-wise pattern search (small local tweaks)\n            if evals < budget and work_allow > 0:\n                # for each dimension try + and - step\n                coord_step = np.maximum(trust_radius, 1e-12)\n                # choose a subset of coordinates if dim large and budget small\n                coords = np.arange(n)\n                max_coords = min(n, max(3, int(work_allow // 2)))\n                if max_coords < n:\n                    coords = rng.choice(coords, size=max_coords, replace=False)\n                for i in coords:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    for sign in (+1.0, -1.0):\n                        x_try = x_best.copy()\n                        x_try[i] = x_try[i] + sign * coord_step[i]\n                        x_try = np.clip(x_try, lb, ub)\n                        # skip redundant\n                        if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            f_best = float(f_try); x_best = x_try.copy()\n                            improved_in_iter = True\n                            trust_radius = np.minimum(trust_radius * 1.2, self.trust_max * rng_range)\n                            break\n                    # early stop if success\n                # end coordinate loop\n\n            # Occasional heavy-tailed jump (Cauchy) to escape local traps\n            if evals < budget and rng.rand() < self.cauchy_jump_prob:\n                # draw Cauchy scaled by typical scale\n                scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                # sample one jump for all dims via independent standard Cauchy scaled per-dim\n                jump = rng.standard_cauchy(size=n)\n                # cap the jump magnitude to avoid infinity or huge numbers: clip quantiles\n                jump = np.clip(jump, -10, 10)\n                x_jump = x_best + jump * scale\n                x_jump = np.clip(x_jump, lb, ub)\n                out = safe_eval(x_jump)\n                if out is not None:\n                    f_jump, x_jump = out\n                    if f_jump < f_best - 1e-12:\n                        f_best = float(f_jump); x_best = x_jump.copy()\n                        improved_in_iter = True\n                        # successful jump, expand trust a bit\n                        trust_radius = np.minimum(trust_radius * 2.0, self.trust_max * rng_range)\n                    else:\n                        # unsuccessful jump shrinks trust moderately\n                        trust_radius = np.maximum(trust_radius * 0.8, self.trust_min * rng_range)\n\n            # If nothing improved this iteration, slightly shrink trust (conservative)\n            if not improved_in_iter:\n                trust_radius = np.maximum(trust_radius * self.failure_shrink, self.trust_min * rng_range)\n\n            # keep archive size reasonable (prune oldest keeping diversity and best)\n            max_archive = max(2000, 50 * n)\n            if len(X) > max_archive:\n                # keep the best 200 and then a strided sample of the rest\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                # pick a strided subset of rest to keep size\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # loop continues until budget exhausted\n        # end main loop\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm LevySurrogateDirectionalSearch scored 0.481 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "80014b04-8d18-4853-8f88-dd939ade8f97", "operator": null, "metadata": {"aucs": [0.10795525499184588, 0.14565351384750114, 0.9217117110667462, 0.9721658161417319, 0.2660093496711854, 0.9577403822348832, 0.1874739276609625, 0.3065759165733565, 0.8034838258364548, 0.14192302054662254]}, "task_prompt": ""}
{"id": "c93151ac-a180-4360-bcac-e8fd17372fde", "fitness": "-inf", "name": "DMLSA", "description": "The design is a hybrid CMA‑ES–style strategy: it keeps a full covariance matrix C with evolution paths (ps, pc), weighted recombination (lambda, mu, weights), rank‑one and rank‑mu updates and periodic eigendecomposition, augmented by per‑coordinate RMS‑style scaling (coord_var) and mirrored sampling to reduce estimator variance. A small directional memory (mem_size default 6) stores normalized successful steps and is used for occasional heavy‑tailed Lévy/Cauchy jumps along remembered directions (p_jump=0.2, cauchy_scale=1.0) to increase exploratory moves. Step‑size sigma is adapted via the ps path using cs/damps/chi_n with an initial sigma of 0.3·range and safeguards (min sigma, clipped coord_var), and the algorithm clips all candidates to box bounds. An archive of evaluated points is kept and periodically (model_every) used to fit a ridge‑regularized diagonal quadratic model for targeted local proposals, while numerical fallbacks handle ill conditioning or eigen decomposition failures.", "code": "import numpy as np\nfrom collections import deque\n\nclass DMLSA:\n    \"\"\"\n    Directional Memory and Lévy Subspace Adaptation (DMLSA)\n    - CMA-ES style covariance adaptation + per-coordinate scaling (RMS-like)\n    - Maintains a small directional memory of successful normalized steps\n    - Mixes Gaussian sampling with occasional Cauchy (Lévy-style) jumps along memory directions\n    - Uses mirrored sampling to reduce sampling variance\n    - Periodically attempts a diagonal-quadratic local model proposal using the top points\n    One-line: CMA-like covariance + directional memory + Lévy jumps + adaptive coordinate scaling.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 mem_size=6, p_jump=0.2, cauchy_scale=1.0, model_every=20):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population parameters (CMA-like)\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n        # additional controls\n        self.mem_size = int(mem_size)            # directional memory capacity\n        self.p_jump = float(p_jump)              # probability to add a heavy-tailed jump\n        self.cauchy_scale = float(cauchy_scale)  # base scale for cauchy jumps (multiplied by sigma)\n        self.model_every = int(model_every)      # attempt quadratic-model proposal every this generations\n\n    def __call__(self, func):\n        n = self.dim\n        budget = self.budget\n\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # broadcast scalar bounds\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # weights like CMA-ES\n        lam = self.lambda_\n        mu = self.mu\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # adaptation constants (CMA-ish)\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        c1 = 2 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1 - c1, 2 * (mu_eff - 2 + 1 / mu_eff) / ((n + 2) ** 2 + mu_eff))\n        damps = 1 + 2 * max(0, np.sqrt((mu_eff - 1) / (n + 1)) - 1) + cs\n\n        # expectation of ||N(0,I)||\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # dynamic state\n        m = np.random.uniform(lb, ub)                # initial mean inside bounds\n        sigma = 0.3 * np.mean(ub - lb)               # initial step size\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eig_every = max(1, int(10 * n))\n        eigen_counter = 0\n        gen_count = 0\n\n        # per-coordinate scaling (RMS-like) to adapt step anisotropies\n        coord_var = np.ones(n) * 1e-6\n        coord_alpha = 0.2  # smoothing for coord_var\n\n        # directional memory: deque of normalized direction vectors (y / ||y||)\n        mem = deque(maxlen=self.mem_size)\n\n        # archive for additional diversity and modeling\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of m\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            if fm < f_opt:\n                f_opt = fm\n                x_opt = xm.copy()\n\n        # main optimization loop (generations)\n        while evals < budget:\n            remaining = budget - evals\n            # plan to produce up to lam offspring this generation but no more than remaining\n            planned = min(lam, remaining)\n            candidates = []\n            zs = []  # store raw z vectors for possible mirrored application\n\n            # mirrored sampling: produce ceil(planned/2) base z, then create mirrored pairs\n            pairs = (planned + 1) // 2\n            for _ in range(pairs):\n                z = np.random.randn(n)\n                zs.append(z)\n            # build candidates from zs into up to planned offspring\n            for i, z in enumerate(zs):\n                if len(candidates) >= planned:\n                    break\n                # transform z using current decomposition and per-coordinate scaling\n                y = (B * D) @ z   # approx sqrt(C) @ z\n                # apply per-coordinate scaling (elementwise)\n                scaled_y = y * np.sqrt(coord_var)\n                x = m + sigma * scaled_y\n\n                # optionally add a directional memory Cauchy jump\n                if (mem and (np.random.rand() < self.p_jump)):\n                    # pick a remembered direction and add a Cauchy-scaled jump along it\n                    u = mem[np.random.randint(len(mem))]\n                    # Cauchy heavy tail via standard Cauchy\n                    jump = np.tan(np.pi * (np.random.rand() - 0.5))\n                    x = x + (sigma * self.cauchy_scale) * jump * u\n\n                # clip and store\n                x = np.clip(x, lb, ub)\n                candidates.append(x)\n\n                # mirrored counterpart (if we still need more offspring)\n                if len(candidates) < planned:\n                    z2 = -z\n                    y2 = (B * D) @ z2\n                    scaled_y2 = y2 * np.sqrt(coord_var)\n                    x2 = m + sigma * scaled_y2\n                    if (mem and (np.random.rand() < self.p_jump)):\n                        # use a different memory direction maybe\n                        u2 = mem[np.random.randint(len(mem))]\n                        jump2 = np.tan(np.pi * (np.random.rand() - 0.5))\n                        x2 = x2 + (sigma * self.cauchy_scale) * jump2 * u2\n                    x2 = np.clip(x2, lb, ub)\n                    candidates.append(x2)\n\n            # Evaluate candidates (stop if budget hits)\n            arfit = np.full(len(candidates), np.inf)\n            for k, x in enumerate(candidates):\n                if evals >= budget:\n                    break\n                f = func(x)\n                evals += 1\n                arfit[k] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n\n            # selection & recombination (use only evaluated ones)\n            valid = np.isfinite(arfit)\n            if not np.any(valid):\n                # no new valid evaluations (budget maybe reached) -> break\n                break\n            # work with evaluated portion\n            eval_count = np.sum(valid)\n            arx = np.array(candidates)[:eval_count]\n            arfit = arfit[:eval_count]\n\n            idx = np.argsort(arfit)\n            sel_idx = idx[:min(mu, eval_count)]\n            x_sel = arx[sel_idx]\n            # weighted recombination to update mean\n            m_old = m.copy()\n            m = np.sum(weights[:len(sel_idx)][:, None] * x_sel, axis=0)\n\n            # compute y_sel = (x_sel - m_old) / sigma\n            y_sel = (x_sel - m_old) / (sigma + 1e-20)\n            # weighted mean step in y-space\n            y_w = np.sum(weights[:len(sel_idx)][:, None] * y_sel, axis=0)\n\n            # update evolution paths\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n            norm_ps = np.linalg.norm(ps)\n            # Heuristic for hsig similar to CMA-ES\n            denom = np.sqrt(1 - (1 - cs) ** (2 * (gen_count + 1)))\n            hsig = 1.0 if (norm_ps / (denom + 1e-20) / chi_n) < (1.4 + 2 / (n + 1)) else 0.0\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n\n            # adapt covariance matrix (rank-one + rank-mu)\n            rank_one = np.outer(pc, pc)\n            rank_mu = np.zeros((n, n))\n            for i in range(len(sel_idx)):\n                yi = y_sel[i][:, None]\n                rank_mu += (weights[i] * (yi @ yi.T))\n            C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n\n            # per-coordinate variance adaptive update (RMS of selected y_sel)\n            # compute weighted second moments (var per coord in y)\n            sec_mom = np.sum(weights[:len(sel_idx)][:, None] * (y_sel ** 2), axis=0)\n            coord_var = (1 - coord_alpha) * coord_var + coord_alpha * (sec_mom + 1e-12)\n            # ensure positive and bounded\n            coord_var = np.clip(coord_var, 1e-12, 1e6)\n\n            # update sigma using ps\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1))\n\n            # update directional memory with normalized weighted step (if meaningful)\n            yw_norm = np.linalg.norm(y_w)\n            if yw_norm > 1e-12:\n                mem.append((y_w / yw_norm).copy())\n\n            # eigen decomposition occasionally\n            eigen_counter += eval_count\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                # ensure symmetry\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    # fallback\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # small safeguard for sigma\n            if sigma < 1e-12:\n                sigma = 1e-12\n\n            gen_count += 1\n\n            # occasionally attempt a diagonal quadratic model from the top mu archived (every model_every gens)\n            if (gen_count % self.model_every == 0) and (len(archive_X) >= mu) and (evals < budget):\n                # pick the best mu from archive to fit diagonal quadratic f ≈ sum(a_j * x_j^2) + b^T x + c\n                arch_idx = np.argsort(archive_F)[:mu]\n                Xm = np.array([archive_X[i] for i in arch_idx])\n                Fm = np.array([archive_F[i] for i in arch_idx])\n                # design matrix: columns [x_j^2 ... , x_j ..., 1]\n                A_mat = np.hstack([Xm**2, Xm, np.ones((Xm.shape[0], 1))])\n                # ridge regularization\n                reg = 1e-6\n                try:\n                    sol, *_ = np.linalg.lstsq(A_mat.T @ A_mat + reg * np.eye(A_mat.shape[1]),\n                                              A_mat.T @ Fm, rcond=None)\n                    a = sol[:n]\n                    b = sol[n:2*n]\n                    # compute diagonal-minimizer: derivative = 2*a*x + b => x* = -b / (2a)\n                    # require a positive or regularize\n                    a_safe = a.copy()\n                    a_safe[np.abs(a_safe) < 1e-8] = 1e-8 * np.sign(a_safe[np.abs(a_safe) < 1e-8]) + 1e-8\n                    x_star = -0.5 * b / a_safe\n                    # center around current mean to avoid insane proposals\n                    x_prop = np.clip(m + 0.5 * (x_star - m), lb, ub)\n                    # evaluate model proposal if budget permits\n                    if evals < budget:\n                        f_prop = func(x_prop)\n                        evals += 1\n                        archive_X.append(x_prop.copy())\n                        archive_F.append(f_prop)\n                        if f_prop < f_opt:\n                            f_opt = f_prop\n                            x_opt = x_prop.copy()\n                except Exception:\n                    # any numerical issue: skip this model attempt\n                    pass\n\n            # stop if budget reached (loop condition will handle)\n            if evals >= budget:\n                break\n\n        # final return\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 78, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem = deque(maxlen=self.mem_size)", "error": "In the code, line 78, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem = deque(maxlen=self.mem_size)", "parent_ids": "cdb41671-1583-49d0-ae74-4bd81b978104", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "639c70c1-01c3-4633-9188-1d882b4080d1", "fitness": "-inf", "name": "LARDE", "description": "LARDE is a hybrid strategy that combines CMA-style covariance adaptation (cumulation paths ps/pc, rank-one + rank-mu updates, eigen decomposition to maintain B/D/invsqrtC) with self-adaptive DE operators and occasional long Mantegna–Levy jumps for exploration; covariance constants are tuned for faster cumulation and conservative rank-one weight, and eigen recomputation frequency (eig_every = 8*n) is adapted based on pc norm. The population size scales mildly with dim (lambda = max(4, pop_scale + 5·log(n))) and uses descending log-weights to compute a robust mu_eff for weighted recombination and selection of the top-mu offspring. Per-offspring DE mutation/crossover draws F from a log-normal around 0.8 and CR from Beta(2,2), using an archive of past evaluated solutions for differential vectors, while mirrored Gaussian sampling (when even offspring) improves sample symmetry. Step-size sigma is updated by combining cumulative step-size control (cs, damps, chi_n) with a success-rate multiplier (gentle growth/shrink based on offspring success vs. estimated mean fitness), all under strict budget-aware evaluation and bound clipping.", "code": "import numpy as np\n\nclass LARDE:\n    \"\"\"\n    LARDE: Levy Adaptive Rank-Differential Evolution\n    One-line: Hybrid CMA-style covariance adaptation + self-adaptive DE + Mantegna Levy jumps\n    Main differences vs. the given ARDCE:\n      - Different parameter equations (lambda, cc, cs, c1, cmu, damps) (see comments below).\n      - Sigma adapts by combining cumulation (ps) with a success-rate multiplier (1/5-like rule).\n      - Occasional Mantegna Levy-distributed long jumps (alpha=1.5) for rare, large exploratory moves.\n      - Self-adaptive DE-like mutation factors drawn per-offspring (F, CR).\n      - Mirrored sampling when budget allows (improves sampling symmetry).\n      - Adaptive eigen-decomposition frequency (smaller than original in many dims).\n    Notes:\n      - The algorithm respects the strict budget: func() is called at most `budget` times.\n      - Uses func.bounds.lb / ub for problem bounds (Many Affine BBOB uses [-5,5]).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, pop_scale=5):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.pop_scale = pop_scale\n        if seed is not None:\n            np.random.seed(seed)\n\n        # Parameter choices intentionally different from ARDCE:\n        # lambda_ scales slightly larger with log(dim)\n        # cc, cs, c1, cmu, damps use alternative formulas (see __call__ for values)\n        self.lambda_ = None  # set later (depends on dim)\n        self.mu = None\n\n    # helper: generate a Levy vector via Mantegna's algorithm (alpha in (0,2])\n    def _levy_mantegna(self, n, alpha=1.5, scale=1.0):\n        # Mantegna's algorithm parameters\n        sigma_u = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                   (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n        u = np.random.normal(0, sigma_u, size=n)\n        v = np.random.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1.0 / alpha))\n        return scale * step\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (Many Affine BBOB typically -5..5, but use provided bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # population settings (different equation)\n        lam = max(4, int(self.pop_scale + np.floor(5.0 * np.log(max(1, n)))))\n        mu = max(1, lam // 2)\n        self.lambda_ = lam\n        self.mu = mu\n\n        # recombination weights (slightly shifted log-weights)\n        weights = np.log(np.arange(1, mu + 1) + 0.5)\n        weights = (weights[::-1])  # descending\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # Different adaptation constants (intentionally changed formulas):\n        cc = 2.0 / (n + 2.0)                     # faster cumulation\n        cs = 0.4 * (mu_eff / (n + mu_eff + 1))   # slightly more reactive\n        c1 = 1.0 / max(1.0, (n + 3.0) ** 2)      # smaller rank-one weight\n        cmu = 0.6 * (1.0 - c1) * min(1.0, mu_eff / (2.0 * n))  # conservative rank-mu\n        damps = 1.0 + 0.3 * cs + 0.2 * np.sqrt(max(0.0, mu_eff / n))\n\n        # expectation of norm ~ chi_n (standard approx)\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # dynamic state\n        m = np.random.uniform(lb, ub)                      # initial mean in bounds\n        sigma = 0.25 * np.mean(ub - lb)                    # initial step-size (different)\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n\n        # eigen recompute frequency (different, more frequent for small dims)\n        eig_every = max(1, int(8 * n))\n        eigen_eval_counter = 0\n\n        # archive (for DE ops)\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # evaluate initial mean once (useful baseline)\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_opt = fm\n            x_opt = xm.copy()\n            # estimated fitness of the current mean (we will maintain an estimate without extra evals)\n            f_m_est = fm\n        else:\n            f_m_est = np.inf\n\n        # control probabilities\n        p_levy = 0.15        # probability of applying Levy jump\n        p_de = 0.3           # probability of applying DE mutation\n        # self-adaptive DE: sample F from lognormal(ln(0.8), 0.5) and CR from Beta-like\n        # mirrored sampling preference\n        use_mirroring = True\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            # produce at most lam offspring but not exceeding remaining evals\n            current_lambda = min(lam, remaining)\n\n            # mirrored sampling: if even current_lambda and mirroring enabled, generate half z and add negatives\n            if use_mirroring and (current_lambda % 2 == 0):\n                half = current_lambda // 2\n                arz_half = np.random.randn(half, n)\n                arz = np.vstack([arz_half, -arz_half])\n            else:\n                arz = np.random.randn(current_lambda, n)\n\n            # map through B*D (approx sqrt(C)): y = B * (D * z) equivalently (B*(D*z))\n            # (B * D) outer product trick: multiply columns of B by D\n            ary = arz @ (B * D).T  # shape (current_lambda, n)\n            arx = m + sigma * ary   # candidate points (before mutations)\n\n            # Per-offspring self-adaptive DE and Levy mutations\n            # For DE we use archive when available; pick three distinct vectors (current population + archive)\n            for k in range(current_lambda):\n                xk = arx[k].copy()\n\n                # self-adaptive DE factor and crossover\n                Fk = np.exp(np.random.normal(np.log(0.8), 0.5))  # log-normal around 0.8\n                CRk = np.clip(np.random.beta(2.0, 2.0), 0.0, 1.0)  # moderate CR\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    # choose two distinct archive vectors to form differential\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_vec = Fk * (archive_X[i1] - archive_X[i2])\n                    # apply DE crossover (binomial)\n                    mask = np.random.rand(n) < CRk\n                    if not np.any(mask):\n                        mask[np.random.randint(n)] = True\n                    xk[mask] = xk[mask] + de_vec[mask]\n\n                # Levy jump occasionally (adds rare large exploratory jumps)\n                if np.random.rand() < p_levy:\n                    levy_step = self._levy_mantegna(n, alpha=1.5, scale=0.5 * sigma)\n                    xk = xk + levy_step\n\n                # clip to bounds\n                arx[k] = np.clip(xk, lb, ub)\n\n            # Evaluate offspring (strictly respecting budget)\n            arfit = np.full(current_lambda, np.inf)\n            for k in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[k]\n                f = func(x)\n                evals += 1\n                arfit[k] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n\n            # Selection & recombination (ranked)\n            idx = np.argsort(arfit)\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)  # normalized steps\n\n            m_old = m.copy()\n            m = np.sum(weights[:, np.newaxis] * x_sel, axis=0)\n\n            # Weighted mean step in normalized space\n            y_w = np.sum(weights[:, np.newaxis] * y_sel, axis=0)\n\n            # cumulation paths (using invsqrtC from eig)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrtC @ y_w)\n            norm_ps = np.linalg.norm(ps)\n\n            # heuristic hsig for pc (slightly different threshold behavior)\n            denom = np.sqrt(1.0 - (1.0 - cs) ** (2.0 * max(1.0, evals / max(1, current_lambda))))\n            hsig = 1.0 if (norm_ps / (denom + 1e-20) / chi_n) < (1.5 + 1.0 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # Covariance matrix adaptation (rank-one + rank-mu)\n            rank_one = np.outer(pc, pc)\n            rank_mu = np.zeros((n, n))\n            for i in range(mu):\n                yi = y_sel[i][:, None]\n                rank_mu += weights[i] * (yi @ yi.T)\n            C = (1.0 - c1 - cmu) * C + c1 * (rank_one + (1.0 - hsig) * cc * (2.0 - cc) * C) + cmu * rank_mu\n\n            # Success-rate based multiplier: compare offspring fitnesses to the previous mean-estimated fitness\n            # Use estimated fitness of previous mean (f_m_est) which we maintained without extra evaluations.\n            if 'f_m_est' not in locals():\n                f_m_est = np.median(arfit[np.isfinite(arfit)])  # fallback\n            success_rate = np.mean(arfit[np.isfinite(arfit)] < f_m_est) if np.any(np.isfinite(arfit)) else 0.0\n\n            # update the estimated mean fitness as weighted average of selected offspring fitnesses\n            f_m_est = np.sum(weights * arfit[sel_idx])\n\n            # Sigma update: combine cumulation-rule with success-rate multiplier\n            sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n            # success multiplier (gentle): encourage growth if success rate reasonably high, shrink otherwise\n            if success_rate > 0.2:\n                sigma *= 1.12\n            elif success_rate < 0.05:\n                sigma *= 0.88\n            # clamp tiny sigma\n            sigma = max(sigma, 1e-12)\n\n            # recompute eigendecomposition occasionally to update B, D and invsqrtC\n            eigen_eval_counter += current_lambda\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                # enforce symmetry\n                C = np.triu(C) + np.triu(C, 1).T\n                # numerical fix: clamp diagonal slightly positive\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    # fallback\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n                # optionally adapt eig_every to be larger if C is stable (small pc)\n                if np.linalg.norm(pc) < 1e-6:\n                    eig_every = max(1, int(eig_every * 1.2))\n                else:\n                    eig_every = max(1, int(eig_every * 0.9))\n\n            # stop if budget exhausted (loop condition will handle)\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "cdb41671-1583-49d0-ae74-4bd81b978104", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "1cd18b78-4292-4835-bd6a-f1ed9968d5ee", "fitness": 0.2639121484220498, "name": "ASRES", "description": "The algorithm mixes an axis-aligned diagonal Gaussian (per-coordinate scales D) with a learned low-rank correlated subspace U (k ≈ √n) so steps can capture both independent and principal correlated directions while keeping cost low; population size λ scales like 4+O(log n) and recombination uses log-weights/μ-eff for stability. Sampling uses mirrored pairs for variance reduction, occasional heavy-tailed Cauchy jumps (p≈0.12) to escape traps, and DE-style archive difference mutations (p≈0.20) with a bounded archive to boost exploration. Adaptation is CMA-like: a path-length ps controls global step-size σ (initialized as 0.2·range) and D is updated by an exponential moving second-moment (c_d=0.25); successful weighted steps are buffered and an SVD on that buffer periodically updates the low-rank U. Practical controls include strict bound clipping, stagnation detection that inflates σ and nudges the mean toward archive points, enforcement of the function-evaluation budget, and safeguarding of σ and archive sizes.", "code": "import numpy as np\n\nclass ASRES:\n    \"\"\"\n    Adaptive Subspace Rotational Evolutionary Search (ASRES)\n\n    Key ideas:\n    - Maintain a diagonal scaling vector D (per-coordinate variances) and a low-rank subspace U\n      (k orthonormal directions) learned from recent successful steps. Sample steps as a\n      combination of diagonal-Gaussian + low-rank Gaussian to capture both axis-aligned and\n      correlated structure at low cost.\n    - Use mirrored sampling per generation for variance reduction.\n    - Mix-in heavy-tailed Cauchy (Lévy-like) jumps occasionally to escape local traps.\n    - Use an archive to provide DE-style difference mutations sometimes to boost exploration.\n    - Adapt step-size sigma using a path-length control inspired by CMA-ES, approximating the\n      inverse-square-root of the covariance by the diagonal part for efficiency.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population similar to CMA-ES heuristic\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n        # subspace dimension (low-rank part)\n        if subspace_k is None:\n            # small low-rank to capture principal directions: ~sqrt(n)\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = self.budget\n\n        # bounds (BBOB usually -5..5, but respect func.bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # strategy params\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        # log-based recombination weights\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path length control constants (CMA-like)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial dynamic state\n        m = np.random.uniform(lb, ub)               # initial mean inside bounds\n        sigma = 0.2 * np.mean(ub - lb)              # initial global step-size\n        # diagonal scales (standard deviations per coordinate)\n        D = np.ones(n)\n        # low-rank subspace U (n x k) orthonormal\n        # initialize as k orthonormal random directions\n        if self.k >= 1:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                U, _ = np.linalg.qr(rand_mat)\n                U = U[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n        # path for sigma control\n        ps = np.zeros(n)\n\n        # buffer of recent successful steps (for subspace learning)\n        success_buffer = []\n        buffer_max = max(10 * self.k, 20)\n\n        # archive used for DE-like differences\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of m\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            if fm < f_opt:\n                f_opt = fm\n                x_opt = xm.copy()\n\n        # control parameters for novel behaviors\n        p_cauchy = 0.12     # probability of heavy-tailed jump per candidate\n        cauchy_scale = 1.0  # scale multiplier for Cauchy jumps\n        p_de = 0.20         # probability of using archive-difference mutation\n        F_de = 0.7          # factor for DE difference\n        mirrored = True     # use mirrored sampling inside generation\n        stagnation_gen = 0\n        last_improvement_eval = 0\n        stagnation_thresh = max(5, int(2 * n / (self.k if self.k>0 else 1)))\n\n        # adaptation learning rates\n        c_d = 0.25  # update rate for diagonal variance estimate\n        subspace_update_every = max(1, int(5))  # recompute U from buffer occasionally\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            # number of offspring to produce this generation; respect remaining budget\n            current_lambda = min(lam, remaining)\n            # prepare arrays\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            # produce base gaussian samples (we will mix with mirrors / cauchy / de)\n            base_z = np.random.randn(current_lambda, n)\n\n            for k_idx in range(current_lambda):\n                z = base_z[k_idx].copy()\n                # low-rank part\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U @ z_low  # shape (n,)\n                    # combine diagonal Gaussian and low-rank Gaussian\n                    beta = 0.8 * np.mean(D)  # weight of low-rank component\n                    y = (D * z) + beta * low\n                else:\n                    y = D * z\n\n                # With small probability, replace gaussian step by a heavy-tailed step in a random direction\n                if np.random.rand() < p_cauchy:\n                    # generate a Cauchy scalar and random direction (normalized z)\n                    r = np.random.standard_cauchy() * cauchy_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * np.mean(D)\n\n                # mirrored sampling: for odd/even pairs, use opposite direction to reduce noise\n                if mirrored and (k_idx % 2 == 1):\n                    y = -y\n\n                x = m + sigma * y\n\n                # occasionally add DE-style archive difference mutation\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                arx[k_idx] = x\n                arz[k_idx] = y\n\n            # Evaluate candidates, making sure not to exceed budget\n            arfit = np.full(current_lambda, np.inf)\n            for k_idx in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[k_idx]\n                f = func(x)\n                evals += 1\n                arfit[k_idx] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                # keep archive bounded\n                if len(archive_X) > 5000:\n                    # pop oldest\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection and weighted recombination\n            # sort by fitness (lower is better)\n            idx = np.argsort(arfit)\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]  # scaled steps (y = (x - m)/sigma approximately)\n            m_old = m.copy()\n            # recombine new mean (weighted)\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # compute weighted mean step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # update path for sigma: approximate invsqrtC by dividing by D (ignore low-rank for speed)\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n\n            # update sigma by path length control\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            # lower/upper bounds for sigma\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e2 * np.mean(ub - lb) + 1e-12)\n\n            # update diagonal scales D: exponential moving variance on successful y's\n            # estimate new variance via weighted second-moment of selected y\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)  # E[y^2]\n            # D stores std-dev approx: update squared then sqrt\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n\n            # store weighted successful steps (scaled by sigma) into buffer for subspace learning\n            success_buffer.append((y_w.copy()))\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace U from success_buffer occasionally\n            if (len(success_buffer) >= self.k) and (evals % subspace_update_every == 0):\n                # build matrix of recent successes\n                Y = np.vstack(success_buffer).T  # shape n x m\n                # center the buffer\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                # compute top-k left singular vectors\n                try:\n                    # economy SVD is fine for small k\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    # take top k columns (if available)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        # If k decreased unexpectedly (rare), pad\n                        if U.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U.shape[1]))\n                            U = np.hstack([U, pad])\n                    else:\n                        # fallback to random orthonormal if SVD fails to give modes\n                        rand_mat = np.random.randn(n, self.k)\n                        U, _ = np.linalg.qr(rand_mat)\n                except np.linalg.LinAlgError:\n                    # keep previous U in case of failure\n                    pass\n\n            # stagnation detection: if no improvement for many evaluations, boost exploration\n            if (evals - last_improvement_eval) > stagnation_thresh * current_lambda:\n                # inflate sigma modestly and nudge mean towards a random archive point\n                sigma *= 1.8\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.7 * m + 0.3 * archive_X[pick]\n                # clear buffer to allow new directions\n                success_buffer = []\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # if budget exhausted break (loop condition will exit)\n            if evals >= budget:\n                break\n\n        # return best found\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ASRES scored 0.264 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "cdb41671-1583-49d0-ae74-4bd81b978104", "operator": null, "metadata": {"aucs": [0.11127698337452452, 0.13820897223485973, 0.5111709724293229, 0.41035913631138266, 0.1650046131080296, 0.6679652401280354, 0.19570060842158865, 0.1405393756645943, 0.1644975836870738, 0.1343979988610864]}, "task_prompt": ""}
{"id": "968cad77-749f-4d42-b994-56541ea41d6f", "fitness": "-inf", "name": "ARDE_Enhanced", "description": "ARDE_Enhanced is a hybrid CMA-ES/DE heuristic that uses Lehmer-like (log-skewed, power-weighted) recombination weights and a modestly increased population size (lambda_) with mu ≈ lambda_/2 to bias selection and provide robust ranked-mu covariance updates. It adaptively mixes rank-one (pc path) and rank-mu updates with tailored learning rates (cc, cs, c1, cmu, damps) and maintains B, D, invsqrtC via periodic eigen-decomposition (every eigen_every_factor * n evaluations) with numerical safeguards. Exploration is augmented by an archive-driven DE operator (probability p_de) with a decaying F_de for late-stage exploitation and occasional Lévy jumps (per-offspring p_levy and occasional mean jump on stagnation) to escape local optima. Step-size is adapted by a median-success / target-success rule (s_rate vs target_s=0.2 with dimension-scaled adapt_strength) and practical safeguards (sigma floor, clipping to bounds, fallback C=I on decomposition failure).", "code": "import numpy as np\n\nclass ARDE_Enhanced:\n    \"\"\"\n    ARDE_Enhanced (Adaptive Rotational Differential Evolution - Enhanced)\n    One-line: Hybrid CMA-ES / DE with Lehmer-like weights, median-success sigma adaptation,\n    adaptive DE factor and occasional Lévy flights for robust affine-invariant exploration.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 p_de=0.25, p_levy=0.05, levy_beta=1.5,\n                 eigen_every_factor=5):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.p_de = float(p_de)\n        self.p_levy = float(p_levy)\n        self.levy_beta = float(levy_beta)\n        self.eigen_every_factor = int(eigen_every_factor)\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n        # population sizes (different from original: slightly larger baseline)\n        self.lambda_ = max(6, int(6 + 4 * np.log(self.dim)))\n        self.mu = max(2, self.lambda_ // 2)\n\n    def _levy_step(self, n, beta):\n        # Mantegna's algorithm for symmetric Levy stable step-size\n        # beta in (0,2]; for beta=2 it becomes Gaussian\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = self.rng.normal(0, sigma_u, size=n)\n        v = self.rng.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1 / beta))\n        return step\n\n    def __call__(self, func):\n        n = self.dim\n        budget = self.budget\n\n        # bounds from func (Many Affine BBOB uses -5..5 but remain general)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # Lehmer-like recombination weights (skewed to reduce tail weight)\n        mu = self.mu\n        raw = (np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))) ** 1.2\n        weights = raw / np.sum(raw)\n        # effective selection mass (alternate equation but compatible)\n        mu_eff = (np.sum(weights) ** 2) / np.sum(weights ** 2)\n\n        # Adaptation constants (different formulas from original ARDCE)\n        cc = 0.7 / np.sqrt(n + mu_eff)                     # stronger momentum decay for pc\n        cs = 0.3 / (1.0 + mu_eff / (n + 1.0))             # slightly smaller for ps\n        # smaller rank-one, moderate rank-mu emphasis with mu_eff scaling\n        c1 = 1.5 / (((n + 1.5) ** 1.8) + 0.5 * mu_eff)\n        cmu = min(0.9 * (1 - c1), 0.6 * mu_eff / (mu_eff + 2.0) * (1 - c1))\n        damps = 1.0 + cs + 2.0 * max(0.0, np.sqrt(mu_eff / (n + 1.0)) - 1.0)\n\n        # expectation of ||N(0,I)||\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize dynamic state\n        m = self.rng.uniform(lb, ub)  # initial mean in bounds\n        sigma = 0.25 * np.mean(ub - lb)  # slightly different initial step-size\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eigen_eval_counter = 0\n        eig_every = max(1, int(self.eigen_every_factor * n))  # eigen every factor * n\n\n        # archive of evaluated points for DE-differences\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of m (useful baseline)\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            if fm < f_opt:\n                f_opt = fm\n                x_opt = xm.copy()\n            f_parent = fm  # baseline parent fitness for success computation\n        else:\n            f_parent = np.inf\n\n        # Main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(self.lambda_, remaining)\n\n            # draw standard normals and map through B*D\n            arz = self.rng.standard_normal(size=(current_lambda, n))\n            # ary = arz @ (B * D).T  # sampling step; B * D scales columns of B by D\n            # implement explicit multiplication to avoid broadcasting surprises\n            BD = B * D[np.newaxis, :]\n            ary = arz @ BD.T\n            arx = m + sigma * ary\n\n            # adaptive DE factor decreases over run to encourage exploitation later\n            F_de = 0.6 + 0.4 * (1.0 - evals / max(1, budget))\n\n            # optionally apply DE-style differential mutations (adaptive F)\n            for k in range(current_lambda):\n                if (self.rng.random() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    arx[k] = arx[k] + de_mut\n\n                # occasional Levy jump applied to some offspring, improves long-range escape\n                if self.rng.random() < self.p_levy:\n                    levy = self._levy_step(n, self.levy_beta)\n                    # scale by current sigma and a small factor\n                    arx[k] = arx[k] + 0.5 * sigma * levy\n\n                # clip to bounds\n                arx[k] = np.clip(arx[k], lb, ub)\n\n            # Evaluate candidates one by one, keep within budget\n            arfit = np.full(current_lambda, np.inf)\n            for k in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[k]\n                f = func(x)\n                evals += 1\n                arfit[k] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n\n            # If no valid evaluations occurred (shouldn't happen), break\n            valid_count = np.sum(np.isfinite(arfit))\n            if valid_count == 0:\n                break\n\n            # selection: indices of best mu (handle case mu > current_lambda)\n            sel_mu = min(mu, np.sum(np.isfinite(arfit)).astype(int))\n            idx = np.argsort(arfit)[:sel_mu]\n            x_sel = arx[idx]\n            # compute y vectors relative to m and normalized by sigma\n            y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n\n            # store previous mean fitness for success rate comparisons\n            f_parent_local = f_parent\n            # update new mean by Lehmer-weighted recombination (weights may have different length)\n            # if sel_mu < mu, re-normalize weights subset\n            if sel_mu < mu:\n                w_sub = weights[:sel_mu]\n                w_sub = w_sub / np.sum(w_sub)\n                m_old = m.copy()\n                m = np.sum(w_sub[:, np.newaxis] * x_sel, axis=0)\n                # recompute weighted mean step y_w same subset\n                y_w = np.sum(w_sub[:, np.newaxis] * y_sel, axis=0)\n            else:\n                m_old = m.copy()\n                m = np.sum(weights[:, np.newaxis] * x_sel, axis=0)\n                y_w = np.sum(weights[:, np.newaxis] * y_sel, axis=0)\n\n            # compute success rate: fraction of offspring better than previous mean fitness\n            # if f_parent unknown (inf), fallback to comparing to median of offspring\n            if np.isfinite(f_parent_local):\n                successes = np.sum(arfit < f_parent_local)\n                s_rate = successes / max(1, current_lambda)\n            else:\n                # fallback: fraction better than median of current offspring values\n                median_f = np.median(arfit[np.isfinite(arfit)])\n                successes = np.sum(arfit < median_f)\n                s_rate = successes / max(1, current_lambda)\n\n            # evolution path updates (use different scaling)\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n            norm_ps = np.linalg.norm(ps)\n\n            # normalized generation count approx for hsig behaviour (different criterion)\n            # use success-driven hsig: require moderate success to set hsig=1\n            hsig = 1.0 if (s_rate > 0.15) else 0.0\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n\n            # rank-one and rank-mu updates (slightly different mixing)\n            rank_one = np.outer(pc, pc)\n            rank_mu = np.zeros((n, n))\n            # Weights array might need slicing if sel_mu < mu\n            if sel_mu < mu:\n                w_for = w_sub\n            else:\n                w_for = weights\n            for i in range(sel_mu):\n                yi = y_sel[i][:, None]\n                rank_mu += w_for[i] * (yi @ yi.T)\n\n            C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n\n            # median-success / target-success sigma adaptation (different rule than original)\n            target_s = 0.2\n            # sensitivity decreases with dimension\n            adapt_strength = 0.6 / (1.0 + 0.05 * n)\n            sigma *= np.exp(adapt_strength * (s_rate - target_s) / (damps + 1e-20))\n\n            # occasionally perform a mean Lévy jump if the run stagnates (few improvements)\n            # detect stagnation: few improvements in recent archive tail\n            if len(archive_F) >= max(10, n) and (np.mean(archive_F[-min(len(archive_F), 50):]) - f_opt) < 1e-12:\n                if self.rng.random() < 0.02:\n                    levy = self._levy_step(n, self.levy_beta)\n                    m = np.clip(m + 0.8 * sigma * levy, lb, ub)\n\n            # update f_parent to the best seen at the new mean if present in archive, else keep previous\n            # compute fitness at new mean only if we have budget and the mean has changed meaningfully\n            if evals < budget and np.linalg.norm(m - m_old) > 1e-12:\n                xm = np.clip(m, lb, ub)\n                fm = func(xm)\n                evals += 1\n                archive_X.append(xm.copy())\n                archive_F.append(fm)\n                if fm < f_opt:\n                    f_opt = fm\n                    x_opt = xm.copy()\n                f_parent = fm\n            else:\n                # preserve previous parent fitness\n                f_parent = f_parent_local\n\n            # update eigen decomposition periodically to get B, D and invsqrtC\n            eigen_eval_counter += current_lambda\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                C = np.triu(C) + np.triu(C, 1).T  # enforce symmetry\n                # numerical safeguard: ensure finite\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    # fallback to identity if decomposition fails\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # safeguard sigma lower bound\n            sigma = max(sigma, 1e-12)\n\n            # end of generation loop (budget check via while header)\n\n        # final best found\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "cdb41671-1583-49d0-ae74-4bd81b978104", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "23706aef-6c96-4809-a699-cb0016187663", "fitness": "-inf", "name": "", "description": "ARCADE is built on a CMA-ES backbone (standard recombination weights, cumulative evolution paths, rank-one and rank-mu covariance updates) with classical parameter choices (lambda ≈ 4+3·log n, sigma0 = 0.3·range, cs/cc/c1/cmu tied to mu_eff) to ensure robust adaptation and stability. It increases sampling efficiency and exploration by mirrored sampling (generate half Gaussian z and mirror them) plus occasional DE-style archived perturbations (probability p_de, factor F_de) drawn from a capped archive of past evaluated solutions, with bounds clipping and archive pruning of worst entries. To escape stagnation it applies a small randomized coordinate pattern-search around the best solution and an adaptive restart mechanism that increases population size and sigma and reinitializes the mean near the best, limited by a restart cap. Numerical and algorithmic robustness is enforced via periodic eigendecomposition (≈ every 10·n evals) to update B/D/invsqrtC, explicit symmetrization and diagonal regularization of C, minimal sigma/eigenvalue safeguards, and careful bookkeeping so the budget and bounds are always respected.", "code": "import numpy as np\n\nclass ARCADE (object):\n    \"\"\"\n    ARCADE: Adaptive Restarting CMA-ES with Archived Differential Evolution perturbations\n    + mirrored sampling and occasional coordinate pattern-search refinement.\n    One-line: CMA-ES backbone for affine-robust adaptation, DE-style archived leaps for exploration,\n              mirrored sampling for efficiency, and small local pattern-search + restarts for exploitation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 lambda0=None, p_de=0.25, F_de=0.8, max_archive=1000):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.lambda0 = lambda0\n        self.p_de = p_de\n        self.F_de = F_de\n        self.max_archive = max_archive\n\n    def __call__(self, func):\n        n = int(self.dim)\n        budget = int(self.budget)\n\n        # bounds from func or default to [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0\n            ub = 5.0\n        if lb.shape == ():\n            lb = np.full(n, lb, dtype=float)\n        if ub.shape == ():\n            ub = np.full(n, ub, dtype=float)\n\n        # population size similar to CMA-ES default if not specified\n        if self.lambda0 is None:\n            lam = max(4, int(4 + np.floor(3 * np.log(n))))\n        else:\n            lam = int(self.lambda0)\n        mu = max(1, lam // 2)\n\n        # recombination weights (positive)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1).astype(float))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # adaptation constants (CMA-ES style)\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        c1 = 2.0 / ((n + 1.3) ** 2 + mu_eff)\n        # cmu uses a safe cap\n        cmu = min(1.0 - c1, 2.0 * (mu_eff - 2.0 + 1.0 / mu_eff) / ((n + 2.0) ** 2 + mu_eff))\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize dynamic state\n        m = np.random.uniform(lb, ub)  # initial mean in bounds\n        sigma = 0.3 * np.mean(ub - lb)\n        if sigma <= 0:\n            sigma = 1.0\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eigen_eval_counter = 0\n        eig_every = max(1, int(10 * n))\n\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # bookkeeping for restarts and stagnation\n        stall_gen = 0\n        stall_limit = max(5, int(5 + n / 2))\n        restart_count = 0\n        max_restarts = 5\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            if fm < f_opt:\n                f_opt = fm; x_opt = xm.copy()\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # mirrored sampling: generate half z and mirror them (if odd, one extra)\n            half = (current_lambda + 1) // 2\n            arz = np.random.randn(half, n)\n            arz_full = np.vstack([arz, -arz])[:current_lambda]\n\n            # transform\n            BD = (B * D)  # shape (n,n): B * D where D broadcast across columns\n            ary = arz_full @ BD.T  # y = B*D*z\n            arx = m + sigma * ary\n\n            # apply DE-style difference mutations with archive occasionally\n            for k in range(current_lambda):\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    # pick two distinct archive members and add scaled difference\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    arx[k] = arx[k] + de_mut\n                # clip to bounds\n                arx[k] = np.clip(arx[k], lb, ub)\n\n            # evaluate offspring in order\n            arfit = np.full(current_lambda, np.inf)\n            for k in range(current_lambda):\n                if evals >= budget:\n                    break\n                xk = arx[k]\n                fk = func(xk)\n                evals += 1\n                arfit[k] = fk\n                archive_X.append(xk.copy()); archive_F.append(fk)\n                # keep archive bounded\n                if len(archive_X) > self.max_archive:\n                    # drop worst to keep archive focused; preserve best\n                    worst_idx = int(np.argmax(archive_F))\n                    del archive_X[worst_idx]; del archive_F[worst_idx]\n                if fk < f_opt:\n                    f_opt = fk; x_opt = xk.copy()\n            if evals >= budget:\n                break\n\n            # selection: choose top mu candidates\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n\n            # recombine to form new mean\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # update evolution paths\n            # ensure invsqrtC is up to date (it may be identity initially)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrtC @ (np.sum(weights[:, None] * y_sel, axis=0)))\n            norm_ps = np.linalg.norm(ps)\n            # hsig for pc update\n            denom = np.sqrt(1.0 - (1.0 - cs) ** (2.0 * (1 + evals / max(1, current_lambda))))\n            hsig = 1.0 if (norm_ps / (denom + 1e-20) / chi_n) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * np.sum(weights[:, None] * y_sel, axis=0)\n\n            # rank-mu update (positive weights)\n            rank_mu = np.zeros((n, n))\n            for i in range(mu):\n                yi = y_sel[i][:, None]\n                rank_mu += weights[i] * (yi @ yi.T)\n\n            # small \"active\" negative covariance from worst individuals to help reduce variance where bad samples were found\n            mu_neg = max(1, min(mu, current_lambda - mu))\n            if current_lambda - mu >= 1:\n                worst_idx = idx[-mu_neg:]\n                wneg = -0.2 / mu_neg  # small negative weight\n                for j in worst_idx:\n                    yj = ((arx[j] - m_old) / (sigma + 1e-20))[:, None]\n                    rank_mu += wneg * (yj @ yj.T)\n\n            # rank-one\n            rank_one = np.outer(pc, pc)\n\n            # update covariance matrix\n            C = (1.0 - c1 - cmu) * C + c1 * (rank_one + (1.0 - hsig) * cc * (2.0 - cc) * C) + cmu * rank_mu\n\n            # ensure symmetry and numerical stability\n            C = np.triu(C) + np.triu(C, 1).T\n            # small diagonal fudge\n            C += 1e-12 * np.eye(n)\n\n            # adapt step-size\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            if sigma <= 1e-20:\n                sigma = 1e-20\n\n            # eigen decomposition occasionally\n            eigen_eval_counter += current_lambda\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    # fallback\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # check improvement and stagnation\n            # if best didn't improve during last generation, increase stall count\n            # (we can use last archive entries to check improvement; use stall_gen to monitor)\n            # simple heuristic: if m hasn't improved f_opt and x_opt far from m, count as stall\n            if np.linalg.norm(m - x_opt) < 1e-12 or f_opt <= np.min(archive_F[max(0, len(archive_F) - current_lambda):]) + 1e-20:\n                stall_gen += 1\n            else:\n                stall_gen = 0\n\n            # occasional local pattern search when we are stalling\n            if stall_gen >= stall_limit and evals < budget:\n                # small coordinate search around the best found so far\n                sigma_local = 0.2 * sigma if sigma > 0 else 0.1 * np.mean(ub - lb)\n                improved = False\n                # try coordinates in random order to diversify\n                dims = np.random.permutation(n)\n                for d in dims:\n                    if evals >= budget:\n                        break\n                    step = sigma_local * np.sign(np.random.randn()) * max(1e-6, (ub[d] - lb[d]))\n                    x_trial = x_opt.copy()\n                    # try positive direction\n                    x_trial[d] = np.clip(x_trial[d] + step, lb[d], ub[d])\n                    f_trial = func(x_trial); evals += 1\n                    archive_X.append(x_trial.copy()); archive_F.append(f_trial)\n                    if f_trial < f_opt:\n                        f_opt = f_trial; x_opt = x_trial.copy(); improved = True\n                        # move mean toward the improved point to benefit CMA\n                        m = 0.5 * (m + x_opt)\n                    else:\n                        if evals >= budget:\n                            break\n                        # try negative direction\n                        x_trial2 = x_opt.copy()\n                        x_trial2[d] = np.clip(x_trial2[d] - step, lb[d], ub[d])\n                        f_trial2 = func(x_trial2); evals += 1\n                        archive_X.append(x_trial2.copy()); archive_F.append(f_trial2)\n                        if f_trial2 < f_opt:\n                            f_opt = f_trial2; x_opt = x_trial2.copy(); improved = True\n                            m = 0.5 * (m + x_opt)\n                    # cap archive\n                    if len(archive_X) > self.max_archive:\n                        worst_idx = int(np.argmax(archive_F))\n                        del archive_X[worst_idx]; del archive_F[worst_idx]\n                if improved:\n                    stall_gen = 0\n                else:\n                    # if no improvement after local search, consider a restart\n                    stall_gen = stall_limit + 1\n\n            # restart mechanism: reinitialize near best if we are stuck\n            if stall_gen > (2 * stall_limit) and restart_count < max_restarts and evals < budget:\n                restart_count += 1\n                stall_gen = 0\n                # increase population and sigma moderately\n                lam = int(min(4 + np.floor(3 * np.log(n)) * (1 + restart_count), max(4, lam * 2)))\n                mu = max(1, lam // 2)\n                # reinitialize mean near best with some random jitter\n                radius = min(0.5 * np.mean(ub - lb), 0.5 * sigma * (1 + restart_count))\n                m = np.clip(x_opt + radius * np.random.randn(n), lb, ub)\n                sigma = max(1e-3, sigma * (1.5 + 0.5 * restart_count))\n                # reset covariance to isotropic to avoid pathological directions\n                C = np.eye(n) * (sigma ** 2)\n                ps = np.zeros(n); pc = np.zeros(n)\n                B = np.eye(n); D = np.ones(n); invsqrtC = np.eye(n)\n                eigen_eval_counter = 0\n\n        # return best found\n        if x_opt is None:\n            # fallback random sample if nothing found (shouldn't happen)\n            x_opt = np.clip(np.random.uniform(lb, ub), lb, ub)\n            f_opt = func(x_opt)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 152, in evaluate, the following error occurred:\nKeyError: ''\nOn line: # ensure invsqrtC is up to date (it may be identity initially)", "error": "In the code, line 152, in evaluate, the following error occurred:\nKeyError: ''\nOn line: # ensure invsqrtC is up to date (it may be identity initially)", "parent_ids": "cdb41671-1583-49d0-ae74-4bd81b978104", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "12b7c2ad-f1be-4498-b845-93f332347f42", "fitness": "-inf", "name": "SCAT", "description": "The design is a hybrid global-local heuristic that alternates a local surrogate-guided trust-region step (separable quadratic fit on nearest neighbors with weighted least-squares and per-dimension trust radii) with a CMA-ES–style generation (adaptive mean, sigma, evolution paths, rank-one/rank-mu covariance updates and log-weights derived from mu_eff) so the method can exploit cheap local models while retaining robust global search. Trust-region mechanics are explicit (trust_init_frac=0.35, per-dimension trust_radius clamped and expanded/shrunk on surrogate/CMA successes or failures, and a tiny trust_min_frac) to control model proposals and regularize curvature; the surrogate regularizes small Hessian entries and accepts model minimizers only when they improve. Diversification and global escape mechanisms include DE-style difference mutations (p_de=0.2, F_de=0.8) applied to offspring, occasional principal-direction probes, and heavy-tailed Cauchy jumps (cauchy_prob=0.1, cauchy_scale_frac=0.35) to jump out of traps, while archives of evaluated points (pruned to ~O(n)–O(2000)) feed the surrogate and DE moves. Practical safeguards and parameter tuning (lambda scaling ~4+3 log(n), eigen recomputation every ~12·n evals, ridge regularization, sigma floor, D/B eigendecomposition, and archive pruning) ensure numerical stability and budget-respecting evaluation.", "code": "import numpy as np\n\nclass SCAT:\n    \"\"\"\n    Surrogate-guided Covariance-Adaptive Trust (SCAT)\n    One-line: Fit a local separable quadratic surrogate to propose trust-region minimizers,\n    otherwise sample offspring from a CMA-ES-like Gaussian (with evolution paths and rank-one/rank-mu updates),\n    use an archive for modeling and DE-style difference mutations, and occasional Cauchy jumps to escape traps.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # CMA-like population defaults\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # surrogate / trust settings\n        self.model_neighbor_multiplier = 8\n        self.trust_init_frac = 0.35  # initial trust radius as fraction of range\n        self.trust_min_frac = 1e-6\n        self.trust_max_frac = 2.0\n\n        # exploration diversifiers\n        self.p_de = 0.20\n        self.F_de = 0.8\n        self.cauchy_prob = 0.10\n        self.cauchy_scale_frac = 0.35\n\n        # eigen recomputation frequency\n        self.eig_every = max(1, int(12 * self.dim))\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        # Strategy parameters (CMA-style)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        c1 = 2 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1 - c1, 2 * (mu_eff - 2 + 1 / mu_eff) / ((n + 2) ** 2 + mu_eff))\n        damps = 1 + 2 * max(0, np.sqrt((mu_eff - 1) / (n + 1)) - 1) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize state\n        m = np.random.uniform(lb, ub)  # mean\n        sigma = 0.3 * np.mean(rng_range)\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eigen_eval_counter = 0\n\n        # trust radius (per-dim)\n        trust_radius = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n\n        # archives\n        X = []\n        F = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # safe eval wrapper\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            if evals >= budget:\n                return None\n            x_clipped = np.clip(x, lb, ub)\n            try:\n                f = func(x_clipped)\n            except Exception:\n                f = np.inf\n            evals += 1\n            X.append(x_clipped.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x_clipped.copy()\n            return float(f), x_clipped\n\n        # initial sample at mean\n        out = safe_eval(np.clip(m, lb, ub))\n        if out is not None:\n            # ensure m aligned with evaluated best if improves\n            f_m, xm = out\n            if f_m < f_best:\n                f_best = f_m; x_best = xm.copy()\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n\n            improved_in_iter = False\n\n            # Try surrogate when enough neighbors\n            neighbors_needed = max(2 * n + 1, self.model_neighbor_multiplier * n)\n            if len(X) >= neighbors_needed:\n                # pick nearest neighbors to current mean m\n                X_arr = np.asarray(X)\n                dists = np.linalg.norm(X_arr - m, axis=1)\n                idx_sorted = np.argsort(dists)[:min(len(X), neighbors_needed)]\n                X_nei = X_arr[idx_sorted]\n                F_nei = np.asarray(F)[idx_sorted]\n\n                # fit separable quadratic around m: f ~ a + sum b_i dx_i + 0.5 sum h_i dx_i^2\n                dx = X_nei - m\n                m_rows = dx.shape[0]\n                M = np.ones((m_rows, 1 + 2 * n))\n                M[:, 1:1 + n] = dx\n                M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n                y = F_nei\n\n                # weights by proximity\n                w = 1.0 / (dists[idx_sorted] + 1e-12)\n                w = w / (np.max(w) + 1e-12)\n                W = np.sqrt(w)[:, None]\n                A = W * M\n                b = W * y\n\n                ridge = 1e-8 * np.eye(M.shape[1])\n                try:\n                    params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ b, rcond=None)\n                    params = params.flatten()\n                    a_par = params[0]\n                    b_lin = params[1:1 + n]\n                    h_diag = params[1 + n:1 + 2 * n]\n                    # regularize curvature\n                    h_reg = np.copy(h_diag)\n                    h_reg[h_reg < 1e-8] = 1e-8\n\n                    # model minimizer delta\n                    delta_model = -b_lin / (h_reg + 1e-20)\n                    # clamp by trust region\n                    delta_limited = np.clip(delta_model, -trust_radius, trust_radius)\n                    x_model = m + delta_limited\n                    x_model = np.clip(x_model, lb, ub)\n\n                    # evaluate model candidate if budget allows\n                    if evals < budget:\n                        out = safe_eval(x_model)\n                        if out is not None:\n                            f_model, x_model = out\n                            if f_model < f_best - 1e-12:\n                                # accept surrogate point, update mean and treat as success\n                                # compute y_w for CMA update as (x_model - m)/sigma\n                                if sigma < 1e-20:\n                                    sigma = max(1e-12, 0.3 * np.mean(rng_range))\n                                y_w = (x_model - m) / (sigma + 1e-20)\n                                m_old = m.copy()\n                                m = x_model.copy()\n                                # update evolution paths as if a successful step of weight 1\n                                ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n                                norm_ps = np.linalg.norm(ps)\n                                hsig = 1.0 if (norm_ps / np.sqrt(1 - (1 - cs) ** (2 * (evals / max(1, lam) + 1))) / chi_n) < (1.4 + 2 / (n + 1)) else 0.0\n                                pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n                                # rank-one update and small rank-mu with single pseudo-weight\n                                rank_one = np.outer(pc, pc)\n                                rank_mu = np.outer(y_w, y_w)\n                                C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n                                # step-size adaptation (treat as success)\n                                sigma *= np.exp((cs / damps) * (np.linalg.norm(ps) / chi_n - 1))\n                                # expand trust on success\n                                trust_radius = np.minimum(trust_radius * 1.4, self.trust_max_frac * rng_range)\n                                improved_in_iter = True\n                            else:\n                                # surrogate failed; shrink trust moderately\n                                trust_radius = np.maximum(trust_radius * 0.7, self.trust_min_frac * rng_range)\n                except Exception:\n                    # on any failure skip surrogate\n                    pass\n\n            # If surrogate didn't improve, generate a CMA-style generation of offspring\n            if not improved_in_iter and evals < budget:\n                remaining = budget - evals\n                current_lambda = min(lam, remaining)\n                # sample z ~ N(0,I)\n                arz = np.random.randn(current_lambda, n)\n                # generate y = B * (D * z)\n                # shapes: D is length n, scale columns\n                ary = arz * D[np.newaxis, :]  # scale\n                ary = ary @ B.T  # map by B\n                arx = m + sigma * ary\n\n                # apply DE-style difference mutations from archive with some probability\n                for k in range(current_lambda):\n                    if (np.random.rand() < self.p_de) and (len(X) >= 2):\n                        i1, i2 = np.random.choice(len(X), size=2, replace=False)\n                        de_mut = self.F_de * (X[i1] - X[i2])\n                        arx[k] = arx[k] + de_mut\n                    arx[k] = np.clip(arx[k], lb, ub)\n\n                # evaluate offspring until budget or done\n                arfit = np.full(current_lambda, np.inf)\n                for k in range(current_lambda):\n                    if evals >= budget:\n                        break\n                    out = safe_eval(arx[k])\n                    if out is None:\n                        break\n                    f_k, xk = out\n                    arfit[k] = f_k\n                # selection and recombination if any evaluated\n                valid_idx = np.where(np.isfinite(arfit))[0]\n                if valid_idx.size > 0:\n                    idx_sorted = np.argsort(arfit[valid_idx])\n                    sel_idx = valid_idx[idx_sorted[:mu]]\n                    x_sel = arx[sel_idx]\n                    y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n                    m_old = m.copy()\n                    m = np.sum(weights[:, None] * x_sel, axis=0)\n                    y_w = np.sum(weights[:, None] * y_sel, axis=0)\n                    # update evolution paths\n                    ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n                    norm_ps = np.linalg.norm(ps)\n                    hsig = 1.0 if (norm_ps / np.sqrt(1 - (1 - cs) ** (2 * (evals / max(1, lam) + 1))) / chi_n) < (1.4 + 2 / (n + 1)) else 0.0\n                    pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n                    # covariance update\n                    rank_one = np.outer(pc, pc)\n                    rank_mu = np.zeros((n, n))\n                    for i in range(y_sel.shape[0]):\n                        yi = y_sel[i][:, None]\n                        rank_mu += weights[i] * (yi @ yi.T)\n                    C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n                    # update sigma\n                    sigma *= np.exp((cs / damps) * (np.linalg.norm(ps) / chi_n - 1))\n                    # small trust increase if overall generation helped best\n                    if np.min(arfit[valid_idx]) < f_best - 1e-12:\n                        trust_radius = np.minimum(trust_radius * 1.2, self.trust_max_frac * rng_range)\n\n                # occasionally try a cheap directional probe if generation stagnates\n                if (not improved_in_iter) and (np.random.rand() < 0.2) and evals < budget:\n                    # pick a direction from top offspring variance (principal eigenvector)\n                    try:\n                        # use leading eigenvector of C\n                        vals, vecs = np.linalg.eigh(C)\n                        leading = vecs[:, -1]\n                        step = np.linalg.norm(trust_radius) / np.sqrt(max(1.0, n))\n                        x_probe = np.clip(m + leading * (step * np.random.uniform(0.5, 1.5)), lb, ub)\n                        out = safe_eval(x_probe)\n                        if out is not None:\n                            f_probe, xp = out\n                            if f_probe < f_best - 1e-12:\n                                trust_radius = np.minimum(trust_radius * 1.3, self.trust_max_frac * rng_range)\n                    except Exception:\n                        pass\n\n            # occasional heavy-tailed jump\n            if evals < budget and (np.random.rand() < self.cauchy_prob):\n                scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                jump = np.clip(np.random.standard_cauchy(size=n), -10, 10)\n                x_jump = np.clip(m + jump * scale, lb, ub)\n                out = safe_eval(x_jump)\n                if out is not None:\n                    f_jump, x_jump = out\n                    if f_jump < f_best - 1e-12:\n                        # accept jump as new mean (diversify)\n                        m = x_jump.copy()\n                        sigma = max(sigma, 0.2 * np.mean(rng_range))\n                        trust_radius = np.minimum(trust_radius * 2.0, self.trust_max_frac * rng_range)\n\n            # recompute eigen decomposition occasionally\n            eigen_eval_counter += 1\n            if eigen_eval_counter >= self.eig_every:\n                eigen_eval_counter = 0\n                # enforce symmetry\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # safeguard sigma\n            if sigma < 1e-12:\n                sigma = 1e-12\n\n            # shrink trust conservatively if no improvement recently\n            # detect recent improvement via comparison of best in archive vs previous stored best\n            # (we already update f_best whenever improvement occurs)\n            # If nothing improved this outer loop, shrink modestly\n            # We'll check by seeing if last few archives changed best (light heuristic)\n            # Simple decay if no improvement\n            if not improved_in_iter:\n                trust_radius = np.maximum(trust_radius * 0.95, self.trust_min_frac * rng_range)\n\n            # prune archive to reasonable size for modeling\n            max_archive = max(2000, 50 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # small termination shortcut if extremely good\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 230, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,1) (2,10) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "error": "In the code, line 230, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,1) (2,10) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "parent_ids": "cdb41671-1583-49d0-ae74-4bd81b978104", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ca301f45-5832-4cfb-91ca-c76035cb0d3c", "fitness": "-inf", "name": "ARSSL", "description": "ARSSL builds a cheap, adaptive search by fitting small random-subspace quadratic surrogates (k ≈ subspace_k_frac * n, limited to ≤8) using weighted ridge regression on a neighborhood of x_best (neighbors_needed = max(4*k+6, model_neighbor_multiplier*n)), stabilizing the Hessian and analytically solving for the subspace minimizer which is clipped by an isotropic trust_radius. Around that core surrogate it generates an ensemble of multi-scale proposals — surrogate minimizer and principal-direction PCA probes, residual-guided directional samples, local Gaussian draws, coordinate tweaks, and occasional Lévy (Cauchy) jumps — evaluated in randomized order with a per-iteration work_allow cap (max_eval_per_iter). Search dynamics are controlled by an adaptive trust radius (trust_init_frac, success_expand, failure_shrink, trust_min/max), greedy space-filling initialization, and periodic Lévy restarts to escape stagnation. Practical touches include archive pruning (keep best + diverse via farthest-point sampling), neighbor weighting, ridge regularization, strict safe_eval budget accounting, and parameter choices favoring modest subspace/model sizes and conservative pruning to balance exploration/exploitation.", "code": "import numpy as np\n\nclass ARSSL:\n    \"\"\"\n    Adaptive Random-Subspace Surrogate with Lévy Restarts (ARSSL)\n\n    Main ideas / novel parts:\n    - Use small random-subspace quadratic surrogates (fit in a k << n dimensional random orthonormal subspace)\n      to get cheap local curvature information where data is concentrated.\n    - Solve the quadratic surrogate analytically in the subspace (regularize Hessian to be PD).\n    - Create an ensemble of proposals: surrogate minimizer (projected), multi-scale line probes along surrogate\n      principal directions, coordinate tweaks, random-local Gaussian samples, and occasional Lévy (Cauchy) jumps.\n    - Guide directional sampling by surrogate residuals (areas where model is uncertain get more exploration).\n    - Adaptive scalar trust radius controls the step magnitudes; expands on success, shrinks on failures.\n    - Archive pruning preserves best points and diversity via greedy farthest-point sampling of remaining archive.\n    - Strict budget handling via safe_eval wrapper; never calls func() more than budget times.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_ratio=0.10, min_init=None,\n                 subspace_k_frac=0.33, model_neighbor_multiplier=6,\n                 trust_init_frac=0.35, trust_min_frac=1e-6, trust_max_frac=2.0,\n                 success_expand=1.6, failure_shrink=0.65,\n                 levy_prob=0.10, levy_scale_frac=0.5,\n                 max_eval_per_iter=80, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_ratio = float(init_ratio)\n        self.min_init = (min_init if min_init is not None else max(10, 2*self.dim))\n        self.subspace_k_frac = float(subspace_k_frac)\n        self.model_neighbor_multiplier = int(model_neighbor_multiplier)\n        self.trust_init_frac = float(trust_init_frac)\n        self.trust_min_frac = float(trust_min_frac)\n        self.trust_max_frac = float(trust_max_frac)\n        self.success_expand = float(success_expand)\n        self.failure_shrink = float(failure_shrink)\n        self.levy_prob = float(levy_prob)\n        self.levy_scale_frac = float(levy_scale_frac)\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        self.verbose = bool(verbose)\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        n = self.dim\n        # bounds extraction\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        # enforce that Many Affine BBOB tasks lie in [-5,5], but allow func-provided bounds\n        # budget / counters\n        budget = int(self.budget)\n        evals = 0\n\n        # archive\n        X = []\n        F = []\n\n        # best tracker\n        f_best = np.inf\n        x_best = None\n\n        # initial sampling budget\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, int(0.4*budget)))\n        init_budget = max(1, init_budget)\n\n        # Space-filling-ish initial sample: greedy max-min selection from candidates\n        # Draw an initial pool and pick greedily to maximize minimal distance (cheap and effective)\n        pool_size = max(5 * init_budget, init_budget + 20)\n        pool = rng.uniform(lb, ub, size=(pool_size, n))\n        chosen_idx = []\n        # start with a random point\n        first = rng.randint(pool_size)\n        chosen_idx.append(first)\n        while len(chosen_idx) < init_budget:\n            chosen = pool[chosen_idx]\n            # compute distances from pool points to chosen set\n            dists = np.min(np.linalg.norm(pool - chosen[:, None, :], axis=2), axis=0)\n            # pick point with maximum minimal distance (farthest)\n            next_i = int(np.argmax(dists))\n            chosen_idx.append(next_i)\n        for idx in chosen_idx:\n            if evals >= budget:\n                break\n            x = pool[idx].copy()\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n        if evals >= budget:\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # trust radius as scalar (applied isotropically), expressed as fraction of typical range (L2 scale)\n        range_norm = np.linalg.norm(rng_range)\n        trust_radius = max(self.trust_init_frac * range_norm, 1e-12)\n        trust_min = max(self.trust_min_frac * range_norm, 1e-12)\n        trust_max = max(self.trust_max_frac * range_norm, 1e-12)\n\n        # helper safe evaluation\n        def safe_eval(x):\n            nonlocal evals, budget\n            if evals >= budget:\n                return None\n            x_clipped = np.clip(np.asarray(x, dtype=float), lb, ub)\n            f = func(x_clipped)\n            evals += 1\n            X.append(x_clipped.copy()); F.append(float(f))\n            return float(f), x_clipped\n\n        # helper: prune archive keeping best and diverse rest via greedy farthest sampling\n        def prune_archive(X_list, F_list, max_size):\n            if len(X_list) <= max_size:\n                return X_list, F_list\n            X_arr = np.asarray(X_list)\n            F_arr = np.asarray(F_list)\n            # keep top few best\n            keep_best_n = min(200, max(20, int(10 + 5*n)))\n            best_idx = np.argsort(F_arr)[:keep_best_n]\n            remaining_idx = [i for i in range(len(X_arr)) if i not in best_idx]\n            # pick diverse subset of remaining via greedy farthest selection\n            target_rest = max_size - len(best_idx)\n            if target_rest <= 0:\n                keep_idx = best_idx[:max_size]\n                return [X_list[i] for i in keep_idx], [F_list[i] for i in keep_idx]\n            rest_points = X_arr[remaining_idx]\n            chosen = []\n            if rest_points.shape[0] > 0:\n                # seed with the farthest from the best centroid\n                centroid = np.mean(X_arr[best_idx], axis=0) if len(best_idx) > 0 else np.mean(rest_points, axis=0)\n                d0 = np.linalg.norm(rest_points - centroid, axis=1)\n                first = int(np.argmax(d0))\n                chosen.append(first)\n                while len(chosen) < target_rest and len(chosen) < rest_points.shape[0]:\n                    # distances to chosen set\n                    chosen_pts = rest_points[chosen]\n                    dists = np.min(np.linalg.norm(rest_points[:, None, :] - chosen_pts[None, :, :], axis=2), axis=1)\n                    # avoid picking already chosen\n                    dists[chosen] = -np.inf\n                    nxt = int(np.argmax(dists))\n                    chosen.append(nxt)\n            # map chosen indices back to original indices\n            chosen_orig = [remaining_idx[i] for i in chosen]\n            keep_idx = np.concatenate([best_idx, chosen_orig])\n            # ensure unique and bounded\n            keep_idx = keep_idx[:max_size]\n            return [X_list[i] for i in keep_idx], [F_list[i] for i in keep_idx]\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            improved = False\n\n            # Build surrogate in a random low-dimensional subspace if enough archive\n            # choose subspace dim k\n            k = int(np.clip(max(2, int(np.ceil(self.subspace_k_frac * n))), 2, min(8, n)))\n            neighbors_needed = max(4*k + 6, self.model_neighbor_multiplier * n)\n            if len(X) >= neighbors_needed:\n                # prepare arrays\n                X_arr = np.asarray(X)\n                F_arr = np.asarray(F)\n                # pick neighbors near x_best (Euclidean)\n                dists = np.linalg.norm(X_arr - x_best, axis=1)\n                idx_sorted = np.argsort(dists)[:min(len(X), neighbors_needed)]\n                X_nei = X_arr[idx_sorted]\n                F_nei = F_arr[idx_sorted]\n                # Build random orthonormal basis anchored at x_best\n                A = rng.randn(n, k)\n                # orthonormalize with QR\n                Q, _ = np.linalg.qr(A, mode='reduced')  # shape (n,k)\n                # project neighbor displacements into subspace coords\n                Z = (X_nei - x_best) @ Q  # shape (m, k)\n                # design matrix for full quadratic in subspace: 1, linear(z), quadratic diag and cross terms\n                # number of quadratic terms = k + k*(k-1)/2\n                m = Z.shape[0]\n                # build feature matrix phi of shape (m, 1 + k + k + k*(k-1)/2)\n                # order: [1, z1..zk, 0.5*z1^2..0.5*zk^2, cross_ij = z_i * z_j for i<j]\n                cross_count = k*(k-1)//2\n                p = 1 + k + k + cross_count\n                Phi = np.ones((m, p))\n                # linear\n                Phi[:, 1:1+k] = Z\n                # quadratic diag (scaled by 0.5 to match second derivative interpretation)\n                Phi[:, 1+k:1+2*k] = 0.5 * (Z**2)\n                # cross terms\n                col = 1 + 2*k\n                for i in range(k):\n                    for j in range(i+1, k):\n                        Phi[:, col] = Z[:, i] * Z[:, j]\n                        col += 1\n                y = F_nei\n\n                # weight by proximity in original space (closer points get more weight)\n                prox = 1.0 / (dists[idx_sorted] + 1e-12)\n                prox = prox / (np.max(prox) + 1e-12)\n                W = np.sqrt(prox)[:, None]\n                A_mat = W * Phi\n                b_vec = W * y\n\n                # ridge solve: params shape (p,)\n                ridge_lambda = 1e-6 + 1e-4 * (1.0 / max(1, m))\n                try:\n                    # normal equations with ridge\n                    lhs = A_mat.T @ A_mat + ridge_lambda * np.eye(p)\n                    rhs = A_mat.T @ b_vec\n                    params = np.linalg.solve(lhs, rhs).flatten()\n                    # extract quadratic form in subspace: constant a, linear g (grad), Hessian H (k x k)\n                    a = params[0]\n                    g_lin = params[1:1+k]  # derivative coefficients\n                    h_diag = params[1+k:1+2*k]  # half of diagonal second-deriv (we used 0.5*z^2), so h_ii = diag entries\n                    # reconstruct Hessian\n                    H = np.zeros((k, k))\n                    np.fill_diagonal(H, h_diag)\n                    # fill cross terms\n                    col = 1 + 2*k\n                    for i in range(k):\n                        for j in range(i+1, k):\n                            # cross term coefficient corresponds to z_i * z_j (coefficient c_ij). The Hessian symmetric entry is c_ij\n                            c = params[col]\n                            H[i, j] = c\n                            H[j, i] = c\n                            col += 1\n                    # Now H is the Hessian in subspace (approx). Ensure symmetry\n                    H = 0.5 * (H + H.T)\n\n                    # regularize Hessian to be positive definite for minimizer computation\n                    # eigen stabilization\n                    try:\n                        eigvals = np.linalg.eigvalsh(H)\n                        min_eig = np.min(eigvals)\n                    except Exception:\n                        min_eig = -1.0\n                    if min_eig <= 1e-8:\n                        # add shift to make it slightly positive definite\n                        shift = (1e-8 - min_eig) + 1e-6\n                        H_reg = H + shift * np.eye(k)\n                    else:\n                        H_reg = H.copy()\n\n                    # compute minimizer in subspace: solve H_reg * z* = -g_lin\n                    # if H_reg singular or ill-conditioned, fallback to pseudo-inverse\n                    try:\n                        z_star = -np.linalg.solve(H_reg, g_lin)\n                    except np.linalg.LinAlgError:\n                        z_star = -np.linalg.pinv(H_reg) @ g_lin\n\n                    # limit subspace step by trust_radius: scale z_star so that ||Q z_star||_2 <= trust_radius\n                    step_vec = Q @ z_star\n                    step_norm = np.linalg.norm(step_vec)\n                    if step_norm > 0:\n                        scale = min(1.0, trust_radius / (step_norm + 1e-16))\n                    else:\n                        scale = 1.0\n                    z_limited = z_star * scale\n                    x_model = np.clip(x_best + Q @ z_limited, lb, ub)\n\n                    # evaluate surrogate candidate\n                    if evals < budget:\n                        out = safe_eval(x_model)\n                        if out is not None:\n                            f_model, x_model = out\n                            work_allow -= 1\n                            if f_model < f_best - 1e-12:\n                                f_best = float(f_model); x_best = x_model.copy()\n                                improved = True\n                                trust_radius = min(trust_radius * self.success_expand, trust_max)\n                            else:\n                                # shrink trust moderately\n                                trust_radius = max(trust_radius * self.failure_shrink, trust_min)\n                except Exception:\n                    # if regression fails, skip surrogate this iteration\n                    pass\n\n            # Ensemble proposals: create a list of candidate points (until work_allow exhausted)\n            candidates = []\n\n            # 1) Surrogate principal directions multi-scale probes (if surrogate available)\n            if len(X) >= 2 and work_allow > 0:\n                # compute a local PCA on neighbors around x_best to get principal directions\n                X_arr = np.asarray(X)\n                dists = np.linalg.norm(X_arr - x_best, axis=1)\n                idx_local = np.argsort(dists)[:min(len(X), max(8, 6*n//5))]\n                local = X_arr[idx_local] - x_best\n                if local.shape[0] >= 2:\n                    # covariance and eigenvectors\n                    C = np.cov(local.T)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(C)\n                        # pick top directions (1..k_pc)\n                        order = np.argsort(-np.abs(eigvals))\n                        k_pc = min(max(1, int(np.clip(k, 1, n))), n)\n                        for j in range(k_pc):\n                            v = eigvecs[:, order[j]]\n                            # multi-scale steps along +/-\n                            for scale in (0.25, 0.6, 1.0, 1.6):\n                                step = v * (scale * trust_radius)\n                                candidates.append(np.clip(x_best + step, lb, ub))\n                                candidates.append(np.clip(x_best - step, lb, ub))\n\n            # 2) Residual-guided random directions: compute surrogate residuals if surrogate existed\n            if len(X) >= 6 and work_allow > 0:\n                X_arr = np.asarray(X)\n                F_arr = np.asarray(F)\n                dists = np.linalg.norm(X_arr - x_best, axis=1)\n                idx = np.argsort(dists)[:min(len(X),  max(8, neighbors_needed))]\n                localX = X_arr[idx]\n                localF = F_arr[idx]\n                # simple local linear surrogate for residual estimation: fit linear model quickly\n                Z = localX - x_best\n                # predict from nearest neighbors by weighted average of their offsets scaled by residuals\n                # residuals\n                medf = np.median(localF)\n                res = np.abs(localF - medf) + 1e-12\n                weights = res / np.sum(res)\n                # compute a direction as weighted sum of neighbor displacements, favoring high residuals\n                dir_vec = np.sum((Z.T * weights).T, axis=0)\n                if np.linalg.norm(dir_vec) > 1e-12:\n                    dir_unit = dir_vec / np.linalg.norm(dir_vec)\n                    # add a few scaled perturbations along this direction\n                    for s in (0.3, 0.8, 1.4):\n                        candidates.append(np.clip(x_best + dir_unit * (s * trust_radius), lb, ub))\n                        candidates.append(np.clip(x_best - dir_unit * (s * trust_radius), lb, ub))\n\n            # 3) Local random Gaussian samples around x_best\n            if work_allow > 0:\n                n_gauss = min(6, max(2, int(work_allow // 6)))\n                for _ in range(n_gauss):\n                    step = rng.randn(n)\n                    step = step / (np.linalg.norm(step) + 1e-16) * (rng.randn() * 0.6 + 0.8) * trust_radius\n                    candidates.append(np.clip(x_best + step, lb, ub))\n\n            # 4) Coordinate tweaks\n            if work_allow > 0:\n                coords = np.arange(n)\n                rng.shuffle(coords)\n                max_coords = min(n, max(3, int(work_allow // 2)))\n                for i in coords[:max_coords]:\n                    for s in (0.5, 1.0):\n                        delta = np.zeros(n)\n                        delta[i] = s * (trust_radius / np.sqrt(n))\n                        candidates.append(np.clip(x_best + delta, lb, ub))\n                        candidates.append(np.clip(x_best - delta, lb, ub))\n\n            # 5) Occasional Lévy (Cauchy) jump candidate\n            if rng.rand() < self.levy_prob and work_allow > 0:\n                scale = self.levy_scale_frac * np.maximum(rng_range, 1e-9)\n                jump = rng.standard_cauchy(size=n)\n                jump = np.clip(jump, -8, 8)\n                x_jump = np.clip(x_best + jump * scale, lb, ub)\n                candidates.append(x_jump)\n\n            # Evaluate candidate set in random order until work_allow exhausted or an improvement is found\n            if len(candidates) > 0:\n                # deduplicate candidates by rounding to reduce identical evaluations\n                cand_arr = np.asarray(candidates)\n                # shuffle order\n                order = np.arange(cand_arr.shape[0])\n                rng.shuffle(order)\n                for i in order:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    x_try = cand_arr[i]\n                    # skip if identical to last archived point to avoid duplicate evals\n                    if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                        continue\n                    out = safe_eval(x_try)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    f_try, x_try = out\n                    if f_try < f_best - 1e-12:\n                        f_best = float(f_try); x_best = x_try.copy()\n                        improved = True\n                        # expand trust on success (a bit more aggressively than baseline)\n                        trust_radius = min(trust_radius * self.success_expand, trust_max)\n                        # break to rebuild surrogate next outer loop\n                        break\n                # end for candidates\n\n            # If nothing improved, perform a small exploratory contraction or random restart if stagnated\n            if not improved and evals < budget:\n                # With small probability perform a shallow local restart: sample near-best within small radius\n                if rng.rand() < 0.08 and remaining > 3:\n                    for _ in range(min(3, remaining)):\n                        step = rng.randn(n)\n                        step = step / (np.linalg.norm(step) + 1e-16) * (0.6 * trust_radius)\n                        x_try = np.clip(x_best + step, lb, ub)\n                        out = safe_eval(x_try)\n                        if out is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            f_best = float(f_try); x_best = x_try.copy()\n                            improved = True\n                            trust_radius = min(trust_radius * 1.3, trust_max)\n                            break\n                # If still nothing, shrink trust\n                if not improved:\n                    trust_radius = max(trust_radius * self.failure_shrink, trust_min)\n\n            # Occasionally do a more aggressive Lévy restart when stagnation persists\n            # we detect stagnation by low improvement in a small window: use archive tail properties\n            if rng.rand() < 0.03 and remaining > 0:\n                # draw scaled Cauchy jump relative to full range_norm\n                scale = self.levy_scale_frac * rng_range\n                jump = rng.standard_cauchy(size=n)\n                jump = np.clip(jump, -10, 10)\n                x_jump = np.clip(x_best + jump * scale, lb, ub)\n                out = safe_eval(x_jump)\n                if out is not None:\n                    f_jump, x_jump = out\n                    if f_jump < f_best - 1e-12:\n                        f_best = float(f_jump); x_best = x_jump.copy()\n                        improved = True\n                        trust_radius = min(trust_radius * 2.0, trust_max)\n                    else:\n                        trust_radius = max(trust_radius * 0.8, trust_min)\n\n            # prune archive periodically to keep memory and model costs bounded\n            if len(X) > max(500, 40*n):\n                X, F = prune_archive(X, F, max(500, 40*n))\n\n            # verbose logging (optional)\n            if self.verbose:\n                print(f\"evals={evals}/{budget}, f_best={f_best:.6g}, trust={trust_radius:.3g}, archive={len(X)}\")\n\n            # continue loop until budget exhausted\n        # return best found\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 143, in evaluate, the following error occurred:\nSyntaxError: expected 'except' or 'finally' block\nOn line: dists = np.min(np.linalg.norm(rest_points[:, None, :] - chosen_pts[None, :, :], axis=2), axis=1)", "error": "In the code, line 143, in evaluate, the following error occurred:\nSyntaxError: expected 'except' or 'finally' block\nOn line: dists = np.min(np.linalg.norm(rest_points[:, None, :] - chosen_pts[None, :, :], axis=2), axis=1)", "parent_ids": "aa267358-ed7e-4f49-b4a7-35c55ce9ec9d", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "0cdf25fc-cf49-4f8a-a51c-f5e48c4666c6", "fitness": "-inf", "name": "AdaptiveEnsembleLevyQuadraticSearch", "description": "The algorithm is a hybrid, budget-aware heuristic that mixes a larger space-filling initialization (greedy max‑min sampling with init_samples_ratio=0.18 and min_init = max(12,3*dim)) with per-dimension trust regions and iterative cycles limited by max_eval_per_iter=80. Locally it fits a weighted, ridge-regularized separable quadratic model around the current best (using adaptive distance weights and diag-Hessian) and adds an ensemble correction along the principal local eigenvector to capture dominant mixed directions. When the model fails it falls back to orthogonalized multi-scale directional probing and coordinate-wise pattern search, while aggressive trust updates (trust_init=0.25, success_expand=1.8, failure_shrink=0.6, trust_max_factor=3.0) steer exploration vs exploitation. Global exploration is supported by frequent, large heavy‑tailed jumps (Student‑t like with jump_prob=0.18 and jump_scale_frac=0.6), safe bound clipping, and archive pruning that preserves the best plus a stratified diverse subset.", "code": "import numpy as np\n\nclass AdaptiveEnsembleLevyQuadraticSearch:\n    \"\"\"\n    Adaptive Ensemble Levy-Quadratic Search (AELQS)\n\n    Main ideas / differences from the provided algorithm:\n    - Slightly larger and more space-filling initialization (init_samples_ratio, min_init increased).\n    - Ensemble local strategy: try a separable quadratic minimizer AND an eigen-directional correction\n      (captures a dominant mixed-direction) before falling back to directional probing.\n    - More aggressive trust-region expansion on success and stronger shrink on failure.\n    - Higher probability and larger typical scale for heavy-tailed jumps (Student-t/Cauchy-like).\n    - A modestly larger per-iteration evaluation allowance to explore multi-scale directions more widely.\n    - Archive pruning keeps best plus a stratified diverse subset.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 # algorithm hyperparameters (tuned to be different / more aggressive)\n                 init_samples_ratio=0.18,   # larger initial sampling fraction\n                 min_init=None,             # default based on dim if None\n                 max_init_frac=0.5,         # allow more of budget for initialization (capped)\n                 model_neighbor_multiplier=6,  # fewer neighbors multiplier\n                 trust_init=0.25,           # smaller initial trust fraction of range\n                 trust_min=1e-8,\n                 trust_max_factor=3.0,      # allow larger trust region\n                 success_expand=1.8,        # more aggressive expansion\n                 failure_shrink=0.6,        # stronger shrink on failure\n                 jump_prob=0.18,            # higher jump probability\n                 jump_scale_frac=0.6,       # larger jump scale\n                 max_eval_per_iter=80):     # more per-outer-iteration evaluations\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # params\n        self.init_samples_ratio = float(init_samples_ratio)\n        self.min_init = int(min_init) if min_init is not None else max(12, 3 * self.dim)\n        self.max_init_frac = float(max_init_frac)\n        self.model_neighbor_multiplier = int(model_neighbor_multiplier)\n        self.trust_init = float(trust_init)\n        self.trust_min = float(trust_min)\n        self.trust_max_factor = float(trust_max_factor)\n        self.success_expand = float(success_expand)\n        self.failure_shrink = float(failure_shrink)\n        self.jump_prob = float(jump_prob)\n        self.jump_scale_frac = float(jump_scale_frac)\n        self.max_eval_per_iter = int(max_eval_per_iter)\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n\n        n = self.dim\n        # bounds (func.bounds expected)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archives (list of numpy arrays)\n        X = []\n        F = []\n\n        # best\n        f_best = np.inf\n        x_best = None\n\n        # Determine initial sampling budget\n        max_init = min(int(self.max_init_frac * budget),  int(0.6 * budget))\n        init_budget = int(np.clip(self.init_samples_ratio * budget,\n                                  self.min_init, max_init))\n        init_budget = max(1, init_budget)\n\n        # Space-filling-ish initialization:\n        # We sample a candidate pool and pick points sequentially maximizing min-distance.\n        pool_factor = 6\n        pool_size = max(init_budget * pool_factor, init_budget + 20)\n        pool = rng.uniform(lb, ub, size=(int(pool_size), n))\n        chosen = []\n        # greedy maximin selection\n        for i in range(init_budget):\n            if i == 0:\n                idx = rng.randint(0, pool.shape[0])\n                chosen.append(pool[idx])\n            else:\n                cur = np.array(chosen)\n                # compute min distance of each pool point to chosen set\n                dists = np.min(np.linalg.norm(pool - cur[:, None, :], axis=2), axis=0)\n                idx = np.argmax(dists)\n                chosen.append(pool[idx])\n        for x in chosen:\n            if evals >= budget: break\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n        if evals >= budget:\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # trust region per-dim\n        trust_radius = np.maximum(self.trust_init * rng_range, 1e-12)\n        trust_max = self.trust_max_factor * rng_range\n\n        # helper safe_eval\n        def safe_eval(x):\n            nonlocal evals, budget\n            if evals >= budget:\n                return None\n            x_clipped = np.clip(x, lb, ub)\n            f = func(x_clipped)\n            evals += 1\n            X.append(x_clipped.copy()); F.append(float(f))\n            return float(f), x_clipped\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            improved = False\n\n            # Surrogate attempt if enough data\n            neighbors_needed = max(2 * n + 1, self.model_neighbor_multiplier * n)\n            if len(X) >= neighbors_needed:\n                X_arr = np.asarray(X)\n                dists = np.linalg.norm(X_arr - x_best, axis=1)\n                idx_sorted = np.argsort(dists)[:min(len(X), neighbors_needed)]\n                X_nei = X_arr[idx_sorted]\n                F_nei = np.asarray(F)[idx_sorted]\n\n                # fit separable quadratic (diagonal Hessian) with stronger ridge and distance weights\n                dx = X_nei - x_best\n                m = dx.shape[0]\n                M = np.ones((m, 1 + 2 * n))\n                M[:, 1:1 + n] = dx\n                M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n                y = F_nei\n\n                # weights: gaussian kernel on distance with adaptive bandwidth\n                bandwidth = np.median(dists[idx_sorted]) + 1e-12\n                w = np.exp(-(dists[idx_sorted] ** 2) / (2.0 * (bandwidth ** 2 + 1e-12)))\n                w = w / (np.max(w) + 1e-12)\n                W = np.sqrt(w)[:, None]\n                A = W * M\n                b = W * y\n\n                ridge_scale = 1e-4 * (1.0 + np.mean(np.abs(y)))  # larger ridge when residuals larger\n                ridge = ridge_scale * np.eye(M.shape[1])\n\n                try:\n                    params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ b, rcond=None)\n                    params = params.flatten()\n                    a_lin = params[0]\n                    b_lin = params[1:1 + n]\n                    h_diag = params[1 + n:1 + 2 * n]\n\n                    # enforce positive curvature but allow small negatives to keep some exploration\n                    h_reg = np.copy(h_diag)\n                    h_reg[h_reg < 1e-7] = 1e-7\n\n                    # separable minimizer candidate\n                    delta_model = -b_lin / (h_reg + 1e-20)\n                    delta_limited = np.clip(delta_model, -trust_radius, trust_radius)\n                    x_model = np.clip(x_best + delta_limited, lb, ub)\n\n                    # Additional ensemble correction: compute dominant eigenvector of local scatter to capture\n                    # a main mixed-direction and propose a second candidate along the negative directional derivative.\n                    # compute weighted covariance, take leading eigenvector\n                    Xw = (W * dx).T  # shape (n, m)\n                    cov_local = Xw @ Xw.T + 1e-12 * np.eye(n)\n                    try:\n                        vals, vecs = np.linalg.eigh(cov_local)\n                        principal = vecs[:, -1]\n                        # estimate directional derivative along principal using local linear fit\n                        proj = dx @ principal\n                        # fit scalar slope s in f ~ s * proj + c (weighted)\n                        if np.ptp(proj) > 1e-12:\n                            s = np.sum(w * proj * y) / (np.sum(w * proj * proj) + 1e-12)\n                            # direction move length proportional to (s / variance) but limited by trust\n                            step_dir = -np.sign(s) * np.minimum(np.linalg.norm(trust_radius), np.linalg.norm(trust_radius) * 0.7)\n                            x_corr = np.clip(x_best + principal * step_dir, lb, ub)\n                        else:\n                            x_corr = x_model  # fallback\n                    except Exception:\n                        x_corr = x_model\n\n                    # evaluate both ensemble candidates (model then correction) if budget allows\n                    for x_cand in (x_model, x_corr):\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        # skip if identical to last stored\n                        if len(X) > 0 and np.allclose(x_cand, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_cand)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_cand, x_cand = out\n                        if f_cand < f_best - 1e-12:\n                            f_best = float(f_cand); x_best = x_cand.copy()\n                            improved = True\n                            # more aggressive expansion\n                            trust_radius = np.minimum(trust_radius * self.success_expand, trust_max)\n                            break\n                    if not improved:\n                        # shrink trust more strongly if surrogate misses\n                        trust_radius = np.maximum(trust_radius * self.failure_shrink, self.trust_min * rng_range)\n                except Exception:\n                    # If regression failed, skip surrogate this iteration\n                    pass\n\n            # If surrogate didn't help, directional multi-scale probing with orthogonalization\n            if (not improved) and evals < budget and work_allow > 0:\n                num_directions = int(np.clip(6 + n // 3, 6, 16))\n                base_step = np.linalg.norm(trust_radius) / np.sqrt(float(n) + 1e-12)\n                # generate orthogonalized random directions (Gram-Schmidt on small batch)\n                D = []\n                while len(D) < num_directions:\n                    v = rng.randn(n)\n                    # orthogonalize against existing directions to enhance coverage\n                    for u in D:\n                        v = v - np.dot(u, v) * u\n                    nv = np.linalg.norm(v)\n                    if nv < 1e-12:\n                        # fallback random axis\n                        v = np.zeros(n); v[rng.randint(0, n)] = 1.0; nv = 1.0\n                    D.append(v / nv)\n                scales = np.array([0.125, 0.25, 0.5, 1.0, 2.0])  # more fine scales included\n                rng.shuffle(D)\n                for d in D:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    # generate candidate list along + and - directions\n                    candidates = []\n                    for s in scales:\n                        step = s * base_step\n                        candidates.append(np.clip(x_best + d * step, lb, ub))\n                        candidates.append(np.clip(x_best - d * step, lb, ub))\n                    # evaluate in random order\n                    order = np.arange(len(candidates))\n                    rng.shuffle(order)\n                    for idx in order:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        x_try = candidates[idx]\n                        if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            f_best = float(f_try); x_best = x_try.copy()\n                            improved = True\n                            # moderate expansion\n                            trust_radius = np.minimum(trust_radius * 1.4, trust_max)\n                            break\n                    # if found improvement, break to let surrogate refit in next iteration\n                # end directional probing\n\n            # coordinate-wise local pattern fine-tuning (only a subset if budget tight)\n            if evals < budget and work_allow > 0:\n                coord_step = np.maximum(trust_radius * 0.5, 1e-12)\n                coords = np.arange(n)\n                max_coords = min(n, max(3, int(work_allow // 2)))\n                if max_coords < n:\n                    coords = rng.choice(coords, size=max_coords, replace=False)\n                for i in coords:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    for sign in (+1.0, -1.0):\n                        x_try = x_best.copy()\n                        x_try[i] = x_try[i] + sign * coord_step[i]\n                        x_try = np.clip(x_try, lb, ub)\n                        if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            f_best = float(f_try); x_best = x_try.copy()\n                            improved = True\n                            trust_radius = np.minimum(trust_radius * 1.4, trust_max)\n                            break\n                    # early stop if improved\n                # end coords\n\n            # Heavy-tailed jump (Student-t like). Use df=1.5 for heavier tails than Gaussian but not pure Cauchy\n            if evals < budget and rng.rand() < self.jump_prob:\n                df = 1.5\n                # draw t with df per-dim via scaled normal / chi2\n                g = rng.randn(n)\n                chi2 = rng.chisquare(df, size=1)[0]\n                t_sample = g / np.sqrt(chi2 / df + 1e-12)\n                # clip extremes\n                t_sample = np.clip(t_sample, -15, 15)\n                scale = self.jump_scale_frac * np.maximum(rng_range, 1e-12)\n                x_jump = np.clip(x_best + t_sample * scale, lb, ub)\n                out = safe_eval(x_jump)\n                if out is not None:\n                    f_jump, x_jump = out\n                    if f_jump < f_best - 1e-12:\n                        f_best = float(f_jump); x_best = x_jump.copy()\n                        improved = True\n                        # successful jump expands trust significantly (more exploration)\n                        trust_radius = np.minimum(trust_radius * 2.5, trust_max)\n                    else:\n                        # unsuccessful jump penalizes trust region more strongly\n                        trust_radius = np.maximum(trust_radius * 0.5, self.trust_min * rng_range)\n\n            # Conservative shrink if nothing improved\n            if not improved:\n                trust_radius = np.maximum(trust_radius * self.failure_shrink, self.trust_min * rng_range)\n\n            # archive pruning: keep best K and stratified subset of remainder\n            max_archive = max(3000, 60 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:300]\n                rest_idx = idx_sorted[300:]\n                stride = max(1, len(rest_idx) // (max_archive - 300))\n                keep_rest = rest_idx[::stride]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # main loop continues until budget exhausted\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "aa267358-ed7e-4f49-b4a7-35c55ce9ec9d", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "92e81818-c4d6-4292-a12c-c8029f8bf3a4", "fitness": "-inf", "name": "ASQLevy", "description": "The algorithm is a hybrid adaptive search that combines a small maximin-initialized archive with alternating local quadratic surrogate optimization in low-dimensional PCA subspaces and global/stochastic sampling guided by an empirical covariance and occasional heavy‑tailed teleports. It builds and prunes an archive of evaluated points, computes a PCA basis from elites to fit a regularized quadratic model in the subspace (weighted by proximity), solves the (eigen-shifted) Newton step z* = -H^{-1}g clipped by a subspace trust radius, and adapts trust radii and step sizes on success/failure. When the surrogate is not trusted or insufficient data exists the method draws covariance-guided Gaussian/Student-t samples (with Cholesky fallback), performs coordinate-wise adaptive pattern searches prioritized by per-dimension step pools, and occasionally does Cauchy jumps to escape basins. Practical safeguards include strict budget accounting via safe_eval, bound clipping, ridge and eigenvalue regularization (ridge, h_reg_min), per-iteration limits (max_eval_per_iter), archive size control, and tunable probabilities/scales (init_ratio, cauchy_prob, trust expand/shrink) to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ASQLevy:\n    \"\"\"\n    Adaptive Subspace Quadratic + Lévy-Guided Evolution (ASQ-Levy).\n\n    Main ideas and novel mechanisms:\n    - Build a small, diverse archive initially.\n    - Maintain an archive of evaluated points; derive an adaptive PCA subspace from elites.\n    - Fit a full quadratic surrogate in a low-dimensional subspace (k << n):\n        f(z) ≈ c + g^T z + 0.5 z^T H z,  z = B^T (x - x_center)\n      Solve the local quadratic minimizer with curvature regularization, clamp by subspace trust radius.\n    - When surrogate helps, accept & expand trust; otherwise shrink.\n    - Alternate with covariance-guided Gaussian sampling (CMA-lite) where covariance is adapted from elites,\n      and occasional heavy-tailed t/Cauchy \"teleport\" moves with adaptive scale to escape basins.\n    - Per-dimension step-size pool for coordinate pattern improvements and adaptive success/failure updates.\n    - Archive pruning keeps best and diverse points to stabilize modeling.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_ratio=0.12, min_init=None, max_init_frac=0.4,\n                 subspace_factor=0.5, max_eval_per_iter=80, cauchy_prob=0.12):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # initialization sizes\n        self.init_ratio = float(init_ratio)\n        self.min_init = int(min_init) if min_init is not None else max(10, 2 * self.dim)\n        self.max_init = max(20, int(max_init_frac * self.budget))\n\n        # subspace size factor: k = max(2, int(subspace_factor * sqrt(n) * ...)\n        self.subspace_factor = float(subspace_factor)\n\n        # trust region controls (global scale and per-subspace)\n        self.trust_init_frac = 0.5\n        self.trust_min_frac = 1e-6\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.5\n        self.failure_shrink = 0.7\n\n        # evaluation control\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        self.cauchy_prob = float(cauchy_prob)\n        self.cauchy_scale_frac = 0.35\n\n        # model regularization\n        self.ridge = 1e-8\n        self.h_reg_min = 1e-8\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n        n = int(self.dim)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        center_box = 0.5 * (lb + ub)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archive\n        X = []  # list of arrays\n        F = []\n\n        # best\n        f_best = np.inf\n        x_best = None\n\n        # Determine init budget and sample (space-filling-ish)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        # use simple maximin-ish: draw many and pick well-spread subset\n        pool_mult = 6\n        pool_size = min(max(init_budget * pool_mult, init_budget + 10), 2000)\n        pool = rng.uniform(lb, ub, size=(pool_size, n))\n        # select greedily maxmin\n        chosen = []\n        if pool_size > 0:\n            # start with a random point\n            idx0 = rng.randint(pool_size)\n            chosen.append(idx0)\n            while len(chosen) < init_budget:\n                pts = pool[np.array(chosen)]\n                # distances from pool to chosen set (min)\n                dists = np.min(np.linalg.norm(pool - pts[:, None], axis=2), axis=0)\n                # choose point with max min-dist\n                idx = int(np.argmax(dists))\n                if idx in chosen:\n                    # fallback random\n                    idx = int(rng.randint(pool_size))\n                chosen.append(idx)\n        for i in chosen:\n            if evals >= budget:\n                break\n            x = pool[i].copy()\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n\n        # trust region (per-dimension and subspace radius)\n        trust_radius = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n        trust_subspace = np.linalg.norm(trust_radius) * 0.6  # initial subspace radius (L2 in subspace coords)\n\n        # per-dimension adaptive step sizes for coordinate tweaks\n        coord_steps = np.maximum(0.1 * trust_radius, 1e-9)\n\n        # helper safe eval\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_clipped = np.clip(x, lb, ub)\n            f = func(x_clipped)\n            evals += 1\n            X.append(x_clipped.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x_clipped.copy()\n            return float(f), x_clipped\n\n        # helper: compute elite PCA basis (k dims) using top M elites\n        def compute_pca_basis(X_arr, F_arr, k):\n            # X_arr shape (m, n)\n            m = X_arr.shape[0]\n            if m <= k:\n                # fallback to random orthonormal\n                B = np.eye(n)[:, :k]\n                return B\n            # select elites (lowest F)\n            pick = min(max(2 * n, k + 5), m)\n            idx = np.argsort(F_arr)[:pick]\n            elites = X_arr[idx]\n            # compute centered covariance\n            mu = np.mean(elites, axis=0)\n            C = np.cov((elites - mu).T)\n            # SVD for PCA, handle degenerate cases\n            try:\n                U, S, _ = np.linalg.svd(C)\n            except Exception:\n                # fallback to identity-subspace\n                U = np.eye(n)\n                S = np.ones(n)\n            B = U[:, :k]  # shape (n, k)\n            return B\n\n        # helper: fit quadratic in k-dim subspace around x_center\n        def fit_quadratic_subspace(X_nei, F_nei, B, x_center):\n            # Project neighbors to subspace coords z = B^T (x - x_center)\n            Z = (X_nei - x_center) @ B  # shape (m, k)\n            m, k = Z.shape\n            # Build design matrix with columns: 1, z_i (k), quadratic unique terms z_i*z_j for i<=j (k*(k+1)/2)\n            q_terms = k * (k + 1) // 2\n            P = 1 + k + q_terms\n            M = np.ones((m, P))\n            M[:, 1:1 + k] = Z\n            # fill quadratic terms\n            idx = 1 + k\n            for i in range(k):\n                for j in range(i, k):\n                    M[:, idx] = Z[:, i] * Z[:, j]\n                    idx += 1\n            y = np.asarray(F_nei).reshape(-1)\n            # weighting by proximity in subspace (closer z smaller norm -> larger weight)\n            z_norm = np.linalg.norm(Z, axis=1)\n            w = 1.0 / (1e-12 + z_norm)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            b = W * y\n            # ridge\n            ridge_mat = self.ridge * np.eye(P)\n            try:\n                theta, *_ = np.linalg.lstsq(A.T @ A + ridge_mat, A.T @ b, rcond=None)\n                theta = theta.flatten()\n            except Exception:\n                return None\n            c = theta[0]\n            g = theta[1:1 + k].copy()\n            # reconstruct symmetric H of size kxk from packed quadratic coefficients\n            H = np.zeros((k, k))\n            idx = 1 + k\n            for i in range(k):\n                for j in range(i, k):\n                    val = theta[idx]\n                    if i == j:\n                        H[i, j] = val * 2.0  # because quadratic terms in model are z_i*z_j (we want 0.5 z^T H z)\n                    else:\n                        H[i, j] = val\n                        H[j, i] = val\n                    idx += 1\n            # Convert so that model is c + g^T z + 0.5 z^T H z\n            # Currently H is 2*diag entries for i==j; we made diag double so that 0.5*H gives correct diag\n            return c, g, H\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            improved = False\n\n            # decide subspace size k based on dimension and archive richness\n            k = max(1, int(min(self.dim, max(2, int(self.subspace_factor * np.sqrt(max(1, self.dim)) * 2)))))\n            # prepare arrays\n            X_arr = np.asarray(X) if len(X) > 0 else np.zeros((0, n))\n            F_arr = np.asarray(F) if len(F) > 0 else np.zeros((0,))\n\n            # Attempt subspace quadratic surrogate if enough neighbours\n            neighbors_needed = max(2 * n + 5, 6 + k * (k + 3) // 2)\n            if len(X) >= neighbors_needed:\n                # pick nearest neighbors to x_best (or center=archive mean of elites)\n                x_center = x_best.copy() if x_best is not None else center_box.copy()\n                dists = np.linalg.norm(X_arr - x_center, axis=1)\n                idx_sorted = np.argsort(dists)[:min(len(X), neighbors_needed * 2)]\n                X_nei = X_arr[idx_sorted]\n                F_nei = F_arr[idx_sorted]\n                # compute PCA basis B (n x k)\n                B = compute_pca_basis(X_arr, F_arr, k)\n                # fit quadratic in subspace - using a trimmed neighbor set closest in subspace coords\n                fit = fit_quadratic_subspace(X_nei, F_nei, B, x_center)\n                if fit is not None:\n                    c, g, H = fit\n                    # regularize H to be positive definite-ish: eigen-shift\n                    try:\n                        # symmetrize\n                        H = 0.5 * (H + H.T)\n                        eigvals, eigvecs = np.linalg.eigh(H)\n                        min_ev = np.min(eigvals)\n                        if min_ev < self.h_reg_min:\n                            shift = (self.h_reg_min - min_ev) + 1e-8\n                            H = H + shift * np.eye(k)\n                        # Solve H z = -g for Newton minimizer in subspace\n                        z_star = -np.linalg.solve(H, g)\n                    except Exception:\n                        z_star = -g  # fallback to gradient step\n                    # clamp to trust_subspace radius\n                    z_norm = np.linalg.norm(z_star)\n                    if z_norm > 0:\n                        if z_norm > trust_subspace:\n                            z_star = z_star * (trust_subspace / (z_norm + 1e-20))\n                    # map back to original space\n                    x_model = x_center + (B @ z_star)\n                    x_model = np.clip(x_model, lb, ub)\n                    # evaluate if budget allows and candidate is not trivially duplicate\n                    if work_allow > 0 and evals < budget:\n                        if len(X) == 0 or not np.allclose(x_model, X[-1], atol=1e-12):\n                            out = safe_eval(x_model)\n                            work_allow -= 1\n                            if out is not None:\n                                f_model, x_model = out\n                                if f_model < f_best - 1e-12:\n                                    improved = True\n                                    # expand trust\n                                    trust_subspace = min(trust_subspace * self.success_expand, np.linalg.norm(rng_range) * self.trust_max_frac + 1e-12)\n                                    trust_radius = np.minimum(trust_radius * self.success_expand, self.trust_max_frac * rng_range)\n                                    # update coord steps\n                                    coord_steps = np.minimum(coord_steps * 1.2, self.trust_max_frac * rng_range)\n                                else:\n                                    # shrink on failure\n                                    trust_subspace = max(trust_subspace * self.failure_shrink, np.linalg.norm(rng_range) * self.trust_min_frac)\n                                    trust_radius = np.maximum(trust_radius * self.failure_shrink, self.trust_min_frac * rng_range)\n\n            # If surrogate not improving or not used, perform covariance-guided sampling + coordinate tweaks\n            if not improved and evals < budget and work_allow > 0:\n                # Build covariance from elites (top M)\n                m_cov = min(max(4 * n, 20), len(X)) if len(X) > 0 else 0\n                if m_cov >= 2:\n                    idx_e = np.argsort(F)[:m_cov]\n                    elites = X_arr[idx_e]\n                    mu = np.mean(elites, axis=0)\n                    C = np.cov((elites - mu).T)\n                    # ensure positive definite and scale by trust_radius\n                    # scale diagonal by trust_radius^2 to keep step-size consistent\n                    scale_diag = (trust_radius ** 2)\n                    C = C + np.diag(scale_diag) * 0.1 + (1e-12 * np.eye(n))\n                else:\n                    mu = x_best.copy() if x_best is not None else center_box.copy()\n                    C = np.diag(trust_radius ** 2 + 1e-12)\n                # draw lambda samples (dependent on dimension)\n                lam = int(min(20 + n, max(6, work_allow)))\n                try:\n                    L = np.linalg.cholesky(C + 1e-12 * np.eye(n))\n                except Exception:\n                    # fallback to diag\n                    L = np.diag(np.sqrt(np.diag(C) + 1e-12))\n                # sample half gaussian, half heavy-tailed\n                for i in range(lam):\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    if rng.rand() < 0.6:\n                        z = rng.randn(n)\n                        x_try = mu + (L @ z)\n                    else:\n                        # student-t like heavy tail (v degrees)\n                        v = 3.0\n                        g = rng.randn(n)\n                        chi = np.sqrt(v / np.random.chisquare(v))\n                        x_try = mu + (L @ (g * chi))\n                    x_try = np.clip(x_try, lb, ub)\n                    if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                        continue\n                    out = safe_eval(x_try)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    f_try, x_try = out\n                    if f_try < f_best - 1e-12:\n                        improved = True\n                        # on success, increase trust a bit\n                        trust_radius = np.minimum(trust_radius * 1.3, self.trust_max_frac * rng_range)\n                        trust_subspace = min(trust_subspace * 1.4, np.linalg.norm(rng_range) * self.trust_max_frac)\n                        coord_steps = np.minimum(coord_steps * 1.2, self.trust_max_frac * rng_range)\n                        break\n\n            # Coordinate-wise adaptive pattern search using coord_steps\n            if evals < budget and work_allow > 0:\n                # select a subset of coordinates to try (prioritize large coord_steps)\n                coords_order = np.argsort(-coord_steps)\n                max_coords = min(n, max(3, int(work_allow // 2)))\n                coords = coords_order[:max_coords]\n                for i in coords:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    for sign in (+1.0, -1.0):\n                        x_try = x_best.copy()\n                        step = coord_steps[i] * sign\n                        x_try[i] = np.clip(x_try[i] + step, lb[i], ub[i])\n                        if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            improved = True\n                            # increase this coordinate's step\n                            coord_steps[i] = min(coord_steps[i] * 1.5, self.trust_max_frac * rng_range[i])\n                            # slightly expand trust radii\n                            trust_radius[i] = min(trust_radius[i] * 1.2, self.trust_max_frac * rng_range[i])\n                            break\n                        else:\n                            # shrink this coordinate's step on failure\n                            coord_steps[i] = max(coord_steps[i] * 0.85, self.trust_min_frac * rng_range[i])\n\n            # Occasional heavy-tailed teleport (Cauchy-like scaled per-dim)\n            if evals < budget and rng.rand() < self.cauchy_prob and work_allow > 0:\n                scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                jump = rng.standard_cauchy(size=n)\n                # clip extreme quantiles to keep numeric stability\n                jump = np.clip(jump, -10, 10)\n                x_jump = x_best + jump * scale\n                x_jump = np.clip(x_jump, lb, ub)\n                out = safe_eval(x_jump)\n                work_allow -= 1\n                if out is not None:\n                    f_jump, x_jump = out\n                    if f_jump < f_best - 1e-12:\n                        improved = True\n                        # reward by boosting trust and steps\n                        trust_radius = np.minimum(trust_radius * 2.0, self.trust_max_frac * rng_range)\n                        trust_subspace = min(trust_subspace * 2.0, np.linalg.norm(rng_range) * self.trust_max_frac)\n                        coord_steps = np.minimum(coord_steps * 1.5, self.trust_max_frac * rng_range)\n                    else:\n                        # moderate shrink\n                        trust_radius = np.maximum(trust_radius * 0.8, self.trust_min_frac * rng_range)\n                        coord_steps = np.maximum(coord_steps * 0.9, self.trust_min_frac * rng_range)\n\n            # If nothing improved, conservative shrink\n            if not improved:\n                trust_radius = np.maximum(trust_radius * self.failure_shrink, self.trust_min_frac * rng_range)\n                trust_subspace = max(trust_subspace * self.failure_shrink, np.linalg.norm(rng_range) * self.trust_min_frac)\n                coord_steps = np.maximum(coord_steps * 0.95, self.trust_min_frac * rng_range)\n\n            # Archive pruning: keep best and diverse subset to limit memory and improve modeling\n            max_archive = max(1000, 40 * n)\n            if len(X) > max_archive:\n                X_arr = np.asarray(X)\n                F_arr = np.asarray(F)\n                idx_sorted = np.argsort(F_arr)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                if len(rest_idx) > 0:\n                    stride = max(1, len(rest_idx) // (max_archive - 200))\n                    keep_rest = rest_idx[::stride]\n                    keep_idx = np.concatenate([keep_best, keep_rest])[:max_archive]\n                else:\n                    keep_idx = keep_best[:max_archive]\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # loop continues until budget exhausted\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 275, in __call__, the following error occurred:\nIndexError: index 12 is out of bounds for axis 0 with size 12\nOn line: elites = X_arr[idx_e]", "error": "In the code, line 275, in __call__, the following error occurred:\nIndexError: index 12 is out of bounds for axis 0 with size 12\nOn line: elites = X_arr[idx_e]", "parent_ids": "aa267358-ed7e-4f49-b4a7-35c55ce9ec9d", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9d8d8441-8925-4057-aa04-c895045b6087", "fitness": "-inf", "name": "ABSTARS", "description": "1) The core local model is a blockwise separable quadratic surrogate fitted by weighted ridge least-squares around the current best point (block_fraction≈0.35, model_neighbor_multiplier=6) with diagonal Hessian regularization and Newton-like steps clipped by per-dimension trust radii (trust_init_frac=0.4, trust_max_frac=2.5) to produce reliable bounded steps.  \n2) When surrogates fail the algorithm falls back to multi-scale mirrored directional probes (geometric scales 0.125→2.0, num_directions grows with dim) and a greedy coordinate pattern search that use the trust radii as adaptive step sizes, with max_eval_per_iter (80) limiting inner-loop cost.  \n3) Global exploration is supported by a relatively large LHS-like initialization (init_samples_ratio=0.20, min_init biased upward) and occasional heavy-tailed Student‑t restarts (student_t_jump_prob=0.10, df=3, scale_frac=0.6) to escape local traps.  \n4) Practical robustness is added via distance-weighted fitting, small ridge (1e-6) to stabilize linear solves, multiplicative trust radius expand/shrink rules (success_expand=1.3, failure_shrink=0.6), strict budget enforcement and archive pruning to keep memory and evaluation behavior controlled.", "code": "import numpy as np\n\nclass ABSTARS:\n    \"\"\"\n    Adaptive Blockwise Student-t Restart Search (ABSTARS)\n\n    Main idea (one-line): Fit small blockwise separable quadratic surrogates around the best point,\n    propose bounded Newton-like steps, use mirrored multi-scale directional probes, and escape\n    local traps with occasional heavy-tailed Student-t restarts — all under a strict evaluation budget.\n\n    Notes on main algorithm parameters (defaults chosen differently from the provided LSDS algorithm):\n      - init_samples_ratio = 0.20     (fraction of budget used for initialization; larger than original)\n      - min_init = max(12, 3*dim)     (stronger initial sampling for low-dim problems)\n      - max_init = min(500, int(0.25*budget))\n      - model_neighbor_multiplier = 6 (neighbors = multiplier * dim; different multiplier)\n      - trust_init_frac = 0.4         (initial trust radius fraction of range, per-dim)\n      - trust_min = 1e-8\n      - trust_max_frac = 2.5          (max trust radius relative to range)\n      - success_expand = 1.3          (smaller expansion than LSDS)\n      - failure_shrink = 0.6          (slightly stronger shrink)\n      - student_t_jump_prob = 0.10    (probability for heavy jump)\n      - student_t_df = 3.0            (degrees of freedom for Student-t tails)\n      - student_t_scale_frac = 0.6    (scale fraction of range, a bit larger)\n      - block_fraction = 0.35         (fraction of dimensions in a block for blockwise surrogate)\n      - max_eval_per_iter = 80        (limit evaluations per outer iteration)\n      - ridge = 1e-6                  (ridge regularization for regressions)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 # tunable parameters (exposed)\n                 init_samples_ratio=0.20,\n                 min_init=None, max_init=None,\n                 model_neighbor_multiplier=6,\n                 trust_init_frac=0.4, trust_min=1e-8, trust_max_frac=2.5,\n                 success_expand=1.3, failure_shrink=0.6,\n                 student_t_jump_prob=0.10, student_t_df=3.0, student_t_scale_frac=0.6,\n                 block_fraction=0.35,\n                 max_eval_per_iter=80,\n                 ridge=1e-6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # Parameters\n        self.init_samples_ratio = float(init_samples_ratio)\n        self.min_init = min_init if min_init is not None else max(12, 3 * self.dim)\n        self.max_init = max_init if max_init is not None else min(500, int(0.25 * self.budget))\n        self.model_neighbor_multiplier = int(model_neighbor_multiplier)\n        self.trust_init_frac = float(trust_init_frac)\n        self.trust_min = float(trust_min)\n        self.trust_max_frac = float(trust_max_frac)\n        self.success_expand = float(success_expand)\n        self.failure_shrink = float(failure_shrink)\n        self.student_t_jump_prob = float(student_t_jump_prob)\n        self.student_t_df = float(student_t_df)\n        self.student_t_scale_frac = float(student_t_scale_frac)\n        self.block_fraction = float(block_fraction)\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        self.ridge = float(ridge)\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n        n = self.dim\n\n        # Respect func bounds if provided; otherwise default to [-5,5]^n (as required)\n        if hasattr(func, \"bounds\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        else:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # Archive\n        X = []\n        F = []\n\n        f_best = np.inf\n        x_best = None\n\n        # Determine initial sampling budget (LHS-like sampling)\n        init_budget = int(np.clip(self.init_samples_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n\n        # Latin Hypercube-ish initial sampling for better space-filling\n        def lhs_samples(m, n):\n            # returns m x n samples in [0,1]\n            rng_local = rng\n            cut = np.linspace(0.0, 1.0, m + 1)\n            u = rng_local.uniform(size=(m, n))\n            a = cut[:m].reshape(m, 1)\n            b = cut[1:m + 1].reshape(m, 1)\n            pts = a + u * (b - a)\n            # permute in each dimension\n            for j in range(n):\n                rng_local.shuffle(pts[:, j])\n            return pts\n\n        # initial sampling\n        m = init_budget\n        uv = lhs_samples(m, n)\n        for i in range(m):\n            x = lb + uv[i] * rng_range\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # trust radius per-dim\n        trust_radius = np.maximum(self.trust_init_frac * rng_range, 1e-12)\n        trust_max = np.maximum(self.trust_max_frac * rng_range, 1e-12)\n\n        # helper: safe evaluate\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return None\n            x_clipped = np.clip(x, lb, ub)\n            f = func(x_clipped)\n            evals += 1\n            X.append(x_clipped.copy()); F.append(float(f))\n            return float(f), x_clipped\n\n        # helper: nearest neighbors indices (Euclidean)\n        def nearest_indices(x_query, k):\n            if len(X) == 0:\n                return np.array([], dtype=int)\n            X_arr = np.asarray(X)\n            d = np.linalg.norm(X_arr - x_query, axis=1)\n            idx = np.argsort(d)[:min(k, len(d))]\n            return idx\n\n        # Outer loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            improved_in_iter = False\n\n            # Choose block of coordinates randomly (blockwise surrogate)\n            block_size = max(1, int(np.ceil(self.block_fraction * n)))\n            if block_size >= n:\n                blocks = [np.arange(n)]\n            else:\n                # sample a random contiguous-ish block or random indices\n                start = rng.randint(0, n)\n                if start + block_size <= n:\n                    blocks = [np.arange(start, start + block_size)]\n                else:\n                    # wrap-around block\n                    blocks = [np.concatenate((np.arange(start, n), np.arange(0, (start + block_size) % n)))]\n                # occasionally use a random scattered block\n                if rng.rand() < 0.3:\n                    blocks = [rng.choice(n, size=block_size, replace=False)]\n\n            # For each block try to fit a small separable quadratic surrogate (only on block dims)\n            neighbors_needed = max(3 * block_size + 1, self.model_neighbor_multiplier * block_size)\n            for block in blocks:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                block = np.asarray(block, dtype=int)\n                # Prepare neighbors around x_best\n                idxs = nearest_indices(x_best, neighbors_needed)\n                if len(idxs) < (block.size + 2):\n                    continue\n                X_nei = np.asarray(X)[idxs]\n                F_nei = np.asarray(F)[idxs]\n\n                # Build design matrix for separable quadratic in block dims\n                dx = X_nei - x_best  # offsets\n                m_pts = dx.shape[0]\n                p = 1 + block.size + block.size  # a + linear + 0.5*diag quadratic\n                M = np.ones((m_pts, p))\n                # linear terms (for block dims)\n                M[:, 1:1 + block.size] = dx[:, block]\n                # quadratic diag terms (0.5 * dx^2)\n                M[:, 1 + block.size:1 + 2 * block.size] = 0.5 * (dx[:, block] ** 2)\n                y = F_nei\n\n                # weights based on distance in full space but emphasize block dims\n                d_full = np.linalg.norm(X_nei - x_best, axis=1)\n                w = 1.0 / (d_full + 1e-12)\n                w = w / (np.max(w) + 1e-12)\n                W = np.sqrt(w)[:, None]\n                A = W * M\n                b = W * y\n\n                # ridge solve\n                try:\n                    lhs = A.T @ A + self.ridge * np.eye(p)\n                    rhs = A.T @ b\n                    params = np.linalg.solve(lhs, rhs).flatten()\n                    a0 = params[0]\n                    b_lin = params[1:1 + block.size]\n                    h_diag = params[1 + block.size:1 + 2 * block.size]\n\n                    # ensure minimal positive curvature for block dims\n                    h_reg = np.array(h_diag, copy=True)\n                    h_reg[h_reg < 1e-8] = 1e-8\n\n                    # Newton step for block dims\n                    delta_block = - b_lin / (h_reg + 1e-20)\n\n                    # limit block delta by trust region for these dims\n                    limited = np.clip(delta_block, -trust_radius[block], trust_radius[block])\n\n                    # form candidate\n                    x_model = x_best.copy()\n                    x_model[block] = x_model[block] + limited\n                    x_model = np.clip(x_model, lb, ub)\n\n                    # evaluate candidate if budget allows and it's distinct\n                    if evals < budget and work_allow > 0:\n                        if len(X) == 0 or not np.allclose(x_model, X[-1], atol=1e-12):\n                            out = safe_eval(x_model)\n                            work_allow -= 1\n                            if out is None:\n                                break\n                            f_model, x_model = out\n                            if f_model < f_best - 1e-12:\n                                f_best = float(f_model); x_best = x_model.copy()\n                                improved_in_iter = True\n                                # moderate trust increase for block success\n                                trust_radius = np.minimum(trust_radius * self.success_expand, trust_max)\n                            else:\n                                # contract trust for the affected block dims\n                                trust_radius[block] = np.maximum(trust_radius[block] * self.failure_shrink, self.trust_min)\n                except np.linalg.LinAlgError:\n                    # ill-conditioned => skip block surrogate\n                    continue\n\n            # If no surrogate success, perform mirrored multi-scale directional probes\n            if not improved_in_iter and evals < budget and work_allow > 0:\n                # number directions adaptively: grow with dimension but cap\n                num_directions = int(np.clip(6 + n // 3, 6, 16))\n                # base step is geometric mean of trust radii\n                base_step = np.exp(np.mean(np.log(np.maximum(trust_radius, 1e-12))))\n                directions = []\n                for _ in range(num_directions):\n                    v = rng.randn(n)\n                    v /= (np.linalg.norm(v) + 1e-12)\n                    directions.append(v)\n                # include coordinate directions sometimes\n                if rng.rand() < 0.4:\n                    for i in range(min(n, 4)):\n                        e = np.zeros(n); e[i] = 1.0\n                        directions.append(e)\n                scales = np.array([0.125, 0.25, 0.5, 1.0, 2.0])  # geometric relative to base_step\n\n                rng.shuffle(directions)\n                for d in directions:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    # mirrored probes + and -, evaluate in increasing scale until a success\n                    for s in scales:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        step = s * base_step\n                        for sign in (+1.0, -1.0):\n                            x_try = x_best + sign * d * step\n                            x_try = np.clip(x_try, lb, ub)\n                            if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                continue\n                            out = safe_eval(x_try)\n                            work_allow -= 1\n                            if out is None:\n                                break\n                            f_try, x_try = out\n                            if f_try < f_best - 1e-12:\n                                f_best = float(f_try); x_best = x_try.copy()\n                                improved_in_iter = True\n                                # expand trust modestly\n                                trust_radius = np.minimum(trust_radius * 1.25, trust_max)\n                                break\n                        if improved_in_iter:\n                            break\n                    if improved_in_iter:\n                        break\n\n            # Coordinate pattern local search (greedy) when budget permits\n            if evals < budget and work_allow > 0:\n                coord_indices = np.arange(n)\n                # choose subset based on availability\n                max_coords = min(n, max(4, int(work_allow // 3)))\n                if max_coords < n:\n                    coord_indices = rng.choice(coord_indices, size=max_coords, replace=False)\n                for i in coord_indices:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    # try both signs with step equal to trust radius in that coordinate\n                    step = max(trust_radius[i], 1e-12)\n                    for sign in (+1.0, -1.0):\n                        x_try = x_best.copy()\n                        x_try[i] = x_try[i] + sign * step\n                        x_try = np.clip(x_try, lb, ub)\n                        if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            f_best = float(f_try); x_best = x_try.copy()\n                            improved_in_iter = True\n                            trust_radius = np.minimum(trust_radius * 1.25, trust_max)\n                            break\n                    if improved_in_iter:\n                        break\n\n            # Occasionally attempt Student-t heavy-tailed jump for escapes\n            if evals < budget and rng.rand() < self.student_t_jump_prob:\n                # generate student-t with df self.student_t_df\n                # use scaled independent Student-t per-dim\n                # draw as standard normals / sqrt(chi2/df)\n                z = rng.randn(n)\n                chi2 = rng.chisquare(self.student_t_df, size=n)\n                t = z / (np.sqrt(chi2 / self.student_t_df) + 1e-12)\n                scale = self.student_t_scale_frac * np.maximum(rng_range, 1e-12)\n                jump = np.clip(t, -10, 10) * scale\n                x_jump = np.clip(x_best + jump, lb, ub)\n                if len(X) == 0 or not np.allclose(x_jump, X[-1], atol=1e-12):\n                    out = safe_eval(x_jump)\n                    if out is not None:\n                        f_jump, x_jump = out\n                        if f_jump < f_best - 1e-12:\n                            f_best = float(f_jump); x_best = x_jump.copy()\n                            improved_in_iter = True\n                            # successful jump: reset trust to somewhat larger around mid-range\n                            trust_radius = np.minimum(np.maximum(trust_radius * 2.0, 0.3 * rng_range), trust_max)\n                        else:\n                            # unsuccessful: shrink trust moderately\n                            trust_radius = np.maximum(trust_radius * 0.7, self.trust_min)\n\n            # If nothing improved, shrink trust more aggressively to encourage exploration\n            if not improved_in_iter:\n                trust_radius = np.maximum(trust_radius * self.failure_shrink, self.trust_min)\n\n            # Archive pruning to keep memory modest\n            max_archive = max(1500, 40 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best_count = min(250, max_archive // 5)\n                keep_best = idx_sorted[:keep_best_count]\n                rest = idx_sorted[keep_best_count:]\n                if len(rest) > 0:\n                    stride = max(1, len(rest) // (max_archive - keep_best_count))\n                    keep_rest = rest[::stride]\n                    keep_idx = np.concatenate([keep_best, keep_rest])\n                else:\n                    keep_idx = keep_best\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # safety: if budget nearly exhausted do random scatter around best to try final improvements\n            if remaining <= 5 and evals < budget:\n                while evals < budget:\n                    perturb = rng.randn(n) * (0.05 * np.maximum(rng_range, 1e-12))\n                    x_try = np.clip(x_best + perturb, lb, ub)\n                    out = safe_eval(x_try)\n                    if out is None:\n                        break\n                    f_try, x_try = out\n                    if f_try < f_best - 1e-12:\n                        f_best = float(f_try); x_best = x_try.copy()\n                    # don't overrun\n            # end outer iteration\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "aa267358-ed7e-4f49-b4a7-35c55ce9ec9d", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a9759013-c0d4-4e3f-816c-6216ddcbcef5", "fitness": 0.3616330488941156, "name": "EnsembleAdaptiveSurrogateLevy", "description": "The algorithm maintains a small ensemble of promising \"centers\" drawn from a space-filling initialization and an archive, with per-center adaptive trust radii and a replacement policy (center_replace_patience=8) to keep diversity. Around each center it fits cheap local surrogates (a distance-weighted separable quadratic and a linear model—model_neighbor_multiplier=8 and tiny ridge=1e-6 emphasize many neighbors with light regularization) and proposes clipped surrogate minimizers plus perturbed ensemble variants. If surrogates fail it falls back to multi‑scale directional probes (direction_scales = [0.25,0.5,1,2]), coordinate tweaks, and occasional heavy‑tailed Cauchy jumps (cauchy_prob=0.12, cauchy_scale_frac=0.45) to escape basins, while adapting trust radii by success_expand=1.6 and failure_shrink=0.65. Practical controls include a capped per-iteration evaluation budget (max_eval_per_iter=60), an initial sampling fraction (init_ratio=0.12, bounded by min_init/max_init), archive pruning (max_archive = max(2000,50*n)), and strict safe_eval enforcement of overall budget and bounds.", "code": "import numpy as np\n\nclass EnsembleAdaptiveSurrogateLevy:\n    \"\"\"\n    Ensemble Adaptive Surrogate + Lévy-Enhanced Directional Search (EASL)\n\n    Main idea (one-line): Maintain a small ensemble of promising centers, fit cheap local surrogate\n    ensembles (separable quadratic + linear) around each center, propose and evaluate trusted\n    surrogate minimizers, fallback to multi-scale directional and coordinate probes, and use\n    occasional Lévy/Cauchy jumps and adaptive per-dimension trust radii to escape basins.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # Initialization sizing\n        self.init_ratio = 0.12\n        self.min_init = max(10, 2 * self.dim)\n        self.max_init = min(400, int(0.4 * self.budget))\n\n        # Ensemble & modeling\n        self.ensemble_size = max(2, min(6, self.dim // 2 + 1))\n        self.model_neighbor_multiplier = 8  # neighbors = multiplier * dim\n        self.ridge = 1e-6\n\n        # trust region behavior\n        self.trust_init_frac = 0.5\n        self.trust_min = 1e-6\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.6\n        self.failure_shrink = 0.65\n\n        # directional probing\n        self.max_eval_per_iter = 60\n        self.direction_scales = np.array([0.25, 0.5, 1.0, 2.0])\n\n        # Lévy / heavy-tail jumps\n        self.cauchy_prob = 0.12\n        self.cauchy_scale_frac = 0.45\n\n        # center management\n        self.max_centers = max(3, min(12, self.ensemble_size * 2))\n        self.center_replace_patience = 8  # iterations with no improvement to replace center\n\n        # randomness\n        self.rng = np.random.RandomState(self.seed)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # Archive\n        X = []\n        F = []\n\n        # Best so far\n        f_best = np.inf\n        x_best = None\n\n        # Initialize with a small space-filling sample (uniform + jitter)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # Initialize centers: pick best few distinct points\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                # avoid near-duplicates\n                if all(np.linalg.norm(x - c) > 1e-8 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.max_centers, len(X)))\n        if len(centers) == 0:\n            centers = [np.array(self.rng.uniform(lb, ub))]\n\n        # per-center trust radii and metadata\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n        stagnation = [0 for _ in centers]\n\n        # safe eval helper that obeys budget and bounds and updates archive\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # helper: fit separable quadratic (a + b^T dx + 0.5 * h diag dx^2) around center\n        def fit_separable_quad(center, neighbors_X, neighbors_F):\n            # neighbors_* are arrays\n            dx = neighbors_X - center\n            m = dx.shape[0]\n            M = np.ones((m, 1 + 2 * n))\n            M[:, 1:1 + n] = dx\n            M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n            y = neighbors_F\n            # weights by distance\n            dists = np.linalg.norm(dx, axis=1)\n            w = 1.0 / (dists + 1e-12)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            bvec = W * y\n            try:\n                ridge = self.ridge * np.eye(M.shape[1])\n                params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ bvec, rcond=None)\n                params = params.flatten()\n                a = params[0]\n                b_lin = params[1:1 + n]\n                h_diag = params[1 + n:1 + 2 * n]\n                # regularize curvatures\n                h_reg = np.copy(h_diag)\n                h_reg[h_reg < 1e-8] = 1e-8\n                return a, b_lin, h_reg\n            except Exception:\n                return None\n\n        # main loop\n        iter_since_center_refresh = 0\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            # periodically refresh centers from archive (to diversify)\n            if (iter_since_center_refresh % 6) == 0:\n                topk = min(self.max_centers, max(1, len(X)//10))\n                new_centers = get_top_centers(topk)\n                # Ensure we keep at least existing centers if they are good\n                centers = list({tuple(c): c for c in (centers + new_centers)}.values())[:self.max_centers]\n                # reset expectation vectors if centers list changed size\n                if len(trust_radius) != len(centers):\n                    trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n                    stagnation = [0 for _ in centers]\n            iter_since_center_refresh += 1\n\n            improved_global = False\n\n            # iterate centers in random order (but deterministic per RNG)\n            center_order = list(range(len(centers)))\n            self.rng.shuffle(center_order)\n\n            for cidx in center_order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[cidx])\n                tr = trust_radius[cidx]\n\n                # gather neighbors\n                if len(X) >= max(2 * n + 1, self.model_neighbor_multiplier * n):\n                    X_arr = np.asarray(X)\n                    F_arr = np.asarray(F)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), max(2 * n + 1, self.model_neighbor_multiplier * n))\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n\n                    # Fit two cheap models: separable quadratic and global linear (over neighbors)\n                    quad = fit_separable_quad(center, X_nei, F_nei)\n                    # linear fit\n                    try:\n                        A_lin = np.hstack([np.ones((X_nei.shape[0], 1)), X_nei - center])\n                        params_lin, *_ = np.linalg.lstsq(A_lin, F_nei, rcond=None)\n                        a_lin = params_lin[0]\n                        b_lin_only = params_lin[1:].flatten()\n                    except Exception:\n                        params_lin = None\n                        b_lin_only = None\n\n                    proposals = []\n                    # from quadratic model propose minimizer\n                    if quad is not None:\n                        a_q, b_q, h_q = quad\n                        delta_q = -b_q / (h_q + 1e-20)\n                        delta_q = np.clip(delta_q, -tr, tr)\n                        x_q = np.clip(center + delta_q, lb, ub)\n                        proposals.append(('quad', x_q))\n                    # from linear propose step opposite to linear gradient estimate scaled by trust\n                    if b_lin_only is not None:\n                        step = -0.5 * np.sign(b_lin_only) * tr  # move along sign of descent\n                        x_lin = np.clip(center + step, lb, ub)\n                        proposals.append(('lin', x_lin))\n\n                    # also include ensemble stochastic proposals: small random weighted mixes of proposals\n                    if proposals:\n                        # score proposals by predicted improvement (cheap proxies)\n                        # we'll evaluate top-K proposals in order\n                        # add slight perturbations to proposals to diversify\n                        ensemble_props = []\n                        for name, xprop in proposals:\n                            ensemble_props.append(xprop)\n                            # add perturbed variants\n                            for s in (0.3, 0.6):\n                                jitter = (self.rng.randn(n) * 0.05) * tr\n                                ensemble_props.append(np.clip(xprop + s * jitter, lb, ub))\n                        # unique them\n                        unique_props = []\n                        for p in ensemble_props:\n                            if not any(np.allclose(p, q, atol=1e-12) for q in unique_props):\n                                unique_props.append(p)\n                        # rank proposals by distance from center (closer first)\n                        unique_props.sort(key=lambda z: np.linalg.norm(z - center))\n                        # Evaluate proposals until success or work_allow exhausted\n                        for xprop in unique_props:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            out = safe_eval(xprop)\n                            work_allow -= 1\n                            if out is None:\n                                break\n                            fprop, xprop = out\n                            if fprop < f_best - 1e-12:\n                                improved_global = True\n                                # update center to new better point\n                                centers[cidx] = xprop.copy()\n                                trust_radius[cidx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                                stagnation[cidx] = 0\n                                break\n                        # end proposals\n                    # if models did not improve, fallback to directional probes\n                    if not improved_global and work_allow > 0 and evals < budget:\n                        # directional multi-scale probes around center\n                        num_dirs = int(np.clip(4 + n // 2, 4, 12))\n                        dirs = []\n                        for _ in range(num_dirs):\n                            v = self.rng.randn(n)\n                            nv = np.linalg.norm(v)\n                            if nv == 0:\n                                v = np.ones(n); nv = np.linalg.norm(v)\n                            dirs.append(v / nv)\n                        self.rng.shuffle(dirs)\n                        base_step = np.linalg.norm(tr) / np.sqrt(float(n) + 1e-12)\n                        for d in dirs:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            # build candidates along + and - of multiple scales\n                            cand = []\n                            for s in self.direction_scales:\n                                step = s * base_step\n                                cand.append(np.clip(center + d * step, lb, ub))\n                                cand.append(np.clip(center - d * step, lb, ub))\n                            self.rng.shuffle(cand)\n                            for x_try in cand:\n                                if work_allow <= 0 or evals >= budget:\n                                    break\n                                # skip if identical to last\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.25, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n                            # end cand loop\n                        # end dirs\n                    # coordinate-wise small adjustments if still no improvement\n                    if not improved_global and work_allow > 0 and evals < budget:\n                        coord_step = np.maximum(tr, 1e-12)\n                        coords = np.arange(n)\n                        max_coords = min(n, max(3, int(work_allow // 2)))\n                        if max_coords < n:\n                            coords = self.rng.choice(coords, size=max_coords, replace=False)\n                        for i in coords:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            for sign in (+1.0, -1.0):\n                                x_try = centers[cidx].copy()\n                                x_try[i] = x_try[i] + sign * coord_step[i]\n                                x_try = np.clip(x_try, lb, ub)\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.2, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n                            # early break if improved\n                    # end coordinate tweak\n\n                    # occasional heavy-tailed jump from this center\n                    if work_allow > 0 and evals < budget and self.rng.rand() < self.cauchy_prob:\n                        scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                        jump = self.rng.standard_cauchy(size=n)\n                        jump = np.clip(jump, -10, 10)\n                        x_jump = np.clip(centers[cidx] + jump * scale, lb, ub)\n                        out = safe_eval(x_jump)\n                        if out is not None:\n                            work_allow -= 1\n                            fjump, xjump = out\n                            if fjump < f_best - 1e-12:\n                                improved_global = True\n                                centers[cidx] = xjump.copy()\n                                trust_radius[cidx] = np.minimum(tr * 2.0, self.trust_max_frac * rng_range)\n                                stagnation[cidx] = 0\n                            else:\n                                trust_radius[cidx] = np.maximum(tr * 0.8, self.trust_min * rng_range)\n\n                    # if no improvement for this center increment stagnation and shrink trust\n                    if not improved_global:\n                        stagnation[cidx] += 1\n                        trust_radius[cidx] = np.maximum(tr * self.failure_shrink, self.trust_min * rng_range)\n                    else:\n                        # decrease the stagnation counter for other centers only on global improvement\n                        # (we already reset this center above)\n                        pass\n\n                else:\n                    # Not enough data to build models: do exploratory probes from center\n                    base_step = np.linalg.norm(trust_radius[0]) / np.sqrt(float(n) + 1e-12)\n                    for _ in range(min(6, work_allow)):\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        s = self.rng.choice(self.direction_scales)\n                        x_try = np.clip(center + d * s * base_step, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xtry.copy()\n                            trust_radius[cidx] = np.minimum(trust_radius[cidx] * 1.2, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                # if center stagnated for long, replace it with a diverse archive sample\n                if stagnation[cidx] >= self.center_replace_patience:\n                    # pick a random archive point far from current centers to diversify\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # compute distance to nearest center\n                        d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        # choose top distant candidate among moderately good ones\n                        cand_idx = np.argsort(d_to_centers)[-max(1, min(20, len(X_arr))):]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[cidx] = X_arr[pick].copy()\n                        trust_radius[cidx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                        stagnation[cidx] = 0\n\n            # end centers loop\n\n            # archive pruning to keep memory manageable\n            max_archive = max(2000, 50 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # update centers occasionally to the best points in archive\n            if len(X) > 0:\n                best_centers = get_top_centers(min(len(centers), max(1, len(X)//10)))\n                for i in range(min(len(best_centers), len(centers))):\n                    centers[i] = best_centers[i].copy()\n\n        # End main loop\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm EnsembleAdaptiveSurrogateLevy scored 0.362 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "aa267358-ed7e-4f49-b4a7-35c55ce9ec9d", "operator": null, "metadata": {"aucs": [0.17941993027528047, 0.24018311161534822, 0.41190001894201766, 0.7799104420416646, 0.37457784673853134, 0.4675189476155249, 0.2326183364655513, 0.3751751787181, 0.3844567998633832, 0.17056987666575474]}, "task_prompt": ""}
{"id": "f832be38-a79b-47ad-8667-363781d6908f", "fitness": "-inf", "name": "SCLS", "description": "SCLS probes compact random affine subspaces (k ≈ ceil(sqrt(n)), probes = max(4,2k)) to reduce dimensionality and maintains a small k×k subspace covariance S with cumulation ps (cs=0.35, cov_beta=0.2) to adapt preferred directions from successful probes. It mixes local moves (Gaussian-probed directions, mirrored sampling, initial step ≈0.4·range) with population-difference mutations from an archive (p_de=0.25, F_de=0.8), occasional Mantegna–Lévy long jumps (p_levy=0.12) and budget-aware short line searches to intensify along successes. Step-size is adapted by a combination of cumulation norm vs. expected chi_k and success-rate multipliers (succ_grow=1.12, succ_shrink=0.88), while a direction memory and archive pruning retain useful past information. Practical safeguards include bound clipping, a safe_eval budget counter, mirrored probing for symmetry, small-rank updates of S, and occasional Lévy nudged restarts for stagnation.", "code": "import numpy as np\n\nclass SCLS:\n    \"\"\"\n    Subspace-Covariance Lévy Search (SCLS)\n    One-line: Probe small random affine subspaces, adapt a compact subspace covariance\n    with a cumulation path from successful probes, use archive-based DE diffs and\n    occasional Mantegna-Lévy jumps, and combine cumulation + success-rate step adaptation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, memory_size=8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # subspace & probing defaults\n        self.p_de = 0.25\n        self.F_de = 0.8\n        self.p_levy = 0.12\n        self.pop_mirroring = True\n\n        # cumulation and covariance adaptation params (in subspace of size k)\n        self.cs = 0.35        # cumulation (subspace)\n        self.cov_beta = 0.2   # simple rank-one learning rate in subspace\n        # success multiplier\n        self.succ_grow = 1.12\n        self.succ_shrink = 0.88\n\n    # Mantegna Levy generator\n    def _levy_mantegna(self, n, alpha=1.5, scale=1.0):\n        sigma_u = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                   (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n        u = np.random.normal(0, sigma_u, size=n)\n        v = np.random.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1.0 / alpha))\n        return scale * step\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (Many Affine BBOB typical [-5,5])\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n\n        # initial point\n        x_cur = np.random.uniform(lb, ub)\n        evals = 0\n        # archive\n        X = []\n        F = []\n        # safe_eval wrapper\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return None, None\n            x_clipped = np.clip(x, lb, ub)\n            try:\n                f = float(func(x_clipped))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X.append(x_clipped.copy())\n            F.append(f)\n            return f, x_clipped\n\n        # initial evaluation\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            # budget zero\n            return np.inf, np.zeros(n)\n        f_best = float(f_cur)\n        x_best = x_cur.copy()\n\n        # initial step-size (like ARSS)\n        step = 0.4 * np.mean(rng_range)\n        min_step = 1e-8 * max(1.0, np.mean(rng_range))\n\n        # subspace dimension k ~ sqrt(n)\n        k = max(1, int(np.ceil(np.sqrt(n))))\n        probes = max(4, 2 * k)\n\n        # small subspace covariance (k x k) and cumulation path\n        S = np.eye(k)  # covariance in subspace coordinates\n        ps = np.zeros(k)\n        # eigen factors for S (will be updated when needed)\n        B_sub = np.eye(k)\n        D_sub = np.ones(k)\n        invsqrtS = np.eye(k)\n\n        # memory of successful directions (unit vectors in R^n)\n        dir_memory = []\n\n        # helper: simple golden-ish 1D refinement (budget-aware, few evals)\n        def short_line_search(x0, f0, d, init_step, max_evals=8):\n            nonlocal evals\n            if evals >= budget:\n                return None, None\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            remaining = budget - evals\n            remaining = min(remaining, max_evals)\n            if remaining <= 0:\n                return None, None\n\n            # try small bracket at +init_step and -init_step\n            a0 = 0.0\n            fa = f0\n            b = init_step\n            x_b = np.clip(x0 + b * d, lb, ub)\n            out = safe_eval(x_b)\n            if out[0] is None:\n                return None, None\n            fb, xb = out\n            remaining -= 1\n            if fb >= fa:\n                # try opposite\n                b = -init_step\n                x_b = np.clip(x0 + b * d, lb, ub)\n                out = safe_eval(x_b)\n                if out[0] is None:\n                    return None, None\n                fb2, xb2 = out\n                remaining -= 1\n                if fb2 >= fa:\n                    return None, None\n                fb, xb = fb2, xb2\n\n            # limited golden search between 0 and b with very small number of steps\n            gr = (np.sqrt(5) - 1) / 2\n            a = 0.0\n            c = b - gr * (b - a)\n            d_alpha = a + gr * (b - a)\n            x_c = np.clip(x0 + c * d, lb, ub)\n            out = safe_eval(x_c)\n            if out[0] is None:\n                return None, None\n            fc, xc = out\n            remaining -= 1\n            x_d = np.clip(x0 + d_alpha * d, lb, ub)\n            out = safe_eval(x_d)\n            if out[0] is None:\n                return None, None\n            fd, xd = out\n            remaining -= 1\n\n            best_f = fa\n            best_x = x0.copy()\n            if fc < best_f:\n                best_f = fc; best_x = xc.copy()\n            if fd < best_f:\n                best_f = fd; best_x = xd.copy()\n\n            it = 0\n            max_iter = max(1, remaining)\n            while it < max_iter:\n                it += 1\n                if fc < fd:\n                    b = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    c = b - gr * (b - a)\n                    x_c = np.clip(x0 + c * d, lb, ub)\n                    out = safe_eval(x_c)\n                    if out[0] is None:\n                        break\n                    fc, xc = out\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    a = c\n                    c = d_alpha\n                    fc = fd\n                    d_alpha = a + gr * (b - a)\n                    x_d = np.clip(x0 + d_alpha * d, lb, ub)\n                    out = safe_eval(x_d)\n                    if out[0] is None:\n                        break\n                    fd, xd = out\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0:\n                return best_f, best_x\n            return None, None\n\n        # main loop\n        while evals < budget:\n            # recompute k/probes adaptively to allow slight variation\n            k = max(1, int(np.clip(int(np.ceil(np.sqrt(n))), 1, n)))\n            probes = max(4, 2 * k)\n\n            # build subspace basis (use some memory directions if available)\n            use_mem = min(len(dir_memory), k // 2)\n            basis_cols = []\n            if use_mem > 0:\n                # take most recent memory directions\n                for i in range(use_mem):\n                    basis_cols.append(dir_memory[i].copy())\n            # supplement with random columns\n            needed = k - len(basis_cols)\n            if needed > 0:\n                R = np.random.randn(n, needed)\n                if basis_cols:\n                    R = np.column_stack((np.column_stack(basis_cols), R))\n                Q, _ = np.linalg.qr(R)\n                basis = Q[:, :k]\n            else:\n                Q, _ = np.linalg.qr(np.column_stack(basis_cols))\n                basis = Q[:, :k]\n\n            # ensure subspace covariance S has size k (resize if needed)\n            if S.shape[0] != k:\n                # embed previous S into new S by padding with identity\n                S_new = np.eye(k)\n                m = min(S.shape[0], k)\n                S_new[:m, :m] = S[:m, :m]\n                S = S_new\n                ps = np.zeros(k)\n                B_sub = np.eye(k)\n                D_sub = np.ones(k)\n                invsqrtS = np.eye(k)\n\n            # eigendecompose subspace covariance for sampling / invsqrt\n            try:\n                vals, vecs = np.linalg.eigh(S)\n                vals = np.maximum(vals, 1e-20)\n                D_sub = np.sqrt(vals)\n                B_sub = vecs\n                invsqrtS = (B_sub * (1.0 / D_sub)) @ B_sub.T\n            except np.linalg.LinAlgError:\n                B_sub = np.eye(k)\n                D_sub = np.ones(k)\n                invsqrtS = np.eye(k)\n\n            successes = 0\n            success_y_list = []  # store subspace coords of successful moves for covariance update\n\n            # Mirrored probing: if mirroring enabled and probes even, generate half directions and mirror\n            if self.pop_mirroring and (probes % 2 == 0):\n                half = probes // 2\n                coeffs_list = [np.random.randn(k) for _ in range(half)]\n                coeffs_list = coeffs_list + [-c for c in coeffs_list]\n            else:\n                coeffs_list = [np.random.randn(k) for _ in range(probes)]\n\n            for coeffs in coeffs_list:\n                if evals >= budget:\n                    break\n                # map to full space: d = basis @ coeffs\n                d = basis @ coeffs\n                dnrm = np.linalg.norm(d)\n                if dnrm == 0:\n                    continue\n                d = d / dnrm\n\n                # sample alpha uniformly in [-step, step]\n                alpha = np.random.uniform(-step, step)\n\n                # base candidate\n                x_try = x_cur + alpha * d\n\n                # apply DE-diff mutation from archive sometimes\n                if (np.random.rand() < self.p_de) and (len(X) >= 2):\n                    i1, i2 = np.random.choice(len(X), size=2, replace=False)\n                    de = self.F_de * (X[i1] - X[i2])\n                    x_try = x_try + de\n\n                # occasional Levy long jump\n                if np.random.rand() < self.p_levy:\n                    levy = self._levy_mantegna(n, alpha=1.5, scale=0.5 * step)\n                    x_try = x_try + levy\n\n                x_try = np.clip(x_try, lb, ub)\n                out = safe_eval(x_try)\n                if out[0] is None:\n                    break\n                f_try, x_try = out\n\n                if f_try < f_cur - 1e-12:\n                    # accept immediately\n                    prev_x = x_cur.copy()\n                    x_cur = x_try.copy()\n                    f_cur = f_try\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n                    successes += 1\n                    # compute subspace coordinate y (coeffs scaled by alpha/step)\n                    # compute normalized subspace coordinate vector z so that magnitude indicates relative move\n                    z = coeffs.copy()\n                    z_norm = np.linalg.norm(z)\n                    if z_norm > 0:\n                        z = z / z_norm\n                        y = (alpha / (step + 1e-20)) * z\n                    else:\n                        y = np.zeros(k)\n                    success_y_list.append(y.copy())\n\n                    # update memory of directions (LRU)\n                    dir_succ = (x_cur - prev_x)\n                    dn = np.linalg.norm(dir_succ)\n                    if dn > 0:\n                        dir_succ = dir_succ / dn\n                        dir_memory.insert(0, dir_succ.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n\n                    # short line-search to intensify along successful direction (budget-aware)\n                    remaining_budget = budget - evals\n                    if remaining_budget >= 2:\n                        ls_max = min(10, remaining_budget)\n                        # direction for linesearch is the step direction\n                        ls_out = short_line_search(x_cur, f_cur, dir_succ, init_step=abs(alpha) if abs(alpha) > 1e-12 else step, max_evals=ls_max)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                f_cur = f_ls\n                                x_cur = x_ls.copy()\n                                if f_ls < f_best:\n                                    f_best = f_ls\n                                    x_best = x_ls.copy()\n                # else: maybe small chance to do a short focused line search from current point along -d/+d\n                else:\n                    if np.random.rand() < 0.03 and (budget - evals) >= 3:\n                        ls_out = short_line_search(x_cur, f_cur, d, init_step=step, max_evals=4)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                # accept\n                                prev_x = x_cur.copy()\n                                x_cur = x_ls.copy()\n                                f_cur = f_ls\n                                if f_ls < f_best:\n                                    f_best = f_ls\n                                    x_best = x_ls.copy()\n                                # store direction\n                                dir_succ = (x_cur - prev_x)\n                                dn = np.linalg.norm(dir_succ)\n                                if dn > 0:\n                                    dir_succ = dir_succ / dn\n                                    dir_memory.insert(0, dir_succ.copy())\n                                    if len(dir_memory) > self.memory_size:\n                                        dir_memory.pop()\n                                # add to success list for covariance update\n                                # approximate y using coeffs\n                                z = coeffs.copy()\n                                z_norm = np.linalg.norm(z)\n                                if z_norm > 0:\n                                    z = z / z_norm\n                                    y = (alpha / (step + 1e-20)) * z\n                                else:\n                                    y = np.zeros(k)\n                                success_y_list.append(y.copy())\n                                successes += 1\n\n            # end probes loop\n\n            # compute success_rate and update step by simple multiplier\n            attempted = len(coeffs_list)\n            success_rate = successes / max(1, attempted)\n\n            # cumulation update from average successful y (if any)\n            if success_y_list:\n                y_mean = np.mean(np.vstack(success_y_list), axis=0)\n                ps = (1.0 - self.cs) * ps + np.sqrt(self.cs * (2.0 - self.cs)) * (invsqrtS @ y_mean) if (np.linalg.norm(invsqrtS) > 0) else (1.0 - self.cs) * ps + np.sqrt(self.cs * (2.0 - self.cs)) * y_mean\n            else:\n                ps = (1.0 - self.cs) * ps\n\n            # simple step-size adaptation combining cumulation norm and success-rate\n            # expected norm chi_k\n            chi_k = np.sqrt(max(1, k)) * (1.0 - 1.0 / (4.0 * max(1, k)) + 1.0 / (21.0 * max(1, k) ** 2))\n            norm_ps = np.linalg.norm(ps)\n            # cumulation multiplier (gentle)\n            step *= np.exp(0.2 * (norm_ps / (chi_k + 1e-20) - 1.0) / max(1.0, k))\n            # success-rate multiplier\n            if success_rate > 0.2:\n                step *= self.succ_grow\n            elif success_rate < 0.05:\n                step *= self.succ_shrink\n            step = max(step, min_step)\n\n            # update subspace covariance S with successes (simple rank-one / rank-mu)\n            if success_y_list:\n                # compute rank-mu like update from mean of success y scaled\n                y_mean = np.mean(np.vstack(success_y_list), axis=0)\n                rank = np.outer(y_mean, y_mean)\n                S = (1.0 - self.cov_beta) * S + self.cov_beta * (rank + 1e-12 * np.eye(k))\n\n            # keep archive moderate (prune if needed)\n            max_archive = max(2000, 50 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # small restart diversification if no improvements for many probes (stagnation control)\n            # measure inactivity by small success_rate over many loops; use quick heuristic\n            if success_rate == 0 and np.random.rand() < 0.05:\n                # random Lévy nudged restart around best with preserved memory\n                if f_best < np.inf:\n                    perturb = self._levy_mantegna(n, alpha=1.5, scale=0.6 * step)\n                    x_new = np.clip(x_best + perturb, lb, ub)\n                    out = safe_eval(x_new)\n                    if out[0] is not None:\n                        f_new, x_new = out\n                        if f_new < f_cur:\n                            x_cur = x_new.copy()\n                            f_cur = f_new\n                            if f_new < f_best:\n                                f_best = f_new\n                                x_best = x_new.copy()\n            # loop until budget exhausted\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "aa267358-ed7e-4f49-b4a7-35c55ce9ec9d", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "f2f29081-d1c2-4d76-a1c0-a17d70eca292", "fitness": 0.1844981619198906, "name": "BLRDS", "description": "The algorithm maintains a low-rank orthonormal basis of p search directions (p ≈ max(2, ceil(sqrt(n))) ) and treats each direction as a bandit arm with UCB selection (counts, cumulative rewards, tunable ucb_beta) so it focuses evaluations on promising subspace directions. It probes along chosen directions with adaptive per-arm step-sizes (initial steps = 0.35·range, growth ~×1.18 on success, shrink ×0.80 on failure, bounded by min_step), uses occasional opposition sampling and a small probabilistic acceptance of non‑improving moves, and refines successful probes by cheap three‑point parabolic interpolation to estimate a local step optimum. Successful moves trigger soft updates of the corresponding direction toward the move (learning rate γ≈0.30) and are stored in a small recent_moves memory (memory_size) used when rotating/replacing poor arms; the basis is periodically re‑orthonormalized to preserve stability. Escapes from local minima are supported by occasional global Gaussian jumps, budget‑aware soft restarts around the best point, rotation/replacement of worst arms (every rotate_every iterations), and final polishing when stagnation or restart limits are reached, while all evaluations are clipped to bounds and guarded by a safe_eval counter.", "code": "import numpy as np\n\nclass BLRDS:\n    \"\"\"\n    Bandit-guided Low-Rank Directional Search (BLRDS)\n\n    Key ideas:\n    - Maintain a low-rank basis (n x p) of candidate search directions.\n    - Treat each basis direction as a multi-armed bandit arm (UCB selection).\n    - Probe along selected directions with per-direction adaptive step-sizes.\n    - When a probe improves, perform a cheap quadratic (parabolic) interpolation\n      using up to two more evaluations along that same direction to refine the step.\n    - Update direction vectors from successful moves (soft update) and periodically\n      rotate/replace poor directions with random ones to recover diversity.\n    - Occasional global Gaussian jumps and restarts if stagnation observed.\n    - All evaluations are budget-safe (safe_eval) and bounds-respecting.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 subspace_size=None, memory_size=8, ucb_beta=1.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # low-rank basis dimension p\n        self.p = subspace_size if subspace_size is not None else max(2, int(np.ceil(np.sqrt(self.dim))))\n        self.memory_size = int(memory_size)  # store a few recent successful moves (for warm restarts or rotation)\n        self.ucb_beta = float(ucb_beta)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds (Many Affine BBOB uses [-5,5] but support func-provided)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # helper: clipping to bounds\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # evaluation bookkeeping and safe evaluation\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initialize current point\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, _ = safe_eval(x_cur)\n        if f_cur is None:\n            # no budget\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # initialize low-rank directions: orthonormal (n x p)\n        p = min(self.p, n)\n        R = np.random.randn(n, p)\n        Q, _ = np.linalg.qr(R)\n        D = Q[:, :p].copy()\n\n        # per-direction statistics for UCB\n        counts = np.zeros(p, dtype=int)\n        rewards = np.zeros(p, dtype=float)   # cumulative reward (improvement)\n        steps = np.full(p, 0.35 * np.mean(ub - lb))  # per-direction step-size\n        min_step = 1e-8 * max(1.0, np.mean(ub - lb))\n\n        # small memory of recent successful move vectors\n        recent_moves = []\n\n        # control params\n        max_probes_per_round = max(2, 2 * p)\n        stagnation_iters = 0\n        stagnation_limit = max(12, int(8 + np.log(1 + n) * 4))\n        restarts = 0\n        max_restarts = 6\n        rotate_every = max(20, p * 5)\n        iteration = 0\n\n        # convenience UCB selector\n        def select_direction_ucb():\n            total = counts.sum()\n            if total == 0:\n                # choose a random untried or random index\n                untried = np.where(counts == 0)[0]\n                if len(untried) > 0:\n                    return int(np.random.choice(untried))\n                else:\n                    return int(np.random.randint(0, p))\n            # compute average reward per arm (avoid divide by zero)\n            avg = np.zeros_like(rewards)\n            nonzero = counts > 0\n            avg[nonzero] = rewards[nonzero] / counts[nonzero]\n            # UCB score\n            bonus = np.sqrt(np.log(1 + total) / (1 + counts))\n            scores = avg + self.ucb_beta * bonus\n            return int(np.argmax(scores))\n\n        # parabolic interpolation along direction using 3 points (alpha1, alpha2, alpha3)\n        # returns (f_star, x_star, alpha_star) or (None,None,None) if not enough budget or degenerate\n        def parabolic_vertex(alpha_vals, f_vals, x0, d):\n            # fit parabola f(alpha) = A alpha^2 + B alpha + C with alpha_vals length 3\n            if len(alpha_vals) != 3 or len(f_vals) != 3:\n                return None, None, None\n            a1, a2, a3 = alpha_vals\n            f1, f2, f3 = f_vals\n            # Solve for coefficients via finite differences\n            denom = (a1 - a2) * (a1 - a3) * (a2 - a3)\n            if abs(denom) < 1e-16:\n                return None, None, None\n            # Lagrange interpolation for vertex alpha* = -B/(2A)\n            # compute A, B using formulas\n            # Using three points, compute vertex directly:\n            A = ( (f1*(a2 - a3) + f2*(a3 - a1) + f3*(a1 - a2)) ) / denom\n            B = ( (f1*(a3**2 - a2**2) + f2*(a1**2 - a3**2) + f3*(a2**2 - a1**2)) ) / denom\n            if abs(A) < 1e-16:\n                return None, None, None\n            alpha_star = -B / (2 * A)\n            # clamp alpha_star to convex hull of tested points extended moderately\n            low = min(alpha_vals) - 2.0 * max(1.0, abs(min(alpha_vals)))\n            high = max(alpha_vals) + 2.0 * max(1.0, abs(max(alpha_vals)))\n            alpha_star = float(np.clip(alpha_star, low, high))\n            x_star = clip(x0 + alpha_star * d)\n            f_star, _ = safe_eval(x_star)\n            if f_star is None:\n                return None, None, None\n            return f_star, x_star, alpha_star\n\n        # main loop\n        while evals < self.budget:\n            iteration += 1\n            probes = min(max_probes_per_round, max(1, (self.budget - evals)))\n            improved_in_round = False\n            for _probe in range(probes):\n                if evals >= self.budget:\n                    break\n\n                # choose a direction arm\n                idx = select_direction_ucb()\n\n                # direction vector (ensured unit norm)\n                d = D[:, idx]\n                dn = np.linalg.norm(d)\n                if dn == 0:\n                    # regenerate this direction\n                    v = np.random.randn(n)\n                    v /= np.linalg.norm(v) + 1e-20\n                    D[:, idx] = v\n                    d = D[:, idx]\n                # sample alpha (two-stage: try a main alpha and maybe opposition)\n                alpha0 = np.random.normal(loc=0.0, scale=steps[idx])  # gaussian sample\n                if abs(alpha0) < 1e-16:\n                    alpha0 = steps[idx] * (0.5 - np.random.rand())\n\n                x_try = clip(x_cur + alpha0 * d)\n                f_try, x_ret = safe_eval(x_try)\n                if f_try is None:\n                    break\n                counts[idx] += 1\n\n                # Opposite sampling (opposition-based) with small probability\n                if np.random.rand() < 0.12 and evals < self.budget:\n                    alpha_op = -alpha0 * (0.5 + 0.5 * np.random.rand())\n                    x_op = clip(x_cur + alpha_op * d)\n                    f_op, _ = safe_eval(x_op)\n                    if f_op is None:\n                        break\n                    counts[idx] += 1\n                    # keep the better of the two\n                    if f_op < f_try:\n                        f_try = f_op\n                        x_ret = clip(x_cur + alpha_op * d)\n\n                # compute immediate reward (positive improvement)\n                reward = max(0.0, f_cur - f_try)\n                rewards[idx] += reward\n\n                if f_try < f_cur - 1e-12:\n                    # acceptance: greedy improvement\n                    x_prev = x_cur.copy()\n                    f_prev = f_cur\n                    x_cur = x_ret.copy()\n                    f_cur = f_try\n                    improved_in_round = True\n                    stagnation_iters = 0\n                    # update success memory\n                    move_vec = x_cur - x_prev\n                    mvnorm = np.linalg.norm(move_vec)\n                    if mvnorm > 0:\n                        unit_move = move_vec / mvnorm\n                        recent_moves.insert(0, unit_move.copy())\n                        if len(recent_moves) > self.memory_size:\n                            recent_moves.pop()\n\n                        # soft-update the direction vector toward the successful move:\n                        gamma = 0.30  # learning rate for the direction\n                        D[:, idx] = (1 - gamma) * D[:, idx] + gamma * unit_move\n                        D[:, idx] /= (np.linalg.norm(D[:, idx]) + 1e-20)\n\n                    # increase step for this direction modestly\n                    steps[idx] = min(steps[idx] * 1.18, 5.0 * np.mean(ub - lb))\n\n                    # do a cheap parabolic refinement if budget allows (use up to two more evals)\n                    remaining = self.budget - evals\n                    if remaining >= 2:\n                        # prepare two additional alphas: one in + direction, one in - direction relative to alpha0\n                        a1 = alpha0\n                        a2 = alpha0 * 0.5\n                        a3 = alpha0 * 1.5\n                        # ensure they are distinct\n                        alpha_vals = [a2, 0.0, a3]  # include center 0 to help fit\n                        f_vals = []\n                        pts = []\n                        # we already have f at a1 (maybe not exactly used); we'll evaluate for alpha_vals\n                        for a in alpha_vals:\n                            x_a = clip(x_prev + a * d)\n                            f_a, _ = safe_eval(x_a)\n                            if f_a is None:\n                                break\n                            f_vals.append(f_a)\n                            pts.append(x_a.copy())\n                        if len(f_vals) == 3:\n                            f_par, x_par, a_par = parabolic_vertex(alpha_vals, f_vals, x_prev, d)\n                            if f_par is not None and f_par < f_cur - 1e-12:\n                                # accept parabolic better point\n                                x_cur = x_par.copy()\n                                f_cur = f_par\n                                # update memory/steps\n                                mv = x_cur - x_prev\n                                mn = np.linalg.norm(mv)\n                                if mn > 0:\n                                    unit_mv = mv / mn\n                                    D[:, idx] = (1 - 0.25) * D[:, idx] + 0.25 * unit_mv\n                                    D[:, idx] /= (np.linalg.norm(D[:, idx]) + 1e-20)\n                                steps[idx] = min(steps[idx] * 1.15, 5.0 * np.mean(ub - lb))\n\n                else:\n                    # no improvement: shrink step for this direction\n                    steps[idx] = max(steps[idx] * 0.80, min_step)\n                    stagnation_iters += 1\n\n                # small chance to accept a worse move probabilistically to escape local minima\n                if (not improved_in_round) and (np.random.rand() < 0.01) and (f_try is not None):\n                    # accept with small prob if not much worse\n                    if f_try <= f_cur + 1e-6:\n                        x_cur = x_ret.copy()\n                        f_cur = f_try\n\n                # periodic small global Gaussian exploration\n                if np.random.rand() < 0.03 and (self.budget - evals) > 0:\n                    sigma = 0.5 * np.mean(ub - lb) * (0.5 + np.random.rand())\n                    xg = clip(x_cur + np.random.randn(n) * sigma)\n                    fg, _ = safe_eval(xg)\n                    if fg is None:\n                        break\n                    if fg < f_cur - 1e-12:\n                        # update current\n                        mv = xg - x_cur\n                        x_cur = xg.copy()\n                        f_cur = fg\n                        # adapt the worst direction towards this successful global move\n                        worst_idx = int(np.argmin(rewards + 1e-12))\n                        mvn = np.linalg.norm(mv)\n                        if mvn > 0:\n                            D[:, worst_idx] = (1 - 0.25) * D[:, worst_idx] + 0.25 * (mv / mvn)\n                            D[:, worst_idx] /= (np.linalg.norm(D[:, worst_idx]) + 1e-20)\n                            steps[worst_idx] = max(steps[worst_idx], 0.8 * sigma)\n\n            # after probe block: re-orthonormalize basis occasionally to keep numeric stability\n            if iteration % 7 == 0:\n                Q, _ = np.linalg.qr(D)\n                D = Q[:, :p].copy()\n\n            # replace/rotate poor directions every rotate_every iterations\n            if iteration % rotate_every == 0:\n                # identify poorly performing arms (low reward per count)\n                score = np.zeros_like(rewards) - 1e9\n                nonzero = counts > 0\n                score[nonzero] = rewards[nonzero] / counts[nonzero]\n                worst = np.argsort(score)[:max(1, p // 4)]\n                for wi in worst:\n                    # replace by a normalized combination of a random vector and one recent move if available\n                    rnd = np.random.randn(n)\n                    if len(recent_moves) > 0 and np.random.rand() < 0.6:\n                        candidate = 0.5 * rnd + 0.5 * recent_moves[np.random.randint(len(recent_moves))]\n                    else:\n                        candidate = rnd\n                    candidate /= (np.linalg.norm(candidate) + 1e-20)\n                    D[:, wi] = candidate\n                    counts[wi] = 0\n                    rewards[wi] = 0.0\n                    steps[wi] = 0.35 * np.mean(ub - lb)\n\n            # stagnation & restart policy\n            if stagnation_iters >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final polishing: reduce step sizes to min, try local radial search around best\n                    steps = np.maximum(steps * 0.45, min_step)\n                    # try a few local tiny perturbations\n                    local_tries = min(10, self.budget - evals)\n                    for _ in range(local_tries):\n                        if evals >= self.budget:\n                            break\n                        dx = np.random.randn(n) * (0.02 * np.mean(ub - lb))\n                        xt = clip(x_best + dx)\n                        ft, _ = safe_eval(xt)\n                        if ft is None:\n                            break\n                    # if still no improvement, break out to return best\n                    if stagnation_iters > stagnation_limit * 3:\n                        break\n                else:\n                    # soft restart around x_best with enlarged step and fresh directions\n                    if x_best is not None:\n                        perturb_scale = 0.6 * np.mean(ub - lb)\n                        x_cur = clip(x_best + np.random.randn(n) * perturb_scale)\n                        fc, _ = safe_eval(x_cur)\n                        if fc is None:\n                            break\n                        f_cur = fc\n                    # reinitialize directions partially\n                    for i in range(max(1, p // 2)):\n                        v = np.random.randn(n)\n                        v /= (np.linalg.norm(v) + 1e-20)\n                        D[:, i] = v\n                        counts[i] = 0\n                        rewards[i] = 0.0\n                        steps[i] = min(1.0 * np.mean(ub - lb), steps[i] * 2.0)\n                    stagnation_iters = 0\n\n            # ensure steps not too small\n            steps = np.maximum(steps, min_step)\n\n            # quick termination if extremely good\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm BLRDS scored 0.184 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "1170b68b-fa6d-49c9-986c-c4c18a103838", "operator": null, "metadata": {"aucs": [0.06303600476448323, 0.1466772409654441, 0.32170892879765156, 0.1639098437606974, 0.19037088617711406, 0.31295447188486014, 0.19865033371406937, 0.168551647193117, 0.1571873707818494, 0.12193489115961975]}, "task_prompt": ""}
{"id": "dbfb0c67-e5d2-4c51-89f0-3034b212c7f1", "fitness": 0.212722376683891, "name": "MASTL", "description": "MASTL explores the search space by building small random affine subspaces (subspace dim k ≈ n^(2/3)) biased by an LRU memory of recent successful unit directions and sampling Langevin-style proposals that combine momentum from memory with Gaussian noise. It uses larger per-round probing (probes = max(6, 3*k)) and aggressive step adaptation (initial step = 0.25·domain_mean, grow=1.25, shrink=0.70, min/max step caps) to accelerate successful moves while shrinking quickly on failure. Budget-awareness and robustness are enforced via safe_eval and clipping to bounds, plus a very cheap, tightly limited golden-section-like short_line_search (≤6 evals) and occasional focused line searches for intensification. Stagnation handling includes memory-clearing restarts that perturb around the best, and small full-space Gaussian polishing to exploit local gains.", "code": "import numpy as np\n\nclass MASTL:\n    \"\"\"\n    MASTL: Memory-Accelerated Adaptive Subspace with Tunable Langevin proposals\n\n    Short description:\n      - Builds small random affine subspaces biased by a memory of successful directions.\n      - Proposes Langevin-style steps (momentum + noise) inside the subspace, accepts improvements,\n        and runs cheap short 1-D intensifications along successful directions.\n      - Uses more aggressive step adaptation and larger probe counts than ARSS, a different\n        subspace dimension scaling, and stricter short line-search budgets.\n\n    Main algorithm parameters (tuned differently from the provided ARSS):\n      - subspace dimension k  : ~ n^(2/3)  (ARSS used ~ sqrt(n))\n      - probes per round      : max(6, 3*k) (ARSS used max(4, 2*k))\n      - initial step (step)   : 0.25 * domain_mean (ARSS used 0.4 * domain_mean)\n      - minimal step (min_step): 1e-6 * domain_mean (ARSS used 1e-8 * domain_mean)\n      - growth factor (on success): 1.25 (ARSS used 1.12-1.15)\n      - shrink factor (on fail)   : 0.70 (ARSS used 0.85)\n      - memory_size (LRU of unit directions): default 12 (ARSS default 8)\n      - line search budget per invocation: <= 6 evaluations (ARSS allowed up to 12)\n      - stagnation_limit: a bit larger to allow more exploration before restarts\n      - restarts allowed: larger number to diversify more\n\n    This implementation strictly enforces the evaluation budget and bounds by using safe_eval.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=12, seed_offset=0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed + seed_offset)\n        self.memory_size = int(memory_size)\n\n    def __call__(self, func):\n        n = self.dim\n        # Accept scalar bounds or vector bounds from func.bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        domain_mean = np.mean(ub - lb)\n        # internal evaluation counter and best trackers\n        evals = 0\n        # start at random point inside bounds\n        x_cur = np.random.uniform(lb, ub)\n        # safe initial evaluation\n        def clip_to_bounds(x):\n            return np.clip(x, lb, ub)\n\n        # safe eval wrapper\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(x)\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            # no budget at all\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # initial step sizes and parameters (different from ARSS)\n        step = 0.25 * domain_mean\n        min_step = 1e-6 * max(1.0, domain_mean)\n        # subspace dimension scaling ~ n^(2/3)\n        k = max(1, int(np.ceil(n ** (2.0 / 3.0))))\n        probes = max(6, 3 * k)\n        dir_memory = []  # LRU memory of unit directions\n        # adaptation multipliers (different)\n        grow = 1.25\n        shrink = 0.70\n        max_step = 5.0 * domain_mean\n\n        # stagnation and restarts\n        no_improve = 0\n        stagnation_limit = max(15, int(8 * np.log(1 + n)))\n        max_restarts = 8\n        restarts = 0\n\n        # cheap limited golden-section style line-search (very budget-aware)\n        def short_line_search(x0, f0, d, init_step, max_evals=6):\n            # returns (f_new, x_new) or (None, None) if nothing or budget exhausted\n            nonlocal evals\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0 or evals >= self.budget:\n                return None, None\n            d = d / dnrm\n            remaining = max(0, self.budget - evals)\n            if remaining <= 0:\n                return None, None\n            # Try coarse bracket: 0, +s, -s (up to 2 evals)\n            s = float(init_step)\n            # Evaluate +s\n            xa = clip_to_bounds(x0 + s * d)\n            fa, _ = safe_eval(xa)\n            if fa is None:\n                return None, None\n            remaining = max(0, self.budget - evals)\n            # If improvement, set bracket [0, s]\n            if fa < f0:\n                a, b = 0.0, s\n                fa_val, fb_val = f0, fa\n            else:\n                # try -s\n                xb = clip_to_bounds(x0 - s * d)\n                fb, _ = safe_eval(xb)\n                if fb is None:\n                    return None, None\n                remaining = max(0, self.budget - evals)\n                if fb < f0:\n                    a, b = -s, 0.0\n                    fa_val, fb_val = fb, f0\n                else:\n                    return None, None  # no improvement on coarse probes\n            # Now perform a few golden-section steps (very limited)\n            gr = (np.sqrt(5) - 1) / 2\n            # interior points\n            c = b - gr * (b - a)\n            d_alpha = a + gr * (b - a)\n            xc = clip_to_bounds(x0 + c * d)\n            fc, _ = safe_eval(xc)\n            if fc is None:\n                return None, None\n            remaining = max(0, self.budget - evals)\n            xd = clip_to_bounds(x0 + d_alpha * d)\n            fd, _ = safe_eval(xd)\n            if fd is None:\n                return None, None\n            remaining = max(0, self.budget - evals)\n            best_f = f0\n            best_x = x0.copy()\n            for val, px in ((fa_val, xa if fa_val is not None else None),\n                            (fb_val, xb if 'xb' in locals() else None),\n                            (fc, xc), (fd, xd)):\n                try:\n                    if val is not None and val < best_f:\n                        best_f = val; best_x = px.copy()\n                except Exception:\n                    pass\n            iters = 0\n            while remaining > 0 and iters < max_evals:\n                iters += 1\n                # choose which side to shrink\n                if fc < fd:\n                    b = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    c = b - gr * (b - a)\n                    xc = clip_to_bounds(x0 + c * d)\n                    fc, _ = safe_eval(xc)\n                    if fc is None:\n                        break\n                else:\n                    a = c\n                    c = d_alpha\n                    fc = fd\n                    d_alpha = a + gr * (b - a)\n                    xd = clip_to_bounds(x0 + d_alpha * d)\n                    fd, _ = safe_eval(xd)\n                    if fd is None:\n                        break\n                remaining = max(0, self.budget - evals)\n                for val, px in ((fc, xc), (fd, xd)):\n                    if val is not None and val < best_f:\n                        best_f = val; best_x = px.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # Main loop\n        while evals < self.budget:\n            # adapt k occasionally to current dimension (allow slight variation)\n            k = max(1, int(np.clip(int(np.ceil(n ** (2.0 / 3.0))), 1, n)))\n            probes = max(6, 3 * k)\n\n            improved = False\n            # Build basis: reuse up to half from memory (bias heavier than ARSS)\n            use_mem = min(len(dir_memory), max(1, k // 2))\n            basis = np.zeros((n, 0))\n            if use_mem > 0:\n                # take most recent ones (LRU head)\n                mem_sel = np.array(dir_memory[:use_mem])\n                basis = np.column_stack((basis, mem_sel.T))\n            # fill remaining with random directions and orthonormalize\n            needed = k - basis.shape[1]\n            if needed > 0:\n                R = np.random.randn(n, needed)\n                if basis.size > 0:\n                    R = np.column_stack((basis, R))\n                Q, _ = np.linalg.qr(R)\n                basis = Q[:, :k]\n            else:\n                Q, _ = np.linalg.qr(basis)\n                basis = Q[:, :k]\n\n            # Within subspace, propose Langevin-style moves:\n            # proposal = current + momentum * prev_dir + gaussian_noise (projected to subspace)\n            # momentum derived from last memory head if exists\n            momentum = np.zeros(n)\n            if len(dir_memory) > 0:\n                momentum = dir_memory[0] * 0.5  # modest momentum bias\n\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample coefficients inside subspace with anisotropic scaling\n                coeffs = np.random.normal(scale=1.0, size=k)\n                dir_sub = basis @ coeffs\n                if np.linalg.norm(dir_sub) == 0:\n                    continue\n                dir_sub = dir_sub / np.linalg.norm(dir_sub)\n                # combine momentum and subspace direction\n                alpha_m = 0.6  # weight for momentum component\n                candidate_dir = (alpha_m * momentum + (1 - alpha_m) * dir_sub)\n                if np.linalg.norm(candidate_dir) == 0:\n                    candidate_dir = dir_sub\n                candidate_dir = candidate_dir / np.linalg.norm(candidate_dir)\n                # propose a stochastic step length (Langevin noise + step)\n                noise = np.random.randn() * 0.15 * step\n                length = np.random.uniform(-step, step) + noise\n                x_prop = clip_to_bounds(x_cur + length * candidate_dir)\n                f_prop, x_prop = safe_eval(x_prop)\n                if f_prop is None:\n                    break\n                if f_prop < f_cur - 1e-12:\n                    # successful; store direction (from x_cur to x_prop)\n                    dir_succ = x_prop - x_cur\n                    dn = np.linalg.norm(dir_succ)\n                    if dn > 0:\n                        dir_succ = dir_succ / dn\n                        dir_memory.insert(0, dir_succ.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                    # quick 1D intensification with a very small budget\n                    remaining_budget = max(0, self.budget - evals)\n                    line_budget = min(6, remaining_budget)\n                    f_line, x_line = short_line_search(x_cur, f_cur, dir_succ, init_step=abs(length) + 0.5 * step, max_evals=line_budget)\n                    if f_line is not None and f_line < f_prop - 1e-12:\n                        f_prop = f_line\n                        x_prop = x_line\n                    # accept improvement\n                    x_cur = x_prop.copy()\n                    f_cur = f_prop\n                    improved = True\n                    no_improve = 0\n                    # adapt step (aggressive growth)\n                    step = min(step * grow, max_step)\n                else:\n                    # with small chance do a focused short line search from current along candidate_dir\n                    if np.random.rand() < 0.06 and evals < self.budget:\n                        remaining_budget = max(0, self.budget - evals)\n                        if remaining_budget >= 3:\n                            f_line, x_line = short_line_search(x_cur, f_cur, candidate_dir, init_step=step, max_evals=min(5, remaining_budget))\n                            if f_line is not None and f_line < f_cur - 1e-12:\n                                # accept\n                                dir_succ = x_line - x_cur\n                                dn = np.linalg.norm(dir_succ)\n                                if dn > 0:\n                                    dir_succ = dir_succ / dn\n                                    dir_memory.insert(0, dir_succ.copy())\n                                    if len(dir_memory) > self.memory_size:\n                                        dir_memory.pop()\n                                x_cur = x_line.copy()\n                                f_cur = f_line\n                                improved = True\n                                no_improve = 0\n                                step = min(step * grow, max_step)\n\n            # After a probing round, adapt step if no improvement\n            if not improved:\n                no_improve += 1\n                step = max(min_step, step * shrink)\n            else:\n                # local polishing: do a few small Gaussian proposals in full space\n                extras = min(6, max(1, int(np.ceil(np.log(1 + n)))))\n                for _ in range(extras):\n                    if evals >= self.budget:\n                        break\n                    d = np.random.randn(n)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                    a = np.random.uniform(-0.5 * step, 0.5 * step)\n                    f_try, x_try = safe_eval(clip_to_bounds(x_cur + a * d))\n                    if f_try is None:\n                        break\n                    if f_try < f_cur - 1e-12:\n                        # store direction relative to previous x_cur\n                        dir_succ = x_try - x_cur\n                        dn = np.linalg.norm(dir_succ)\n                        if dn > 0:\n                            dir_succ = dir_succ / dn\n                            dir_memory.insert(0, dir_succ.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                        x_cur = x_try.copy()\n                        f_cur = f_try\n\n            # stagnation handling\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final polishing: reduce step strongly and continue\n                    step = max(step * 0.5, min_step)\n                    no_improve = 0\n                else:\n                    # restart biased around best with moderate enlargement of step\n                    if x_best is not None:\n                        perturb = np.random.randn(n) * (0.4 * domain_mean)\n                        x_cur = clip_to_bounds(x_best + perturb)\n                        f_new, x_new = safe_eval(x_cur)\n                        if f_new is None:\n                            break\n                        f_cur = f_new\n                    else:\n                        # fresh sample if no best\n                        x_cur = np.random.uniform(lb, ub)\n                        f_new, x_new = safe_eval(x_cur)\n                        if f_new is None:\n                            break\n                        f_cur = f_new\n                    dir_memory = []  # clear memory to diversify\n                    step = min(max_step, step * 1.8)\n                    no_improve = 0\n\n            # enforce minimal step\n            if step < min_step:\n                step = min_step\n\n            # quick stop if extremely low objective\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            # in case nothing improved, return current\n            return float(f_cur), np.array(x_cur, dtype=float)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm MASTL scored 0.213 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "1170b68b-fa6d-49c9-986c-c4c18a103838", "operator": null, "metadata": {"aucs": [0.025967116402732082, 0.14722054137418594, 0.9204430555166244, 0.09323406584993033, 0.0980111693501522, 0.11095287940179677, 0.0215268686145228, 0.47262820279763695, 0.07811863478334591, 0.15912123274798262]}, "task_prompt": ""}
{"id": "d3bac529-3666-4b22-8add-4fb85d47bfb4", "fitness": 0.192289019066453, "name": "ADES", "description": "1) ADES keeps a short memory of recent successful step directions and computes a low-rank PCA (plus random augmentations) to build small k-dimensional orthonormal subspaces, biasing sampling toward principal directions while supplementing with randomness.  \n2) Inside each subspace it runs an ensemble of variance-weighted Gaussian directional probes (probes ≈ max(4,3*k)), immediately accepts improving probes and often refines them with a cheap 3‑point parabolic line fit (at ±δ and an estimated minimizer) to get extra gain for only a few evaluations.  \n3) The algorithm adapts a global absolute step size (initialized from domain via init_step_factor), growing mildly on success (×1.08–1.10) and shrinking on failure (×0.87), and maintains per-memory trust weights that are boosted on success and decayed otherwise to bias future sampling.  \n4) Practical safeguards ensure budget-safety and box constraints (safe_eval, clipping), include tiny local jitter polishing, pruning of low-trust memories, heuristic choices (memory_size=16, min_step floor), stagnation detection with randomized restarts around the best point, and a final tiny-probe polish when restarts are exhausted.", "code": "import numpy as np\n\nclass ADES:\n    \"\"\"\n    Adaptive Directional Ensemble Search (ADES)\n\n    Main ideas / novel mechanisms:\n    - Maintain a small recent-step memory and compute a low-rank PCA to identify\n      promising directions; build small k-dimensional subspaces from these PCA\n      axes + random augmentations.\n    - In each subspace perform an ensemble of cheap directional probes whose\n      coefficients are sampled from a variance-weighted Gaussian (variance ~\n      PCA singular values), preferring directions of recent success.\n    - When a probe yields improvement, perform a low-cost parabolic/3-point\n      line-fit (quadratic fit in alpha) to estimate an improved step along that\n      direction, limiting evaluations to preserve budget.\n    - Keep trust weights for memory directions; increase trust on success,\n      decay on failure; use them to bias future sampling.\n    - Adaptive global step size with multiplicative grow/shrink; stagnation\n      triggers randomized restart around the best solution found so far.\n    - Budget-safe evaluations (never call func more than budget).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=16, min_step=1e-8, init_step_factor=0.5):\n        \"\"\"\n        budget: maximum number of function evaluations\n        dim: dimensionality of the problem\n        seed: optional RNG seed\n        memory_size: number of recent successful steps to keep\n        min_step: absolute minimal step size relative to domain\n        init_step_factor: fraction of domain used for initial step size\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.memory_size = int(memory_size)\n        self._min_step_param = float(min_step)\n        self._init_step_factor = float(init_step_factor)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds from func (Many Affine BBOB: typically -5..5)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # ensure shapes\n        lb = lb.reshape(n,)\n        ub = ub.reshape(n,)\n\n        # budget and eval tracking\n        evals = 0\n\n        # safe eval wrapper\n        def clip_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initialization: start uniformly\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            # budget zero\n            return float(f_best), np.asarray(x_best, dtype=float)\n        # consistent types\n        x_cur = np.asarray(x_cur, dtype=float)\n        f_cur = float(f_cur)\n\n        # step-size initialization (absolute)\n        domain_size = np.mean(ub - lb)\n        step = max(self._init_step_factor * domain_size, 1e-12)\n        min_step = max(self._min_step_param * max(1.0, domain_size), 1e-12)\n\n        # subspace dimension (we choose a bit different heuristic than sqrt(n))\n        k_base = max(2, int(np.clip(int(np.ceil(np.log1p(n) * 3)), 2, n)))  # small adaptive subspace\n        # number of probes per round\n        probes = max(4, 3 * k_base)\n\n        # memory of successful steps and trust weights\n        mem_steps = []  # list of vectors (n,)\n        mem_trust = []  # corresponding trust weights (float)\n\n        # stagnation control\n        no_improve_iters = 0\n        stagnation_limit = max(12, int(8 + 2 * np.log1p(n)))\n        max_restarts = 6\n        restarts = 0\n\n        # helper: build a k-dimensional orthonormal basis biased by PCA of mem_steps\n        def build_subspace(k):\n            # returns n x k orthonormal matrix\n            if len(mem_steps) >= 2:\n                M = np.vstack(mem_steps)  # shape (m, n)\n                # center rows\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                # SVD of Mc (m x n): U S Vt; Vt has principal directions (n)\n                try:\n                    U, S, Vt = np.linalg.svd(Mc, full_matrices=False)\n                except np.linalg.LinAlgError:\n                    # fall back to random\n                    R = np.random.randn(n, k)\n                    Q, _ = np.linalg.qr(R)\n                    return Q[:, :k]\n                # principal directions\n                pcs = Vt.T  # n x r\n                # trust-scaled variance for sampling\n                # combine singular values with trust (if any)\n                # compute effective variances for top components\n                # if mem_trust available, compute weighted variances along pcs\n                # but for simplicity use S (singular values) with small floor\n                ranks = pcs.shape[1]\n                take = min(k, ranks)\n                basis_cols = pcs[:, :take]\n                if take < k:\n                    # supplement with random vectors\n                    R = np.random.randn(n, k - take)\n                    if basis_cols.size == 0:\n                        Q, _ = np.linalg.qr(R)\n                        return Q[:, :k]\n                    stacked = np.column_stack((basis_cols, R))\n                    Q, _ = np.linalg.qr(stacked)\n                    return Q[:, :k]\n                else:\n                    # ensure orthonormal\n                    Q, _ = np.linalg.qr(basis_cols)\n                    return Q[:, :k]\n            else:\n                # insufficient memory, produce random orthonormal basis\n                R = np.random.randn(n, k)\n                Q, _ = np.linalg.qr(R)\n                return Q[:, :k]\n\n        # parabolic (3-point) line minimization along direction d from x0\n        # uses at most 3 evaluations (at +/- delta and at 0 maybe derived)\n        def parabolic_line_min(x0, f0, d, delta=None):\n            # returns (f_new, x_new) or (None, None) if no improvement or budget exhausted\n            nonlocal evals\n            if delta is None:\n                delta = step * 0.8\n            if np.linalg.norm(d) == 0:\n                return None, None\n            d = d / (np.linalg.norm(d) + 1e-20)\n            # points alpha: -delta, 0, +delta\n            remaining = self.budget - evals\n            if remaining <= 0:\n                return None, None\n            # Evaluate at +delta and -delta if not already known\n            x_p = clip_to_bounds(x0 + delta * d)\n            f_p, x_p = safe_eval(x_p)\n            if f_p is None:\n                return None, None\n            remaining = self.budget - evals\n            x_m = clip_to_bounds(x0 - delta * d)\n            if remaining <= 0:\n                # only +delta available; check improvement\n                if f_p < f0 - 1e-12:\n                    return f_p, x_p\n                return None, None\n            f_m, x_m = safe_eval(x_m)\n            if f_m is None:\n                return None, None\n            # Quadratic fit: f(a) = A*a^2 + B*a + C using points (-d, f_m), (0,f0), (d, f_p)\n            # Solve analytically: A = (f_p + f_m - 2*f0) / (2*delta^2); B = (f_p - f_m) / (2*delta)\n            denom = 2.0 * (delta ** 2)\n            A = (f_p + f_m - 2.0 * f0) / denom\n            B = (f_p - f_m) / (2.0 * delta)\n            if A <= 0 or np.isclose(A, 0.0):\n                # parabola not convex or flat: pick best of sampled\n                if f_p < f0 - 1e-12 or f_m < f0 - 1e-12:\n                    if f_p < f_m:\n                        return f_p, x_p\n                    else:\n                        return f_m, x_m\n                return None, None\n            # minimizer alpha* = -B/(2*A)\n            alpha_star = -B / (2.0 * A)\n            # only accept if within reasonable bracket (say |alpha| <= 4*delta)\n            if abs(alpha_star) > 4.0 * delta:\n                # out-of-range suggested; pick best of sampled\n                if f_p < f0 - 1e-12 or f_m < f0 - 1e-12:\n                    if f_p < f_m:\n                        return f_p, x_p\n                    else:\n                        return f_m, x_m\n                return None, None\n            # Evaluate at alpha_star (guard budget)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                # return best of sampled\n                if f_p < f0 - 1e-12 or f_m < f0 - 1e-12:\n                    if f_p < f_m:\n                        return f_p, x_p\n                    else:\n                        return f_m, x_m\n                return None, None\n            x_s = clip_to_bounds(x0 + alpha_star * d)\n            f_s, x_s = safe_eval(x_s)\n            if f_s is None:\n                return None, None\n            if f_s < f0 - 1e-12:\n                return f_s, x_s\n            # else fallback to best of sampled\n            best_fx = f0\n            best_xx = x0.copy()\n            if f_p < best_fx - 1e-12:\n                best_fx = f_p; best_xx = x_p.copy()\n            if f_m < best_fx - 1e-12:\n                best_fx = f_m; best_xx = x_m.copy()\n            if best_fx < f0 - 1e-12:\n                return best_fx, best_xx\n            return None, None\n\n        # main optimization loop: rounds of subspace sampling\n        while evals < self.budget:\n            # adapt k slightly across iterations\n            k = min(n, max(1, int(max(2, k_base + int(np.random.randn() * 0.5)))))\n            basis = build_subspace(k)  # n x k\n\n            # compute coefficient sampling variances from memory SVD if available\n            if len(mem_steps) >= 2:\n                # approximate variances along top k basis vectors by projecting mem_steps\n                M = np.vstack(mem_steps)  # (m, n)\n                proj = M @ basis  # (m, k)\n                var_coef = np.var(proj, axis=0) + 1e-12\n                # bias by average trust\n                avg_trust = np.mean(mem_trust) if mem_trust else 1.0\n                var_coef = var_coef * (1.0 + 0.5 * avg_trust)\n            else:\n                var_coef = np.ones(k, dtype=float)\n\n            # number of probes in this round\n            probes = max(4, 3 * k)\n\n            improved_round = False\n            # candidate storage to optionally select best candidate for additional modeling\n            candidate_list = []\n\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample coefficients with variance weighting\n                coeffs = np.random.randn(k) * np.sqrt(var_coef)\n                d = basis @ coeffs\n                nd = np.linalg.norm(d)\n                if nd == 0:\n                    continue\n                d = d / nd\n                # sample alpha in [-step, step] but biased: sometimes try larger (exploration)\n                if np.random.rand() < 0.15:\n                    alpha = np.random.uniform(-2.0 * step, 2.0 * step)\n                else:\n                    alpha = np.random.uniform(-step, step)\n                x_try = clip_to_bounds(x_cur + alpha * d)\n                f_try, x_try = safe_eval(x_try)\n                if f_try is None:\n                    break\n                candidate_list.append((f_try, x_try.copy(), d.copy(), alpha))\n\n                # immediate acceptance if improved\n                if f_try < f_cur - 1e-12:\n                    # perform cheap parabolic refinement along direction d\n                    # choose delta based on alpha magnitude or step if alpha ~0\n                    delta = max(abs(alpha), 0.9 * step)\n                    f_line, x_line = parabolic_line_min(x_cur, f_cur, d, delta=delta)\n                    if f_line is not None and f_line < f_try - 1e-12:\n                        f_try = f_line\n                        x_try = x_line.copy()\n                    # accept move\n                    step *= 1.10  # mild step growth on success\n                    step = min(step, 5.0 * domain_size)\n                    # update memory: store the step (normalized) and trust\n                    delta_vec = x_try - x_cur\n                    norm_dv = np.linalg.norm(delta_vec)\n                    if norm_dv > 0:\n                        dir_unit = (delta_vec / norm_dv).copy()\n                        mem_steps.insert(0, dir_unit)\n                        mem_trust.insert(0, 1.0)  # fresh trust\n                        # trim memory\n                        if len(mem_steps) > self.memory_size:\n                            mem_steps.pop()\n                            mem_trust.pop()\n                    # update current\n                    x_cur = x_try.copy()\n                    f_cur = float(f_try)\n                    improved_round = True\n                    no_improve_iters = 0\n                    # small local polish: try a tiny orthogonal jitter\n                    if evals < self.budget and np.random.rand() < 0.35:\n                        jitter = np.random.randn(n)\n                        jitter = jitter / (np.linalg.norm(jitter) + 1e-20)\n                        jitter *= 0.2 * step\n                        f_j, x_j = safe_eval(clip_to_bounds(x_cur + jitter))\n                        if f_j is not None and f_j < f_cur - 1e-12:\n                            # update\n                            delta_vec = x_j - x_cur\n                            norm_dv = np.linalg.norm(delta_vec)\n                            if norm_dv > 0:\n                                mem_steps.insert(0, (delta_vec / norm_dv).copy())\n                                mem_trust.insert(0, 1.0)\n                                if len(mem_steps) > self.memory_size:\n                                    mem_steps.pop(); mem_trust.pop()\n                            x_cur = x_j.copy(); f_cur = float(f_j)\n                    # decay trust of other mem entries slightly (they may become stale)\n                    mem_trust = [t * 0.98 for t in mem_trust]\n                    # boost trust of the latest\n                    if mem_trust:\n                        mem_trust[0] = mem_trust[0] + 0.2\n                else:\n                    # slight chance to attempt parabolic fit anyway (to exploit subtle curvature)\n                    if np.random.rand() < 0.04 and (self.budget - evals) >= 3:\n                        f_line, x_line = parabolic_line_min(x_cur, f_cur, d, delta=step * 0.6)\n                        if f_line is not None and f_line < f_cur - 1e-12:\n                            # accept\n                            delta_vec = x_line - x_cur\n                            norm_dv = np.linalg.norm(delta_vec)\n                            if norm_dv > 0:\n                                mem_steps.insert(0, (delta_vec / norm_dv).copy())\n                                mem_trust.insert(0, 1.0)\n                                if len(mem_steps) > self.memory_size:\n                                    mem_steps.pop(); mem_trust.pop()\n                            x_cur = x_line.copy(); f_cur = float(f_line)\n                            improved_round = True\n                            no_improve_iters = 0\n                            step *= 1.08\n                            mem_trust = [t * 0.98 for t in mem_trust]\n\n                # end single probe\n\n            # End of probes in this round\n\n            # if no improvement, try to use candidate_list to pick promising direction and attempt a model-improvement\n            if not improved_round and candidate_list:\n                # pick top few best candidates\n                candidate_list.sort(key=lambda z: z[0])\n                topk = min(3, len(candidate_list))\n                for i in range(topk):\n                    f_try, x_try, d_try, alpha_try = candidate_list[i]\n                    # attempt parabolic fit along d_try but anchored at x_cur\n                    if self.budget - evals >= 3:\n                        f_line, x_line = parabolic_line_min(x_cur, f_cur, d_try, delta=max(step, abs(alpha_try)))\n                        if f_line is not None and f_line < f_cur - 1e-12:\n                            # accept\n                            delta_vec = x_line - x_cur\n                            norm_dv = np.linalg.norm(delta_vec)\n                            if norm_dv > 0:\n                                mem_steps.insert(0, (delta_vec / norm_dv).copy())\n                                mem_trust.insert(0, 1.0)\n                                if len(mem_steps) > self.memory_size:\n                                    mem_steps.pop(); mem_trust.pop()\n                            x_cur = x_line.copy(); f_cur = float(f_line)\n                            improved_round = True\n                            no_improve_iters = 0\n                            step *= 1.09\n                            break\n\n            # Step-size adaptation: shrink on failure\n            if not improved_round:\n                no_improve_iters += 1\n                step *= 0.87\n                # decay trust of memory entries that did not help recently\n                mem_trust = [t * 0.96 for t in mem_trust]\n            else:\n                # slight trust increase for memory top entries\n                mem_trust = [min(t * 1.05, 10.0) for t in mem_trust]\n\n            # prune very small trust entries\n            if mem_trust:\n                keep_idx = [i for i, t in enumerate(mem_trust) if t > 1e-3]\n                if len(keep_idx) != len(mem_trust):\n                    mem_steps = [mem_steps[i] for i in keep_idx]\n                    mem_trust = [mem_trust[i] for i in keep_idx]\n\n            # safeguard step lower bound\n            if step < min_step:\n                step = min_step\n\n            # stagnation / restart logic\n            if no_improve_iters >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final polishing: try many tiny random local probes until budget exhausted\n                    while evals < self.budget:\n                        d = np.random.randn(n)\n                        d /= (np.linalg.norm(d) + 1e-20)\n                        alpha = np.random.uniform(-0.2 * step, 0.2 * step)\n                        f_try, x_try = safe_eval(clip_to_bounds(x_cur + alpha * d))\n                        if f_try is None:\n                            break\n                        if f_try < f_cur - 1e-12:\n                            x_cur = x_try.copy(); f_cur = float(f_try)\n                            mem_steps.insert(0, (x_cur - x_best) / (np.linalg.norm(x_cur - x_best) + 1e-20))\n                            if len(mem_steps) > self.memory_size:\n                                mem_steps.pop()\n                        # small shrink\n                        step *= 0.95\n                        if step < min_step:\n                            break\n                    break  # budget or polish done\n                else:\n                    # restart around best found so far (diversify)\n                    if x_best is not None:\n                        radius = min(1.0, 0.6 * (1.0 + restarts / 2.0)) * domain_size\n                        perturb = np.random.randn(n) * radius\n                        x_cur = clip_to_bounds(x_best + perturb)\n                        f_new, x_cur = safe_eval(x_cur)\n                        if f_new is None:\n                            break\n                        f_cur = float(f_new)\n                    else:\n                        # fallback random restart\n                        x_cur = np.random.uniform(lb, ub)\n                        f_new, x_cur = safe_eval(x_cur)\n                        if f_new is None:\n                            break\n                        f_cur = float(f_new)\n                    # enlarge step to escape basins\n                    step = min(2.0 * step + 0.2 * domain_size, 5.0 * domain_size)\n                    # reset memory slightly\n                    mem_trust = [t * 0.3 for t in mem_trust]\n                    no_improve_iters = 0\n\n            # optional early exit if f_best extremely small (practical sentinel)\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            # fallback: return current\n            x_best = x_cur.copy()\n            f_best = float(f_cur)\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ADES scored 0.192 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "1170b68b-fa6d-49c9-986c-c4c18a103838", "operator": null, "metadata": {"aucs": [0.1003123690031208, 0.14915034911102154, 0.23513175362111183, 0.338776726215414, 0.15585888170932904, 0.22707975140582815, 0.22162404347186504, 0.1567305503776486, 0.20224200528506642, 0.13598376046412453]}, "task_prompt": ""}
{"id": "4c818849-323e-4065-9772-bd26cb830a8b", "fitness": 0.3938266728217954, "name": "ADES", "description": "ADES combines adaptive subspace search with directional memory and budget-aware local refinement: it builds a moderately large subspace k ≈ n^0.6 (larger than sqrt(n)) and uses many probes per round (probes = max(6, 3*k) with heavy‑tailed random coefficients) to favor broader exploration. It keeps a probabilistic FIFO memory of good unit directions (memory_size=12) and a per-coordinate scaling vector to bias anisotropic perturbations and large jumps, while orthonormalizing random + memory vectors via QR to form the search basis. Step management is conservative initially (init_step_scale=0.25 of domain) but adapts aggressively on outcomes (grow=1.30, shrink=0.60), with cheap 3‑point parabolic 1‑D refinements along promising directions to exploit curvature using very few extra evaluations. Finally, the algorithm is strictly budget-aware (safe_eval limits calls), includes occasional anisotropic restarts and light local polishing (Gaussian coordinate perturbations and small coordinate sweeps) to escape stagnation and refine solutions.", "code": "import numpy as np\n\n# Main algorithm parameters (from the original ARSS and how ADES differs):\n# - subspace dimension k: ARSS used k ≈ sqrt(n). ADES uses k ≈ n**0.6 (slightly larger for moderate n).\n# - probes per round: ARSS used probes = max(4, 2*k). ADES uses probes = max(6, 3*k) to explore more directions.\n# - initial step: ARSS used 0.4 * domain. ADES uses a smaller init_step_scale (default 0.25 * domain) for more conservative local moves.\n# - min_step: scaled similarly but slightly more permissive in ADES.\n# - step adaptation: ARSS grow ~1.12–1.15 and shrink 0.85. ADES uses more aggressive multipliers (grow 1.30, shrink 0.60).\n# - memory of directions: ARSS memory_size default 8. ADES uses a larger default (12) and samples memory probabilistically.\n# - line search: ARSS used golden section with expansion. ADES uses cheap 3-point parabolic interpolation (fewer evals).\n# - stagnation & restart: ADES uses adaptive patience and occasional large jump restarts with anisotropic scaling.\n#\n# ADES design: keep safe, budget-aware evaluations; combine memory-guided directions with random directions,\n# perform quick parabolic 1-D refinements when promising, maintain a directional ensemble and a diagonal\n# adaptive scaling (per-coordinate) to bias perturbations.\n\nclass ADES:\n    \"\"\"\n    Adaptive Directional Ensemble Search (ADES)\n    - budget: number of allowed function evaluations\n    - dim: problem dimensionality\n    Optional tuning parameters (safe defaults chosen for Many Affine BBOB [-5,5] tasks).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=12,\n                 init_step_scale=0.25,   # fraction of domain for initial step\n                 subspace_exp=0.6,       # exponent for subspace dimension: k = ceil(n**subspace_exp)\n                 probes_factor=3,        # probes per round = max(6, probes_factor * k)\n                 grow=1.30, shrink=0.60, # step multipliers on success/failure (more aggressive than ARSS)\n                 max_restarts=6,\n                 min_step_scale=1e-9):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.init_step_scale = float(init_step_scale)\n        self.subspace_exp = float(subspace_exp)\n        self.probes_factor = int(probes_factor)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.max_restarts = int(max_restarts)\n        self.min_step_scale = float(min_step_scale)\n        # RNG local to the instance\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain = np.maximum(1e-12, np.mean(ub - lb))\n\n        evals = 0\n\n        # start at a random point inside bounds\n        x_cur = self.rng.uniform(lb, ub)\n        f_cur = func(x_cur); evals += 1\n        x_best = x_cur.copy()\n        f_best = float(f_cur)\n\n        # step sizes and limits\n        step = self.init_step_scale * domain\n        min_step = self.min_step_scale * domain\n        max_step = 2.0 * domain\n\n        # subspace dimension: slightly larger than sqrt(n)\n        k = max(1, int(np.ceil(n ** self.subspace_exp)))\n        probes = max(6, self.probes_factor * k)\n\n        # memory of good unit directions\n        dir_memory = []\n\n        # per-coordinate scaling (diagonal adapt) to bias perturbations\n        coord_scale = np.ones(n)\n\n        # stagnation\n        no_improve = 0\n        stagnation_limit = max(12, int(8 + np.log(1 + n) * 6))\n        restarts = 0\n\n        # helper: clip to bounds\n        def clip_to_bounds(x):\n            return np.minimum(ub, np.maximum(lb, x))\n\n        # safe evaluation: returns (f, x_clipped) or (None, None) if budget exhausted\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(x)\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # cheap 3-point parabolic 1-D refinement along direction d:\n        # Uses at most max_evals evaluations (including endpoints), returns improved (f, x) or (None, None)\n        def quick_parabola(x0, f0, d, init_step=1.0, max_evals=5):\n            dn = np.linalg.norm(d)\n            if dn == 0 or max_evals < 1:\n                return None, None\n            d = d / dn\n            remaining = max(0, self.budget - evals)\n            if remaining <= 0:\n                return None, None\n            # Evaluate at +s and -s (if budget allows)\n            s = init_step\n            pts = [(0.0, f0, x0.copy())]\n            # attempt +s\n            if remaining >= 1:\n                x1 = clip_to_bounds(x0 + s * d)\n                f1, _ = safe_eval(x1)\n                if f1 is None:\n                    return None, None\n                pts.append((s, f1, x1.copy()))\n                remaining -= 1\n            # attempt -s\n            if remaining >= 1:\n                x2 = clip_to_bounds(x0 - s * d)\n                f2, _ = safe_eval(x2)\n                if f2 is None:\n                    return None, None\n                pts.append((-s, f2, x2.copy()))\n                remaining -= 1\n            # If we only have one side better, try doubling that side (cheap expansion)\n            pts_sorted = sorted(pts, key=lambda t: t[1])\n            best_alpha, best_f, best_x = pts_sorted[0]\n            # If best at non-zero alpha and we still have budget, try doubling to get curvature\n            if best_alpha != 0.0 and remaining >= 1 and abs(best_alpha) < 10 * domain:\n                new_alpha = 2.0 * best_alpha\n                x3 = clip_to_bounds(x0 + new_alpha * d)\n                f3, _ = safe_eval(x3)\n                if f3 is None:\n                    return None, None\n                pts.append((new_alpha, f3, x3.copy()))\n                remaining -= 1\n                pts_sorted = sorted(pts, key=lambda t: t[1])\n                best_alpha, best_f, best_x = pts_sorted[0]\n\n            # Need three distinct alpha points to fit parabola; pick top 3 by evaluation order or by spread\n            if len(pts) < 3:\n                # Not enough info; return the best non-zero point if better than f0\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n\n            # choose three alphas with distinct positions and reasonably spread\n            alphas = np.array([p[0] for p in pts])\n            fs = np.array([p[1] for p in pts])\n            xs = [p[2] for p in pts]\n            # pick min f and two other points with alpha different\n            idx_sorted = np.argsort(fs)\n            i0 = idx_sorted[0]\n            # pick a left and right anchor among remaining\n            others = [i for i in idx_sorted[1:]]\n            # choose two anchors maximizing spread in alpha\n            if len(others) >= 2:\n                # choose pair with max |alpha - alpha_best|\n                dists = [abs(alphas[i] - alphas[i0]) for i in others]\n                j = np.argmax(dists)\n                i1 = others[j]\n                others2 = [o for o in others if o != i1]\n                if others2:\n                    # pick next most distant\n                    dists2 = [abs(alphas[o] - alphas[i0]) for o in others2]\n                    k_idx = np.argmax(dists2)\n                    i2 = others2[k_idx]\n                else:\n                    i2 = i1\n            elif len(others) == 1:\n                i1 = others[0]; i2 = others[0]\n            else:\n                # fallback\n                i1 = i0; i2 = i0\n\n            a1, f1, x1 = alphas[i0], fs[i0], xs[i0]\n            a2, f2, x2 = alphas[i1], fs[i1], xs[i1]\n            a3, f3, x3 = alphas[i2], fs[i2], xs[i2]\n\n            # Fit parabola f(alpha) = A*alpha^2 + B*alpha + C via three points\n            M = np.array([[a1*a1, a1, 1.0],\n                          [a2*a2, a2, 1.0],\n                          [a3*a3, a3, 1.0]])\n            y = np.array([f1, f2, f3])\n            try:\n                coeffs = np.linalg.solve(M, y)\n                A, B, C = coeffs\n                if A == 0:\n                    return (best_f, best_x) if best_f < f0 - 1e-12 else (None, None)\n                alpha_opt = -B / (2.0 * A)\n            except np.linalg.LinAlgError:\n                # ill-conditioned; return best seen\n                return (best_f, best_x) if best_f < f0 - 1e-12 else (None, None)\n\n            # enforce alpha_opt not too far from sampled alphas\n            max_span = max(abs(alphas.max() - alphas.min()), 1e-12)\n            alpha_opt = np.clip(alpha_opt, alphas.min() - 0.5 * max_span, alphas.max() + 0.5 * max_span)\n            if remaining >= 1:\n                x_opt = clip_to_bounds(x0 + alpha_opt * d)\n                f_opt, _ = safe_eval(x_opt)\n                if f_opt is None:\n                    return None, None\n                if f_opt < f0 - 1e-12:\n                    return f_opt, x_opt\n                # else, return best seen if better than f0\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n            else:\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n            return None, None\n\n        # Main optimization loop\n        while evals < self.budget:\n            # adapt subspace dimension occasionally\n            k = max(1, int(np.clip(int(np.ceil(n ** self.subspace_exp)), 1, n)))\n            probes = max(6, self.probes_factor * k)\n\n            improved = False\n\n            # Build subspace basis: probabilistically reuse some memory directions\n            use_mem = min(len(dir_memory), max(0, k // 2))\n            chosen = []\n            if use_mem > 0:\n                # sample without replacement favoring recent (front of list)\n                probs = np.linspace(1.0, 0.5, len(dir_memory))\n                probs = probs / probs.sum()\n                idxs = self.rng.choice(len(dir_memory), size=use_mem, replace=False, p=probs)\n                for i in idxs:\n                    chosen.append(dir_memory[i])\n            # fill remaining with random vectors (and apply coord_scale anisotropy)\n            needed = k - len(chosen)\n            if needed > 0:\n                R = self.rng.normal(size=(n, needed)) * coord_scale.reshape(-1, 1)\n                if chosen:\n                    R = np.column_stack((np.column_stack(chosen), R))\n                # orthonormalize via QR and take first k\n                Q, _ = np.linalg.qr(R)\n                basis = Q[:, :k]\n            else:\n                B = np.column_stack(chosen)\n                Q, _ = np.linalg.qr(B)\n                basis = Q[:, :k]\n\n            # Probing within subspace\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample coefficients with heavier-tailed distribution (student-t like by normal scaled)\n                coeffs = self.rng.normal(size=k) * (1.0 + 0.5 * self.rng.standard_exponential(size=k))\n                d = basis @ coeffs\n                dn = np.linalg.norm(d)\n                if dn == 0:\n                    continue\n                d = d / dn\n                # perturbation scaled coordinate-wise and by step\n                alpha = self.rng.uniform(-1.0, 1.0) * step\n                x_try = clip_to_bounds(x_cur + alpha * d)\n                f_try, x_ret = safe_eval(x_try)\n                if f_try is None:\n                    break\n                if f_try < f_cur - 1e-12:\n                    # success: record direction and perform quick parabola along the direction\n                    dir_succ = x_ret - x_cur\n                    dn2 = np.linalg.norm(dir_succ)\n                    if dn2 > 0:\n                        dir_unit = dir_succ / dn2\n                        dir_memory.insert(0, dir_unit.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                        # update coordinate scales to favor components that changed\n                        coord_scale = 0.85 * coord_scale + 0.15 * (1.0 + np.abs(dir_unit))\n                    # cheap 3-point refinement\n                    remaining_budget = max(0, self.budget - evals)\n                    maxevals_parab = min(5, remaining_budget)\n                    f_line, x_line = quick_parabola(x_cur, f_cur, dir_unit if dn2 > 0 else d,\n                                                    init_step=alpha if abs(alpha) > 1e-16 else step,\n                                                    max_evals=maxevals_parab)\n                    if f_line is not None and f_line < f_try - 1e-12:\n                        f_try = f_line\n                        x_ret = x_line\n                    # accept\n                    x_cur = x_ret.copy()\n                    f_cur = f_try\n                    improved = True\n                    no_improve = 0\n                    # adapt step more aggressively on success\n                    step = min(step * self.grow, max_step)\n                else:\n                    # occasionally try a small parabola from current point along this d (rare)\n                    if self.rng.random() < 0.04 and self.budget - evals >= 3:\n                        f_line, x_line = quick_parabola(x_cur, f_cur, d, init_step=0.5 * step, max_evals=4)\n                        if f_line is not None and f_line < f_cur - 1e-12:\n                            x_cur = x_line.copy()\n                            f_cur = f_line\n                            improved = True\n                            no_improve = 0\n                            # store direction\n                            dir_memory.insert(0, d.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                            step = min(step * self.grow, max_step)\n\n            # After probing round\n            if not improved:\n                no_improve += 1\n                # shrink step\n                step = max(step * self.shrink, min_step)\n                # slightly random jump occasionally to escape local traps\n                if no_improve % max(2, stagnation_limit // 3) == 0 and restarts < self.max_restarts:\n                    # anisotropic large jump biased by coord_scale and memory\n                    large = (0.8 * domain) * (1.0 + 0.5 * self.rng.standard_exponential(size=n)) * (coord_scale / coord_scale.max())\n                    jump = (self.rng.normal(size=n) * large)\n                    # also mix in a weighted memory direction if available\n                    if dir_memory:\n                        mem_dir = dir_memory[0]\n                        jump = 0.6 * jump + 0.4 * (mem_dir * domain * 0.6)\n                    x_new = clip_to_bounds(x_best + jump)\n                    f_new, _ = safe_eval(x_new)\n                    if f_new is None:\n                        break\n                    if f_new < f_cur - 1e-12:\n                        x_cur = x_new.copy()\n                        f_cur = f_new\n                        improved = True\n                        no_improve = 0\n                        step = min(step * self.grow, max_step)\n            else:\n                # light local polishing: a few small gaussian coordinate perturbations\n                polishes = min(6, 2 + int(np.log(1 + n)))\n                for _ in range(polishes):\n                    if evals >= self.budget:\n                        break\n                    noise = self.rng.normal(scale=0.25 * step * coord_scale)\n                    x_p = clip_to_bounds(x_cur + noise)\n                    f_p, x_r = safe_eval(x_p)\n                    if f_p is None:\n                        break\n                    if f_p < f_cur - 1e-12:\n                        # store direction based on displacement\n                        dir_succ = x_r - x_cur\n                        if np.linalg.norm(dir_succ) > 0:\n                            dir_unit = dir_succ / (np.linalg.norm(dir_succ) + 1e-20)\n                            dir_memory.insert(0, dir_unit.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                            coord_scale = 0.9 * coord_scale + 0.1 * (1.0 + np.abs(dir_unit))\n                        x_cur = x_r.copy()\n                        f_cur = f_p\n                        no_improve = 0\n\n            # stagnation handling: restart around best with anisotropic scaling or terminate after many restarts\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                if restarts > self.max_restarts or step <= min_step * 1.1:\n                    # final polishing: reduce step a lot and do short adaptive random search\n                    step = max(step * 0.4, min_step)\n                    # attempt a small local sweep using coordinate-wise mini search\n                    coords = self.rng.permutation(n)[:min(n, 6)]\n                    for i in coords:\n                        if evals >= self.budget:\n                            break\n                        # try plus/minus along coordinate direction\n                        d = np.zeros(n); d[i] = 1.0\n                        for sign in (+1, -1):\n                            a = sign * 0.8 * step * coord_scale[i]\n                            f_try, x_ret = safe_eval(clip_to_bounds(x_cur + a * d))\n                            if f_try is None:\n                                break\n                            if f_try < f_cur - 1e-12:\n                                x_cur = x_ret.copy(); f_cur = f_try\n                                improved = True\n                                no_improve = 0\n                else:\n                    # diversify: restart around best with larger step and some memory reset\n                    perturb = self.rng.normal(scale=0.6 * domain, size=n) * (coord_scale / coord_scale.max())\n                    x_cur = clip_to_bounds(x_best + perturb)\n                    res = safe_eval(x_cur)\n                    if res[0] is None:\n                        break\n                    f_cur, _ = res\n                    # clear half of memory and inflate step\n                    keep = max(0, len(dir_memory) // 2)\n                    dir_memory = dir_memory[:keep]\n                    step = min(max_step, step * 2.0)\n                    no_improve = 0\n\n            # safeguard step lower bound\n            if step < min_step:\n                step = min_step\n\n            # early exit if extremely small best\n            if f_best <= 1e-14:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ADES scored 0.394 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "1170b68b-fa6d-49c9-986c-c4c18a103838", "operator": null, "metadata": {"aucs": [0.11119327317007233, 0.16343192515848015, 0.6444907070127319, 0.9574365326762819, 0.20203408284402002, 0.9473472440862144, 0.20631402482392835, 0.30861488916979507, 0.26427599805152135, 0.13312805122490823]}, "task_prompt": ""}
{"id": "fe83e6cb-1ae6-4047-b1c4-c53a2ab7dedf", "fitness": "-inf", "name": "MiSCH", "description": "MiSCH mixes isotropic Gaussian probes with a low-rank LRU memory of recent successful displacement directions (deque dir_mem, orthonormalized by QR) so search favors learned subspace directions while retaining full-space exploration. Samples are generated as a mixture of a memory-driven unit direction and an isotropic unit vector with a folded-normal radius, evaluated in small populations (pop_size ~ max(6,4+√n)) and clipped to box bounds. Step-size sigma is adapted multiplicatively (success_alpha=1.18, failure_alpha=0.82) and coupled to an evolution path (evo_path with decay 0.85) used for occasional budget-aware 1-D golden-section line-search and cheap local polishing when improvements occur. The algorithm is budget-aware (never exceeds self.budget), includes stagnation detection with limited restarts and larger perturbations, and keeps conservative parameter choices (mem_mix=0.6, limited line-search/max evaluations, sigma clamping) to balance exploration, exploitation and robustness.", "code": "import numpy as np\nfrom collections import deque\n\nclass MiSCH:\n    \"\"\"\n    Memory-biased Subspace Covariance Hybrid (MiSCH)\n    - Mixes isotropic Gaussian probes with low-rank memory directions (LRU memory).\n    - Uses multiplicative sigma adaptation (grow on success, shrink on failure).\n    - Performs cheap budget-aware 1-D line-search polishing along evolution-path directions.\n    - Uses orthonormalization (QR) for memory basis and restarts on stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=None, memory_size=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.memory_size = int(memory_size)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.pop_size = int(pop_size) if pop_size is not None else max(6, 4 + int(np.sqrt(self.dim)))\n\n    def __call__(self, func):\n        n = self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # helper: clip to bounds\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n        # initial mean uniform in bounds\n        mean = np.random.uniform(lb, ub)\n        # best tracking\n        f_best = np.inf\n        x_best = mean.copy()\n\n        # initial evaluation\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(x)\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x.copy()\n\n        f_mean, x_mean = safe_eval(mean)\n        if f_mean is None:\n            return float(f_best), np.array(x_best, dtype=float)\n        # initial parameters\n        sigma = 0.25 * np.mean(ub - lb)  # step-size\n        sigma_min = 1e-8 * np.maximum(1.0, np.mean(ub - lb))\n        sigma_max = 5.0 * np.mean(ub - lb)\n        # memory of directions (unit vectors), LRU: newest at left\n        dir_mem = deque(maxlen=self.memory_size)\n        # evolution path accumulator\n        evo_path = np.zeros(n)\n        evo_decay = 0.85\n        # parameters\n        pop_size = max(2, int(self.pop_size))\n        mem_mix = 0.6  # weight of memory component when available (blended)\n        success_alpha = 1.18\n        failure_alpha = 0.82\n        stagnation_limit = max(12, int(12 + 2 * np.log(1 + n)))\n        no_improve = 0\n        restarts = 0\n        max_restarts = 5\n\n        # small budget-aware golden-section line search along direction d from x0\n        def line_search(x0, f0, d, init_step=None, max_evals=12):\n            nonlocal evals\n            if init_step is None:\n                init_step = sigma\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            remain = max(0, self.budget - evals)\n            if remain <= 0:\n                return None, None\n\n            # Try +init and -init\n            a0 = 0.0\n            fa = f0\n            b = init_step\n            xb = clip(x0 + b * d)\n            fb, _ = safe_eval(xb)\n            if fb is None:\n                return None, None\n            remain -= 1\n            if fb >= fa:\n                b = -init_step\n                xb = clip(x0 + b * d)\n                fb2, _ = safe_eval(xb)\n                if fb2 is None:\n                    return None, None\n                remain -= 1\n                if fb2 >= fa:\n                    return None, None\n                fb = fb2\n\n            # Expand bracket a bit\n            expand = 1.5\n            max_exp = 4\n            exp_count = 0\n            while remain > 0 and exp_count < max_exp:\n                new_b = b * expand\n                xn = clip(x0 + new_b * d)\n                fn, _ = safe_eval(xn)\n                if fn is None:\n                    return None, None\n                remain -= 1\n                if fn < fb:\n                    b = new_b\n                    fb = fn\n                    exp_count += 1\n                    continue\n                break\n\n            # golden section on [0, b]\n            if remain <= 0:\n                if fb < fa:\n                    return fb, clip(x0 + b * d)\n                return None, None\n\n            gr = (np.sqrt(5) - 1) / 2\n            a = a0\n            b0 = b\n            c = b0 - gr * (b0 - a)\n            d_alpha = a + gr * (b0 - a)\n            xc = clip(x0 + c * d)\n            fc, _ = safe_eval(xc)\n            if fc is None:\n                return None, None\n            remain -= 1\n            xd = clip(x0 + d_alpha * d)\n            fd, _ = safe_eval(xd)\n            if fd is None:\n                return None, None\n            remain -= 1\n\n            bestf = fa\n            bestx = x0.copy()\n            for val, xcand in ((fc, xc), (fd, xd)):\n                if val < bestf:\n                    bestf = val\n                    bestx = xcand.copy()\n\n            it = 0\n            max_iters = max_evals\n            while remain > 0 and it < max_iters and abs(b0 - a) > 1e-12:\n                it += 1\n                if fc < fd:\n                    b0 = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    d_alpha = a + gr * (b0 - a)\n                    xd = clip(x0 + d_alpha * d)\n                    fd, _ = safe_eval(xd)\n                    if fd is None:\n                        break\n                    remain -= 1\n                    if fd < bestf:\n                        bestf = fd; bestx = xd.copy()\n                else:\n                    a = c\n                    c = d_alpha\n                    fc = fd\n                    c = b0 - gr * (b0 - a)\n                    xc = clip(x0 + c * d)\n                    fc, _ = safe_eval(xc)\n                    if fc is None:\n                        break\n                    remain -= 1\n                    if fc < bestf:\n                        bestf = fc; bestx = xc.copy()\n\n            if bestf < f0:\n                return bestf, bestx\n            return None, None\n\n        # function to build orthonormal basis from dir_mem\n        def build_basis():\n            if len(dir_mem) == 0:\n                return None\n            M = np.column_stack(list(dir_mem))  # n x m\n            # QR to orthonormalize\n            try:\n                Q, _ = np.linalg.qr(M)\n            except Exception:\n                # fallback: normalize columns individually\n                cols = []\n                for v in M.T:\n                    vn = np.linalg.norm(v)\n                    if vn > 1e-16:\n                        cols.append(v / vn)\n                if len(cols) == 0:\n                    return None\n                Q = np.column_stack(cols)\n            return Q  # n x m_orth\n\n        # Main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            if remaining <= 0:\n                break\n            # adapt pop for remaining budget\n            n_eval_pop = min(pop_size, remaining)\n            # build memory basis\n            basis = build_basis()  # n x m or None\n\n            samples = []\n            # sample population\n            for i in range(n_eval_pop):\n                # sample direction as mixture of isotropic and memory-subspace\n                if basis is not None and basis.shape[1] > 0 and np.random.rand() < 0.7:\n                    # memory-driven component\n                    coeffs = np.random.randn(basis.shape[1])\n                    mem_comp = basis @ coeffs\n                    mem_comp = mem_comp / (np.linalg.norm(mem_comp) + 1e-20)\n                    iso = np.random.randn(n)\n                    iso = iso / (np.linalg.norm(iso) + 1e-20)\n                    mix = mem_mix\n                    d = mix * mem_comp + (1.0 - mix) * iso\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                else:\n                    d = np.random.randn(n)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                # sample radius from normal to allow varied step lengths\n                r = np.abs(np.random.randn())  # folded normal\n                x = mean + sigma * r * d\n                x = clip(x)\n                samples.append((x, d, r))\n\n            # evaluate samples\n            evaluated = []\n            for x, d, r in samples:\n                f, x_clipped = safe_eval(x)\n                if f is None:\n                    break\n                evaluated.append((f, x_clipped, d, r))\n            if len(evaluated) == 0:\n                break\n\n            # choose best in population\n            evaluated.sort(key=lambda t: t[0])\n            f_pop_best, x_pop_best, d_pop_best, r_pop_best = evaluated[0]\n\n            improved = False\n            # if population best improved global best or mean, update\n            if f_pop_best < f_best - 1e-15:\n                # move mean toward this good sample (small learning rate)\n                step_lr = 0.6\n                old_mean = mean.copy()\n                mean = mean + step_lr * (x_pop_best - mean)\n                # update evolution path\n                evo_step = (mean - old_mean)\n                evo_path = evo_decay * evo_path + (1.0 - evo_decay) * evo_step\n                # store normalized successful displacement in memory\n                disp = x_pop_best - old_mean\n                dn = np.linalg.norm(disp)\n                if dn > 1e-16:\n                    vec = disp / dn\n                    dir_mem.appendleft(vec.copy())\n                # increase sigma moderately\n                sigma = min(sigma * success_alpha, sigma_max)\n                improved = True\n                no_improve = 0\n            else:\n                # move mean slightly toward the best sample in pop to keep exploration\n                lr = 0.05\n                old_mean = mean.copy()\n                mean = mean + lr * (x_pop_best - mean)\n                evo_step = (mean - old_mean)\n                evo_path = evo_decay * evo_path + (1.0 - evo_decay) * evo_step\n                # shrink sigma\n                sigma = max(sigma * failure_alpha, sigma_min)\n                no_improve += 1\n\n            # occasional cheap line_search along the evolution path if it has magnitude\n            if np.linalg.norm(evo_path) > 1e-12 and np.random.rand() < 0.35 and (self.budget - evals) >= 4:\n                d_evo = evo_path.copy()\n                f_mean_curr, _ = safe_eval(mean)  # evaluate current mean if needed (may already be evaluated)\n                if f_mean_curr is None:\n                    break\n                # attempt line search\n                max_ls = min(10, self.budget - evals)\n                res = line_search(mean, f_mean_curr, d_evo, init_step=sigma, max_evals=max_ls)\n                if res is not None:\n                    f_ls, x_ls = res\n                    if f_ls is not None and f_ls < f_mean_curr - 1e-15:\n                        mean = x_ls.copy()\n                        # store direction\n                        disp = x_ls - mean\n                        if np.linalg.norm(disp) > 1e-16:\n                            dir_mem.appendleft(disp / (np.linalg.norm(disp)))\n                        sigma = min(sigma * success_alpha, sigma_max)\n                        improved = True\n                        no_improve = 0\n\n            # micro-polishing around new mean when there was improvement\n            if improved:\n                # a few local small probes\n                local_probe = min(6, max(1, int(np.ceil(np.log(1 + n)))))\n                for _ in range(local_probe):\n                    if evals >= self.budget:\n                        break\n                    dloc = np.random.randn(n)\n                    dloc = dloc / (np.linalg.norm(dloc) + 1e-20)\n                    a = np.random.uniform(-0.5 * sigma, 0.5 * sigma)\n                    f_try, x_try = safe_eval(clip(mean + a * dloc))\n                    if f_try is None:\n                        break\n                    if f_try < f_best - 1e-15:\n                        mean = x_try.copy()\n                        # add to memory\n                        disp = x_try - mean\n                        if np.linalg.norm(disp) > 1e-16:\n                            dir_mem.appendleft(disp / (np.linalg.norm(disp)))\n                        sigma = min(sigma * success_alpha, sigma_max)\n                        no_improve = 0\n\n            # stagnation / restart handling\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final polishing: reduce sigma\n                    sigma = max(sigma * 0.5, sigma_min)\n                    no_improve = 0\n                else:\n                    # diverse restart around global best with larger sigma\n                    if np.isfinite(f_best):\n                        perturb = np.random.randn(n) * (0.4 * np.mean(ub - lb))\n                        mean = clip(x_best + perturb)\n                        fe, _ = safe_eval(mean)\n                        if fe is None:\n                            break\n                    else:\n                        mean = np.random.uniform(lb, ub)\n                        fe, _ = safe_eval(mean)\n                        if fe is None:\n                            break\n                    # clear memory and enlarge sigma\n                    dir_mem = deque(maxlen=self.memory_size)\n                    sigma = min(0.8 * np.mean(ub - lb), sigma * 2.0)\n                    no_improve = 0\n\n            # safety clamp sigma\n            sigma = np.clip(sigma, sigma_min, sigma_max)\n\n            # quick exit if best is extremely small\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 66, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_mem = deque(maxlen=self.memory_size)", "error": "In the code, line 66, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_mem = deque(maxlen=self.memory_size)", "parent_ids": "1170b68b-fa6d-49c9-986c-c4c18a103838", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "83618236-2a28-4ed9-8668-80c937a24407", "fitness": 0.17897048458515347, "name": "MDARSS", "description": "The algorithm mixes global and local moves by probing randomized low-dimensional subspaces (k ≈ ceil(√n)) built from an orthonormal QR basis, using mirrored sampling (paired ± coefficient vectors) and per-coordinate RMS scaling to bias directions according to recent step magnitudes. It keeps an LRU directional memory of successful unit moves (memory_size=8) and augments exploration with occasional DE-like archive-differences and Cauchy-style directional jumps (mem_jump_prob and cauchy_scale) to escape basins, while adapting a global step size (grow/shrink factors) on success/failure. Budget-aware exploitation is provided by cheap 1D golden-section line searches along successful directions and periodic diagonal quadratic model proposals (ridge-regressed x_j^2 and x_j terms) built from the best archive points to suggest focused minimizers. Practical robustness comes from strict budget checks, boundary clipping, archive pruning, small initial step (0.38·mean(range)), min_step safeguards, and conservative parameter choices (model_every=18, small coord variance smoothing) to balance exploration and reliable local improvement.", "code": "import numpy as np\n\nclass MDARSS:\n    \"\"\"\n    Memory-Directed Adaptive Random Subspace Search (MDARSS)\n    One-line: Combine random subspace probing and cheap 1D line-search with per-coordinate RMS scaling,\n    mirrored sampling and directional memory + occasional Cauchy jumps and lightweight diagonal quadratic\n    model proposals for robust budget-aware global-local optimization.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=8, model_every=18, mem_jump_prob=0.18, cauchy_scale=0.6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.model_every = int(model_every)\n        self.mem_jump_prob = float(mem_jump_prob)\n        self.cauchy_scale = float(cauchy_scale)\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        # initial point\n        x_cur = self.rng.uniform(lb, ub)\n        evals = 0\n\n        # safe eval wrapper\n        X_archive = []\n        F_archive = []\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            x_clipped = np.clip(x, lb, ub)\n            try:\n                f = float(func(x_clipped))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x_clipped.copy())\n            F_archive.append(f)\n            return f, x_clipped\n\n        # initial evaluate\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            return float(np.inf), np.zeros(n)\n        f_best = float(f_cur)\n        x_best = x_cur.copy()\n\n        # settings\n        step = 0.38 * np.mean(domain_range)\n        min_step = 1e-9 * max(1.0, np.mean(domain_range))\n        succ_grow = 1.12\n        succ_shrink = 0.85\n\n        # memory of successful unit directions (LRU)\n        dir_memory = []\n\n        # per-coordinate RMS-like variance for scaling coordinates (starts neutral)\n        coord_var = np.ones(n) * 1e-3\n        coord_alpha = 0.18  # smoothing for coord_var updates\n        coord_eps = 1e-12\n\n        # bookkeeping for model proposals\n        iter_count = 0\n\n        # short budget-aware 1D golden-section search (tight, few evals)\n        def short_line_search(x0, f0, d, init_step, max_evals=8):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            remaining = self.budget - evals\n            remaining = min(remaining, max_evals)\n            if remaining <= 0:\n                return None, None\n            a = 0.0\n            fa = f0\n            b = init_step\n            xb = np.clip(x0 + b * d, lb, ub)\n            out = safe_eval(xb)\n            if out[0] is None:\n                return None, None\n            fb, xb = out\n            remaining -= 1\n            # try other side if no improvement\n            if fb >= fa:\n                b = -init_step\n                xb = np.clip(x0 + b * d, lb, ub)\n                out = safe_eval(xb)\n                if out[0] is None:\n                    return None, None\n                fb, xb = out\n                remaining -= 1\n                if fb >= fa:\n                    return None, None\n            # limited golden search in [0,b]\n            gr = (np.sqrt(5) - 1) / 2\n            left = a\n            right = b\n            c = right - gr * (right - left)\n            dd = left + gr * (right - left)\n            xc = np.clip(x0 + c * d, lb, ub)\n            out = safe_eval(xc)\n            if out[0] is None:\n                return None, None\n            fc, xc = out\n            remaining -= 1\n            xd = np.clip(x0 + dd * d, lb, ub)\n            out = safe_eval(xd)\n            if out[0] is None:\n                return None, None\n            fd, xd = out\n            remaining -= 1\n            best_f = fa\n            best_x = x0.copy()\n            if fc < best_f:\n                best_f = fc; best_x = xc.copy()\n            if fd < best_f:\n                best_f = fd; best_x = xd.copy()\n            iters = 0\n            max_iters = remaining\n            while iters < max_iters and abs(right - left) > 1e-12:\n                iters += 1\n                if fc < fd:\n                    right = dd\n                    dd = c\n                    fd = fc\n                    c = right - gr * (right - left)\n                    xc = np.clip(x0 + c * d, lb, ub)\n                    out = safe_eval(xc)\n                    if out[0] is None:\n                        break\n                    fc, xc = out\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    left = c\n                    c = dd\n                    fc = fd\n                    dd = left + gr * (right - left)\n                    xd = np.clip(x0 + dd * d, lb, ub)\n                    out = safe_eval(xd)\n                    if out[0] is None:\n                        break\n                    fd, xd = out\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # periodic diagonal-quadratic model proposal (uses top k archive points)\n        def try_diagonal_model():\n            nonlocal evals, f_best, x_best\n            if len(X_archive) < (n + 2):\n                return\n            # select up to 2n best points\n            k = min(len(X_archive), 2 * n + 5)\n            idx = np.argsort(F_archive)[:k]\n            Xm = np.array([X_archive[i] for i in idx])\n            Fm = np.array([F_archive[i] for i in idx])\n            # design matrix columns: x_j^2, x_j, intercept\n            A = np.hstack([Xm ** 2, Xm, np.ones((Xm.shape[0], 1))])\n            reg = 1e-6\n            try:\n                # ridge regression via normal equations safely\n                sol, *_ = np.linalg.lstsq(A.T @ A + reg * np.eye(A.shape[1]), A.T @ Fm, rcond=None)\n                sol = sol.flatten()\n                a = sol[:n]\n                b = sol[n:2 * n]\n                # compute diag minimizer: -b/(2a) with regularization to avoid zero/neg issues\n                a_safe = a.copy()\n                a_safe[np.abs(a_safe) < 1e-8] = 1e-8 * np.sign(a_safe[np.abs(a_safe) < 1e-8]) + 1e-8\n                x_star = -0.5 * b / (a_safe + 1e-20)\n                # propose around current x_cur to avoid far proposals\n                x_prop = np.clip(x_cur + 0.6 * (x_star - x_cur), lb, ub)\n                if evals < self.budget:\n                    out = safe_eval(x_prop)\n                    if out[0] is None:\n                        return\n                    f_prop, x_prop = out\n                    if f_prop < f_best:\n                        f_best = f_prop\n                        x_best = x_prop.copy()\n                        # also accept to move current point\n                        # small chance to accept as new current if improved\n                        return True\n            except Exception:\n                pass\n            return False\n\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n            k = max(1, int(np.ceil(np.sqrt(n))))\n            probes = max(4, 2 * k)\n\n            # build basis: reuse up to half from memory\n            use_mem = min(len(dir_memory), k // 2)\n            basis_cols = []\n            if use_mem > 0:\n                # take most recent memory directions\n                for i in range(use_mem):\n                    basis_cols.append(dir_memory[i].copy())\n            needed = k - len(basis_cols)\n            if needed > 0:\n                R = self.rng.standard_normal(size=(n, needed))\n                if basis_cols:\n                    R = np.column_stack((np.column_stack(basis_cols), R))\n                # orthonormalize first k columns\n                Q, _ = np.linalg.qr(R, mode='reduced')\n                basis = Q[:, :k]\n            else:\n                Q, _ = np.linalg.qr(np.column_stack(basis_cols), mode='reduced')\n                basis = Q[:, :k]\n\n            # mirrored sampling: generate half coeffs then mirror\n            coeffs_list = []\n            half = (probes + 1) // 2\n            for _ in range(half):\n                coeffs_list.append(self.rng.standard_normal(k))\n            coeffs_list = coeffs_list + [(-c) for c in coeffs_list]\n            coeffs_list = coeffs_list[:probes]\n\n            improved_in_round = False\n            attempted = 0\n            successes = 0\n\n            for coeffs in coeffs_list:\n                if evals >= self.budget:\n                    break\n                attempted += 1\n                # map to full space\n                d = basis @ coeffs\n                dnrm = np.linalg.norm(d)\n                if dnrm == 0:\n                    continue\n                d = d / dnrm\n                # apply per-coordinate RMS scaling to bias direction magnitudes\n                coord_scale = np.sqrt(coord_var + coord_eps)\n                d = d * coord_scale\n                if np.linalg.norm(d) == 0:\n                    continue\n                d = d / (np.linalg.norm(d) + 1e-20)\n\n                alpha = self.rng.uniform(-step, step)\n                x_try = x_cur + alpha * d\n\n                # occasional DE-like archive difference perturb (light)\n                if (len(X_archive) >= 2) and (self.rng.random() < 0.12):\n                    i1, i2 = self.rng.choice(len(X_archive), size=2, replace=False)\n                    de = 0.6 * (X_archive[i1] - X_archive[i2])\n                    x_try = x_try + de * 0.5\n\n                # occasional directional Cauchy jump using memory\n                if dir_memory and (self.rng.random() < self.mem_jump_prob):\n                    u = dir_memory[self.rng.integers(len(dir_memory))]\n                    jump = np.tan(np.pi * (self.rng.random() - 0.5))\n                    x_try = x_try + (self.cauchy_scale * jump * step) * u\n\n                x_try = np.clip(x_try, lb, ub)\n                out = safe_eval(x_try)\n                if out[0] is None:\n                    break\n                f_try, x_try = out\n\n                if f_try < f_cur - 1e-12:\n                    # accept immediately\n                    prev_x = x_cur.copy()\n                    x_cur = x_try.copy()\n                    f_cur = f_try\n                    successes += 1\n                    improved_in_round = True\n                    # update best\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n                    # store direction in memory\n                    dir_succ = x_cur - prev_x\n                    dn = np.linalg.norm(dir_succ)\n                    if dn > 0:\n                        dir_unit = dir_succ / dn\n                        dir_memory.insert(0, dir_unit.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                    # update coord_var with normalized y (relative step)\n                    y = (x_cur - prev_x) / (abs(alpha) + 1e-20)\n                    sec = y ** 2\n                    coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (sec + 1e-12)\n                    # short local line-search along successful direction (budget-aware)\n                    remaining_budget = self.budget - evals\n                    if remaining_budget >= 2:\n                        ls_max = min(10, remaining_budget)\n                        ls_out = short_line_search(x_cur, f_cur, dir_unit if dn > 0 else d, init_step=abs(alpha) if abs(alpha) > 1e-12 else step, max_evals=ls_max)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                x_cur = x_ls.copy()\n                                f_cur = f_ls\n                                if f_ls < f_best:\n                                    f_best = f_ls\n                                    x_best = x_ls.copy()\n                    # increase step gently on success\n                    step = min(step * succ_grow, 5.0 * np.mean(domain_range))\n                else:\n                    # maybe try a short focused line-search from current point with small prob\n                    if (self.rng.random() < 0.04) and (self.budget - evals) >= 3:\n                        ls_max = min(6, self.budget - evals)\n                        ls_out = short_line_search(x_cur, f_cur, d, init_step=step, max_evals=ls_max)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                prev_x = x_cur.copy()\n                                x_cur = x_ls.copy()\n                                f_cur = f_ls\n                                if f_ls < f_best:\n                                    f_best = f_ls\n                                    x_best = x_ls.copy()\n                                dir_succ = x_cur - prev_x\n                                dn = np.linalg.norm(dir_succ)\n                                if dn > 0:\n                                    dir_unit = dir_succ / dn\n                                    dir_memory.insert(0, dir_unit.copy())\n                                    if len(dir_memory) > self.memory_size:\n                                        dir_memory.pop()\n                                # update coord_var\n                                y = (x_cur - prev_x) / (np.linalg.norm(x_cur - prev_x) + 1e-20)\n                                sec = y ** 2\n                                coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (sec + 1e-12)\n                                successes += 1\n                                improved_in_round = True\n                # end candidate handling\n\n            # after all probes\n            if not improved_in_round:\n                # shrink step on failure\n                step = max(step * succ_shrink, min_step)\n            # prune archive if too large\n            max_archive = max(2000, 50 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                keep_rest = rest[::max(1, len(rest) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # periodic diagonal model attempt to propose a focused candidate\n            if (iter_count % self.model_every == 0) and (self.budget - evals > 0):\n                # try model; if it suggests and improves, accept\n                try_diagonal_model()\n\n            # small random Cauchy nudge around best occasionally when stagnating\n            if (self.rng.random() < 0.02) and (self.budget - evals > 0):\n                jump = np.tan(np.pi * (self.rng.random(n) - 0.5))\n                x_nudge = np.clip(x_best + 0.7 * step * self.cauchy_scale * jump, lb, ub)\n                out = safe_eval(x_nudge)\n                if out[0] is None:\n                    break\n                f_nudge, x_nudge = out\n                if f_nudge < f_best:\n                    f_best = f_nudge\n                    x_best = x_nudge.copy()\n                    # accept as current to exploit\n                    x_cur = x_best.copy()\n                    f_cur = f_best\n\n            # tiny safeguard\n            if step < min_step:\n                step = min_step\n\n            # exit early if extremely good\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm MDARSS scored 0.179 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "1170b68b-fa6d-49c9-986c-c4c18a103838", "operator": null, "metadata": {"aucs": [0.08176989591856154, 0.16487548420775244, 0.23496734734004943, 0.3766552330648685, 0.11152157525509454, 0.1407280446554623, 0.20738059618325588, 0.15425333647518413, 0.20832291649134216, 0.10923041625996366]}, "task_prompt": ""}
{"id": "c394d249-ff0d-46ef-8a61-83d430d79fe5", "fitness": "-inf", "name": "AMoDS", "description": "AMoDS runs multiple \"anchors\" (small trust regions) in parallel and repeatedly probes directional steps inside an adaptive radius, growing radii on repeated successes and shrinking them otherwise to balance exploration and exploitation. It remembers recent successful displacements and builds a PCA-derived subspace (subspace_frac≈0.6, memory_size=18) plus an anisotropic per-coordinate scale (smoothed multiplicatively) to bias future probes, while using heavy‑tailed Pareto amplitudes (pareto_tail≈1.6 and probes_factor=4 → many probes per round) to enable both local steps and occasional large jumps. Local refinement is boosted by inexpensive symmetric quadratic 1-D relaxations, micro‑relaxations and coordinate sweeps, and global diversification comes from anchor recombination and occasional Lévy-like long jumps; all proposals are clipped to bounds and evaluated under strict budget accounting. Conservative numeric choices (init_radius≈0.2 of domain, anchors=3, grow_factor≈1.4, shrink_factor≈0.55, and gentle coord_scale decay) plus safe_eval and small smoothing weights prevent runaway anisotropy and keep the method robust across BBOB-style bounded problems.", "code": "import numpy as np\n\nclass AMoDS:\n    \"\"\"\n    Anticipatory Multi-scale Directional Search (AMoDS)\n\n    - budget: total allowed function evaluations\n    - dim: problem dimensionality\n    - seed: RNG seed\n    Optional parameters have reasonable defaults for BBOB [-5,5] style tasks.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 anchors=3,\n                 memory_size=18,        # store recent successful displacements\n                 init_radius=0.20,      # initial trust radius fraction of domain\n                 min_radius=1e-9,\n                 subspace_frac=0.6,     # fraction of dim used for PCA subspace (rounded)\n                 probes_factor=4,       # probes per round = max(6, probes_factor * subspace_dim)\n                 pareto_tail=1.6,       # heavy-tail parameter for step length (pareto)\n                 grow_factor=1.40, shrink_factor=0.55,  # trust radius adaptation\n                 line_relax_evals=4):   # max extra evals for symmetric quadratic relaxation\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.anchors = int(max(1, anchors))\n        self.memory_size = int(memory_size)\n        self.init_radius = float(init_radius)\n        self.min_radius = float(min_radius)\n        self.subspace_frac = float(subspace_frac)\n        self.probes_factor = int(probes_factor)\n        self.pareto_tail = float(pareto_tail)\n        self.grow_factor = float(grow_factor)\n        self.shrink_factor = float(shrink_factor)\n        self.line_relax_evals = int(line_relax_evals)\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds support\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain = np.maximum(1e-12, np.mean(ub - lb))\n\n        # safe eval bookkeeping\n        evals = 0\n\n        def clip(x):\n            return np.minimum(ub, np.maximum(lb, x))\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x.copy()\n\n        # initialize anchors uniformly in domain\n        anchors = []\n        for _ in range(self.anchors):\n            x0 = self.rng.uniform(lb, ub)\n            f0, x0_r = safe_eval(x0)\n            if f0 is None:\n                # budget exhausted on initialization\n                return float('inf'), np.zeros(n)\n            anchors.append({'x': x0_r.copy(), 'f': float(f0), 'radius': self.init_radius * domain})\n\n        # global best record\n        idx_best = int(np.argmin([a['f'] for a in anchors]))\n        x_best = anchors[idx_best]['x'].copy()\n        f_best = float(anchors[idx_best]['f'])\n\n        # adaptive per-coordinate scale (diagonal covariance like)\n        coord_scale = np.ones(n)\n\n        # memory of recent successful displacements (for PCA)\n        step_memory = []  # list of vectors (n,)\n\n        # helper: incremental PCA on step_memory -> principal directions (columns)\n        def pca_basis(mem, max_components):\n            if len(mem) == 0:\n                return None\n            M = np.stack(mem, axis=1)  # shape (n, m)\n            # center columns (they are displacements, mean removal can help)\n            M = M - np.mean(M, axis=1, keepdims=True)\n            # small SVD\n            try:\n                U, S, Vt = np.linalg.svd(M, full_matrices=False)\n            except np.linalg.LinAlgError:\n                return None\n            comps = min(max_components, U.shape[1])\n            return U[:, :comps]  # (n, comps)\n\n        # inexpensive symmetric quadratic 1-D relaxation (uses -s, 0, +s, optionally +-2s)\n        # returns (f_opt, x_opt) or (None, None)\n        def symmetric_quadratic(x0, f0, d, s):\n            # d should be a unit direction\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            pts = [(0.0, f0, x0.copy())]\n            # Try -s and +s\n            for alpha in (-s, s):\n                if evals >= self.budget:\n                    break\n                x = clip(x0 + alpha * d)\n                f, x_r = safe_eval(x)\n                if f is None:\n                    break\n                pts.append((alpha, f, x_r.copy()))\n            if len(pts) < 3:\n                return None, None\n            # Fit parabola through three symmetric points (alpha = -s, 0, s) -> because symmetric we can use simple formula:\n            # f(-s)=A s^2 - B s + C, f(0)=C, f(s)=A s^2 + B s + C\n            # For symmetric data B approximates odd component; the quadratic coefficient A = (f(-s) + f(s) - 2 f0) / (2 s^2)\n            # Optimal alpha = -B/(2A). But we can compute B = (f(s) - f(-s)) / (2 s)\n            alphas = np.array([p[0] for p in pts])\n            fs = np.array([p[1] for p in pts])\n            # try to extract -s and +s matching\n            # pick points with nonzero alphas\n            nonzero_idx = np.where(alphas != 0)[0]\n            if len(nonzero_idx) < 2:\n                return None, None\n            # find -s and +s by sign\n            p_minus = [i for i in nonzero_idx if alphas[i] < 0]\n            p_plus = [i for i in nonzero_idx if alphas[i] > 0]\n            if not p_minus or not p_plus:\n                return None, None\n            i_minus = p_minus[0]; i_plus = p_plus[0]\n            f_minus = fs[i_minus]; f_plus = fs[i_plus]; f0_loc = fs[alphas == 0][0]\n            s_actual = abs(alphas[i_plus])\n            A = (f_minus + f_plus - 2.0 * f0_loc) / (2.0 * (s_actual ** 2) + 1e-16)\n            B = (f_plus - f_minus) / (2.0 * s_actual)\n            if A >= 0 and abs(A) > 1e-18:\n                alpha_opt = -B / (2.0 * A)\n            else:\n                # curvature not suitable; choose best of sampled\n                best_idx = int(np.argmin(fs))\n                best_f = float(fs[best_idx])\n                best_x = pts[best_idx][2]\n                if best_f < f0_loc - 1e-12:\n                    return best_f, best_x\n                return None, None\n            # constrain alpha_opt\n            alpha_opt = np.clip(alpha_opt, -2.0 * s_actual, 2.0 * s_actual)\n            # evaluate alpha_opt if budget allows\n            if evals < self.budget:\n                x_opt = clip(x0 + alpha_opt * d)\n                f_opt, x_r = safe_eval(x_opt)\n                if f_opt is None:\n                    return None, None\n                if f_opt < f0_loc - 1e-12:\n                    return float(f_opt), x_r\n                # otherwise, return best sampled if it improved\n                best_idx = int(np.argmin(fs))\n                best_f = float(fs[best_idx])\n                best_x = pts[best_idx][2]\n                if best_f < f0_loc - 1e-12:\n                    return best_f, best_x\n            return None, None\n\n        # main optimization loop\n        while evals < self.budget:\n            # compute PCA basis from memory\n            sub_k = max(1, int(min(n, int(np.ceil(n * self.subspace_frac)))))\n            pca_B = pca_basis(step_memory, sub_k)\n            probes = max(6, self.probes_factor * sub_k)\n            # choose an anchor to work on via fitness-proportional (best favored)\n            fs = np.array([a['f'] for a in anchors])\n            # invert to positive weights; add small epsilon\n            w = (np.max(fs) + 1e-8) - fs\n            if w.sum() <= 0:\n                probs = np.ones(len(anchors)) / len(anchors)\n            else:\n                probs = w / w.sum()\n            anchor_idx = int(self.rng.choice(len(anchors), p=probs))\n            anchor = anchors[anchor_idx]\n            x_anchor = anchor['x'].copy()\n            f_anchor = anchor['f']\n            radius = float(anchor['radius'])\n            success_count = 0\n\n            # Build a basis combining PCA components (if available) and random complement\n            if pca_B is not None and pca_B.shape[1] >= 1:\n                # take up to sub_k PCA components\n                comps = pca_B[:, :min(sub_k, pca_B.shape[1])]\n                # fill remaining with randomized vectors (apply coord_scale anisotropy)\n                need = sub_k - comps.shape[1]\n                if need > 0:\n                    R = self.rng.normal(size=(n, need)) * coord_scale.reshape(-1, 1)\n                    # orthonormalize combined\n                    M = np.column_stack((comps, R))\n                    Q, _ = np.linalg.qr(M)\n                    basis = Q[:, :sub_k]\n                else:\n                    # just orthonormalize comps (in case not orthonormal)\n                    Q, _ = np.linalg.qr(comps)\n                    basis = Q[:, :sub_k]\n            else:\n                # purely random basis with anisotropy\n                R = self.rng.normal(size=(n, sub_k)) * coord_scale.reshape(-1, 1)\n                Q, _ = np.linalg.qr(R)\n                basis = Q[:, :sub_k]\n\n            # Probing loop within anchor's trust radius\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample directional coefficients (mix Gaussian and heavy-tailed Pareto on amplitude)\n                coeffs = self.rng.normal(size=sub_k)\n                d = basis @ coeffs\n                dn = np.linalg.norm(d) + 1e-20\n                d = d / dn\n                # sample heavy-tailed length using Pareto: L = radius * (1 + pareto_sample)\n                # self.rng.pareto(a) returns samples ~ U^(-1/a) - 1, so adding 1 yields heavy tail\n                pareto_sample = self.rng.pareto(self.pareto_tail)\n                length = radius * (1.0 + pareto_sample * self.rng.uniform(0.5, 1.5))\n                # also mix in a small sign randomization\n                alpha = (self.rng.choice([-1.0, 1.0]) * length)\n                x_try = clip(x_anchor + alpha * d)\n                f_try, x_r = safe_eval(x_try)\n                if f_try is None:\n                    break\n                if f_try < f_anchor - 1e-12:\n                    # success: adopt as new anchor position\n                    success_count += 1\n                    disp = x_r - x_anchor\n                    if np.linalg.norm(disp) > 0:\n                        # store displacement (scaled by alpha) into memory\n                        step_memory.insert(0, (disp / (np.linalg.norm(disp) + 1e-20)))\n                        if len(step_memory) > self.memory_size:\n                            step_memory.pop()\n                        # update coord_scale toward absolute displacement direction (multiplicative smoothing)\n                        coord_scale = 0.92 * coord_scale + 0.08 * (1.0 + np.abs(disp) / (np.linalg.norm(disp) + 1e-20))\n                    # attempt cheap symmetric quadratic relaxation along direction d with small extra budget\n                    remaining = self.budget - evals\n                    s_relax = min(radius, max(1e-12, 0.8 * abs(alpha)))\n                    if remaining >= 2:\n                        f_rel, x_rel = symmetric_quadratic(x_anchor, f_anchor, d, s_relax)\n                        if f_rel is not None and f_rel < f_try - 1e-12:\n                            # improved by relaxation\n                            f_try = f_rel\n                            x_r = x_rel\n                    # accept improvement\n                    x_anchor = x_r.copy()\n                    f_anchor = float(f_try)\n                    # store new anchor back\n                    anchors[anchor_idx]['x'] = x_anchor.copy()\n                    anchors[anchor_idx]['f'] = f_anchor\n                    # adapt radius up on success\n                    anchors[anchor_idx]['radius'] = min(domain * 2.0, anchors[anchor_idx]['radius'] * self.grow_factor)\n                else:\n                    # small probability to perform a micro-relaxation centered at current anchor\n                    if self.rng.random() < 0.03 and (self.budget - evals) >= 3:\n                        s_micro = 0.5 * radius\n                        fm, xm = symmetric_quadratic(x_anchor, f_anchor, d, s_micro)\n                        if fm is not None and fm < f_anchor - 1e-12:\n                            x_anchor = xm.copy()\n                            f_anchor = fm\n                            anchors[anchor_idx]['x'] = x_anchor.copy()\n                            anchors[anchor_idx]['f'] = f_anchor\n                            anchors[anchor_idx]['radius'] = min(domain * 2.0, anchors[anchor_idx]['radius'] * self.grow_factor)\n                            success_count += 1\n\n            # After probing: adjust trust radius based on success ratio\n            succ_ratio = success_count / max(1.0, probes)\n            if succ_ratio > 0.12:\n                anchors[anchor_idx]['radius'] = min(domain * 2.0, anchors[anchor_idx]['radius'] * (1.0 + self.grow_factor * succ_ratio))\n            else:\n                anchors[anchor_idx]['radius'] = max(self.min_radius * domain, anchors[anchor_idx]['radius'] * (self.shrink_factor + 0.05 * succ_ratio))\n\n            # occasional anchor-to-anchor exchange: try a mid-point or directed recombination between best two anchors\n            if len(anchors) >= 2 and self.rng.random() < 0.08 and evals < self.budget:\n                # pick two anchors proportionally to fitness differences\n                idxs = self.rng.choice(len(anchors), size=2, replace=False)\n                a1 = anchors[idxs[0]]; a2 = anchors[idxs[1]]\n                # create recombined point biased toward better\n                w1 = max(0.01, (np.max([a1['f'], a2['f']]) - a1['f']))\n                w2 = max(0.01, (np.max([a1['f'], a2['f']]) - a2['f']))\n                mix = w1 / (w1 + w2)\n                x_rec = clip(mix * a1['x'] + (1 - mix) * a2['x'] + self.rng.normal(scale=0.1 * domain) * coord_scale)\n                f_rec, xr = safe_eval(x_rec)\n                if f_rec is None:\n                    break\n                if f_rec < a1['f'] or f_rec < a2['f']:\n                    # replace worse anchor\n                    worse_idx = idxs[0] if anchors[idxs[0]]['f'] > anchors[idxs[1]]['f'] else idxs[1]\n                    anchors[worse_idx]['x'] = xr.copy()\n                    anchors[worse_idx]['f'] = float(f_rec)\n                    anchors[worse_idx]['radius'] = 0.8 * domain\n\n            # occasional Levy-like long jump from best to explore new basins\n            if self.rng.random() < 0.04 and evals < self.budget:\n                # best anchor index\n                idx_b = int(np.argmin([a['f'] for a in anchors]))\n                base_x = anchors[idx_b]['x']\n                # generate a long jump with Pareto heavy tail and anisotropic scaling\n                pareto_s = self.rng.pareto(self.pareto_tail)\n                long_jump = (0.9 * domain) * (1.0 + pareto_s) * (coord_scale / coord_scale.max()) * self.rng.normal(size=n)\n                x_jump = clip(base_x + long_jump)\n                f_jump, xj = safe_eval(x_jump)\n                if f_jump is None:\n                    break\n                # if jump improved, add as new anchor by replacing worst\n                worst_idx = int(np.argmax([a['f'] for a in anchors]))\n                if f_jump < anchors[worst_idx]['f']:\n                    anchors[worst_idx]['x'] = xj.copy()\n                    anchors[worst_idx]['f'] = float(f_jump)\n                    anchors[worst_idx]['radius'] = domain * 0.6\n                    # add to memory\n                    step_memory.insert(0, (xj - base_x) / (np.linalg.norm(xj - base_x) + 1e-20))\n                    if len(step_memory) > self.memory_size:\n                        step_memory.pop()\n\n            # small local polishing around the current global best occasionally\n            if self.rng.random() < 0.06 and evals < self.budget:\n                idx_b = int(np.argmin([a['f'] for a in anchors]))\n                xb = anchors[idx_b]['x'].copy()\n                fb = anchors[idx_b]['f']\n                # coordinate sweep with 2 points per selected coordinate\n                coords = self.rng.permutation(n)[:min(n, 5)]\n                for i in coords:\n                    if evals >= self.budget:\n                        break\n                    dvec = np.zeros(n); dvec[i] = 1.0\n                    for sign in (+1, -1):\n                        step_c = sign * 0.7 * anchors[idx_b]['radius'] * coord_scale[i]\n                        f_c, xr = safe_eval(clip(xb + step_c * dvec))\n                        if f_c is None:\n                            break\n                        if f_c < anchors[idx_b]['f'] - 1e-12:\n                            anchors[idx_b]['x'] = xr.copy()\n                            anchors[idx_b]['f'] = float(f_c)\n                            xb = xr.copy()\n                            # update memory and coord_scale\n                            disp = xr - xb\n                            if np.linalg.norm(disp) > 0:\n                                step_memory.insert(0, (disp / (np.linalg.norm(disp) + 1e-20)))\n                                if len(step_memory) > self.memory_size:\n                                    step_memory.pop()\n                                coord_scale = 0.9 * coord_scale + 0.1 * (1.0 + np.abs(disp) / (np.linalg.norm(disp) + 1e-20))\n\n            # refresh global best variables\n            idx_best = int(np.argmin([a['f'] for a in anchors]))\n            if anchors[idx_best]['f'] < f_best - 1e-16:\n                f_best = float(anchors[idx_best]['f'])\n                x_best = anchors[idx_best]['x'].copy()\n\n            # safety: shrink coord_scale gently toward 1 to avoid runaway anisotropy\n            coord_scale = 0.995 * coord_scale + 0.005 * np.ones_like(coord_scale)\n\n            # global termination checks\n            if evals >= self.budget:\n                break\n            if f_best <= 1e-14:\n                break\n\n        # final return of best found\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 64, in safe_eval, the following error occurred:\nNameError: cannot access free variable 'f_best' where it is not associated with a value in enclosing scope\nOn line: if f < f_best:", "error": "In the code, line 64, in safe_eval, the following error occurred:\nNameError: cannot access free variable 'f_best' where it is not associated with a value in enclosing scope\nOn line: if f < f_best:", "parent_ids": "4c818849-323e-4065-9772-bd26cb830a8b", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "89c357c3-d71a-4239-897b-26641cd501c7", "fitness": 0.4200540437237697, "name": "PRAS", "description": "PRAS maintains a small adaptive population of \"centers\" seeded around a random start (pop_size chosen small and adaptive) and probabilistically samples from better centers while sometimes replacing the worst to balance exploitation and exploration. It probes low‑dimensional subspaces (k = ceil(n**subspace_exp) with subspace_exp=0.45 smaller than some baselines) built from a mix of recent direction memory and random normals (QR orthonormalization), draws heavy‑tailed Cauchy coefficients (clipped) to produce exploratory directions, uses probes = max(4, probes_factor*k) with probes_factor=2 (fewer probes), and applies a cheap bidirectional quadratic refinement (up to 3 extra evals) for local polishing. Adaptation is conservative: a relatively large initial step (init_step_scale=0.5 of domain) with gentle grow/shrink (grow=1.15, shrink=0.85), short directional memory (memory_size=6), per‑coordinate scaling (coord_scale) and occasional anisotropic restarts or coordinate sweeps when stagnating, plus safeguards on min/max step and early stopping.", "code": "import numpy as np\n\nclass PRAS:\n    \"\"\"\n    Population Rotational Adaptive Search (PRAS)\n\n    - budget: total allowed function evaluations\n    - dim: problem dimensionality\n    Optional tunables are exposed but set to defaults chosen to differ from ADES:\n      * subspace_exp (default 0.45): subspace dim k = ceil(n**subspace_exp) (smaller than ADES's 0.6)\n      * probes_factor (default 2): probes = max(4, probes_factor * k) (fewer probes than ADES)\n      * memory_size (default 6): keeps a shorter directional memory than ADES\n      * init_step_scale (default 0.5): larger initial step (fraction of domain)\n      * grow (default 1.15), shrink (default 0.85): conservative step adaptation (opposite to ADES aggressiveness)\n      * pop_size (default None): a small population of centers (if None it's set adaptively)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 subspace_exp=0.45, probes_factor=2,\n                 memory_size=6,\n                 init_step_scale=0.5,\n                 grow=1.15, shrink=0.85,\n                 pop_size=None,\n                 min_step_scale=1e-8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.subspace_exp = float(subspace_exp)\n        self.probes_factor = int(probes_factor)\n        self.memory_size = int(memory_size)\n        self.init_step_scale = float(init_step_scale)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.min_step_scale = float(min_step_scale)\n        # adaptive population size: a small pool to allow multi-center exploration\n        if pop_size is None:\n            self.pop_size = max(2, int(min(dim, max(2, int(np.ceil(np.sqrt(dim)))))))\n        else:\n            self.pop_size = int(pop_size)\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds can be scalars or arrays (assume func.bounds.lb/ub exist)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        # domain scale (typical half-range mean)\n        domain = np.maximum(1e-12, np.mean(ub - lb))\n\n        evals = 0\n\n        # helpers\n        def clip(x):\n            return np.minimum(ub, np.maximum(lb, x))\n\n        # safe evaluation wrapper\n        best_x = None\n        best_f = np.inf\n\n        def safe_eval(x):\n            nonlocal evals, best_f, best_x\n            x = clip(x)\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < best_f:\n                best_f = float(f)\n                best_x = x.copy()\n            return f, x\n\n        # initialize: start population around a random seed\n        centers = []\n        # initial central point\n        x0 = self.rng.uniform(lb, ub)\n        res = safe_eval(x0)\n        if res[0] is None:\n            return float(best_f), np.array(best_x, dtype=float)\n        x0 = res[1]\n        f0 = res[0]\n        centers.append((x0.copy(), float(f0)))\n\n        # populate additional centers by sampling around x0\n        for _ in range(self.pop_size - 1):\n            if evals >= self.budget:\n                break\n            noise = self.rng.normal(scale=0.5 * self.init_step_scale * domain, size=n)\n            x = clip(x0 + noise)\n            r = safe_eval(x)\n            if r[0] is None:\n                break\n            centers.append((r[1].copy(), float(r[0])))\n\n        # per-coordinate scale (diagonal-like adaptation)\n        coord_scale = np.ones(n)\n\n        # step settings\n        step = self.init_step_scale * domain\n        min_step = self.min_step_scale * domain\n        max_step = 2.0 * domain\n\n        # subspace and probes settings (distinct from ADES)\n        k = max(1, int(np.ceil(n ** self.subspace_exp)))\n        probes = max(4, self.probes_factor * k)\n\n        # direction memory (shorter than ADES)\n        dir_memory = []\n\n        # stagnation tracking\n        no_improve = 0\n        patience = max(8, int(6 + np.log(1 + n) * 4))\n        restarts = 0\n        max_restarts = 4\n\n        # a cheap bidirectional quadratic refinement using at most 3 extra evals\n        # returns (f_new, x_new) or (None, None)\n        def bidir_quadratic(x0, f0, d, step0):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                return None, None\n            d = d / dn\n            # evaluate +s and -s if budget allows\n            s = step0\n            samples = [(0.0, f0, x0.copy())]\n            if evals < self.budget:\n                x_p = clip(x0 + s * d)\n                r = safe_eval(x_p)\n                if r[0] is None:\n                    return None, None\n                samples.append((s, r[0], r[1].copy()))\n            if evals < self.budget:\n                x_m = clip(x0 - s * d)\n                r = safe_eval(x_m)\n                if r[0] is None:\n                    return None, None\n                samples.append((-s, r[0], r[1].copy()))\n            if len(samples) < 3:\n                # not enough info; return best if improved\n                best = min(samples, key=lambda t: t[1])\n                if best[1] < f0 - 1e-12:\n                    return best[1], best[2]\n                return None, None\n            # fit quadratic f(alpha) = A*alpha^2 + B*alpha + C using the three samples\n            alphas = np.array([p[0] for p in samples])\n            fs = np.array([p[1] for p in samples])\n            # pick the three distinct alphas (should be -s,0,s)\n            try:\n                M = np.vstack([alphas**2, alphas, np.ones_like(alphas)]).T\n                coeffs = np.linalg.lstsq(M, fs, rcond=None)[0]\n                A, B, C = coeffs\n                if abs(A) < 1e-20:\n                    return None, None\n                alpha_opt = -B / (2.0 * A)\n            except Exception:\n                return None, None\n            # clip alpha_opt to reasonable range around sampled alphas\n            span = max(np.ptp(alphas), 1e-12)\n            alpha_opt = np.clip(alpha_opt, alphas.min() - 0.5 * span, alphas.max() + 0.5 * span)\n            if evals < self.budget:\n                x_opt = clip(x0 + alpha_opt * d)\n                r = safe_eval(x_opt)\n                if r[0] is None:\n                    return None, None\n                if r[0] < f0 - 1e-12:\n                    return r[0], r[1]\n            return None, None\n\n        # main loop: probe ensembles until budget exhausted\n        while evals < self.budget:\n            # recompute adaptive parameters occasionally\n            k = max(1, min(n, int(np.ceil(n ** self.subspace_exp))))\n            probes = max(4, self.probes_factor * k)\n\n            improved_in_round = False\n\n            # build a subspace basis: mix a few memory directions and random vectors\n            use_mem = min(len(dir_memory), max(0, k // 3))\n            chosen = []\n            if use_mem > 0:\n                # prefer more recent memory with exponential bias\n                weights = np.exp(-0.6 * np.arange(len(dir_memory)))\n                weights = weights / weights.sum()\n                idxs = self.rng.choice(len(dir_memory), size=use_mem, replace=False, p=weights)\n                for i in idxs:\n                    chosen.append(dir_memory[i])\n            needed = k - len(chosen)\n            if needed > 0:\n                R = self.rng.normal(size=(n, needed)) * coord_scale.reshape(-1, 1)\n                if chosen:\n                    R = np.column_stack((np.column_stack(chosen), R))\n                # QR to form an orthonormal basis\n                try:\n                    Q, _ = np.linalg.qr(R, mode='reduced')\n                except Exception:\n                    # fallback to normalizing columns\n                    Q = R.copy()\n                    for j in range(Q.shape[1]):\n                        norm = np.linalg.norm(Q[:, j])\n                        if norm > 0:\n                            Q[:, j] /= norm\n                basis = Q[:, :k]\n            else:\n                B = np.column_stack(chosen)\n                try:\n                    Q, _ = np.linalg.qr(B, mode='reduced')\n                except Exception:\n                    Q = B.copy()\n                    for j in range(Q.shape[1]):\n                        norm = np.linalg.norm(Q[:, j])\n                        if norm > 0:\n                            Q[:, j] /= norm\n                basis = Q[:, :k]\n\n            # choose a center to probe from: bias toward better centers but allow exploration\n            center_idxs = np.arange(len(centers))\n            center_scores = np.array([c[1] for c in centers])\n            # convert to probabilities (lower f -> higher prob)\n            if len(center_scores) > 0:\n                inv = center_scores.max() - center_scores + 1e-12\n                probs = inv / inv.sum()\n            else:\n                probs = None\n\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # pick a center\n                if len(centers) == 0:\n                    cur_center = x0.copy()\n                    cur_f = f0\n                else:\n                    if probs is not None and len(probs) == len(centers):\n                        idx = self.rng.choice(len(centers), p=probs)\n                    else:\n                        idx = self.rng.integers(0, len(centers))\n                    cur_center, cur_f = centers[idx][0].copy(), centers[idx][1]\n\n                # sample heavy-tailed coefficients via Cauchy (more extreme than normal)\n                # but clipped to avoid numerical issues\n                coeffs = self.rng.standard_cauchy(size=k)\n                coeffs = np.clip(coeffs, -10, 10)  # safety clip\n                # scale by small factor so direction magnitude is reasonable\n                coeffs = coeffs * 0.4\n                d = basis @ coeffs\n                dn = np.linalg.norm(d)\n                if dn == 0:\n                    continue\n                d = d / dn\n\n                # sample alpha from uniform but with small gaussian jitter\n                alpha = self.rng.uniform(-1.0, 1.0) * step * (1.0 + 0.15 * self.rng.normal())\n                x_try = clip(cur_center + alpha * d)\n                r = safe_eval(x_try)\n                if r[0] is None:\n                    break\n                f_try, x_ret = r\n\n                if f_try < cur_f - 1e-12:\n                    # success: incorporate direction into memory (shorter memory, slower blending)\n                    dir_succ = x_ret - cur_center\n                    nd = np.linalg.norm(dir_succ)\n                    if nd > 0:\n                        dir_unit = dir_succ / nd\n                        dir_memory.insert(0, dir_unit.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory = dir_memory[:self.memory_size]\n                        # gentle coordinate adaptation\n                        coord_scale = 0.92 * coord_scale + 0.08 * (1.0 + np.abs(dir_unit))\n                    # small bidirectional quadratic refinement (cheap, up to 3 extra evals)\n                    remaining = self.budget - evals\n                    max_eval_ref = min(3, remaining)\n                    if max_eval_ref >= 1:\n                        q = bidir_quadratic(cur_center, cur_f, dir_unit if nd > 0 else d, abs(alpha) if abs(alpha) > 1e-16 else step)\n                        if q[0] is not None and q[0] < f_try - 1e-12:\n                            f_try, x_ret = q[0], q[1]\n                    # update the chosen center in the population (replace if improved or replace worst)\n                    centers[idx] = (x_ret.copy(), float(f_try))\n                    # if not the best center, sometimes insert into population replacing worst\n                    # keep population size limited\n                    if f_try < centers[idx][1]:\n                        centers[idx] = (x_ret.copy(), float(f_try))\n                    # ensure best tracked globally already via safe_eval\n                    improved_in_round = True\n                    no_improve = 0\n                    # conservative growth\n                    step = min(step * self.grow, max_step)\n                else:\n                    # occasional tiny quadratic refinement from current center when probe is close\n                    if self.rng.random() < 0.03 and (self.budget - evals) >= 2:\n                        q = bidir_quadratic(cur_center, cur_f, d, 0.5 * step)\n                        if q[0] is not None:\n                            # found improvement\n                            centers[idx] = (q[1].copy(), float(q[0]))\n                            improved_in_round = True\n                            no_improve = 0\n                            step = min(step * self.grow, max_step)\n                    # otherwise continue probing\n                    pass\n\n            # end probes for this round\n            if not improved_in_round:\n                no_improve += 1\n                step = max(step * self.shrink, min_step)\n            else:\n                # light polish around the best center: a few gaussian nudges\n                polishes = min(4, 2 + int(np.log(1 + n)))\n                for _ in range(polishes):\n                    if evals >= self.budget:\n                        break\n                    if best_x is None:\n                        break\n                    noise = self.rng.normal(scale=0.2 * step * coord_scale)\n                    x_p = clip(best_x + noise)\n                    r = safe_eval(x_p)\n                    if r[0] is None:\n                        break\n                    if r[0] < best_f - 1e-12:\n                        # update memory based on displacement\n                        disp = r[1] - best_x\n                        nd = np.linalg.norm(disp)\n                        if nd > 0:\n                            dir_unit = disp / nd\n                            dir_memory.insert(0, dir_unit.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory = dir_memory[:self.memory_size]\n                            coord_scale = 0.95 * coord_scale + 0.05 * (1.0 + np.abs(dir_unit))\n                        # incorporate into population replacing worst\n                        # find worst center index\n                        worst_idx = np.argmax([c[1] for c in centers])\n                        centers[worst_idx] = (r[1].copy(), float(r[0]))\n                        no_improve = 0\n\n            # stagnation / restart logic: keep it conservative\n            if no_improve >= patience:\n                restarts += 1\n                # If we've restarted too often, do a small targeted coordinate sweep\n                if restarts >= max_restarts or step <= min_step * 1.2:\n                    # small coordinate-wise exploration around global best\n                    if best_x is not None:\n                        coords = self.rng.permutation(n)[:min(n, 6)]\n                        for i in coords:\n                            if evals >= self.budget:\n                                break\n                            for sign in (+1, -1):\n                                a = sign * 0.7 * step * coord_scale[i]\n                                r = safe_eval(clip(best_x + a * np.eye(1, n, i).reshape(-1)))\n                                if r[0] is None:\n                                    break\n                    # reduce step and continue; reset counters\n                    step = max(step * 0.4, min_step)\n                    no_improve = 0\n                else:\n                    # diversify population around best with anisotropic perturbations\n                    if best_x is not None:\n                        for j in range(len(centers)):\n                            if evals >= self.budget:\n                                break\n                            perturb = self.rng.normal(scale=0.8 * domain * (coord_scale / coord_scale.max()))\n                            x_new = clip(best_x + perturb)\n                            r = safe_eval(x_new)\n                            if r[0] is None:\n                                break\n                            centers[j] = (r[1].copy(), float(r[0]))\n                    # inflate step moderately and partially clear direction memory\n                    step = min(max_step, step * 1.6)\n                    # keep most recent few directions only\n                    dir_memory = dir_memory[:max(0, len(dir_memory)//2)]\n                    no_improve = 0\n\n            # safeguard step lower bound\n            if step < min_step:\n                step = min_step\n\n            # early stopping if excellent solution found\n            if best_f <= 1e-14:\n                break\n\n        # return best found (safe_eval maintained best_x,best_f)\n        if best_x is None:\n            # fallback to first center\n            if centers:\n                return float(centers[0][1]), np.array(centers[0][0], dtype=float)\n            else:\n                return float(np.inf), np.zeros(n, dtype=float)\n        return float(best_f), np.array(best_x, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm PRAS scored 0.420 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "4c818849-323e-4065-9772-bd26cb830a8b", "operator": null, "metadata": {"aucs": [0.11843985267194612, 0.15722621572327322, 0.8410064434937881, 0.9343893242378034, 0.23560776472285272, 0.9225481771234976, 0.23416842457833587, 0.266051684557454, 0.3413772502817698, 0.1497252998469757]}, "task_prompt": ""}
{"id": "cd450e9f-1833-4d37-9081-3d1646047bcc", "fitness": 0.2767677415809118, "name": "MESAQuad", "description": "MESA-Quad is a budget-aware ensemble optimizer that mixes three proposal modes each round—anisotropic Gaussian perturbations around the current best, directional steps drawn from a short-lived memory of successful unit directions (or random anisotropic directions), and exploitation via a ridge-regularized quadratic surrogate minimized in a random low-dimensional subspace—while storing all evaluations in a bounded archive for surrogate fitting and selection. Per-coordinate anisotropic step sizes sigma are initialized large (init_sigma = 0.6 * domain) and adapted multiplicatively on success/failure (grow=1.25, shrink=0.70) with hard min/max bounds and an accum vector that biases directions where past successes occurred. The quadratic surrogate fits only in a subspace of dimension k ≈ ceil(n^subspace_exp) (subspace_exp=0.6) using recent archive points, regularizes with a small ridge, solves for the subspace minimizer and enforces a trust radius to avoid overly large extrapolations. To balance exploration and exploitation the method uses heavy-tailed step lengths for jumps, short local polishes after improvements, aging and pruning of directional memory (memory_size=16), stagnation detection that triggers archive-based soft restarts/diversification, and strict budget accounting via safe_eval.", "code": "import numpy as np\n\nclass MESAQuad:\n    \"\"\"\n    MESA-Quad: Multi-scale Ensemble with Quadratic surrogate\n\n    - budget: max function evaluations\n    - dim: problem dimensionality\n    Optional parameters tune ensemble sizes, memory and adaptation rates.\n    The algorithm proposes candidates from a mixture of Gaussian, memory-directional\n    and low-dimensional quadratic-surrogate minima, adapts anisotropic scales,\n    and is strictly budget-aware.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_sigma=0.6,           # initial isotropic sigma fraction of domain\n                 ensemble=8,               # proposals per round (subject to remaining budget)\n                 memory_size=16,           # stored successful directions\n                 subspace_exp=0.6,         # exponent for surrogate subspace dimension\n                 grow=1.25, shrink=0.70,   # scale multipliers on success/failure\n                 ridge=1e-8,               # ridge for quadratic regression\n                 max_archive=200):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_sigma = float(init_sigma)\n        self.ensemble = int(ensemble)\n        self.memory_size = int(memory_size)\n        self.subspace_exp = float(subspace_exp)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.ridge = float(ridge)\n        self.max_archive = int(max_archive)\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # handle bounds (functions on BBOB have bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain = np.maximum(1e-12, np.mean(ub - lb))\n\n        evals = 0\n\n        # sampling helper\n        def clip(x):\n            return np.minimum(ub, np.maximum(lb, x))\n\n        # safe evaluation counting and best-tracking\n        x0 = self.rng.uniform(lb, ub)\n        try:\n            f0 = func(x0)\n        except Exception:\n            f0 = np.inf\n        evals += 1\n        f_best = float(f0)\n        x_best = x0.copy()\n\n        # archive of past evaluated points (x, f)\n        archive = [(x_best.copy(), f_best)]\n\n        # anisotropic sigma (per-coordinate), scaled to domain\n        sigma = np.ones(n) * (self.init_sigma * domain)\n        sigma_min = 1e-12 * domain\n        sigma_max = 4.0 * domain\n\n        # directional memory: list of (dir_unit, age)\n        dir_memory = []\n\n        # per-coordinate accumulated squared successful steps (for anisotropy)\n        accum = np.zeros(n)\n\n        # counters for stagnation\n        no_improve = 0\n        stagnation_limit = max(10, int(5 + np.log(1 + n) * 4))\n\n        # internal safe eval\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(x)\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            # store in archive\n            archive.append((x.copy(), float(f)))\n            if len(archive) > self.max_archive:\n                archive.pop(0)\n            return float(f), x.copy()\n\n        # build quadratic surrogate in k-dim random subspace based on recent archive\n        def propose_quadratic_minimum(center):\n            # subspace dimension\n            k = max(2, min(n, int(np.ceil(n ** self.subspace_exp))))\n            # construct random orthonormal basis U (n x k)\n            R = self.rng.normal(size=(n, k))\n            U, _ = np.linalg.qr(R)\n            U = U[:, :k]\n            # select recent points\n            num_features = (k * (k + 1)) // 2 + k + 1\n            m = min(len(archive), max(num_features + 2, 3))\n            if m < num_features + 1:\n                return None  # not enough data to fit meaningful quadratic\n            recent = archive[-m:]\n            # build design matrix A and vector y\n            A = np.zeros((m, num_features))\n            y = np.zeros(m)\n            for i, (x_i, f_i) in enumerate(recent):\n                z = U.T @ (x_i - center)   # coordinates in subspace\n                # feature vector: [z_i*z_j for i<=j], [z_i], [1]\n                feat = []\n                for ii in range(k):\n                    for jj in range(ii, k):\n                        feat.append(z[ii] * z[jj])\n                for ii in range(k):\n                    feat.append(z[ii])\n                feat.append(1.0)\n                A[i, :] = np.array(feat)\n                y[i] = f_i\n            # ridge regression to get coefficients c\n            ATA = A.T @ A\n            reg = self.ridge * np.eye(ATA.shape[0])\n            try:\n                c = np.linalg.solve(ATA + reg, A.T @ y)\n            except np.linalg.LinAlgError:\n                # fallback least squares\n                c, *_ = np.linalg.lstsq(A, y, rcond=None)\n            # map c to quadratic Q_sub, linear b_sub, const\n            idx = 0\n            Q_sub = np.zeros((k, k))\n            for ii in range(k):\n                for jj in range(ii, k):\n                    val = c[idx]\n                    if ii == jj:\n                        Q_sub[ii, jj] = 2.0 * val  # because feature is z_i^2 and in 0.5 z^T Q z diag contributes 0.5 Q_ii z_i^2 -> Q_ii = 2*coef\n                    else:\n                        Q_sub[ii, jj] = val\n                        Q_sub[jj, ii] = val\n                    idx += 1\n            b_sub = c[idx:idx + k].copy()\n            idx += k\n            c0 = c[idx]\n            # Solve Q_sub z + b_sub = 0  => z* = -inv(Q_sub) b_sub\n            # Regularize Q_sub to be invertible / PD\n            # We prefer a small step: limit norm of z*\n            try:\n                # ensure symmetric\n                Q_sub = 0.5 * (Q_sub + Q_sub.T)\n                # attempt to solve; add ridge if needed\n                lam = max(self.ridge, 1e-12 * np.linalg.norm(Q_sub))\n                try:\n                    z_star = -np.linalg.solve(Q_sub + lam * np.eye(k), b_sub)\n                except np.linalg.LinAlgError:\n                    # fallback to pseudo-inverse\n                    z_star = -np.linalg.pinv(Q_sub + lam * np.eye(k)) @ b_sub\n            except Exception:\n                return None\n            # map back to full space and limit to a reasonable step\n            x_candidate = center + U @ z_star\n            # bound candidate within a trust radius from center: at most 3*mean(sigma)\n            trust = max(2.0 * np.mean(sigma), 0.5 * domain)\n            if np.linalg.norm(x_candidate - center) > trust:\n                # shorten z_star proportionally\n                x_candidate = center + (x_candidate - center) * (trust / np.linalg.norm(x_candidate - center))\n            return clip(x_candidate)\n\n        # main loop: propose an ensemble of candidates each round\n        while evals < self.budget:\n            # proposals per round bounded by remaining budget\n            per_round = min(self.ensemble, self.budget - evals)\n            improved_this_round = False\n\n            # mixture probabilities, adaptively tilt toward exploitation if low sigma\n            p_quad = 0.15\n            p_dir = 0.30\n            p_gauss = 0.55\n            probs = np.array([p_gauss, p_dir, p_quad])\n            probs = probs / probs.sum()\n\n            for _ in range(per_round):\n                if evals >= self.budget:\n                    break\n                choice = self.rng.choice(3, p=probs)\n                x_try = None\n                if choice == 0:\n                    # Gaussian proposal centered at x_best: anisotropic\n                    noise = self.rng.normal(size=n) * sigma\n                    x_try = clip(x_best + noise)\n                elif choice == 1:\n                    # Directional step using memory or a random direction if empty\n                    if dir_memory and self.rng.random() < 0.9:\n                        # sample memory by recency (younger more likely)\n                        ages = np.array([d[1] for d in dir_memory], dtype=float)\n                        # transform ages into weights favoring smaller age (more recent)\n                        w = np.exp(-0.5 * (ages / (1.0 + ages.mean())))\n                        idx = self.rng.choice(len(dir_memory), p=(w / w.sum()))\n                        dir_unit = dir_memory[idx][0]\n                    else:\n                        # random anisotropic direction influenced by accum\n                        v = self.rng.normal(size=n) * (1.0 + 0.5 * accum / (1.0 + accum.max()))\n                        dn = np.linalg.norm(v)\n                        dir_unit = v / (dn + 1e-20)\n                    # step length drawn heavy-tailed to allow jumps\n                    step_len = (self.rng.standard_exponential() + 0.5) * (0.9 * np.mean(sigma))\n                    x_try = clip(x_best + step_len * dir_unit)\n                else:\n                    # Quadratic surrogate minimum in low-dim subspace around a center (x_best or random archive point)\n                    if len(archive) < 6:\n                        # fallback to gaussian\n                        noise = self.rng.normal(size=n) * sigma * 0.7\n                        x_try = clip(x_best + noise)\n                    else:\n                        # random choice for center: sometimes center at x_best, sometimes at a promising archive point\n                        if self.rng.random() < 0.8:\n                            center = x_best.copy()\n                        else:\n                            # pick a top-10% archive point or recent\n                            sorted_by_f = sorted(archive, key=lambda t: t[1])\n                            pick_pool = sorted_by_f[:max(1, len(sorted_by_f) // 6)]\n                            center = pick_pool[self.rng.integers(len(pick_pool))][0].copy()\n                        qx = propose_quadratic_minimum(center)\n                        if qx is None:\n                            # fallback small gaussian\n                            noise = self.rng.normal(size=n) * (0.5 * sigma)\n                            x_try = clip(center + noise)\n                        else:\n                            x_try = qx\n\n                # Evaluate candidate\n                if x_try is None:\n                    continue\n                f_try, x_ret = safe_eval(x_try)\n                if f_try is None:\n                    break  # budget exhausted\n                # Acceptance and adaptation\n                if f_try < f_best - 1e-12:\n                    # found new best (safe_eval already updated f_best and x_best)\n                    # create direction vector from previous best to new best (we need previous best)\n                    # We stored previous best when entering the loop (x_best may already be updated inside safe_eval),\n                    # so reconstruct from archive second last if available, else use x_ret\n                    prev_x = archive[-2][0] if len(archive) >= 2 else x_ret.copy()\n                    dir_vec = x_ret - prev_x\n                    dn = np.linalg.norm(dir_vec)\n                    if dn > 1e-16:\n                        dir_unit = dir_vec / dn\n                        # insert fresh memory with age 0\n                        dir_memory.insert(0, [dir_unit.copy(), 0.0])\n                        # cap memory size\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                        # update accum (anisotropy)\n                        accum = 0.85 * accum + 0.15 * (dir_unit ** 2) * dn\n                    else:\n                        # small change: nudge accum lightly\n                        accum = 0.98 * accum + 0.02 * (np.ones(n))\n                    # grow sigma moderately\n                    sigma = np.minimum(sigma * self.grow, sigma_max)\n                    improved_this_round = True\n                    no_improve = 0\n                else:\n                    # failure: shrink sigma slightly\n                    sigma = np.maximum(sigma * self.shrink, sigma_min)\n                    no_improve += 1\n                    # age memory entries (increase age), and decay very old ones\n                    for item in dir_memory:\n                        item[1] += 1.0\n                    # occasionally remove very old ones\n                    if dir_memory and len(dir_memory) > 3 and self.rng.random() < 0.05:\n                        dir_memory.pop(-1)\n\n                # ensure sigma positivity\n                sigma = np.clip(sigma, sigma_min, sigma_max)\n\n            # End of per_round proposals\n            # If improvement occurred, do a short local polish: a few coordinate-wise tries around x_best\n            if improved_this_round:\n                polishes = min(6, 2 + int(np.log(1 + n)))\n                for _ in range(polishes):\n                    if evals >= self.budget:\n                        break\n                    # coordinate Gaussian scaled by sigma\n                    noise = self.rng.normal(scale=0.25 * sigma)\n                    x_p = clip(x_best + noise)\n                    f_p, x_p_ret = safe_eval(x_p)\n                    if f_p is None:\n                        break\n                    if f_p < f_best - 1e-12:\n                        # update direction memory\n                        dir_succ = x_p_ret - x_best\n                        dn = np.linalg.norm(dir_succ)\n                        if dn > 1e-16:\n                            dir_unit = dir_succ / dn\n                            dir_memory.insert(0, [dir_unit.copy(), 0.0])\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                            accum = 0.9 * accum + 0.1 * (dir_unit ** 2) * dn\n                        sigma = np.minimum(sigma * self.grow, sigma_max)\n                        no_improve = 0\n            else:\n                # if no improvement in the whole round, consider diversification\n                if no_improve >= max(3, stagnation_limit // 3):\n                    # random anisotropic jump around the best based on accum and sigma\n                    weight = 0.5 + 0.5 * (accum / (1.0 + accum.max()))\n                    jump = self.rng.normal(size=n) * sigma * (1.0 + 0.8 * self.rng.exponential(size=n)) * weight\n                    x_jump = clip(x_best + jump)\n                    f_j, x_jret = safe_eval(x_jump)\n                    if f_j is None:\n                        break\n                    if f_j < f_best - 1e-12:\n                        # success\n                        dir_vec = x_jret - x_best\n                        dn = np.linalg.norm(dir_vec)\n                        if dn > 1e-16:\n                            dir_memory.insert(0, [dir_vec / dn, 0.0])\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                            accum = 0.9 * accum + 0.1 * (dir_vec / dn) ** 2 * dn\n                        sigma = np.minimum(sigma * (self.grow ** 1.5), sigma_max)\n                        no_improve = 0\n                    else:\n                        # failed jump: shrink more aggressively\n                        sigma = np.maximum(sigma * (self.shrink ** 1.2), sigma_min)\n                        no_improve += 1\n\n            # occasional soft restart if severe stagnation\n            if no_improve >= stagnation_limit:\n                # broaden sigma and re-center to a good archive point\n                no_improve = 0\n                # pick one of top archive points randomly\n                sorted_by_f = sorted(archive, key=lambda t: t[1])\n                top_k = max(1, len(sorted_by_f) // 6)\n                chosen = sorted_by_f[self.rng.integers(min(len(sorted_by_f), top_k))][0]\n                perturb = self.rng.normal(size=n) * (1.0 + self.rng.exponential(size=n)) * (0.5 * domain)\n                x_new = clip(chosen + perturb * (1.0 + accum / (1.0 + accum.max())))\n                res = safe_eval(x_new)\n                if res[0] is None:\n                    break\n                if res[0] < f_best - 1e-12:\n                    # keep broadened sigma\n                    sigma = np.minimum(sigma * 2.0, sigma_max)\n                else:\n                    # reduce sigma modestly and continue\n                    sigma = np.maximum(sigma * 0.5, sigma_min)\n                # forget a portion of memory to diversify\n                keep = max(0, int(len(dir_memory) * 0.6))\n                dir_memory = dir_memory[:keep]\n\n            # safeguard sigma bounds\n            sigma = np.clip(sigma, sigma_min, sigma_max)\n\n            # early stop if extremely small best\n            if f_best <= 1e-14:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm MESAQuad scored 0.277 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "4c818849-323e-4065-9772-bd26cb830a8b", "operator": null, "metadata": {"aucs": [0.10872825070434688, 0.14805718804623025, 0.3555814582628839, 0.45610244955682344, 0.29108912116306473, 0.3820342602574055, 0.23812690239796575, 0.3318638156691808, 0.2879902857278682, 0.1681036840233483]}, "task_prompt": ""}
{"id": "1967cd8e-51b2-4080-be3a-f35a622f02a8", "fitness": 0.4330779029326619, "name": "LGAES", "description": "LGAES is a Lévy-guided adaptive ensemble search that samples in a compact random orthonormal subspace each round (k = ceil(n^0.45), probes = max(4, 2*k)) and mixes many small Gaussian probes with occasional Cauchy-like heavy-tailed jumps to balance local refinement and global exploration, using an enlarged initial step (0.45·domain) and bounded, conservatively adapted step sizes (grow=1.15, shrink=0.80). It keeps a small LRU memory of successful unit directions (memory_size=6) and a per‑coordinate scaling vector to bias sampling and adaptively emphasize promising dimensions; the basis is built by combining recent memory directions with Gaussian vectors and orthonormalizing (QR). The algorithm is strictly budget-aware via a safe_eval wrapper with bound clipping, and it performs cheap local exploitation (micro coordinate searches, single-step refinements and small Gaussian ‘polishes’) whenever probes produce improvements. Stagnation is handled by a few conservative mechanisms—limited Lévy-style restarts biased toward the best, memory decay, deterministic coordinate sweeps, and hard step min/max limits—designed to diversify without excessive resets (max_restarts=4).", "code": "import numpy as np\n\nclass LGAES:\n    \"\"\"\n    Lévy-Guided Adaptive Ensemble Search (LGAES)\n\n    Main algorithm parameters (chosen to differ from ADES):\n      - subspace_exp = 0.45         # k = ceil(n**0.45) (smaller subspace than ADES's 0.6)\n      - probes_factor = 2           # probes = max(4, 2*k) (fewer probes per round than ADES)\n      - init_step_scale = 0.45      # initial step as fraction of domain (larger than ADES's 0.25)\n      - grow = 1.15, shrink = 0.80  # more conservative step adaptation than ADES\n      - memory_size = 6             # smaller and more selective memory than ADES (12)\n      - heavy_tail: use Cauchy-like jumps occasionally to explore globally\n      - max_restarts = 4\n      - min_step_scale = 1e-8\n    These choices aim for a different exploration/exploitation bias vs ADES.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=6,\n                 init_step_scale=0.45,\n                 subspace_exp=0.45,\n                 probes_factor=2,\n                 grow=1.15, shrink=0.80,\n                 max_restarts=4,\n                 min_step_scale=1e-8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.init_step_scale = float(init_step_scale)\n        self.subspace_exp = float(subspace_exp)\n        self.probes_factor = int(probes_factor)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.max_restarts = int(max_restarts)\n        self.min_step_scale = float(min_step_scale)\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        # characteristic domain scale (mean range across dims)\n        domain = np.maximum(1e-12, np.mean(ub - lb))\n\n        evals = 0\n\n        # start at random point\n        x_cur = self.rng.uniform(lb, ub)\n        f_cur = func(x_cur); evals += 1\n        x_best = x_cur.copy()\n        f_best = float(f_cur)\n\n        # steps and limits\n        step = self.init_step_scale * domain\n        min_step = self.min_step_scale * domain\n        max_step = 2.0 * domain\n\n        # subspace dimension and probes\n        k = max(1, int(np.ceil(n ** self.subspace_exp)))\n        probes = max(4, self.probes_factor * k)\n\n        # compact memory of successful unit directions (LRU: newest at front)\n        dir_memory = []\n\n        # per-coordinate scaling (diagonal adapt)\n        coord_scale = np.ones(n)\n\n        # stagnation tracking\n        no_improve = 0\n        stagnation_limit = max(10, int(6 + np.log1p(n) * 4))\n        restarts = 0\n\n        def clip_to_bounds(x):\n            return np.minimum(ub, np.maximum(lb, x))\n\n        # safe evaluation wrapper (returns (f, x_clipped) or (None, None) if budget exhausted)\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(x)\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # generate a normalized direction from a vector (guard zero)\n        def unitize(v):\n            norm = np.linalg.norm(v)\n            if norm == 0:\n                return None\n            return v / norm\n\n        # Main loop: budget-aware\n        while evals < self.budget:\n            # adapt subspace size occasionally (recompute)\n            k = max(1, int(np.clip(int(np.ceil(n ** self.subspace_exp)), 1, n)))\n            probes = max(4, self.probes_factor * k)\n\n            improved_round = False\n\n            # Build basis: include up to use_mem memory directions, fill the rest with gaussian vectors\n            use_mem = min(len(dir_memory), max(0, k // 3))\n            chosen = []\n            if use_mem > 0:\n                # prefer more recent memory directions\n                probs = np.linspace(1.0, 0.6, len(dir_memory))\n                probs = probs / probs.sum()\n                idxs = self.rng.choice(len(dir_memory), size=use_mem, replace=False, p=probs)\n                for i in idxs:\n                    chosen.append(dir_memory[int(i)].copy())\n            needed = k - len(chosen)\n            if needed > 0:\n                R = self.rng.normal(size=(n, needed)) * coord_scale.reshape(-1, 1)\n                if chosen:\n                    R = np.column_stack((np.column_stack(chosen), R))\n                # orthonormalize; if rank < k, QR returns available columns\n                try:\n                    Q, _ = np.linalg.qr(R)\n                except Exception:\n                    # fallback: orthonormalize by Gram-Schmidt manual (rare)\n                    Q = np.zeros((n, R.shape[1]))\n                    for i in range(R.shape[1]):\n                        v = R[:, i].copy()\n                        for j in range(i):\n                            v = v - np.dot(Q[:, j], v) * Q[:, j]\n                        nv = np.linalg.norm(v)\n                        if nv > 0:\n                            Q[:, i] = v / nv\n                basis = Q[:, :k]\n            else:\n                B = np.column_stack(chosen)\n                Q, _ = np.linalg.qr(B)\n                basis = Q[:, :k]\n\n            # Probing: mix local Gaussian probing and occasional heavy-tailed jumps (Cauchy)\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n\n                # choose a coefficient vector (gaussian) and form direction\n                coeffs = self.rng.normal(size=k)\n                d = basis @ coeffs\n                du = unitize(d)\n                if du is None:\n                    continue\n\n                # choose scale: mostly small (uniform in [-step, step]) with small chance of Cauchy heavy tail\n                if self.rng.random() < 0.10:\n                    # heavy-tailed long jump (Cauchy-like)\n                    try:\n                        tail = self.rng.standard_cauchy()\n                    except Exception:\n                        # fallback using ratio of normals\n                        tail = self.rng.normal() / (self.rng.normal() + 1e-16)\n                    alpha = np.clip(tail, -10.0, 10.0) * step * 2.5\n                else:\n                    alpha = self.rng.uniform(-1.0, 1.0) * step\n\n                x_try = clip_to_bounds(x_cur + alpha * du)\n                f_try, x_ret = safe_eval(x_try)\n                if f_try is None:\n                    break\n\n                if f_try < f_cur - 1e-12:\n                    # success: register unit displacement direction and update coord_scale gently\n                    dir_succ = x_ret - x_cur\n                    dir_unit = unitize(dir_succ)\n                    if dir_unit is not None:\n                        dir_memory.insert(0, dir_unit.copy())\n                        # keep memory size small (LRU)\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                        # update per-coordinate scales: push up dims where move was strong\n                        coord_scale = 0.92 * coord_scale + 0.08 * (1.0 + np.abs(dir_unit))\n                        # normalize coord_scale to avoid runaway\n                        coord_scale = coord_scale / np.maximum(coord_scale.mean(), 1e-12)\n                    # accept\n                    x_cur = x_ret.copy()\n                    f_cur = f_try\n                    improved_round = True\n                    no_improve = 0\n                    # moderate grow on success\n                    step = min(step * self.grow, max_step)\n\n                    # local small polishing: a single directional refined probe along accepted direction\n                    # (cheap, one extra eval if budget allows)\n                    if evals < self.budget:\n                        # try a shorter step in same direction\n                        x_local = clip_to_bounds(x_cur + 0.4 * (-alpha) * du)\n                        f_local, x_local_ret = safe_eval(x_local)\n                        if f_local is not None and f_local < f_cur - 1e-12:\n                            # accept local improvement\n                            x_cur = x_local_ret.copy()\n                            f_cur = f_local\n                            if dir_unit is not None:\n                                dir_memory.insert(0, dir_unit.copy())\n                                if len(dir_memory) > self.memory_size:\n                                    dir_memory.pop()\n                else:\n                    # unsuccessful: with small probability attempt a coordinate-wise micro-search (cheap)\n                    if self.rng.random() < 0.06 and evals + 2 <= self.budget:\n                        idx = self.rng.integers(0, n)\n                        dvec = np.zeros(n); dvec[idx] = 1.0\n                        for sign in (+1, -1):\n                            a = sign * 0.6 * step * coord_scale[idx]\n                            f_cand, x_cand = safe_eval(clip_to_bounds(x_cur + a * dvec))\n                            if f_cand is None:\n                                break\n                            if f_cand < f_cur - 1e-12:\n                                # accept coordinate improvement\n                                dir_succ = x_cand - x_cur\n                                dir_unit = unitize(dir_succ)\n                                if dir_unit is not None:\n                                    dir_memory.insert(0, dir_unit.copy())\n                                    if len(dir_memory) > self.memory_size:\n                                        dir_memory.pop()\n                                    coord_scale = 0.95 * coord_scale + 0.05 * (1.0 + np.abs(dir_unit))\n                                    coord_scale = coord_scale / np.maximum(coord_scale.mean(), 1e-12)\n                                x_cur = x_cand.copy()\n                                f_cur = f_cand\n                                improved_round = True\n                                no_improve = 0\n                                step = min(step * self.grow, max_step)\n                                break\n\n            # end probing\n\n            if not improved_round:\n                no_improve += 1\n                # shrink step moderately\n                step = max(step * self.shrink, min_step)\n\n                # occasional Lévy/global restart jump when stagnating\n                if no_improve % max(2, stagnation_limit // 4) == 0 and restarts < self.max_restarts:\n                    restarts += 1\n                    # draw heavy-tailed perturbation vector scaled by coord_scale\n                    try:\n                        tail_vec = self.rng.standard_cauchy(size=n)\n                    except Exception:\n                        tail_vec = self.rng.normal(size=n) / (self.rng.normal(size=n) + 1e-16)\n                    # clip extremes\n                    tail_vec = np.clip(tail_vec, -8.0, 8.0)\n                    jump = tail_vec * (0.6 * domain) * (coord_scale / np.maximum(coord_scale.max(), 1e-12))\n                    # bias towards best found to focus restart\n                    x_new = clip_to_bounds(x_best + 0.5 * jump)\n                    f_new, x_n = safe_eval(x_new)\n                    if f_new is None:\n                        break\n                    if f_new < f_cur - 1e-12:\n                        x_cur = x_n.copy()\n                        f_cur = f_new\n                        no_improve = 0\n                        step = min(step * 1.2, max_step)\n                    else:\n                        # mild diversification: random offset around current\n                        x_cur = clip_to_bounds(x_cur + self.rng.normal(scale=0.25 * domain, size=n) * (coord_scale / np.maximum(coord_scale.max(), 1e-12)))\n                        res = safe_eval(x_cur)\n                        if res[0] is None:\n                            break\n                        f_cur, _ = res\n                        no_improve = 0\n                        # slightly reset memory to encourage new directions\n                        if dir_memory:\n                            # keep only every other direction (decay memory)\n                            dir_memory = dir_memory[::2]\n\n            else:\n                # local polishing after successful round: a few small gaussian nudges, cheap\n                polishes = min(4, 2 + int(np.log1p(n)))\n                for _ in range(polishes):\n                    if evals >= self.budget:\n                        break\n                    noise = self.rng.normal(scale=0.18 * step * coord_scale)\n                    x_p = clip_to_bounds(x_cur + noise)\n                    f_p, x_r = safe_eval(x_p)\n                    if f_p is None:\n                        break\n                    if f_p < f_cur - 1e-12:\n                        # accept and store direction\n                        dir_succ = x_r - x_cur\n                        dir_unit = unitize(dir_succ)\n                        if dir_unit is not None:\n                            dir_memory.insert(0, dir_unit.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                            coord_scale = 0.94 * coord_scale + 0.06 * (1.0 + np.abs(dir_unit))\n                            coord_scale = coord_scale / np.maximum(coord_scale.mean(), 1e-12)\n                        x_cur = x_r.copy()\n                        f_cur = f_p\n                        no_improve = 0\n\n            # stagnation severe: deterministic local coordinate sweep and possibly terminate or re-center\n            if no_improve >= stagnation_limit:\n                if restarts >= self.max_restarts or step <= min_step * 1.1:\n                    # small coordinate sweep to polish then break preferences to conserve budget\n                    coords = np.arange(n)\n                    self.rng.shuffle(coords)\n                    for i in coords[:min(n, 8)]:\n                        if evals >= self.budget:\n                            break\n                        for sign in (+1, -1):\n                            a = sign * 0.6 * step * coord_scale[i]\n                            f_try, x_ret = safe_eval(clip_to_bounds(x_cur + a * np.eye(1, n, i).ravel()))\n                            if f_try is None:\n                                break\n                            if f_try < f_cur - 1e-12:\n                                x_cur = x_ret.copy()\n                                f_cur = f_try\n                                no_improve = 0\n                    # reduce step to try finer search\n                    step = max(step * 0.5, min_step)\n                    no_improve = 0\n                    restarts = self.max_restarts  # discourage more restarts\n                else:\n                    # re-center near best but keep some memory\n                    restarts += 1\n                    x_cur = clip_to_bounds(x_best + self.rng.normal(scale=0.3 * domain, size=n) * (coord_scale / np.maximum(coord_scale.max(), 1e-12)))\n                    res = safe_eval(x_cur)\n                    if res[0] is None:\n                        break\n                    f_cur, _ = res\n                    # shrink memory slightly\n                    if dir_memory:\n                        dir_memory = dir_memory[:max(1, len(dir_memory)//2)]\n                    step = min(max_step, step * 1.5)\n                    no_improve = 0\n\n            # safeguard step lower bound\n            if step < min_step:\n                step = min_step\n\n            # early exit if extremely small best\n            if f_best <= 1e-14 or evals >= self.budget:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm LGAES scored 0.433 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "4c818849-323e-4065-9772-bd26cb830a8b", "operator": null, "metadata": {"aucs": [0.062239662725282696, 0.14821880895646433, 0.9566979355587949, 0.9730205742685963, 0.21381540867522897, 0.9700304704166159, 0.25515430330291144, 0.3741335607211226, 0.2761531572945778, 0.10131514740702352]}, "task_prompt": ""}
{"id": "ef3a29ec-219a-4252-a1c4-ccf7f43de111", "fitness": "-inf", "name": "HASE", "description": "HASE is a hybrid adaptive subspace ensemble that alternates budget‑aware global probing in randomly chosen low‑dimensional orthonormal subspaces (subspace size stochastically sampled between ~sqrt(n) and n^0.62, with probes = probes_factor * k) and local refinement via cheap line-search heuristics (quick_parabola and short_golden) to exploit promising directions. It learns and reuses structure online by keeping a small recent direction memory and success pool (memory_size=14, success_pool=16), estimating a principal displacement direction via SVD of recent successes, and biasing future random probes with an anisotropic coord_scale that is slowly nudged toward active coordinates (weighted updates like 0.88/0.12 or 0.9/0.1). Exploration vs. exploitation is controlled by multiplicative step-size adaptation (grow=1.22, shrink=0.75, init_step_scale=0.30), heavy‑tailed coefficient sampling to allow both small and large jumps, stagnation restarts with anisotropic perturbations, and light polishing (coordinate sweeps and Gaussian tweaks) — all implemented to respect the strict evaluation budget and box bounds.", "code": "import numpy as np\nfrom collections import deque\n\nclass HASE:\n    \"\"\"\n    Hybrid Adaptive Subspace Ensemble (HASE)\n    - budget: total allowed function evaluations\n    - dim: problem dimension\n    Optional tuning exposed in __init__.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=14,\n                 success_pool=16,\n                 init_step_scale=0.30,\n                 grow=1.22, shrink=0.75,\n                 subspace_exp_low=0.5, subspace_exp_high=0.62,\n                 probes_factor=3,\n                 max_line_evals=6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.memory_size = int(memory_size)\n        self.success_pool = int(success_pool)\n        self.init_step_scale = float(init_step_scale)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.subspace_exp_low = float(subspace_exp_low)\n        self.subspace_exp_high = float(subspace_exp_high)\n        self.probes_factor = int(probes_factor)\n        self.max_line_evals = int(max_line_evals)\n\n    def __call__(self, func):\n        n = self.dim\n        # Read bounds (support scalar or vector bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain = np.maximum(1e-12, np.mean(ub - lb))\n\n        evals = 0\n\n        # helpers\n        def clip_to_bounds(x):\n            return np.minimum(ub, np.maximum(lb, x))\n\n        f_best = np.inf\n        x_best = None\n\n        # start at a random point\n        x_cur = self.rng.uniform(lb, ub)\n        # safe initial eval\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x.copy()\n\n        f_res, x_res = safe_eval(x_cur)\n        if f_res is None:\n            # no budget\n            return float(f_best), np.array(x_best, dtype=float)\n        f_cur = float(f_res)\n        x_cur = x_res.copy()\n\n        # step sizes and limits\n        step = self.init_step_scale * domain\n        min_step = 1e-9 * domain\n        max_step = 4.0 * domain\n\n        # memory structures\n        dir_memory = deque(maxlen=self.memory_size)  # store unit directions (recent best)\n        recent_success = deque(maxlen=self.success_pool)  # raw displacement vectors for PCA-like estimate\n        coord_scale = np.ones(n)  # per-coordinate anisotropy\n\n        no_improve = 0\n        stagnation_limit = max(12, int(6 + np.log(1 + n) * 5))\n        restarts = 0\n        max_restarts = max(4, int(np.ceil(np.log(1 + n))))\n\n        # quick 3-point parabolic refinement (budget-aware, returns improved (f,x) or (None,None))\n        def quick_parabola(x0, f0, d, init_step=1.0, max_evals=5):\n            dn = np.linalg.norm(d)\n            if dn == 0 or (self.budget - evals) <= 0:\n                return None, None\n            d = d / dn\n            remaining = self.budget - evals\n            pts = [(0.0, f0, x0.copy())]\n            # +s\n            if remaining >= 1:\n                x1 = clip_to_bounds(x0 + init_step * d)\n                f1, x1c = safe_eval(x1)\n                if f1 is None:\n                    return None, None\n                pts.append((init_step, f1, x1c))\n                remaining = self.budget - evals\n            # -s\n            if remaining >= 1:\n                x2 = clip_to_bounds(x0 - init_step * d)\n                f2, x2c = safe_eval(x2)\n                if f2 is None:\n                    return None, None\n                pts.append((-init_step, f2, x2c))\n                remaining = self.budget - evals\n            # if best non-zero and still budget, try doubling\n            pts_sorted = sorted(pts, key=lambda t: t[1])\n            best_alpha, best_f, best_x = pts_sorted[0]\n            if best_alpha != 0.0 and (self.budget - evals) >= 1 and abs(best_alpha) < 10 * domain:\n                new_alpha = 2.0 * best_alpha\n                x3 = clip_to_bounds(x0 + new_alpha * d)\n                f3, x3c = safe_eval(x3)\n                if f3 is None:\n                    return None, None\n                pts.append((new_alpha, f3, x3c))\n                pts_sorted = sorted(pts, key=lambda t: t[1])\n                best_alpha, best_f, best_x = pts_sorted[0]\n            if len(pts) < 3:\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            alphas = np.array([p[0] for p in pts])\n            fs = np.array([p[1] for p in pts])\n            xs = [p[2] for p in pts]\n            # pick three points with spread around best\n            idxs = np.argsort(fs)[:3]\n            a1, f1, x1 = alphas[idxs[0]], fs[idxs[0]], xs[idxs[0]]\n            a2, f2, x2 = alphas[idxs[1]], fs[idxs[1]], xs[idxs[1]]\n            a3, f3, x3 = alphas[idxs[2]], fs[idxs[2]], xs[idxs[2]]\n            M = np.array([[a1*a1, a1, 1.0],\n                          [a2*a2, a2, 1.0],\n                          [a3*a3, a3, 1.0]])\n            y = np.array([f1, f2, f3])\n            try:\n                A, B, C = np.linalg.solve(M, y)\n                if abs(A) < 1e-18:\n                    return (best_f, best_x) if best_f < f0 - 1e-12 else (None, None)\n                alpha_opt = -B / (2.0 * A)\n            except np.linalg.LinAlgError:\n                return (best_f, best_x) if best_f < f0 - 1e-12 else (None, None)\n            # clip alpha_opt near sampled range\n            span = max(1e-12, alphas.max() - alphas.min())\n            alpha_opt = np.clip(alpha_opt, alphas.min() - 0.6*span, alphas.max() + 0.6*span)\n            if (self.budget - evals) >= 1:\n                x_opt = clip_to_bounds(x0 + alpha_opt * d)\n                f_opt, x_optc = safe_eval(x_opt)\n                if f_opt is None:\n                    return None, None\n                if f_opt < f0 - 1e-12:\n                    return f_opt, x_optc\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n            else:\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n            return None, None\n\n        # small golden-section fallback for promising direction (conservative evals)\n        def short_golden(x0, f0, d, init_step=1.0, max_evals=6):\n            dn = np.linalg.norm(d)\n            if dn == 0 or (self.budget - evals) <= 0:\n                return None, None\n            d = d / dn\n            remaining = self.budget - evals\n            # try +s then expand\n            a0 = 0.0; fa = f0\n            s = init_step\n            xb = clip_to_bounds(x0 + s*d)\n            f_b, xb_c = safe_eval(xb)\n            if f_b is None:\n                return None, None\n            remaining = self.budget - evals\n            if f_b >= fa:\n                xb = clip_to_bounds(x0 - s*d)\n                f_b2, xb2c = safe_eval(xb)\n                if f_b2 is None:\n                    return None, None\n                remaining = self.budget - evals\n                if f_b2 >= fa:\n                    return None, None\n                f_b = f_b2; xb_c = xb2c\n                s = -s\n            # bracket found (0, s)\n            gr = (np.sqrt(5)-1)/2\n            a = a0; b = s\n            c = b - gr*(b-a)\n            d_alpha = a + gr*(b-a)\n            xc = clip_to_bounds(x0 + c*d)\n            f_c, xc_c = safe_eval(xc)\n            if f_c is None:\n                return None, None\n            remaining = self.budget - evals\n            xd = clip_to_bounds(x0 + d_alpha*d)\n            f_d, xd_c = safe_eval(xd)\n            if f_d is None:\n                return None, None\n            remaining = self.budget - evals\n            best_f = fa; best_x = x0.copy()\n            for v,fv,xv in [(f_c, f_c, xc_c), (f_d, f_d, xd_c)]:\n                if fv < best_f:\n                    best_f = fv; best_x = xv.copy()\n            it = 0\n            while (self.budget - evals) > 0 and it < max_evals and abs(b-a) > 1e-12:\n                it += 1\n                if f_c < f_d:\n                    b = d_alpha\n                    d_alpha = c\n                    f_d = f_c\n                    d_alpha = a + gr*(b-a)\n                    xd = clip_to_bounds(x0 + d_alpha*d)\n                    f_d, xd_c = safe_eval(xd)\n                    if f_d is None:\n                        break\n                else:\n                    a = c\n                    c = d_alpha\n                    f_c = f_d\n                    c = b - gr*(b-a)\n                    xc = clip_to_bounds(x0 + c*d)\n                    f_c, xc_c = safe_eval(xc)\n                    if f_c is None:\n                        break\n                if f_c < best_f:\n                    best_f = f_c; best_x = xc.copy()\n                if f_d < best_f:\n                    best_f = f_d; best_x = xd_c.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # Main loop\n        while evals < self.budget:\n            # adapt subspace dimension stochastically between sqrt(n) and ~n**0.62\n            exp = self.rng.uniform(self.subspace_exp_low, self.subspace_exp_high)\n            k = max(1, int(np.clip(int(np.ceil(n ** exp)), 1, n)))\n            probes = max(6, self.probes_factor * k)\n\n            improved = False\n\n            # Build basis: combine principal learned direction (if available), top memory, and random vectors\n            # compute principal direction from recent_success via SVD (low-cost small matrix)\n            principal = None\n            if len(recent_success) >= 3:\n                S = np.vstack(recent_success)  # m x n where m small\n                try:\n                    # row-centered SVD to get principal displacement\n                    U, svals, Vt = np.linalg.svd(S - S.mean(axis=0, keepdims=True), full_matrices=False)\n                    principal = Vt[0, :]\n                    if np.linalg.norm(principal) > 0:\n                        principal = principal / np.linalg.norm(principal)\n                except Exception:\n                    principal = None\n\n            chosen_dirs = []\n            if principal is not None and self.rng.random() < 0.9:\n                chosen_dirs.append(principal)\n            # reuse some memory preferentially\n            use_mem = min(len(dir_memory), k // 2)\n            if use_mem > 0:\n                # take most recent ones\n                for i in range(use_mem):\n                    chosen_dirs.append(dir_memory[i])\n            # fill remainder with random anisotropic vectors\n            needed = k - len(chosen_dirs)\n            if needed <= 0:\n                B = np.column_stack(chosen_dirs[:k])\n                Q, _ = np.linalg.qr(B)\n                basis = Q[:, :k]\n            else:\n                R = self.rng.normal(size=(n, needed)) * coord_scale.reshape(-1,1)\n                if chosen_dirs:\n                    R = np.column_stack((np.column_stack(chosen_dirs), R))\n                try:\n                    Q, _ = np.linalg.qr(R)\n                    basis = Q[:, :k]\n                except np.linalg.LinAlgError:\n                    # fallback to random orthonormal\n                    R2 = self.rng.normal(size=(n, k))\n                    Q, _ = np.linalg.qr(R2)\n                    basis = Q[:, :k]\n\n            # Probing within subspace: mix heavy-tailed coefficients to allow both small and large probes\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                coeffs = self.rng.normal(size=k) * (1.0 + 0.6 * self.rng.standard_exponential(size=k))\n                d = basis @ coeffs\n                dn = np.linalg.norm(d)\n                if dn == 0:\n                    continue\n                d = d / dn\n                # consider mixing coordinate anisotropy\n                alpha = self.rng.uniform(-1.0, 1.0) * step\n                x_try = clip_to_bounds(x_cur + alpha * d)\n                f_try, x_tryc = safe_eval(x_try)\n                if f_try is None:\n                    break\n                if f_try < f_cur - 1e-12:\n                    # success: record displacement & memory, update coord_scale\n                    disp = x_tryc - x_cur\n                    dn2 = np.linalg.norm(disp)\n                    if dn2 > 0:\n                        unit = disp / dn2\n                        dir_memory.appendleft(unit.copy())\n                        recent_success.appendleft(disp.copy())\n                        # nudge coord_scale towards abs(unit) to favor active coordinates\n                        coord_scale = 0.88 * coord_scale + 0.12 * (1.0 + np.abs(unit))\n                    # quick parabola refinement along displacement direction (cheap)\n                    f_line, x_line = quick_parabola(x_cur, f_cur, unit if dn2>0 else d,\n                                                    init_step=alpha if abs(alpha) > 1e-16 else step,\n                                                    max_evals=min(self.max_line_evals, self.budget - evals))\n                    if f_line is not None and f_line < f_try - 1e-12:\n                        f_try, x_tryc = f_line, x_line\n                    # accept\n                    x_cur = x_tryc.copy()\n                    f_cur = float(f_try)\n                    improved = True\n                    no_improve = 0\n                    step = min(step * self.grow, max_step)\n                else:\n                    # rare local parabola from current point along this probe direction\n                    if self.rng.random() < 0.035 and (self.budget - evals) >= 3:\n                        f_line, x_line = quick_parabola(x_cur, f_cur, d, init_step=0.5*step, max_evals=4)\n                        if f_line is not None and f_line < f_cur - 1e-12:\n                            # store direction and accept\n                            disp = x_line - x_cur\n                            if np.linalg.norm(disp) > 0:\n                                dir_memory.appendleft((disp / np.linalg.norm(disp)).copy())\n                                recent_success.appendleft(disp.copy())\n                                coord_scale = 0.9 * coord_scale + 0.1 * (1.0 + np.abs(d))\n                            x_cur = x_line.copy()\n                            f_cur = float(f_line)\n                            improved = True\n                            no_improve = 0\n                            step = min(step * self.grow, max_step)\n\n            # After probing\n            if not improved:\n                no_improve += 1\n                step = max(step * self.shrink, min_step)\n            else:\n                # light polishing around x_cur: few anisotropic gaussian jumps\n                polishes = min(6, 2 + int(np.log(1 + n)))\n                for _ in range(polishes):\n                    if evals >= self.budget:\n                        break\n                    noise = self.rng.normal(scale=0.25 * step * coord_scale)\n                    x_p = clip_to_bounds(x_cur + noise)\n                    f_p, x_pc = safe_eval(x_p)\n                    if f_p is None:\n                        break\n                    if f_p < f_cur - 1e-12:\n                        disp = x_pc - x_cur\n                        if np.linalg.norm(disp) > 0:\n                            unit = disp / np.linalg.norm(disp)\n                            dir_memory.appendleft(unit.copy())\n                            recent_success.appendleft(disp.copy())\n                            coord_scale = 0.9 * coord_scale + 0.1 * (1.0 + np.abs(unit))\n                        x_cur = x_pc.copy()\n                        f_cur = float(f_p)\n                        no_improve = 0\n\n            # stagnation handling\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts or step <= 1.5 * min_step:\n                    # final local polishing: coordinate sweeps and tiny gaussian tweaks\n                    step = max(step * 0.4, min_step)\n                    coords = self.rng.permutation(n)[:min(n, 6)]\n                    for i in coords:\n                        if evals >= self.budget:\n                            break\n                        d = np.zeros(n); d[i] = 1.0\n                        for sgn in (+1, -1):\n                            a = sgn * 0.9 * step * coord_scale[i]\n                            f_try, x_tryc = safe_eval(clip_to_bounds(x_cur + a * d))\n                            if f_try is None:\n                                break\n                            if f_try < f_cur - 1e-12:\n                                disp = x_tryc - x_cur\n                                if np.linalg.norm(disp) > 0:\n                                    unit = disp / np.linalg.norm(disp)\n                                    dir_memory.appendleft(unit.copy())\n                                    recent_success.appendleft(disp.copy())\n                                    coord_scale = 0.9 * coord_scale + 0.1 * (1.0 + np.abs(unit))\n                                x_cur = x_tryc.copy(); f_cur = float(f_try); no_improve = 0\n                else:\n                    # anisotropic diversification restart around the best found so far\n                    if x_best is None:\n                        x_best = x_cur.copy()\n                    perturb = self.rng.normal(scale=0.6 * domain, size=n) * (coord_scale / coord_scale.max())\n                    x_cur = clip_to_bounds(x_best + perturb)\n                    res = safe_eval(x_cur)\n                    if res[0] is None:\n                        break\n                    f_cur, x_cur = res\n                    # clear half of memories to keep some structure\n                    keep = max(0, len(dir_memory) // 2)\n                    while len(dir_memory) > keep:\n                        dir_memory.pop()\n                    recent_success.clear()\n                    step = min(max_step, step * 2.0)\n                    no_improve = 0\n\n            # safety lower bound\n            if step < min_step:\n                step = min_step\n\n            # early exit if extremely small\n            if f_best <= 1e-14:\n                break\n\n        # final return\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 84, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_memory = deque(maxlen=self.memory_size)  # store unit directions (recent best)", "error": "In the code, line 84, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_memory = deque(maxlen=self.memory_size)  # store unit directions (recent best)", "parent_ids": "4c818849-323e-4065-9772-bd26cb830a8b", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "98a6ef08-aa28-41f9-952a-3f0d448bb2ef", "fitness": 0.28773566984502397, "name": "SurrogateAdaptiveDirectionalEnsemble", "description": "The algorithm begins with space-filling initial sampling within the given bounds and sets per-dimension trust radii and a global step based on domain scale (init_samples_ratio, trust_init_frac, init_step_factor) to give sensible starting scales and evaluation caps (max_eval_per_iter, budget-aware safe_eval). It fits a cheap separable quadratic surrogate on nearby archive points (weighted by proximity) to propose per-coordinate minimizers and clamps those proposals by per-dimension trust radii, expanding trust on success and shrinking on failure (success_expand, failure_shrink, trust_min/max). When the surrogate is unreliable it runs a PCA- and memory-biased ensemble of directional probes (build_subspace using recent successful directions mem_steps and mem_trust), evaluates many randomized directions, applies parabolic three-point line refinements to promising directions, and does cheap coordinate tweaks for local polishing. To escape stagnation it uses occasional heavy-tailed Cauchy jumps, keeps a short memory of successful direction vectors with weights for biasing future search, performs randomized restarts around the best solution, and prunes the archive to keep computation and modeling efficient.", "code": "import numpy as np\n\nclass SurrogateAdaptiveDirectionalEnsemble:\n    \"\"\"\n    Surrogate-Directed Adaptive Directional Ensemble (SD-ADES)\n\n    One-line: Fit a cheap separable quadratic surrogate under per-dimension trust radii\n    and, when it is unreliable, run a PCA/memory-biased ensemble of directional probes\n    (with parabolic 3-point refinement), coordinate tweaks and occasional heavy-tailed jumps.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_samples_ratio=0.12, min_init=None, max_init=400,\n                 model_neighbor_multiplier=8,\n                 trust_init_frac=0.45, trust_min_frac=1e-6, trust_max_frac=2.0,\n                 success_expand=1.4, failure_shrink=0.72,\n                 memory_size=16, init_step_factor=0.5,\n                 max_eval_per_iter=80, cauchy_prob=0.12):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_samples_ratio = float(init_samples_ratio)\n        self.min_init = int(min_init) if min_init is not None else max(12, 2 * self.dim)\n        self.max_init = int(max_init)\n        self.model_neighbor_multiplier = int(model_neighbor_multiplier)\n        self.trust_init_frac = float(trust_init_frac)\n        self.trust_min_frac = float(trust_min_frac)\n        self.trust_max_frac = float(trust_max_frac)\n        self.success_expand = float(success_expand)\n        self.failure_shrink = float(failure_shrink)\n        self.memory_size = int(memory_size)\n        self.init_step_factor = float(init_step_factor)\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        self.cauchy_prob = float(cauchy_prob)\n        # rng\n        if seed is None:\n            self.rng = np.random.RandomState()\n        else:\n            self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds handling (works for scalar or vector bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        lb = lb.reshape(n,)\n        ub = ub.reshape(n,)\n        rng_range = ub - lb\n        domain_mean = np.mean(rng_range)\n        budget = int(self.budget)\n\n        evals = 0\n        X = []\n        F = []\n        f_best = np.inf\n        x_best = None\n\n        # safe_eval wrapper\n        def clip_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(np.asarray(x, dtype=float))\n            if evals >= budget:\n                return None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x\n\n        # initial space-filling-ish sampling\n        init_budget = int(np.clip(self.init_samples_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, min(init_budget, budget // 4))\n        # greedy max-min from a candidate pool\n        pool_size = max(5 * init_budget, init_budget + 20)\n        pool = self.rng.uniform(lb, ub, size=(pool_size, n))\n        chosen_idx = []\n        first = self.rng.randint(pool_size)\n        chosen_idx.append(first)\n        while len(chosen_idx) < init_budget:\n            chosen = pool[chosen_idx]\n            # distances from candidates to chosen set\n            dists = np.min(np.linalg.norm(pool - chosen[:, None, :], axis=2), axis=0)\n            next_i = int(np.argmax(dists))\n            chosen_idx.append(next_i)\n        for idx in chosen_idx:\n            if evals >= budget:\n                break\n            x = pool[idx].copy()\n            out = safe_eval(x)\n            if out is None:\n                break\n        if evals >= budget:\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # per-dimension trust radii (vector)\n        trust_radius = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n        trust_min = np.maximum(self.trust_min_frac * rng_range, 1e-12)\n        trust_max = np.maximum(self.trust_max_frac * rng_range, 1e-12)\n\n        # global step (L2-like) initial\n        step = max(self.init_step_factor * domain_mean, 1e-12)\n        min_step = max(1e-8 * domain_mean, 1e-12)\n\n        # memory of recent successful directions and trust\n        mem_steps = []  # list of unit vectors\n        mem_trust = []  # weights\n\n        # stagnation control\n        no_improve_iters = 0\n        stagnation_limit = max(12, int(8 + 2 * np.log1p(n)))\n        restarts = 0\n        max_restarts = 6\n\n        # helper: build subspace biased by PCA on mem_steps\n        def build_subspace(k):\n            if len(mem_steps) >= 2:\n                M = np.vstack(mem_steps)  # (m,n)\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                try:\n                    U, S, Vt = np.linalg.svd(Mc, full_matrices=False)\n                    pcs = Vt.T  # n x r\n                    take = min(k, pcs.shape[1])\n                    basis_cols = pcs[:, :take]\n                    if take < k:\n                        R = self.rng.randn(n, k - take)\n                        stacked = np.column_stack((basis_cols, R))\n                        Q, _ = np.linalg.qr(stacked)\n                        return Q[:, :k]\n                    else:\n                        Q, _ = np.linalg.qr(basis_cols)\n                        return Q[:, :k]\n                except np.linalg.LinAlgError:\n                    R = self.rng.randn(n, k)\n                    Q, _ = np.linalg.qr(R)\n                    return Q[:, :k]\n            else:\n                R = self.rng.randn(n, k)\n                Q, _ = np.linalg.qr(R)\n                return Q[:, :k]\n\n        # parabolic 3-point line refine (uses safe_eval)\n        def parabolic_line_min(x0, f0, d, delta=None):\n            nonlocal evals\n            if delta is None:\n                delta = step * 0.8\n            if np.linalg.norm(d) == 0:\n                return None\n            d = d / (np.linalg.norm(d) + 1e-20)\n            remaining = budget - evals\n            if remaining <= 0:\n                return None\n            x_p = clip_to_bounds(x0 + delta * d)\n            out = safe_eval(x_p)\n            if out is None:\n                return None\n            f_p, x_p = out\n            remaining = budget - evals\n            if remaining <= 0:\n                return (f_p, x_p) if f_p < f0 - 1e-12 else None\n            x_m = clip_to_bounds(x0 - delta * d)\n            out = safe_eval(x_m)\n            if out is None:\n                return None\n            f_m, x_m = out\n            denom = 2.0 * (delta ** 2)\n            A = (f_p + f_m - 2.0 * f0) / denom\n            B = (f_p - f_m) / (2.0 * delta)\n            if A <= 0 or np.isclose(A, 0.0):\n                # pick best of sampled if better\n                best_f = f0; best_x = x0.copy()\n                if f_p < best_f - 1e-12:\n                    best_f = f_p; best_x = x_p.copy()\n                if f_m < best_f - 1e-12:\n                    best_f = f_m; best_x = x_m.copy()\n                return (best_f, best_x) if best_f < f0 - 1e-12 else None\n            alpha_star = -B / (2.0 * A)\n            if abs(alpha_star) > 4.0 * delta:\n                best_f = f0; best_x = x0.copy()\n                if f_p < best_f - 1e-12:\n                    best_f = f_p; best_x = x_p.copy()\n                if f_m < best_f - 1e-12:\n                    best_f = f_m; best_x = x_m.copy()\n                return (best_f, best_x) if best_f < f0 - 1e-12 else None\n            remaining = budget - evals\n            if remaining <= 0:\n                return None\n            x_s = clip_to_bounds(x0 + alpha_star * d)\n            out = safe_eval(x_s)\n            if out is None:\n                return None\n            f_s, x_s = out\n            if f_s < f0 - 1e-12:\n                return (f_s, x_s)\n            # fallback\n            best_f = f0; best_x = x0.copy()\n            if f_p < best_f - 1e-12:\n                best_f = f_p; best_x = x_p.copy()\n            if f_m < best_f - 1e-12:\n                best_f = f_m; best_x = x_m.copy()\n            return (best_f, best_x) if best_f < f0 - 1e-12 else None\n\n        # main optimization loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            improved_round = False\n\n            # Attempt fast separable quadratic surrogate if enough archive\n            neighbors_needed = max(2 * n + 1, self.model_neighbor_multiplier * n)\n            if x_best is not None and len(X) >= neighbors_needed and work_allow >= 1:\n                X_arr = np.asarray(X)\n                dists = np.linalg.norm(X_arr - x_best, axis=1)\n                idx_sorted = np.argsort(dists)[:min(len(X_arr), neighbors_needed)]\n                X_nei = X_arr[idx_sorted]\n                F_nei = np.asarray(F)[idx_sorted]\n                dx = X_nei - x_best  # (m,n)\n                m = dx.shape[0]\n                # design for separable quadratic: [1, dx_i, 0.5*dx_i^2]\n                M = np.ones((m, 1 + 2 * n))\n                M[:, 1:1 + n] = dx\n                M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n                y = F_nei\n                # weights by proximity\n                prox = 1.0 / (dists[idx_sorted] + 1e-12)\n                prox = prox / (np.max(prox) + 1e-12)\n                W = np.sqrt(prox)[:, None]\n                A = W * M\n                b = W * y\n                ridge = 1e-6 * np.eye(M.shape[1])\n                try:\n                    params = np.linalg.lstsq(A.T @ A + ridge, A.T @ b, rcond=None)[0].flatten()\n                    a = params[0]\n                    b_lin = params[1:1 + n]\n                    h_diag = params[1 + n:1 + 2 * n]\n                    # regularize curvature: push small/negative to small positive\n                    h_reg = np.copy(h_diag)\n                    h_reg[h_reg < 1e-8] = 1e-8\n                    delta_model = -b_lin / (h_reg + 1e-20)\n                    # clamp per-dim by trust radius\n                    delta_limited = np.clip(delta_model, -trust_radius, trust_radius)\n                    x_model = clip_to_bounds(x_best + delta_limited)\n                    if evals < budget:\n                        out = safe_eval(x_model)\n                        work_allow -= 1\n                        if out is not None:\n                            f_model, x_model = out\n                            if f_model < f_best - 1e-12:\n                                # surrogate succeeded\n                                improved_round = True\n                                step = min(step * self.success_expand, 5.0 * domain_mean)\n                                # record mem step\n                                delta_vec = x_model - (x_best if x_best is not None else x_model)\n                                ndv = np.linalg.norm(delta_vec)\n                                if ndv > 0:\n                                    dir_unit = delta_vec / ndv\n                                    mem_steps.insert(0, dir_unit.copy())\n                                    mem_trust.insert(0, 1.0)\n                                    if len(mem_steps) > self.memory_size:\n                                        mem_steps.pop(); mem_trust.pop()\n                                # expand trust radii where direction succeeded\n                                trust_radius = np.minimum(trust_radius * self.success_expand, trust_max)\n                        # shrink trust if failed\n                        if not improved_round:\n                            trust_radius = np.maximum(trust_radius * self.failure_shrink, trust_min)\n                except Exception:\n                    # regression failed: skip surrogate\n                    pass\n\n            # If surrogate didn't improve, perform PCA-biased directional ensemble probes\n            if not improved_round and evals < budget and work_allow > 0:\n                # choose subspace dim k\n                k_base = max(2, int(min(n, np.clip(int(np.ceil(np.log1p(n) * 3)), 2, n))))\n                k = min(n, max(1, int(k_base + int(self.rng.randn() * 0.4))))\n                basis = build_subspace(k)  # n x k\n\n                # compute coefficient variances from mem_steps projection if available\n                if len(mem_steps) >= 2:\n                    M = np.vstack(mem_steps)\n                    proj = M @ basis  # (m,k)\n                    var_coef = np.var(proj, axis=0) + 1e-12\n                    avg_trust = np.mean(mem_trust) if mem_trust else 1.0\n                    var_coef = var_coef * (1.0 + 0.5 * avg_trust)\n                else:\n                    var_coef = np.ones(k, dtype=float)\n\n                probes = max(6, 3 * k)\n                candidate_list = []\n                for p in range(probes):\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    coeffs = self.rng.randn(k) * np.sqrt(var_coef)\n                    d = basis @ coeffs\n                    nd = np.linalg.norm(d)\n                    if nd == 0:\n                        continue\n                    d = d / nd\n                    # bias sampling by trust radii: preferred step scaling toward coordinates with larger trust\n                    trust_scale = np.linalg.norm(trust_radius) / (np.linalg.norm(rng_range) + 1e-20)\n                    if self.rng.rand() < 0.15:\n                        alpha = self.rng.uniform(-2.0 * step * trust_scale, 2.0 * step * trust_scale)\n                    else:\n                        alpha = self.rng.uniform(-step * trust_scale, step * trust_scale)\n                    x_try = clip_to_bounds((x_best if x_best is not None else X[-1]) + alpha * d)\n                    out = safe_eval(x_try)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    f_try, x_try = out\n                    candidate_list.append((f_try, x_try.copy(), d.copy(), alpha))\n                    if f_try < f_best - 1e-12:\n                        # parabolic refinement\n                        delta = max(abs(alpha), 0.9 * step)\n                        res = parabolic_line_min(x_best, f_best, d, delta=delta)\n                        if res is not None:\n                            f_line, x_line = res\n                            if f_line < f_try - 1e-12:\n                                f_try = f_line; x_try = x_line.copy()\n                        # accept\n                        step = min(step * 1.10, 5.0 * domain_mean)\n                        delta_vec = x_try - (x_best if x_best is not None else x_try)\n                        ndv = np.linalg.norm(delta_vec)\n                        if ndv > 0:\n                            mem_steps.insert(0, (delta_vec / ndv).copy())\n                            mem_trust.insert(0, 1.0)\n                            if len(mem_steps) > self.memory_size:\n                                mem_steps.pop(); mem_trust.pop()\n                        improved_round = True\n                        no_improve_iters = 0\n                        # update best\n                        if f_try < f_best - 1e-12:\n                            f_best = float(f_try); x_best = x_try.copy()\n                        # small local jitter polish\n                        if evals < budget and self.rng.rand() < 0.35 and work_allow > 0:\n                            jitter = self.rng.randn(n)\n                            jitter = jitter / (np.linalg.norm(jitter) + 1e-20)\n                            jitter *= 0.2 * step\n                            out_j = safe_eval(clip_to_bounds(x_best + jitter))\n                            if out_j is not None:\n                                f_j, x_j = out_j\n                                work_allow -= 1\n                                if f_j < f_best - 1e-12:\n                                    f_best = float(f_j); x_best = x_j.copy()\n                                    delta_vec = x_best - x_try\n                                    ndv = np.linalg.norm(delta_vec)\n                                    if ndv > 0:\n                                        mem_steps.insert(0, (delta_vec / ndv).copy())\n                                        mem_trust.insert(0, 1.0)\n                                        if len(mem_steps) > self.memory_size:\n                                            mem_steps.pop(); mem_trust.pop()\n                        # decay trusts a bit and boost head\n                        mem_trust = [t * 0.98 for t in mem_trust]\n                        if mem_trust:\n                            mem_trust[0] = mem_trust[0] + 0.2\n                    else:\n                        # occasional small parabola attempt even if not immediately improving\n                        if self.rng.rand() < 0.03 and (budget - evals) >= 3:\n                            res = parabolic_line_min(x_best if x_best is not None else X[-1], f_best, d, delta=0.7 * step)\n                            if res is not None:\n                                f_line, x_line = res\n                                if f_line < f_best - 1e-12:\n                                    improved_round = True\n                                    step = min(step * 1.08, 5.0 * domain_mean)\n                                    delta_vec = x_line - (x_best if x_best is not None else X[-1])\n                                    ndv = np.linalg.norm(delta_vec)\n                                    if ndv > 0:\n                                        mem_steps.insert(0, (delta_vec / ndv).copy())\n                                        mem_trust.insert(0, 1.0)\n                                        if len(mem_steps) > self.memory_size:\n                                            mem_steps.pop(); mem_trust.pop()\n                                    if f_line < f_best - 1e-12:\n                                        f_best = float(f_line); x_best = x_line.copy()\n                                mem_trust = [t * 0.98 for t in mem_trust]\n\n                # try to use candidate_list to intensify if still no improvement\n                if not improved_round and candidate_list and work_allow >= 3:\n                    candidate_list.sort(key=lambda z: z[0])\n                    topk = min(3, len(candidate_list))\n                    for i in range(topk):\n                        f_try, x_try, d_try, alpha_try = candidate_list[i]\n                        res = parabolic_line_min(x_best if x_best is not None else X[-1], f_best, d_try, delta=max(step, abs(alpha_try)))\n                        if res is not None:\n                            f_line, x_line = res\n                            if f_line < f_best - 1e-12:\n                                improved_round = True\n                                step = min(step * 1.09, 5.0 * domain_mean)\n                                delta_vec = x_line - (x_best if x_best is not None else X[-1])\n                                ndv = np.linalg.norm(delta_vec)\n                                if ndv > 0:\n                                    mem_steps.insert(0, (delta_vec / ndv).copy())\n                                    mem_trust.insert(0, 1.0)\n                                    if len(mem_steps) > self.memory_size:\n                                        mem_steps.pop(); mem_trust.pop()\n                                f_best = float(f_line); x_best = x_line.copy()\n                                break\n\n            # coordinate pattern tweaks when ensemble is quiet (cheap)\n            if not improved_round and evals < budget and work_allow > 0:\n                coords = np.arange(n)\n                self.rng.shuffle(coords)\n                max_coords = min(n, max(2, int(work_allow // 3)))\n                base_step = np.linalg.norm(trust_radius) / (np.sqrt(float(n)) + 1e-20)\n                for i in coords[:max_coords]:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    for sign in (+1.0, -1.0):\n                        x_try = clip_to_bounds((x_best if x_best is not None else X[-1]).copy())\n                        delta = sign * base_step / np.sqrt(n)\n                        x_try[i] += delta\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            improved_round = True\n                            step = min(step * 1.12, 5.0 * domain_mean)\n                            delta_vec = x_try - (x_best if x_best is not None else X[-1])\n                            ndv = np.linalg.norm(delta_vec)\n                            if ndv > 0:\n                                mem_steps.insert(0, (delta_vec / ndv).copy())\n                                mem_trust.insert(0, 1.0)\n                                if len(mem_steps) > self.memory_size:\n                                    mem_steps.pop(); mem_trust.pop()\n                            f_best = float(f_try); x_best = x_try.copy()\n                            break\n\n            # Occasional heavy-tailed jump to escape\n            if evals < budget and self.rng.rand() < self.cauchy_prob and not improved_round:\n                scale = 0.4 * np.maximum(rng_range, 1e-9)\n                jump = self.rng.standard_cauchy(size=n)\n                jump = np.clip(jump, -8, 8)\n                x_jump = clip_to_bounds((x_best if x_best is not None else X[-1]) + jump * scale)\n                out = safe_eval(x_jump)\n                if out is not None:\n                    f_jump, x_jump = out\n                    if f_jump < f_best - 1e-12:\n                        improved_round = True\n                        step = min(step * 1.25, 5.0 * domain_mean)\n                        delta_vec = x_jump - (x_best if x_best is not None else X[-1])\n                        ndv = np.linalg.norm(delta_vec)\n                        if ndv > 0:\n                            mem_steps.insert(0, (delta_vec / ndv).copy())\n                            mem_trust.insert(0, 1.0)\n                            if len(mem_steps) > self.memory_size:\n                                mem_steps.pop(); mem_trust.pop()\n                        f_best = float(f_jump); x_best = x_jump.copy()\n                    else:\n                        trust_radius = np.maximum(trust_radius * 0.8, trust_min)\n\n            # Adaptation after the round\n            if improved_round:\n                # small growth on success\n                step = min(step * 1.10, 5.0 * domain_mean)\n                mem_trust = [min(t * 1.05, 10.0) for t in mem_trust]\n            else:\n                no_improve_iters += 1\n                step = max(step * 0.86, min_step)\n                mem_trust = [t * 0.95 for t in mem_trust]\n                # shrink trust radii moderately\n                trust_radius = np.maximum(trust_radius * self.failure_shrink, trust_min)\n\n            # prune very small trust memory entries\n            if mem_trust:\n                keep_idx = [i for i, t in enumerate(mem_trust) if t > 1e-3]\n                if len(keep_idx) != len(mem_trust):\n                    mem_steps = [mem_steps[i] for i in keep_idx]\n                    mem_trust = [mem_trust[i] for i in keep_idx]\n\n            # stagnation handling: randomized restart around best\n            if no_improve_iters >= stagnation_limit:\n                restarts += 1\n                no_improve_iters = 0\n                mem_steps = []\n                mem_trust = []\n                if restarts > max_restarts:\n                    # final polishing: many tiny probes until budget exhausted\n                    while evals < budget:\n                        d = self.rng.randn(n)\n                        d /= (np.linalg.norm(d) + 1e-20)\n                        alpha = self.rng.uniform(-0.2 * step, 0.2 * step)\n                        out = safe_eval(clip_to_bounds((x_best if x_best is not None else X[-1]) + alpha * d))\n                        if out is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            x_best = x_try.copy(); f_best = float(f_try)\n                        step *= 0.95\n                        if step < min_step:\n                            break\n                    break\n                else:\n                    # restart around best with moderate perturbation\n                    if x_best is not None:\n                        radius = min(1.0, 0.5 * (1.0 + restarts / 2.0)) * domain_mean\n                        perturb = self.rng.randn(n) * radius\n                        x_new = clip_to_bounds(x_best + perturb)\n                        out = safe_eval(x_new)\n                        if out is None:\n                            break\n                        f_new, x_new = out\n                        step = min(2.0 * step + 0.2 * domain_mean, 5.0 * domain_mean)\n                        # slightly reset trust radii\n                        trust_radius = np.minimum(trust_radius * 1.2, trust_max)\n                    else:\n                        # random restart\n                        x_new = self.rng.uniform(lb, ub)\n                        out = safe_eval(x_new)\n                        if out is None:\n                            break\n                        f_new, x_new = out\n                        step = min(2.0 * step + 0.2 * domain_mean, 5.0 * domain_mean)\n\n            # archive pruning occasionally\n            if len(X) > max(2000, 100 * n):\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:300]\n                rest_idx = idx_sorted[300:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max(2000, 100 * n) - 300))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # quick termination if very good\n            if f_best <= 1e-12:\n                break\n\n        if x_best is None:\n            # fallback\n            x_best = X[-1].copy()\n            f_best = float(F[-1])\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm SurrogateAdaptiveDirectionalEnsemble scored 0.288 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "4c818849-323e-4065-9772-bd26cb830a8b", "operator": null, "metadata": {"aucs": [0.09838665523394263, 0.14010406737118586, 0.4065249285691288, 0.5926185018977295, 0.22271817181485498, 0.44536452086190514, 0.24159389797261688, 0.3779608940216086, 0.21010521657469539, 0.14197984413257192]}, "task_prompt": ""}
{"id": "47cecb71-7a8f-466c-8ec5-bd2c57bc0ff0", "fitness": "-inf", "name": "AdaptiveSubspaceTrustLevy", "description": "The algorithm learns a compact low-dimensional search subspace from recent successful steps or nearby archive points (PCA on a history capped at 40, subspace dim ≤ 6) and fits a full symmetric quadratic surrogate in that subspace by weighted ridge regression to propose trust-region steps. It combines surrogate-driven optimization (solve -H^{-1}b in subspace) with multiscale subspace probes, full-space Gaussian/coordinate stochastic moves, cheap finite-difference directional moves, and occasional heavy-tailed Cauchy (Lévy-like) jumps (cauchy_prob=0.1, scale_frac≈0.45) to escape basins. Adaptation is done with a scalar trust radius (init ≈0.35·domain, expand 1.5 on success, shrink 0.6 on failure), a Metropolis-like soft acceptance using an annealed temperature (temp_init=1e-2, temp_cool=0.997), and stagnation-triggered Lévy restarts that boost exploration. Practical safeguards ensure strict budget/bound respecting via safe_eval, archive maintenance/pruning (max_archive ≈ max(2000,50·n)), limited initial sampling (init_ratio≈0.08, capped), and conservative regularization/ridge to keep surrogate fits stable.", "code": "import numpy as np\n\nclass AdaptiveSubspaceTrustLevy:\n    \"\"\"\n    Adaptive Subspace Trust-Region with Lévy Restarts (ASTL-R)\n\n    One-line idea:\n      Learn a compact subspace of promising step-directions from recent successful moves,\n      fit a quadratic surrogate inside that subspace (full symmetric quadratic), optimize\n      the surrogate inside an adaptive trust region, accept moves with a soft (Metropolis-like)\n      rule to enable escapes, and occasionally perform Lévy (Cauchy) restarts when stagnation occurs.\n\n    Notes:\n      - Designed for continuous bounded black-box optimization (works with func.bounds.lb/ub).\n      - Respects the evaluation budget strictly.\n      - Uses only numpy; no external dependencies.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_ratio=0.08, min_init=None, max_init=400,\n                 max_history=40, max_subspace_dim=6,\n                 success_expand=1.5, failure_shrink=0.6,\n                 cauchy_prob=0.10, cauchy_scale_frac=0.45,\n                 temp_init=1e-2, temp_cool=0.997,\n                 max_eval_per_iter=30):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n\n        self.init_ratio = init_ratio\n        self.min_init = max(10, 2 * self.dim) if min_init is None else int(min_init)\n        self.max_init = int(max_init)\n\n        self.max_history = int(max_history)\n        self.max_subspace_dim = int(max_subspace_dim)\n\n        self.success_expand = float(success_expand)\n        self.failure_shrink = float(failure_shrink)\n\n        self.cauchy_prob = float(cauchy_prob)\n        self.cauchy_scale_frac = float(cauchy_scale_frac)\n\n        self.temp = float(temp_init)\n        self.temp_cool = float(temp_cool)\n\n        self.max_eval_per_iter = int(max_eval_per_iter)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # Read bounds; the problem statement expects [-5,5] but use func bounds if available\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # enforce numerical shapes\n        rng_range = (ub - lb)\n        rng_mean = 0.5 * (ub + lb)\n        scale_mean = np.mean(rng_range)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # Archive of evaluated points\n        X = []\n        F = []\n\n        # best global\n        f_best = np.inf\n        x_best = None\n\n        # Initial space-filling sampling (uniform random)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            if evals >= budget:\n                break\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n\n        # If archive is empty (shouldn't be), initialize a center randomly\n        if len(X) == 0:\n            x0 = self.rng.uniform(lb, ub)\n            f0 = func(x0); evals += 1\n            X.append(x0.copy()); F.append(float(f0))\n            f_best = float(f0); x_best = x0.copy()\n\n        # start from best found\n        idx0 = int(np.argmin(F))\n        center = np.array(X[idx0]).copy()\n        f_center = float(F[idx0])\n\n        # trust radius (scalar, measured in Euclidean norm)\n        trust_radius = 0.35 * np.linalg.norm(rng_range)  # initial trust (global fraction)\n        trust_min = 1e-6 * np.linalg.norm(rng_range)\n        trust_max = 2.0 * np.linalg.norm(rng_range)\n\n        # keep recent successful steps to build subspace via PCA/SVD\n        history = []  # list of step vectors (x_new - x_old)\n\n        # stagnation counter for restarts\n        stagnation = 0\n        stagnation_restart = max(8, 6 + n // 2)\n\n        # safe evaluator: obey budget and bounds, update archive and best\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # helper: pick nearest neighbors to center\n        def nearest_neighbors(center, m):\n            if len(X) == 0:\n                return np.empty((0, n)), np.empty((0,))\n            X_arr = np.asarray(X)\n            F_arr = np.asarray(F)\n            dists = np.linalg.norm(X_arr - center, axis=1)\n            idx_sorted = np.argsort(dists)\n            m_use = min(m, len(X_arr))\n            idx = idx_sorted[:m_use]\n            return X_arr[idx], F_arr[idx]\n\n        # helper: fit quadratic surrogate in a k-dimensional subspace\n        # model: y(s) = a + b^T s + 0.5 s^T H s, where s are coordinates in subspace\n        def fit_quad_in_subspace(center, U_k, neigh_X, neigh_F, ridge=1e-8):\n            # U_k: (n,k) orthonormal basis columns; project dx = x - center to s coords\n            if neigh_X.shape[0] < (1 + U_k.shape[1] + U_k.shape[1] * (U_k.shape[1] + 1) // 2):\n                return None\n            dx = neigh_X - center\n            S = dx.dot(U_k)  # shape m x k\n            y = neigh_F\n            m, k = S.shape\n\n            # Build design matrix: [1, s1..sk, s1*s1, s1*s2, ..., s_k*s_k] unique pairs (j<=l)\n            P = 1 + k + (k * (k + 1)) // 2\n            M = np.empty((m, P), dtype=float)\n            M[:, 0] = 1.0\n            M[:, 1:1 + k] = S\n            col = 1 + k\n            for j in range(k):\n                for l in range(j, k):\n                    M[:, col] = S[:, j] * S[:, l]\n                    col += 1\n\n            # Weighted least squares: weight by distance (closer neighbors heavier)\n            dists = np.linalg.norm(dx, axis=1) + 1e-12\n            w = 1.0 / (dists)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            bvec = W.flatten() * y\n\n            # ridge regression\n            try:\n                # Solve normal equations with ridge\n                ATA = A.T.dot(A)\n                ATb = A.T.dot(bvec)\n                ATA += ridge * np.eye(ATA.shape[0])\n                params = np.linalg.solve(ATA, ATb)\n            except Exception:\n                try:\n                    params, *_ = np.linalg.lstsq(A, bvec, rcond=None)\n                except Exception:\n                    return None\n\n            params = np.array(params).flatten()\n            a = params[0]\n            b = params[1:1 + k].copy()\n            quad_params = params[1 + k:].copy()  # length k*(k+1)/2\n            # reconstruct symmetric H in subspace\n            H = np.zeros((k, k), dtype=float)\n            idx = 0\n            for j in range(k):\n                for l in range(j, k):\n                    c_jl = quad_params[idx]\n                    if j == l:\n                        # recall our quadratic features are s_j*s_j with coefficient c_jj = 0.5 * H_jj\n                        H[j, j] = 2.0 * c_jl\n                    else:\n                        H[j, l] = c_jl\n                        H[l, j] = c_jl\n                    idx += 1\n\n            # ensure numeric symmetry\n            H = 0.5 * (H + H.T)\n            return a, b, H, U_k\n\n        # helper: compute PCA basis from history or neighbors; returns U (n,k)\n        def compute_subspace(center, neigh_X, max_k):\n            # prefer using recent history if enough, else neighbors' dx\n            mat = None\n            if len(history) >= max(3, min(max_k, len(history))):\n                mat = np.vstack(history[-(self.max_history):])  # shape h x n\n            elif neigh_X.shape[0] > 3:\n                mat = (neigh_X - center)\n            else:\n                return None  # not enough info\n\n            # compute SVD on matrix of steps (rows are samples)\n            try:\n                # center columns (zero mean)\n                mat = np.asarray(mat, dtype=float)\n                if mat.shape[0] < 2:\n                    return None\n                M = mat.copy()\n                M -= M.mean(axis=0, keepdims=True)\n                U_s, S_s, Vt = np.linalg.svd(M, full_matrices=False)\n                V = Vt.T  # principal directions columns\n                # pick top-k directions with nonzero singular values\n                k = min(max_k, V.shape[1])\n                # ensure k >= 1\n                if k <= 0:\n                    return None\n                U_k = V[:, :k]  # shape n x k\n                # normalize (should already be orthonormal)\n                for j in range(U_k.shape[1]):\n                    normj = np.linalg.norm(U_k[:, j])\n                    if normj > 0:\n                        U_k[:, j] /= normj\n                return U_k\n            except Exception:\n                return None\n\n        # main loop\n        while evals < budget:\n            # limit number of proposals per \"iteration\" to avoid wasting compute on one center\n            work_allow = min(self.max_eval_per_iter, budget - evals)\n            if work_allow <= 0:\n                break\n\n            improved_this_iter = False\n\n            # choose neighbors around center\n            m_neigh = min(len(X), max(4, 6 * n))\n            neigh_X, neigh_F = nearest_neighbors(center, m_neigh)\n\n            # choose a subspace to build surrogate\n            U_k = compute_subspace(center, neigh_X, max_k=min(self.max_subspace_dim, n))\n            quad_model = None\n            if U_k is not None and neigh_X.shape[0] >= (1 + U_k.shape[1] + U_k.shape[1] * (U_k.shape[1] + 1) // 2):\n                quad_model = fit_quad_in_subspace(center, U_k, neigh_X, neigh_F, ridge=1e-6 * (scale_mean + 1e-12))\n\n            # Strategy A: Optimize subspace quadratic surrogate if available\n            if quad_model is not None and work_allow > 0:\n                a_q, b_q, H_q, U_used = quad_model\n                k = U_used.shape[1]\n                # regularize H to be invertible\n                eps = 1e-8 * (np.trace(H_q) / max(1.0, k) + 1e-12)\n                H_reg = H_q + max(eps, 1e-8) * np.eye(k)\n                # attempt solve for s* = -H^{-1} b\n                try:\n                    s_star = -np.linalg.solve(H_reg, b_q)\n                except Exception:\n                    # fallback to truncated negative gradient direction\n                    s_star = -b_q\n                # clip s_star to trust radius in subspace (norm in full space)\n                x_candidate = center + U_used.dot(s_star)\n                step_norm = np.linalg.norm(x_candidate - center)\n                if step_norm > trust_radius:\n                    s_star = s_star * (trust_radius / (np.linalg.norm(U_used.dot(s_star)) + 1e-12))\n                    x_candidate = center + U_used.dot(s_star)\n\n                # ensure within bounds\n                x_candidate = np.clip(x_candidate, lb, ub)\n\n                # evaluate candidate\n                out = safe_eval(x_candidate)\n                work_allow -= 1\n                if out is not None:\n                    f_cand, x_cand = out\n                    # acceptance criterion: always accept if better, otherwise with annealed probability\n                    delta = f_cand - f_center\n                    accept = False\n                    if f_cand < f_center - 1e-12:\n                        accept = True\n                    else:\n                        # probabilistic uphill acceptance (Metropolis-like)\n                        p_accept = np.exp(-max(0.0, delta) / (self.temp + 1e-12))\n                        if self.rng.rand() < p_accept:\n                            accept = True\n                    if accept:\n                        # record step into history\n                        step = x_cand - center\n                        if np.linalg.norm(step) > 0:\n                            history.append(step / (np.linalg.norm(step) + 1e-12))\n                            if len(history) > self.max_history:\n                                history.pop(0)\n                        # update center and fitness\n                        center = x_cand.copy()\n                        f_center = f_cand\n                        stagnation = 0\n                        improved_this_iter = True\n                        # enlarge trust region moderately\n                        trust_radius = min(trust_radius * self.success_expand, trust_max)\n                        # cool temperature slightly on success\n                        self.temp *= 0.995\n                    else:\n                        # shrink trust on rejection\n                        trust_radius = max(trust_radius * self.failure_shrink, trust_min)\n                        stagnation += 1\n                else:\n                    # budget exhausted\n                    break\n\n            # Strategy B: if no surrogate or not accepted, try directional and multiscale probing in learned subspace and full space\n            while work_allow > 0 and (not improved_this_iter) and evals < budget:\n                # pick one of exploratory modes: subspace-directed multiscale, coordinate, or cauchy\n                mode_prob = self.rng.rand()\n                if mode_prob < 0.55 and U_k is not None:\n                    # subspace multiscale along principal axes and random linear combos\n                    k = U_k.shape[1]\n                    choices = []\n                    # principal axis steps\n                    for j in range(k):\n                        v = U_k[:, j]\n                        for scale in (0.3, 0.6, 1.0):\n                            step = v * (scale * trust_radius)\n                            choices.append(center + step)\n                            choices.append(center - step)\n                    # random linear combos within subspace\n                    for _ in range(3):\n                        coeffs = self.rng.randn(k)\n                        coeffs /= (np.linalg.norm(coeffs) + 1e-12)\n                        scale = self.rng.choice([0.3, 0.7, 1.2])\n                        step = U_k.dot(coeffs * scale * trust_radius)\n                        choices.append(center + step)\n                    self.rng.shuffle(choices)\n                    # evaluate up to a few\n                    for x_try in choices[:min(6, work_allow)]:\n                        if evals >= budget:\n                            break\n                        x_try = np.clip(x_try, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_center - 1e-12:\n                            # accept improvement\n                            step = xtry - center\n                            if np.linalg.norm(step) > 0:\n                                history.append(step / (np.linalg.norm(step) + 1e-12))\n                                if len(history) > self.max_history:\n                                    history.pop(0)\n                            center = xtry.copy()\n                            f_center = ftry\n                            trust_radius = min(trust_radius * self.success_expand, trust_max)\n                            stagnation = 0\n                            improved_this_iter = True\n                            break\n                        else:\n                            # soft acceptance rarely\n                            if self.rng.rand() < np.exp(-(ftry - f_center) / max(self.temp, 1e-12)):\n                                step = xtry - center\n                                if np.linalg.norm(step) > 0:\n                                    history.append(step / (np.linalg.norm(step) + 1e-12))\n                                    if len(history) > self.max_history:\n                                        history.pop(0)\n                                center = xtry.copy()\n                                f_center = ftry\n                                trust_radius = max(trust_radius * 0.9, trust_min)\n                                stagnation = 0\n                                improved_this_iter = True\n                                break\n                            else:\n                                trust_radius = max(trust_radius * 0.95, trust_min)\n                                stagnation += 1\n                    # done exploring subspace\n                elif mode_prob < 0.85:\n                    # full-space coordinate and stochastic probes (small gaussian)\n                    choices = []\n                    # gaussian perturbation scaled by trust\n                    for _ in range(6):\n                        step = self.rng.randn(n)\n                        step = step / (np.linalg.norm(step) + 1e-12) * (self.rng.rand() ** 0.8) * trust_radius\n                        choices.append(center + step)\n                    # coordinate tweaks\n                    coord_step = (self.rng.randn(n) * 0.5 + 0.5) * (trust_radius / max(1.0, np.sqrt(n)))\n                    for i in range(min(n, 6)):\n                        e = np.zeros(n)\n                        idx = self.rng.randint(0, n)\n                        e[idx] = coord_step[idx]\n                        choices.append(center + e)\n                        choices.append(center - e)\n                    self.rng.shuffle(choices)\n                    for x_try in choices[:min(6, work_allow)]:\n                        if evals >= budget:\n                            break\n                        x_try = np.clip(x_try, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_center - 1e-12:\n                            step = xtry - center\n                            if np.linalg.norm(step) > 0:\n                                history.append(step / (np.linalg.norm(step) + 1e-12))\n                                if len(history) > self.max_history:\n                                    history.pop(0)\n                            center = xtry.copy()\n                            f_center = ftry\n                            trust_radius = min(trust_radius * self.success_expand, trust_max)\n                            stagnation = 0\n                            improved_this_iter = True\n                            break\n                        else:\n                            trust_radius = max(trust_radius * self.failure_shrink, trust_min)\n                            stagnation += 1\n                else:\n                    # occasional heavy-tailed (Cauchy) jump to escape basin\n                    if self.rng.rand() < self.cauchy_prob:\n                        scale = self.cauchy_scale_frac * rng_range\n                        jump = self.rng.standard_cauchy(size=n)\n                        # Clip extreme tails\n                        jump = np.clip(jump, -10.0, 10.0)\n                        x_jump = np.clip(center + jump * scale, lb, ub)\n                        out = safe_eval(x_jump)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        fjump, xjump = out\n                        if fjump < f_center - 1e-12:\n                            step = xjump - center\n                            if np.linalg.norm(step) > 0:\n                                history.append(step / (np.linalg.norm(step) + 1e-12))\n                                if len(history) > self.max_history:\n                                    history.pop(0)\n                            center = xjump.copy()\n                            f_center = fjump\n                            trust_radius = min(trust_radius * 1.8, trust_max)\n                            stagnation = 0\n                            improved_this_iter = True\n                            break\n                        else:\n                            trust_radius = max(trust_radius * self.failure_shrink, trust_min)\n                            stagnation += 1\n                    else:\n                        # deterministic small step along negative estimated gradient by finite differences (cheap directional)\n                        # Estimate directional gradient using two probes\n                        dir_v = self.rng.randn(n)\n                        dir_v /= (np.linalg.norm(dir_v) + 1e-12)\n                        step = dir_v * (trust_radius * 0.6)\n                        xp = np.clip(center + step, lb, ub)\n                        xm = np.clip(center - step, lb, ub)\n                        outp = safe_eval(xp)\n                        work_allow -= 1\n                        if outp is None:\n                            break\n                        fp, xp = outp\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        outm = safe_eval(xm)\n                        work_allow -= 1\n                        if outm is None:\n                            break\n                        fm, xm = outm\n                        # finite-difference slope along dir_v\n                        slope = (fp - fm) / (2.0 * (np.linalg.norm(step) + 1e-12))\n                        # if slope positive, move towards negative direction\n                        if slope > 0:\n                            x_try = np.clip(center - dir_v * trust_radius, lb, ub)\n                        else:\n                            x_try = np.clip(center + dir_v * trust_radius, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_center - 1e-12:\n                            step = xtry - center\n                            if np.linalg.norm(step) > 0:\n                                history.append(step / (np.linalg.norm(step) + 1e-12))\n                                if len(history) > self.max_history:\n                                    history.pop(0)\n                            center = xtry.copy()\n                            f_center = ftry\n                            trust_radius = min(trust_radius * self.success_expand, trust_max)\n                            stagnation = 0\n                            improved_this_iter = True\n                            break\n                        else:\n                            trust_radius = max(trust_radius * self.failure_shrink, trust_min)\n                            stagnation += 1\n\n            # end inner work_allow loop\n\n            # global archive maintenance: prune if too large\n            max_archive = max(2000, 50 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # update global best from center\n            if f_center < f_best - 1e-12:\n                f_best = float(f_center)\n                x_best = center.copy()\n\n            # occasional center refresh: move center to a top archive point (diversification)\n            if self.rng.rand() < 0.06 and len(X) > 10:\n                # pick among top 5% of archive with some randomness\n                kpick = max(1, int(0.05 * len(X)))\n                idx_sorted = np.argsort(F)\n                candidates = idx_sorted[:kpick]\n                choice = int(self.rng.choice(candidates))\n                new_center = np.array(X[choice])\n                if np.linalg.norm(new_center - center) > 1e-6:\n                    center = new_center.copy()\n                    f_center = float(F[choice])\n                    trust_radius = max(0.5 * trust_radius, trust_min)\n                    history = []  # reset local history\n                    stagnation = 0\n\n            # stagnation check -> Lévy restart\n            if stagnation >= stagnation_restart:\n                # pick a distant archive point among good ones or random sample\n                if len(X) > 5:\n                    X_arr = np.asarray(X)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    # choose from top quality but far: sort by F + lambda * (-dist)\n                    lam = 0.05 * np.mean(rng_range)\n                    scores = np.asarray(F) - lam * dists\n                    idx_sorted = np.argsort(scores)\n                    pick_idx = int(self.rng.choice(idx_sorted[:max(1, min(len(idx_sorted), 20))]))\n                    center = X_arr[pick_idx].copy()\n                    f_center = float(F[pick_idx])\n                else:\n                    center = self.rng.uniform(lb, ub)\n                    out = safe_eval(center)\n                    if out is not None:\n                        f_center, center = out\n                # perform aggressive Cauchy jump around new center\n                if evals < budget and self.rng.rand() < 0.8:\n                    scale = self.cauchy_scale_frac * rng_range\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -10, 10)\n                    xj = np.clip(center + jump * scale, lb, ub)\n                    out = safe_eval(xj)\n                    if out is not None:\n                        fj, xj = out\n                        if fj < f_center - 1e-12:\n                            center = xj.copy()\n                            f_center = fj\n                # boost trust region and temperature moderately\n                trust_radius = min(max(1.5 * trust_radius, 0.5 * np.linalg.norm(rng_range)), trust_max)\n                self.temp = max(self.temp * 2.0, 1e-6)\n                history = []\n                stagnation = 0\n\n            # cool global temperature slowly\n            self.temp *= self.temp_cool\n\n            # safety: if center got worse than best overall, move center to best archive occasionally\n            if self.rng.rand() < 0.03 and x_best is not None:\n                center = np.array(x_best).copy()\n                # update f_center to actual archived value\n                f_center = float(f_best)\n\n        # end main loop\n\n        # If x_best remains None (unlikely), pick best from archive\n        if x_best is None and len(X) > 0:\n            idx = int(np.argmin(F))\n            x_best = np.array(X[idx]).copy()\n            f_best = float(F[idx])\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "a9759013-c0d4-4e3f-816c-6216ddcbcef5", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "b0c95ed0-582f-48b8-ac8c-f55b18df2044", "fitness": "-inf", "name": "AMDE_PDLR", "description": "The optimizer keeps a modest ensemble of centers (ensemble_size tied to dim) seeded by a clipped space‑filling initialization (init_ratio, min/max) and an archive of evaluated points for reseeding and pruning.  \nAround each center it fits a distance‑weighted linear surrogate (neighbor_multiplier, small ridge) to get a gradient estimate, then takes per‑dimension scaled steps controlled by trust radii (trust_init_frac, trust_min/max) and adaptive per‑dimension learning rates (lr_init, lr_up, lr_down) that expand on successes (success_expand) and shrink on failures (failure_shrink).  \nWhen the surrogate is unreliable it uses multi‑scale orthogonal directional probes and coordinate‑biased perturbations (direction_scales, lr weighting), plus occasional heavy‑tailed Student‑t jumps (student_t_prob, student_t_df, student_t_scale_frac) to escape basins.  \nRobustness and diversity are enforced via stagnation counters and center replacement (center_replace_patience), periodic archive pruning/resampling (max_archive logic), conservative numeric regularizers (ridge, small eps), and caps on per‑iteration evaluations (max_eval_per_iter).", "code": "import numpy as np\n\nclass AMDE_PDLR:\n    \"\"\"\n    Adaptive Multi-scale Directional Ensemble with Per-dimension Learning Rates (AMDE-PDLR)\n\n    Main idea (one-line): Maintain a modest ensemble of centers, fit\n    distance-weighted local linear surrogates to estimate descent directions,\n    adapt per-dimension learning rates and trust radii multiplicatively on\n    successes/failures, explore with multi-scale orthogonal directional probes,\n    and occasionally perform heavy-tailed Student-t jumps for basin escape.\n\n    Main parameters (tunable):\n      - init_ratio: fraction of budget used for initial space-filling samples\n      - ensemble_size: number of active centers\n      - neighbor_multiplier: neighbors = neighbor_multiplier * dim used to fit local surrogate\n      - ridge: Tikhonov regularization for weighted linear fit\n      - trust_init_frac: initial trust radius as fraction of (ub-lb)\n      - trust_min_frac, trust_max_frac: bounds on trust radius (fractions of range)\n      - success_expand, failure_shrink: multiplicative trust radius updates on success/failure\n      - lr_init: initial per-dimension learning rates (multiplicative factors)\n      - lr_up, lr_down: factors to adjust per-dimension rates on coordinate successes/failures\n      - direction_scales: multi-scale probe multipliers\n      - student_t_prob, student_t_df, student_t_scale_frac: heavy-tail jump policy\n      - center_replace_patience: iterations without improvement to replace a center\n      - max_eval_per_iter: cap of function evaluations per outer iteration\n      - max_archive: cap for pruning stored archive\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 # tunable parameters (different philosophy and values than the provided algorithm)\n                 init_ratio=0.08,\n                 ensemble_size=None,\n                 neighbor_multiplier=6,\n                 ridge=1e-4,\n                 trust_init_frac=0.7,\n                 trust_min_frac=1e-6,\n                 trust_max_frac=1.5,\n                 success_expand=1.3,\n                 failure_shrink=0.5,\n                 lr_init=0.6,\n                 lr_up=1.15,\n                 lr_down=0.85,\n                 direction_scales=np.array([0.2, 0.5, 1.0, 2.5]),\n                 student_t_prob=0.10,\n                 student_t_df=3.0,\n                 student_t_scale_frac=0.5,\n                 center_replace_patience=10,\n                 max_eval_per_iter=80,\n                 max_archive=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # Initialization sizing\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(8, 2 * self.dim)\n        self.max_init = min(500, int(0.35 * self.budget))\n\n        # ensemble sizing\n        if ensemble_size is None:\n            # slightly larger ensemble than single-digit but tied to dim\n            self.ensemble_size = max(2, min(8, 1 + self.dim // 2))\n        else:\n            self.ensemble_size = int(max(1, ensemble_size))\n\n        # modeling\n        self.neighbor_multiplier = int(max(3, neighbor_multiplier))\n        self.ridge = float(ridge)\n\n        # trust region behavior\n        self.trust_init_frac = float(trust_init_frac)\n        self.trust_min_frac = float(trust_min_frac)\n        self.trust_max_frac = float(trust_max_frac)\n        self.success_expand = float(success_expand)\n        self.failure_shrink = float(failure_shrink)\n\n        # per-dimension learning rates (multiplicative factors controlling coordinate sensitivity)\n        self.lr_init = float(lr_init)\n        self.lr_up = float(lr_up)\n        self.lr_down = float(lr_down)\n\n        # probing & heavy-tail\n        self.direction_scales = np.asarray(direction_scales, dtype=float)\n        self.student_t_prob = float(student_t_prob)\n        self.student_t_df = float(student_t_df)\n        self.student_t_scale_frac = float(student_t_scale_frac)\n\n        # center management\n        self.center_replace_patience = int(center_replace_patience)\n        self.max_eval_per_iter = int(max_eval_per_iter)\n\n        # archive cap\n        self.max_archive = max_archive  # if None will be set relative to dim later\n\n    def __call__(self, func):\n        n = self.dim\n        # unify bounds to arrays\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archive\n        X = []\n        F = []\n\n        # best\n        f_best = np.inf\n        x_best = None\n\n        # initial space-filling sampling (uniform jittered)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # pick initial centers: top distinct points\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if all(np.linalg.norm(x - c) > 1e-10 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.ensemble_size, len(X)))\n        if len(centers) < 1:\n            centers = [np.array(self.rng.uniform(lb, ub))]\n\n        # per-center trust radii (vector per-dim) and per-dim learning rates\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n        per_dim_lr = [np.full(n, self.lr_init) for _ in centers]\n        stagnation = [0 for _ in centers]\n\n        # safe evaluator\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # local weighted linear fit (distance-weighted) returning intercept and gradient estimate\n        def fit_weighted_linear(center, neighbors_X, neighbors_F):\n            # design = [1, dx] with weight = 1 / (dist + small)^p\n            dx = neighbors_X - center\n            m = dx.shape[0]\n            if m < 2:\n                return None\n            # build design\n            A = np.hstack([np.ones((m, 1)), dx])\n            y = neighbors_F\n            dists = np.linalg.norm(dx, axis=1)\n            # weight exponent slightly larger to emphasize near points\n            w = 1.0 / (dists + 1e-8) ** 1.2\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            Aw = W * A\n            yw = W * y\n            try:\n                ridge = self.ridge * np.eye(A.shape[1])\n                params, *_ = np.linalg.lstsq(Aw.T @ Aw + ridge, Aw.T @ yw, rcond=None)\n                params = params.flatten()\n                a0 = params[0]\n                grad = params[1:].flatten()\n                return a0, grad\n            except Exception:\n                return None\n\n        # archive pruning helper\n        def prune_archive():\n            nonlocal X, F\n            max_arc = self.max_archive if self.max_archive is not None else max(3000, 100 * n)\n            if len(X) > max_arc:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:300]\n                rest_idx = idx_sorted[300:]\n                if len(rest_idx) > 0:\n                    # sample fairly from rest to keep representation\n                    keep_rest = rest_idx[::max(1, len(rest_idx) // (max_arc - 300))]\n                    keep_idx = np.concatenate([keep_best, keep_rest])\n                else:\n                    keep_idx = keep_best\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n        # main loop\n        iter_count = 0\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            # refresh centers occasionally by injecting top archive points\n            if (iter_count % 5) == 0 and len(X) > 0:\n                topk = min(len(centers), max(1, len(X) // 8))\n                bests = get_top_centers(topk)\n                # overlay bests into first slots to keep promising leads\n                for i, b in enumerate(bests):\n                    centers[i] = b.copy()\n                    trust_radius[i] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                    per_dim_lr[i] = np.full(n, self.lr_init)\n                    stagnation[i] = 0\n\n            improved_global = False\n\n            # iterate centers in shuffled order\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n            for ci in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[ci])\n                tr = np.array(trust_radius[ci])\n                lr = per_dim_lr[ci]\n\n                # gather neighbors for local model\n                X_arr = np.asarray(X)\n                F_arr = np.asarray(F)\n                if len(X) >= max(2 * n + 1, self.neighbor_multiplier * n):\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), max(2 * n + 1, self.neighbor_multiplier * n))\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n\n                    # fit weighted linear surrogate\n                    lin = fit_weighted_linear(center, X_nei, F_nei)\n\n                    proposals = []\n\n                    # propose from linear surrogate if available: gradient descent with per-dim lr scaling\n                    if lin is not None:\n                        _, grad = lin\n                        # normalized gradient direction with per-dim scaling\n                        # compute step per-dim: -alpha * lr * (grad / (|grad|+eps)) * tr\n                        eps = 1e-12\n                        gsign = np.sign(grad)\n                        gmag = np.abs(grad) + eps\n                        # choose alpha conservative\n                        alpha = 0.9\n                        step = -alpha * lr * (gsign / gmag) * tr\n                        # clip to tr elementwise (some dimensions may want smaller moves)\n                        step = np.clip(step, -tr, tr)\n                        x_lin = np.clip(center + step, lb, ub)\n                        proposals.append(('lin', x_lin, step))\n\n                        # also propose a scaled variant (half and double step) to get multi-scale direction from surrogate\n                        x_lin_half = np.clip(center + 0.5 * step, lb, ub)\n                        proposals.append(('lin_half', x_lin_half, 0.5 * step))\n                        x_lin_double = np.clip(center + 1.8 * step, lb, ub)\n                        proposals.append(('lin_double', x_lin_double, 1.8 * step))\n\n                    # combine with small randomized mixes and coordinate-biased proposals\n                    # coordinate biased: pick dims with largest lr to move\n                    top_dims = np.argsort(-lr)[:max(1, min(n, 3))]\n                    coord_step = center.copy()\n                    for d in top_dims:\n                        # try +/- in each top dim scaled by its lr and trust\n                        delta = np.zeros(n)\n                        delta[d] = lr[d] * tr[d]\n                        proposals.append((f'coord+{d}', np.clip(center + delta, lb, ub), delta))\n                        proposals.append((f'coord-{d}', np.clip(center - delta, lb, ub), -delta))\n\n                    # unique proposals by value\n                    unique = []\n                    unique_steps = []\n                    for name, xprop, step in proposals:\n                        if not any(np.allclose(xprop, u, atol=1e-12) for u in unique):\n                            unique.append(xprop)\n                            unique_steps.append(step)\n\n                    # rank proposals by expected conservativeness (closer first)\n                    unique_idx = list(range(len(unique)))\n                    unique_idx.sort(key=lambda i: np.linalg.norm(unique[i] - center))\n                    for i in unique_idx:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        xprop = unique[i]\n                        out = safe_eval(xprop)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        fprop, xprop = out\n                        if fprop < f_best - 1e-12:\n                            improved_global = True\n                            # update center, trust, per-dim lr\n                            centers[ci] = xprop.copy()\n                            # modest expansion on success\n                            trust_radius[ci] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                            # per-dim learning rate: increase for dims that changed significantly toward improvement\n                            stepvec = (xprop - center)\n                            affected = np.abs(stepvec) > 1e-12\n                            # raise lr for changed dims, slightly reduce others\n                            lr[affected] = np.minimum(lr[affected] * self.lr_up, 4.0)\n                            lr[~affected] = np.maximum(lr[~affected] * self.lr_down, 1e-3)\n                            per_dim_lr[ci] = lr\n                            stagnation[ci] = 0\n                            break\n\n                    # if surrogate proposals failed, do multi-scale orthogonal directional probes\n                    if not improved_global and work_allow > 0 and evals < budget:\n                        # form a small orthonormal basis of random directions biased by lr\n                        kdirs = max(4, min(12, 4 + n // 3))\n                        # sample directions proportional to lr to favor sensitive dims\n                        dir_vectors = []\n                        for _ in range(kdirs):\n                            v = self.rng.randn(n) * lr\n                            nv = np.linalg.norm(v)\n                            if nv < 1e-12:\n                                v = np.ones(n)\n                                nv = np.linalg.norm(v)\n                            dir_vectors.append(v / nv)\n                        # shuffle and try multi-scale along each (both signs)\n                        self.rng.shuffle(dir_vectors)\n                        base_scale = np.maximum(np.linalg.norm(tr) / np.sqrt(float(n) + 1e-12), 1e-12)\n                        for d in dir_vectors:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            scales = self.direction_scales\n                            # order scales from small to large\n                            for s in scales:\n                                if work_allow <= 0 or evals >= budget:\n                                    break\n                                step = s * base_scale\n                                cand1 = np.clip(center + d * step, lb, ub)\n                                cand2 = np.clip(center - d * step, lb, ub)\n                                for cand in (cand1, cand2):\n                                    if work_allow <= 0 or evals >= budget:\n                                        break\n                                    # avoid repeating last eval\n                                    if len(X) > 0 and np.allclose(cand, X[-1], atol=1e-12):\n                                        continue\n                                    out = safe_eval(cand)\n                                    work_allow -= 1\n                                    if out is None:\n                                        break\n                                    ftry, xtry = out\n                                    if ftry < f_best - 1e-12:\n                                        improved_global = True\n                                        centers[ci] = xtry.copy()\n                                        trust_radius[ci] = np.minimum(tr * (1.2 + 0.1 * s), self.trust_max_frac * rng_range)\n                                        # update lr along direction dims\n                                        # boost dims aligned with d\n                                        align = np.abs(d) > (np.max(np.abs(d)) * 0.3)\n                                        lr[align] = np.minimum(lr[align] * (1.0 + 0.05 * s), 4.0)\n                                        lr[~align] = np.maximum(lr[~align] * 0.99, 1e-3)\n                                        per_dim_lr[ci] = lr\n                                        stagnation[ci] = 0\n                                        break\n                                # end cand loop\n                            # end scales loop\n                        # end directions\n\n                    # coordinate-wise fine adjustments if still no success\n                    if not improved_global and work_allow > 0 and evals < budget:\n                        # select coordinates with highest lr to test small perturbations\n                        coords = np.argsort(-lr)[:min(n, max(3, int(work_allow // 3)))]\n                        for d in coords:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            for sign in (+1.0, -1.0):\n                                delta = np.zeros(n)\n                                delta[d] = sign * lr[d] * tr[d] * 0.8\n                                x_try = np.clip(center + delta, lb, ub)\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[ci] = xtry.copy()\n                                    trust_radius[ci] = np.minimum(tr * 1.15, self.trust_max_frac * rng_range)\n                                    # if coord move helped, strengthen lr for that dim\n                                    lr[d] = np.minimum(lr[d] * self.lr_up, 4.0)\n                                    per_dim_lr[ci] = lr\n                                    stagnation[ci] = 0\n                                    break\n                            # end sign loop\n                        # end coords\n\n                    # occasional heavy-tailed Student-t jump to escape local minima\n                    if (not improved_global) and work_allow > 0 and evals < budget and self.rng.rand() < self.student_t_prob:\n                        # sample student-t scaled by range\n                        scale = self.student_t_scale_frac * np.maximum(rng_range, 1e-9)\n                        # using standard student-t via inverse CDF technique: sample normal / sqrt(chi2/df)\n                        z = self.rng.standard_normal(n)\n                        chi2 = self.rng.chisquare(self.student_t_df)\n                        jump = z / (np.sqrt(chi2 / self.student_t_df) + 1e-12)\n                        x_jump = np.clip(center + jump * scale, lb, ub)\n                        out = safe_eval(x_jump)\n                        if out is not None:\n                            work_allow -= 1\n                            fj, xj = out\n                            if fj < f_best - 1e-12:\n                                improved_global = True\n                                centers[ci] = xj.copy()\n                                trust_radius[ci] = np.minimum(tr * 2.0, self.trust_max_frac * rng_range)\n                                # reward per-dim lr for dims that changed most\n                                change = np.abs(xj - center)\n                                thresh = np.percentile(change, 60)\n                                big = change >= thresh\n                                lr[big] = np.minimum(lr[big] * 1.3, 4.0)\n                                lr[~big] = np.maximum(lr[~big] * 0.95, 1e-3)\n                                per_dim_lr[ci] = lr\n                                stagnation[ci] = 0\n                            else:\n                                # shrink trust on failed large jump\n                                trust_radius[ci] = np.maximum(tr * self.failure_shrink, self.trust_min_frac * rng_range)\n\n                    # if this center didn't help, shrink trust and increment stagnation\n                    if not improved_global:\n                        stagnation[ci] += 1\n                        trust_radius[ci] = np.maximum(tr * self.failure_shrink, self.trust_min_frac * rng_range)\n                        # decay learning rates slightly to avoid overconfident steps\n                        per_dim_lr[ci] = np.maximum(per_dim_lr[ci] * (0.98), 1e-4)\n                    else:\n                        # on global improvement, reduce stagnation for other centers slightly\n                        for j in range(len(stagnation)):\n                            if j != ci:\n                                stagnation[j] = max(0, stagnation[j] - 1)\n                else:\n                    # not enough data to fit surrogates: random-direction exploratory probes\n                    base_step = np.maximum(np.linalg.norm(trust_radius[0]) / np.sqrt(float(n) + 1e-12), 1e-12)\n                    tries = min(6, work_allow)\n                    for _ in range(tries):\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        s = self.rng.choice(self.direction_scales)\n                        x_try = np.clip(center + d * s * base_step, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_global = True\n                            centers[ci] = xtry.copy()\n                            trust_radius[ci] = np.minimum(trust_radius[ci] * 1.3, self.trust_max_frac * rng_range)\n                            per_dim_lr[ci] = np.minimum(per_dim_lr[ci] * 1.1, 4.0)\n                            stagnation[ci] = 0\n                            break\n\n                # replace center if stagnated long without contribution\n                if stagnation[ci] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # choose a point far from existing centers to increase diversity but not terrible fitness\n                        # compute distance to current centers\n                        d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        # prefer moderately good points: select among top 40% by quality then far ones\n                        idx_sorted = np.argsort(F)\n                        cutoff = max(1, int(0.4 * len(idx_sorted)))\n                        candidates = idx_sorted[:cutoff]\n                        if len(candidates) > 0:\n                            cand_dists = d_to_centers[candidates]\n                            pick_idx = np.argmax(cand_dists)\n                            pick = candidates[pick_idx]\n                        else:\n                            pick = int(self.rng.choice(len(X_arr)))\n                        centers[ci] = X_arr[pick].copy()\n                        trust_radius[ci] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                        per_dim_lr[ci] = np.full(n, self.lr_init)\n                        stagnation[ci] = 0\n\n            # end for centers\n\n            # prune archive periodically\n            if (iter_count % 3) == 0:\n                prune_archive()\n\n            # occasionally re-seed centers with best archive entries\n            if len(X) > 0 and (iter_count % 7) == 0:\n                topk = min(len(centers), max(1, len(X) // 12))\n                bests = get_top_centers(topk)\n                for i in range(len(bests)):\n                    centers[i] = bests[i].copy()\n\n        # end main loop\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 269, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (35,) \nOn line: step = -alpha * lr * (gsign / gmag) * tr", "error": "In the code, line 269, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (35,) \nOn line: step = -alpha * lr * (gsign / gmag) * tr", "parent_ids": "a9759013-c0d4-4e3f-816c-6216ddcbcef5", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "34ffe33c-ccaa-4135-9bf6-a7148bfe419b", "fitness": "-inf", "name": "AdaptiveAnisotropicPCAQuadLevy", "description": "The algorithm runs a small ensemble of adaptive \"centers\" (max_centers scaled by dimension) initialized by quasi-space-filling jittered sampling (init_frac of budget) inside the given bounds, and maintains an archive of evaluated points with periodic pruning and re-centering. Each center carries anisotropic per-dimension trust scales that are adaptively expanded (success_expand) or shrunk (failure_shrink) and tracked with a stagnation counter that triggers replacement from diverse good archive points after replace_patience iterations; per-iteration evaluation is capped (max_eval_per_iter) to control budget use. For local modeling it selects a neighborhood (neighbor_mul * dim nearest samples), performs distance-weighted PCA and fits a lightweight separable quadratic surrogate in PCA coordinates via ridge-regularized weighted least squares (predicting z* = -b/c and clipping by projected trust). Promising proposals are refined by two-point quadratic line-search along leading PCA directions (fitting a parabola through three samples), occasional directional Lévy/Cauchy jumps (cauchy_prob, cauchy_scale) to escape traps, and centers are probabilistically sampled biased toward better fitness for focused exploitation.", "code": "import numpy as np\n\nclass AdaptiveAnisotropicPCAQuadLevy:\n    \"\"\"\n    Adaptive Anisotropic PCA-Quadratic + Directional Line-Search with Occasional Lévy Jumps (AAPQ-L)\n\n    Main idea (one-line): Maintain a small ensemble of centers each with anisotropic per-component\n    step scales, fit lightweight quadratic surrogates in a local PCA subspace to propose bounded\n    minimizers, refine promising directions with two-point quadratic line-search, and use\n    adaptive Lévy-directed jumps plus center-splitting to escape stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_frac=0.10, neighbor_mul=7, max_centers=None,\n                 success_expand=1.6, failure_shrink=0.6,\n                 replace_patience=10, cauchy_prob=0.10, cauchy_scale=0.35):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # initialization sizing\n        self.init_frac = float(init_frac)\n        self.min_init = max(10, 2 * self.dim)\n        self.max_init = min(400, max(self.min_init, int(0.3 * self.budget)))\n\n        # neighborhood and modeling\n        self.neighbor_mul = int(neighbor_mul)\n        self.ridge = 1e-6\n\n        # trust / scale management (per-dimension anisotropic)\n        self.trust_init_frac = 0.5\n        self.trust_min = 1e-6\n        self.trust_max_frac = 2.0\n        self.success_expand = float(success_expand)\n        self.failure_shrink = float(failure_shrink)\n\n        # centers management\n        self.max_centers = max(2, min(self.dim + 4, self.dim if max_centers is None else int(max_centers)))\n        self.replace_patience = int(replace_patience)\n\n        # directional & jump behavior\n        self.cauchy_prob = float(cauchy_prob)\n        self.cauchy_scale = float(cauchy_scale)\n\n        # per-iteration eval cap (safety)\n        self.max_eval_per_iter = 80\n\n    def __call__(self, func):\n        n = int(self.dim)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        span = ub - lb\n        mid = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # Archive\n        X = []\n        F = []\n\n        # best so far\n        f_best = np.inf\n        x_best = None\n\n        # initial quasi-space-filling jittered sampling\n        init_budget = int(np.clip(self.init_frac * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        # jittered grid: sample along Latin-like by stratifying each dimension\n        for i in range(init_budget):\n            # stratified coordinate: use low-discrepancy-ish by permuting grid offsets\n            u = self.rng.rand(n)\n            x = lb + u * span\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper: safe evaluation obeying budget and bounds, update archive\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if float(f) < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # helper: get top-k unique centers from archive\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx = np.argsort(F)\n            centers = []\n            for ii in idx:\n                xi = np.array(X[ii])\n                if not any(np.linalg.norm(xi - c) < 1e-9 for c in centers):\n                    centers.append(xi)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        # initialize centers: best few points\n        centers = get_top_centers(min(self.max_centers, len(X)))\n        if len(centers) == 0:\n            centers = [np.array(self.rng.uniform(lb, ub))]\n\n        # per-center anisotropic trust scales (vector per-dim)\n        trust_scales = [np.maximum(self.trust_init_frac * span, 1e-9).copy() for _ in centers]\n        stagn = [0 for _ in centers]\n\n        # PCA + local quadratic surrogate fit in reduced subspace\n        def fit_local_pca_quad(center, X_arr, F_arr, max_components=None):\n            # returns dict with basis U (n x k), a, b (k), c (k) representing\n            # f(center + U z) ≈ a + b^T z + 0.5 * c^T z^2  (separable in PCA coords)\n            dx = X_arr - center\n            dists = np.linalg.norm(dx, axis=1)\n            # weights decreasing with distance\n            w = 1.0 / (1e-8 + dists)\n            w = w / (np.max(w) + 1e-12)\n            # center weighted PCA\n            W = np.sqrt(w)[:, None]\n            Xw = W * dx\n            # compute covariance\n            try:\n                cov = (Xw.T @ Xw)\n                # SVD for principal directions\n                U, S, Vt = np.linalg.svd(cov, full_matrices=False)\n                # choose k components (min dims with some variance)\n                if max_components is None:\n                    k = min(n, max(2, n // 2))\n                else:\n                    k = min(n, max(2, int(max_components)))\n                Uk = U[:, :k]  # note shape (n, k)\n                # project dx onto subspace\n                Z = dx @ Uk  # shape (m, k)\n                # build design matrix M = [ones, Z, 0.5 Z^2]\n                m = Z.shape[0]\n                M = np.ones((m, 1 + 2 * k))\n                M[:, 1:1 + k] = Z\n                M[:, 1 + k:1 + 2 * k] = 0.5 * (Z ** 2)\n                # weighted least squares solve\n                Aw = W * M\n                bw = W * F_arr\n                ridge = self.ridge * np.eye(M.shape[1])\n                params, *_ = np.linalg.lstsq(Aw.T @ Aw + ridge, Aw.T @ bw, rcond=None)\n                params = params.flatten()\n                a = float(params[0])\n                b = params[1:1 + k].flatten()\n                c = params[1 + k:1 + 2 * k].flatten()\n                # regularize small curvatures\n                c_reg = np.copy(c)\n                c_reg[c_reg < 1e-8] = 1e-8\n                return {'U': Uk, 'a': a, 'b': b, 'c': c_reg}\n            except Exception:\n                return None\n\n        # two-point quadratic line-search helper along direction d (unit) from x0\n        def two_point_quadratic_line_search(x0, f0, d, alphas):\n            # alphas is iterable of two positive steps (alpha1, alpha2)\n            # evaluate at x0 + alpha * d for both alphas if budget allows\n            (a1, a2) = tuple(alphas)\n            pts = [a1, a2]\n            vals = []\n            for a in pts:\n                if evals >= budget:\n                    return None\n                x_try = np.clip(x0 + a * d, lb, ub)\n                out = safe_eval(x_try)\n                if out is None:\n                    return None\n                f_try, x_try = out\n                vals.append((a, f_try, x_try))\n            # fit parabola p(alpha) through (0,f0),(a1,f1),(a2,f2)\n            A = np.array([[0.0**2, 0.0, 1.0],\n                          [vals[0][0]**2, vals[0][0], 1.0],\n                          [vals[1][0]**2, vals[1][0], 1.0]])\n            b = np.array([f0, vals[0][1], vals[1][1]])\n            try:\n                coeffs = np.linalg.solve(A, b)  # coeffs: [c2, c1, c0] where p(a)=c2 a^2 + c1 a + c0\n                c2, c1, c0 = coeffs\n                if c2 <= 0:\n                    # parabola opens downward or flat: pick best of sampled points\n                    best_idx = np.argmin([f0, vals[0][1], vals[1][1]])\n                    if best_idx == 0:\n                        return (f0, x0.copy())\n                    elif best_idx == 1:\n                        return (vals[0][1], vals[0][2].copy())\n                    else:\n                        return (vals[1][1], vals[1][2].copy())\n                alpha_star = -c1 / (2.0 * c2)\n                # clamp alpha to sampled bracket [min, max] extended by factor 1.2\n                lo = min(a1, a2) * 0.0  # allow 0..max\n                hi = max(a1, a2) * 1.2\n                alpha_star = float(np.clip(alpha_star, lo, hi))\n                x_star = np.clip(x0 + alpha_star * d, lb, ub)\n                if evals >= budget:\n                    return None\n                out = safe_eval(x_star)\n                if out is None:\n                    return None\n                f_star, x_star = out\n                return (f_star, x_star.copy())\n            except Exception:\n                # fallback: return best sampled\n                cand = [(f0, x0.copy()), (vals[0][1], vals[0][2].copy()), (vals[1][1], vals[1][2].copy())]\n                cand.sort(key=lambda p: p[0])\n                return (cand[0][0], cand[0][1].copy())\n\n        # main loop\n        iter_no = 0\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            # occasionally refresh centers from archive (diversify by distance)\n            if iter_no % 7 == 0 and len(X) > 0:\n                k = min(self.max_centers, max(1, len(X) // 12))\n                new_best = get_top_centers(k)\n                # merge while keeping uniqueness and limit size\n                merged = centers + new_best\n                uniq = []\n                for c in merged:\n                    if not any(np.linalg.norm(c - u) < 1e-9 for u in uniq):\n                        uniq.append(c)\n                    if len(uniq) >= self.max_centers:\n                        break\n                centers = uniq\n                # expand trust arrays if needed\n                if len(trust_scales) != len(centers):\n                    trust_scales = [np.maximum(self.trust_init_frac * span, 1e-9).copy() for _ in centers]\n                    stagn = [0 for _ in centers]\n\n            iter_no += 1\n            improved_global = False\n\n            # choose order of centers probabilistically biased by potential (lower F => higher chance)\n            if len(centers) > 1:\n                # compute center fitness by nearest archive point\n                center_vals = []\n                for c in centers:\n                    # approximate using nearest archived point\n                    if len(X) == 0:\n                        center_vals.append(1.0)\n                    else:\n                        dists = np.linalg.norm(np.asarray(X) - c, axis=1)\n                        idx = np.argmin(dists)\n                        center_vals.append(F[idx])\n                ranks = np.argsort(center_vals)\n                # make a softprob: use exponential wins\n                scores = np.array([1.0 / (1.0 + center_vals[i] - min(center_vals) + 1e-12) for i in range(len(centers))])\n                probs = scores / np.sum(scores)\n                order = list(self.rng.choice(range(len(centers)), size=len(centers), replace=False, p=probs))\n            else:\n                order = list(range(len(centers)))\n\n            for cidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[cidx])\n                scales = trust_scales[cidx]\n\n                # pick neighbors\n                X_arr = np.asarray(X)\n                F_arr = np.asarray(F)\n                if len(X) >= max(2 * n + 1, self.neighbor_mul * n):\n                    # use nearest m neighbors\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    m = min(len(X), max(2 * n + 1, self.neighbor_mul * n))\n                    idxn = np.argsort(dists)[:m]\n                    Xn = X_arr[idxn]\n                    Fn = F_arr[idxn]\n\n                    # fit PCA-quadratic surrogate in reduced subspace\n                    model = fit_local_pca_quad(center, Xn, Fn, max_components=min(n, max(3, n//2)))\n                    proposed_any = False\n\n                    if model is not None:\n                        U = model['U']              # n x k\n                        a0 = model['a']\n                        b = model['b']              # k\n                        c = model['c']              # k positive (curvatures)\n                        # predicted minimizer in z-space: z* = -b / c\n                        z_star = -b / (c + 1e-12)\n                        # scale z_star by per-component projected trust: map scales into z units\n                        proj_scale = np.sqrt(np.sum((U * scales[None, :]).T ** 2, axis=0))\n                        # Avoid zero\n                        proj_scale[proj_scale <= 0] = 1e-9\n                        z_star_clipped = np.clip(z_star, -proj_scale, proj_scale)\n                        x_pred = np.clip(center + U @ z_star_clipped, lb, ub)\n                        # Evaluate surrogate proposal\n                        if work_allow > 0 and evals < budget:\n                            out = safe_eval(x_pred)\n                            work_allow -= 1\n                            if out is None:\n                                break\n                            f_pred, x_pred = out\n                            proposed_any = True\n                            if f_pred < f_best - 1e-12:\n                                improved_global = True\n                                centers[cidx] = x_pred.copy()\n                                trust_scales[cidx] = np.minimum(scales * self.success_expand, self.trust_max_frac * span)\n                                stagn[cidx] = 0\n                            else:\n                                # refine direction by directional two-point line-search along leading PCA axes\n                                # choose top directions by |b| magnitude\n                                dir_order = np.argsort(-np.abs(b))\n                                for jj in dir_order[:min(3, len(dir_order))]:\n                                    if work_allow <= 1 or evals >= budget:\n                                        break\n                                    zd = np.zeros_like(b); zd[jj] = 1.0\n                                    dvec = U @ zd\n                                    dnorm = np.linalg.norm(dvec)\n                                    if dnorm == 0:\n                                        continue\n                                    dunit = dvec / dnorm\n                                    # pick two step lengths based on projected scale\n                                    s_proj = proj_scale[jj]\n                                    a1 = 0.6 * s_proj\n                                    a2 = 1.2 * s_proj\n                                    # perform two point line-search\n                                    res = two_point_quadratic_line_search(center, a0, dunit, (a1, a2))\n                                    if res is None:\n                                        break\n                                    f_line, x_line = res\n                                    # we have already consumed evaluations inside line-search\n                                    work_allow = min(work_allow, budget - evals)\n                                    if f_line < f_best - 1e-12:\n                                        improved_global = True\n                                        centers[cidx] = x_line.copy()\n                                        trust_scales[cidx] = np.minimum(scales * self.success_expand, self.trust_max_frac * span)\n                                        stagn[cidx] = 0\n                                        break\n                                # end for directions\n                                if not improved_global:\n                                    # shrink trust a bit\n                                    trust_scales[cidx] = np.maximum(scales * self.failure_shrink, self.trust_min * span)\n                                    stagn[cidx] += 1\n                        # done model branch\n\n                    else:\n                        # model fitting failed; do a few exploratory directional probes in PCA directions or random\n                        num_dirs = min(6, max(3, n // 2))\n                        for _ in range(num_dirs):\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            d = self.rng.randn(n)\n                            d = d / (np.linalg.norm(d) + 1e-12)\n                            step = np.maximum(np.linalg.norm(scales) / np.sqrt(n + 1e-12), 1e-8)\n                            x_try = np.clip(center + d * step, lb, ub)\n                            out = safe_eval(x_try)\n                            work_allow -= 1\n                            if out is None:\n                                break\n                            ftry, xtry = out\n                            if ftry < f_best - 1e-12:\n                                improved_global = True\n                                centers[cidx] = xtry.copy()\n                                trust_scales[cidx] = np.minimum(scales * self.success_expand, self.trust_max_frac * span)\n                                stagn[cidx] = 0\n                                break\n                        if not improved_global:\n                            stagn[cidx] += 1\n                            trust_scales[cidx] = np.maximum(scales * self.failure_shrink, self.trust_min * span)\n\n                    # occasional directional Lévy/Cauchy jump along weak curvature direction\n                    if work_allow > 0 and evals < budget and self.rng.rand() < self.cauchy_prob:\n                        # choose direction: if model exists pick weakest curvature direction, else random\n                        if model is not None:\n                            jj = np.argmin(c)  # weakest curvature\n                            d = U[:, jj]\n                        else:\n                            d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        scale = self.cauchy_scale * np.maximum(span, 1e-9)\n                        jump = self.rng.standard_cauchy(size=1)[0]\n                        alpha = np.clip(jump, -8, 8) * np.mean(scale)\n                        x_jump = np.clip(center + alpha * d, lb, ub)\n                        out = safe_eval(x_jump)\n                        if out is not None:\n                            work_allow -= 1\n                            fj, xj = out\n                            if fj < f_best - 1e-12:\n                                improved_global = True\n                                centers[cidx] = xj.copy()\n                                trust_scales[cidx] = np.minimum(scales * 2.0, self.trust_max_frac * span)\n                                stagn[cidx] = 0\n                            else:\n                                # modest shrink on failure\n                                trust_scales[cidx] = np.maximum(scales * 0.85, self.trust_min * span)\n                                stagn[cidx] += 1\n\n                else:\n                    # not enough archive to build rich local model: perform random exploratory samples near center\n                    num_try = min(6, work_allow)\n                    for _ in range(num_try):\n                        if evals >= budget:\n                            break\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        step = np.maximum(np.linalg.norm(trust_scales[cidx]) / np.sqrt(n + 1e-12), 1e-8)\n                        x_try = np.clip(center + d * step * self.rng.rand(), lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xtry.copy()\n                            trust_scales[cidx] = np.minimum(trust_scales[cidx] * self.success_expand, self.trust_max_frac * span)\n                            stagn[cidx] = 0\n                            break\n                        else:\n                            stagn[cidx] += 1\n                            trust_scales[cidx] = np.maximum(trust_scales[cidx] * self.failure_shrink, self.trust_min * span)\n\n                # center replacement if stagnated\n                if stagn[cidx] >= self.replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # pick a moderately good point far from all centers\n                        d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        # favor points not too bad: choose among top 30% by fitness then pick the one farthest\n                        cutoff = int(max(1, len(X_arr) * 0.3))\n                        idx_sorted = np.argsort(F)\n                        cand_idx = idx_sorted[:cutoff]\n                        if len(cand_idx) == 0:\n                            pick = int(self.rng.randint(len(X_arr)))\n                        else:\n                            far_idx = cand_idx[np.argmax(d_to_centers[cand_idx])]\n                            pick = int(far_idx)\n                        centers[cidx] = X_arr[pick].copy()\n                        trust_scales[cidx] = np.maximum(self.trust_init_frac * span, 1e-9).copy()\n                        stagn[cidx] = 0\n\n            # end centers loop\n\n            # archive pruning: keep the most informative points\n            max_archive = max(2000, 50 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:max(200, n * 10)]\n                rest_idx = idx_sorted[ max(200, n*10) : ]\n                if len(rest_idx) > 0:\n                    step = max(1, len(rest_idx) // (max_archive - len(keep_best)))\n                    keep_rest = rest_idx[::step]\n                else:\n                    keep_rest = []\n                keep_idx = np.concatenate([keep_best, keep_rest]).astype(int)\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasionally re-center ensemble to top archive points\n            if len(X) > 0 and iter_no % 5 == 0:\n                k = min(len(centers), max(1, len(X) // 12))\n                best_centers = get_top_centers(k)\n                for i in range(len(best_centers)):\n                    centers[i] = best_centers[i].copy()\n\n        # end main loop\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 296, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (10,5) (1,10) \nOn line: proj_scale = np.sqrt(np.sum((U * scales[None, :]).T ** 2, axis=0))", "error": "In the code, line 296, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (10,5) (1,10) \nOn line: proj_scale = np.sqrt(np.sum((U * scales[None, :]).T ** 2, axis=0))", "parent_ids": "a9759013-c0d4-4e3f-816c-6216ddcbcef5", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "fbec7c73-80af-4533-bf3e-ca16ae12cded", "fitness": 0.39449133840683187, "name": "EnsembleCovarianceAdaptiveSearch", "description": "The algorithm maintains a small ensemble of distinct centers with anisotropic per‑dimension trust radii (initialized as 0.6 of the search range) and stagnation-based center replacement to preserve diversity. Around each center it fits cheap weighted local models — a linear term plus separable per‑dimension quadratic curvature (using a 1e‑4 ridge and neighbors = 6*dim or at least ~2*dim) — and generates a rich set of conservative proposals (separable-quadratic minimizers clipped by per‑dim trusts, gradient-ish steps, Gaussian-mixture jitters, principal‑direction/eigen steps, multiscale directional probes, coordinate tweaks) which are deduplicated and ranked by proximity to the center. Trust radii adapt on feedback (expand by 1.4 on improvement, shrink by 0.6 on failure), occasional tempered Cauchy escapes (probability 0.08, scale 0.6) provide heavy‑tailed jumps, and a strict safe_eval plus per-iteration eval cap (max 40) enforces the global budget and box bounds. Practical safeguards include archive pruning (large max_archive_base ~max(1500,40*dim)), periodic center refresh from archive bests, conservative parameter choices for stability (small ensemble size, moderate init sampling ratio 0.15, RNG seeding, regularization).", "code": "import numpy as np\n\nclass EnsembleCovarianceAdaptiveSearch:\n    \"\"\"\n    Ensemble Covariance-Aware RBF-Style Adaptive Search (ECRAS)\n\n    Main idea: Maintain an ensemble of centers with anisotropic trust radii; fit\n    cheap weighted linear + separable curvature estimates from local neighbors;\n    propose (1) clipped surrogate minimizers, (2) Gaussian-mixture draws around\n    center, and (3) principal-direction steps from local covariance; occasionally\n    attempt tempered Cauchy escapes. Adapt per-dimension trust radii by modest\n    expansion on success and stronger shrink on failure. Archive and prune to\n    control memory.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # Initialization sizing (different from original)\n        self.init_ratio = 0.15\n        self.min_init = max(12, 2 * self.dim)\n        self.max_init = min(300, int(0.3 * self.budget))\n\n        # Ensemble & local modeling\n        self.ensemble_size = max(3, min(6, self.dim // 2 + 1))\n        self.model_neighbor_multiplier = 6  # neighbors = multiplier * dim (smaller than original)\n        self.ridge = 1e-4  # stronger regularization\n\n        # trust region (anisotropic per-dimension)\n        self.trust_init_frac = 0.6\n        self.trust_min = 1e-6\n        self.trust_max_frac = 1.5\n        self.success_expand = 1.4\n        self.failure_shrink = 0.6\n\n        # sampling controls\n        self.max_eval_per_iter = 40\n        self.direction_scales = np.array([0.2, 0.5, 1.0, 1.8])\n\n        # tempered heavy-tail jumps\n        self.cauchy_prob = 0.08\n        self.cauchy_scale_frac = 0.6\n\n        # management\n        self.max_centers = max(3, min(12, self.ensemble_size * 2))\n        self.center_replace_patience = 10\n\n        # archive pruning\n        self.max_archive_base = max(1500, 40 * self.dim)\n\n        # RNG\n        self.rng = np.random.RandomState(self.seed)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # Use provided bounds if available, otherwise default to [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        X = []\n        F = []\n        f_best = np.inf\n        x_best = None\n\n        # initial space-filling random draws (uniform jitter)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper: pick distinct top centers\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if all(np.linalg.norm(x - c) > 1e-10 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.max_centers, len(X)))\n        if len(centers) == 0:\n            centers = [np.array(self.rng.uniform(lb, ub))]\n\n        # per-center anisotropic trust radii (vectors)\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n        stagnation = [0 for _ in centers]\n\n        # safe eval that respects budget and bounds, updates archive and best\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # fit weighted linear + separable curvature (per-dim) around center\n        def fit_local_linear_and_sepquad(center, neighbors_X, neighbors_F):\n            dx = neighbors_X - center  # m x n\n            m = dx.shape[0]\n            if m < 2:\n                return None\n            # weights by inverse distance (clipped)\n            dists = np.linalg.norm(dx, axis=1)\n            w = 1.0 / (dists + 1e-12)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            # linear part\n            A_lin = np.hstack([np.ones((m,1)), dx])\n            try:\n                params_lin, *_ = np.linalg.lstsq(W * A_lin, W * neighbors_F, rcond=None)\n                a_lin = float(params_lin[0])\n                b_lin = params_lin[1:].flatten()\n            except Exception:\n                return None\n            # estimate separable curvature (per-dim) by fitting quadratic terms (0.5 * h_i * dx_i^2)\n            # build matrix with quadratic column per dimension\n            A_q = np.hstack([np.ones((m,1)), dx, 0.5 * (dx**2)])\n            try:\n                ridge = self.ridge * np.eye(A_q.shape[1])\n                params_q, *_ = np.linalg.lstsq((W * A_q).T @ (W * A_q) + ridge,\n                                               (W * A_q).T @ (W * neighbors_F), rcond=None)\n                params_q = params_q.flatten()\n                h_diag = params_q[1 + n: 1 + 2*n]\n                # regularize curvatures to avoid negative tiny values\n                h_diag = np.maximum(h_diag, 1e-8)\n            except Exception:\n                h_diag = np.ones(n) * 1e-6\n            return a_lin, b_lin, h_diag\n\n        iter_count = 0\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            iter_count += 1\n\n            # refresh centers periodically with best archive points\n            if (iter_count % 7) == 0 and len(X) > 0:\n                topk = min(self.max_centers, max(1, len(X)//12))\n                new_centers = get_top_centers(topk)\n                # merge preserving uniqueness\n                merged = []\n                for c in (centers + new_centers):\n                    key = tuple(np.round(np.asarray(c), 12))\n                    if not any(np.allclose(c, m, atol=1e-12) for m in merged):\n                        merged.append(np.array(c))\n                    if len(merged) >= self.max_centers:\n                        break\n                centers = merged[:self.max_centers]\n                if len(trust_radius) != len(centers):\n                    trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n                    stagnation = [0 for _ in centers]\n\n            improved_global = False\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            for cidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[cidx])\n                tr = trust_radius[cidx]  # vector length n\n\n                # gather neighbors if enough points\n                X_arr = np.asarray(X) if len(X) > 0 else np.empty((0, n))\n                F_arr = np.asarray(F) if len(F) > 0 else np.empty((0,))\n                enough_data = len(X) >= max(2 * n + 1, self.model_neighbor_multiplier * n)\n                if enough_data:\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), max(2 * n + 1, self.model_neighbor_multiplier * n))\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n\n                    # local fits\n                    local = fit_local_linear_and_sepquad(center, X_nei, F_nei)\n                    if local is not None:\n                        a_loc, b_loc, h_loc = local\n                    else:\n                        a_loc, b_loc, h_loc = None, None, None\n\n                    proposals = []\n\n                    # surrogate-based minimizer (separable quad)\n                    if b_loc is not None and h_loc is not None:\n                        delta = - b_loc / (h_loc + 1e-20)\n                        # clip by per-dim trust radii\n                        delta = np.clip(delta, -tr, tr)\n                        x_q = np.clip(center + delta, lb, ub)\n                        proposals.append(('sepquad', x_q))\n\n                    # gradient-ish linear step (scaled by trust and predicted gradient norm)\n                    if b_loc is not None:\n                        g = b_loc\n                        # choose step length proportional to trust and inverse gradient magnitude\n                        gn = np.linalg.norm(g)\n                        step_len = np.minimum(tr, np.where(gn > 0, tr * (1.0 / (1.0 + gn)), tr * 0.5))\n                        # direction is -g\n                        step = -np.sign(g) * step_len * 0.6\n                        x_lin = np.clip(center + step, lb, ub)\n                        proposals.append(('lin', x_lin))\n\n                    # Gaussian-mixture around center (small & medium)\n                    for scale_frac in (0.25, 0.6, 1.0):\n                        cov_scale = scale_frac * tr\n                        jitter = self.rng.randn(n) * cov_scale\n                        proposals.append((f'gauss_{scale_frac}', np.clip(center + jitter, lb, ub)))\n\n                    # principal directions from local covariance\n                    try:\n                        C = np.cov((X_nei - center).T)\n                        eigvals, eigvecs = np.linalg.eigh(C + 1e-12 * np.eye(n))\n                        # take top 2 eigenvectors\n                        idx_e = np.argsort(-np.abs(eigvals))[:min(3, n)]\n                        for idx_ei in idx_e:\n                            v = eigvecs[:, idx_ei]\n                            for s in (0.5, 1.0):\n                                step = s * (tr * (np.abs(eigvals[idx_ei]) + 1e-12) / (np.max(np.abs(eigvals)) + 1e-12))\n                                # directional step using per-dim scaling\n                                step_vec = v * step\n                                proposals.append((f'eig_{idx_ei}_{s}', np.clip(center + step_vec, lb, ub)))\n                    except Exception:\n                        pass\n\n                    # deduplicate proposals\n                    unique_props = []\n                    prop_names = []\n                    for name, p in proposals:\n                        if not any(np.allclose(p, q, atol=1e-12) for q in unique_props):\n                            unique_props.append(p)\n                            prop_names.append(name)\n\n                    # rank proposals: prefer smaller norm from center (conservative)\n                    unique_props.sort(key=lambda z: np.linalg.norm(z - center))\n\n                    # evaluate proposals until improvement or work_allow exhausted\n                    for xprop in unique_props:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        out = safe_eval(xprop)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        fprop, xprop = out\n                        if fprop < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xprop.copy()\n                            # expand per-dim trusts conservatively\n                            trust_radius[cidx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                    # if no improvement, directional multi-scale probes\n                    if (not improved_global) and work_allow > 0 and evals < budget:\n                        # create a few random directions biased by local covariance (if available)\n                        num_dirs = min(10, 4 + n // 2)\n                        dirs = []\n                        for _ in range(num_dirs):\n                            v = self.rng.randn(n)\n                            v_norm = np.linalg.norm(v)\n                            if v_norm == 0:\n                                v = np.ones(n); v_norm = np.linalg.norm(v)\n                            dirs.append(v / v_norm)\n                        self.rng.shuffle(dirs)\n                        base_step = np.linalg.norm(tr) / np.sqrt(float(n) + 1e-12)\n                        for d in dirs:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            cand = []\n                            for s in self.direction_scales:\n                                step = s * base_step\n                                cand.append(np.clip(center + d * step, lb, ub))\n                                cand.append(np.clip(center - d * step, lb, ub))\n                            self.rng.shuffle(cand)\n                            for x_try in cand:\n                                if work_allow <= 0 or evals >= budget:\n                                    break\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.25, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n                            # end cand loop\n                        # end dirs\n\n                    # coordinate tweaks if still nothing\n                    if (not improved_global) and work_allow > 0 and evals < budget:\n                        # try a subset of coordinates\n                        coord_step = np.maximum(tr, 1e-12)\n                        coords_idx = np.arange(n)\n                        max_coords = min(n, max(3, int(work_allow // 2)))\n                        if max_coords < n:\n                            coords_idx = self.rng.choice(coords_idx, size=max_coords, replace=False)\n                        for i in coords_idx:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            for sign in (+1.0, -1.0):\n                                x_try = centers[cidx].copy()\n                                x_try[i] = x_try[i] + sign * coord_step[i]\n                                x_try = np.clip(x_try, lb, ub)\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.2, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n                            # early break if improved\n\n                    # occasional tempered Cauchy escape\n                    if (not improved_global) and work_allow > 0 and evals < budget and self.rng.rand() < self.cauchy_prob:\n                        scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                        jump = self.rng.standard_cauchy(size=n)\n                        jump = np.clip(jump, -8, 8)  # temper extremes\n                        x_jump = np.clip(centers[cidx] + jump * scale, lb, ub)\n                        out = safe_eval(x_jump)\n                        if out is not None:\n                            work_allow -= 1\n                            fjump, xjump = out\n                            if fjump < f_best - 1e-12:\n                                improved_global = True\n                                centers[cidx] = xjump.copy()\n                                trust_radius[cidx] = np.minimum(tr * 2.0, self.trust_max_frac * rng_range)\n                                stagnation[cidx] = 0\n                            else:\n                                # unsuccessful jump shrinks trust\n                                trust_radius[cidx] = np.maximum(tr * 0.85, self.trust_min * rng_range)\n\n                    # update trust/stagnation if still no improvement for this center\n                    if not improved_global:\n                        stagnation[cidx] += 1\n                        trust_radius[cidx] = np.maximum(tr * self.failure_shrink, self.trust_min * rng_range)\n                    else:\n                        # reduce stagnation counters of others modestly to maintain diversity\n                        for j in range(len(stagnation)):\n                            if j != cidx and stagnation[j] > 0:\n                                stagnation[j] = max(0, stagnation[j] - 1)\n\n                else:\n                    # insufficient data: random exploratory probes around center\n                    base_step = np.linalg.norm(trust_radius[0]) / np.sqrt(float(n) + 1e-12)\n                    probes = min(6, work_allow)\n                    for _ in range(probes):\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        s = self.rng.choice(self.direction_scales)\n                        x_try = np.clip(center + d * s * base_step, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xtry.copy()\n                            trust_radius[cidx] = np.minimum(trust_radius[cidx] * 1.2, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                # replace center if stagnated\n                if stagnation[cidx] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # distance to nearest center\n                        d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        cand_idx = np.argsort(d_to_centers)[-max(1, min(30, len(X_arr))):]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[cidx] = X_arr[pick].copy()\n                        trust_radius[cidx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                        stagnation[cidx] = 0\n\n            # archive pruning\n            max_archive = max(self.max_archive_base, 20 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:150]\n                rest_idx = idx_sorted[150:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 150))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasionally refresh centers from archive best\n            if len(X) > 0 and (iter_count % 5 == 0):\n                best_centers = get_top_centers(min(len(centers), max(1, len(X)//12)))\n                for i in range(min(len(best_centers), len(centers))):\n                    centers[i] = best_centers[i].copy()\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm EnsembleCovarianceAdaptiveSearch scored 0.394 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "a9759013-c0d4-4e3f-816c-6216ddcbcef5", "operator": null, "metadata": {"aucs": [0.15432654122442224, 0.1614252751420412, 0.46300893343600436, 0.8786907970370668, 0.4880631422606033, 0.5345814210894088, 0.23262107446762292, 0.4233395011738105, 0.4468592413968292, 0.16199745684050937]}, "task_prompt": ""}
{"id": "e9f3e383-6b8c-4beb-9ac3-012fb83c40e5", "fitness": 0.3200828224425922, "name": "EnsemblePCAAdaptiveSurrogate", "description": "The design keeps an ensemble of adaptive \"trust\" centers (ensemble_size ≈ max(2,min(8,dim//2+1))) to balance parallel local search and diversity, with modest space-filling initialization (init_ratio=0.12, min_init = max(10,2*dim)) and archive pruning (max_archive = max(2000,50*n)). For local exploitation it fits cheap distance-weighted separable quadratic and linear surrogates (model_neighbor_multiplier=8, ridge=1e-6) and uses their minimizers plus PCA-guided low-dimensional moves to propose focused steps. Exploration is provided by multi-scale directional/coordinate probes (direction_scales = [0.25,0.5,1.0,2.0], max_eval_per_iter=60), jittered variants, crossover toward the global best, and occasional heavy-tailed Cauchy jumps (cauchy_prob=0.12, cauchy_scale_frac=0.45). Trust regions adapt on success/failure (trust_init_frac=0.5, success_expand=1.6, failure_shrink=0.66, trust_min small) and centers are replaced after stagnation (center_replace_patience=8) to maintain exploration while keeping per-iteration evaluation cost and memory under control.", "code": "import numpy as np\n\nclass EnsemblePCAAdaptiveSurrogate:\n    \"\"\"\n    Ensemble PCA Adaptive Surrogate Search (EPASS)\n\n    One-line: Maintain an ensemble of adaptive-trust centers, fit cheap local separable-quadratic\n    and linear surrogates on neighbors, propose surrogate minimizers and PCA-guided low-dim probes,\n    fall back to multi-scale directional/coordinate probing and occasional Cauchy jumps, and replace\n    stagnating centers to maintain diversity.\n\n    __init__(self, budget, dim, seed=None, ensemble_size=None, init_ratio=0.12)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, ensemble_size=None, init_ratio=0.12):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # initialization sizing\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(10, 2 * self.dim)\n        self.max_init = min(400, int(0.4 * self.budget))\n\n        # ensemble sizing\n        if ensemble_size is None:\n            self.ensemble_size = max(2, min(8, self.dim // 2 + 1))\n        else:\n            self.ensemble_size = max(1, int(ensemble_size))\n\n        # modeling & neighbors\n        self.model_neighbor_multiplier = 8  # neighbors = multiplier * dim\n        self.ridge = 1e-6\n\n        # trust region settings\n        self.trust_init_frac = 0.5\n        self.trust_min = 1e-6\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.6\n        self.failure_shrink = 0.66\n\n        # probing\n        self.direction_scales = np.array([0.25, 0.5, 1.0, 2.0])\n        self.max_eval_per_iter = 60\n        self.cauchy_prob = 0.12\n        self.cauchy_scale_frac = 0.45\n\n        # center management\n        self.center_replace_patience = 8\n        self.max_archive = None  # will be set to max(2000,50*n) in call\n\n    def __call__(self, func):\n        n = int(self.dim)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        self.max_archive = max(2000, 50 * n)\n\n        budget = int(self.budget)\n        evals = 0\n\n        X = []  # list of points\n        F = []  # list of function values\n\n        f_best = np.inf\n        x_best = None\n\n        # Determine initial sampling budget\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n\n        # space-filling-ish initialization\n        for i in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            fx = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(fx))\n            if fx < f_best:\n                f_best = float(fx); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # initialize centers as top distinct points\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if all(np.linalg.norm(x - c) > 1e-9 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.ensemble_size, len(X)))\n        # ensure we have enough centers\n        while len(centers) < self.ensemble_size:\n            centers.append(self.rng.uniform(lb, ub))\n        centers = [np.array(c) for c in centers[:self.ensemble_size]]\n\n        # per-center trust and stagnation\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n        stagnation = [0 for _ in centers]\n\n        # safe evaluation wrapper\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            xc = np.clip(x, lb, ub)\n            fx = func(xc)\n            evals += 1\n            X.append(xc.copy()); F.append(float(fx))\n            if fx < f_best - 1e-12:\n                f_best = float(fx); x_best = xc.copy()\n            return float(fx), xc\n\n        # fit separable quadratic around a center using neighbor arrays\n        def fit_separable_quad(center, neighbors_X, neighbors_F):\n            dx = neighbors_X - center\n            m = dx.shape[0]\n            M = np.ones((m, 1 + 2 * n))\n            M[:, 1:1 + n] = dx\n            M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n            y = neighbors_F\n            dists = np.linalg.norm(dx, axis=1)\n            w = 1.0 / (dists + 1e-12)\n            maxw = np.max(w) if np.max(w) > 0 else 1.0\n            w = w / (maxw + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            bvec = W * y\n            try:\n                ridge = self.ridge * np.eye(M.shape[1])\n                params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ bvec, rcond=None)\n                params = params.flatten()\n                a = params[0]\n                b_lin = params[1:1 + n]\n                h_diag = params[1 + n:1 + 2 * n]\n                h_reg = np.copy(h_diag)\n                h_reg[h_reg < 1e-8] = 1e-8\n                return a, b_lin, h_reg\n            except Exception:\n                return None\n\n        # helper to compute PCA directions on neighbors (returns eigenvectors)\n        def pca_directions(neighbors_X, center, max_components=None):\n            dx = neighbors_X - center\n            if dx.shape[0] <= 1:\n                return None\n            # center rows\n            cov = np.cov(dx.T)\n            # eigh for symmetric cov\n            try:\n                vals, vecs = np.linalg.eigh(cov)\n                # sort descending\n                idx = np.argsort(vals)[::-1]\n                vals = vals[idx]; vecs = vecs[:, idx]\n                if max_components is None:\n                    max_components = min(n, max(1, int(0.2 * n)))\n                k = min(max_components, vecs.shape[1])\n                return vecs[:, :k], vals[:k]\n            except Exception:\n                return None\n\n        iter_count = 0\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            # occasionally refresh centers from archive (keep best + diverse ones)\n            if iter_count % 6 == 0 and len(X) > 0:\n                topk = min(len(centers), max(1, len(X) // 10))\n                new_centers = get_top_centers(topk)\n                # merge preserving uniqueness\n                merged = centers + new_centers\n                uniq = []\n                for c in merged:\n                    if not any(np.linalg.norm(c - u) < 1e-9 for u in uniq):\n                        uniq.append(c)\n                    if len(uniq) >= self.ensemble_size:\n                        break\n                while len(uniq) < self.ensemble_size:\n                    uniq.append(self.rng.uniform(lb, ub))\n                centers = [np.array(c) for c in uniq[:self.ensemble_size]]\n                # ensure trust and stagnation lengths\n                if len(trust_radius) != len(centers):\n                    trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n                    stagnation = [0 for _ in centers]\n\n            improved_global = False\n\n            # iterate centers in randomized order\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            for cidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n\n                center = np.array(centers[cidx])\n                tr = trust_radius[cidx]\n\n                # gather neighbors if available\n                neighbors_needed = max(2 * n + 1, self.model_neighbor_multiplier * n)\n                if len(X) >= neighbors_needed:\n                    X_arr = np.asarray(X)\n                    F_arr = np.asarray(F)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), neighbors_needed)\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n\n                    # fit models\n                    quad = fit_separable_quad(center, X_nei, F_nei)\n                    # linear fit (around center)\n                    try:\n                        A_lin = np.hstack([np.ones((X_nei.shape[0], 1)), X_nei - center])\n                        params_lin, *_ = np.linalg.lstsq(A_lin, F_nei, rcond=None)\n                        b_lin_only = params_lin[1:].flatten()\n                    except Exception:\n                        b_lin_only = None\n\n                    # PCA directions\n                    pca = pca_directions(X_nei, center)\n                    # propose candidates list\n                    proposals = []\n\n                    # surrogate quadratic minimizer\n                    if quad is not None:\n                        a_q, b_q, h_q = quad\n                        delta_q = -b_q / (h_q + 1e-20)\n                        delta_q = np.clip(delta_q, -tr, tr)\n                        x_q = np.clip(center + delta_q, lb, ub)\n                        proposals.append(('quad', x_q))\n\n                    # linear-based sign step\n                    if b_lin_only is not None:\n                        step = -0.5 * np.sign(b_lin_only) * tr\n                        x_lin = np.clip(center + step, lb, ub)\n                        proposals.append(('lin', x_lin))\n\n                    # PCA-guided proposals: move along top components in subspace\n                    if pca is not None:\n                        vecs, vals = pca\n                        # propose along + and - of each top PC with multiple scales\n                        for i_pc in range(vecs.shape[1]):\n                            v = vecs[:, i_pc]\n                            # scale step by tr projected magnitude\n                            for s in (0.5, 1.0, 1.5):\n                                step = s * (np.linalg.norm(tr) / np.sqrt(float(n))) * v\n                                x_p_plus = np.clip(center + step, lb, ub)\n                                x_p_minus = np.clip(center - step, lb, ub)\n                                proposals.append((f'pca_{i_pc}_+', x_p_plus))\n                                proposals.append((f'pca_{i_pc}_-', x_p_minus))\n\n                    # small ensemble perturbations of proposals\n                    unique_props = []\n                    for name, p in proposals:\n                        # add base\n                        if not any(np.allclose(p, q, atol=1e-12) for q in unique_props):\n                            unique_props.append(p)\n                        # add jittered variants\n                        jitter = (self.rng.randn(n) * 0.03) * tr\n                        pj = np.clip(p + jitter, lb, ub)\n                        if not any(np.allclose(pj, q, atol=1e-12) for q in unique_props):\n                            unique_props.append(pj)\n\n                    # also include a crossover between this center and best center (if different)\n                    if x_best is not None and np.linalg.norm(center - x_best) > 1e-9:\n                        alpha = self.rng.uniform(0.2, 0.8)\n                        x_cross = np.clip(alpha * center + (1 - alpha) * x_best, lb, ub)\n                        if not any(np.allclose(x_cross, q, atol=1e-12) for q in unique_props):\n                            unique_props.insert(0, x_cross)\n\n                    # sort proposals by proximity to center (local first)\n                    unique_props.sort(key=lambda z: np.linalg.norm(z - center))\n\n                    # evaluate proposals until success or work_allow exhausted\n                    for xprop in unique_props:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        out = safe_eval(xprop)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        fprop, xprop = out\n                        if fprop < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xprop.copy()\n                            trust_radius[cidx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                    # fallback directional probes if no improvement\n                    if not improved_global and work_allow > 0 and evals < budget:\n                        num_dirs = int(np.clip(4 + n // 2, 4, 12))\n                        dirs = []\n                        for _ in range(num_dirs):\n                            v = self.rng.randn(n)\n                            nv = np.linalg.norm(v)\n                            if nv == 0:\n                                v = np.ones(n); nv = np.linalg.norm(v)\n                            dirs.append(v / nv)\n                        self.rng.shuffle(dirs)\n                        base_step = np.linalg.norm(tr) / np.sqrt(float(n) + 1e-12)\n                        for d in dirs:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            cand = []\n                            for s in self.direction_scales:\n                                cand.append(np.clip(center + d * (s * base_step), lb, ub))\n                                cand.append(np.clip(center - d * (s * base_step), lb, ub))\n                            self.rng.shuffle(cand)\n                            for x_try in cand:\n                                if work_allow <= 0 or evals >= budget:\n                                    break\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.25, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n                            # end cand\n                        # end dirs\n\n                    # coordinate tweaks if still no improvement\n                    if not improved_global and work_allow > 0 and evals < budget:\n                        coord_step = np.maximum(tr, 1e-12)\n                        coords = np.arange(n)\n                        max_coords = min(n, max(3, int(work_allow // 2)))\n                        if max_coords < n:\n                            coords = self.rng.choice(coords, size=max_coords, replace=False)\n                        for i in coords:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            for sign in (+1.0, -1.0):\n                                x_try = centers[cidx].copy()\n                                x_try[i] = x_try[i] + sign * coord_step[i]\n                                x_try = np.clip(x_try, lb, ub)\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.2, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n\n                    # occasional heavy-tailed jump\n                    if not improved_global and work_allow > 0 and evals < budget and self.rng.rand() < self.cauchy_prob:\n                        scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                        jump = self.rng.standard_cauchy(size=n)\n                        jump = np.clip(jump, -10, 10)\n                        x_jump = np.clip(centers[cidx] + jump * scale, lb, ub)\n                        out = safe_eval(x_jump)\n                        if out is not None:\n                            work_allow -= 1\n                            fjump, xjump = out\n                            if fjump < f_best - 1e-12:\n                                improved_global = True\n                                centers[cidx] = xjump.copy()\n                                trust_radius[cidx] = np.minimum(tr * 2.0, self.trust_max_frac * rng_range)\n                                stagnation[cidx] = 0\n                            else:\n                                trust_radius[cidx] = np.maximum(tr * 0.8, self.trust_min * rng_range)\n\n                    # update stagnation/trust if no improvement for this center\n                    if not improved_global:\n                        stagnation[cidx] += 1\n                        trust_radius[cidx] = np.maximum(tr * self.failure_shrink, self.trust_min * rng_range)\n                    else:\n                        # small positive effect on other centers' stagnation reset (encourage diversification)\n                        for j in range(len(stagnation)):\n                            if j != cidx and stagnation[j] > 0:\n                                stagnation[j] = max(0, stagnation[j] - 1)\n\n                else:\n                    # not enough data: exploratory random directional probes from center\n                    base_step = np.linalg.norm(trust_radius[0]) / np.sqrt(float(n) + 1e-12)\n                    for _ in range(min(6, work_allow)):\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        s = self.rng.choice(self.direction_scales)\n                        x_try = np.clip(center + d * s * base_step, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xtry.copy()\n                            trust_radius[cidx] = np.minimum(trust_radius[cidx] * 1.2, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                # replace center if stagnated\n                if stagnation[cidx] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # choose a candidate far from current centers among moderately good points\n                        dists_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        # pick among top distant indexes that are not too bad (rank by F and distance)\n                        cand_idx = np.argsort(dists_to_centers)[-max(1, min(20, len(X_arr))):]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[cidx] = X_arr[pick].copy()\n                        trust_radius[cidx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                        stagnation[cidx] = 0\n\n            # prune archive to keep memory manageable\n            if len(X) > self.max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (self.max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasionally nudge centers to the current best\n            if x_best is not None and len(centers) > 0:\n                for i in range(min(len(centers), max(1, len(X)//20))):\n                    if self.rng.rand() < 0.2:\n                        centers[i] = x_best.copy()\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm EnsemblePCAAdaptiveSurrogate scored 0.320 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "a9759013-c0d4-4e3f-816c-6216ddcbcef5", "operator": null, "metadata": {"aucs": [0.12917089000141524, 0.16390109213018467, 0.39928785260287847, 0.5470232049828483, 0.3917224651466302, 0.4659209994124124, 0.2594849354162153, 0.3653068254236246, 0.2776481213884433, 0.2013618379212696]}, "task_prompt": ""}
{"id": "8783517a-628f-4526-acc9-265eaaf01216", "fitness": "-inf", "name": "BSAN_St", "description": "The algorithm mixes global and local model-based moves with lightweight heuristics: it builds PCA-derived low-dimensional quadratic surrogates around the best points and also fits block-separable quadratic models on nearby samples, using ridge-regularized weighted least-squares and eigen-regularization to produce trust-clipped Newton-like steps. Initialization uses a maximin-ish pooled sampler to spread starting points, and the search alternates model-driven proposals with mirrored multi-scale directional probes and coordinate pattern search for robust local exploration. Adaptation is driven by per-dimension trust radii and a subspace radius that expand on improvements and shrink on failures, and occasional Student-t heavy-tailed teleports (controlled by probability, degrees of freedom and scale fraction) provide escapes from local traps. Practical budget-awareness and robustness are enforced via per-iteration eval caps, clipping to problem bounds, archive pruning to limit memory, and many tunable hyperparameters (subspace_factor, block_fraction, trust_init_frac, ridge, student_t_jump_prob, etc.) plus RNG seed control for reproducibility.", "code": "import numpy as np\n\nclass BSAN_St:\n    \"\"\"\n    BSAN_St: Block-Subspace Adaptive Newton with Student-t escapes\n\n    One-line: Combine PCA-subspace quadratic surrogates and blockwise separable\n    quadratic Newton steps (both ridge-regularized and trust-clipped), with\n    multi-scale mirrored directional probes, coordinate pattern search and\n    occasional Student-t teleports for robust global/local search under a strict budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_ratio=0.12, min_init=None, max_init_frac=0.4,\n                 subspace_factor=0.5, block_fraction=0.35,\n                 trust_init_frac=0.4, trust_min=1e-8, trust_max_frac=2.0,\n                 success_expand=1.5, failure_shrink=0.65,\n                 max_eval_per_iter=80, ridge=1e-8,\n                 student_t_jump_prob=0.12, student_t_df=3.0, student_t_scale_frac=0.45):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # initialization sizing\n        self.init_ratio = float(init_ratio)\n        self.min_init = int(min_init) if min_init is not None else max(10, 2 * self.dim)\n        self.max_init = max(20, int(max_init_frac * self.budget))\n\n        # subspace & block modelling\n        self.subspace_factor = float(subspace_factor)\n        self.block_fraction = float(block_fraction)\n\n        # trust region controls\n        self.trust_init_frac = float(trust_init_frac)\n        self.trust_min = float(trust_min)\n        self.trust_max_frac = float(trust_max_frac)\n        self.success_expand = float(success_expand)\n        self.failure_shrink = float(failure_shrink)\n\n        # evaluation and regularization\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        self.ridge = float(ridge)\n\n        # heavy-tailed escapes\n        self.student_t_jump_prob = float(student_t_jump_prob)\n        self.student_t_df = float(student_t_df)\n        self.student_t_scale_frac = float(student_t_scale_frac)\n\n        # RNG\n        self.rng = np.random.RandomState(self.seed)\n\n    def __call__(self, func):\n        rng = self.rng\n        n = int(self.dim)\n        budget = int(self.budget)\n\n        # bounds (Many Affine BBOB uses -5..5 usually)\n        if hasattr(func, \"bounds\"):\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        else:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n\n        rng_range = ub - lb\n        center_box = 0.5 * (lb + ub)\n\n        evals = 0\n        X = []\n        F = []\n\n        f_best = np.inf\n        x_best = None\n\n        # Maximin-ish initial sampling: draw a pool and greedily pick spread points\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        pool_mult = 6\n        pool_size = min(max(init_budget * pool_mult, init_budget + 10), 2000)\n        pool = rng.uniform(lb, ub, size=(pool_size, n))\n        chosen = []\n        if pool_size > 0:\n            idx0 = rng.randint(pool_size)\n            chosen.append(idx0)\n            while len(chosen) < init_budget:\n                pts = pool[np.array(chosen)]\n                # distances from pool to chosen set: compute min distance to chosen for each pool point\n                dists = np.min(np.linalg.norm(pool[:, None, :] - pts[None, :, :], axis=2), axis=1)\n                # choose the point with maximum minimal distance\n                idx = int(np.argmax(dists))\n                if idx in chosen:\n                    idx = int(rng.randint(pool_size))\n                chosen.append(idx)\n        for i in chosen:\n            if evals >= budget:\n                break\n            x = pool[i].copy()\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n\n        # trust radii per-dim and subspace radius\n        trust_radius = np.maximum(self.trust_init_frac * rng_range, 1e-12)\n        trust_max = np.maximum(self.trust_max_frac * rng_range, 1e-12)\n        trust_subspace = np.linalg.norm(trust_radius) * 0.6\n\n        # per-dimension coord steps\n        coord_steps = np.maximum(0.08 * trust_radius, 1e-12)\n\n        # helpers\n        def safe_eval(x):\n            nonlocal evals, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        def nearest_indices(xq, k):\n            if len(X) == 0:\n                return np.array([], dtype=int)\n            X_arr = np.asarray(X)\n            d = np.linalg.norm(X_arr - xq, axis=1)\n            idx = np.argsort(d)[:min(k, len(d))]\n            return idx\n\n        def compute_pca_basis(X_arr, F_arr, k):\n            m = X_arr.shape[0]\n            if m <= k:\n                return np.eye(n)[:, :k]\n            pick = min(max(2 * n, k + 5), m)\n            idx = np.argsort(F_arr)[:pick]\n            elites = X_arr[idx]\n            mu = np.mean(elites, axis=0)\n            C = np.cov((elites - mu).T)\n            try:\n                U, S, _ = np.linalg.svd(C)\n            except Exception:\n                U = np.eye(n)\n            return U[:, :k]\n\n        def fit_quadratic_subspace(X_nei, F_nei, B, x_center):\n            # fit quadratic in z = B^T (x - x_center), full quadratic in subspace with ridge\n            Z = (X_nei - x_center) @ B  # (m,k)\n            m, k = Z.shape\n            q_terms = k * (k + 1) // 2\n            P = 1 + k + q_terms\n            M = np.ones((m, P))\n            M[:, 1:1 + k] = Z\n            idx = 1 + k\n            for i in range(k):\n                for j in range(i, k):\n                    M[:, idx] = Z[:, i] * Z[:, j]\n                    idx += 1\n            y = np.asarray(F_nei).reshape(-1)\n            z_norm = np.linalg.norm(Z, axis=1)\n            w = 1.0 / (1e-12 + z_norm)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            b = W * y\n            ridge_mat = self.ridge * np.eye(P)\n            try:\n                lhs = A.T @ A + ridge_mat\n                rhs = A.T @ b\n                theta = np.linalg.solve(lhs, rhs).flatten()\n            except Exception:\n                return None\n            c = theta[0]\n            g = theta[1:1 + k].copy()\n            H = np.zeros((k, k))\n            idx = 1 + k\n            for i in range(k):\n                for j in range(i, k):\n                    val = theta[idx]\n                    if i == j:\n                        H[i, j] = val * 2.0\n                    else:\n                        H[i, j] = val\n                        H[j, i] = val\n                    idx += 1\n            return c, g, H\n\n        def fit_block_separable(center, block, neighbors_X, neighbors_F):\n            # build separable quadratic (a + b^T dx + 0.5 * h diag dx^2) using neighbors centered at center\n            dx = neighbors_X - center\n            m_pts = dx.shape[0]\n            bsz = block.size\n            p = 1 + bsz + bsz\n            M = np.ones((m_pts, p))\n            M[:, 1:1 + bsz] = dx[:, block]\n            M[:, 1 + bsz:1 + 2 * bsz] = 0.5 * (dx[:, block] ** 2)\n            y = np.asarray(neighbors_F).reshape(-1)\n            d_full = np.linalg.norm(neighbors_X - center, axis=1)\n            w = 1.0 / (d_full + 1e-12)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            bvec = W * y\n            try:\n                lhs = A.T @ A + self.ridge * np.eye(p)\n                rhs = A.T @ bvec\n                params = np.linalg.solve(lhs, rhs).flatten()\n                b_lin = params[1:1 + bsz]\n                h_diag = params[1 + bsz:1 + 2 * bsz]\n                h_reg = h_diag.copy()\n                h_reg[h_reg < 1e-8] = 1e-8\n                return b_lin, h_reg\n            except Exception:\n                return None\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            improved_iter = False\n\n            # Choose whether to attempt subspace surrogate or block surrogate (prefer subspace if archive rich)\n            X_arr = np.asarray(X) if len(X) > 0 else np.zeros((0, n))\n            F_arr = np.asarray(F) if len(F) > 0 else np.zeros((0,))\n\n            # Decide subspace dimension k\n            k = max(1, int(min(n, max(2, int(self.subspace_factor * np.sqrt(max(1, n)) * 2)))))\n            neighbors_needed = max(2 * n + 5, 6 + k * (k + 3) // 2)\n\n            # Try subspace quadratic surrogate first if enough data\n            used_surrogate = False\n            if len(X) >= neighbors_needed and x_best is not None and work_allow > 0:\n                used_surrogate = True\n                x_center = x_best.copy()\n                dists = np.linalg.norm(X_arr - x_center, axis=1)\n                idx_sorted = np.argsort(dists)[:min(len(X), neighbors_needed * 2)]\n                X_nei = X_arr[idx_sorted]\n                F_nei = F_arr[idx_sorted]\n                B = compute_pca_basis(X_arr, F_arr, k)\n                fit = fit_quadratic_subspace(X_nei, F_nei, B, x_center)\n                if fit is not None:\n                    c, g, H = fit\n                    # regularize H\n                    H = 0.5 * (H + H.T)\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(H)\n                        min_ev = np.min(eigvals)\n                        if min_ev < 1e-8:\n                            shift = (1e-8 - min_ev) + 1e-8\n                            H = H + shift * np.eye(k)\n                        z_star = -np.linalg.solve(H, g)\n                    except Exception:\n                        z_star = -g\n                    z_norm = np.linalg.norm(z_star)\n                    if z_norm > 0:\n                        if z_norm > trust_subspace:\n                            z_star = z_star * (trust_subspace / (z_norm + 1e-20))\n                    x_model = np.clip(x_center + (B @ z_star), lb, ub)\n                    if work_allow > 0 and evals < budget:\n                        if len(X) == 0 or not np.allclose(x_model, X[-1], atol=1e-12):\n                            out = safe_eval(x_model)\n                            work_allow -= 1\n                            if out is not None:\n                                f_model, x_model = out\n                                if f_model < f_best - 1e-12:\n                                    improved_iter = True\n                                    trust_subspace = min(trust_subspace * self.success_expand, np.linalg.norm(rng_range) * self.trust_max_frac + 1e-12)\n                                    trust_radius = np.minimum(trust_radius * self.success_expand, trust_max)\n                                    coord_steps = np.minimum(coord_steps * 1.2, trust_max)\n                                else:\n                                    trust_subspace = max(trust_subspace * self.failure_shrink, np.linalg.norm(rng_range) * self.trust_min)\n                                    trust_radius = np.maximum(trust_radius * self.failure_shrink, self.trust_min)\n                                    coord_steps = np.maximum(coord_steps * 0.9, self.trust_min)\n\n            # If subspace not used or unsuccessful, attempt blockwise separable quadratic steps\n            if (not improved_iter) and work_allow > 0:\n                # select a random block of dims\n                block_size = max(1, int(np.ceil(self.block_fraction * n)))\n                if block_size >= n:\n                    block = np.arange(n)\n                else:\n                    start = rng.randint(0, n)\n                    if start + block_size <= n:\n                        block = np.arange(start, start + block_size)\n                    else:\n                        block = np.concatenate((np.arange(start, n), np.arange(0, (start + block_size) % n)))\n                    # occasionally random scattered block\n                    if rng.rand() < 0.25:\n                        block = rng.choice(n, size=block_size, replace=False)\n                # prepare neighbors and fit\n                neighbors_needed_blk = max(3 * block_size + 1, 6 * block_size)\n                if x_best is None:\n                    center = center_box.copy()\n                else:\n                    center = x_best.copy()\n                idxs = nearest_indices(center, neighbors_needed_blk)\n                if len(idxs) >= (block.size + 2):\n                    X_nei = np.asarray(X)[idxs]\n                    F_nei = np.asarray(F)[idxs]\n                    fit_blk = fit_block_separable(center, block, X_nei, F_nei)\n                    if fit_blk is not None:\n                        b_lin, h_reg = fit_blk\n                        delta_block = - b_lin / (h_reg + 1e-20)\n                        limited = np.clip(delta_block, -trust_radius[block], trust_radius[block])\n                        x_model = center.copy()\n                        x_model[block] = x_model[block] + limited\n                        x_model = np.clip(x_model, lb, ub)\n                        if work_allow > 0 and evals < budget:\n                            if len(X) == 0 or not np.allclose(x_model, X[-1], atol=1e-12):\n                                out = safe_eval(x_model)\n                                work_allow -= 1\n                                if out is not None:\n                                    f_model, x_model = out\n                                    if f_model < f_best - 1e-12:\n                                        improved_iter = True\n                                        trust_radius = np.minimum(trust_radius * self.success_expand, trust_max)\n                                        trust_subspace = min(trust_subspace * 1.2, np.linalg.norm(rng_range) * self.trust_max_frac)\n                                    else:\n                                        trust_radius[block] = np.maximum(trust_radius[block] * self.failure_shrink, self.trust_min)\n\n            # If neither surrogate produced improvement, fallback to mirrored multi-scale directional probes\n            if (not improved_iter) and evals < budget and work_allow > 0:\n                num_directions = int(np.clip(6 + n // 3, 6, 20))\n                base_step = np.exp(np.mean(np.log(np.maximum(trust_radius, 1e-12))))\n                directions = []\n                for _ in range(num_directions):\n                    v = rng.randn(n)\n                    v /= (np.linalg.norm(v) + 1e-12)\n                    directions.append(v)\n                if rng.rand() < 0.4:\n                    for i in range(min(n, 4)):\n                        e = np.zeros(n); e[i] = 1.0\n                        directions.append(e)\n                scales = np.array([0.125, 0.25, 0.5, 1.0, 2.0])\n                rng.shuffle(directions)\n                for d in directions:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    for s in scales:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        step = s * base_step\n                        for sign in (+1.0, -1.0):\n                            x_try = (x_best if x_best is not None else center_box) + sign * d * step\n                            x_try = np.clip(x_try, lb, ub)\n                            if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                continue\n                            out = safe_eval(x_try)\n                            work_allow -= 1\n                            if out is None:\n                                break\n                            f_try, x_try = out\n                            if f_try < f_best - 1e-12:\n                                improved_iter = True\n                                trust_radius = np.minimum(trust_radius * 1.25, trust_max)\n                                trust_subspace = min(trust_subspace * 1.15, np.linalg.norm(rng_range) * self.trust_max_frac)\n                                break\n                        if improved_iter:\n                            break\n                    if improved_iter:\n                        break\n\n            # Coordinate pattern search\n            if evals < budget and work_allow > 0 and not improved_iter:\n                coords = np.arange(n)\n                max_coords = min(n, max(4, int(work_allow // 3)))\n                if max_coords < n:\n                    coords = rng.choice(coords, size=max_coords, replace=False)\n                for i in coords:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    step = max(coord_steps[i], 1e-12)\n                    for sign in (+1.0, -1.0):\n                        x_try = (x_best if x_best is not None else center_box).copy()\n                        x_try[i] = x_try[i] + sign * step\n                        x_try = np.clip(x_try, lb, ub)\n                        if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            improved_iter = True\n                            trust_radius = np.minimum(trust_radius * 1.25, trust_max)\n                            coord_steps[i] = np.minimum(coord_steps[i] * 1.4, trust_max[i])\n                            break\n                    if improved_iter:\n                        break\n\n            # Occasional Student-t heavy-tailed teleport to escape traps\n            if evals < budget and rng.rand() < self.student_t_jump_prob and work_allow > 0:\n                z = rng.randn(n)\n                chi2 = rng.chisquare(self.student_t_df, size=n)\n                t = z / (np.sqrt(chi2 / self.student_t_df) + 1e-12)\n                scale = self.student_t_scale_frac * np.maximum(rng_range, 1e-12)\n                jump = np.clip(t, -10, 10) * scale\n                x_jump = np.clip((x_best if x_best is not None else center_box) + jump, lb, ub)\n                if len(X) == 0 or not np.allclose(x_jump, X[-1], atol=1e-12):\n                    out = safe_eval(x_jump)\n                    work_allow -= 1\n                    if out is not None:\n                        f_jump, x_jump = out\n                        if f_jump < f_best - 1e-12:\n                            improved_iter = True\n                            trust_radius = np.minimum(np.maximum(trust_radius * 2.0, 0.3 * rng_range), trust_max)\n                            trust_subspace = min(trust_subspace * 2.0, np.linalg.norm(rng_range) * self.trust_max_frac)\n                            coord_steps = np.minimum(coord_steps * 1.5, trust_max)\n                        else:\n                            trust_radius = np.maximum(trust_radius * 0.7, self.trust_min)\n\n            # If nothing improved, shrink trust to encourage other moves\n            if not improved_iter:\n                trust_radius = np.maximum(trust_radius * self.failure_shrink, self.trust_min)\n                trust_subspace = max(trust_subspace * self.failure_shrink, np.linalg.norm(rng_range) * self.trust_min)\n\n            # Archive pruning to keep memory modest\n            max_archive = max(1200, 40 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best_count = min(250, max_archive // 4)\n                keep_best = idx_sorted[:keep_best_count]\n                rest = idx_sorted[keep_best_count:]\n                if len(rest) > 0:\n                    stride = max(1, len(rest) // (max_archive - keep_best_count))\n                    keep_rest = rest[::stride]\n                    keep_idx = np.concatenate([keep_best, keep_rest])[:max_archive]\n                else:\n                    keep_idx = keep_best\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n        # final return\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "a9759013-c0d4-4e3f-816c-6216ddcbcef5", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "446f78f2-083b-4293-8204-2cad655bf6b2", "fitness": 0.4100523541368388, "name": "BARS", "description": "The algorithm maintains a searchable center m, a global step-size sigma, per-coordinate scales s, and a lightweight learned rotation matrix R plus a block partition of coordinates to capture anisotropy and correlated directions while keeping updates cheap. Candidates are generated by a small, mirrored population of rotated, per-coordinate-scaled Gaussian steps with occasional heavy-tailed (Cauchy) jumps, DE-style archive differences, and block-swap permutations, all clipped to bounds. Adaptation is done by weighted recombination (log-weights) to form a new center, exponential-moving updates of s from selected local steps, and gentle updates of R via SVD/polar alignment on a small success buffer, while a bounded archive stores past points for DE mutations. Exploration vs exploitation is controlled by a temperature-based acceptance of worse centers, adaptive sigma multipliers based on success rate, reheating/perturbation and block rearrangements on stagnation, and periodic short 1D line-searches along the principal learned direction.", "code": "import numpy as np\n\nclass BARS:\n    \"\"\"\n    Blockwise Annealed Rotational Search (BARS)\n\n    Main ideas:\n    - Maintain a searchable center m, global step-size sigma and per-coordinate scales s.\n    - Maintain a lightweight rotation matrix R (initialized identity) that is adaptively updated\n      using an SVD/sketch of recent successful steps to capture correlated directions but applied\n      as sparse rotations (blockwise) to keep cost manageable.\n    - Partition coordinates into blocks; occasionally do block-swaps (permute coordinates inside blocks)\n      to enable combinatorial reassignments of coordinates.\n    - Use a population per generation (mirrored sampling) comprised of rotated Gaussian proposals,\n      occasional heavy-tailed Cauchy proposals, DE-style archive differences, and block-swap mutations.\n    - Use a temperature (simulated-annealing-like) to accept worse means probabilistically\n      for escaping traps, and adaptive reheating on stagnation.\n    - Periodically perform a short 1D exploratory line-search along the top learned direction.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, blocks=None, pop_factor=3):\n        \"\"\"\n        Args:\n            budget (int): function evaluation budget.\n            dim (int): problem dimension.\n            seed (int|None): RNG seed.\n            blocks (int|None): number of coordinate blocks; if None chosen as approx sqrt(dim).\n            pop_factor (float): multiplier to set population size ~ pop_factor*log(dim).\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population size heuristic (small)\n        self.lambda_ = max(4, int(pop_factor * max(4, np.log(max(2, self.dim)))))\n        # blocks (coordinate grouping)\n        if blocks is None:\n            self.n_blocks = max(1, int(np.ceil(np.sqrt(self.dim) / 1.5)))\n        else:\n            self.n_blocks = max(1, int(min(blocks, self.dim)))\n        # block partitions (basic contiguous partition initially)\n        self.blocks = []\n        sizes = [self.dim // self.n_blocks] * self.n_blocks\n        for i in range(self.dim % self.n_blocks):\n            sizes[i] += 1\n        idx = 0\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx+s)))\n            idx += s\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (expect -5..5 but read from func)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # state\n        m = np.random.uniform(lb, ub)                 # center\n        sigma = 0.18 * np.mean(ub - lb)               # initial step-size\n        s = np.ones(n)                                # per-coordinate scales\n        # rotation matrix R (start as identity)\n        R = np.eye(n)\n\n        # temperature for annealed acceptance of mean updates (higher allows uphill moves)\n        T0 = 1.0\n        Temp = T0\n\n        # buffers for learning rotation: store recent successful rotated steps (small)\n        success_buf = []\n        buf_max = max(20, 6 * int(np.ceil(np.sqrt(n))))  # keep small sketch\n\n        # small archive for DE-like differences\n        archive_X = []\n        archive_F = []\n\n        # parameters\n        p_cauchy = 0.10\n        cauchy_scale = 1.0\n        p_de = 0.18\n        F_de = 0.6\n        p_blockswap = 0.12\n        mirrored = True\n        # how often to update rotation\n        rot_update_every = max(1, int(3))\n        # how often to attempt line-search\n        line_search_every = max(20, int(5 * (n / 10 + 1)))\n        # stagnation controls\n        stagnation_limit = max(20, int(8 * n / max(1, len(self.blocks))))\n        stagn_count = 0\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation\n        x0 = np.clip(m, lb, ub)\n        f0 = func(x0)\n        evals += 1\n        archive_X.append(x0.copy())\n        archive_F.append(f0)\n        f_opt = f0\n        x_opt = x0.copy()\n        last_improv = evals\n\n        gen = 0\n        # main loop: generations producing lambda_ candidates clipped by remaining budget\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            # sample base normals\n            Z = np.random.randn(lam, n)\n            # container\n            X = np.zeros((lam, n))\n            proposals_info = []  # store y (pre-rotation scaled) to learn rotation\n            for i in range(lam):\n                z = Z[i].copy()\n                # mirrored sampling\n                if mirrored and (i % 2 == 1):\n                    z = -z\n\n                # apply per-coordinate scaling then rotate\n                y_local = (s * z)  # coordinate-wise scaled\n                y_rot = R.dot(y_local)   # rotated step in original coordinate system\n\n                # occasionally use heavy-tailed direction: sample scalar Cauchy and normalize z\n                if np.random.rand() < p_cauchy:\n                    r = np.random.standard_cauchy() * cauchy_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    dir_unit = (z / nz)\n                    # apply rotation to unit direction to get correlated jump direction\n                    y_rot = R.dot(s * dir_unit) * r * np.mean(s)\n\n                x = m + sigma * y_rot\n\n                # occasionally do DE-style archive mutation\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # block-swap mutation: permute entries inside a random block\n                if (np.random.rand() < p_blockswap):\n                    bidx = np.random.randint(len(self.blocks))\n                    b = self.blocks[bidx]\n                    if len(b) > 1:\n                        perm = np.random.permutation(len(b))\n                        x_block = x[b].copy()\n                        x[b] = x_block[perm]\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                X[i] = x\n                # store candidate step in rotated coordinates y_rot scaled by sigma (useful for learning)\n                proposals_info.append(y_rot.copy() / (sigma + 1e-20))\n\n            # Evaluate candidates (careful with budget)\n            Fvals = np.full(lam, np.inf)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy())\n                archive_F.append(fi)\n                # keep archive bounded\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi\n                    x_opt = xi.copy()\n                    last_improv = evals\n\n            # selection: pick top mu (weighted mean) — use log-weights similar to CMA\n            mu = max(1, lam // 2)\n            idx = np.argsort(Fvals)\n            sel = idx[:mu]\n            X_sel = X[sel]\n            Y_sel = np.vstack([proposals_info[j] for j in sel])  # shape mu x n\n            # recombination weights (log-style)\n            weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n            weights = weights / np.sum(weights)\n            # compute candidate new center (weighted)\n            m_new = np.sum(weights[:, None] * X_sel, axis=0)\n\n            # temperature-based acceptance for mean (Metropolis-like)\n            # accept if better else with prob exp(-(f_m_new - f_m_current)/Temp)\n            # approximate f at m_new by evaluating its clipped position if budget allows, otherwise use surrogate average\n            accept_mean = True\n            if evals < budget:\n                # evaluate m_new cheaply (but count eval)\n                xm_clip = np.clip(m_new, lb, ub)\n                fm_new = func(xm_clip)\n                evals += 1\n                archive_X.append(xm_clip.copy()); archive_F.append(fm_new)\n                if fm_new < f_opt:\n                    f_opt = fm_new; x_opt = xm_clip.copy(); last_improv = evals\n                # compare to current f0 (we may not maintain f(m) explicitly; use f_opt estimate for acceptance baseline)\n                # We keep a surrogate \"current center value\" estimate as f_center\n                # Initialize if missing\n                if 'f_center' not in locals():\n                    f_center = f0\n                delta = fm_new - f_center\n                if delta <= 0:\n                    accept_mean = True\n                    f_center = fm_new\n                    m = xm_clip.copy()\n                else:\n                    p = np.exp(-delta / max(1e-12, Temp))\n                    if np.random.rand() < p:\n                        accept_mean = True\n                        f_center = fm_new\n                        m = xm_clip.copy()\n                    else:\n                        accept_mean = False\n                        # keep old m, optionally nudge it slightly toward m_new with tiny probability\n                        if np.random.rand() < 0.05:\n                            m = 0.9 * m + 0.1 * m_new\n                # cooling\n                Temp *= 0.995\n                Temp = max(1e-6, Temp)\n            else:\n                # no budget left to evaluate center candidate; fallback to weighted recombination without center eval\n                m = np.clip(m_new, lb, ub)\n\n            # update per-coordinate scale s using exponential moving variance on selected rotated steps (Y_sel)\n            # map selected Y back to local pre-rotation coordinates for variance estimate: local = R^T y_rot\n            Ylocal = (R.T @ Y_sel.T).T  # mu x n\n            # weighted second moment\n            y2 = np.sum(weights[:, None] * (Ylocal ** 2), axis=0)  # shape n\n            c_s = 0.2\n            s2 = (1.0 - c_s) * (s ** 2) + c_s * (y2 + 1e-20)\n            s = np.sqrt(s2)\n            # clip s to avoid degenerate coordinates\n            s = np.clip(s, 1e-6, 1e3)\n\n            # store successful rotated directions (use best half as \"successes\")\n            half = max(1, mu // 2)\n            for j in range(half):\n                success_buf.append(proposals_info[sel[j]].copy())\n                if len(success_buf) > buf_max:\n                    success_buf.pop(0)\n\n            # occasionally refine rotation matrix R from success buffer using SVD on a small sketch\n            gen += 1\n            if (len(success_buf) >= min(6, n)) and (gen % rot_update_every == 0):\n                # Build data matrix (n x m)\n                Ymat = np.vstack(success_buf).T  # n x m\n                # center\n                Ymat = Ymat - np.mean(Ymat, axis=1, keepdims=True)\n                try:\n                    # economy SVD compute top-k where k small\n                    k_take = min(max(1, int(np.ceil(np.sqrt(n)))), Ymat.shape[1])\n                    U_s, S_s, Vt_s = np.linalg.svd(Ymat, full_matrices=False)\n                    U_top = U_s[:, :k_take]  # n x k\n                    # incorporate top modes into R by gently rotating R towards U_top\n                    # compute a small rotation matrix Q that maps current first k columns to U_top\n                    # Let A = R[:, :k_take] (current subspace), want to align A -> U_top\n                    A = R[:, :k_take]\n                    # compute cross-covariance and small correction via polar decomposition\n                    M = U_top.T @ A\n                    try:\n                        U_m, _, Vt_m = np.linalg.svd(M)\n                        Qsmall = U_m @ Vt_m  # k x k orthonormal\n                        # Apply correction: update R columns 0:k_take\n                        R[:, :k_take] = U_top @ Qsmall\n                        # Re-orthonormalize R to ensure orthonormality (cheap QR)\n                        Qr, _ = np.linalg.qr(R)\n                        R = Qr[:, :n]\n                    except np.linalg.LinAlgError:\n                        # fallback small random perturbation\n                        R = R + 0.01 * np.random.randn(n, n)\n                        Qr, _ = np.linalg.qr(R)\n                        R = Qr[:, :n]\n                except np.linalg.LinAlgError:\n                    # skip rotation update on failure\n                    pass\n\n            # small adaptive multiplicative update of sigma based on success rate in this generation\n            successes = np.sum(Fvals < (f_opt + 1e-12))\n            # heuristic: if many successes shrink, if few then expand\n            if lam > 0:\n                psucc = successes / lam\n                if psucc > 0.2:\n                    sigma *= 0.95\n                elif psucc < 0.05:\n                    sigma *= 1.08\n                else:\n                    sigma *= 1.0\n            # clamp sigma\n            sigma = np.clip(sigma, 1e-12, 5.0 * np.mean(ub - lb) + 1e-12)\n\n            # stagnation detection: if too long without improvement, reheating and perturb rotation/blocks\n            if (evals - last_improv) > stagnation_limit:\n                stagn_count += 1\n                # reheat temperature and inflate sigma\n                Temp = max(Temp, 0.5 + 0.1 * stagn_count)\n                sigma *= 1.5\n                # randomize one block partition (swap between two blocks)\n                if np.random.rand() < 0.5 and len(self.blocks) > 1:\n                    b1, b2 = np.random.choice(len(self.blocks), size=2, replace=False)\n                    # swap a random element between blocks if possible\n                    if len(self.blocks[b1]) > 0 and len(self.blocks[b2]) > 0:\n                        i1 = np.random.choice(self.blocks[b1])\n                        i2 = np.random.choice(self.blocks[b2])\n                        # swap positions in blocks\n                        self.blocks[b1].remove(i1); self.blocks[b1].append(i2)\n                        self.blocks[b2].remove(i2); self.blocks[b2].append(i1)\n                # perturb rotation lightly\n                R = R + 0.02 * np.random.randn(n, n)\n                Qr, _ = np.linalg.qr(R)\n                R = Qr[:, :n]\n                # reset success buffer\n                success_buf = []\n                last_improv = evals\n\n            # periodic short line-search along top learned direction\n            if (gen % line_search_every == 0) and (evals < budget):\n                # pick top principal direction as first column of R (if meaningful)\n                d = R[:, 0]\n                # sample few points along + and - direction within a window respecting bounds and budget\n                steps = [0.5, 0.25, -0.25, -0.5]\n                for alpha in steps:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + sigma * alpha * d * np.mean(s), lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = fp; x_opt = probe.copy(); last_improv = evals\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # avoid infinite loop: exit if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm BARS scored 0.410 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "1cd18b78-4292-4835-bd6a-f1ed9968d5ee", "operator": null, "metadata": {"aucs": [0.15111303134052145, 0.1665772515415086, 0.34920883112187906, 0.774098623853865, 0.3270608820236981, 0.8642169956895722, 0.26349262679129215, 0.3141154393112172, 0.7104751781485399, 0.18016468154629428]}, "task_prompt": ""}
{"id": "a99b834f-9e23-4db3-a765-918a3d6daefd", "fitness": 0.18279239668454006, "name": "OSDAS", "description": "The algorithm is a hybrid that samples steps as a mixture of axis-wise scaled Gaussian noise (per-coordinate scales D) and low-rank correlated directions learned from an incremental covariance S whose top-k eigenvectors U are refreshed periodically, with mirrored sampling to reduce variance and occasional heavy-tailed (Cauchy) jumps and DE-style archive differences to boost exploration. Global step-size sigma is adapted with a path-length style update (ps) using modified constants, while D is adapted robustly via MAD blending (c_d small for inertia) and the incremental S uses a modest learning rate (alpha_S) to slowly capture dominant subspace structure. A bounded archive supplies candidates for DE mutations and stagnation nudges, and stagnation is handled by inflating sigma, nudging the mean toward archive points, and damping S to encourage new subspace discovery; all candidate points and the mean are kept strictly within problem bounds. Parameter choices bias toward conservative local adaptation (sigma init = 0.18*range, c_d=0.10, alpha_S=0.06) with moderate exploration probabilities (p_cauchy=0.08, p_de=0.25), a small-to-moderate population scaling, and a subspace rank tied to ~0.7*sqrt(n) to balance expressiveness and cost.", "code": "import numpy as np\n\nclass OSDAS:\n    \"\"\"\n    OSDAS (Oja-incremental Subspace Differential Adaptive Search)\n\n    Main idea:\n      - Maintain per-coordinate scales D (axis-wise) and a low-rank correlated subspace U\n        learned from an incremental covariance matrix S. The top-k eigenvectors of S\n        form U (updated periodically).\n      - Sample steps as a mixture: D * N(0,I) + gamma * U * N(0,I_k). Mirror samples\n        to reduce variance. Occasionally apply a heavy-tailed Cauchy jump and DE-style\n        archive differences to boost exploration.\n      - Adapt global step-size sigma using a path-length style update (CMA-inspired),\n        but with a slightly different scaling constant. Diagonal scales D are adapted\n        using a robust moving-median absolute deviation (MAD) update.\n      - Subspace S is updated incrementally (low cost) and eigen-decomposed every\n        few generations to refresh U (avoids full SVD on every step).\n\n    Main tunable parameters (defaults set in __init__):\n      - lambda_: population size (depends on dim)\n      - mu: number of recombined parents\n      - weights: recombination weights (exponential-type here)\n      - mu_eff: effective mu\n      - cs, damps, chi_n: path-length constants (different formulation from ASRES)\n      - c_d: diagonal adaptation rate (MAD blending)\n      - alpha_S: incremental covariance learning rate for low-rank S\n      - p_cauchy: probability of heavy-tailed jump\n      - p_de: prob of DE-style archive difference mutation\n      - F_de_base: base DE factor (randomized per mutation)\n      - subspace_k: rank of subspace\n      - subspace_update_every: how often to eigen-decompose S\n      - mirrored: whether to use mirrored sampling\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # Population sizing (different scaling than ASRES)\n        # A bit larger baseline and milder log scaling\n        self.lambda_ = max(8, int(8 + np.floor(2.0 * np.log(max(1, self.dim)))))\n        self.mu = max(1, self.lambda_ // 3)\n\n        # Subspace rank\n        if subspace_k is None:\n            # choose somewhat smaller subspace than sqrt(n) to be different\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim) * 0.7)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # algorithm hyper-parameters (tunable)\n        self.c_d = 0.10            # diagonal MAD blending rate (smaller, more inertia)\n        self.alpha_S = 0.06        # incremental covariance learning rate for S\n        self.p_cauchy = 0.08       # probability for Cauchy jump (slightly less)\n        self.p_de = 0.25           # probability of applying DE-style archive mutation\n        self.F_de_base = 0.65      # base DE factor (will be randomized a bit)\n        self.mirrored = True\n        self.subspace_update_every = 7  # eigen-decompose S every few generations\n        self.stagnation_mult = 2.0  # multiplier to inflate sigma on stagnation\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (Many Affine BBOB typically -5..5 but read from func)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n\n        # recombination weights: exponentially decaying (different from log-weights)\n        # sharper concentration on top individuals\n        ranks = np.arange(mu)\n        weights = np.exp(-ranks / max(1.0, (mu / 3.0)))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # Path-length style constants (different formula)\n        cs = 0.30 * mu_eff / (n + mu_eff + 1.0)\n        damps = 1.0 + cs + 0.5  # slightly different damping\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (2.0 * n))\n\n        # initial state\n        m = np.random.uniform(lb, ub)                      # initial mean\n        sigma = 0.18 * np.mean(ub - lb)                    # initial step-size (different scale)\n        D = np.ones(n)                                     # per-coordinate scales (std-like)\n        ps = np.zeros(n)                                   # evolution path for sigma\n\n        # low-rank machinery: incremental covariance S (n x n) and eigenvectors U\n        S = np.zeros((n, n), dtype=float)\n        if self.k >= 1:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                U, _ = np.linalg.qr(rand_mat)\n                U = U[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        # archive for DE-style mutations\n        archive_X = []\n        archive_F = []\n        archive_max = 5000\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n\n        # Evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            if fm < f_opt:\n                f_opt = fm\n                x_opt = xm.copy()\n                last_improvement_eval = evals\n\n        # stagnation threshold in evaluations (different heuristic)\n        stagnation_thresh = max(8, int(3 * n / max(1, self.k)))\n\n        # main loop\n        generation = 0\n        while evals < budget:\n            generation += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # produce base gaussian samples\n            base_z = np.random.randn(current_lambda, n)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))  # effective y = (x-m)/sigma\n\n            for k_idx in range(current_lambda):\n                z = base_z[k_idx].copy()  # standard normal in R^n\n\n                # low-rank component draw\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U @ z_low                    # shape (n,)\n                    # different mixing weight: proportional to median of D, scaled down by k\n                    gamma = 0.6 * np.median(D) / (1.0 + 0.3 * self.k)\n                    y = D * z + gamma * low\n                else:\n                    y = D * z\n\n                # occasional heavy-tailed jump (Cauchy) in a random direction\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy()\n                    nz = np.linalg.norm(z) + 1e-20\n                    # scale Cauchy jump to median scale to avoid too large explosion\n                    y = np.sign(r) * (np.log1p(abs(r)) + 0.5) * (z / nz) * np.median(D)\n\n                # mirrored sampling pattern to reduce variance\n                if self.mirrored and (k_idx % 2 == 1):\n                    y = -y\n\n                x = m + sigma * y\n\n                # DE-style archive mutation sometimes\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    # slightly randomized F_de to add diversity\n                    F_de = self.F_de_base * (0.8 + 0.4 * np.random.rand())\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # clip to bounds strictly\n                x = np.clip(x, lb, ub)\n\n                arx[k_idx] = x\n                arz[k_idx] = y\n\n            # Evaluate candidates (sequentially, respecting budget)\n            arfit = np.full(current_lambda, np.inf)\n            for k_idx in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[k_idx]\n                f = func(x)\n                evals += 1\n                arfit[k_idx] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                # keep archive bounded\n                if len(archive_X) > archive_max:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # Selection and recombination (lower fitness is better)\n            idx = np.argsort(arfit)\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            m_old = m.copy()\n            # weighted recombination in x-space\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # update incremental covariance S using normalized y_w (avoid scale blowup)\n            norm_yw = np.linalg.norm(y_w) + 1e-20\n            v = y_w / norm_yw\n            S = (1.0 - self.alpha_S) * S + self.alpha_S * np.outer(v, v)\n\n            # update low-rank U (via eigen-decomposition of S) occasionally\n            if (generation % self.subspace_update_every) == 0:\n                try:\n                    # compute top-k eigenvectors of symmetric S\n                    # use eigh (returns ascending eigenvalues)\n                    eigvals, eigvecs = np.linalg.eigh(S)\n                    # take largest k eigenvectors\n                    if eigvecs.shape[1] >= self.k:\n                        U_new = eigvecs[:, -self.k:]\n                        # ensure orthonormality (eigh gives orthonormal columns)\n                        U = U_new.copy()\n                except np.linalg.LinAlgError:\n                    # keep previous U in case of failure\n                    pass\n\n            # update path ps: slightly different scaling (divide by sqrt(D) rather than D)\n            inv_sqrtD = 1.0 / (np.sqrt(D) + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * inv_sqrtD)\n            norm_ps = np.linalg.norm(ps)\n\n            # sigma adaptation (slightly different exponent constants)\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            # keep sigma in reasonable bounds\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e3 * np.mean(ub - lb) + 1e-12)\n\n            # robust diagonal adaptation using MAD of selected y's (median absolute deviation)\n            # compute median and MAD across selected y vectors\n            if y_sel.shape[0] >= 1:\n                med = np.median(y_sel, axis=0)\n                mad = np.median(np.abs(y_sel - med), axis=0)\n                # convert MAD to approximate std-scale (1.4826 factor) then blend with D\n                approx_std = 1.4826 * (mad + 1e-20)\n                # blend (exponential-like but via linear blending to keep simplicity)\n                D = (1.0 - self.c_d) * D + self.c_d * (approx_std + 1e-12)\n            else:\n                # small random jitter if no selected dims\n                D = D * (1.0 + 1e-6 * (np.random.randn(n)))\n\n            # ensure D positive and bounded\n            D = np.maximum(D, 1e-12)\n            D = np.minimum(D, 1e3 * np.ones_like(D))\n\n            # stagnation detection: if no improvement for many evals, boost exploration\n            if (evals - last_improvement_eval) > stagnation_thresh:\n                # increase sigma and nudge mean towards a random archive point (if exists)\n                sigma *= self.stagnation_mult\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                # reset S modestly to encourage new subspace discovery\n                S *= 0.5\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # safety: if epsilon budget left break (loop top will exit)\n            if evals >= budget:\n                break\n\n        # return best found (ensure result types)\n        if x_opt is None:\n            # fallback: return current mean evaluation if nothing improved\n            xm = np.clip(m, lb, ub)\n            f_final = func(xm)\n            return float(f_final), np.array(xm, dtype=float)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm OSDAS scored 0.183 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "1cd18b78-4292-4835-bd6a-f1ed9968d5ee", "operator": null, "metadata": {"aucs": [0.09222638636039748, 0.1589791583591743, 0.2786045326115406, 0.1837053516816315, 0.18109161587071998, 0.2211697026517978, 0.20528188620802879, 0.19672824862975424, 0.1758419422258667, 0.1342951422464893]}, "task_prompt": ""}
{"id": "e0803be8-3b99-4e93-af4c-1f31db4bf8ee", "fitness": 0.22976677717023652, "name": "OASLIS", "description": "OASLIS is a hybrid search that combines an online low-rank subspace (U) learned with a small-step Oja rule (oja_eta=0.08, k≈√dim) and a per-coordinate AdaGrad-like diagonal scaler (G init 1e-2, c_adagrad=0.2 → D=(G+eps)^-0.5) to give both coordinated low-rank moves and adaptive coordinate-wise steps. Sampling mixes the two components by an adaptive coefficient alpha (starts 0.6, updated with alpha_lr=0.07 from a short-term success window) and includes variance-reduction (mirrored/antithetic pairs), occasional heavy-tailed Cauchy jumps (p_cauchy=0.14) and DE-style archive recombination (p_de=0.18) to promote exploration and recombination. The method carries inertia via a momentum vector (momentum_decay=0.85) to exploit directional trends, adapts global step-size with a smoothed 1/5th-like multiplicative rule, keeps a bounded archive (limit 4000) and small population (λ≈4+3 log(dim)) for evaluation efficiency. Finally, it re-orthonormalizes U occasionally, uses weighted rank-based recombination for the mean, and triggers occasional Levy/inertia bursts on detected stagnation to escape local traps.", "code": "import numpy as np\n\nclass OASLIS:\n    \"\"\"\n    OASLIS: Oja-driven Adaptive Subspace & Lévy Inertia Search\n\n    Key novelties (compared to typical diagonal+low-rank schemes):\n    - Online Oja-rule update for a k-dimensional subspace U from successful steps (no costly SVD).\n    - AdaGrad-like per-coordinate accumulator G for diagonal scaling (D = (G + eps)^-0.5), providing adaptive learning rates.\n    - Momentum/inertia on the mean to carry promising directions forward and allow inertial escapes.\n    - Adaptive mixing coefficient alpha between diagonal noise and projected low-rank noise adjusted from recent success fraction.\n    - Tempered heavy-tailed Lévy/Cauchy leaps with inertia bursts to escape traps.\n    - DE-style archive recombination applied as a mutation operator, but with probabilistic crossover with candidate.\n    - Mirrors / antithetic pairing to reduce sampling variance.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, init_sigma=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size heuristic (small scaling with dim)\n        self.lambda_ = max(6, int(4 + np.floor(3 * np.log(self.dim))))\n        # selection size\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # initial global step-size if provided, else a fraction of typical BBOB range\n        if init_sigma is None:\n            self.init_sigma = 0.2 * 10.0  # default 0.2 * (ub-lb) where BBOB uses -5..5\n        else:\n            self.init_sigma = float(init_sigma)\n\n        # algorithm hyper-parameters (tunable)\n        self.p_cauchy = 0.14\n        self.cauchy_scale = 1.0\n        self.p_de = 0.18\n        self.F_de = 0.6\n        self.mirror = True\n\n        # Oja update rate for subspace (small)\n        self.oja_eta = 0.08\n\n        # inertia/momentum damping\n        self.momentum_decay = 0.85\n\n        # adaptive alpha learning rate (mix between diag and subspace)\n        self.alpha_lr = 0.07\n        self.alpha_target = 0.25  # target success rate that increases subspace usage\n\n        # AdaGrad-like smoothing\n        self.G_eps = 1e-8\n        self.c_adagrad = 0.2  # smoothing for accumulator update\n\n        # archive size limit\n        self.archive_limit = 4000\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (use func.bounds if present otherwise default -5..5)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initialize state\n        m = np.random.uniform(lb, ub)        # mean\n        sigma = max(1e-12, self.init_sigma)  # global step-size\n        # diagonal accumulator G and derived scale D\n        G = np.ones(n) * 1e-2\n        D = (G + self.G_eps) ** -0.5\n\n        # low-rank subspace U (n x k) orthonormal init via QR\n        rand = np.random.randn(n, self.k)\n        U, _ = np.linalg.qr(rand)\n        U = U[:, :self.k]\n\n        # momentum for mean (inertia)\n        v = np.zeros(n)\n\n        # adaptive mixing coefficient alpha in [0,1] (0 => diag only, 1 => subspace projected only)\n        alpha = 0.6\n\n        # archive for DE-like recombination\n        archive_X = []\n        archive_F = []\n\n        # weighting scheme: rank-based linear weights\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        ranks = np.arange(1, mu + 1)\n        weights = (mu + 1 - ranks).astype(float)  # linear decreasing\n        weights /= np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_best = fm\n            x_best = xm.copy()\n\n        # short-term success counter for alpha adaptation\n        recent_successes = []\n        success_window = max(10, lam * 2)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # prepare arrays\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # produce base gaussian samples\n            base_z = np.random.randn(current_lambda, n)\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # project z on learned subspace and orthogonal complement\n                proj_low = U @ (U.T @ z)  # k-projection contribution\n                proj_high = z - proj_low   # remainder\n\n                # sample mixing: diagonal (adapted D) applied to high component, scaled subspace applied to low\n                # note: apply elementwise diag on proj_high to capture per-coordinate scaling\n                y_diag = D * proj_high\n                # scale subspace projection with separate scale (~mean(D) times alpha)\n                y_sub = np.mean(D) * proj_low\n\n                # combined step\n                y = (1.0 - alpha) * y_diag + alpha * y_sub\n\n                # occasional heavy-tailed Cauchy jump in a random direction (magnitude drawn from Cauchy)\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * np.mean(D)  # direction of z scaled by mean diag\n\n                # mirrored/antithetic sampling\n                if self.mirror and (i % 2 == 1):\n                    y = -y\n\n                # form candidate with momentum/inertia applied (inertia acts as drift on mean)\n                x = m + sigma * y + 0.4 * v\n\n                # DE-style archive recombination as mutation (probabilistic)\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    # blend via a probabilistic crossover (binomial-like per-dimension)\n                    mask = (np.random.rand(n) < 0.5)\n                    x[mask] += de_mut[mask]\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                # store candidate and its step y (approx (x-m)/sigma ignoring v)\n                arx[i] = x\n                arz[i] = y\n\n            # Evaluate candidates carefully without surpassing budget\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # selection and recombination (rank-based weights defined above)\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            m_old = m.copy()\n            # recombine new mean (weighted)\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # compute weighted mean step in y-space (approx)\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # compute success rate: fraction of offspring improving relative to previous mean fitness estimate\n            # use median or best to gauge; compare best offspring vs best-so-far (or vs m_old fitness if known)\n            # We track whether any selected offspring beat current best to update alpha\n            sel_best_f = np.min(arfit[sel])\n            success_flag = 1 if sel_best_f < f_best else 0  # conservative (beat global best)\n            # also consider relative improvement vs m_old if we had fm_old in archive: pick nearest archive point to m_old\n            # For simplicity, use relative improvement in selected set vs median of selection\n            # record whether any selected strictly improved over the mean's last known evaluation (approx)\n            recent_successes.append(1 if sel_best_f < f_best else 0)\n            if len(recent_successes) > success_window:\n                recent_successes.pop(0)\n            success_rate = float(np.mean(recent_successes)) if recent_successes else 0.0\n\n            # adapt alpha (more subspace weight when success rate is above target)\n            alpha *= np.exp(self.alpha_lr * (success_rate - self.alpha_target))\n            alpha = float(np.clip(alpha, 0.05, 0.97))\n\n            # update momentum v: inertia carries previous momentum plus new drift from y_w\n            v = self.momentum_decay * v + 0.9 * sigma * y_w\n\n            # update AdaGrad-like accumulator G with weighted second moment of selected y's (in original coordinate)\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            G = (1.0 - self.c_adagrad) * G + self.c_adagrad * (y2 + self.G_eps)\n            D = (G + self.G_eps) ** -0.5\n\n            # adapt global sigma based on success_rate (simple 1/5th-like rule smoothed)\n            # target success is 0.2; do multiplicative update\n            sigma *= np.exp(0.6 * (success_rate - 0.2) / max(0.05, np.sqrt(1.0 + n / 10.0)))\n            # clamp sigma to sane bounds relative to domain size\n            domain_scale = np.mean(ub - lb)\n            sigma = float(np.clip(sigma, 1e-12, 100.0 * domain_scale))\n\n            # Online Oja update for subspace U using weighted successful directions\n            # Use the weighted mean step y_w (centered) as signal vector\n            # Normalize signal\n            signal = y_w.copy()\n            if np.linalg.norm(signal) > 1e-12:\n                signal = signal / (np.linalg.norm(signal) + 1e-12)\n                # update each eigenvector via simplified Oja:\n                for j in range(U.shape[1]):\n                    u = U[:, j]\n                    proj = np.dot(u, signal)\n                    # Oja increment: u <- u + eta * proj * (signal - proj * u)\n                    u = u + self.oja_eta * proj * (signal - proj * u)\n                    # renormalize\n                    u = u / (np.linalg.norm(u) + 1e-12)\n                    U[:, j] = u\n                # re-orthonormalize occasionally to keep U stable\n                if np.random.rand() < 0.2:\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # occasional inertia burst & Levy escape if stagnating (no improvement on best for a while)\n            # detect stagnation based on absence of archive improvement in recent evaluations\n            stagnation_length = max(50, 5 * n)\n            if (len(archive_F) >= 2) and (evals % stagnation_length == 0):\n                # if no improvement in last stagnation_length evals, do a burst\n                last_vals = archive_F[-stagnation_length:]\n                if len(last_vals) == stagnation_length and np.min(last_vals) >= f_best - 1e-12:\n                    # burst: inflate sigma and perform a few temporary Levy jumps (without saving mean unless better)\n                    old_sigma = sigma\n                    sigma *= 3.0\n                    for _ in range(min(5, budget - evals)):\n                        z = np.random.randn(n)\n                        r = np.random.standard_cauchy() * 1.5\n                        y = r * (z / (np.linalg.norm(z) + 1e-12)) * np.mean(D)\n                        x_try = np.clip(m + sigma * y, lb, ub)\n                        f_try = func(x_try)\n                        evals += 1\n                        archive_X.append(x_try.copy()); archive_F.append(f_try)\n                        if f_try < f_best:\n                            f_best = f_try; x_best = x_try.copy()\n                            # adopt new mean and reset momentum\n                            m = x_try.copy()\n                            v = np.zeros(n)\n                    sigma = old_sigma  # restore nominal sigma\n\n            # keep mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # enforce budget check done by while\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm OASLIS scored 0.230 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "1cd18b78-4292-4835-bd6a-f1ed9968d5ee", "operator": null, "metadata": {"aucs": [0.143372966540405, 0.1552509041366863, 0.23979989032687798, 0.15802595092128746, 0.22134713795340188, 0.44844130421332284, 0.2600179352020505, 0.24645327730881328, 0.281068286137121, 0.1438901189623989]}, "task_prompt": ""}
{"id": "63bfe055-c94c-489c-a0d2-92204f5e88aa", "fitness": 0.19405922530196845, "name": "AMSDLR", "description": "AMSDLR blends adaptive per-coordinate scaling and path-length sigma control with a learned low-rank search subspace and DE/Levy-style perturbations to balance exploitation and exploration. It uses a small population (lambda ≈ 8+2·log n) with linear descending recombination weights and a compact subspace k = ceil(log1p(n)) learned from a rolling success_buffer via an inexpensive Gram eigen-decomposition; candidate steps mix per-coordinate Gaussian noise and low-rank directions with a randomized mixing coefficient. Exploration is enhanced by occasional Lévy-like heavy-tailed jumps (p_levy≈0.08) and relatively frequent archive-based DE differences (p_de≈0.35, F_de≈0.9), while mirrored (antithetic) sampling reduces variance. Robust per-coordinate scale updates use a MAD-based exponential smoothing (c_d≈0.15) together with path-length sigma adaptation (cs/damps/chi_n), an archive for diversity, strict bound clipping, and a stagnation policy that inflates sigma and partially restarts toward archived good points.", "code": "import numpy as np\n\nclass AMSDLR:\n    \"\"\"\n    AMSDLR - Adaptive Mixed Subspace Differential Evolution with Lévy Recombination\n\n    Main algorithm parameters (and meanings) -- deliberately different choices from ASRES:\n    - lambda_ : population size (here ~ 8 + 2*log(n) floor; (ASRES used ~4+3*log(n)))\n    - mu      : number of parents used for recombination (here ~ ceil(lambda_/3))\n    - k       : subspace dimension (here ~ max(1, ceil(log1p(n))) instead of ~sqrt(n))\n    - sigma   : global step-size (initialized to 0.15 * range, different multiplier)\n    - D       : per-coordinate std estimates updated via a MAD-like update (c_d = 0.15)\n    - p_levy  : probability of heavy-tailed Lévy-like jump (here 0.08 vs 0.12)\n    - p_de    : probability of using archive-difference mutation (here 0.35 vs 0.20)\n    - F_de    : DE factor for difference (here 0.9 vs 0.7)\n    - cs/damps/chi_n : path length control constants computed with a different cs formula\n    - success_buffer_size : controls low-rank learning frequency and buffer length\n    - subspace_update_every : frequency of subspace recompute (uses small-rank PCA via gram-eig)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population / recombination design (different scaling)\n        self.lambda_ = max(6, int(8 + np.floor(2.0 * np.log(max(1, self.dim)))))\n        self.mu = max(1, int(np.ceil(self.lambda_ / 3.0)))\n\n        # subspace dimension: prefer a small log-scale subspace (different from sqrt)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.log1p(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # parameters that will be used/modified (sensible defaults different from ASRES)\n        self.c_d = 0.15          # learning rate for per-coordinate scale (MAD-like)\n        self.p_levy = 0.08       # probability of Lévy-style jump\n        self.levy_scale = 1.2    # scale of heavy-tailed jump\n        self.p_de = 0.35         # probability to apply DE-like archive difference mutation\n        self.F_de = 0.9          # DE scaling factor\n        self.mirrored = True     # use antithetic pairs\n        self.max_archive = 5000\n\n    def __call__(self, func):\n        n = self.dim\n        budget = self.budget\n\n        # bounds (assume provided, else default -5..5)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n\n        # linear recombination weights (different from log-weights in ASRES)\n        weights = np.arange(mu, 0, -1.0)  # descending weights\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # alternative path-length constants (different cs formula)\n        cs = 0.4 / (n + 0.4)\n        damps = 1.0 + cs + 0.2 * np.sqrt(mu_eff / max(1.0, n))\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        m = np.random.uniform(lb, ub)                       # initialize mean uniformly within bounds\n        sigma = 0.15 * np.mean(ub - lb)                     # different starting multiplier\n        D = np.ones(n)                                      # per-coordinate std (1 = neutral)\n        # small orthonormal subspace (n x k)\n        if self.k >= 1:\n            R = np.random.randn(n, self.k)\n            try:\n                U, _ = np.linalg.qr(R)\n                U = U[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        ps = np.zeros(n)  # path for sigma control\n\n        # success buffer for subspace learning (store small weighted steps)\n        success_buffer = []\n        buffer_max = max(8 * self.k, 12)\n        subspace_update_every = max(1, int(7))  # less frequent updates than ASRES\n\n        # archive for DE differences\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation at mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            f = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(f)\n            f_opt = f\n            x_opt = xm.copy()\n\n        # stagnation controls (different thresholds)\n        last_improvement_eval = evals\n        stagnation_window = max(6, int(1.5 * n))  # if no improvement for this many evals, intervene\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # arrays for candidates\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))  # stored normalized step direction y such that x = m + sigma*y\n\n            # produce base normals; to add diversity we'll allow mixing weight alpha per candidate\n            base_z = np.random.randn(current_lambda, n)\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # construct low-rank contribution with a randomized mixing coefficient (different than fixed beta)\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U @ z_low\n                    # mixing: choose alpha in [0.3,0.9] to vary emphasis natively\n                    alpha = np.random.uniform(0.3, 0.9)\n                    y = alpha * (D * z) + (1.0 - alpha) * (np.mean(D) * low)\n                else:\n                    y = D * z\n\n                # occasional Lévy-like heavy-tailed jump (less frequent than ASRES)\n                if np.random.rand() < self.p_levy:\n                    # create a Cauchy-proportional random magnitude and random direction biased by D\n                    r = np.random.standard_cauchy() * self.levy_scale\n                    dirn = z / (np.linalg.norm(z) + 1e-20)\n                    y = r * dirn * (np.median(D) + 1e-10)\n\n                # mirrored (antithetic) pairs to reduce variance\n                if self.mirrored and (i % 2 == 1):\n                    y = -y\n\n                x = m + sigma * y\n\n                # sometimes add DE-style archive difference, but more often and stronger than ASRES\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # strict clipping\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates (respect budget)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection\n            idx = np.argsort(arfit)\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]  # selected y steps\n\n            m_old = m.copy()\n            # recombine new mean using the linear weights (works directly with x_sel)\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space (for adaptation)\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # update path ps (different scaling / normalization: divide by (D+eps) but use sqrt of D)\n            invD = 1.0 / (np.sqrt(D) + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invD)\n            norm_ps = np.linalg.norm(ps)\n\n            # sigma update (same exponential rule but different damps and cs)\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            # safeguard sigma\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 2.0 * np.mean(ub - lb) + 1e-12)\n\n            # Update per-coordinate D using a MAD-like robust estimate from selected y's\n            # compute coordinate-wise median absolute deviation of y_sel weighted\n            if y_sel.size > 0:\n                # robust center\n                med = np.median(y_sel, axis=0)\n                mad = np.median(np.abs(y_sel - med), axis=0)\n                # convert MAD to approximate std (multiplier ~1.4826 for normal)\n                approx_std = 1.4826 * (mad + 1e-12)\n                # combine with previous D using exponential smoothing (different c_d)\n                D = (1.0 - self.c_d) * D + self.c_d * (approx_std + 1e-12)\n\n            # store the weighted step into success buffer (used to learn subspace)\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace from success_buffer occasionally using small Gram eig (different SVD approach)\n            if (len(success_buffer) >= self.k) and (evals % subspace_update_every == 0):\n                # build Y (n x m)\n                Y = np.vstack(success_buffer).T  # n x m\n                # center columns\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                # compute small m x m gram matrix and eigen-decompose for top modes cheaply\n                try:\n                    G = Y.T @ Y  # m x m\n                    # ensure symmetry\n                    G = (G + G.T) / 2.0\n                    eigvals, eigvecs = np.linalg.eigh(G)\n                    # pick top-k eigenvectors in the original space: U = Y * v / sqrt(eig)\n                    order = np.argsort(eigvals)[::-1]\n                    take = min(self.k, len(order))\n                    U_new = np.zeros((n, self.k))\n                    for j in range(take):\n                        v = eigvecs[:, order[j]]\n                        lamj = eigvals[order[j]]\n                        if lamj > 1e-12:\n                            U_new[:, j] = (Y @ v) / np.sqrt(lamj)\n                        else:\n                            U_new[:, j] = np.random.randn(n)\n                    # orthonormalize columns (QR)\n                    try:\n                        Q, _ = np.linalg.qr(U_new)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        # fallback: keep old U\n                        pass\n                except np.linalg.LinAlgError:\n                    pass\n\n            # stagnation intervention (different policy): if no improvement for many evals, diversify\n            if (evals - last_improvement_eval) > stagnation_window:\n                # increase sigma moderately and random-restart a fraction toward a good archive point\n                sigma *= 2.0\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    # jump halfway towards that archive point\n                    m = 0.5 * m + 0.5 * archive_X[pick]\n                # clear success buffer to force fresh subspace learning\n                success_buffer = []\n\n            # enforce mean bounds\n            m = np.clip(m, lb, ub)\n\n            # loop continues until budget exhausted\n\n        # return best found\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AMSDLR scored 0.194 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "1cd18b78-4292-4835-bd6a-f1ed9968d5ee", "operator": null, "metadata": {"aucs": [0.10831121715850767, 0.1630371414676789, 0.25687262429070057, 0.20706007483139077, 0.19829271635279078, 0.2540861940692841, 0.21428681340354006, 0.20966493395561325, 0.1801856600368441, 0.1487948774533342]}, "task_prompt": ""}
{"id": "72f4ba5b-2276-4c3f-800f-269ed722d539", "fitness": 0.19107459393327347, "name": "HybridDiagSubspaceCMA", "description": "The optimizer is a hybrid of diagonal and low-rank covariance modeling: per-coordinate scaling D is adapted via an exponential moving second moment while a learned low-rank subspace U with singular scales S is updated by SVD on a buffer of recent successful steps (k ≈ ceil(sqrt(n))). It uses CMA-like log-weighted recombination, evolution-paths (ps, pc) and sigma control (chi_n, cs, damps) to adapt step-size, with mirrored sampling for variance reduction, occasional archive-based DE difference mutations and occasional Cauchy jumps for heavy-tailed exploration. Periodic eigen-reconditioning builds an approximate full covariance (diag(D^2)+U diag(S^2) U^T) to compute a stable inverse-sqrt for better conditioning, and simple stagnation detection inflates sigma and nudges the mean toward archive points for modest restarts. All sampling and updates respect box bounds via clipping, a population-size heuristic (lambda ≈ 4+3 log n) and budget-aware per-evaluation selection to guarantee the function-evaluation limit.", "code": "import numpy as np\n\nclass HybridDiagSubspaceCMA:\n    \"\"\"\n    Hybrid Diagonal+Subspace CMA optimizer for bounded continuous optimization.\n\n    Key ideas:\n    - Combine per-coordinate diagonal scaling D with a learned low-rank subspace U (k ~ sqrt(n)).\n    - Use CMA-like log-weighted recombination and evolution-path sigma control (ps).\n    - Mirrored sampling, archive-based DE difference mutations, occasional Cauchy jumps.\n    - Update D by exponential moving second-moment; update U and subspace scales via SVD on recent successful steps.\n    - Periodically recondition via an approximate full eigen-decomposition for numerical stability.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population size heuristic similar to CMA-ES\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n        # subspace dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # strategy parameters\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path control constants (CMA-like)\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        cc = (4.0 + mu_eff / n) / (n + 4.0 + 2.0 * mu_eff / n)\n        c1 = 2.0 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1.0 - c1, 2.0 * (mu_eff - 2.0 + 1.0 / mu_eff) / ((n + 2.0) ** 2 + mu_eff))\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.2 * np.mean(ub - lb)\n        # diagonal std (per-coordinate)\n        D = np.ones(n)\n        # low-rank orthonormal U (n x k)\n        if self.k >= 1:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand_mat)\n                U = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n        # subspace singular-value scales (positive)\n        S = np.ones(self.k)\n\n        ps = np.zeros(n)  # path for sigma\n        pc = np.zeros(n)  # path for covariance (used partially)\n\n        # buffers and archive\n        success_buffer = []  # store recent successful weighted y_w (shape n)\n        buffer_max = max(10 * self.k, 20)\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # behavior params\n        p_cauchy = 0.12\n        cauchy_scale = 1.0\n        p_de = 0.20\n        F_de = 0.7\n        mirrored = True\n\n        # eigen reconditioning frequency (in evaluations)\n        eig_every = max(50, 10 * n)\n        eigen_counter = 0\n        # approximate full covariance components for occasional exact invsqrtC\n        B_full = np.eye(n)\n        D_full = np.ones(n)\n        invsqrtC_full = np.eye(n)\n        use_full = False  # whether we have a recent full decomposition\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_opt = float(fm)\n            x_opt = xm.copy()\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # ensure even for mirrored pairing\n            if mirrored and (current_lambda % 2 == 1):\n                current_lambda -= 1\n                if current_lambda <= 0:\n                    current_lambda = 1\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))  # stored y = (x - m)/sigma approximation\n\n            # produce samples (mirrored pairs if enabled)\n            k_idx = 0\n            while k_idx < current_lambda and evals + (current_lambda - k_idx) > 0:\n                # base Gaussian components\n                z_diag = np.random.randn(n)\n                z_low = np.random.randn(self.k) if self.k > 0 else np.zeros(0)\n                # construct low-rank contribution\n                low = (U @ (S * z_low)) if self.k > 0 else 0.0\n                # diagonal part\n                y = D * z_diag + (0.9 * np.mean(D)) * low  # mix weight for low-rank\n                # occasional Cauchy jump\n                if np.random.rand() < p_cauchy:\n                    r = np.random.standard_cauchy() * cauchy_scale\n                    nz = np.linalg.norm(z_diag) + 1e-20\n                    y = r * (z_diag / nz) * np.mean(D)\n                # mirrored partner\n                x = m + sigma * y\n                # occasionally DE mutation\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    x = x + F_de * (archive_X[i1] - archive_X[i2])\n                x = np.clip(x, lb, ub)\n                arx[k_idx] = x\n                arz[k_idx] = y\n                k_idx += 1\n\n                # mirrored partner (-y) if room\n                if mirrored and k_idx < current_lambda:\n                    y2 = -y\n                    x2 = m + sigma * y2\n                    if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        x2 = x2 + F_de * (archive_X[i1] - archive_X[i2])\n                    x2 = np.clip(x2, lb, ub)\n                    arx[k_idx] = x2\n                    arz[k_idx] = y2\n                    k_idx += 1\n\n            # evaluate candidates (one by one to respect budget)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n\n            # selection & recombination among evaluated (some may be inf if budget ended early)\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            idx = np.argsort(arfit[valid_idx])\n            sel_idx_rel = valid_idx[idx[:min(mu, len(idx))]]\n            sel_count = len(sel_idx_rel)\n            if sel_count == 0:\n                break\n            x_sel = arx[sel_idx_rel]\n            y_sel = arz[sel_idx_rel]\n            # recombine mean\n            w = weights[:sel_count].copy()\n            w = w / np.sum(w)\n            m_old = m.copy()\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # weighted mean step (in y-space)\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n            # approximate inverse sqrt C by diag(1/D) for ps update (ignore low-rank to save cost)\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n            # hsig for covariance path\n            hsig = 1.0 if (norm_ps / np.sqrt(1.0 - (1.0 - cs) ** (2.0 * evals / max(1, lam))) / chi_n) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # adapt sigma\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e2 * np.mean(ub - lb) + 1e-12)\n\n            # update diagonal D via exponential moving second-moment using selected y's\n            c_d = 0.25\n            y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n\n            # store successful weighted step into buffer (scaled by sigma)\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace U and scales S periodically via SVD on buffer\n            if (len(success_buffer) >= self.k) and (evals % max(1, min(10, n)) == 0):\n                Y = np.vstack(success_buffer).T  # n x m\n                # center\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        S = (svals[:k_take] / (np.sqrt(max(1, Y.shape[1])) + 1e-12)).copy()\n                        # if less than desired k, pad\n                        if U.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U.shape[1]))\n                            U = np.hstack([U, pad])\n                            S = np.concatenate([S, np.ones(self.k - len(S))])\n                    # else keep previous\n                except np.linalg.LinAlgError:\n                    pass\n\n            # occasional reconditioning: compute approximate full covariance and eigen-decomp\n            eigen_counter += current_lambda\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                # build approximate covariance: diag(D^2) + U @ diag(S^2) @ U.T\n                try:\n                    cov_approx = np.diag(D ** 2)\n                    if self.k > 0:\n                        cov_approx = cov_approx + (U @ np.diag(S ** 2) @ U.T)\n                    # enforce symmetry\n                    cov_approx = 0.5 * (cov_approx + cov_approx.T)\n                    Dvals, B = np.linalg.eigh(cov_approx)\n                    Dvals = np.maximum(Dvals, 1e-20)\n                    D_full = np.sqrt(Dvals)\n                    B_full = B\n                    invsqrtC_full = (B_full * (1.0 / D_full)) @ B_full.T\n                    use_full = True\n                except np.linalg.LinAlgError:\n                    use_full = False\n                    B_full = np.eye(n)\n                    D_full = np.ones(n)\n                    invsqrtC_full = np.eye(n)\n\n            # occasionally perform a small covariance update on subspace with rank-mu using selected y_proj\n            # project selected y's onto U to refine subspace scales S\n            if self.k > 0 and sel_count > 0:\n                try:\n                    Yproj = (U.T @ y_sel.T)  # k x sel_count\n                    # compute weighted second moments in projected coords\n                    proj_mom = np.sum((w[None, :] * (Yproj ** 2)), axis=1)  # k\n                    # EMA on S^2\n                    beta_s = 0.2\n                    S2 = (1.0 - beta_s) * (S ** 2) + beta_s * (proj_mom + 1e-20)\n                    S = np.sqrt(S2)\n                except Exception:\n                    pass\n\n            # stagnation detection & modest restart/boost\n            # if no archive improvement for long, inflate sigma and nudge mean\n            if len(archive_F) > 0:\n                best_age = evals - archive_F.index(min(archive_F)) if (min(archive_F) in archive_F) else 1e9\n                # simpler stagnation: if no improvement for many evaluations, inflates\n                if evals > 100 and (evals - np.argmin(archive_F) > max(20, 5 * n)):\n                    sigma *= 1.6\n                    # nudge mean toward a random archive point\n                    if len(archive_X) > 0:\n                        pick = np.random.randint(len(archive_X))\n                        m = 0.6 * m + 0.4 * archive_X[pick]\n                    success_buffer = []\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # enforce budget loop condition\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDiagSubspaceCMA scored 0.191 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "1cd18b78-4292-4835-bd6a-f1ed9968d5ee", "operator": null, "metadata": {"aucs": [0.07791895631567136, 0.14994854905931831, 0.18726541048682144, 0.1256955333786285, 0.19629680584064646, 0.4114918034840578, 0.25371747203913375, 0.2577849624383559, 0.12978469622565825, 0.12084175006444287]}, "task_prompt": ""}
{"id": "b85551ec-5e7a-4fc0-91d6-9dc55c64f647", "fitness": "-inf", "name": "MemCovRSS", "description": "MemCovRSS performs budget-aware randomized low-dimensional subspace probing: it builds a k-dimensional basis (default k = ceil(sqrt(n))) mixing an LRU memory of recent successful unit directions with fresh Gaussian columns, and evaluates mirrored coefficient samples (probes ≈ probes_factor * k) lifted to the full space. Adaptation is lightweight-CMA-like: a rank‑one covariance C, evolution path ps and multiplicative sigma update steer step sizes, while periodic eigendecomposition refreshes principal axes (eig_every_factor * n) for sampling and ps calculations. Exploration/exploitation balance is enforced by per-coordinate RMS scaling, occasional DE-style difference mutations and heavy-tailed (Cauchy) jumps, short budget-aware line-searches along successful directions, and an inexpensive diagonal quadratic model fitted to archived best points for targeted proposals. Practical budget and memory safeguards include strict evaluation counting, archive pruning, small regularizers, and modest defaults (memory_size=8, probes_factor=6, p_de≈0.12, F_de≈0.6) tuned for efficient progress under tight evaluation limits.", "code": "import numpy as np\n\nclass MemCovRSS:\n    \"\"\"\n    MemCovRSS: Memory-Covariance Random-Subspace Search\n    One-line: Mirror-sampled low-dim subspace probes guided by an LRU direction memory and a lightweight\n    CMA-like covariance + evolution-path sigma adaptation; occasional DE-differences and short line-searches\n    provide exploration/exploitation balance under strict budget control.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=8, subspace_k=None, probes_factor=6,\n                 p_de=0.12, F_de=0.6, model_every=20, eig_every_factor=10):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.subspace_k = subspace_k  # if None, set to ceil(sqrt(n))\n        self.probes_factor = probes_factor  # how many probes per subspace dim (approx)\n        self.p_de = float(p_de)\n        self.F_de = float(F_de)\n        self.model_every = int(model_every)\n        self.eig_every_factor = int(eig_every_factor)\n\n    def __call__(self, func):\n        rng = np.random.RandomState(self.seed)\n        n = int(self.dim)\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        range_mean = np.mean(rng_range)\n        # initial point and evaluations tracking\n        evals = 0\n        X_archive = []\n        F_archive = []\n        # safe eval wrapper\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return None, None\n            x_clipped = np.clip(np.asarray(x, dtype=float), lb, ub)\n            try:\n                f = float(func(x_clipped))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x_clipped.copy()); F_archive.append(f)\n            return f, x_clipped\n\n        # Initialization\n        x_cur = rng.uniform(lb, ub)\n        out = safe_eval(x_cur)\n        if out[0] is None:\n            return float(np.inf), np.zeros(n)\n        f_cur, x_cur = out\n        f_best = float(f_cur); x_best = x_cur.copy()\n\n        # step-size and covariance state (lightweight)\n        sigma = max(1e-9, 0.38 * range_mean)\n        C = np.eye(n) * (sigma ** 2 * 0.5)  # initial modest covariance\n        ps = np.zeros(n)  # evolution path for sigma\n        # small adaptation constants (lightweight CMA-like)\n        mu_eff = 1.0\n        cs = 0.2\n        c1 = 0.12\n        damps = 1.0 + cs + 0.3\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # eigen/sqrt factors for sampling; updated periodically\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eig_counter = 0\n        eig_every = max(1, self.eig_every_factor * n)\n\n        # per-coordinate RMS-like scaling\n        coord_var = np.ones(n) * 1e-4\n        coord_alpha = 0.18\n        coord_eps = 1e-12\n\n        # LRU memory of successful unit directions\n        dir_memory = []\n\n        # basic short line search (budget-aware)\n        def short_line_search(x0, f0, d, init_step, max_evals=8):\n            nonlocal evals\n            if evals >= budget:\n                return None, None\n            d = np.asarray(d, dtype=float)\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            remaining = min(max_evals, budget - evals)\n            if remaining <= 0:\n                return None, None\n            # try initial forward and backward\n            a = 0.0; fa = f0\n            b = init_step\n            xb = np.clip(x0 + b * d, lb, ub)\n            out = safe_eval(xb)\n            if out[0] is None:\n                return None, None\n            fb, xb = out\n            remaining -= 1\n            if fb >= fa:\n                b = -init_step\n                xb = np.clip(x0 + b * d, lb, ub)\n                out = safe_eval(xb)\n                if out[0] is None:\n                    return None, None\n                fb, xb = out\n                remaining -= 1\n                if fb >= fa:\n                    return None, None\n            # simple golden-section in [0, b]\n            gr = (np.sqrt(5) - 1) / 2\n            left, right = 0.0, b\n            c = right - gr * (right - left)\n            dd = left + gr * (right - left)\n            xc = np.clip(x0 + c * d, lb, ub)\n            out = safe_eval(xc)\n            if out[0] is None:\n                return None, None\n            fc, xc = out; remaining -= 1\n            xd = np.clip(x0 + dd * d, lb, ub)\n            out = safe_eval(xd)\n            if out[0] is None:\n                return None, None\n            fd, xd = out; remaining -= 1\n            best_f = fa; best_x = x0.copy()\n            if fc < best_f:\n                best_f = fc; best_x = xc.copy()\n            if fd < best_f:\n                best_f = fd; best_x = xd.copy()\n            iters = 0\n            max_iters = remaining\n            while iters < max_iters and abs(right - left) > 1e-12:\n                iters += 1\n                if fc < fd:\n                    right = dd\n                    dd = c; fd = fc\n                    c = right - gr * (right - left)\n                    xc = np.clip(x0 + c * d, lb, ub)\n                    out = safe_eval(xc)\n                    if out[0] is None:\n                        break\n                    fc, xc = out\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    left = c\n                    c = dd; fc = fd\n                    dd = left + gr * (right - left)\n                    xd = np.clip(x0 + dd * d, lb, ub)\n                    out = safe_eval(xd)\n                    if out[0] is None:\n                        break\n                    fd, xd = out\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # periodic diagonal model like MDARSS (cheap) to propose candidate\n        def try_diagonal_model(x_center):\n            if len(X_archive) < (n + 2):\n                return None, None\n            k = min(len(X_archive), 2 * n + 5)\n            idx = np.argsort(F_archive)[:k]\n            Xm = np.array([X_archive[i] for i in idx])\n            Fm = np.array([F_archive[i] for i in idx])\n            A = np.hstack([Xm ** 2, Xm, np.ones((Xm.shape[0], 1))])\n            reg = 1e-6\n            try:\n                sol, *_ = np.linalg.lstsq(A.T @ A + reg * np.eye(A.shape[1]), A.T @ Fm, rcond=None)\n                sol = sol.flatten()\n                a = sol[:n]; b = sol[n:2 * n]\n                a_safe = a.copy()\n                small_mask = np.abs(a_safe) < 1e-8\n                if np.any(small_mask):\n                    a_safe[small_mask] = 1e-8 * np.sign(a_safe[small_mask] + 1e-12) + 1e-8\n                x_star = -0.5 * b / (a_safe + 1e-20)\n                x_prop = np.clip(x_center + 0.6 * (x_star - x_center), lb, ub)\n                f_prop, x_prop = safe_eval(x_prop)\n                return f_prop, x_prop\n            except Exception:\n                return None, None\n\n        # main loop\n        iter_count = 0\n        # default subspace dim\n        if self.subspace_k is None:\n            k_default = max(1, int(np.ceil(np.sqrt(n))))\n        else:\n            k_default = min(n, max(1, int(self.subspace_k)))\n\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n            # set subspace dim and probes\n            k = k_default\n            probes = min(max(4, self.probes_factor * k), remaining)\n            # build basis using memory + random columns\n            use_mem = min(len(dir_memory), k // 2)\n            cols = []\n            if use_mem > 0:\n                for i in range(use_mem):\n                    cols.append(dir_memory[i].copy())\n            needed = k - len(cols)\n            if needed > 0:\n                R = rng.randn(n, needed)\n                if cols:\n                    R = np.column_stack(cols + [R[:, j] for j in range(R.shape[1])])\n                Q, _ = np.linalg.qr(R, mode='reduced')\n                basis = Q[:, :k]\n            else:\n                Q, _ = np.linalg.qr(np.column_stack(cols), mode='reduced')\n                basis = Q[:, :k]\n\n            # generate mirrored coefficients and form candidate directions\n            half = (probes + 1) // 2\n            coeffs_list = [rng.standard_normal(k) for _ in range(half)]\n            coeffs_list = coeffs_list + [(-c) for c in coeffs_list]\n            coeffs_list = coeffs_list[:probes]\n\n            improved_in_round = False\n            successes = 0\n            for coeffs in coeffs_list:\n                if evals >= budget:\n                    break\n                # map to full space: use low-dim coefficients in basis then transform via B*D to reflect covariance\n                z = np.asarray(coeffs, dtype=float)\n                # sample direction in subspace and lift\n                d_sub = basis @ z\n                # incorporate B and D sampling: transform direction by current principal axes scaling (but keep deterministic)\n                d_full = (B * D) @ d_sub  # note: B*D shapes broadcast => scales columns; result n-vector\n                # bias by per-coordinate RMS\n                coord_scale = np.sqrt(coord_var + coord_eps)\n                d_full = d_full * coord_scale\n                if np.linalg.norm(d_full) < 1e-16:\n                    continue\n                d_unit = d_full / (np.linalg.norm(d_full) + 1e-20)\n                # choose scalar step uniformly in [-sigma, sigma]\n                alpha = rng.uniform(-sigma, sigma)\n                x_try = x_cur + alpha * d_unit\n\n                # DE-style archived difference mutation occasionally\n                if (len(X_archive) >= 2) and (rng.rand() < self.p_de):\n                    i1, i2 = rng.choice(len(X_archive), size=2, replace=False)\n                    de = self.F_de * (X_archive[i1] - X_archive[i2])\n                    x_try = x_try + 0.5 * de\n\n                # occasional memory Cauchy jump\n                if dir_memory and (rng.rand() < 0.12):\n                    u = dir_memory[rng.randint(len(dir_memory))]\n                    jump = np.tan(np.pi * (rng.rand() - 0.5))\n                    x_try = x_try + (0.6 * jump * sigma) * u\n\n                x_try = np.clip(x_try, lb, ub)\n                out = safe_eval(x_try)\n                if out[0] is None:\n                    break\n                f_try, x_try = out\n\n                # if improved relative to current, accept and update statistics\n                if f_try < f_cur - 1e-12:\n                    prev_x = x_cur.copy(); prev_f = f_cur\n                    x_cur = x_try.copy(); f_cur = f_try\n                    successes += 1; improved_in_round = True\n                    if f_try < f_best:\n                        f_best = f_try; x_best = x_try.copy()\n                    # store unit direction in memory (LRU)\n                    dir_succ = x_cur - prev_x\n                    dn = np.linalg.norm(dir_succ)\n                    if dn > 0:\n                        u = dir_succ / dn\n                        dir_memory.insert(0, u.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                    # update coord_var (RMS-like)\n                    y = (x_cur - prev_x) / (abs(alpha) + 1e-20)\n                    coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (y ** 2 + 1e-12)\n                    # update covariance (rank-one lightweight)\n                    y_C = ((x_cur - prev_x) / max(sigma, 1e-20))[:, None]\n                    C = (1.0 - c1) * C + c1 * (y_C @ y_C.T) * (sigma ** 2)\n                    # update evolution path for sigma using invsqrtC\n                    try:\n                        inv_y = invsqrtC @ (y_C.flatten())\n                    except Exception:\n                        inv_y = y_C.flatten()\n                    ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs)) * inv_y\n                    # adapt sigma\n                    norm_ps = np.linalg.norm(ps)\n                    sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n                    sigma = max(sigma, 1e-12 * range_mean)\n                    # try short line search along successful direction (budget aware)\n                    rem_b = budget - evals\n                    if rem_b >= 2:\n                        ls_max = min(8, rem_b)\n                        ls_out = short_line_search(x_cur, f_cur, u if dn > 0 else d_unit, init_step=abs(alpha) if abs(alpha) > 1e-12 else sigma, max_evals=ls_max)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                prev_x = x_cur.copy()\n                                x_cur = x_ls.copy(); f_cur = f_ls\n                                if f_ls < f_best:\n                                    f_best = f_ls; x_best = x_ls.copy()\n                                # additional covariance update\n                                y_C2 = ((x_cur - prev_x) / max(sigma, 1e-20))[:, None]\n                                C = (1.0 - c1) * C + c1 * (y_C2 @ y_C2.T) * (sigma ** 2)\n                                # small ps update\n                                try:\n                                    inv_y2 = invsqrtC @ y_C2.flatten()\n                                except Exception:\n                                    inv_y2 = y_C2.flatten()\n                                ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs)) * inv_y2\n                                norm_ps = np.linalg.norm(ps)\n                                sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n                                sigma = max(sigma, 1e-12 * range_mean)\n                else:\n                    # on failure slightly shrink sigma\n                    sigma *= 0.997\n                    sigma = max(sigma, 1e-12 * range_mean)\n\n                # periodic diagonal model attempt occasionally\n                if (iter_count % self.model_every == 0) and (budget - evals > 0) and (rng.rand() < 0.25):\n                    f_m, x_m = try_diagonal_model(x_cur)\n                    if f_m is not None and f_m < f_cur - 1e-12:\n                        prev_x = x_cur.copy()\n                        x_cur = x_m.copy(); f_cur = f_m\n                        if f_m < f_best:\n                            f_best = f_m; x_best = x_m.copy()\n                        # update covariance and ps similarly\n                        y_C = ((x_cur - prev_x) / max(sigma, 1e-20))[:, None]\n                        C = (1.0 - c1) * C + c1 * (y_C @ y_C.T) * (sigma ** 2)\n                        try:\n                            inv_y = invsqrtC @ y_C.flatten()\n                        except Exception:\n                            inv_y = y_C.flatten()\n                        ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs)) * inv_y\n                        norm_ps = np.linalg.norm(ps)\n                        sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n                        sigma = max(sigma, 1e-12 * range_mean)\n\n            # periodic eigendecomposition to update B,D,invsqrtC for sampling & ps updates\n            eig_counter += 1\n            if eig_counter >= eig_every:\n                eig_counter = 0\n                # symmetrize\n                C = 0.5 * (C + C.T)\n                # regularize\n                try:\n                    vals, vecs = np.linalg.eigh(C)\n                    vals = np.maximum(vals, 1e-20)\n                    D = np.sqrt(vals)\n                    B = vecs\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    B = np.eye(n); D = np.ones(n); invsqrtC = np.eye(n)\n\n            # prune archive occasionally to limit memory\n            max_archive = max(2000, 50 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                keep_rest = rest[::max(1, len(rest) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])[:max_archive]\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # tiny safeguard on sigma\n            sigma = max(sigma, 1e-12 * range_mean)\n            # early exit if sufficiently good\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 1179, in numpy.random.mtrand.RandomState.uniform, the following error occurred:\nOverflowError: Range exceeds valid bounds", "error": "In the code, line 1179, in numpy.random.mtrand.RandomState.uniform, the following error occurred:\nOverflowError: Range exceeds valid bounds", "parent_ids": "1cd18b78-4292-4835-bd6a-f1ed9968d5ee", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "b8fbedfd-c7e0-4d22-8c2c-26dc1beb0b2d", "fitness": "-inf", "name": "AMESR", "description": "The algorithm builds a low-rank PCA subspace from a short LRU memory of recent successful unit steps (mem_size=20) and samples mirrored Gaussian proposals inside that subspace (k ≈ ceil(n^0.6), probes = max(8,4*k)) to focus exploration on promising directions while adding small orthogonal jitter for diversity. It maintains a decaying directional tabu (bad_dirs, threshold 0.92 and decay 0.9) to avoid repeatedly trying unproductive directions, and uses a simulated-annealing style acceptance (temp_init ~ domain scale, temp_decay = 4/budget with a small extra acceptance multiplier) so it can occasionally accept worsenings to escape traps. Promising moves trigger a budget-aware analytic 1D parabolic refinement using at most a few extra evaluations to polish the step, and successful steps are stored and recency-weighted for future PCA. Step sizes are adapted aggressively (grow=1.30 on success, shrink=0.65 on failure) within trust-region caps (step init = 0.2·domain_mean, min_step tiny, max_step = 4·domain_mean), with occasional full-space Gaussian polishing and best-biased restarts when stagnation is detected, and all evaluations are clipped to bounds and strictly budget-limited.", "code": "import numpy as np\n\nclass AMESR:\n    \"\"\"\n    AMESR: Adaptive Mirror Ensemble with Surrogate Refinement\n\n    Key ideas:\n      - Maintain a small LRU archive of recent successful steps and derive a low-rank PCA subspace (adapted covariance).\n      - In each round sample paired (mirrored) proposals in the subspace to encourage exploration in balanced directions.\n      - Use a directional-tabu list (decaying) to avoid repeatedly sampling unproductive directions.\n      - Accept improvements always; accept worsened proposals probabilistically via a temperature schedule (simulated annealing) to escape local traps.\n      - When a promising direction is found, perform a budget-aware analytic parabolic 1D refinement (using <=2 extra evaluations).\n      - Aggressive but safe step adaptation (grow on success, shrink on failure) with trust-region caps and occasional full-space polishing.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 mem_size=20, seed_offset=0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.mem_size = int(mem_size)\n        if seed is not None:\n            np.random.seed(seed + seed_offset)\n\n    def __call__(self, func):\n        n = self.dim\n        # allow scalar or vector bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # Domain mean used for scaling\n        domain_mean = np.mean(ub - lb)\n        # evaluation accounting\n        evals = 0\n\n        def clip_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # safe evaluation wrapper\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initialize at random location\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best if x_best is not None else np.inf), np.array(x_best if x_best is not None else x_cur)\n\n        # algorithm state\n        step = 0.2 * domain_mean                      # initial step\n        min_step = 1e-6 * max(1.0, domain_mean)\n        max_step = 4.0 * domain_mean                  # trust-region cap\n        mem_success = []                              # LRU of unit successful steps\n        mem_weights = []                              # recency weights for PCA\n        bad_dirs = []                                 # decaying tabu directions (unit vectors)\n        bad_decay = 0.9                               # decay factor per round\n        round_failures = 0\n        stagnation_limit = max(20, int(6 + np.log(1 + n) * 4))\n        rounds_since_improve = 0\n        max_rounds = max(1, self.budget // 5)\n\n        # parameters\n        # effective subspace dimension scaling different from MASTL\n        k = max(1, min(n, int(np.clip(int(np.ceil(n ** 0.6)), 1, n))))\n        probes = max(8, 4 * k)                        # number of mirrored probe pairs per round\n        grow = 1.30                                   # aggressive grow on success\n        shrink = 0.65                                 # aggressive shrink on failure\n        temp_init = max(1.0, domain_mean)             # initial temperature\n        temp_decay = 4.0 / max(1.0, self.budget)      # controls annealing schedule\n\n        # helper: compute principal components from recent weighted steps\n        def make_basis(mem_success, mem_weights, k_target):\n            # If not enough memory, produce random orthonormal basis\n            if len(mem_success) == 0:\n                R = np.random.randn(n, k_target)\n                Q, _ = np.linalg.qr(R)\n                return Q[:, :k_target]\n            # build matrix of weighted steps\n            S = np.vstack(mem_success) * np.array(mem_weights)[:, None]\n            # compute (economy) SVD\n            try:\n                U, svals, Vt = np.linalg.svd(S, full_matrices=False)\n            except Exception:\n                R = np.random.randn(n, k_target)\n                Q, _ = np.linalg.qr(R)\n                return Q[:, :k_target]\n            # U contains left singular vectors (n x r) as columns; select top k_target\n            r = min(U.shape[1], k_target)\n            U_k = U[:, :r]\n            if r < k_target:\n                # pad with random directions orthonormal to existing\n                R = np.random.randn(n, k_target - r)\n                if r > 0:\n                    R = R - U_k @ (U_k.T @ R)\n                Q2, _ = np.linalg.qr(R)\n                return np.column_stack((U_k, Q2[:, :k_target - r]))\n            return U_k[:, :k_target]\n\n        # helper: check if direction is near any bad direction\n        def is_taboo(dir_unit):\n            for bd, w in bad_dirs:\n                if abs(np.dot(dir_unit, bd)) > 0.92:  # nearly parallel -> taboo\n                    return True\n            return False\n\n        # cheap parabolic 1D refinement along direction d (unit)\n        def parabolic_refine(x0, f0, d, s):\n            # Fit parabola using f(-s), f(0)=f0, f(+s) and evaluate minimizer if valid.\n            if evals >= self.budget:\n                return None, None\n            # ensure s > 0\n            s = abs(float(s))\n            xa = clip_to_bounds(x0 - s * d)\n            fa, _ = safe_eval(xa)\n            if fa is None:\n                return None, None\n            xb = clip_to_bounds(x0 + s * d)\n            fb, _ = safe_eval(xb)\n            if fb is None:\n                return None, None\n            # fit parabola f(alpha) = A*alpha^2 + B*alpha + C with alpha in [-s,0,s]\n            # C = f0\n            # use finite differences to estimate A and B:\n            # f(+s) = A s^2 + B s + C\n            # f(-s) = A s^2 - B s + C\n            A = (fb + fa - 2.0 * f0) / (2.0 * s * s)\n            B = (fb - fa) / (2.0 * s)\n            if A <= 1e-16:\n                return None, None\n            alpha_min = -B / (2.0 * A)\n            # only accept minimizer inside [-2s, 2s] to avoid wild moves\n            if not (-2.0 * s <= alpha_min <= 2.0 * s):\n                return None, None\n            x_min = clip_to_bounds(x0 + alpha_min * d)\n            f_min, x_min = safe_eval(x_min)\n            if f_min is None:\n                return None, None\n            if f_min < f0 - 1e-12:\n                return f_min, x_min\n            return None, None\n\n        # main iterative loop over rounds until budget exhausted\n        while evals < self.budget:\n            # adapt parameters per round\n            k = max(1, min(n, int(np.clip(int(np.ceil(n ** 0.6)), 1, n))))\n            probes = max(8, 4 * k)\n\n            # build PCA-adapted basis of dimension k\n            weights = mem_weights.copy()\n            basis = make_basis(mem_success, weights, k)\n\n            improved_round = False\n            accepted = 0\n            rejected = 0\n\n            # temperature for this round (annealing)\n            temp = temp_init * np.exp(-temp_decay * evals * 1.0)\n            temp = max(temp, 1e-8)\n\n            # generate probe pairs (mirrored): for each random coefficient vector we evaluate + and - (balanced)\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample coefficients in subspace (Gaussian)\n                coeffs = np.random.randn(k)\n                dir_sub = basis @ coeffs\n                nrm = np.linalg.norm(dir_sub)\n                if nrm == 0:\n                    continue\n                dir_unit = dir_sub / nrm\n\n                # check directional tabu; avoid almost-parallel bad directions\n                if is_taboo(dir_unit):\n                    # small probability to violate tabu (to avoid permanent block)\n                    if np.random.rand() > 0.03:\n                        continue\n\n                # mirrored pair proposals with different multipliers (mirrors help balanced exploration)\n                # random scale factor from folded normal to encourage both short and longer steps\n                scale = step * (1.0 + 0.8 * np.abs(np.random.randn()))\n                # propose plus and minus\n                for sign in (+1.0, -1.0):\n                    if evals >= self.budget:\n                        break\n                    # small random jitter orthogonal to dir_unit to maintain diversity\n                    orth_jitter = np.random.randn(n)\n                    # project out component along dir_unit to keep jitter orthogonal\n                    orth_jitter = orth_jitter - dir_unit * (dir_unit @ orth_jitter)\n                    if np.linalg.norm(orth_jitter) > 0:\n                        orth_jitter = orth_jitter / (np.linalg.norm(orth_jitter) + 1e-16) * (0.06 * step)\n                    else:\n                        orth_jitter = 0.0 * dir_unit\n                    x_prop = clip_to_bounds(x_cur + sign * scale * dir_unit + orth_jitter)\n                    f_prop, x_prop = safe_eval(x_prop)\n                    if f_prop is None:\n                        break\n                    delta = f_prop - f_cur\n                    accept = False\n                    # always accept if improved significantly\n                    if f_prop < f_cur - 1e-12:\n                        accept = True\n                    else:\n                        # simulated annealing acceptance of worsenings (small probability)\n                        try:\n                            prob = np.exp(-max(0.0, delta) / (temp + 1e-16))\n                        except Exception:\n                            prob = 0.0\n                        if np.random.rand() < prob * 0.12:\n                            accept = True\n                    if accept:\n                        # store successful step (unit)\n                        step_vec = x_prop - x_cur\n                        sv_nrm = np.linalg.norm(step_vec)\n                        if sv_nrm > 0:\n                            u = step_vec / sv_nrm\n                            mem_success.insert(0, u.copy())\n                            mem_weights.insert(0, 1.0)\n                            # cap memory\n                            if len(mem_success) > self.mem_size:\n                                mem_success.pop()\n                                mem_weights.pop()\n                        # reduce weights slightly (recency weighting)\n                        mem_weights = [w * 0.95 for w in mem_weights]\n                        # parabolic refine if strong improvement or if reasonably promising uphill accepted\n                        # refine uses small extra budget (<=2 evals)\n                        if f_prop < f_cur - 1e-6:\n                            s_try = min(max_step, abs(scale))\n                            f_ref, x_ref = parabolic_refine(x_cur, f_cur, dir_unit, s_try)\n                            if f_ref is not None and f_ref < f_prop - 1e-12:\n                                # accept refined\n                                x_prop = x_ref.copy()\n                                f_prop = f_ref\n                        # accept move\n                        x_cur = x_prop.copy()\n                        f_cur = f_prop\n                        accepted += 1\n                        improved_round = True\n                        rounds_since_improve = 0\n                        # increase step\n                        step = min(max_step, step * grow)\n                    else:\n                        rejected += 1\n                        # mark this direction as bad (taboo) with small weight\n                        bad_dirs.insert(0, (dir_unit.copy(), 1.0))\n                        # keep size bounded\n                        if len(bad_dirs) > 2 * self.mem_size:\n                            bad_dirs.pop()\n                        # decay the weights of bad dirs a bit\n                        bad_dirs = [(d, w * 0.95) for (d, w) in bad_dirs]\n                        # small random chance to run a cheap 1D parabolic probe from current along dir_unit\n                        if np.random.rand() < 0.04 and evals < self.budget:\n                            if (self.budget - evals) >= 3:\n                                f_ref, x_ref = parabolic_refine(x_cur, f_cur, dir_unit, 0.6 * step)\n                                if f_ref is not None and f_ref < f_cur - 1e-12:\n                                    # accept polished direction\n                                    step_vec = x_ref - x_cur\n                                    sv_nrm = np.linalg.norm(step_vec)\n                                    if sv_nrm > 0:\n                                        u = step_vec / sv_nrm\n                                        mem_success.insert(0, u.copy())\n                                        mem_weights.insert(0, 1.0)\n                                        if len(mem_success) > self.mem_size:\n                                            mem_success.pop(); mem_weights.pop()\n                                    x_cur = x_ref.copy()\n                                    f_cur = f_ref\n                                    accepted += 1\n                                    improved_round = True\n                                    rounds_since_improve = 0\n                                    step = min(max_step, step * grow)\n\n                # budget check break\n                if evals >= self.budget:\n                    break\n\n            # end of probes\n\n            # decay bad_dirs strengths; prune tiny ones\n            bad_dirs = [(d, w * bad_decay) for (d, w) in bad_dirs]\n            bad_dirs = [(d, w) for (d, w) in bad_dirs if w > 1e-3]\n            # normalize mem_weights if present\n            if mem_weights:\n                total_w = sum(mem_weights) + 1e-16\n                mem_weights = [w / total_w for w in mem_weights]\n\n            # adapt step after round\n            if improved_round:\n                round_failures = 0\n            else:\n                round_failures += 1\n                step = max(min_step, step * shrink)\n                rounds_since_improve += 1\n\n            # occasional full-space Gaussian polish when progress stagnates\n            if rounds_since_improve >= stagnation_limit // 2:\n                polish_trials = min(8, max(1, int(np.ceil(np.log(1 + n)))))\n                for _ in range(polish_trials):\n                    if evals >= self.budget:\n                        break\n                    d = np.random.randn(n)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                    a = np.random.uniform(-0.6 * step, 0.6 * step)\n                    f_try, x_try = safe_eval(clip_to_bounds(x_cur + a * d))\n                    if f_try is None:\n                        break\n                    if f_try < f_cur - 1e-12:\n                        # register success\n                        sv = x_try - x_cur\n                        sval = np.linalg.norm(sv)\n                        if sval > 0:\n                            mem_success.insert(0, (sv / sval).copy())\n                            mem_weights.insert(0, 1.0)\n                            if len(mem_success) > self.mem_size:\n                                mem_success.pop(); mem_weights.pop()\n                        x_cur = x_try.copy()\n                        f_cur = f_try\n                        improved_round = True\n                        rounds_since_improve = 0\n                        step = min(max_step, step * grow)\n\n            # if no improvement for many rounds, perform a restart biased around the best\n            if rounds_since_improve >= stagnation_limit:\n                rounds_since_improve = 0\n                # reduce bad_dirs influence and clear small mem to diversify\n                bad_dirs = []\n                if x_best is not None:\n                    perturb = np.random.randn(n) * (0.35 * domain_mean)\n                    x_try = clip_to_bounds(x_best + perturb)\n                    f_try, x_try = safe_eval(x_try)\n                    if f_try is not None:\n                        x_cur = x_try.copy()\n                        f_cur = f_try\n                else:\n                    x_try = np.random.uniform(lb, ub)\n                    f_try, x_try = safe_eval(x_try)\n                    if f_try is not None:\n                        x_cur = x_try.copy()\n                        f_cur = f_try\n                mem_success = []\n                mem_weights = []\n                # enlarge step moderately to escape\n                step = min(max_step, max(step * 1.6, 0.5 * domain_mean))\n\n            # enforce minimal step\n            if step < min_step:\n                step = min_step\n\n            # very small objective early exit\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            return float(f_cur), np.array(x_cur, dtype=float)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 121, in is_taboo, the following error occurred:\nValueError: shapes (5,) and (2,) not aligned: 5 (dim 0) != 2 (dim 0)\nOn line: if abs(np.dot(dir_unit, bd)) > 0.92:  # nearly parallel -> taboo", "error": "In the code, line 121, in is_taboo, the following error occurred:\nValueError: shapes (5,) and (2,) not aligned: 5 (dim 0) != 2 (dim 0)\nOn line: if abs(np.dot(dir_unit, bd)) > 0.92:  # nearly parallel -> taboo", "parent_ids": "dbfb0c67-e5d2-4c51-89f0-3034b212c7f1", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "219f27d7-12f6-4068-8c96-26361680e2d9", "fitness": 0.4355726502773053, "name": "HASTS", "description": "HASTS builds a hierarchical low-dimensional search by sampling a k≈√n orthonormal subspace (augmented with up to k/3 recent successful unit directions kept in a short LRU memory) and probes many directions per round (probes = max(8, 2k)) to balance broad exploration and focused exploitation. Candidate directions use an Ornstein–Uhlenbeck style momentum (theta=0.7) blended with fresh noise (sigma_scale=0.18) and occasional heavy‑tailed length proposals (~15% Cauchy-like) to help escape local minima, while low-rank covariance sampling from recent memory (≈8% chance) enables concentrated searches along promising manifolds. Step control is conservative and budget-aware (initial step = 0.15·domain_mean, min_step tiny, max_step capped), with mild grow (1.15) and shrink (0.80) factors, short backtracking line-searchs (≤5 evals) to intensify successes, and small Gaussian polishing after improvements. Robustness features include a safe_eval wrapper that enforces the evaluation budget and tracks the global best, stagnation-triggered biased restarts with controlled perturbation and limited restarts (≤6), and early stopping when the objective is essentially zero.", "code": "import numpy as np\n\nclass HASTS:\n    \"\"\"\n    HASTS: Hierarchical Adaptive Subspace Tempered Search\n\n    Key design differences from the provided MASTL-style algorithm:\n      - subspace dimension k ~ sqrt(n) (smaller than n^(2/3))\n      - probes per round = max(8, 2*k)\n      - smaller initial step (0.15 * domain_mean)\n      - milder growth/shrink (grow=1.15, shrink=0.80)\n      - Ornstein-Uhlenbeck (OU) style momentum blending (theta=0.7)\n      - heavy-tailed length proposals occasionally (mixture with Cauchy-like tail)\n      - backtracking line-search limited to <= 5 evaluations\n      - low-rank covariance sampling from the memory for focused exploration\n      - memory_size default 8 (short LRU)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, memory_size=8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        # use a local RNG to avoid altering global np state\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # Accept scalar or vector bounds from func.bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        domain_mean = np.mean(ub - lb)\n        evals = 0\n\n        def clip_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        f_best = np.inf\n        x_best = None\n\n        # safe evaluation wrapper that enforces budget and tracks best\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x.copy()\n\n        # initial random starting point\n        x_cur = self.rng.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            # budget exhausted at start\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # Parameters (deliberately different from the provided algorithm)\n        step = 0.15 * domain_mean\n        min_step = 1e-7 * max(1.0, domain_mean)\n        k = max(1, int(np.ceil(n ** 0.5)))       # sqrt(n) scaling\n        probes = max(8, 2 * k)\n        memory = []                              # LRU memory of unit directions\n        grow = 1.15\n        shrink = 0.80\n        max_step = 3.0 * domain_mean\n        theta = 0.7           # OU momentum decay\n        sigma_scale = 0.18    # noise amplitude for MU blending\n        no_improve = 0\n        stagnation_limit = max(12, int(6 * np.log(1 + n)))\n        restarts = 0\n        max_restarts = 6\n        momentum = np.zeros(n)\n        iter_count = 0\n\n        # Cheap backtracking line search (budget-aware, <= max_evals)\n        def backtrack_line_search(x0, f0, d, init_step, max_evals=5):\n            nonlocal evals\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0 or evals >= self.budget:\n                return None, None\n            d = d / dnrm\n            remaining = max(0, self.budget - evals)\n            if remaining <= 0:\n                return None, None\n            alpha = 1.0\n            tries = 0\n            # try decreasing step sizes until improvement or budget used\n            while tries < max_evals and evals < self.budget:\n                s = init_step * alpha\n                x_try = clip_to_bounds(x0 + s * d)\n                f_try, x_try = safe_eval(x_try)\n                if f_try is None:\n                    return None, None\n                if f_try < f0 - 1e-12:\n                    return f_try, x_try\n                alpha *= 0.5\n                tries += 1\n            return None, None\n\n        # Main optimization loop\n        while evals < self.budget:\n            iter_count += 1\n            # allow slight re-evaluation of k (keeps adaptive structure)\n            k = max(1, int(np.clip(int(np.ceil(n ** 0.5)), 1, n)))\n            probes = max(8, 2 * k)\n\n            # Build basis: keep up to k//3 directions from memory (recent ones)\n            use_mem = min(len(memory), max(0, k // 3))\n            basis_cols = []\n            if use_mem > 0:\n                # use most recent directions\n                basis_cols.extend(memory[:use_mem])\n            needed = k - len(basis_cols)\n            if needed > 0:\n                R = self.rng.randn(n, needed)\n                if len(basis_cols) > 0:\n                    R = np.column_stack((np.column_stack(basis_cols), R))\n                Q, _ = np.linalg.qr(R)\n                basis = [Q[:, i].copy() for i in range(k)]\n            else:\n                # orthonormalize existing memory-based basis\n                B = np.column_stack(basis_cols)\n                Q, _ = np.linalg.qr(B)\n                basis = [Q[:, i].copy() for i in range(k)]\n\n            improved = False\n\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n\n                # sample a direction in the subspace\n                coeffs = self.rng.randn(k)\n                dir_sub = np.zeros(n)\n                for i in range(k):\n                    dir_sub += coeffs[i] * basis[i]\n                dir_sub /= (np.linalg.norm(dir_sub) + 1e-20)\n\n                # OU style momentum update toward the sampled direction\n                momentum = theta * momentum + (1.0 - theta) * dir_sub\n\n                # candidate direction mixes momentum and fresh direction with light noise\n                candidate_dir = momentum + sigma_scale * self.rng.randn() * dir_sub\n                if np.linalg.norm(candidate_dir) == 0:\n                    candidate_dir = dir_sub\n                candidate_dir /= (np.linalg.norm(candidate_dir) + 1e-20)\n\n                # length proposal: mostly uniform in [-step, step], occasionally heavy-tailed\n                if self.rng.rand() < 0.85:\n                    length = self.rng.uniform(-step, step)\n                else:\n                    # Cauchy-like heavy tail scaled down\n                    u = self.rng.rand()\n                    length = step * np.tan(np.pi * (u - 0.5)) * 0.28\n                    # cap extreme lengths\n                    length = np.clip(length, -5.0 * step, 5.0 * step)\n\n                x_prop = clip_to_bounds(x_cur + length * candidate_dir)\n                f_prop, x_prop = safe_eval(x_prop)\n                if f_prop is None:\n                    break\n\n                if f_prop < f_cur - 1e-12:\n                    # successful direction; store normalized displacement\n                    dir_succ = x_prop - x_cur\n                    dn = np.linalg.norm(dir_succ)\n                    if dn > 0:\n                        dir_succ = dir_succ / dn\n                        memory.insert(0, dir_succ.copy())\n                        if len(memory) > self.memory_size:\n                            memory.pop()\n\n                    # intensify with a short backtracking line search\n                    remaining_budget = max(0, self.budget - evals)\n                    line_budget = min(5, remaining_budget)\n                    f_line, x_line = backtrack_line_search(\n                        x_cur, f_cur, dir_succ, init_step=abs(length) + 0.4 * step, max_evals=line_budget\n                    )\n                    if f_line is not None and f_line < f_prop - 1e-12:\n                        f_prop = f_line\n                        x_prop = x_line\n\n                    # accept improvement\n                    x_cur = x_prop.copy()\n                    f_cur = f_prop\n                    improved = True\n                    no_improve = 0\n                    # mild step growth\n                    step = min(step * grow, max_step)\n                else:\n                    # occasional focused low-rank covariance exploration from memory\n                    if self.rng.rand() < 0.08 and len(memory) >= 2:\n                        m = min(len(memory), 4)\n                        V = np.column_stack(memory[:m])\n                        z = self.rng.randn(m)\n                        d_full = V @ z\n                        d_full /= (np.linalg.norm(d_full) + 1e-20)\n                        remaining_budget = max(0, self.budget - evals)\n                        if remaining_budget >= 3:\n                            f_line, x_line = backtrack_line_search(\n                                x_cur, f_cur, d_full, init_step=step, max_evals=min(4, remaining_budget)\n                            )\n                            if f_line is not None and f_line < f_cur - 1e-12:\n                                dir_succ = x_line - x_cur\n                                dn = np.linalg.norm(dir_succ)\n                                if dn > 0:\n                                    dir_succ = dir_succ / dn\n                                    memory.insert(0, dir_succ.copy())\n                                    if len(memory) > self.memory_size:\n                                        memory.pop()\n                                x_cur = x_line.copy()\n                                f_cur = f_line\n                                improved = True\n                                no_improve = 0\n                                step = min(step * 1.2, max_step)\n\n            # after probe round: adapt step if no improvement\n            if not improved:\n                no_improve += 1\n                step = max(min_step, step * shrink)\n            else:\n                # small full-space Gaussian polishing attempts\n                extras = min(6, max(1, int(np.ceil(np.sqrt(n)))))\n                for _ in range(extras):\n                    if evals >= self.budget:\n                        break\n                    d = self.rng.randn(n)\n                    d /= (np.linalg.norm(d) + 1e-20)\n                    a = self.rng.uniform(-0.4 * step, 0.4 * step)\n                    x_try = clip_to_bounds(x_cur + a * d)\n                    f_try, x_try = safe_eval(x_try)\n                    if f_try is None:\n                        break\n                    if f_try < f_cur - 1e-12:\n                        dir_succ = x_try - x_cur\n                        dn = np.linalg.norm(dir_succ)\n                        if dn > 0:\n                            dir_succ = dir_succ / dn\n                            memory.insert(0, dir_succ.copy())\n                            if len(memory) > self.memory_size:\n                                memory.pop()\n                        x_cur = x_try.copy()\n                        f_cur = f_try\n\n            # stagnation handling: biased restart around best or random, moderate step bump\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # stronger step reduction to attempt local tightening\n                    step = max(step * 0.5, min_step)\n                    no_improve = 0\n                else:\n                    if x_best is not None:\n                        radius = 0.25 * domain_mean * (0.9 ** restarts)\n                        perturb = self.rng.randn(n) * radius\n                        x_new = clip_to_bounds(x_best + perturb)\n                    else:\n                        x_new = self.rng.uniform(lb, ub)\n                    f_new, x_new = safe_eval(x_new)\n                    if f_new is None:\n                        break\n                    x_cur = x_new.copy()\n                    f_cur = f_new\n                    memory = []            # clear memory to diversify search\n                    momentum = np.zeros(n) # reset momentum\n                    step = min(max_step, step * 1.5)\n                    no_improve = 0\n\n            # enforce minimal step\n            if step < min_step:\n                step = min_step\n\n            # early stop if near-optimal\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            return float(f_cur), np.array(x_cur, dtype=float)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HASTS scored 0.436 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "dbfb0c67-e5d2-4c51-89f0-3034b212c7f1", "operator": null, "metadata": {"aucs": [2.0000000000020002e-05, 0.14813727829164058, 0.9536430978007172, 0.9793144649081901, 0.22100311398956762, 0.9724782291847255, 0.21668378590587978, 0.5191192519318466, 0.24829373981034908, 0.09703354095013594]}, "task_prompt": ""}
{"id": "8100453a-ec68-4c22-b0b2-23c991cacc8b", "fitness": "-inf", "name": "LevyPCAEnsemble", "description": "Levy-PCA Ensemble keeps a small sorted archive of best points and mixes three move types—PCA-local sampling inside a low-rank subspace computed from the archive, ensemble directional moves formed from archive differences, and heavy-tailed Lévy-like global jumps—so it intensifies where the archive is rich and explores sporadically to escape basins. The PCA subspace dimension is chosen as n^0.8 and is recomputed periodically to focus sampling along dominant problem directions, while heavy tails come from truncated Pareto and occasional Cauchy perturbations (several alpha/trunc parameters) to generate long jumps. Budget-aware intensification is provided by a tiny short_line_search (golden-ish zoom with very limited evals) and short local polishing rounds, and the global step scale is adapted on success/failure (grow ~1.18, shrink ~0.68) with a small annealed chance to accept worse moves. Stagnation triggers Cauchy-style perturbation around the best or full random restarts, archive deduplication and bound clipping are enforced, and all evaluations are strictly budget-checked via a safe_eval wrapper.", "code": "import numpy as np\n\nclass LevyPCAEnsemble:\n    \"\"\"\n    Levy-PCA Ensemble (LPE)\n\n    Main ideas / novel mechanisms:\n      - Maintain a small archive of best points and compute a low-rank PCA subspace from it;\n        sample locally inside that subspace for intensive search.\n      - Use heavy-tailed (Pareto/Cauchy-like) step lengths for occasional global exploration\n        (a truncated Lévy-like behaviour) to escape basins.\n      - Use archive differences as promising directional pulls (ensemble directions).\n      - Budget-aware short line intensification invoked sparingly.\n      - Adaptive global step-scale that grows on success and shrinks on failures, with per-iteration\n        mild annealed chance to accept a worse move to avoid premature stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, archive_size=24):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.archive_size = int(archive_size)\n        if seed is not None:\n            # keep a local RNG instance to avoid global np RNG side-effects\n            self.rng = np.random.RandomState(seed)\n        else:\n            self.rng = np.random.RandomState()\n\n    def __call__(self, func):\n        n = self.dim\n        # Bounds (BBOB default [-5,5], but use func.bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        def clip_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # safe eval wrapper to enforce budget and catch exceptions\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x\n\n        # initialization: random starting point\n        x_cur = self.rng.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # domain characteristics\n        domain_mean = np.mean(ub - lb)\n        # step scale parameters\n        step = 0.3 * domain_mean\n        min_step = 1e-6 * max(1.0, domain_mean)\n        max_step = 6.0 * domain_mean\n        grow = 1.18\n        shrink = 0.68\n\n        # archive of best points (kept sorted by objective)\n        archive = [(f_cur, x_cur.copy())]  # list of (f,x)\n        max_archive = max(6, min(self.archive_size, 4 * n))\n\n        # PCA subspace dimension (novel exponent ~ n^0.8 to differ from ARSS/MASTL)\n        def subspace_dim():\n            return max(1, min(n, int(np.ceil(n ** 0.8))))\n\n        pca_dirs = None\n        pca_update_every = max(3, min(10, n // 2))\n        iters_since_pca = 0\n\n        # stagnation & restart\n        no_improve = 0\n        stagnation_limit = max(20, int(6 * np.log(1 + n)))\n        restarts = 0\n        max_restarts = 6\n\n        # annealed acceptance base probability for worse moves (small)\n        base_bad_accept = 0.01\n\n        # truncated Pareto sampler for heavy tails (shape alpha > 1)\n        def truncated_pareto(alpha=1.6, trunc=8.0):\n            # sample u in (0,1), transform by Pareto: r = (1 - u)^(-1/alpha) - 1\n            # then truncate at 'trunc'\n            u = self.rng.rand()\n            r = ( (1.0 - u) ** (-1.0 / alpha) ) - 1.0\n            return min(r, trunc)\n\n        # budget-aware short line search (very small budget)\n        def short_line_search(x0, f0, d, init_alpha, max_evals=6):\n            nonlocal evals\n            d = np.asarray(d, dtype=float)\n            nd = np.linalg.norm(d)\n            if nd == 0 or evals >= self.budget:\n                return None, None\n            d = d / nd\n            remaining = max(0, self.budget - evals)\n            if remaining <= 0:\n                return None, None\n            # bracket at 0, +a, -a (2 evals)\n            a = float(init_alpha)\n            x_plus = clip_to_bounds(x0 + a * d)\n            f_plus, _ = safe_eval(x_plus)\n            if f_plus is None:\n                return None, None\n            x_minus = clip_to_bounds(x0 - a * d)\n            f_minus, _ = safe_eval(x_minus)\n            if f_minus is None:\n                return None, None\n            best_f = f0\n            best_x = x0.copy()\n            if f_plus < best_f:\n                best_f = f_plus; best_x = x_plus.copy()\n            if f_minus < best_f:\n                best_f = f_minus; best_x = x_minus.copy()\n            # If none improved, try small interpolation points toward the better side\n            if best_f >= f0:\n                return None, None\n            # otherwise perform a tiny secant-like zoom (remaining budget)\n            remaining = max(0, self.budget - evals)\n            steps = min(max_evals, remaining)\n            # simple golden-ish shrink around best interval between 0 and chosen sign\n            lo = 0.0\n            hi = a if best_f == f_plus else -a\n            gr = (np.sqrt(5) - 1) / 2.0\n            c = hi - gr * (hi - lo)\n            d_alpha = lo + gr * (hi - lo)\n            for _ in range(steps):\n                x_c = clip_to_bounds(x0 + c * d)\n                f_c, _ = safe_eval(x_c)\n                if f_c is None:\n                    break\n                if f_c < best_f:\n                    best_f = f_c; best_x = x_c.copy()\n                x_d = clip_to_bounds(x0 + d_alpha * d)\n                f_d, _ = safe_eval(x_d)\n                if f_d is None:\n                    break\n                if f_d < best_f:\n                    best_f = f_d; best_x = x_d.copy()\n                if f_c < f_d:\n                    hi = d_alpha\n                else:\n                    lo = c\n                c = hi - gr * (hi - lo)\n                d_alpha = lo + gr * (hi - lo)\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # helper to add to archive, keep sorted by f\n        def archive_add(fv, xv):\n            # keep small archive of unique-ish points\n            for f_exist, x_exist in archive:\n                if np.allclose(xv, x_exist, atol=1e-12):\n                    return\n            archive.append((fv, xv.copy()))\n            archive.sort(key=lambda t: t[0])\n            while len(archive) > max_archive:\n                archive.pop()\n\n        # main loop\n        while evals < self.budget:\n            # occasionally recompute PCA directions from archive top\n            iters_since_pca += 1\n            if pca_dirs is None or iters_since_pca >= pca_update_every:\n                iters_since_pca = 0\n                # form matrix from top min(m, len(archive)) points\n                m = min(len(archive), max(3, int(max_archive / 2)))\n                if m >= 2:\n                    X = np.vstack([archive[i][1] for i in range(m)])\n                    Xc = X - X.mean(axis=0, keepdims=True)\n                    # compute top-k principal components (economy)\n                    k = min(n, subspace_dim())\n                    try:\n                        # compute SVD on (m x n) matrix\n                        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n                        pca_dirs = Vt[:k].T  # shape (n,k)\n                    except Exception:\n                        pca_dirs = None\n                else:\n                    pca_dirs = None\n\n            # Decide move type mixture\n            # Probabilities adaptive: favor local when archive is rich, else global\n            p_local = 0.6 if len(archive) >= 6 else 0.3\n            p_directional = 0.2 if len(archive) >= 3 else 0.05\n            p_global = max(0.0, 1.0 - p_local - p_directional)\n\n            r = self.rng.rand()\n            chosen = None\n            # Candidate formation\n            if r < p_local and pca_dirs is not None:\n                chosen = 'pca_local'\n                # sample in PCA subspace coefficients with Gaussian scale proportional to step\n                coeffs = self.rng.normal(scale=step / (np.linalg.norm(pca_dirs, ord=2) + 1e-12),\n                                         size=pca_dirs.shape[1])\n                direction = pca_dirs @ coeffs\n                # small isotropic noise added\n                direction += 0.08 * step * self.rng.randn(n)\n                length_scale = np.linalg.norm(direction) + 1e-20\n                candidate_dir = direction / length_scale\n                # overall length magnitude small-ish\n                length = self.rng.normal(loc=0.0, scale=0.9 * step)\n            elif r < p_local + p_directional and len(archive) >= 2:\n                chosen = 'directional'\n                # pick two of best archive points to get an ensemble direction\n                # bias toward more recent/better ones\n                i = self.rng.randint(0, min(len(archive), max(3, int(len(archive)/2))))\n                j = self.rng.randint(i+1, len(archive))\n                xi = archive[i][1]\n                xj = archive[j][1]\n                direction = xj - xi\n                if np.linalg.norm(direction) < 1e-12:\n                    direction = self.rng.randn(n)\n                candidate_dir = direction / (np.linalg.norm(direction) + 1e-20)\n                # use mix of step and adaptive pareto multiplicative factor\n                length = (1.0 + truncated_pareto(alpha=1.8, trunc=6.0)) * step * (0.6 + 0.8 * self.rng.rand())\n            else:\n                chosen = 'global_levy'\n                # heavy-tailed radial multiplier\n                pareto_r = truncated_pareto(alpha=1.4, trunc=10.0)\n                # random direction isotropic\n                dir_raw = self.rng.randn(n)\n                candidate_dir = dir_raw / (np.linalg.norm(dir_raw) + 1e-20)\n                # combine with occasional Cauchy perturbation to get Lévy-like property\n                cauchy = np.tan(np.pi * (self.rng.rand() - 0.5))  # standard Cauchy\n                cauchy = np.clip(cauchy, -8.0, 8.0)\n                length = step * (1.0 + pareto_r) * np.sign(self.rng.randn() + 1e-6) + 0.25 * step * cauchy\n\n            # propose\n            x_prop = clip_to_bounds(x_cur + length * candidate_dir)\n            f_prop, x_prop = safe_eval(x_prop)\n            if f_prop is None:\n                break\n\n            accepted = False\n            # if improved accept and update archive\n            if f_prop < f_cur - 1e-12:\n                accepted = True\n            else:\n                # small annealed chance to accept worse move depending on stagnation\n                anneal_factor = 1.0 + (no_improve / max(1, stagnation_limit))\n                prob_accept = base_bad_accept * anneal_factor\n                if self.rng.rand() < prob_accept:\n                    accepted = True\n\n            if accepted:\n                # store difference as directional info (insert into archive)\n                archive_add(f_prop, x_prop)\n                # occasionally run a small short line intensification along the accepted direction\n                if self.rng.rand() < 0.22 and evals < self.budget:\n                    remaining_budget = max(0, self.budget - evals)\n                    ls_budget = min(6, remaining_budget)\n                    f_line, x_line = short_line_search(x_cur, f_cur, x_prop - x_cur, init_alpha=abs(length) + 0.6 * step, max_evals=ls_budget)\n                    if f_line is not None and f_line < f_prop - 1e-12:\n                        f_prop = f_line\n                        x_prop = x_line\n                        archive_add(f_prop, x_prop)\n                # accept move\n                x_cur = x_prop.copy()\n                f_cur = f_prop\n                no_improve = 0\n                # adapt step: grow moderately if move came from local/pca or directional, more if global succeeded\n                if chosen == 'global_levy':\n                    step = min(max_step, step * (grow * 1.05))\n                else:\n                    step = min(max_step, step * grow)\n            else:\n                # rejection: shrink step\n                no_improve += 1\n                step = max(min_step, step * shrink)\n                # occasionally try a focused short line search from current along candidate_dir if rejected\n                if self.rng.rand() < 0.04 and evals < self.budget:\n                    remaining_budget = max(0, self.budget - evals)\n                    if remaining_budget >= 3:\n                        f_line, x_line = short_line_search(x_cur, f_cur, candidate_dir, init_alpha=0.8 * step, max_evals=min(4, remaining_budget))\n                        if f_line is not None and f_line < f_cur - 1e-12:\n                            archive_add(f_line, x_line)\n                            x_cur = x_line.copy()\n                            f_cur = f_line\n                            no_improve = 0\n                            step = min(max_step, step * grow)\n\n            # local polishing when archive rich and after improvements\n            if no_improve == 0 and len(archive) >= 4:\n                extras = min(6, 2 + n // 4)\n                for _ in range(extras):\n                    if evals >= self.budget:\n                        break\n                    d = self.rng.randn(n)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                    a = self.rng.uniform(-0.4 * step, 0.4 * step)\n                    f_try, x_try = safe_eval(clip_to_bounds(x_cur + a * d))\n                    if f_try is None:\n                        break\n                    if f_try < f_cur - 1e-12:\n                        archive_add(f_try, x_try)\n                        x_cur = x_try.copy()\n                        f_cur = f_try\n\n            # stagnation / restart handling\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                no_improve = 0\n                # if too many restarts, perform milder perturbation; else strong perturbation around best archive\n                if restarts <= max_restarts and len(archive) > 0:\n                    # pick best and perturb with Cauchy-like noise\n                    xb = archive[0][1]\n                    cauchy_vec = np.tan(np.pi * (self.rng.rand(n) - 0.5))\n                    cauchy_vec = np.clip(cauchy_vec, -6.0, 6.0)\n                    perturb = 0.35 * domain_mean * cauchy_vec\n                    x_new = clip_to_bounds(xb + perturb)\n                    f_new, x_new = safe_eval(x_new)\n                    if f_new is None:\n                        break\n                    x_cur = x_new.copy()\n                    f_cur = f_new\n                    archive_add(f_new, x_new)\n                    step = min(max_step, max(step * 1.6, 0.5 * domain_mean))\n                else:\n                    # full restart random\n                    x_new = self.rng.uniform(lb, ub)\n                    f_new, x_new = safe_eval(x_new)\n                    if f_new is None:\n                        break\n                    x_cur = x_new.copy()\n                    f_cur = f_new\n                    archive_add(f_new, x_new)\n                    step = max(min_step, 0.6 * domain_mean)\n                    restarts = 0  # reset restarts to allow repeated attempts\n\n            # quick stop if extremely low objective\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            return float(f_cur), np.array(x_cur, dtype=float)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 1334, in numpy.random._bounded_integers._rand_int64, the following error occurred:\nValueError: low >= high", "error": "In the code, line 1334, in numpy.random._bounded_integers._rand_int64, the following error occurred:\nValueError: low >= high", "parent_ids": "dbfb0c67-e5d2-4c51-89f0-3034b212c7f1", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "8df1f8d0-a449-4cb6-98f6-523fc2806b78", "fitness": "-inf", "name": "AAPSS", "description": "The algorithm is an adaptive affine-perturbed subspace search that enforces the evaluation budget and bounds via safe_eval and clipping while combining targeted low-dimensional probing with occasional full-space perturbations for global exploration. It builds an orthonormal subspace (QR) of dimension k ≈ n^(3/4) using a mix of recent successful directions and random vectors, performs probes (2*k+4) inside that subspace, and adapts a lightweight subspace covariance (Lc) from projected memory to bias sampling. Step control is conservative-to-adaptive: a small initial step (0.15·domain_mean), a scaled minimum step, aggressive growth (×1.35) on success and strong shrink (×0.60) on failures, with stagnation triggers that restart around the best solution and trim the LRU memory (memory_size=6) up to 4 times. Local refinement uses a tight backtracking-style line search (≤4 evals) along successful directions, plus occasional isotropic polishing and small ambient perturbations to maintain robustness across Many Affine BBOB functions.", "code": "import numpy as np\nfrom collections import deque\n\nclass AAPSS:\n    \"\"\"\n    Adaptive Affine-Perturbed Subspace Search (AAPSS)\n\n    Main design choices & differences vs the provided MASTL reference:\n      - Subspace dimension k : ~ n^(3/4) (different scaling)\n      - probes per round     : 2*k + 4 (moderate probing)\n      - initial step         : 0.15 * domain_mean (smaller, more conservative)\n      - minimal step         : 1e-5 * max(1.0, domain_mean)\n      - growth factor        : 1.35 (aggressive growth on success)\n      - shrink factor        : 0.60 (stronger shrink on failures)\n      - memory_size (LRU)   : 6 (smaller, more focused memory)\n      - line search budget   : <= 4 evals (tight, backtracking-style)\n      - stagnation_limit     : smaller (10) to trigger intensification earlier\n      - restarts allowed     : 4 (more conservative)\n      - covariance adapt     : light rank-1like adaptation in subspace using stored successes\n\n    Notes:\n      - Strictly enforces evaluation budget via safe_eval.\n      - Expects func.bounds.lb / ub (scalars or vectors). Typical range [-5, 5].\n      - __init__(budget, dim) required by contract; seed optional for reproducibility.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, memory_size=6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        n = self.dim\n\n        # bounds handling (support scalar or vector)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        def clip_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # evaluation bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = f\n                x_best = x.copy()\n            return f, x\n\n        # start point\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            # no budget\n            return float(f_best), np.array(x_best, dtype=float)\n\n        domain_mean = np.mean(ub - lb)\n\n        # algorithm parameters (see header for rationale)\n        step = 0.15 * domain_mean\n        min_step = 1e-5 * max(1.0, domain_mean)\n        max_step = 6.0 * domain_mean\n\n        # subspace dimension scaling ~ n^(3/4)\n        k = max(1, int(np.ceil(n ** (3.0 / 4.0))))\n        probes = max(4, 2 * k + 4)\n\n        # LRU memory of successful normalized directions\n        dir_memory = deque(maxlen=self.memory_size)\n\n        grow = 1.35\n        shrink = 0.60\n\n        no_improve = 0\n        stagnation_limit = max(8, 10)  # favor earlier intensification\n        restarts = 0\n        max_restarts = 4\n\n        # lightweight subspace covariance accumulator (k x k when inside subspace)\n        # We won't keep a global covariance in ambient n due to cost; we'll adapt inside subspace each round.\n\n        # Tight backtracking-style line search: tries step, then halves until improvement or budget exhausted.\n        def backtrack_line_search(x0, f0, d, init_step, max_evals=4):\n            nonlocal evals\n            d = np.asarray(d, dtype=float)\n            dn = np.linalg.norm(d)\n            if dn == 0 or evals >= self.budget:\n                return None, None\n            d = d / dn\n            remaining = max(0, self.budget - evals)\n            if remaining <= 0:\n                return None, None\n            s = float(abs(init_step))\n            # try a small number of halvings\n            best_f = f0\n            best_x = x0.copy()\n            attempts = 0\n            while s > 1e-15 and attempts < max_evals and evals < self.budget:\n                x_try = clip_to_bounds(x0 + s * d)\n                f_try, x_try = safe_eval(x_try)\n                if f_try is None:\n                    break\n                attempts += 1\n                if f_try < best_f - 1e-12:\n                    best_f = f_try\n                    best_x = x_try.copy()\n                    # accept early (we allow small further probing by trying slightly longer step once)\n                    s = min(s * 1.5, max_step)\n                    # try one more slightly longer probe if budget allows\n                    if attempts < max_evals and evals < self.budget:\n                        x_try2 = clip_to_bounds(x0 + s * d)\n                        f_try2, x_try2 = safe_eval(x_try2)\n                        if f_try2 is None:\n                            break\n                        attempts += 1\n                        if f_try2 < best_f - 1e-12:\n                            best_f = f_try2\n                            best_x = x_try2.copy()\n                    return best_f, best_x\n                # else reduce step and continue backtracking\n                s *= 0.5\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # Main optimization loop\n        while evals < self.budget:\n            # refresh k/probes in case dim changed externally\n            k = max(1, min(n, int(np.ceil(n ** (3.0 / 4.0)))))\n            probes = max(4, 2 * k + 4)\n\n            improved_round = False\n\n            # build an orthonormal basis for the subspace:\n            # mix recent memory directions and random directions\n            use_mem = min(len(dir_memory), max(1, k // 3))\n            if use_mem > 0:\n                mem_vecs = np.array(list(dir_memory)[:use_mem])  # most recent first\n                # place mem vectors as columns (n x use_mem)\n                R = mem_vecs.T.copy()\n                if R.shape[1] < k:\n                    # append random columns\n                    R = np.column_stack((R, np.random.randn(n, k - R.shape[1])))\n            else:\n                R = np.random.randn(n, k)\n            # orthonormalize (QR)\n            try:\n                Q, _ = np.linalg.qr(R)\n            except Exception:\n                # fallback random basis\n                Q = np.linalg.qr(np.random.randn(n, k))[0]\n            basis = Q[:, :k]  # n x k\n\n            # compute lightweight subspace covariance using memory projected into basis\n            if len(dir_memory) >= 2:\n                proj = (np.array(list(dir_memory)) @ basis)  # (m x n) @ (n x k) -> m x k\n                # compute empirical covariance in subspace (k x k), small regularization\n                C = np.cov(proj.T) if proj.shape[0] > 1 else np.atleast_2d(proj.T @ proj)\n                if C.shape == ():\n                    C = C.reshape(1, 1)\n                # regularize\n                C = C + 1e-6 * np.eye(C.shape[0])\n                # attempt a Cholesky or fallback to identity\n                try:\n                    Lc = np.linalg.cholesky(C)\n                except Exception:\n                    Lc = np.eye(k)\n            else:\n                Lc = np.eye(k)\n\n            # build a subspace mean direction from memory (if present)\n            subspace_mean = np.zeros(k)\n            if len(dir_memory) > 0:\n                # project most recent few into basis and take weighted mean (recent weighted more)\n                weights = np.linspace(1.0, 0.4, min(len(dir_memory), 6))\n                mem_arr = np.array(list(dir_memory)[:len(weights)])\n                proj = mem_arr @ basis\n                weighted = (proj.T * weights).T\n                subspace_mean = np.mean(weighted, axis=0)\n\n            # probes inside subspace\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample in subspace using mean + scaled covariance noise\n                z = subspace_mean + Lc @ np.random.randn(k) * 0.9\n                # map back to ambient\n                d = basis @ z\n                dn = np.linalg.norm(d)\n                if dn == 0:\n                    continue\n                d = d / dn\n                # combine small ambient gaussian to keep full-space exploration (perturb)\n                if np.random.rand() < 0.08:\n                    d_full = np.random.randn(n)\n                    d_full /= (np.linalg.norm(d_full) + 1e-20)\n                    d = 0.65 * d + 0.35 * d_full\n                    d /= (np.linalg.norm(d) + 1e-20)\n\n                # stochastic length\n                length = np.random.normal(loc=0.0, scale=0.9 * step)\n                x_prop = clip_to_bounds(x_cur + length * d)\n                f_prop, x_prop = safe_eval(x_prop)\n                if f_prop is None:\n                    break\n                if f_prop < f_cur - 1e-12:\n                    # success: store normalized direction and update memory\n                    dir_succ = x_prop - x_cur\n                    dn2 = np.linalg.norm(dir_succ)\n                    if dn2 > 0:\n                        dir_succ = dir_succ / dn2\n                        dir_memory.appendleft(dir_succ.copy())\n                    # targeted backtracking line search along this direction (tight budget)\n                    remaining_budget = max(0, self.budget - evals)\n                    line_budget = min(4, remaining_budget)\n                    f_line, x_line = backtrack_line_search(x_cur, f_cur, dir_succ, init_step=max(abs(length), 0.5 * step), max_evals=line_budget)\n                    if f_line is not None and f_line < f_prop - 1e-12:\n                        f_prop = f_line\n                        x_prop = x_line\n                    # accept improvement and expand step\n                    x_cur = x_prop.copy()\n                    f_cur = f_prop\n                    improved_round = True\n                    no_improve = 0\n                    step = min(step * grow, max_step)\n                else:\n                    # occasionally try a short focused backtrack using the candidate direction\n                    if np.random.rand() < 0.04 and evals < self.budget:\n                        remaining_budget = max(0, self.budget - evals)\n                        if remaining_budget >= 2:\n                            f_line, x_line = backtrack_line_search(x_cur, f_cur, d, init_step=step, max_evals=min(3, remaining_budget))\n                            if f_line is not None and f_line < f_cur - 1e-12:\n                                dir_succ = x_line - x_cur\n                                dn2 = np.linalg.norm(dir_succ)\n                                if dn2 > 0:\n                                    dir_succ = dir_succ / dn2\n                                    dir_memory.appendleft(dir_succ.copy())\n                                x_cur = x_line.copy()\n                                f_cur = f_line\n                                improved_round = True\n                                no_improve = 0\n                                step = min(step * grow, max_step)\n\n            # after probes: if no improvement, shrink; else do small Gaussian polishing\n            if not improved_round:\n                no_improve += 1\n                step = max(min_step, step * shrink)\n            else:\n                # full-space polishing: a few small isotropic tries\n                extras = min(6, max(1, int(np.ceil(np.log(1 + n)))))\n                for _ in range(extras):\n                    if evals >= self.budget:\n                        break\n                    d_full = np.random.randn(n)\n                    d_full /= (np.linalg.norm(d_full) + 1e-20)\n                    a = np.random.uniform(-0.4 * step, 0.4 * step)\n                    x_try = clip_to_bounds(x_cur + a * d_full)\n                    f_try, x_try = safe_eval(x_try)\n                    if f_try is None:\n                        break\n                    if f_try < f_cur - 1e-12:\n                        dir_succ = x_try - x_cur\n                        dn2 = np.linalg.norm(dir_succ)\n                        if dn2 > 0:\n                            dir_succ = dir_succ / dn2\n                            dir_memory.appendleft(dir_succ.copy())\n                        x_cur = x_try.copy()\n                        f_cur = f_try\n\n            # stagnation handling: restart biased around best if available\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                no_improve = 0\n                if restarts > max_restarts:\n                    # stronger shrink and clear memory lightly\n                    step = max(min_step, 0.5 * step)\n                    if len(dir_memory) > 0:\n                        # keep only most recent 1/2\n                        keep = max(1, len(dir_memory)//2)\n                        dir_memory = deque(list(dir_memory)[:keep], maxlen=self.memory_size)\n                else:\n                    # restart around best with moderate perturbation amplitude\n                    if x_best is not None:\n                        perturb = np.random.randn(n) * (0.25 * domain_mean)\n                        x_cur = clip_to_bounds(x_best + perturb)\n                        f_new, x_cur = safe_eval(x_cur)\n                        if f_new is None:\n                            break\n                        f_cur = f_new\n                    else:\n                        x_cur = np.random.uniform(lb, ub)\n                        f_new, x_cur = safe_eval(x_cur)\n                        if f_new is None:\n                            break\n                        f_cur = f_new\n                    # clear memory to encourage fresh exploration\n                    dir_memory = deque(maxlen=self.memory_size)\n                    # expand step for exploration\n                    step = min(max_step, step * 1.5)\n\n            # tiny safeguard\n            if step < min_step:\n                step = min_step\n\n            # quick exit if objective very low\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            return float(f_cur), np.array(x_cur, dtype=float)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 88, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_memory = deque(maxlen=self.memory_size)", "error": "In the code, line 88, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_memory = deque(maxlen=self.memory_size)", "parent_ids": "dbfb0c67-e5d2-4c51-89f0-3034b212c7f1", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "7ee407d4-2c7e-4e3c-b6cd-ac292b055b35", "fitness": "-inf", "name": "COMETS", "description": "COMETS alternates multiscale subspace probing (two scales k_small ≈ sqrt(n) and k_med ≈ n^(2/3)) by building orthonormal subspace bases with QR and running a fixed number of anisotropic probes per round to balance exploration and exploitation. It biases sampling with an LRU memory of recent unit moves and a lightweight diagonal covariance estimate (cov_alpha EMA) and uses a momentum‑biased, Langevin‑style proposal (momentum_decay) mixing historical direction and freshly sampled subspace directions. Step-size is multiplicatively adapted (grow=1.20 / shrink=0.78, init step = 0.3·domain_mean) and intensified with a very budget-aware short golden-section line search; occasional annealed uphill acceptance (temp0 and tiny acceptance probability) helps escape local minima. Stagnation triggers targeted restarts or shrinking, and the algorithm includes short full-space polishing probes, strict bound clipping and strict budget accounting to respect evaluation limits.", "code": "import numpy as np\n\nclass COMETS:\n    \"\"\"\n    COMETS: COmbined Multi-scale covariance- and Memory-Enhanced Two-Scale search\n    - Alternates multiscale subspace probing (small sqrt(n) and medium n^(2/3)),\n      biases subspaces with an LRU memory and a lightweight covariance estimate,\n      uses momentum-biased Langevin proposals, adaptive multiplicative step-size,\n      very budget-aware short line-search for intensification, occasional annealed uphill moves,\n      and restarts/polishing on stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, memory_size=16):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.memory_size = int(memory_size)\n\n    def __call__(self, func):\n        n = self.dim\n        # accept scalar or vector bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        domain_mean = np.mean(ub - lb)\n        # evaluation bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def clip_to_bounds(x):\n            return np.clip(x, lb, ub)\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(x)\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initialization: start random\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, x_tmp = safe_eval(x_cur)\n        if f_cur is None:\n            # no budget\n            return float(f_best), np.array(x_best, dtype=float)\n        x_cur = x_tmp.copy()\n\n        # parameters\n        step = 0.3 * domain_mean\n        min_step = 1e-7 * max(1.0, domain_mean)\n        max_step = 5.0 * domain_mean\n        grow = 1.20\n        shrink = 0.78\n        # multi-scale subspace sizes\n        k_small = max(1, int(np.ceil(np.sqrt(n))))\n        k_med = max(1, int(np.ceil(n ** (2.0 / 3.0))))\n        # probes per round (adaptive)\n        probes_small = max(4, 2 * k_small)\n        probes_med = max(6, 3 * k_med)\n        # memory and lightweight covariance (diagonal-ish) for biasing\n        dir_memory = []  # LRU of unit directions\n        cov_diag = np.ones(n) * (step ** 2 * 0.5)  # running diagonal covariance estimate\n        cov_alpha = 0.15  # EMA rate for covariance updates\n        momentum = np.zeros(n)\n        momentum_decay = 0.85\n\n        # stagnation/restarts\n        no_improve = 0\n        stagnation_limit = max(12, int(6 * np.log(1 + n)))\n        max_restarts = 6\n        restarts = 0\n\n        # simple budget-aware short golden-section like 1D intensification\n        def short_line_search(x0, f0, d, init_step=1.0, max_evals=10):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                return None, None\n            d = d / dn\n            remaining = max(0, self.budget - evals)\n            if remaining <= 0:\n                return None, None\n            s = float(abs(init_step))\n            # coarse checks: 0, +s, -s (up to 2)\n            fa = f0\n            xb = clip_to_bounds(x0 + s * d)\n            fb, _ = safe_eval(xb)\n            if fb is None:\n                return None, None\n            remaining = max(0, self.budget - evals)\n            if fb < fa:\n                a, b = 0.0, s\n            else:\n                xb2 = clip_to_bounds(x0 - s * d)\n                fb2, _ = safe_eval(xb2)\n                if fb2 is None:\n                    return None, None\n                remaining = max(0, self.budget - evals)\n                if fb2 < fa:\n                    a, b = -s, 0.0\n                else:\n                    return None, None\n            # golden-section refinement, but very limited iterations\n            gr = (np.sqrt(5) - 1) / 2\n            c = b - gr * (b - a)\n            d_alpha = a + gr * (b - a)\n            xc = clip_to_bounds(x0 + c * d)\n            fc, _ = safe_eval(xc)\n            if fc is None:\n                return None, None\n            xd = clip_to_bounds(x0 + d_alpha * d)\n            fd, _ = safe_eval(xd)\n            if fd is None:\n                return None, None\n            best_f = f0\n            best_x = x0.copy()\n            for val, px in ((fb, clip_to_bounds(x0 + s*d)), (fc, xc), (fd, xd)):\n                if val is not None and val < best_f:\n                    best_f = val; best_x = px.copy()\n            iters = 0\n            while remaining > 0 and iters < max_evals:\n                iters += 1\n                if fc < fd:\n                    b = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    c = b - gr * (b - a)\n                    xc = clip_to_bounds(x0 + c * d)\n                    fc, _ = safe_eval(xc)\n                    if fc is None:\n                        break\n                else:\n                    a = c\n                    c = d_alpha\n                    fc = fd\n                    d_alpha = a + gr * (b - a)\n                    xd = clip_to_bounds(x0 + d_alpha * d)\n                    fd, _ = safe_eval(xd)\n                    if fd is None:\n                        break\n                remaining = max(0, self.budget - evals)\n                for val, px in ((fc, xc), (fd, xd)):\n                    if val is not None and val < best_f:\n                        best_f = val; best_x = px.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # main loop\n        toggle_scale = 0\n        # annealing temperature for occasional uphill acceptance (helps escape)\n        temp0 = max(1e-3, abs(f_cur) + 1.0)\n        iters = 0\n        while evals < self.budget:\n            iters += 1\n            # alternate scale\n            if toggle_scale % 3 == 0:\n                k = k_med\n                probes = probes_med\n            else:\n                k = k_small\n                probes = probes_small\n            toggle_scale += 1\n\n            improved = False\n\n            # build basis: reuse some memory directions (up to half), then random\n            use_mem = min(len(dir_memory), max(0, k // 2))\n            basis = np.zeros((n, 0))\n            if use_mem > 0:\n                mem_sel = np.array(dir_memory[:use_mem])\n                # ensure they are column stacked\n                basis = np.column_stack((basis, mem_sel.T))\n            needed = k - basis.shape[1]\n            if needed > 0:\n                R = np.random.randn(n, needed)\n                if basis.size > 0:\n                    R = np.column_stack((basis, R))\n                Q, _ = np.linalg.qr(R)\n                basis = Q[:, :k]\n            else:\n                Q, _ = np.linalg.qr(basis)\n                basis = Q[:, :k]\n\n            # project cov_diag onto subspace (diagonal approx): scale sampling along basis columns\n            basis_scale = np.sqrt(np.maximum(1e-12, (basis ** 2) @ cov_diag))\n            # probe in subspace using momentum-biased Langevin-like proposals\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample in subspace with anisotropic scaling from cov_diag approx\n                coeffs = np.random.randn(k) * (np.clip(basis_scale, 1e-12, None)[:k])\n                d_sub = basis @ coeffs\n                dn = np.linalg.norm(d_sub)\n                if dn == 0:\n                    continue\n                d_unit = d_sub / dn\n                # combine momentum with new direction\n                cand_dir = 0.55 * momentum + 0.45 * d_unit\n                if np.linalg.norm(cand_dir) == 0:\n                    cand_dir = d_unit\n                cand_dir = cand_dir / np.linalg.norm(cand_dir)\n                # draw length from uniform plus small gaussian perturbation\n                length = np.random.uniform(-step, step) + np.random.randn() * 0.12 * step\n                x_prop = clip_to_bounds(x_cur + length * cand_dir)\n                f_prop, x_prop = safe_eval(x_prop)\n                if f_prop is None:\n                    break\n                # accept if improved; with small annealed probability accept slight worsening to escape\n                accept = False\n                if f_prop < f_cur - 1e-12:\n                    accept = True\n                else:\n                    # occasional uphill acceptance decreasing with time and magnitude\n                    delta = f_prop - f_cur\n                    # temperature decays with iterations and improvement count\n                    temp = temp0 / (1.0 + 0.03 * iters)\n                    prob = np.exp(-max(0.0, delta) / (temp + 1e-12))\n                    if np.random.rand() < (0.01 * prob):\n                        accept = True\n                if accept:\n                    # update momentum toward this move\n                    move = x_prop - x_cur\n                    dn2 = np.linalg.norm(move)\n                    if dn2 > 0:\n                        unit_move = move / dn2\n                        momentum = momentum * momentum_decay + 0.9 * unit_move\n                        # update covariance diagonal approx by EMA of squared component magnitudes\n                        cov_diag = (1 - cov_alpha) * cov_diag + cov_alpha * (move ** 2 + 1e-16)\n                        # update memory LRU\n                        dir_memory.insert(0, unit_move.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                    x_cur = x_prop.copy()\n                    f_cur = f_prop\n                    improved = True\n                    no_improve = 0\n                    # try a short line-search along the accepted direction with tiny budget\n                    remaining = max(0, self.budget - evals)\n                    if remaining >= 2:\n                        ls_budget = min(8, remaining)\n                        f_ls, x_ls = short_line_search(x_cur, f_cur, momentum if np.linalg.norm(momentum) > 0 else cand_dir, init_step=abs(length) + 0.6 * step, max_evals=ls_budget)\n                        if f_ls is not None and f_ls < f_cur - 1e-12:\n                            # update based on line search\n                            # update covariance/memory similarly\n                            move2 = x_ls - x_cur\n                            dn3 = np.linalg.norm(move2)\n                            if dn3 > 0:\n                                unit_move2 = move2 / dn3\n                                momentum = momentum * momentum_decay + 0.7 * unit_move2\n                                cov_diag = (1 - cov_alpha) * cov_diag + cov_alpha * (move2 ** 2 + 1e-16)\n                                dir_memory.insert(0, unit_move2.copy())\n                                if len(dir_memory) > self.memory_size:\n                                    dir_memory.pop()\n                            x_cur = x_ls.copy()\n                            f_cur = f_ls\n                    # adapt step up\n                    step = min(step * grow, max_step)\n                else:\n                    # small chance to do focused short line search along candidate dir\n                    if np.random.rand() < 0.05 and (self.budget - evals) >= 3:\n                        ls_budget = min(5, self.budget - evals)\n                        f_ls, x_ls = short_line_search(x_cur, f_cur, cand_dir, init_step=step, max_evals=ls_budget)\n                        if f_ls is not None and f_ls < f_cur - 1e-12:\n                            move2 = x_ls - x_cur\n                            dn3 = np.linalg.norm(move2)\n                            if dn3 > 0:\n                                unit_move2 = move2 / dn3\n                                momentum = momentum * momentum_decay + 0.8 * unit_move2\n                                cov_diag = (1 - cov_alpha) * cov_diag + cov_alpha * (move2 ** 2 + 1e-16)\n                                dir_memory.insert(0, unit_move2.copy())\n                                if len(dir_memory) > self.memory_size:\n                                    dir_memory.pop()\n                            x_cur = x_ls.copy()\n                            f_cur = f_ls\n                            improved = True\n                            no_improve = 0\n                            step = min(step * grow, max_step)\n\n            # adapt step if no improvement this round\n            if not improved:\n                no_improve += 1\n                step = max(min_step, step * shrink)\n                momentum *= momentum_decay\n            else:\n                # local polishing: small gaussian full-space probes\n                extras = min(6, max(1, int(np.ceil(np.log(1 + n)))))\n                for _ in range(extras):\n                    if evals >= self.budget:\n                        break\n                    d = np.random.randn(n)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                    a = np.random.uniform(-0.45 * step, 0.45 * step)\n                    f_try, x_try = safe_eval(clip_to_bounds(x_cur + a * d))\n                    if f_try is None:\n                        break\n                    if f_try < f_cur - 1e-12:\n                        move = x_try - x_cur\n                        dn = np.linalg.norm(move)\n                        if dn > 0:\n                            unit_move = move / dn\n                            dir_memory.insert(0, unit_move.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                            cov_diag = (1 - cov_alpha) * cov_diag + cov_alpha * (move ** 2 + 1e-16)\n                            momentum = momentum * momentum_decay + 0.7 * unit_move\n                        x_cur = x_try.copy()\n                        f_cur = f_try\n\n            # stagnation handling\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # reduce step, clear tiny memory, and continue for final polish\n                    step = max(step * 0.4, min_step)\n                    momentum *= 0.3\n                    no_improve = 0\n                else:\n                    # restart near best (diversify) with enlarged step\n                    if x_best is not None:\n                        perturb = np.random.randn(n) * (0.45 * domain_mean)\n                        x_try = clip_to_bounds(x_best + perturb)\n                        f_try, x_try = safe_eval(x_try)\n                        if f_try is None:\n                            break\n                        x_cur = x_try.copy()\n                        f_cur = f_try\n                    else:\n                        # sample fresh\n                        x_try = np.random.uniform(lb, ub)\n                        f_try, x_try = safe_eval(x_try)\n                        if f_try is None:\n                            break\n                        x_cur = x_try.copy()\n                        f_cur = f_try\n                    dir_memory = []\n                    cov_diag = np.ones(n) * (step ** 2 * 0.5)\n                    momentum *= 0.0\n                    step = min(max_step, step * 1.6)\n                    no_improve = 0\n\n            # enforce bounds on step\n            if step < min_step:\n                step = min_step\n            if step > max_step:\n                step = max_step\n\n            # quick termination if extremely low objective\n            if f_best <= 1e-12:\n                break\n\n        # return best found\n        if x_best is None:\n            return float(f_cur), np.array(x_cur, dtype=float)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 203, in __call__, the following error occurred:\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 5)\nOn line: basis_scale = np.sqrt(np.maximum(1e-12, (basis ** 2) @ cov_diag))", "error": "In the code, line 203, in __call__, the following error occurred:\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 5)\nOn line: basis_scale = np.sqrt(np.maximum(1e-12, (basis ** 2) @ cov_diag))", "parent_ids": "dbfb0c67-e5d2-4c51-89f0-3034b212c7f1", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5754ed50-11f9-4e44-adf4-05ee5ea1b444", "fitness": "-inf", "name": "MBETQS", "description": "The algorithm starts with a budget-aware, space-filling greedy max–min initialization (controlled by init_samples_ratio, min_init and max_init_frac) and scales initial trust radius and global step sigma to the problem domain using init_step_factor. It alternates budget-limited local modeling and directed probes: when enough samples exist it fits a ridge-regularized separable quadratic (intercept + linear + diagonal Hessian) around the current best to propose trust-limited model steps and does 1-D golden/ parabolic polishing, otherwise it performs multi-scale directional probes biased by a low-rank LRU memory of successful, orthonormalized directions and an evolution-path accumulator; multiplicative step-size/trust adaptation (success_alpha / failure_alpha), sigma bounds and max_eval_per_iter keep behavior stable and budget-aware. Global exploration uses occasional heavy-tailed jumps (jump_prob) and stochastic evolution-path line-searches (line_prob), with stagnation-triggered soft restarts that preserve part of memory, archive pruning to limit storage, and budget- and stagnation-aware final polishing.", "code": "import numpy as np\nfrom collections import deque\n\nclass MBETQS:\n    \"\"\"\n    Memory-Biased Ensemble Trust-region Quadratic Search (MBETQS)\n\n    - Space-filling initialization (greedy max-min).\n    - Separable quadratic local surrogate (ridge-regularized) around best when data allow.\n    - Low-rank LRU memory of successful directions (orthonormalized) to bias multi-scale directional probes.\n    - Evolution-path accumulator used for occasional budget-aware 1-D line-search polishing.\n    - Multiplicative step-size / trust-radius adaptation on success/failure.\n    - Heavy-tailed jumps for global escape and stagnation-aware soft restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=12, init_samples_ratio=0.12, min_init=None,\n                 max_init_frac=0.4, max_eval_per_iter=80,\n                 init_step_factor=0.45, success_alpha=1.15, failure_alpha=0.78,\n                 evo_decay=0.85, line_prob=0.35, jump_prob=0.12):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.init_samples_ratio = float(init_samples_ratio)\n        self.min_init = int(min_init) if min_init is not None else max(8, 2 * self.dim)\n        self.max_init_frac = float(max_init_frac)\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        self.init_step_factor = float(init_step_factor)\n        self.success_alpha = float(success_alpha)\n        self.failure_alpha = float(failure_alpha)\n        self.evo_decay = float(evo_decay)\n        self.line_prob = float(line_prob)\n        self.jump_prob = float(jump_prob)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        domain = ub - lb\n        domain_size = np.mean(domain)\n\n        evals = 0\n        X = []  # archive points\n        F = []\n\n        f_best = np.inf\n        x_best = None\n\n        # safe eval wrapper\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            if evals >= budget:\n                return None, None\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X.append(x.copy()); F.append(f)\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            return f, x\n\n        # --- space-filling initialization (greedy max-min from a pool) ---\n        max_init = min(int(self.max_init_frac * budget), int(0.5 * budget))\n        init_budget = int(np.clip(self.init_samples_ratio * budget, self.min_init, max_init))\n        init_budget = max(1, init_budget)\n        pool_factor = 6\n        pool_size = max(int(init_budget * pool_factor), init_budget + 20)\n        pool = np.random.uniform(lb, ub, size=(pool_size, n))\n        chosen = []\n        for i in range(init_budget):\n            if i == 0:\n                idx = np.random.randint(0, pool.shape[0])\n                chosen.append(pool[idx])\n            else:\n                cur = np.vstack(chosen)\n                # compute min distance of each pool point to chosen set\n                dists = np.min(np.linalg.norm(pool - cur[:, None, :], axis=2), axis=0)\n                idx = int(np.argmax(dists))\n                chosen.append(pool[idx])\n        for x in chosen:\n            if evals >= budget:\n                break\n            out = safe_eval(x)\n            if out[0] is None:\n                break\n\n        if evals >= budget:\n            return float(f_best), (x_best.copy() if x_best is not None else None)\n\n        # initialize trust radius and scalar sigma (global step)\n        trust_radius = np.maximum(self.init_step_factor * domain, 1e-12)\n        sigma = max(self.init_step_factor * domain_size, 1e-12)\n        sigma_min = 1e-9 * max(1.0, domain_size)\n        sigma_max = 5.0 * domain_size\n\n        # memory of directions (LRU)\n        dir_mem = deque(maxlen=self.memory_size)\n        evo_path = np.zeros(n)\n\n        # stagnation handling\n        no_improve_iters = 0\n        stagnation_limit = max(12, int(10 + np.log1p(n) * 3))\n        restarts = 0\n        max_restarts = 6\n\n        # helper: build orthonormal basis from dir_mem\n        def build_basis():\n            if len(dir_mem) == 0:\n                return None\n            M = np.column_stack(list(dir_mem))\n            try:\n                Q, _ = np.linalg.qr(M)\n            except Exception:\n                # normalize columns as fallback\n                cols = []\n                for v in M.T:\n                    vn = np.linalg.norm(v)\n                    if vn > 1e-16:\n                        cols.append(v / vn)\n                if len(cols) == 0:\n                    return None\n                Q = np.column_stack(cols)\n            return Q\n\n        # small budget-aware golden-section like line search (limited)\n        def line_search(x0, f0, d, init_step=None, max_evals=12):\n            nonlocal evals\n            if evals >= budget:\n                return None, None\n            if init_step is None:\n                init_step = sigma\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                return None, None\n            d = d / dn\n            remain = max(0, budget - evals)\n            if remain <= 0:\n                return None, None\n            # try +init and -init\n            a0 = 0.0; fa = f0\n            b = init_step\n            xb = np.clip(x0 + b * d, lb, ub)\n            out = safe_eval(xb)\n            if out[0] is None:\n                return None, None\n            fb, xb = out\n            remain = max(0, budget - evals)\n            if fb >= fa:\n                b = -init_step\n                xb = np.clip(x0 + b * d, lb, ub)\n                out = safe_eval(xb)\n                if out[0] is None:\n                    return None, None\n                fb, xb = out\n                remain = max(0, budget - evals)\n                if fb >= fa:\n                    return None, None\n            # limited bracket expansion\n            expand = 1.5\n            max_exp = 3\n            exp_count = 0\n            while remain > 0 and exp_count < max_exp:\n                new_b = b * expand\n                xn = np.clip(x0 + new_b * d, lb, ub)\n                out = safe_eval(xn)\n                if out[0] is None:\n                    return None, None\n                fn, xn = out\n                remain = max(0, budget - evals)\n                if fn < fb:\n                    b = new_b; fb = fn\n                    exp_count += 1\n                    continue\n                break\n            if remain <= 0:\n                if fb < fa:\n                    return fb, np.clip(x0 + b * d, lb, ub)\n                return None, None\n            # golden section between 0 and b\n            gr = (np.sqrt(5) - 1) / 2\n            a = a0; b0 = b\n            c = b0 - gr * (b0 - a)\n            d_alpha = a + gr * (b0 - a)\n            xc = np.clip(x0 + c * d, lb, ub); out = safe_eval(xc)\n            if out[0] is None:\n                return None, None\n            fc, xc = out\n            xd = np.clip(x0 + d_alpha * d, lb, ub); out = safe_eval(xd)\n            if out[0] is None:\n                return None, None\n            fd, xd = out\n            remain = max(0, budget - evals)\n            bestf = fa; bestx = x0.copy()\n            for val, xx in ((fc, xc), (fd, xd)):\n                if val < bestf:\n                    bestf = val; bestx = xx.copy()\n            it = 0\n            max_it = min(max_evals, 8)\n            while remain > 0 and it < max_it and abs(b0 - a) > 1e-12:\n                it += 1\n                if fc < fd:\n                    b0 = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    d_alpha = a + gr * (b0 - a)\n                    xd = np.clip(x0 + d_alpha * d, lb, ub)\n                    out = safe_eval(xd)\n                    if out[0] is None:\n                        break\n                    fd, xd = out\n                else:\n                    a = c\n                    c = d_alpha\n                    fc = fd\n                    c = b0 - gr * (b0 - a)\n                    xc = np.clip(x0 + c * d, lb, ub)\n                    out = safe_eval(xc)\n                    if out[0] is None:\n                        break\n                    fc, xc = out\n                remain = max(0, budget - evals)\n                for val, xx in ((fc, xc), (fd, xd)):\n                    if val < bestf:\n                        bestf = val; bestx = xx.copy()\n            if bestf < f0 - 1e-15:\n                return bestf, bestx\n            return None, None\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            improved = False\n\n            # --- surrogate separable quadratic if enough data ---\n            neighbors_needed = max(2 * n + 1, 6 * n)\n            if len(X) >= neighbors_needed and work_allow >= 2:\n                X_arr = np.asarray(X)\n                F_arr = np.asarray(F)\n                # pick nearest neighbors to x_best\n                dists = np.linalg.norm(X_arr - x_best, axis=1)\n                idx_sorted = np.argsort(dists)[:min(len(X), neighbors_needed)]\n                X_nei = X_arr[idx_sorted]\n                F_nei = F_arr[idx_sorted]\n                dx = X_nei - x_best\n                m = dx.shape[0]\n                # design matrix for intercept + linear + diag quadratic\n                M = np.ones((m, 1 + n + n))\n                M[:, 1:1 + n] = dx\n                M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n                y = F_nei\n                # weights: Gaussian kernel on distance\n                bw = np.median(dists[idx_sorted]) + 1e-12\n                w = np.exp(- (dists[idx_sorted] ** 2) / (2.0 * bw * bw + 1e-12))\n                W = np.sqrt(np.maximum(w, 1e-12))[:, None]\n                A = W * M\n                b = W * y\n                ridge = 1e-6 * (1.0 + np.mean(np.abs(y))) * np.eye(M.shape[1])\n                try:\n                    params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ b, rcond=None)\n                    params = params.flatten()\n                    b_lin = params[1:1 + n]\n                    h_diag = params[1 + n:1 + 2 * n]\n                    # positive curvature floor\n                    h_reg = np.copy(h_diag)\n                    h_reg[h_reg < 1e-8] = 1e-8\n                    delta_model = -b_lin / (h_reg + 1e-20)\n                    # limit by trust_radius\n                    delta_limited = np.clip(delta_model, -trust_radius, trust_radius)\n                    x_model = np.clip(x_best + delta_limited, lb, ub)\n                    # evaluate model candidate\n                    if work_allow > 0 and evals < budget:\n                        out = safe_eval(x_model)\n                        work_allow -= 1\n                        if out[0] is not None:\n                            f_model, x_model = out\n                            if f_model < f_best - 1e-12:\n                                # accept, update trust and memory\n                                improved = True\n                                no_improve_iters = 0\n                                trust_radius = np.minimum(trust_radius * self.success_alpha, np.maximum(trust_radius, domain * 2.0))\n                                # update evo and dir_mem\n                                disp = x_model - x_best\n                                dn = np.linalg.norm(disp)\n                                if dn > 1e-16:\n                                    dir_mem.appendleft((disp / dn).copy())\n                                    evo_path = self.evo_decay * evo_path + (1.0 - self.evo_decay) * (disp)\n                                x_best = x_model.copy(); f_best = float(f_model)\n                    else:\n                        # shrink trust if couldn't evaluate\n                        trust_radius = np.maximum(trust_radius * self.failure_alpha, 1e-12 * domain)\n                except Exception:\n                    # regression failed: ignore\n                    pass\n\n            # If surrogate didn't improve, do memory-biased multi-scale probing\n            if not improved and work_allow > 0:\n                basis = build_basis()  # n x m or None\n                probes = min( max(6, 3 + int(np.sqrt(n)) ), work_allow )\n                # multi-scale factors\n                scales = np.array([0.25, 0.5, 1.0, 1.6])\n                # sample several candidate directions\n                for _ in range(probes):\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    # sample direction as mixture\n                    if basis is not None and basis.shape[1] > 0 and np.random.rand() < 0.65:\n                        coeffs = np.random.randn(basis.shape[1])\n                        mem_comp = basis @ coeffs\n                        mem_comp = mem_comp / (np.linalg.norm(mem_comp) + 1e-20)\n                        iso = np.random.randn(n); iso = iso / (np.linalg.norm(iso) + 1e-20)\n                        mix = 0.6\n                        d = mix * mem_comp + (1.0 - mix) * iso\n                    else:\n                        d = np.random.randn(n)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                    # pick a random scale among scales weighted toward 1.0\n                    if np.random.rand() < 0.3:\n                        s = scales[np.random.randint(len(scales))]\n                    else:\n                        s = 1.0\n                    step_len = s * sigma\n                    # try positive and negative\n                    for sign in (+1.0, -1.0):\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        x_try = np.clip(x_best + sign * step_len * d, lb, ub)\n                        # avoid duplicate evaluation of same last\n                        if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out[0] is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            # accept, small parabolic refinement along d if budget allows\n                            improved = True\n                            no_improve_iters = 0\n                            disp = x_try - x_best\n                            dn = np.linalg.norm(disp)\n                            if dn > 1e-16:\n                                dir_mem.appendleft((disp / dn).copy())\n                                evo_path = self.evo_decay * evo_path + (1.0 - self.evo_decay) * disp\n                            x_best = x_try.copy(); f_best = float(f_try)\n                            sigma = min(sigma * self.success_alpha, sigma_max)\n                            # micro-polish: cheap 1D parabolic if at least 2 evals left\n                            if (budget - evals) >= 2:\n                                # evaluate at a half-step and quarter-step if possible\n                                a1 = sign * 0.5 * step_len\n                                a2 = sign * 1.5 * step_len\n                                xp = np.clip(x_best + a1 * d, lb, ub)\n                                if (budget - evals) > 0:\n                                    outp = safe_eval(xp); work_allow -= 1\n                                    if outp[0] is not None:\n                                        fp, xp = outp\n                                        if fp < f_best - 1e-12:\n                                            x_best = xp.copy(); f_best = float(fp)\n                                            # update memory\n                                            disp2 = xp - x_try\n                                            dn2 = np.linalg.norm(disp2)\n                                            if dn2 > 1e-16:\n                                                dir_mem.appendleft((disp2 / dn2).copy())\n                            break\n                        else:\n                            # no improvement here\n                            sigma = max(sigma * self.failure_alpha, sigma_min)\n                            no_improve_iters += 1\n\n            # occasional evolution-path line search (if path nontrivial)\n            if (np.linalg.norm(evo_path) > 1e-12) and (np.random.rand() < self.line_prob) and (budget - evals) >= 3:\n                # evaluate current best then line search along evo_path\n                f0 = f_best\n                res = line_search(x_best, f0, evo_path, init_step=sigma, max_evals=min(8, budget - evals))\n                if res is not None:\n                    f_ls, x_ls = res\n                    if f_ls is not None and f_ls < f_best - 1e-12:\n                        disp = x_ls - x_best\n                        dn = np.linalg.norm(disp)\n                        if dn > 1e-16:\n                            dir_mem.appendleft((disp / dn).copy())\n                            evo_path = self.evo_decay * evo_path + (1.0 - self.evo_decay) * disp\n                        x_best = x_ls.copy(); f_best = float(f_ls)\n                        sigma = min(sigma * self.success_alpha, sigma_max)\n                        improved = True\n                        no_improve_iters = 0\n\n            # heavy-tailed jump occasionally\n            if (np.random.rand() < self.jump_prob) and evals < budget:\n                df = 1.5\n                g = np.random.randn(n)\n                chi2 = np.random.chisquare(df, size=1)[0]\n                t_sample = g / np.sqrt(chi2 / df + 1e-12)\n                t_sample = np.clip(t_sample, -12, 12)\n                scale = 0.6 * domain\n                x_jump = np.clip(x_best + t_sample * scale, lb, ub)\n                out = safe_eval(x_jump)\n                if out[0] is not None:\n                    f_jump, x_jump = out\n                    if f_jump < f_best - 1e-12:\n                        # accept jump and incorporate direction\n                        disp = x_jump - x_best\n                        dn = np.linalg.norm(disp)\n                        if dn > 1e-16:\n                            dir_mem.appendleft((disp / dn).copy())\n                            evo_path = self.evo_decay * evo_path + (1.0 - self.evo_decay) * disp\n                        x_best = x_jump.copy(); f_best = float(f_jump)\n                        sigma = min(sigma * self.success_alpha * 1.1, sigma_max)\n                        improved = True\n                        no_improve_iters = 0\n                    else:\n                        # penalize sigma/trust\n                        sigma = max(sigma * self.failure_alpha * 0.9, sigma_min)\n\n            # if nothing improved this iteration, shrink a bit\n            if not improved:\n                no_improve_iters += 1\n                sigma = max(sigma * self.failure_alpha, sigma_min)\n                # small decay of evo_path (forget stale)\n                evo_path = self.evo_decay * evo_path\n            else:\n                no_improve_iters = 0\n\n            # stagnation / restart\n            if no_improve_iters >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final polishing: small local random probes until budget depleted\n                    while evals < budget:\n                        d = np.random.randn(n)\n                        d /= (np.linalg.norm(d) + 1e-20)\n                        step = 0.08 * sigma\n                        x_try = np.clip(x_best + (np.random.uniform(-1, 1) * step) * d, lb, ub)\n                        out = safe_eval(x_try)\n                        if out[0] is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            f_best = float(f_try); x_best = x_try.copy()\n                    break\n                else:\n                    # soft restart around best, preserve a small fraction of memory\n                    keep = max(1, int(0.4 * len(dir_mem)))\n                    oldmem = list(dir_mem)[:keep]\n                    dir_mem = deque(oldmem, maxlen=self.memory_size)\n                    # perturb current best to resume search\n                    perturb = np.random.randn(n) * (0.4 * domain)\n                    x_new = np.clip(x_best + perturb, lb, ub)\n                    out = safe_eval(x_new)\n                    if out[0] is None:\n                        break\n                    f_new, x_new = out\n                    if f_new < f_best - 1e-12:\n                        x_best = x_new.copy(); f_best = float(f_new)\n                    # enlarge sigma to escape\n                    sigma = min(1.2 * sigma + 0.2 * domain_size, sigma_max)\n                    no_improve_iters = 0\n\n            # archive pruning to control memory\n            max_archive = max(2000, 50 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # quick exit if near-optimal\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), (x_best.copy() if x_best is not None else None)", "configspace": "", "generation": 0, "feedback": "In the code, line 111, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_mem = deque(maxlen=self.memory_size)", "error": "In the code, line 111, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_mem = deque(maxlen=self.memory_size)", "parent_ids": "dbfb0c67-e5d2-4c51-89f0-3034b212c7f1", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "dd5952f4-6c1c-4ad0-88fc-9c8573e0aa0f", "fitness": 0.4109754667484141, "name": "AS3", "description": "The algorithm maintains a searchable center m with a global step-size sigma and adaptive per-coordinate scales s, plus an orthonormal rotation R and a learned low-rank subspace U (k ≈ sqrt(dim)) extracted from a small success buffer to capture promising directions. Candidates are generated as a mirrored Gaussian population in the local (s-scaled) coordinates rotated by R, with occasional heavy-tailed Cauchy jumps, DE-style differences from a global archive, and cheap block-wise permutations to inject combinatorial variation. Adaptation is multi-scale: s is updated by EMA of squared local steps, sigma is adjusted by a success-rate heuristic, the learned subspace is gently mixed into R via eigendecomposition+QR, and a 1D quadratic surrogate along the principal U[:,0] is probed for fast exploitation; temperature-based acceptance and bounded archives manage exploration/exploitation and budget. Stagnation triggers mild restarts (sigma reheating, random block swaps, R perturbation), parameter caps (clipping/QR), and memory limits to keep behavior robust and budget-aware.", "code": "import numpy as np\n\nclass AS3:\n    \"\"\"\n    Adaptive Subspace-Surrogate Search (AS3)\n\n    Main ideas:\n    - Maintain a searchable center m, a global step-size sigma and per-coordinate scales s.\n    - Learn a low-rank subspace (U) of promising directions from a small buffer of successful steps\n      using covariance-eigen decomposition (cheap because buffer is small). Gently mix the learned\n      subspace into a full-space orthonormal rotation R.\n    - Propose a small mirrored population of Gaussian steps in the local (per-coordinate scaled)\n      space then rotated by R. Include occasional Cauchy (Lévy-like) jumps and DE-like archive diffs.\n    - Build a 1D quadratic surrogate along the principal learned direction from recent samples\n      and attempt a bounded quadratic step to accelerate exploitation.\n    - Adapt per-coordinate scales via EMA of squared local step magnitudes and adapt sigma via\n      success-rate control. Use temperature-based acceptance to escape local traps.\n    - Use block partitions to allow cheap intra-block permutations to explore combinatorial structure,\n      and rarer block reassignments on stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, blocks=None, pop_factor=3.0):\n        \"\"\"\n        Args:\n            budget (int): evaluation budget.\n            dim (int): problem dimension.\n            seed (int|None): RNG seed for reproducibility.\n            blocks (int|None): number of blocks (None -> heuristic from dim).\n            pop_factor (float): scale for population size ~ pop_factor * log(dim).\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.seed = seed\n\n        # population and buffers\n        self.lambda_ = max(4, int(max(4, np.log(max(2, self.dim))) * pop_factor))\n        if blocks is None:\n            self.n_blocks = max(1, int(np.ceil(np.sqrt(self.dim) / 1.6)))\n        else:\n            self.n_blocks = max(1, min(blocks, self.dim))\n\n        # block partition: contiguous initially\n        sizes = [self.dim // self.n_blocks] * self.n_blocks\n        for i in range(self.dim % self.n_blocks):\n            sizes[i] += 1\n        idx = 0\n        self.blocks = []\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n        # subspace rank (small)\n        self.k = min(max(1, int(np.ceil(np.sqrt(self.dim)))), self.dim)\n\n        # parameters\n        self.p_cauchy = 0.12\n        self.cauchy_scale = 1.0\n        self.p_de = 0.18\n        self.F_de = 0.6\n        self.p_blockswap = 0.14\n        self.mirrored = True\n\n        # surrogate window sizes\n        self.success_buf_max = max(20, 6 * int(np.ceil(np.sqrt(self.dim))))\n        self.local_archive_max = 200  # keep some recent evaluations for surrogate fitting\n\n        # adaptation rates\n        self.s_ema_rate = 0.18\n        self.subspace_mix = 0.15  # how strongly to mix new subspace into R\n        self.sigma_inflate = 1.15\n        self.sigma_deflate = 0.94\n\n        # stagnation controls\n        self.stagn_limit = max(20, int(6 * self.dim / max(1, self.n_blocks)))\n        self.reheat_mult = 1.6\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (expected -5..5 but read)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # state variables\n        m = np.random.uniform(lb, ub)                      # center\n        sigma = 0.16 * np.mean(ub - lb)                    # initial step-size\n        s = np.ones(n)                                     # per-coordinate scales\n        R = np.eye(n)                                      # rotation (orthonormal)\n        # low-rank learned subspace U (n x k), init as first k identity columns (or random)\n        U = np.eye(n)[:, :self.k].copy()\n\n        # buffers\n        success_buf = []          # store recent successful rotated steps (pre-scale/rotation)\n        local_archive_X = []      # recent evaluated points (for surrogate)\n        local_archive_F = []\n        global_archive_X = []     # for DE-style usage\n        global_archive_F = []\n\n        # temperature for acceptance\n        Temp = 1.0\n        Tmin = 1e-6\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial center evaluation\n        x0 = np.clip(m, lb, ub)\n        f0 = func(x0)\n        evals += 1\n        f_center = float(f0)\n        f_opt = float(f0)\n        x_opt = x0.copy()\n        local_archive_X.append(x0.copy()); local_archive_F.append(f0)\n        global_archive_X.append(x0.copy()); global_archive_F.append(f0)\n        last_improv = evals\n        stagn_count = 0\n        gen = 0\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            # prepare gaussian draws\n            Z = np.random.randn(lam, n)\n            Xcand = np.zeros((lam, n))\n            cand_info_local = []   # store local pre-rotation vectors for learning\n            Fvals = np.full(lam, np.inf)\n\n            for i in range(lam):\n                z = Z[i].copy()\n                if self.mirrored and (i % 2 == 1):\n                    z = -z\n\n                # local coordinate scaled vector\n                y_local = s * z  # elementwise\n                # rotated into original coordinates\n                y_rot = R.dot(y_local)\n\n                # occasional Cauchy heavy tail along rotated direction or random direction\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    dir_unit = z / nz\n                    y_rot = R.dot(s * dir_unit) * (r * np.mean(s))\n\n                x = m + sigma * y_rot\n\n                # DE-style mutation\n                if (np.random.rand() < self.p_de) and (len(global_archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(global_archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (global_archive_X[i1] - global_archive_X[i2])\n                    x = x + de_mut\n\n                # block-swap: permute entries inside a random block\n                if (np.random.rand() < self.p_blockswap) and (len(self.blocks) > 0):\n                    bidx = np.random.randint(len(self.blocks))\n                    b = self.blocks[bidx]\n                    if len(b) > 1:\n                        perm = np.random.permutation(len(b))\n                        x_block = x[b].copy()\n                        x[b] = x_block[perm]\n\n                # clip\n                x = np.clip(x, lb, ub)\n\n                Xcand[i] = x\n                # store local (pre-rotation) for learning: map rotated back to local coords: local = R^T y_rot\n                cand_info_local.append((R.T @ y_rot) / (sigma + 1e-20))\n\n            # Evaluate candidates carefully counting budget\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                # update archives\n                global_archive_X.append(xi.copy()); global_archive_F.append(fi)\n                local_archive_X.append(xi.copy()); local_archive_F.append(fi)\n                if len(global_archive_X) > 5000:\n                    # keep archive bounded to avoid memory growth\n                    global_archive_X.pop(0); global_archive_F.pop(0)\n                if len(local_archive_X) > self.local_archive_max:\n                    local_archive_X.pop(0); local_archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy()\n                    last_improv = evals\n\n            # selection: rank by objective (smaller better)\n            mu = max(1, lam // 2)\n            idx = np.argsort(Fvals)\n            sel_idx = idx[:mu]\n            X_sel = Xcand[sel_idx]\n            Ylocals_sel = np.vstack([cand_info_local[j] for j in sel_idx])  # mu x n\n\n            # recombination: rank-weighted softmax-like weights (different from previous log-weights)\n            ranks = np.arange(1, mu + 1)\n            # give exponentially decaying weights by rank\n            tau = 0.6\n            w_raw = np.exp(-tau * (ranks - 1))\n            weights = w_raw / np.sum(w_raw)\n\n            # weighted recombination to propose new center candidate\n            m_proposed = np.sum(weights[:, None] * X_sel, axis=0)\n\n            # Temperature-based acceptance for mean\n            accept_mean = True\n            if evals < budget:\n                xm = np.clip(m_proposed, lb, ub)\n                fm = func(xm)\n                evals += 1\n                global_archive_X.append(xm.copy()); global_archive_F.append(fm)\n                local_archive_X.append(xm.copy()); local_archive_F.append(fm)\n                if fm < f_opt:\n                    f_opt = float(fm); x_opt = xm.copy(); last_improv = evals\n                delta = float(fm) - float(f_center)\n                if delta <= 0.0:\n                    accept_mean = True\n                    m = xm.copy()\n                    f_center = float(fm)\n                else:\n                    # accept with probability exp(-delta / Temp)\n                    p = np.exp(-delta / max(1e-12, Temp))\n                    if np.random.rand() < p:\n                        accept_mean = True\n                        m = xm.copy()\n                        f_center = float(fm)\n                    else:\n                        accept_mean = False\n                        # tiny biased move towards proposed mean to keep diversity\n                        if np.random.rand() < 0.06:\n                            m = np.clip(0.92 * m + 0.08 * m_proposed, lb, ub)\n                # cool temperature mildly\n                Temp = max(Tmin, Temp * 0.994)\n            else:\n                # no budget to evaluate --> do weighted move without evaluation\n                m = np.clip(m_proposed, lb, ub)\n\n            # Update per-coordinate scales s using EMA of variance from selected local steps\n            # Ylocals_sel are local pre-rotation vectors; compute weighted second moment\n            y2 = np.sum(weights[:, None] * (Ylocals_sel ** 2), axis=0)\n            s2 = (1.0 - self.s_ema_rate) * (s ** 2) + self.s_ema_rate * (y2 + 1e-20)\n            s = np.sqrt(s2)\n            s = np.clip(s, 1e-6, 1e3)\n\n            # push a subset of good local steps into success buffer\n            half = max(1, mu // 2)\n            for j in range(half):\n                success_buf.append(Ylocals_sel[j].copy())\n                if len(success_buf) > self.success_buf_max:\n                    success_buf.pop(0)\n\n            gen += 1\n\n            # Subspace learning: every few generations, compute small covariance on success buffer to extract top-k eigenvectors\n            if (len(success_buf) >= max(6, self.k)) and (gen % max(1, int(2 + n / 20)) == 0):\n                Ymat = np.vstack(success_buf)  # m x n (rows are samples)\n                # center rows\n                Yc = Ymat - np.mean(Ymat, axis=0, keepdims=True)\n                # small covariance (n x n but success_buf small)\n                try:\n                    C = (Yc.T @ Yc) / max(1.0, Yc.shape[0] - 1)\n                    # eigen-decomposition (symmetric)\n                    vals, vecs = np.linalg.eigh(C)\n                    # take top-k eigenvectors\n                    order = np.argsort(vals)[::-1]\n                    topk = min(self.k, vecs.shape[1])\n                    U_new = vecs[:, order[:topk]]\n                    # form an orthonormal basis for R_new: start with U_new and extend with QR of random complement\n                    # Compose a candidate rotation Qcand with U_new as first cols\n                    Qcand = np.zeros((n, n))\n                    Qcand[:, :topk] = U_new\n                    # fill remaining cols by random vectors orthonormalized\n                    if topk < n:\n                        rnd = np.random.randn(n, n - topk)\n                        # project rnd to orthogonal complement of U_new\n                        proj = U_new @ (U_new.T @ rnd)\n                        comp = rnd - proj\n                        Qcand[:, topk:] = comp\n                    # orthonormalize Qcand\n                    Qr, _ = np.linalg.qr(Qcand)\n                    Qcand = Qr[:, :n]\n                    # gently mix new rotation into R to avoid abrupt changes\n                    alpha = self.subspace_mix\n                    R = (1.0 - alpha) * R + alpha * Qcand\n                    # re-orthonormalize\n                    Qr, _ = np.linalg.qr(R)\n                    R = Qr[:, :n]\n                    # update U as first k cols of R\n                    U = R[:, :self.k].copy()\n                except np.linalg.LinAlgError:\n                    # small perturbation fallback\n                    R = R + 0.01 * np.random.randn(n, n)\n                    Qr, _ = np.linalg.qr(R)\n                    R = Qr[:, :n]\n                    U = R[:, :self.k].copy()\n\n            # Adaptive sigma update using 1/5-trend-like heuristic (target success rate ~ 0.2)\n            successes = np.sum(Fvals < (f_center - 1e-12))  # strictly better than center estimate\n            psucc = successes / max(1, lam)\n            if psucc > 0.25:\n                sigma *= self.sigma_deflate\n            elif psucc < 0.06:\n                sigma *= self.sigma_inflate\n            # keep sigma reasonable\n            sigma = np.clip(sigma, 1e-12, 5.0 * np.mean(ub - lb) + 1e-12)\n\n            # Stagnation detection and mild restart-ish perturbation\n            if (evals - last_improv) > self.stagn_limit:\n                stagn_count += 1\n                # reheat temperature and inflate sigma\n                Temp = max(Temp, 0.6 + 0.2 * stagn_count)\n                sigma *= self.reheat_mult\n                # randomize one element of block partition by swapping indices\n                if np.random.rand() < 0.6 and len(self.blocks) > 1:\n                    b1, b2 = np.random.choice(len(self.blocks), size=2, replace=False)\n                    if len(self.blocks[b1]) > 0 and len(self.blocks[b2]) > 0:\n                        i1 = np.random.choice(self.blocks[b1])\n                        i2 = np.random.choice(self.blocks[b2])\n                        # swap membership\n                        self.blocks[b1].remove(i1); self.blocks[b1].append(i2)\n                        self.blocks[b2].remove(i2); self.blocks[b2].append(i1)\n                # perturb R gently\n                R = R + 0.03 * np.random.randn(n, n)\n                Qr, _ = np.linalg.qr(R)\n                R = Qr[:, :n]\n                # reset some buffers\n                success_buf = []\n                last_improv = evals\n\n            # Periodic 1D quadratic surrogate probe along principal direction U[:,0]\n            if (gen % max(8, int(6 + n / 8)) == 0) and (evals < budget) and (len(local_archive_X) >= 6):\n                d = U[:, 0]\n                # assemble recent samples near center (use local_archive)\n                Xs = np.vstack(local_archive_X[-min(len(local_archive_X), 40):])\n                Fs = np.array(local_archive_F[-min(len(local_archive_F), 40):])\n                # project samples onto d: t = d^T (x - m)\n                ts = (Xs - m) @ d\n                # select samples with moderate |t| to fit robustly\n                order = np.argsort(np.abs(ts))\n                kfit = min(12, len(ts))\n                sel = order[:kfit]\n                t_sel = ts[sel]; f_sel = Fs[sel]\n                # fit quadratic f(t) = a t^2 + b t + c by linear least squares\n                A = np.vstack([t_sel ** 2, t_sel, np.ones_like(t_sel)]).T\n                try:\n                    coeffs, *_ = np.linalg.lstsq(A, f_sel, rcond=None)\n                    a, b, c = coeffs\n                    # propose minimizer if it is a convex parabola\n                    if a > 1e-12:\n                        t_star = -b / (2.0 * a)\n                        # clamp t_star to window (so as not to take huge steps)\n                        tmax = min(3.0 * sigma * np.mean(s), max(1.5, 3.0 * np.std(ts) + 1e-12))\n                        t_star = np.clip(t_star, -tmax, tmax)\n                        probe = np.clip(m + t_star * d, lb, ub)\n                        if evals < budget:\n                            fp = func(probe)\n                            evals += 1\n                            global_archive_X.append(probe.copy()); global_archive_F.append(fp)\n                            local_archive_X.append(probe.copy()); local_archive_F.append(fp)\n                            if fp < f_opt:\n                                f_opt = float(fp); x_opt = probe.copy(); last_improv = evals\n                            # if surrogate step improves center accept it\n                            if fp < f_center or (np.random.rand() < np.exp(-(fp - f_center) / max(1e-12, Temp))):\n                                m = probe.copy(); f_center = float(fp)\n                                # also incorporate the step into success buffer\n                                v_local = (R.T @ ((probe - m) / max(1e-20, sigma)))  # normalized local step\n                                success_buf.append(v_local)\n                                if len(success_buf) > self.success_buf_max:\n                                    success_buf.pop(0)\n                except np.linalg.LinAlgError:\n                    pass\n\n            # ensure center stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # safety exit if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AS3 scored 0.411 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "446f78f2-083b-4293-8204-2cad655bf6b2", "operator": null, "metadata": {"aucs": [0.1625476816866166, 0.16775653239032418, 0.37647201952030485, 0.6225881446282281, 0.817662401527308, 0.8796755435089167, 0.26689351499775893, 0.3291339202442445, 0.3244809605169373, 0.16254394846350217]}, "task_prompt": ""}
{"id": "534d32c2-34bb-4886-b9b7-d54d66e1e415", "fitness": 0.4589444255453056, "name": "SBPAS", "description": "The algorithm mixes multiple proposal mechanisms (small population lam ~ pop_factor·log(dim), per-coordinate Gaussian steps, a learned directional bias v, occasional Student‑t heavy tails, DE‑style archive differences, and block-wise coordinate shuffles) and organizes coordinates into ~sqrt(dim) blocks to enable both local and structured moves. Adaptation is twofold: per-coordinate scales s are robustly estimated from selected steps via MAD, while a global sigma is adjusted by a smoothed success‑rate (1/5‑style) rule; a principal direction v is learned online with a decaying Oja update and occasionally probed with short 1D line searches. Selection/recombination is robust (top‑mu selection and coordinate‑wise median), with a Metropolis‑like probabilistic acceptance using a cooling temperature and reheating/stagnation strategies (sigma bumps, block reshuffles, perturb v) to escape traps. Additional diversity and robustness come from a small archive (for DE differences and memory), conservative hyperparameters (p_tstudent≈0.09, p_de≈0.14, p_blockswap≈0.10, alpha_dir≈0.6), and modest initial sigma (0.14·range) to balance exploration and exploitation.", "code": "import numpy as np\n\nclass SBPAS:\n    \"\"\"\n    Stochastic Blockwise Power-Adaptation Search (SBPAS)\n\n    Key ideas / parameters (readable here):\n    - budget, dim: required\n    - seed: optional RNG seed\n    - pop_factor: controls small population size lam ~ pop_factor * log(dim)\n    - blocks: number of coordinate blocks (None => ~sqrt(dim))\n    - sigma: global step-size (scalar), adapted via success-rate heuristic (1/5-like)\n    - s: per-coordinate adaptive scaling estimated by MAD of selected steps\n    - v: single learned principal direction (Oja rule) used to bias proposals and for line-search\n    - archive: small reservoir to allow DE-like differences\n    - mixture: Gaussian proposals + occasional Student-t heavy tails + DE differences + block shuffles\n    - recombination: coordinate-wise median of top candidates (robust)\n    - acceptance: simulated-annealing-like acceptance of new median center with temperature schedule\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, blocks=None, pop_factor=2.5):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size (small)\n        self.lambda_ = max(6, int(np.round(pop_factor * max(2.0, np.log(self.dim + 2.0)))))\n\n        # blocks: default ~ sqrt(dim)\n        if blocks is None:\n            self.n_blocks = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.n_blocks = max(1, int(min(blocks, self.dim)))\n\n        # create contiguous balanced blocks initially\n        sizes = [self.dim // self.n_blocks] * self.n_blocks\n        for i in range(self.dim % self.n_blocks):\n            sizes[i] += 1\n        idx = 0\n        self.blocks = []\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n        # other hyper-parameters (tunable)\n        self.p_tstudent = 0.09     # probability heavy-tailed jump\n        self.df_t = 3.0            # degrees of freedom for Student-t\n        self.p_de = 0.14           # probability DE mutation\n        self.F_de = 0.5            # DE scaling\n        self.p_blockswap = 0.10    # block shuffle probability\n        self.alpha_dir = 0.6       # weight for directional bias in proposals\n        self.target_success = 0.2  # target success rate for sigma adaptation\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (assume provided via func.bounds.lb/ub, fallback to -5..5)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initialize state\n        m = np.random.uniform(lb, ub)                        # current center\n        sigma = 0.14 * np.mean(ub - lb)                      # initial global step-size (slightly smaller)\n        s = np.ones(n)                                       # per-coordinate scales (std proxies)\n        # single learned principal direction (unit)\n        v = np.random.randn(n)\n        v /= (np.linalg.norm(v) + 1e-20)\n\n        # temperature for probabilistic acceptance\n        Temp = 1.0\n\n        # small archive for DE differences\n        archive_X = []\n        archive_F = []\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation at m\n        xm0 = np.clip(m, lb, ub)\n        f0 = func(xm0)\n        evals += 1\n        archive_X.append(xm0.copy()); archive_F.append(f0)\n        f_opt = f0; x_opt = xm0.copy()\n        f_center = f0\n        last_improv = evals\n\n        gen = 0\n        stagn_count = 0\n        stagn_limit = max(10, int(6 * n / max(1, self.n_blocks)))\n\n        # small bounds for archive\n        max_archive = 2000\n\n        # Oja learning base rate (will decay)\n        oja_base = 0.08\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n\n            # sample independent normals for population\n            Z = np.random.randn(lam, n)\n\n            X = np.zeros((lam, n))\n            Ys = np.zeros((lam, n))   # store underlying step vectors (pre-scaling by sigma) for learning\n            Fvals = np.full(lam, np.inf)\n\n            for i in range(lam):\n                z = Z[i].copy()\n                # coordinate-wise scale\n                y_local = s * z\n                # add directional bias along learned direction v\n                dir_scalar = np.random.randn() * self.alpha_dir\n                y = y_local + dir_scalar * v * np.mean(np.abs(s))\n                # occasionally heavy-tailed jump along v scaled by mean scale\n                if np.random.rand() < self.p_tstudent:\n                    t = np.random.standard_t(self.df_t)\n                    y = v * t * np.mean(s)\n                # candidate\n                x = m + sigma * y\n\n                # DE-like archive difference\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # block-swap: shuffle a block locally\n                if (np.random.rand() < self.p_blockswap) and (len(self.blocks) > 0):\n                    bidx = np.random.randint(len(self.blocks))\n                    b = self.blocks[bidx]\n                    if len(b) > 1:\n                        perm = np.random.permutation(len(b))\n                        x_block = x[b].copy()\n                        x[b] = x_block[perm]\n\n                # clip and store\n                x = np.clip(x, lb, ub)\n                X[i] = x\n                Ys[i] = y\n\n            # Evaluate candidates until budget or population finished\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if len(archive_X) > max_archive:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi\n                    x_opt = xi.copy()\n                    last_improv = evals\n\n            # selection: pick top mu (robust selection)\n            mu = max(1, lam // 3)\n            idx = np.argsort(Fvals)\n            sel = idx[:mu]\n            X_sel = X[sel]\n            Y_sel = Ys[sel]    # steps corresponding to selected candidates\n\n            # recombination: coordinate-wise median (robust)\n            m_new = np.median(X_sel, axis=0)\n\n            # acceptance of new center: evaluate if budget allows, else fallback to median replace\n            accept_mean = False\n            if evals < budget:\n                xm_clip = np.clip(m_new, lb, ub)\n                fm_new = func(xm_clip)\n                evals += 1\n                archive_X.append(xm_clip.copy()); archive_F.append(fm_new)\n                if len(archive_X) > max_archive:\n                    archive_X.pop(0); archive_F.pop(0)\n                # accept if improved or by temperature probability\n                delta = fm_new - f_center\n                if delta <= 0:\n                    accept_mean = True\n                    f_center = fm_new\n                    m = xm_clip.copy()\n                else:\n                    p = np.exp(-delta / max(1e-12, Temp))\n                    if np.random.rand() < p:\n                        accept_mean = True\n                        f_center = fm_new\n                        m = xm_clip.copy()\n                    else:\n                        accept_mean = False\n                        # tiny conservative nudge toward m_new\n                        if np.random.rand() < 0.06:\n                            m = np.clip(0.92 * m + 0.08 * m_new, lb, ub)\n                # update global Temp (gentle exponential cooling)\n                Temp *= 0.990\n                Temp = max(1e-6, Temp)\n            else:\n                # no budget left for center evaluation: adopt robust median but clipped\n                m = np.clip(m_new, lb, ub)\n\n            # update per-coordinate scales using MAD (robust)\n            if Y_sel.shape[0] >= 1:\n                med = np.median(Y_sel, axis=0)\n                mad = np.median(np.abs(Y_sel - med[None, :]), axis=0)\n                # convert MAD to std estimate: 1.4826 factor for Gaussian-like distributions\n                est_std = 1.4826 * (mad + 1e-20)\n                c_mad = 0.18\n                s = np.sqrt((1.0 - c_mad) * (s ** 2) + c_mad * (est_std ** 2))\n                s = np.clip(s, 1e-7, 1e3)\n\n            # Online Oja update for principal direction v using selected steps\n            if Y_sel.shape[0] >= 1:\n                # small decaying learning rate\n                eta = oja_base / (1.0 + 0.015 * gen)\n                # compute batch average outer product times v\n                batch = np.mean(Y_sel, axis=0)\n                # simple Oja batch: v <- v + eta * (C * v), C approximated by covariance-vector product\n                # use covariance-vector: cov_v = (Y_sel.T @ (Y_sel @ v)) / mu\n                cov_v = np.zeros_like(v)\n                Yv = Y_sel.dot(v)              # shape mu\n                cov_v = (Y_sel.T.dot(Yv)) / (max(1, Y_sel.shape[0]))\n                v = v + eta * cov_v\n                # normalize\n                vn = np.linalg.norm(v) + 1e-20\n                v = v / vn\n\n            # sigma adaptation via smoothed 1/5-style rule using fraction of candidates better than f_center\n            if lam > 0:\n                successes = np.sum(Fvals < f_center - 1e-12)\n                psucc = successes / lam\n                # multiplicative factor depending on deviation from target\n                if psucc > self.target_success:\n                    sigma *= 0.97\n                elif psucc < (self.target_success * 0.5):\n                    sigma *= 1.035\n                else:\n                    # slight decay for stability\n                    sigma *= 0.999\n            sigma = np.clip(sigma, 1e-12, 5.0 * np.mean(ub - lb) + 1e-12)\n\n            # stagnation handling\n            gen += 1\n            if (evals - last_improv) > stagn_limit:\n                stagn_count += 1\n                # reheating: bump temperature and sigma moderately\n                Temp = max(Temp, 0.6 + 0.15 * stagn_count)\n                sigma *= 1.6\n                # reshuffle block structure: move a couple coordinates randomly between blocks\n                if len(self.blocks) > 1:\n                    for _ in range(min(3, self.n_blocks)):\n                        b1, b2 = np.random.choice(len(self.blocks), size=2, replace=False)\n                        if (len(self.blocks[b1]) > 0) and (len(self.blocks[b2]) > 0):\n                            i1 = np.random.choice(self.blocks[b1])\n                            i2 = np.random.choice(self.blocks[b2])\n                            # swap membership\n                            self.blocks[b1].remove(i1); self.blocks[b1].append(i2)\n                            self.blocks[b2].remove(i2); self.blocks[b2].append(i1)\n                # perturb principal direction slightly\n                v = v + 0.06 * np.random.randn(n)\n                v /= (np.linalg.norm(v) + 1e-20)\n                last_improv = evals\n\n            # occasional short 1D line-search along v (different scheduling than BARS)\n            if (gen % max(8, int(3 + n // 6)) == 0) and (evals < budget):\n                # try a few alphas scaled by sigma and per-dim mean scale\n                alphas = [0.6, 0.3, -0.3, -0.6]\n                for alpha in alphas:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + sigma * alpha * v * np.mean(s), lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if len(archive_X) > max_archive:\n                        archive_X.pop(0); archive_F.pop(0)\n                    if fp < f_opt:\n                        f_opt = fp; x_opt = probe.copy(); last_improv = evals\n\n            # ensure center stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # break on budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm SBPAS scored 0.459 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "446f78f2-083b-4293-8204-2cad655bf6b2", "operator": null, "metadata": {"aucs": [0.21066977179273338, 0.18096033212004115, 0.3268518831681403, 0.9441813778105911, 0.3834491344264742, 0.9443908284901587, 0.2578060345604205, 0.3397083762349876, 0.8224233210842855, 0.17900319576522405]}, "task_prompt": ""}
{"id": "19996f2e-d5b3-4a5f-87ca-8ba1bc709672", "fitness": 0.5995275553346013, "name": "BSAS", "description": "The algorithm maintains a single searchable center m with an adaptive trust radius r, per-dimension scales s, a small population (lambda_) and an archive/success buffer to store past evaluations and steps for learning. It learns a low-dimensional subspace R by PCA on successful steps and optionally fits a linear descent-like model from the success buffer, while also keeping DE-style difference vectors in the archive for mutation. Candidate generation mixes multiple stochastic operators (Gaussian, heavy-tailed Levy jumps, DE differences, linear-model descent, and block-permute swaps) chosen by a soft bandit (log-weights op_logw updated with reward signals) to adapt operator preferences. Center recombination uses rank-based weighted averaging, center acceptance uses a small simulated-annealing-like tolerance, and the algorithm adapts r and s from success rates and second moments, performs periodic 1D subspace line-search, and applies mild restarts/perturbations on stagnation (with concrete parameter choices such as initial r = 0.18*range, bandit temp≈0.6, and learning rate alpha=0.15).", "code": "import numpy as np\n\nclass BSAS:\n    \"\"\"\n    Bandit-driven Subspace Adaptive Search (BSAS)\n\n    Main ideas (short):\n    - Maintain a searchable center m and trust radius r, per-dimension scales s, and a lightweight\n      learned subspace R (principal directions) from recent successful steps.\n    - Maintain a small archive for DE-style differences and a success buffer for subspace/linear-model learning.\n    - Use multiple stochastic operators (Gaussian, Levy, DE-difference, linear-model descent, block-permute)\n      and select among them adaptively with a soft bandit (operator preference) that is updated by observed rewards.\n    - Combine population sampling with weighted recombination, adaptive resizing of trust radius based on success,\n      and periodic 1D line-search along the top learned direction.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, pop_factor=3):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # population size small, depends on dim\n        self.lambda_ = max(6, int(pop_factor * max(4, np.log(max(2, self.dim))) ))\n        # initial blocks: contiguous small blocks\n        n_blocks = max(1, int(np.ceil(np.sqrt(self.dim))))\n        self.blocks = []\n        sizes = [self.dim // n_blocks] * n_blocks\n        for i in range(self.dim % n_blocks):\n            sizes[i] += 1\n        idx = 0\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx+s)))\n            idx += s\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center (uniform within bounds)\n        m = self.rng.uniform(lb, ub)\n        # trust radius (absolute scale)\n        r = 0.18 * np.mean(ub - lb)\n        # per-dimension scales (relative)\n        s = np.ones(n)\n\n        # learned subspace (rotation) as identity initially\n        R = np.eye(n)\n\n        # small archive for DE-like differences and general memory\n        archive_X = []\n        archive_F = []\n\n        # success buffer (recent successful (x - m)/r steps and f improvements)\n        success_buf_X = []   # local steps: (x - m)/r expressed in R coords\n        success_buf_df = []  # improvements f_center - f_x\n\n        # operator bandit preferences (log-weights)\n        operators = ['gauss', 'levy', 'de', 'lin', 'swap']\n        k_ops = len(operators)\n        op_logw = np.zeros(k_ops)  # start neutral\n        op_count = np.zeros(k_ops, dtype=int)\n\n        # parameters\n        p_mirror = True\n        p_de = 0.2\n        F_de = 0.6\n        p_blockswap = 0.15\n        levy_scale = 1.0\n        lin_reg_lambda = 1e-4\n        buf_max = max(40, 8 * int(np.ceil(np.sqrt(n))))\n        k_sub = min(3, n)  # number of principal directions to learn\n        line_search_every = max(20, int(5 * (n / 10 + 1)))\n        gen = 0\n\n        evals = 0\n        # evaluate initial center\n        m = np.clip(m, lb, ub)\n        f_center = func(m)\n        evals += 1\n        archive_X.append(m.copy()); archive_F.append(f_center)\n        f_opt = float(f_center); x_opt = m.copy()\n        last_improv = evals\n\n        # helper: softmax for operator selection\n        def op_probs(logw, temp=1.0):\n            w = np.exp((logw - np.max(logw)) / max(1e-12, temp))\n            return w / (np.sum(w) + 1e-20)\n\n        # helper: sample levy-like direction (Cauchy scaled)\n        def sample_levy_direction(z):\n            # use standard Cauchy scalar times unit direction\n            # ensure not-too-large by clipping scalar to reasonable range\n            scalar = np.tan(np.pi * (self.rng.random() - 0.5)) * 0.6  # scaled Cauchy-ish\n            normz = np.linalg.norm(z) + 1e-20\n            return (z / normz) * scalar\n\n        # helper: fit linear model gradient from success buffer (Xlocal matrix rows and df)\n        def fit_linear_model(buf_X, buf_df):\n            # We fit df ≈ g^T dx  => solve least squares for g\n            if len(buf_X) < 2:\n                return None\n            A = np.vstack(buf_X)  # m x n: dx in global coords (we will transform back)\n            y = np.array(buf_df)   # m vector (improvement = f_center - f_x)\n            # We want minimize ||A g - y||^2 -> g = (A^T A + λI)^-1 A^T y\n            ATA = A.T @ A\n            reg = lin_reg_lambda * np.eye(n)\n            try:\n                g = np.linalg.solve(ATA + reg, A.T @ y)\n            except np.linalg.LinAlgError:\n                # fallback pseudo-inverse\n                g = np.linalg.pinv(ATA + reg) @ (A.T @ y)\n            return g\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            # operator probabilities\n            probs = op_probs(op_logw, temp=0.6)\n            Xcand = np.zeros((lam, n))\n            Fcand = np.full(lam, np.inf)\n            cand_op_idx = np.zeros(lam, dtype=int)\n            cand_step_local = np.zeros((lam, n))  # store local step (in global coords) scaled by 1 (dx = x - m)\n            # Pre-generate normals\n            Z = self.rng.standard_normal((lam, n))\n            for i in range(lam):\n                # pick operator by sampling categorical\n                op_idx = self.rng.choice(k_ops, p=probs)\n                cand_op_idx[i] = op_idx\n                op = operators[op_idx]\n                z = Z[i].copy()\n                if p_mirror and (i % 2 == 1):\n                    z = -z\n\n                # base step: local coordinate scaled and projected into learned subspace\n                # local in rotated coordinates:\n                y_local = s * z\n                # rotate to original coordinates\n                y_global = R.dot(y_local)\n\n                # default create x as center + r * y_global\n                x = m + r * y_global\n\n                # apply operator specifics\n                if op == 'gauss':\n                    # small perturbation already embedded in y_global\n                    pass\n\n                elif op == 'levy':\n                    # strong heavy-tailed jump in the direction of y_global\n                    lev = sample_levy_direction(z) * levy_scale\n                    yg = R.dot(s * lev)\n                    x = m + r * 2.0 * yg  # larger scale for levy\n                elif op == 'de':\n                    # differential mutation using archive\n                    if len(archive_X) >= 2 and self.rng.random() < p_de:\n                        i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                        de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                        x = x + de_mut\n                elif op == 'lin':\n                    # guided step from linear model fitted on success buffer (in global coords)\n                    # fit using stored (dx = x - center) and improvement df\n                    if len(success_buf_X) >= 3:\n                        # success_buf_X stores steps in global coords scaled by r (i.e., dx = x - m)\n                        # fit model to predict df as function of dx\n                        g = fit_linear_model(success_buf_X, success_buf_df)\n                        if g is not None:\n                            # move along -g (descent) scaled by trust radius\n                            ng = np.linalg.norm(g) + 1e-20\n                            step = - (r * (g / ng)) * (0.5 + 0.5 * self.rng.random())\n                            x = m + step\n                    else:\n                        # fallback to gaussian\n                        pass\n                elif op == 'swap':\n                    # permute a random small block inside x\n                    if (len(self.blocks) > 0) and (self.rng.random() < p_blockswap):\n                        bidx = self.rng.integers(0, len(self.blocks))\n                        b = self.blocks[bidx]\n                        if len(b) > 1:\n                            perm = self.rng.permutation(len(b))\n                            x_block = x[b].copy()\n                            x[b] = x_block[perm]\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                Xcand[i] = x\n                cand_step_local[i] = (x - m)  # store dx = x - m for learning\n\n            # Evaluate candidates (ensuring we do not exceed budget)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fcand[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy(); last_improv = evals\n\n            # Selection: compute improvements relative to current center\n            # We use mu as half of evaluated candidates (or at least 1)\n            mu = max(1, lam // 2)\n            idx_sorted = np.argsort(Fcand)\n            sel_idx = idx_sorted[:mu]\n            X_sel = Xcand[sel_idx]\n            F_sel = Fcand[sel_idx]\n\n            # recombination: weighted mean using rank-based weights\n            ranks = np.arange(1, mu + 1)\n            weights = np.log(mu + 0.5) - np.log(ranks)\n            weights = weights / np.sum(weights)\n            m_new = np.sum(weights[:, None] * X_sel, axis=0)\n            m_new = np.clip(m_new, lb, ub)\n\n            # Evaluate m_new only if we still have budget; else fallback to accept if estimated better by mean of selected\n            accepted_center_move = False\n            if evals < budget:\n                # evaluate candidate center\n                fm_new = func(m_new)\n                evals += 1\n                archive_X.append(m_new.copy()); archive_F.append(fm_new)\n                if fm_new < f_center:\n                    accepted_center_move = True\n                    f_center = fm_new\n                    m = m_new.copy()\n                else:\n                    # accept uphill with small probability decreasing with size of uphill and generation\n                    delta = fm_new - f_center\n                    T = max(0.01, 0.5 * np.exp(-0.001 * gen))\n                    if self.rng.random() < np.exp(-delta / (T + 1e-12)):\n                        accepted_center_move = True\n                        f_center = fm_new\n                        m = m_new.copy()\n                    else:\n                        accepted_center_move = False\n                        # small biased move towards m_new occasionally\n                        if self.rng.random() < 0.03:\n                            m = 0.9 * m + 0.1 * m_new\n            else:\n                # no budget: deterministic move to m_new if its mean selected value seems better\n                mean_sel = np.mean(F_sel)\n                if mean_sel < f_center:\n                    m = m_new.copy()\n                    accepted_center_move = True\n\n            # Update trust radius r based on number of improving candidates (improvement relative to f_center)\n            improvements = np.sum(Fcand < f_center - 1e-12)\n            psucc = improvements / max(1, lam)\n            if psucc > 0.25:\n                # shrink gently to exploit\n                r *= 0.9\n            elif psucc < 0.05:\n                # expand to explore\n                r *= 1.15\n            # clamp r to problem scale\n            r = float(np.clip(r, 1e-8, 5.0 * np.mean(ub - lb)))\n\n            # Update per-dimension scales s using weighted second moments from selected dx in rotated coords\n            # map selected dx to rotated-local coords: local = R^T dx / r   (but guard r zero)\n            if r > 0:\n                Ylocal_sel = (R.T @ ( (X_sel - m).T )).T / (r + 1e-20)\n                y2 = np.sum(weights[:, None] * (Ylocal_sel ** 2), axis=0)\n                c_s = 0.15\n                s2 = (1.0 - c_s) * (s ** 2) + c_s * (y2 + 1e-20)\n                s = np.sqrt(s2)\n                s = np.clip(s, 1e-6, 1e3)\n\n            # Update success buffer from best half of selected (store dx in global coords and improvement)\n            h = max(1, mu // 2)\n            for j in range(h):\n                idx_j = sel_idx[j]\n                dx = (Xcand[idx_j] - m)  # note: relative to possibly updated m\n                df = max(0.0, (f_center - Fcand[idx_j]))  # improvement positive if candidate better than center\n                # store global dx scaled to typical trust radius magnitude\n                if len(success_buf_X) >= buf_max:\n                    success_buf_X.pop(0); success_buf_df.pop(0)\n                success_buf_X.append(dx.copy())\n                success_buf_df.append(df)\n\n            # Update learned subspace R via PCA on success buffer (principal components of steps)\n            if len(success_buf_X) >= min(6, n):\n                M = np.vstack(success_buf_X)  # m x n (rows are dx in global coords)\n                # center rows\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                # compute covariance (n x n)\n                try:\n                    C = (Mc.T @ Mc) / max(1, Mc.shape[0] - 1)\n                    # eigen-decomposition\n                    eigvals, eigvecs = np.linalg.eigh(C)\n                    # take top-k_sub eigenvectors\n                    order = np.argsort(eigvals)[::-1]\n                    top = eigvecs[:, order[:k_sub]]\n                    # gently move R toward top directions by replacing first k_sub columns after orthonormal mixing\n                    # Construct new R candidate: keep existing R but replace subspace spanned by first k_sub columns\n                    A = R[:, :k_sub]  # current basis\n                    # compute cross-covariance and align using small polar rotation\n                    Msmall = top.T @ A\n                    try:\n                        U_m, _, Vt_m = np.linalg.svd(Msmall)\n                        Qsmall = U_m @ Vt_m\n                        R[:, :k_sub] = top @ Qsmall\n                        # orthonormalize full R\n                        Qr, _ = np.linalg.qr(R)\n                        R = Qr[:, :n]\n                    except np.linalg.LinAlgError:\n                        # small perturbation fallback\n                        R = R + 0.01 * self.rng.standard_normal(R.shape)\n                        Qr, _ = np.linalg.qr(R)\n                        R = Qr[:, :n]\n                except np.linalg.LinAlgError:\n                    pass\n\n            # Update operator bandit weights based on observed rewards for each candidate\n            # Reward: normalized improvement relative to f_center prior to center update (use max(0,f_center_old - fi))\n            # We'll use the f_center snapshot before possibly updating it in this generation (we captured f_center earlier)\n            # to compute reward consistently, we approximate with the best known center value before candidate evaluations.\n            # For simplicity use f_candidate relative to previous f_center (we stored f_center variable)\n            # We compute per-operator average reward in this generation and update log-weights via exponential recency\n            op_rewards = np.zeros(k_ops)\n            op_seen = np.zeros(k_ops)\n            # use baseline for improvement calculations as the center value before evaluation of m_new (approx f_center)\n            baseline = f_center\n            for i in range(lam):\n                opi = cand_op_idx[i]\n                # improvement (positive if candidate better than baseline)\n                improv = max(0.0, baseline - Fcand[i]) if np.isfinite(Fcand[i]) else 0.0\n                # scale by magnitude of step\n                step_norm = np.linalg.norm(cand_step_local[i]) + 1e-20\n                reward = improv / (1.0 + step_norm)\n                op_rewards[opi] += reward\n                op_seen[opi] += 1\n            # aggregate and update log weights\n            alpha = 0.15  # learning rate for bandit\n            for oi in range(k_ops):\n                if op_seen[oi] > 0:\n                    avg_reward = op_rewards[oi] / op_seen[oi]\n                    # move log-weight toward log(1 + avg_reward)\n                    target = np.log1p(avg_reward + 1e-12)\n                    op_logw[oi] = (1 - alpha) * op_logw[oi] + alpha * target\n                    op_count[oi] += int(op_seen[oi])\n\n            # Periodic subspace-guided 1D line-search along top direction\n            gen += 1\n            if (gen % line_search_every == 0) and (evals < budget):\n                d = R[:, 0]\n                # bracket and sample a small set of alphas around zero, respecting bounds and budget\n                alphas = [0.0, 0.25, -0.25, 0.5, -0.5]\n                for a in alphas:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + r * a * d, lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = float(fp); x_opt = probe.copy(); last_improv = evals\n                    # if better than current center, adopt it\n                    if fp < f_center:\n                        f_center = fp; m = probe.copy()\n\n            # stagnation detection and mild restart/perturbation if necessary\n            if (evals - last_improv) > max(40, 6 * n):\n                # inflate radius and perturb R and s slightly\n                r *= 1.8\n                s = s * (1.0 + 0.2 * (self.rng.random(n) - 0.5))\n                R = R + 0.03 * self.rng.standard_normal(R.shape)\n                Qr, _ = np.linalg.qr(R)\n                R = Qr[:, :n]\n                # random block swap in blocks to change combinatorics\n                if len(self.blocks) > 1:\n                    b1, b2 = self.rng.choice(len(self.blocks), size=2, replace=False)\n                    if len(self.blocks[b1]) and len(self.blocks[b2]):\n                        i1 = self.rng.choice(self.blocks[b1])\n                        i2 = self.rng.choice(self.blocks[b2])\n                        # swap indexes between block lists if present\n                        try:\n                            self.blocks[b1].remove(i1); self.blocks[b1].append(i2)\n                            self.blocks[b2].remove(i2); self.blocks[b2].append(i1)\n                        except ValueError:\n                            pass\n                last_improv = evals\n\n            # trim archive to reasonable size\n            if len(archive_X) > 5000:\n                archive_X = archive_X[-5000:]\n                archive_F = archive_F[-5000:]\n\n            # enforce bounds for center\n            m = np.clip(m, lb, ub)\n\n            # safety: ensure we don't loop forever\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm BSAS scored 0.600 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "446f78f2-083b-4293-8204-2cad655bf6b2", "operator": null, "metadata": {"aucs": [0.12449593929653469, 0.5751131093673749, 0.6763464663799424, 0.9704965794632882, 0.8226626685425213, 0.8426612292797809, 0.3108455321642811, 0.6670744026192819, 0.8114394260895937, 0.19414020014341382]}, "task_prompt": ""}
{"id": "e256ddcf-bcdf-4570-b35c-49b54c5c01d2", "fitness": 0.19523673736026223, "name": "ABLE_DE", "description": "The algorithm is a hybrid, population-based heuristic that combines blockwise coordinate moves, a learned rotation, heavy-tailed Student‑T (Lévy-like) jumps, and occasional DE-style archive differences to balance global exploration and local refinement (population size set by pop_factor, mirrored sampling enabled, initial sigma = init_sigma_factor * mean(range)). It maintains per-coordinate scales s updated with an RMSprop-like EMA on absolute local steps and learns principal directions R from a small success buffer via a power-method PCA, using R to rotate samples and to drive periodic 1‑D line probes along the dominant direction. Diversity is further encouraged by block-permutation mutations, softmax-weighted recombination of the top mu candidates to form a new mean with logistic acceptance (and occasional nudging), while sigma adapts with a simple 1/5-inspired rule toward a target success rate (0.2) and is bounded. Robustness mechanisms include an archive of evaluated points used for DE deltas, stagnation reheating (sigma inflation, rotation perturbation, partial random restarts and block reshuffling), strict budget accounting, and practical caps on archive and buffer sizes.", "code": "import numpy as np\n\nclass ABLE_DE:\n    \"\"\"\n    Adaptive Blockwise Lévy-Differential Evolution (ABLE-DE)\n\n    Main parameters (tunable):\n      - budget: total function evaluations\n      - dim: problem dimension\n      - pop_factor: influences population size (lambda)\n      - init_sigma_factor: initial global sigma as factor * mean(range)\n      - n_blocks: number of coordinate blocks (if None auto-chosen)\n      - p_levy: probability of heavy-tailed (Student-T) jump (different from Cauchy)\n      - p_de: probability to apply DE-style archive difference\n      - F_de: differential weight for DE mutation\n      - p_blockswap: probability of permuting entries inside a block\n      - mirror: mirrored sampling flag\n      - rot_update_every: generations between rotation (principal direction) updates\n      - line_search_every: periodic 1D probes along learned top direction\n\n    Differences vs BARS (high-level):\n      - uses Student-T (nu>1) heavy tails (Lévy-like) with adaptive scaling instead of pure Cauchy\n      - softmax exponential weights for recombination (not log-weights)\n      - per-coordinate scale updated with RMSprop-like EMA on |local steps|\n      - rotation learned via power-method PCA on the small success buffer (cheap)\n      - sigma adaptation via simple 1/5th-like multiplier toward target success rate 0.2\n      - smaller archive and slightly different heuristics for stagnation and reheating\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 pop_factor=2.5, init_sigma_factor=0.20, n_blocks=None,\n                 p_levy=0.12, levy_df=3.0, p_de=0.20, F_de=0.5,\n                 p_blockswap=0.10, mirror=True):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size (different heuristic)\n        self.lambda_ = max(6, int(np.ceil(pop_factor * max(3, np.log(max(2, self.dim))))))\n        # blocks: favor slightly more blocks than sqrt(dim)\n        if n_blocks is None:\n            self.n_blocks = max(1, int(np.ceil(np.sqrt(self.dim) * 1.25)))\n        else:\n            self.n_blocks = max(1, min(int(n_blocks), self.dim))\n\n        # create contiguous blocks initially\n        sizes = [self.dim // self.n_blocks] * self.n_blocks\n        for i in range(self.dim % self.n_blocks):\n            sizes[i] += 1\n        self.blocks = []\n        idx = 0\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n        # store some hyper-params\n        self.init_sigma_factor = float(init_sigma_factor)\n        self.p_levy = float(p_levy)\n        self.levy_df = float(levy_df)\n        self.p_de = float(p_de)\n        self.F_de = float(F_de)\n        self.p_blockswap = float(p_blockswap)\n        self.mirror = bool(mirror)\n\n        # other internals can be tuned\n        self.rot_update_every = max(2, int(np.ceil(2 + np.sqrt(self.dim) / 3)))\n        self.line_search_every = max(25, int(6 + (self.dim / 8)))\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds assumed in [-5,5] but read generically from func\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center uniformly\n        m = np.random.uniform(lb, ub)\n        # initial sigma scaled to problem range, slightly different scale\n        sigma = self.init_sigma_factor * np.mean(ub - lb)\n        # per-coordinate scales\n        s = np.ones(n)\n        # rotation (learned principal directions); start identity\n        R = np.eye(n)\n\n        # small success buffer for rotation learning (store local coordinates)\n        success_buf = []\n        buf_max = max(16, 5 * int(np.ceil(np.sqrt(n))))\n\n        # DE-like archive\n        archive_X = []\n        archive_F = []\n\n        # tracking\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # evaluate initial center\n        x0 = np.clip(m, lb, ub)\n        f0 = func(x0)\n        evals += 1\n        archive_X.append(x0.copy()); archive_F.append(f0)\n        f_opt = f0; x_opt = x0.copy()\n        f_center = f0\n        last_improv = evals\n        stagnation_limit = max(30, int(8 * n / max(1, len(self.blocks))))\n        stagn_count = 0\n\n        gen = 0\n        # adaptive target success for sigma adaptation (1/5-ish)\n        target_success = 0.2\n        # EMA rate for per-coordinate RMS update\n        ema_beta = 0.85\n\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            # sample base normals\n            Z = np.random.randn(lam, n)\n            X = np.zeros((lam, n))\n            proposals_info = []  # store local (pre-rotation) steps (y_local) for learning\n            Fvals = np.full(lam, np.inf)\n\n            for i in range(lam):\n                z = Z[i].copy()\n                if self.mirror and (i % 2 == 1):\n                    z = -z\n\n                # local scaled step before rotation\n                y_local = s * z\n\n                # apply rotation to get step in original coords\n                y_rot = R.dot(y_local)\n\n                # heavy-tailed (Student-T) jump with some probability\n                if np.random.rand() < self.p_levy:\n                    # sample a scaled t-distribution scalar then apply unit direction\n                    t = np.random.standard_t(self.levy_df)\n                    nz = np.linalg.norm(z) + 1e-20\n                    dir_unit = z / nz\n                    # scale relative to RMS of s\n                    scale = np.mean(s)\n                    y_rot = R.dot(s * dir_unit) * (1.5 * t * scale)\n\n                x = m + sigma * y_rot\n\n                # DE-style mutation from archive (if enough history)\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # block-swap mutation\n                if np.random.rand() < self.p_blockswap and len(self.blocks) > 0:\n                    bidx = np.random.randint(len(self.blocks))\n                    b = self.blocks[bidx]\n                    if len(b) > 1:\n                        perm = np.random.permutation(len(b))\n                        x_block = x[b].copy()\n                        x[b] = x_block[perm]\n\n                # clip\n                x = np.clip(x, lb, ub)\n                X[i] = x\n                # store local step (before rotation) scaled to sigma units for learning\n                proposals_info.append(y_local.copy() / (sigma + 1e-20))\n\n            # evaluate candidates (respect budget)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                # bound archive\n                if len(archive_X) > 3000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi; x_opt = xi.copy(); last_improv = evals\n\n            # selection: choose mu best, use softmax exponential weights (different from log-weights)\n            mu = max(1, lam // 2)\n            idx = np.argsort(Fvals)\n            sel = idx[:mu]\n            X_sel = X[sel]\n            Y_sel_local = np.vstack([proposals_info[j] for j in sel])  # mu x n\n\n            # softmax weights based on rank (higher weight to better)\n            ranks = np.arange(mu)[::-1]  # 0 best -> large after exp\n            # use a temperature for softmax to shape weights\n            w_temp = 1.0\n            scores = (mu - np.arange(mu)) / (mu + 1.0)\n            expw = np.exp(scores / w_temp)\n            weights = expw / np.sum(expw)\n\n            # recombined center (weighted average)\n            m_new = np.sum(weights[:, None] * X_sel, axis=0)\n\n            # accept new mean based on a logistic temperature schedule (slower cooling)\n            accept_mean = True\n            # Only evaluate center if budget allows (we count it)\n            if evals < budget:\n                xm_clip = np.clip(m_new, lb, ub)\n                fm_new = func(xm_clip)\n                evals += 1\n                archive_X.append(xm_clip.copy()); archive_F.append(fm_new)\n                if fm_new < f_opt:\n                    f_opt = fm_new; x_opt = xm_clip.copy(); last_improv = evals\n                delta = fm_new - f_center\n                # logistic acceptance probability (safer than pure exp)\n                if delta <= 0:\n                    accept_mean = True\n                    m = xm_clip.copy()\n                    f_center = fm_new\n                else:\n                    # temp-scaled acceptance: temperature decays slowly\n                    Temp = max(1e-6, 1.0 - 0.0005 * gen)\n                    p = 1.0 / (1.0 + np.exp(delta / max(1e-12, Temp)))\n                    if np.random.rand() < p:\n                        accept_mean = True\n                        m = xm_clip.copy()\n                        f_center = fm_new\n                    else:\n                        accept_mean = False\n                        # occasional small nudging toward m_new\n                        if np.random.rand() < 0.08:\n                            m = np.clip(0.92 * m + 0.08 * m_new, lb, ub)\n            else:\n                # no eval budget to check, accept clipped weighted mean\n                m = np.clip(m_new, lb, ub)\n\n            # update per-coordinate scales s using EMA of absolute local steps from selected set\n            # map to local coords is already Y_sel_local\n            mean_abs = np.sum(weights[:, None] * np.abs(Y_sel_local), axis=0)  # mu-weighted mean abs step\n            # RMS-prop like update (EMA)\n            # store s^2_ema implicitly in s (we maintain s as sqrt of ema)\n            s_old = s.copy()\n            s2 = (ema_beta * (s ** 2)) + (1.0 - ema_beta) * (mean_abs ** 2 + 1e-20)\n            s = np.sqrt(s2)\n            s = np.clip(s, 1e-8, 1e2)\n\n            # store successes (top quartile) into success buffer for rotation learning\n            q = max(1, mu // 4)\n            for j in range(q):\n                success_buf.append(Y_sel_local[j].copy())\n                if len(success_buf) > buf_max:\n                    success_buf.pop(0)\n\n            # rotation update: use power-method PCA on covariance of buffer (cheap)\n            gen += 1\n            if (len(success_buf) >= 6) and (gen % self.rot_update_every == 0):\n                # build small matrix m x n (rows are local steps)\n                S = np.vstack(success_buf)  # m x n\n                # center rows\n                S = S - np.mean(S, axis=0, keepdims=True)\n                # compute top k via power method (k= min(2, n))\n                k_take = min(2, n)\n                # compute covariance-vector products via power iteration to get top eigenvectors\n                try:\n                    # random init\n                    V = np.random.randn(n, k_take)\n                    # orthonormalize\n                    V, _ = np.linalg.qr(V)\n                    for _ in range(6):  # few iterations\n                        Zp = S.T @ (S @ V)  # n x k\n                        V, _ = np.linalg.qr(Zp)\n                    # update R first k_take columns gently toward V\n                    alpha_rot = 0.25  # mixing\n                    R[:, :k_take] = (1 - alpha_rot) * R[:, :k_take] + alpha_rot * V[:, :k_take]\n                    # re-orthonormalize full R via QR\n                    Qr, _ = np.linalg.qr(R)\n                    R = Qr[:, :n]\n                except np.linalg.LinAlgError:\n                    # small noise perturbation fallback\n                    R = R + 0.02 * np.random.randn(n, n)\n                    Qr, _ = np.linalg.qr(R)\n                    R = Qr[:, :n]\n\n            # sigma adaptation: simple 1/5-inspired adjustment based on success proportion this gen\n            # define success as improvement over f_center at sampling time (use Fvals < f_center)\n            successes = np.sum(Fvals < f_center - 1e-12)\n            psucc = successes / max(1, lam)\n            if psucc > target_success + 0.03:\n                sigma *= 0.97\n            elif psucc < target_success - 0.03:\n                sigma *= 1.06\n            else:\n                sigma *= 1.00\n            # bound sigma\n            sigma = np.clip(sigma, 1e-12, 4.0 * np.mean(ub - lb) + 1e-12)\n\n            # stagnation handling\n            if (evals - last_improv) > stagnation_limit:\n                stagn_count += 1\n                # reheat: inflate sigma and randomize a few coordinates\n                sigma *= (1.2 + 0.1 * stagn_count)\n                # randomly shuffle a pair elements between two blocks occasionally\n                if np.random.rand() < 0.6 and len(self.blocks) > 1:\n                    b1, b2 = np.random.choice(len(self.blocks), size=2, replace=False)\n                    if len(self.blocks[b1]) and len(self.blocks[b2]):\n                        i1 = np.random.choice(self.blocks[b1])\n                        i2 = np.random.choice(self.blocks[b2])\n                        # swap mapping (keep coordinates but swap positions in block lists)\n                        self.blocks[b1].remove(i1); self.blocks[b1].append(i2)\n                        self.blocks[b2].remove(i2); self.blocks[b2].append(i1)\n                # perturb rotation a bit\n                R = R + 0.03 * np.random.randn(n, n)\n                Qr, _ = np.linalg.qr(R)\n                R = Qr[:, :n]\n                # clear success buffer\n                success_buf = []\n                # tiny random restart of some dims of m\n                kflip = max(1, n // 10)\n                idxs = np.random.choice(n, size=kflip, replace=False)\n                for ii in idxs:\n                    m[ii] = np.random.uniform(lb[ii], ub[ii])\n                # evaluate new center if budget allows\n                if evals < budget:\n                    m_clip = np.clip(m, lb, ub)\n                    fm = func(m_clip)\n                    evals += 1\n                    archive_X.append(m_clip.copy()); archive_F.append(fm)\n                    if fm < f_opt:\n                        f_opt = fm; x_opt = m_clip.copy(); last_improv = evals\n                    f_center = fm\n                last_improv = evals\n\n            # periodic small 1D line-search along dominant learned direction\n            if (gen % self.line_search_every == 0) and (evals < budget):\n                d = R[:, 0]\n                # small set of probes including adaptive magnitudes\n                magnitudes = [0.6, 0.3, -0.3, -0.6]\n                for alpha in magnitudes:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + sigma * alpha * d * np.mean(s), lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = fp; x_opt = probe.copy(); last_improv = evals\n\n            # ensure center in bounds\n            m = np.clip(m, lb, ub)\n\n            # stop if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ABLE_DE scored 0.195 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "446f78f2-083b-4293-8204-2cad655bf6b2", "operator": null, "metadata": {"aucs": [0.10003822341710444, 0.15931307964730668, 0.24610575439008253, 0.24079394097609508, 0.2090962253139521, 0.23743400224774214, 0.23282321710401632, 0.20452483515537068, 0.18180011561410703, 0.14043797973684546]}, "task_prompt": ""}
{"id": "9e989576-d121-434d-9581-0c585c7ca2a7", "fitness": "-inf", "name": "LARCADE", "description": "LARCADE is a hybrid strategy combining per-coordinate diagonal scaling (D) with a learned low-rank correlated subspace (U) so sampling mixes cheap coordinate-wise exploration and a compact, adaptive correlation model while using CMA-like path-length sigma control (ps, cs, damps, chi_n) to adapt step-size. Candidates are generated with mirrored normal sampling plus low-rank coefficients, occasional heavy-tailed Cauchy jumps, and DE-style archive difference mutations to inject long-range moves and diversity; recombination uses log-style weights (lambda, mu, mu_eff) to form the new mean. Updates use a weighted second-moment EMA to adapt D, a path-length update to adapt sigma, and SVD on a buffer of recent successful steps to update U with gentle blending and re-orthonormalization; stagnation triggers reheating (inflate sigma, nudge toward good archive points, randomize U) to escape local traps. The loop is budget-aware (limits evaluations per generation, bounded archive, occasional center evaluations and short line-searches along the principal learned direction, and aggressive clipping to bounds) so it manages limited evaluations robustly.", "code": "import numpy as np\n\nclass LARCADE:\n    \"\"\"\n    LARCADE: Low-rank Adaptive Rotational CMA-DE hybrid.\n\n    One-line: Combine per-coordinate diagonal scaling with a learned low-rank correlated\n    subspace, CMA-like path-length sigma control, mirrored sampling, DE-style archive\n    difference mutations and occasional Cauchy jumps; update the low-rank subspace via SVD\n    on recent successful steps, and reheating on stagnation — all fully budget-aware.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, lam=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population size heuristic similar to CMA-ES\n        if lam is None:\n            self.lambda_ = max(4, int(4 + np.floor(3 * np.log(max(2, self.dim)))))\n        else:\n            self.lambda_ = int(lam)\n        self.mu = max(1, self.lambda_ // 2)\n        # low-rank dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # read bounds and normalize shapes\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # recombination weights (log-style)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / (np.sum(weights) + 1e-20)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length control constants (CMA-like)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # internal state\n        m = np.random.uniform(lb, ub)                         # mean\n        sigma = 0.2 * np.mean(ub - lb)                        # initial global step-size\n        D = np.ones(n)                                        # per-coordinate std approx\n        # low-rank U (n x k) orthonormal basis\n        rand_mat = np.random.randn(n, self.k)\n        try:\n            Uq, _ = np.linalg.qr(rand_mat)\n            U = Uq[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = np.zeros((n, self.k))\n\n        ps = np.zeros(n)         # path vector for sigma (approx)\n        # success buffer to learn low-rank subspace\n        success_buf = []\n        buf_max = max(10 * self.k, 20)\n\n        # archive for DE differences\n        archive_X = []\n        archive_F = []\n        archive_limit = max(500, 50 * n)\n\n        # exploration controls\n        p_cauchy = 0.12\n        cauchy_scale = 1.0\n        p_de = 0.20\n        F_de = 0.7\n        mirrored = True\n\n        # subspace update cadence\n        subspace_update_every = max(1, int(4))\n\n        # stagnation detection\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n        stagnation_gen = 0\n        stagnation_thresh = max(5, int(4 * n / max(1, self.k)))\n\n        # initial evaluation of m\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_opt = fm; x_opt = xm.copy()\n            last_improvement_eval = evals\n\n        gen = 0\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # draw base normals\n            Z = np.random.randn(current_lambda, n)\n            # low-rank coefficients\n            if self.k > 0:\n                Zlow = np.random.randn(current_lambda, self.k)\n            else:\n                Zlow = np.zeros((current_lambda, 0))\n\n            # produce candidates\n            X = np.zeros((current_lambda, n))\n            Y = np.zeros((current_lambda, n))  # stored pre-scale steps (y = (x-m)/sigma)\n            for i in range(current_lambda):\n                z = Z[i].copy()\n                # mirrored sampling\n                if mirrored and (i % 2 == 1):\n                    z = -z\n                # low-rank part\n                low = (U @ Zlow[i]) if self.k > 0 else 0.0\n                # weight of low-rank relative to diagonal\n                beta = 0.9 * np.mean(D)\n                y = (D * z) + beta * low  # combined step in coordinate space\n\n                # occasional heavy-tailed jump\n                if np.random.rand() < p_cauchy:\n                    r = np.random.standard_cauchy() * cauchy_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * np.mean(D)\n\n                x = m + sigma * y\n\n                # DE-style archive difference mutation\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                X[i] = x\n                Y[i] = y\n\n            # Evaluate candidates budget-aware\n            Fvals = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                # keep archive bounded\n                if len(archive_X) > archive_limit:\n                    # remove oldest\n                    del archive_X[0]; del archive_F[0]\n                if fi < f_opt:\n                    f_opt = fi; x_opt = xi.copy(); last_improvement_eval = evals\n\n            # selection & recombination\n            idx = np.argsort(Fvals)\n            sel_idx = idx[:mu]\n            X_sel = X[sel_idx]\n            Y_sel = Y[sel_idx]  # steps (y)\n            m_old = m.copy()\n            # weighted recombined mean using log-weights\n            m_new = np.sum(weights[:, None] * X_sel, axis=0)\n\n            # compute weighted mean step in y-space relative to old mean\n            # (We consider y = (x - m_old)/sigma for path updates)\n            y_sel_rel = (X_sel - m_old[np.newaxis, :]) / (sigma + 1e-20)\n            y_w = np.sum(weights[:, None] * y_sel_rel, axis=0)\n\n            # approx inverse sqrt covariance via diagonal + low-rank correction\n            invdiag = 1.0 / (D + 1e-20)\n            # approximate invsqrtC @ y_w: use invdiag + small projection on U\n            if self.k > 0:\n                # project y_w on U and scale projections modestly\n                proj = U.T @ y_w  # k\n                # approximate low-rank inverse effect (simple scaling)\n                proj_corr = U @ (proj / (1.0 + np.linalg.norm(proj) + 1e-20))\n                invsqrt_y = y_w * invdiag - 0.5 * proj_corr\n            else:\n                invsqrt_y = y_w * invdiag\n\n            # update ps and sigma\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * invsqrt_y\n            norm_ps = np.linalg.norm(ps)\n            sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n            # clamp sigma\n            sigma = np.clip(sigma, 1e-12, 5.0 * np.mean(ub - lb) + 1e-12)\n\n            # update diagonal scales D with exponential moving second-moment of selected y (pre-scale)\n            y2 = np.sum(weights[:, None] * (y_sel_rel ** 2), axis=0)\n            c_d = 0.25\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n            # guard D\n            D = np.clip(D, 1e-6, 1e6)\n\n            # store weighted successful weighted step for subspace learning (use y_w)\n            success_buf.append(y_w.copy())\n            if len(success_buf) > buf_max:\n                success_buf.pop(0)\n\n            gen += 1\n            # update low-rank subspace U periodically from success buffer via SVD\n            if (len(success_buf) >= self.k) and (gen % subspace_update_every == 0):\n                try:\n                    Ymat = np.vstack(success_buf).T  # n x m\n                    Ymat = Ymat - np.mean(Ymat, axis=1, keepdims=True)\n                    # economy SVD\n                    U_s, S_s, Vt_s = np.linalg.svd(Ymat, full_matrices=False)\n                    k_take = min(self.k, U_s.shape[1])\n                    if k_take > 0:\n                        U_new = U_s[:, :k_take]\n                        # gently blend the subspace to avoid abrupt changes\n                        alpha = 0.6\n                        # if shapes mismatch, pad\n                        if U_new.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U_new.shape[1]))\n                            U_new = np.hstack([U_new, pad])\n                        # orthonormalize new U\n                        Q, _ = np.linalg.qr(U_new)\n                        U = (1.0 - alpha) * U + alpha * Q[:, :self.k]\n                        # re-orthonormalize\n                        Qr, _ = np.linalg.qr(U)\n                        U = Qr[:, :self.k]\n                except np.linalg.LinAlgError:\n                    # keep previous U on failure\n                    pass\n\n            # accept new mean m_new; optionally evaluate center to decide with metropolis temperature if budget allows\n            # Simpler rule: if m_new is within bounds, evaluate it with small probability to save budget; otherwise just move\n            accept_move = True\n            # biased acceptance: if recombination produced a point better than current best candidate, adopt immediately\n            if evals < budget and np.random.rand() < 0.5:\n                # evaluate clipped m_new if budget allows\n                xm_clip = np.clip(m_new, lb, ub)\n                fm_new = func(xm_clip)\n                evals += 1\n                archive_X.append(xm_clip.copy()); archive_F.append(fm_new)\n                if fm_new < f_opt:\n                    f_opt = fm_new; x_opt = xm_clip.copy(); last_improvement_eval = evals\n                # accept if better than current mean surrogate (use f_opt as baseline conservatively)\n                # This is conservative: accept if it doesn't look catastrophic\n                if fm_new < (f_opt + 1e-12) or np.random.rand() < 0.2:\n                    m = xm_clip.copy()\n                else:\n                    # small nudge toward m_new\n                    m = np.clip(0.9 * m + 0.1 * m_new, lb, ub)\n            else:\n                m = np.clip(m_new, lb, ub)\n\n            # stagnation detection and reheating\n            if (evals - last_improvement_eval) > stagnation_thresh * max(1, current_lambda):\n                stagnation_gen += 1\n                # inflate sigma and nudge mean toward a random archive good point\n                sigma *= (1.3 + 0.1 * stagnation_gen)\n                sigma = min(sigma, 5.0 * np.mean(ub - lb))\n                if len(archive_X) > 0:\n                    # pick an archive point among top 10% or random if small\n                    sorted_idx = np.argsort(archive_F)\n                    topk = max(1, int(0.1 * len(sorted_idx)))\n                    pick_idx = sorted_idx[np.random.randint(min(topk, len(sorted_idx)))]\n                    m = 0.7 * m + 0.3 * archive_X[pick_idx]\n                # randomize a bit of low-rank subspace to escape\n                if self.k > 0:\n                    U = U + 0.05 * np.random.randn(*U.shape)\n                    Qr, _ = np.linalg.qr(U)\n                    U = Qr[:, :self.k]\n                # reset success buffer\n                success_buf = []\n                last_improvement_eval = evals\n\n            # periodic short line-search along principal learned direction if budget allows\n            if (gen % max(4, int(5 * n / max(1, self.k)))) == 0 and evals < budget:\n                # use principal direction from U if available else use ps\n                if self.k > 0:\n                    d = U[:, 0]\n                else:\n                    d = ps / (np.linalg.norm(ps) + 1e-20)\n                # small probe steps\n                alphas = [0.5, -0.5, 0.25, -0.25]\n                for alpha in alphas:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + sigma * alpha * d * np.mean(D), lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = fp; x_opt = probe.copy(); last_improvement_eval = evals\n\n            # safety clamps and housekeeping\n            sigma = np.clip(sigma, 1e-12, 10.0 * np.mean(ub - lb) + 1e-12)\n            # ensure archive bounded\n            if len(archive_X) > archive_limit:\n                # remove oldest chunks\n                remove_cnt = len(archive_X) - archive_limit\n                del archive_X[:remove_cnt]; del archive_F[:remove_cnt]\n\n            # main loop continues until budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 175, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,1) (2,2) \nOn line: m_new = np.sum(weights[:, None] * X_sel, axis=0)", "error": "In the code, line 175, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,1) (2,2) \nOn line: m_new = np.sum(weights[:, None] * X_sel, axis=0)", "parent_ids": "446f78f2-083b-4293-8204-2cad655bf6b2", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9d9f78ad-978b-4980-9123-24520d7bfe63", "fitness": 0.5824946338908316, "name": "LARSC", "description": "A hybrid heuristic that mixes Langevin-style stochastic subspace probes with occasional CMA-ES-like covariance generations: it keeps a CMA-like state (mean m, covariance C, evolution paths ps/pc, eigenpairs B/D and invsqrtC) updated periodically and sometimes uses a full CMA sampling round (controlled by p_cma_round and lam) with selection/recombination and archive-based DE perturbations (p_de, F_de). Subspace exploration builds an orthonormal k-dimensional basis from an LRU memory of successful unit directions plus random fill (k ≈ n^subspace_exp, memory_size), then launches many cheap probes per subspace (probes_mult) using Langevin-style lengths, momentum (momentum_scale) and stochastic noise; successful probes are stored in dir_memory and trigger short budget-aware intensification (short_line_search) and moderate updates of m and sigma (sigma init ~0.3*domain_mean). Local control rules adapt step and sigma (grow/shrink, min_step, max_step), occasional full-space polishing draws, and conservative safeguards for numerical stability (min sigma, eigen recomputation cadence eig_every). All evaluations are budget-aware via safe_eval, archive pruning is applied to bound memory, and short golden-section-like line searches use limited remaining evaluations for focused improvement.", "code": "import numpy as np\n\nclass LARSC:\n    \"\"\"\n    LARSC: Langevin-Adapted Rotational Subspace CMA\n\n    One-line: Memory-accelerated Langevin subspace probes with momentum + occasional\n    CMA-ES style covariance generations, archive DE perturbations and budget-aware short\n    line searches for intensified improvement.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=12, subspace_exp=2.0/3.0, probes_mult=3,\n                 p_cma_round=0.12, p_de=0.18, F_de=0.8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.subspace_exp = float(subspace_exp)\n        self.probes_mult = int(probes_mult)\n        self.p_cma_round = float(p_cma_round)  # probability to trigger a CMA generation round\n        self.p_de = float(p_de)\n        self.F_de = float(F_de)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        domain_mean = np.mean(domain_range)\n\n        # RNG\n        rng = np.random.default_rng(self.seed)\n\n        # CMA-like initialization\n        lam0 = max(4, int(4 + np.floor(3 * np.log(max(1, n)))))\n        lam = lam0\n        mu = max(1, lam // 2)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1).astype(float))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        c1 = 2.0 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1.0 - c1, 2.0 * (mu_eff - 2.0 + 1.0 / mu_eff) / ((n + 2.0) ** 2 + mu_eff))\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # dynamic CMA state\n        m = rng.uniform(lb, ub)\n        sigma = max(1e-12, 0.3 * domain_mean)\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eig_every = max(1, int(10 * n))\n        eigen_counter = 0\n\n        # subspace and memory\n        dir_memory = []  # LRU unit directions\n        momentum_scale = 0.45  # weight for memory momentum in Langevin proposals\n\n        # probing parameters\n        k_base = max(1, int(np.ceil(n ** self.subspace_exp)))\n        probes_base = max(6, self.probes_mult * k_base)\n        step = 0.24 * domain_mean\n        min_step = 1e-6 * max(1.0, domain_mean)\n        grow = 1.22\n        shrink = 0.72\n        max_step = 5.0 * domain_mean\n\n        # archive\n        X_archive = []\n        F_archive = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # safe eval wrapper\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = np.clip(x, lb, ub)\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x.copy())\n            F_archive.append(f)\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initial evaluation\n        out = safe_eval(m.copy())\n        if out[0] is None:\n            return float(f_best if x_best is not None else np.inf), np.array(x_best if x_best is not None else np.zeros(n), dtype=float)\n\n        # short budget-aware golden-like line search\n        def short_line_search(x0, f0, d, init_step, max_evals=6):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            remaining = self.budget - evals\n            if remaining <= 0:\n                return None, None\n            s = float(abs(init_step)) + 1e-12\n            # coarse probes +s and -s\n            xa = np.clip(x0 + s * d, lb, ub)\n            out = safe_eval(xa)\n            if out[0] is None:\n                return None, None\n            fa = out[0]\n            remaining = self.budget - evals\n            if fa < f0:\n                a, b = 0.0, s\n                fa_val, fb_val = f0, fa\n            else:\n                xb = np.clip(x0 - s * d, lb, ub)\n                out2 = safe_eval(xb)\n                if out2[0] is None:\n                    return None, None\n                fb = out2[0]\n                remaining = self.budget - evals\n                if fb < f0:\n                    a, b = -s, 0.0\n                    fa_val, fb_val = fb, f0\n                else:\n                    return None, None\n            # limited golden steps\n            gr = (np.sqrt(5) - 1) / 2\n            c = b - gr * (b - a)\n            d_alpha = a + gr * (b - a)\n            xc = np.clip(x0 + c * d, lb, ub)\n            outc = safe_eval(xc)\n            if outc[0] is None:\n                return None, None\n            fc = outc[0]\n            xd = np.clip(x0 + d_alpha * d, lb, ub)\n            outd = safe_eval(xd)\n            if outd[0] is None:\n                return None, None\n            fd = outd[0]\n            best_f = f0\n            best_x = x0.copy()\n            for val, px in ((fa if 'fa' in locals() else None, xa if 'xa' in locals() else None),\n                            (fb if 'fb' in locals() else None, xb if 'xb' in locals() else None),\n                            (fc, xc), (fd, xd)):\n                try:\n                    if val is not None and val < best_f:\n                        best_f = val; best_x = px.copy()\n                except Exception:\n                    pass\n            iters = 0\n            while evals < self.budget and iters < max_evals:\n                iters += 1\n                if fc < fd:\n                    b = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    c = b - gr * (b - a)\n                    xc = np.clip(x0 + c * d, lb, ub)\n                    outc = safe_eval(xc)\n                    if outc[0] is None:\n                        break\n                    fc = outc[0]\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    a = c\n                    c = d_alpha\n                    fc = fd\n                    d_alpha = a + gr * (b - a)\n                    xd = np.clip(x0 + d_alpha * d, lb, ub)\n                    outd = safe_eval(xd)\n                    if outd[0] is None:\n                        break\n                    fd = outd[0]\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            # adapt subspace dimension and probes\n            k = max(1, int(np.clip(int(np.ceil(n ** self.subspace_exp)), 1, n)))\n            probes = max(6, self.probes_mult * k)\n\n            # Build subspace basis using memory + random fill, orthonormalize\n            use_mem = min(len(dir_memory), max(1, k // 2))\n            if use_mem > 0:\n                mem_vecs = np.array(dir_memory[:use_mem])  # shape (use_mem, n)\n                R = np.column_stack((mem_vecs.T, rng.standard_normal(size=(n, k - use_mem))))\n            else:\n                R = rng.standard_normal(size=(n, k))\n            # QR to get orthonormal basis (take first k columns)\n            try:\n                Q, _ = np.linalg.qr(R, mode='reduced')\n                basis = Q[:, :k]\n            except Exception:\n                # fallback to random orthonormal\n                R2 = rng.standard_normal(size=(n, k))\n                Q, _ = np.linalg.qr(R2)\n                basis = Q[:, :k]\n\n            # Decide whether to run a CMA generation this outer round\n            run_cma = (rng.random() < self.p_cma_round) and (remaining >= lam)\n\n            if run_cma:\n                # perform a CMA-style generation using current B,D\n                current_lambda = min(lam, remaining)\n                half = (current_lambda + 1) // 2\n                arz = rng.standard_normal(size=(half, n))\n                arz_full = np.vstack([arz, -arz])[:current_lambda]\n                # sample with BD\n                ary = arz_full * D[np.newaxis, :]  # scale\n                ary = ary @ B.T\n                arx = m + sigma * ary\n                # archive DE perturbations\n                for kidx in range(current_lambda):\n                    if (rng.random() < self.p_de) and (len(X_archive) >= 2):\n                        i1, i2 = rng.choice(len(X_archive), size=2, replace=False)\n                        arx[kidx] = arx[kidx] + self.F_de * (X_archive[i1] - X_archive[i2])\n                    arx[kidx] = np.clip(arx[kidx], lb, ub)\n                arfit = np.full(current_lambda, np.inf)\n                for kidx in range(current_lambda):\n                    if evals >= self.budget:\n                        break\n                    out = safe_eval(arx[kidx])\n                    if out[0] is None:\n                        break\n                    arfit[kidx] = out[0]\n                # selection & recombination\n                valid = np.isfinite(arfit)\n                if np.any(valid):\n                    valid_idx = np.where(valid)[0]\n                    idx_sorted = np.argsort(arfit[valid_idx])\n                    sel_idx = valid_idx[idx_sorted[:mu]]\n                    x_sel = arx[sel_idx]\n                    y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n                    m_old = m.copy()\n                    # recombine\n                    m = np.sum(weights[:, None] * x_sel, axis=0)\n                    y_w = np.sum(weights[:, None] * y_sel, axis=0)\n                    # update evolution paths\n                    try:\n                        ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrtC @ y_w)\n                    except Exception:\n                        ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * y_w\n                    norm_ps = np.linalg.norm(ps)\n                    denom = np.sqrt(1.0 - (1.0 - cs) ** (2.0 * (1 + evals / max(1, current_lambda))))\n                    hsig = 1.0 if (norm_ps / (denom + 1e-20) / chi_n) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n                    pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n                    # rank updates\n                    rank_one = np.outer(pc, pc)\n                    rank_mu = np.zeros((n, n))\n                    for i in range(y_sel.shape[0]):\n                        yi = y_sel[i][:, None]\n                        rank_mu += weights[i] * (yi @ yi.T)\n                    C = (1.0 - c1 - cmu) * C + c1 * (rank_one + (1.0 - hsig) * cc * (2.0 - cc) * C) + cmu * rank_mu\n                    # adapt sigma\n                    sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n                    sigma = max(sigma, 1e-12)\n                # recompute eigen if due\n                eigen_counter += current_lambda\n                if eigen_counter >= eig_every:\n                    eigen_counter = 0\n                    C = np.triu(C) + np.triu(C, 1).T\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(C)\n                        eigvals = np.maximum(eigvals, 1e-20)\n                        D = np.sqrt(eigvals)\n                        B = eigvecs\n                        invsqrtC = (B * (1.0 / D)) @ B.T\n                    except Exception:\n                        C = np.eye(n)\n                        B = np.eye(n); D = np.ones(n); invsqrtC = np.eye(n)\n                # after CMA round, continue to next outer iteration\n                continue\n\n            # Otherwise run many cheap Langevin-style subspace probes\n            improved_round = False\n            momentum = dir_memory[0] * momentum_scale if dir_memory else np.zeros(n)\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                coeffs = rng.normal(size=k)\n                d_sub = basis @ coeffs\n                if np.linalg.norm(d_sub) == 0:\n                    continue\n                d_sub = d_sub / (np.linalg.norm(d_sub) + 1e-20)\n                candidate_dir = momentum + (1.0 - momentum_scale) * d_sub\n                if np.linalg.norm(candidate_dir) == 0:\n                    candidate_dir = d_sub\n                candidate_dir = candidate_dir / (np.linalg.norm(candidate_dir) + 1e-20)\n                # Langevin-style stochastic length\n                noise = rng.normal(scale=0.14 * step)\n                length = rng.uniform(-step, step) + noise\n                x_prop = np.clip(m + length * candidate_dir, lb, ub)\n                out = safe_eval(x_prop)\n                if out[0] is None:\n                    break\n                f_prop = out[0]\n                if f_prop < f_best - 1e-12:\n                    # success: add to memory, try short intensification along direction\n                    dir_succ = x_prop - m\n                    dn = np.linalg.norm(dir_succ)\n                    if dn > 0:\n                        dir_unit = dir_succ / dn\n                        dir_memory.insert(0, dir_unit.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                    # intensify (short line-search)\n                    remaining_budget = self.budget - evals\n                    if remaining_budget >= 2:\n                        ls_budget = min(6, remaining_budget)\n                        ls_out = short_line_search(x_prop, f_prop, dir_unit if dn > 0 else candidate_dir, init_step=abs(length) + 0.5 * step, max_evals=ls_budget)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_prop - 1e-12:\n                                f_prop = f_ls\n                                x_prop = x_ls\n                    # accept improvement: move mean toward x_prop moderately\n                    m = 0.8 * m + 0.2 * x_prop\n                    sigma = max(sigma, 0.18 * domain_mean)\n                    step = min(max_step, step * grow)\n                    improved_round = True\n                    # update CMA pseudo-paths with the successful step (weight it lightly)\n                    y_w = (x_prop - m) / (sigma + 1e-20)\n                    try:\n                        ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrtC @ y_w)\n                    except Exception:\n                        ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * y_w\n                    norm_ps = np.linalg.norm(ps)\n                    hsig = 1.0 if (norm_ps / (chi_n + 1e-20)) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n                    pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n                    C = (1.0 - c1 - cmu) * C + c1 * np.outer(pc, pc) + cmu * np.outer(y_w, y_w)\n                else:\n                    # occasionally try a focused short line search from current m along candidate_dir\n                    if rng.random() < 0.045 and (self.budget - evals) >= 3:\n                        ls_out = short_line_search(m, f_best if x_best is not None else np.inf, candidate_dir, init_step=step, max_evals=min(5, self.budget - evals))\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_best - 1e-12:\n                                # treat as improvement as above\n                                dir_succ = x_ls - m\n                                dn = np.linalg.norm(dir_succ)\n                                if dn > 0:\n                                    dir_unit = dir_succ / dn\n                                    dir_memory.insert(0, dir_unit.copy())\n                                    if len(dir_memory) > self.memory_size:\n                                        dir_memory.pop()\n                                m = 0.8 * m + 0.2 * x_ls\n                                sigma = max(sigma, 0.18 * domain_mean)\n                                step = min(max_step, step * grow)\n                                improved_round = True\n\n            # After subspace probes, adapt step on failure\n            if not improved_round:\n                step = max(min_step, step * shrink)\n            else:\n                # occasional small full-space polishing draws\n                for _ in range(min(4, max(1, n // 4))):\n                    if evals >= self.budget:\n                        break\n                    d = rng.normal(size=n)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                    a = rng.uniform(-0.5 * step, 0.5 * step)\n                    outp = safe_eval(np.clip(m + a * d, lb, ub))\n                    if outp[0] is None:\n                        break\n                    if outp[0] < f_best - 1e-12:\n                        # store direction\n                        dir_succ = outp[1] - m\n                        dn = np.linalg.norm(dir_succ)\n                        if dn > 0:\n                            dir_memory.insert(0, (dir_succ / dn).copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n\n            # archive pruning to keep memory manageable\n            max_archive = max(2000, 50 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                step_keep = max(1, len(rest) // (max_archive - 200))\n                keep_rest = rest[::step_keep]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # periodic eigen recomputation from C\n            eigen_counter += 1\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(C)\n                    eigvals = np.maximum(eigvals, 1e-20)\n                    D = np.sqrt(eigvals)\n                    B = eigvecs\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except Exception:\n                    C = np.eye(n)\n                    B = np.eye(n); D = np.ones(n); invsqrtC = np.eye(n)\n\n            # safeguard sigma and step\n            sigma = max(sigma, 1e-12)\n            step = max(step, min_step)\n\n            # quick stop if excellent\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            # fallback to last mean\n            return float(np.inf), np.clip(m, lb, ub)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm LARSC scored 0.582 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "446f78f2-083b-4293-8204-2cad655bf6b2", "operator": null, "metadata": {"aucs": [0.23086459189869202, 0.3796840962956749, 0.6346915258149552, 0.9010052633655682, 0.7996102576163727, 0.8316246922296556, 0.2817891466229355, 0.6908954383982551, 0.8330315731126428, 0.2417497535535642]}, "task_prompt": ""}
{"id": "cbb311dc-12e9-4b46-9446-f421a5425321", "fitness": "-inf", "name": "AdaptiveSubspaceLevySearch", "description": "The algorithm maintains a small ensemble (3–6) of diverse centers with anisotropic per-dimension trust radii (trust_init_frac=0.5, expand=1.5, shrink=0.55, trust_min tiny) and replaces stagnant centers after limited patience, encouraging both local refinement and exploration. It builds cheap local surrogate information from the archive: per-dimension importance weights from the std of top points, a weighted full-space linear gradient, and a low‑rank (small r, often ≤3) quadratic fitted in a PCA subspace with light regularization, using a neighbor multiplier (~5*n) to select local data. Candidate generation mixes conservative local moves (subspace quadratic minimizers, gradient-scaled steps, backtracking line/subspace search, coordinate-pattern tweaks, mirror/midpoint exploitation) with stochastic exploration (Gaussian scales, rare tempered Cauchy heavy-tail jumps with low probability ~0.09 and clipped magnitude) and prefers small steps by ranking proposals by distance. Budget and archive are strictly enforced (safe_eval, per-iteration max_eval_per_iter ~40, init_ratio bounds, archive pruning), and trust radii/stagnation are adaptively updated on success/failure to steer search.", "code": "import numpy as np\n\nclass AdaptiveSubspaceLevySearch:\n    \"\"\"\n    Adaptive Subspace + Tempered-Levy Search (ASTLeS)\n\n    Main idea:\n      - Maintain a small ensemble of diverse centers with anisotropic trust vectors.\n      - Learn per-dimension importance from the archive (std/variance of good points).\n      - Fit cheap local models by (a) linear gradient in full space and (b) low-rank\n        quadratic curvature in a small PCA subspace of local neighbors.\n      - Generate candidate proposals from several mechanisms:\n          * Subspace quadratic minimizers (cheap low-rank solve)\n          * Backtracked line/subspace search along model descent directions\n          * Tempered Levy/Cauchy heavy-tailed jumps (for global escapes)\n          * Probabilistic coordinate-pattern tweaks biased by importance weights\n          * Mirror-reflection around best points to exploit directions between centers\n      - Adapt per-dimension trust radii per center on success/failure, replace\n        stagnant centers by archive-guided or random samples.\n      - Enforce strict budget via safe_eval and prune the archive to bounded size.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 ensemble_size=None, init_ratio=0.12, max_eval_per_iter=40):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n\n        # ensemble size small for stability\n        if ensemble_size is None:\n            self.ensemble_size = max(3, min(6, self.dim // 2 + 1))\n        else:\n            self.ensemble_size = max(1, int(ensemble_size))\n\n        # initialization & archive sizes\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(12, 2 * self.dim)\n        self.max_init = min(400, int(0.3 * self.budget))\n        self.max_archive = max(1000, 50 * self.dim)\n\n        # trust region controls (per-dim anisotropic)\n        self.trust_init_frac = 0.5\n        self.trust_min = 1e-6\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.5\n        self.failure_shrink = 0.55\n\n        # modeling\n        self.model_neighbors_multiplier = 5  # neighbors = mult * dim\n        self.low_rank_subspace = min(3, max(1, self.dim // 4))\n        self.reg_quadratic = 1e-6\n\n        # sampling controls\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        self.cauchy_prob = 0.09\n        self.cauchy_scale_frac = 0.7\n\n        # stagnation & replacement\n        self.center_replace_patience = 12\n\n        # direction scales used in pattern search\n        self.direction_scales = np.array([0.25, 0.6, 1.0, 1.6])\n\n    def __call__(self, func):\n        n = int(self.dim)\n\n        # bounds: prefer func.bounds if present, else [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n        rng_range = np.maximum(rng_range, 1e-12)\n\n        budget = int(self.budget)\n        evals = 0\n\n        X = []  # list of evaluated points\n        F = []  # corresponding values\n        f_best = np.inf\n        x_best = None\n\n        # small helper - safe eval awaiting budget, returns (f,x_clipped) or None if budget exhausted\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # initial space-filling random draws\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x0 = self.rng.uniform(lb, ub)\n            out = safe_eval(x0)\n            if out is None:\n                break\n\n        # helper: choose up to k diverse best centers (maximize min-distance among chosen)\n        def pick_diverse_centers(k):\n            if len(X) == 0:\n                return [np.array(self.rng.uniform(lb, ub))]\n            idx_sorted = np.argsort(F)\n            chosen = []\n            for idx in idx_sorted:\n                cand = np.array(X[idx])\n                if len(chosen) == 0:\n                    chosen.append(cand)\n                else:\n                    dmin = min(np.linalg.norm(cand - c) for c in chosen)\n                    # require modest diversity relative to range\n                    if dmin > 1e-6 + 0.02 * np.linalg.norm(rng_range):\n                        chosen.append(cand)\n                    elif len(chosen) < k:\n                        chosen.append(cand)  # allow some near duplicates if needed\n                if len(chosen) >= k:\n                    break\n            # pad if not enough\n            while len(chosen) < k:\n                chosen.append(np.array(self.rng.uniform(lb, ub)))\n            return chosen[:k]\n\n        # initialize centers and per-center trust radii and stagnation counters\n        centers = pick_diverse_centers(self.ensemble_size)\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n        stagnation = [0 for _ in centers]\n\n        iter_count = 0\n        # main loop - iterate until budget exhausted\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            # compute per-dimension importance from top subset of archive\n            importance = np.ones(n)\n            if len(X) >= max(5, 2 * n):\n                topk = max(5, min(len(X), 10 * n))\n                best_idx = np.argsort(F)[:topk]\n                pts = np.asarray([X[i] for i in best_idx])\n                stds = np.std(pts, axis=0)\n                # importance proportional to std (higher variance dims get more focus)\n                importance = stds + 1e-12\n                importance = importance / (np.max(importance) + 1e-12)\n                # avoid zeros\n                importance = 0.1 + 0.9 * importance\n            else:\n                # random small bias early on to diversify\n                v = self.rng.rand(n)\n                importance = 0.3 + 0.7 * (v / (np.max(v) + 1e-12))\n\n            # shuffle order of centers to avoid favoritism\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n            improved_global = False\n\n            for cidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n\n                center = np.array(centers[cidx])\n                tr = np.maximum(trust_radius[cidx], self.trust_min * rng_range)\n\n                # gather neighbors from archive\n                X_arr = np.asarray(X) if len(X) > 0 else np.empty((0, n))\n                F_arr = np.asarray(F) if len(F) > 0 else np.empty((0,))\n                enough_data = len(X) >= max(2 * n + 1, self.model_neighbors_multiplier * n)\n\n                # --- LOCAL MODELING: linear full-space gradient; low-rank quadratic in PCA subspace\n                grad = None\n                subspace_V = None\n                sub_h_diag = None\n                if enough_data:\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X_arr), max(2 * n + 1, self.model_neighbors_multiplier * n))\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]  # m x n\n                    F_nei = F_arr[idx_nei]\n\n                    # weighted linear fit for gradient estimate (weights by inverse distance)\n                    dx = X_nei - center\n                    dnorm = np.linalg.norm(dx, axis=1)\n                    w = 1.0 / (1e-6 + dnorm)\n                    w = w / (np.max(w) + 1e-12)\n                    W = np.sqrt(w)[:, None]\n                    A = np.hstack([np.ones((dx.shape[0], 1)), dx])  # [1, dx]\n                    try:\n                        params, *_ = np.linalg.lstsq(W * A, W * F_nei, rcond=None)\n                        grad = params[1:].flatten()\n                    except Exception:\n                        grad = None\n\n                    # PCA on local dx to get low-rank subspace\n                    try:\n                        # center the dx (already around center) and compute covariance\n                        C = np.cov(dx.T)\n                        # ensure positive semi-definite\n                        eigvals, eigvecs = np.linalg.eigh(C + 1e-12 * np.eye(n))\n                        idxp = np.argsort(-np.abs(eigvals))[:min(self.low_rank_subspace, n)]\n                        V = eigvecs[:, idxp]  # n x r\n                        subspace_V = V\n                        # project dx into subspace and fit separable quadratic there:\n                        Z = dx @ V  # m x r\n                        # design matrix [1, z, 0.5*z^2] for per-dim quadratic in subspace\n                        A_q = np.hstack([np.ones((Z.shape[0], 1)), Z, 0.5 * (Z ** 2)])\n                        ridge = self.reg_quadratic * np.eye(A_q.shape[1])\n                        try:\n                            params_q, *_ = np.linalg.lstsq(A_q.T @ A_q + ridge, A_q.T @ F_nei, rcond=None)\n                            params_q = params_q.flatten()\n                            # extract quadratic diag terms in subspace\n                            r = V.shape[1]\n                            h_sub = params_q[1 + r: 1 + 2 * r]\n                            # regularize to positive curvature\n                            h_sub = np.maximum(h_sub, 1e-8)\n                            sub_h_diag = h_sub\n                        except Exception:\n                            sub_h_diag = None\n                    except Exception:\n                        subspace_V = None\n                        sub_h_diag = None\n\n                # propose list\n                proposals = []\n\n                # 1) Subspace quadratic minimizer (cheap low-rank)\n                if grad is not None and subspace_V is not None and sub_h_diag is not None:\n                    # gradient projected into subspace\n                    g_sub = subspace_V.T @ grad\n                    # minimizer in subspace: delta_sub = - g_sub / h_sub (elementwise)\n                    delta_sub = - g_sub / (sub_h_diag + 1e-12)\n                    # clip delta_sub to subspace-projected trust radii: per-dim project tr onto subspace basis\n                    # approximate per-subspace radius by projecting per-dim trust\n                    tr_sub = np.maximum(np.abs(subspace_V).T @ tr, 1e-12)\n                    delta_sub = np.clip(delta_sub, -tr_sub, tr_sub)\n                    xq = center + subspace_V @ delta_sub\n                    xq = np.clip(xq, lb, ub)\n                    proposals.append(('subquad', xq))\n\n                    # also try damped variants\n                    for damp in (0.25, 0.6, 1.0):\n                        xq2 = center + subspace_V @ (damp * delta_sub)\n                        proposals.append((f'subquad_d{damp}', np.clip(xq2, lb, ub)))\n\n                # 2) Gradient-ish step in full space (scaled by per-dim importance and trust)\n                if grad is not None:\n                    g = grad\n                    gn = np.linalg.norm(g)\n                    if gn > 0:\n                        # step lengths shaped by importance and trust\n                        step_len = (tr * importance) * (1.0 / (1.0 + gn)) * 0.9\n                        step = -np.sign(g) * step_len\n                        xlin = np.clip(center + step, lb, ub)\n                        proposals.append(('lin_imp', xlin))\n                        # try scaled versions\n                        proposals.append(('lin_imp_half', np.clip(center + 0.5 * step, lb, ub)))\n                        proposals.append(('lin_imp_double', np.clip(center + 1.6 * step, lb, ub)))\n\n                # 3) Mirror-reflection relative to current best to exploit direction between center and best\n                if x_best is not None:\n                    direction = x_best - center\n                    mirror = x_best + 0.6 * direction  # go beyond best along line\n                    mirror = np.clip(mirror, lb, ub)\n                    proposals.append(('mirror', mirror))\n                    # also a conservative midpoint\n                    midpoint = center + 0.5 * direction\n                    proposals.append(('midpoint', np.clip(midpoint, lb, ub)))\n\n                # 4) Gaussian and tempered-Cauchy mixtures around center for exploration\n                for sf in (0.15, 0.45, 1.0):\n                    noise = self.rng.randn(n) * (sf * tr)\n                    proposals.append((f'gauss_{sf}', np.clip(center + noise, lb, ub)))\n                # tempered cauchy occasionally (prob gating at evaluation time to save budget)\n                if self.rng.rand() < 0.5:\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -6, 6)\n                    scale = self.cauchy_scale_frac * (rng_range * 0.5)\n                    proposals.append(('temp_cauchy', np.clip(center + jump * scale, lb, ub)))\n\n                # 5) Probabilistic coordinate/pattern tweaks biased by importance\n                # choose subset of coordinates with probability proportional to importance\n                prob = importance / (np.sum(importance) + 1e-12)\n                kcoords = min(n, max(2, int(2 + np.round(0.8 * n * (work_allow / (self.max_eval_per_iter + 1))))))\n                coord_idx = self.rng.choice(n, size=kcoords, replace=False, p=prob)\n                coord_step = np.maximum(tr * 0.6, 1e-12)\n                for i in coord_idx:\n                    for s in (0.6, 1.0, 1.6):\n                        xup = center.copy()\n                        xup[i] = xup[i] + s * coord_step[i]\n                        proposals.append((f'coord_{i}_plus_{s}', np.clip(xup, lb, ub)))\n                        xdn = center.copy()\n                        xdn[i] = xdn[i] - s * coord_step[i]\n                        proposals.append((f'coord_{i}_minus_{s}', np.clip(xdn, lb, ub)))\n\n                # deduplicate proposals by exact close equality\n                unique = []\n                unique_names = []\n                for name, p in proposals:\n                    if not any(np.allclose(p, q, atol=1e-12) for q in unique):\n                        unique.append(p)\n                        unique_names.append(name)\n\n                # rank proposals by conservative closeness to center (prefer small steps)\n                unique_sorted = sorted(unique, key=lambda z: np.linalg.norm(z - center))\n\n                # Evaluate proposals (but do not exceed per-iteration allowance)\n                improved_this_center = False\n                for xprop in unique_sorted:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    out = safe_eval(xprop)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    fprop, xprop = out\n                    # if improved global, accept and expand trust for this center\n                    if fprop < f_best - 1e-12:\n                        improved_global = True\n                        improved_this_center = True\n                        centers[cidx] = xprop.copy()\n                        trust_radius[cidx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                        stagnation[cidx] = 0\n                        break\n\n                # 6) If no improvement from proposals, perform a short backtracking line/subspace search along best descent direction\n                if (not improved_this_center) and work_allow > 0 and grad is not None:\n                    # choose direction combining full grad and subspace descent (if available)\n                    if subspace_V is not None and sub_h_diag is not None:\n                        d_sub = - subspace_V @ ( (subspace_V.T @ grad) / (sub_h_diag + 1e-12) )\n                        direction = d_sub\n                    else:\n                        direction = -grad\n                    # normalize direction and apply backtracking reductions until budget used or improvement\n                    dir_norm = np.linalg.norm(direction)\n                    if dir_norm > 1e-12:\n                        direction = direction / dir_norm\n                        # initial step proportional to average trust\n                        step0 = np.mean(tr) * 0.9\n                        alpha = 1.0\n                        # try geometric reductions\n                        for shrink in (1.0, 0.6, 0.3, 0.12):\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            x_try = np.clip(center + direction * (step0 * shrink), lb, ub)\n                            out = safe_eval(x_try)\n                            work_allow -= 1\n                            if out is None:\n                                break\n                            ftry, xtry = out\n                            if ftry < f_best - 1e-12:\n                                improved_global = True\n                                improved_this_center = True\n                                centers[cidx] = xtry.copy()\n                                trust_radius[cidx] = np.minimum(tr * (1.2 + 0.3 * shrink), self.trust_max_frac * rng_range)\n                                stagnation[cidx] = 0\n                                break\n\n                # 7) Tempered heavy-tail escape (lower probability gate but attempt uses at most 1 eval)\n                if (not improved_this_center) and work_allow > 0 and self.rng.rand() < self.cauchy_prob:\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -8, 8)\n                    scale = self.cauchy_scale_frac * rng_range\n                    x_jump = np.clip(center + jump * scale, lb, ub)\n                    out = safe_eval(x_jump)\n                    work_allow -= 1\n                    if out is not None:\n                        fjump, xjump = out\n                        if fjump < f_best - 1e-12:\n                            improved_global = True\n                            improved_this_center = True\n                            centers[cidx] = xjump.copy()\n                            trust_radius[cidx] = np.minimum(tr * 2.0, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                        else:\n                            # unsuccessful jump aggressively shrinks trust in directions that were tried\n                            trust_radius[cidx] = np.maximum(tr * 0.7, self.trust_min * rng_range)\n\n                # final per-center update if no improvement\n                if not improved_this_center:\n                    stagnation[cidx] += 1\n                    trust_radius[cidx] = np.maximum(tr * self.failure_shrink, self.trust_min * rng_range)\n                else:\n                    # when one center improved, reduce stagnation on others (encourage movement)\n                    for j in range(len(stagnation)):\n                        if j != cidx and stagnation[j] > 0:\n                            stagnation[j] = max(0, stagnation[j] - 1)\n\n                # center replacement if stuck too long\n                if stagnation[cidx] >= self.center_replace_patience:\n                    # prefer picking from far tail of archive to maintain exploration\n                    if len(X) > 0:\n                        X_arr2 = np.asarray(X)\n                        # compute distance from current centers and pick a point that is far from all centers\n                        d_to_centers = np.min([np.linalg.norm(X_arr2 - c, axis=1) for c in centers], axis=0)\n                        # pick among the top far-away points (to re-seed)\n                        cand_idx = np.argsort(d_to_centers)[-max(1, min(50, len(X_arr2))):]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[cidx] = X_arr2[pick].copy()\n                    else:\n                        centers[cidx] = np.array(self.rng.uniform(lb, ub))\n                    trust_radius[cidx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                    stagnation[cidx] = 0\n\n            # END for each center\n\n            # archive pruning to control memory\n            if len(X) > self.max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                # sample a downsampled set to keep diversity among rest\n                if len(rest) > 0:\n                    step = max(1, len(rest) // (self.max_archive - 200))\n                    keep_rest = rest[::step]\n                    keep_idx = np.concatenate([keep_best, keep_rest])\n                else:\n                    keep_idx = keep_best\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasional global refresh of centers from archive bests\n            if len(X) > 0 and (iter_count % 6 == 0):\n                new_centers = pick_diverse_centers(self.ensemble_size)\n                for i in range(min(len(new_centers), len(centers))):\n                    centers[i] = new_centers[i].copy()\n                    trust_radius[i] = np.maximum(trust_radius[i], self.trust_init_frac * rng_range)\n                    stagnation[i] = max(0, stagnation[i] - 1)\n\n        # final return\n        if x_best is None:\n            # fallback to any evaluated point\n            if len(X) > 0:\n                idx = int(np.argmin(F))\n                x_best = np.array(X[idx], dtype=float)\n                f_best = float(F[idx])\n            else:\n                x_best = np.array(rng_mean, dtype=float)\n                f_best = float(np.inf)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 243, in __call__, the following error occurred:\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 20 is different from 2)\nOn line: g_sub = subspace_V.T @ grad", "error": "In the code, line 243, in __call__, the following error occurred:\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 20 is different from 2)\nOn line: g_sub = subspace_V.T @ grad", "parent_ids": "fbec7c73-80af-4533-bf3e-ca16ae12cded", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "20258bc9-0b9d-4ffc-ac89-6867efe873ff", "fitness": 0.34945848023457665, "name": "AdaptiveMultiscaleDirectionalEnsemble", "description": "The algorithm maintains an ensemble of top \"centers\" seeded by a relatively large initial space‑filling sample (init_ratio=0.20, ensemble_size tuned to dim) and periodically refreshes/replace centers from the archive to preserve diversity. Around each center it fits a lightweight weighted linear + diagonal quadratic surrogate (ridge=1e-6, model_neighbor_multiplier=4) and keeps anisotropic per‑dimension trust radii and adaptive gains (gain_init=1.0, gain_min=0.2, gain_max=5.0) so step sizes evolve per coordinate. Candidate generation is multiscale and directional: conservative Gaussian jitters, surrogate minimizers and gradient‑like steps, PCA/eigen‑direction probes, coordinate micro‑tweaks and structured multiscale directional sweeps (direction_scales), with a modest per‑iteration cap (max_eval_per_iter=32). Global exploration and robustness come from tempered heavy‑tailed escapes (Student‑t/Cauchy style, escape_prob=0.12, escape_scale_frac=0.8), adaptive trust expansion/shrink rules (success_expand=1.25, failure_shrink=0.5), center rejuvenation, and archive pruning to control memory.", "code": "import numpy as np\n\nclass AdaptiveMultiscaleDirectionalEnsemble:\n    \"\"\"\n    Adaptive Multiscale Directional Ensemble (AMDE)\n\n    Main ideas (differences vs. the provided algorithm):\n    - Different default parameterization: larger initial sampling ratio, smaller neighbor multiplier,\n      smaller ridge regularization, wider allowable trust maximum, milder expansion on success,\n      stronger shrink on failure, and slightly higher escape probability.\n    - Fits a weighted linear + diagonal quadratic (per-dim) surrogate with lighter ridge (1e-6).\n    - Uses multiscale directional probes, anisotropic Gaussian proposals, and tempered Lévy-like escapes.\n    - Maintains per-dimension adaptive \"gain\" factors that modulate step sizes independently.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # Initialization sizing (changed)\n        self.init_ratio = 0.20                # larger initial sampling fraction (was 0.15)\n        self.min_init = max(10, self.dim)     # slightly different\n        self.max_init = min(400, int(0.35 * self.budget))\n\n        # Ensemble & local modeling (different choices)\n        self.ensemble_size = max(4, min(8, self.dim // 2 + 2))  # slightly larger ensemble\n        self.model_neighbor_multiplier = 4   # fewer neighbors than original (was 6)\n        self.ridge = 1e-6                    # lighter regularization than original 1e-4\n\n        # trust region (anisotropic per-dim)\n        self.trust_init_frac = 0.4           # smaller initial trust fraction (was 0.6)\n        self.trust_min = 1e-8\n        self.trust_max_frac = 2.0            # allow larger trusts (was 1.5)\n        self.success_expand = 1.25           # milder expand (was 1.4)\n        self.failure_shrink = 0.5            # stronger shrink (was 0.6)\n\n        # sampling controls\n        self.max_eval_per_iter = 32          # smaller per-iter cap (was 40)\n        self.direction_scales = np.array([0.15, 0.4, 0.9, 1.6])  # different scales\n\n        # tempered heavy-tail escapes (Lévy-like via scaled Student-t/Cauchy)\n        self.escape_prob = 0.12              # more frequent escapes (was 0.08)\n        self.escape_scale_frac = 0.8\n\n        # management\n        self.max_centers = max(3, min(14, self.ensemble_size * 2))\n        self.center_replace_patience = 8     # quicker replacement\n\n        # archive pruning\n        self.max_archive_base = max(1000, 30 * self.dim)\n\n        # per-dimension adaptive gains (modulate step sizes per coordinate)\n        self.gain_min = 0.2\n        self.gain_max = 5.0\n        self.gain_init = 1.0\n\n        # RNG\n        self.rng = np.random.RandomState(self.seed)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        X = []\n        F = []\n        f_best = np.inf\n        x_best = None\n\n        # initial space-filling random draws (uniform jitter)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper: get top distinct centers\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if all(np.linalg.norm(x - c) > 1e-12 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.max_centers, len(X)))\n        if len(centers) == 0:\n            centers = [np.array(self.rng.uniform(lb, ub))]\n\n        # per-center anisotropic trust radii and per-dim gains\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-12) for _ in centers]\n        gains = [np.ones(n) * self.gain_init for _ in centers]\n        stagnation = [0 for _ in centers]\n\n        # safe eval\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # fit weighted linear + diagonal quadratic around center\n        def fit_diagquad(center, neighbors_X, neighbors_F):\n            dx = neighbors_X - center  # m x n\n            m = dx.shape[0]\n            if m < 3:\n                return None\n            dists = np.linalg.norm(dx, axis=1)\n            # weight closer points more, but regularized\n            w = 1.0 / (dists + 1e-9)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            # design: [1, dx_i (n), 0.5 dx_i^2 (n)] -> linear + diagonal quad\n            A = np.hstack([np.ones((m,1)), dx, 0.5 * (dx**2)])\n            try:\n                # ridge on parameters\n                ridge = self.ridge * np.eye(A.shape[1])\n                ATA = (W * A).T @ (W * A) + ridge\n                ATy = (W * A).T @ (W * neighbors_F)\n                params = np.linalg.solve(ATA, ATy)\n                params = params.flatten()\n                a0 = float(params[0])\n                lin = params[1:1+n].flatten()\n                quad_diag = params[1+n:1+2*n].flatten()\n                # enforce reasonable curvature (positive)\n                quad_diag = np.sign(quad_diag) * np.maximum(np.abs(quad_diag), 1e-9)\n                return a0, lin, quad_diag\n            except Exception:\n                return None\n\n        iter_count = 0\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            iter_count += 1\n\n            # occasionally refresh centers from archive bests\n            if (iter_count % 9) == 0 and len(X) > 0:\n                topk = min(self.max_centers, max(1, len(X)//10))\n                new_centers = get_top_centers(topk)\n                merged = []\n                for c in (centers + new_centers):\n                    if not any(np.allclose(c, m, atol=1e-12) for m in merged):\n                        merged.append(np.array(c))\n                    if len(merged) >= self.max_centers:\n                        break\n                centers = merged[:self.max_centers]\n                if len(trust_radius) != len(centers):\n                    trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-12) for _ in centers]\n                    gains = [np.ones(n) * self.gain_init for _ in centers]\n                    stagnation = [0 for _ in centers]\n\n            improved_global = False\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            for cidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[cidx])\n                tr = trust_radius[cidx]\n                gain = gains[cidx]\n\n                # gather neighbors\n                X_arr = np.asarray(X) if len(X) > 0 else np.empty((0, n))\n                F_arr = np.asarray(F) if len(F) > 0 else np.empty((0,))\n                enough_data = len(X) >= max(2 * n + 1, self.model_neighbor_multiplier * n)\n\n                if enough_data:\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), max(2 * n + 1, self.model_neighbor_multiplier * n))\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n\n                    local = fit_diagquad(center, X_nei, F_nei)\n                    if local is not None:\n                        a_loc, b_loc, h_loc = local\n                    else:\n                        a_loc, b_loc, h_loc = None, None, None\n\n                    proposals = []\n\n                    # surrogate diagonal quadratic minimizer\n                    if b_loc is not None and h_loc is not None:\n                        # delta = -g/h (per-dim), but scale by gain and clip to trust\n                        delta = - b_loc / (h_loc + 1e-20)\n                        delta = delta * gain\n                        delta = np.clip(delta, -tr, tr)\n                        x_q = np.clip(center + delta, lb, ub)\n                        proposals.append(('diagmin', x_q))\n\n                    # gradient-like step scaled per-dim by gain and trust\n                    if b_loc is not None:\n                        g = b_loc\n                        # step magnitude per-dim\n                        per_step = (tr * 0.7) * (gain / (1.0 + np.abs(g)))\n                        step = -np.sign(g) * np.minimum(np.abs(g) * 0.5, per_step)\n                        x_lin = np.clip(center + step, lb, ub)\n                        proposals.append(('gradish', x_lin))\n\n                    # anisotropic Gaussian around center (three scales)\n                    for sf in (0.2, 0.6, 1.2):\n                        jitter = self.rng.randn(n) * (sf * tr * gain)\n                        xg = np.clip(center + jitter, lb, ub)\n                        proposals.append((f'gauss_{sf}', xg))\n\n                    # directional eigen-like probes: use PCA on neighbors\n                    try:\n                        C = np.cov((X_nei - center).T)\n                        eigvals, eigvecs = np.linalg.eigh(C + 1e-12 * np.eye(n))\n                        idx_e = np.argsort(-eigvals)[:min(3, n)]\n                        for idx_ei in idx_e:\n                            v = eigvecs[:, idx_ei]\n                            # multi-step lengths proportional to sqrt(eigval)\n                            base = np.sqrt(np.abs(eigvals[idx_ei]) + 1e-12)\n                            for s in (0.5, 1.0, 2.0):\n                                step_len = s * base * (np.linalg.norm(tr) / (np.sqrt(n) + 1e-12))\n                                step_vec = v * step_len * gain\n                                proposals.append((f'pc_{idx_ei}_{s}', np.clip(center + step_vec, lb, ub)))\n                    except Exception:\n                        pass\n\n                    # deduplicate proposals\n                    unique = []\n                    names = []\n                    for name, p in proposals:\n                        if not any(np.allclose(p, q, atol=1e-12) for q in unique):\n                            unique.append(p)\n                            names.append(name)\n\n                    # rank proposals: prefer small radius moves (conservative first)\n                    unique.sort(key=lambda z: np.linalg.norm(z - center))\n\n                    # evaluate proposals until improvement or work_allow exhausted\n                    for xprop in unique:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        out = safe_eval(xprop)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        fprop, xprop = out\n                        if fprop < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xprop.copy()\n                            # expand trust and reward gains modestly\n                            trust_radius[cidx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                            gains[cidx] = np.minimum(gain * 1.15, self.gain_max)\n                            stagnation[cidx] = 0\n                            break\n\n                    # if no improvement, multiscale directional probes (randomized)\n                    if (not improved_global) and work_allow > 0 and evals < budget:\n                        num_dirs = min(12, 6 + n // 2)\n                        dirs = []\n                        for _ in range(num_dirs):\n                            v = self.rng.randn(n)\n                            vn = np.linalg.norm(v)\n                            if vn == 0:\n                                v = np.ones(n); vn = np.linalg.norm(v)\n                            dirs.append(v / vn)\n                        self.rng.shuffle(dirs)\n                        base_step = np.maximum(1e-12, np.linalg.norm(tr) / (np.sqrt(float(n)) + 1e-12))\n                        for d in dirs:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            cand = []\n                            for s in self.direction_scales:\n                                step = s * base_step * gain\n                                cand.append(np.clip(center + d * step, lb, ub))\n                                cand.append(np.clip(center - d * step, lb, ub))\n                            self.rng.shuffle(cand)\n                            for x_try in cand:\n                                if work_allow <= 0 or evals >= budget:\n                                    break\n                                # avoid immediate duplicate\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.18, self.trust_max_frac * rng_range)\n                                    gains[cidx] = np.minimum(gain * 1.10, self.gain_max)\n                                    stagnation[cidx] = 0\n                                    break\n                            # end cand\n                        # end dirs\n\n                    # coordinate-wise micro tweaks if still no improvement\n                    if (not improved_global) and work_allow > 0 and evals < budget:\n                        coord_step = np.maximum(tr, 1e-12) * 0.6 * gain\n                        coords = np.arange(n)\n                        max_coords = min(n, max(2, int(work_allow // 2)))\n                        if max_coords < n:\n                            coords = self.rng.choice(coords, size=max_coords, replace=False)\n                        for i in coords:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            for sign in (+1.0, -1.0):\n                                x_try = centers[cidx].copy()\n                                x_try[i] = x_try[i] + sign * coord_step[i]\n                                x_try = np.clip(x_try, lb, ub)\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.12, self.trust_max_frac * rng_range)\n                                    gains[cidx] = np.minimum(gain * 1.08, self.gain_max)\n                                    stagnation[cidx] = 0\n                                    break\n\n                    # tempered Lévy-like escape if still nothing\n                    if (not improved_global) and work_allow > 0 and evals < budget and self.rng.rand() < self.escape_prob:\n                        # use Student-t with df=2 for heavy tails, temper extremes\n                        df = 2.0\n                        # generate t variates via standard normal / sqrt(chi2/df)\n                        z = self.rng.randn(n)\n                        chi2 = np.random.chisquare(df, size=n)\n                        tvar = z / (np.sqrt(chi2 / df) + 1e-12)\n                        scale = self.escape_scale_frac * np.maximum(rng_range, 1e-9)\n                        jump = np.clip(tvar, -6, 6) * scale * gain\n                        x_jump = np.clip(centers[cidx] + jump, lb, ub)\n                        out = safe_eval(x_jump)\n                        if out is not None:\n                            work_allow -= 1\n                            fjump, xjump = out\n                            if fjump < f_best - 1e-12:\n                                improved_global = True\n                                centers[cidx] = xjump.copy()\n                                trust_radius[cidx] = np.minimum(tr * 2.2, self.trust_max_frac * rng_range)\n                                gains[cidx] = np.minimum(gain * 1.5, self.gain_max)\n                                stagnation[cidx] = 0\n                            else:\n                                # unsuccessful -> shrink trust and reduce gains for this center\n                                trust_radius[cidx] = np.maximum(tr * self.failure_shrink, self.trust_min * rng_range)\n                                gains[cidx] = np.maximum(gain * 0.85, self.gain_min)\n\n                    # update trust/stagnation if still no improvement\n                    if not improved_global:\n                        stagnation[cidx] += 1\n                        trust_radius[cidx] = np.maximum(tr * self.failure_shrink, self.trust_min * rng_range)\n                        gains[cidx] = np.maximum(gain * 0.95, self.gain_min)\n                    else:\n                        # mild rejuvenation of other centers\n                        for j in range(len(stagnation)):\n                            if j != cidx and stagnation[j] > 0:\n                                stagnation[j] = max(0, stagnation[j] - 1)\n\n                else:\n                    # insufficient data: exploratory probes with scaled gains\n                    base_step = np.linalg.norm(trust_radius[0]) / max(1.0, np.sqrt(float(n)))\n                    probes = min(8, work_allow)\n                    for _ in range(probes):\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        s = self.rng.choice(self.direction_scales)\n                        x_try = np.clip(center + d * s * base_step * gain, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xtry.copy()\n                            trust_radius[cidx] = np.minimum(trust_radius[cidx] * 1.25, self.trust_max_frac * rng_range)\n                            gains[cidx] = np.minimum(gain * 1.2, self.gain_max)\n                            stagnation[cidx] = 0\n                            break\n\n                # replace center if stagnated\n                if stagnation[cidx] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # choose a candidate far from existing centers (diversify)\n                        d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        cand_idx = np.argsort(d_to_centers)[-max(1, min(30, len(X_arr))):]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[cidx] = X_arr[pick].copy()\n                        trust_radius[cidx] = np.maximum(self.trust_init_frac * rng_range, 1e-12)\n                        gains[cidx] = np.ones(n) * self.gain_init\n                        stagnation[cidx] = 0\n\n            # archive pruning\n            max_archive = max(self.max_archive_base, 25 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # periodic center refresh\n            if len(X) > 0 and (iter_count % 6 == 0):\n                best_centers = get_top_centers(min(len(centers), max(1, len(X)//15)))\n                for i in range(min(len(best_centers), len(centers))):\n                    centers[i] = best_centers[i].copy()\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveMultiscaleDirectionalEnsemble scored 0.349 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "fbec7c73-80af-4533-bf3e-ca16ae12cded", "operator": null, "metadata": {"aucs": [0.12037526359016193, 0.1839525152188145, 0.4841321500049327, 0.8386550595973464, 0.21765458711344576, 0.605781339599313, 0.2269513652052687, 0.42012274445311104, 0.24453364545363565, 0.1524261321097372]}, "task_prompt": ""}
{"id": "7f1408a6-3d25-4e48-bd31-d2dacd374f35", "fitness": 0.38132631823352664, "name": "EMALP", "description": "EMALP is an ensemble-driven continuous optimizer that keeps a small set of centers (typically 3–7) with per-center state (velocity, per-dimension learning rates and anisotropic trust radii) and begins with a modest quasi‑random exploratory budget (init_ratio=0.12) to seed an archive. For local modeling it fits a weighted linear surrogate plus low‑rank directional quadratic residuals (curv_rank ≤ 3) to produce a conservative per‑dimension curvature proxy and a surrogate quadratic minimizer, and uses surrogate-predicted values to rank proposals. Candidate generation mixes conservative quadratic minimizers, momentum/mirror steps toward the global best, pairwise recombination, surrogate‑guided line probes along dominant PCs or gradient directions, and occasional tempered Lévy/Cauchy escapes, while each center adapts trust radii and per-dimension learning rates on success (expand) or failure (shrink) and damps or replaces stagnated centers from diverse archive points. Practical controls enforce bounds and strict budget usage (safe_eval & per-iteration eval cap), archive pruning to bound memory, and periodic refresh of centers from archive bests to maintain diversity.", "code": "import numpy as np\n\nclass EMALP:\n    \"\"\"\n    Ensemble Mirror-Adaptive Low-Rank Predictor (EMALP)\n\n    Key ideas:\n    - Maintain a small ensemble of centers, each with velocity (momentum), anisotropic trust radii and per-dimension learning rates.\n    - Fit a cheap weighted linear model and low-rank directional curvatures from local neighbors;\n      convert directional curvatures into a per-dimension curvature proxy for conservative quadratic minimizers.\n    - Propose candidates from multiple mechanisms: low-rank quadratic minimizer, momentum/mirror steps,\n      pairwise recombination, surrogate-guided line probes along dominant directions, and tempered Lévy escapes.\n    - Adapt trust radii and per-dimension learning rates on success/failure; replace stagnated centers from distant archive points.\n    - Strict safe_eval bounds total budget and clips to problem box [-5,5] (or func.bounds if provided).\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 ensemble_size=None, init_ratio=0.12, max_eval_per_iter=36):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n\n        # ensemble sizing\n        if ensemble_size is None:\n            self.ensemble_size = max(3, min(7, self.dim // 2 + 1))\n        else:\n            self.ensemble_size = int(ensemble_size)\n\n        # initial sampling ratio\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(8, 2 * self.dim)\n        self.max_init = min(500, int(0.35 * self.budget))\n\n        # neighbor for local models (multiplier * dim)\n        self.nei_mult = 5\n\n        # low-rank model rank (directions to estimate curvature)\n        self.curv_rank = min(3, self.dim)\n\n        # trust region per-dim (fractions of range)\n        self.trust_init_frac = 0.6\n        self.trust_min_frac = 1e-6\n        self.trust_max_frac = 1.6\n        self.success_expand = 1.35\n        self.failure_shrink = 0.55\n\n        # per-dim learning-rate adapt\n        self.lr_init_frac = 0.25\n        self.lr_inc = 1.3\n        self.lr_dec = 0.6\n        self.lr_min_frac = 1e-8\n\n        # velocity / mirror parameters\n        self.momentum_decay = 0.7\n        self.mirror_strength = 0.6\n\n        # recombination & escapes\n        self.recomb_prob = 0.12\n        self.levy_prob = 0.06\n        self.levy_scale_frac = 0.8\n        self.levy_clip = 8.0\n\n        # evaluations per loop\n        self.max_eval_per_iter = int(max_eval_per_iter)\n\n        # archive and center management\n        self.max_archive_base = max(1500, 40 * self.dim)\n        self.center_replace_patience = 12\n        self.max_centers = max(3, min(12, self.ensemble_size * 2))\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds handling\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # storage\n        X = []\n        F = []\n        f_best = np.inf\n        x_best = None\n\n        # initial quasi-random-ish exploration (uniform)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper: distinct sorted top centers\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if all(np.linalg.norm(x - c) > 1e-12 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.max_centers, len(X)))\n        if len(centers) == 0:\n            centers = [np.array(self.rng.uniform(lb, ub))]\n\n        # per-center state\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n        lr = [np.maximum(self.lr_init_frac * rng_range, 1e-12) for _ in centers]  # per-dim learning rates\n        velocity = [np.zeros(n) for _ in centers]\n        stagn = [0 for _ in centers]\n\n        # safe evaluation wrapper\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # local low-rank fit: weighted linear + directional quadratic residuals\n        def fit_local_lowrank(center, X_nei, F_nei, rank):\n            # expects neighbors arrays m x n, m\n            dx = X_nei - center  # m x n\n            m = dx.shape[0]\n            if m < 3:\n                return None\n            dists = np.linalg.norm(dx, axis=1) + 1e-12\n            w = 1.0 / dists\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]  # m x 1\n\n            # linear fit: f ~ a + b^T dx\n            A_lin = np.hstack([np.ones((m, 1)), dx])  # m x (n+1)\n            try:\n                # weighted least squares with mild ridge\n                Ar = (W * A_lin)\n                br = (W * F_nei)\n                ridge = 1e-8 * np.eye(A_lin.shape[1])\n                params_lin, *_ = np.linalg.lstsq(Ar.T @ Ar + ridge, Ar.T @ br, rcond=None)\n                a = float(params_lin[0])\n                b = params_lin[1:].flatten()\n            except Exception:\n                return None\n\n            # residuals after linear part\n            resid = F_nei - (a + dx.dot(b))\n\n            # principal directions of dx (SVD)\n            try:\n                # weight dx by sqrt(weights)\n                Dx = (W * dx)\n                U, S, Vt = np.linalg.svd(Dx, full_matrices=False)\n                pcs = Vt.T[:, :max(1, min(rank, Vt.shape[0]))]  # n x r\n            except Exception:\n                pcs = np.eye(n, 1)  # fallback single direction\n\n            # estimate curvature along each pc by regressing residuals onto p and p^2\n            h_dir = np.zeros(pcs.shape[1])\n            for j in range(pcs.shape[1]):\n                v = pcs[:, j]\n                p = dx.dot(v)  # projection scalar per neighbor\n                P = np.vstack([p, 0.5 * (p ** 2)]).T  # m x 2\n                try:\n                    Pr = (W * P)\n                    c, *_ = np.linalg.lstsq(Pr.T @ Pr + 1e-9 * np.eye(2), Pr.T @ (W.flatten() * resid), rcond=None)\n                    # c[1] corresponds to quadratic coefficient ~ 0.5*h -> because we used 0.5*p^2 in column, the extracted coeff is h\n                    h_dir[j] = float(c[1]) if c.shape[0] > 1 else 0.0\n                except Exception:\n                    h_dir[j] = 0.0\n\n            # convert directional curvatures to per-dim approximate curvature (nonnegative)\n            h_per_dim = np.zeros(n)\n            for j in range(pcs.shape[1]):\n                v = pcs[:, j]\n                # distribute directional curvature to dimensions proportionally to squared loadings\n                h_per_dim += np.abs(h_dir[j]) * (v ** 2)\n            # floor curvature to small positive to avoid division instabilities\n            h_per_dim = np.maximum(h_per_dim, 1e-8)\n            return a, b, h_per_dim, pcs, h_dir\n\n        iter_count = 0\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            iter_count += 1\n\n            # refresh centers periodically from archive bests to maintain diversity\n            if (iter_count % 8) == 0 and len(X) > 0:\n                topk = min(self.max_centers, max(1, len(X) // 14))\n                new_centers = get_top_centers(topk)\n                merged = []\n                for c in (centers + new_centers):\n                    if not any(np.allclose(c, m, atol=1e-12) for m in merged):\n                        merged.append(np.array(c))\n                    if len(merged) >= self.max_centers:\n                        break\n                centers = merged[:self.max_centers]\n                # ensure associated arrays length matches\n                if len(trust_radius) != len(centers):\n                    trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n                    lr = [np.maximum(self.lr_init_frac * rng_range, 1e-12) for _ in centers]\n                    velocity = [np.zeros(n) for _ in centers]\n                    stagn = [0 for _ in centers]\n\n            improved_any = False\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            for idx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[idx])\n                tr = np.array(trust_radius[idx])\n                lr_vec = np.array(lr[idx])\n                vel = np.array(velocity[idx])\n\n                # collect neighbors\n                X_arr = np.asarray(X) if len(X) > 0 else np.empty((0, n))\n                F_arr = np.asarray(F) if len(F) > 0 else np.empty((0,))\n                enough_data = len(X) >= max(3 * n, self.nei_mult * n)\n                local_model = None\n                if enough_data:\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), max(3 * n, self.nei_mult * n))\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n                    local_model = fit_local_lowrank(center, X_nei, F_nei, self.curv_rank)\n\n                proposals = []\n                surrogate_scores = []\n\n                # 1) low-rank quadratic minimizer (conservative)\n                if local_model is not None:\n                    a_loc, b_loc, h_loc, pcs, h_dir = local_model\n                    # delta = - b / h (per-dim), clipped by trust\n                    delta = - b_loc / (h_loc + 1e-20)\n                    delta = np.clip(delta, -tr, tr)\n                    x_q = np.clip(center + delta, lb, ub)\n                    proposals.append(x_q)\n                    # surrogate predicted value (conservative)\n                    pred = a_loc + (delta).dot(b_loc) + 0.5 * np.sum(h_loc * (delta ** 2))\n                    surrogate_scores.append(float(pred))\n                else:\n                    surrogate_scores.append(np.inf)\n\n                # 2) momentum (mirror) step: mirror toward global best + velocity\n                if x_best is not None:\n                    mirror = center + self.mirror_strength * (center - x_best)\n                else:\n                    mirror = center.copy()\n                # combine momentum and mirror, with gaussian jitter proportional to tr\n                noise = self.rng.randn(n) * (0.4 * tr)\n                x_m = np.clip(mirror * 0.6 + center * 0.4 + vel * 0.9 + noise, lb, ub)\n                proposals.append(x_m)\n                surrogate_scores.append(np.nan)\n\n                # 3) recombination with another center (biased by fitness)\n                if (len(centers) > 1) and (self.rng.rand() < self.recomb_prob):\n                    # pick partner weighted by inverse fitness (if available in archive)\n                    weights = np.ones(len(centers))\n                    # prefer centers that are not identical and far\n                    dists_cent = np.array([np.linalg.norm(center - np.array(c)) for c in centers])\n                    prob = dists_cent + 1e-9\n                    prob[idx] = 0.0\n                    if prob.sum() > 0:\n                        prob = prob / prob.sum()\n                        j = int(self.rng.choice(len(centers), p=prob))\n                    else:\n                        j = self.rng.randint(len(centers))\n                    partner = np.array(centers[j])\n                    alpha = self.rng.rand()\n                    child = np.clip(alpha * center + (1 - alpha) * partner + self.rng.randn(n) * (0.2 * tr), lb, ub)\n                    proposals.append(child)\n                    surrogate_scores.append(np.nan)\n\n                # 4) surrogate-guided line probes along dominant local PCs or gradient\n                if local_model is not None:\n                    # choose main direction: negative linear term projected to top pc or full gradient\n                    _, b_loc, h_loc, pcs, h_dir = local_model\n                    g = b_loc\n                    if np.linalg.norm(g) > 1e-12:\n                        g_dir = -g / (np.linalg.norm(g) + 1e-12)\n                    else:\n                        g_dir = pcs[:, 0] if pcs.shape[1] > 0 else self.rng.randn(n)\n                        g_dir = g_dir / (np.linalg.norm(g_dir) + 1e-12)\n                    # geometric sequence of steps within trust\n                    base = np.mean(tr)\n                    for s in (0.25, 0.6, 1.0, 1.8):\n                        step = s * base\n                        x1 = np.clip(center + g_dir * step, lb, ub)\n                        x2 = np.clip(center - g_dir * step, lb, ub)\n                        proposals.append(x1); surrogate_scores.append(np.nan)\n                        proposals.append(x2); surrogate_scores.append(np.nan)\n                else:\n                    # fallback random probes near center\n                    for _ in range(3):\n                        z = np.clip(center + self.rng.randn(n) * (0.6 * tr), lb, ub)\n                        proposals.append(z); surrogate_scores.append(np.nan)\n\n                # 5) temperate Lévy/Cauchy jump occasionally\n                if self.rng.rand() < self.levy_prob:\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -self.levy_clip, self.levy_clip)\n                    scale = self.levy_scale_frac * np.maximum(rng_range, 1e-9)\n                    x_jump = np.clip(center + jump * scale, lb, ub)\n                    proposals.append(x_jump); surrogate_scores.append(np.nan)\n\n                # deduplicate proposals\n                unique = []\n                uniq_scores = []\n                for i, p in enumerate(proposals):\n                    if not any(np.allclose(p, q, atol=1e-12) for q in unique):\n                        unique.append(p)\n                        uniq_scores.append(surrogate_scores[i] if i < len(surrogate_scores) else np.nan)\n\n                # rank proposals: prefer surrogate predicted low first, then small moves (conservative)\n                order_props = list(range(len(unique)))\n                # use surrogate rank for those with finite predictions\n                finite_preds = [i for i, s in enumerate(uniq_scores) if not (s is None or np.isnan(s))]\n                nonfinite = [i for i in order_props if i not in finite_preds]\n                finite_preds.sort(key=lambda i: uniq_scores[i])\n                nonfinite.sort(key=lambda i: np.linalg.norm(unique[i] - center))\n                ranked = finite_preds + nonfinite\n\n                # evaluate proposals until success or work_allow exhausted\n                improved_local = False\n                for ridx in ranked:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    xprop = unique[ridx]\n                    out = safe_eval(xprop)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    fprop, xprop = out\n                    if fprop < f_best - 1e-12:\n                        # success: update this center\n                        improved_any = True\n                        improved_local = True\n                        # update velocity (mirror-like)\n                        new_vel = self.momentum_decay * vel + 0.8 * (xprop - center)\n                        velocity[idx] = new_vel\n                        # update center and states\n                        centers[idx] = xprop.copy()\n                        # expand trust and learning rates modestly\n                        trust_radius[idx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                        lr[idx] = np.minimum(np.array(lr_vec) * self.lr_inc, self.trust_max_frac * rng_range)\n                        stagn[idx] = 0\n                        break\n\n                # if no success, try a short geometric line search along best direction found by local model\n                if (not improved_local) and work_allow > 0 and local_model is not None:\n                    # pick direction: negative gradient direction or top pc\n                    _, b_loc, _, pcs, _ = local_model\n                    if np.linalg.norm(b_loc) > 1e-12:\n                        dir0 = -b_loc / (np.linalg.norm(b_loc) + 1e-12)\n                    else:\n                        dir0 = pcs[:, 0] if pcs.shape[1] > 0 else self.rng.randn(n)\n                        dir0 = dir0 / (np.linalg.norm(dir0) + 1e-12)\n                    # try a small set of step sizes informed by lr (per-dim average)\n                    avg_lr = np.mean(np.maximum(lr_vec, 1e-12))\n                    steps = [0.5, 0.9, 1.5, 2.4]\n                    for s in steps:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        step = s * avg_lr\n                        x_try = np.clip(center + dir0 * step, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_any = True\n                            improved_local = True\n                            centers[idx] = xtry.copy()\n                            velocity[idx] = self.momentum_decay * vel + 0.7 * (xtry - center)\n                            trust_radius[idx] = np.minimum(tr * 1.25, self.trust_max_frac * rng_range)\n                            lr[idx] = np.minimum(np.array(lr_vec) * self.lr_inc, self.trust_max_frac * rng_range)\n                            stagn[idx] = 0\n                            break\n\n                # if still no improvement: shrink trust and learning rates\n                if not improved_local:\n                    stagn[idx] += 1\n                    trust_radius[idx] = np.maximum(tr * self.failure_shrink, self.trust_min_frac * rng_range)\n                    lr[idx] = np.maximum(np.array(lr_vec) * self.lr_dec, self.lr_min_frac * rng_range)\n                    # damp velocity\n                    velocity[idx] = self.momentum_decay * vel\n\n                # replace stagnated center\n                if stagn[idx] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        # choose candidate far from any center to encourage exploration\n                        X_arr = np.asarray(X)\n                        # distance to nearest center\n                        d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        # pick from the top-farthest subset\n                        topk = max(1, min(40, len(X_arr)))\n                        cand_idx = np.argsort(d_to_centers)[-topk:]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[idx] = X_arr[pick].copy()\n                        trust_radius[idx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                        lr[idx] = np.maximum(self.lr_init_frac * rng_range, 1e-12)\n                        velocity[idx] = np.zeros(n)\n                        stagn[idx] = 0\n\n            # archive pruning to keep memory bounded\n            max_archive = max(self.max_archive_base, 20 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                # spread-sample the rest to fill archive\n                if len(rest_idx) > 0:\n                    step = max(1, len(rest_idx) // (max_archive - 200))\n                    keep_rest = rest_idx[::step]\n                else:\n                    keep_rest = np.array([], dtype=int)\n                keep_idx = np.concatenate([keep_best, keep_rest])[:max_archive]\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasional center refresh from top archive\n            if len(X) > 0 and (iter_count % 6 == 0):\n                best_centers = get_top_centers(min(len(centers), max(1, len(X) // 12)))\n                for i in range(min(len(best_centers), len(centers))):\n                    centers[i] = best_centers[i].copy()\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm EMALP scored 0.381 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "fbec7c73-80af-4533-bf3e-ca16ae12cded", "operator": null, "metadata": {"aucs": [0.14182883297679438, 0.17720208853729758, 0.4817002617779733, 0.6284714799970952, 0.48261829109257215, 0.5670593192717039, 0.2352844590474772, 0.4382982773616535, 0.4920390866436748, 0.16876108562902414]}, "task_prompt": ""}
{"id": "092584f9-5c85-4b16-863b-0bc1a03325a3", "fitness": 0.3124790213963412, "name": "MultiScaleCorrelatedEnsemble", "description": "The algorithm maintains a modest ensemble of candidate centers with per-dimension (anisotropic) trust radii and a larger-than-usual space‑filling initialization (init_ratio=0.20, min_init=max(20,4*dim)) that seeds an archive of evaluated points used to pick and refresh centers. For local modeling it fits a cheap weighted linear plus separable quadratic surrogate with ridge regularization (ridge=1e-3, model_neighbor_multiplier=4) and uses local covariance/eigenvectors to extract promising principal directions. Candidate generation is multi-scale and diversified: separable-quadratic minimizers, correlated multivariate Gaussian jitter (covariance blend with trust diag), momentum-like pushes from recent successful displacements, eigen-direction line probes, and occasional tempered Cauchy long jumps (cauchy_prob=0.12, cauchy_scale_frac=1.0), with proposals ranked conservatively and evaluated under a per-iteration cap (max_eval_per_iter=30) and strict budget enforcement. Adaptation and management emphasize aggressive trust updates (trust_init_frac=0.4, success_expand=1.6, failure_shrink=0.5), stagnation-driven center replacement, and archive pruning/refresh to maintain diversity and robustness.", "code": "import numpy as np\n\nclass MultiScaleCorrelatedEnsemble:\n    \"\"\"\n    Multi-Scale Correlated Ensemble Search (MSCES)\n\n    Main idea:\n      - Maintain a modest ensemble of centers with per-dimension anisotropic trust radii.\n      - Fit a cheap weighted linear + separable quadratic model locally (ridge regularized).\n      - Propose candidates using:\n         * separable-quadratic minimizers,\n         * correlated multivariate Gaussian jitter (uses local covariance + trust diag),\n         * momentum-like steps that push centers along recent successful displacements,\n         * principal-direction multiscale probing (line search along eigendirections),\n         * tempered Cauchy long jumps.\n      - Adapt trust radii more aggressively on success and failure and replace stagnated centers\n        by distant archive points to maintain exploration.\n      - Practical safeguards: evaluation budget enforcement, archive pruning, RNG seed.\n\n    Key parameters (different choices from the earlier algorithm):\n      - init_ratio = 0.20 (was 0.15)\n      - min_init = max(20, 4*dim) (larger initial sampling)\n      - ensemble_size = min(8, max(3, dim//1 + 1)) (slightly larger for many dims)\n      - model_neighbor_multiplier = 4 (neighbors = 4 * dim; different from 6)\n      - ridge = 1e-3 (stronger regularization)\n      - trust_init_frac = 0.4 (smaller initial trust)\n      - success_expand = 1.6 (more aggressive expand)\n      - failure_shrink = 0.5 (stronger shrink)\n      - cauchy_prob = 0.12 (more frequent heavy-tail escapes)\n      - cauchy_scale_frac = 1.0 (larger jump scale)\n      - max_eval_per_iter = 30 (per-iteration cap)\n      - max_archive_base = max(2000, 50*dim) (larger archive)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n\n        # initialization sizing\n        self.init_ratio = 0.20\n        self.min_init = max(20, 4 * self.dim)\n        self.max_init = min(400, int(0.35 * self.budget))\n\n        # ensemble & local modeling\n        self.ensemble_size = min(8, max(3, self.dim // 1 + 1))\n        self.model_neighbor_multiplier = 4\n        self.ridge = 1e-3\n\n        # trust region (anisotropic per-dimension)\n        self.trust_init_frac = 0.4\n        self.trust_min = 1e-7\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.6\n        self.failure_shrink = 0.5\n\n        # sampling controls\n        self.max_eval_per_iter = 30\n        self.direction_scales = np.array([0.15, 0.4, 0.8, 1.6])\n\n        # tempered heavy-tail jumps\n        self.cauchy_prob = 0.12\n        self.cauchy_scale_frac = 1.0\n\n        # management\n        self.max_centers = max(3, min(12, self.ensemble_size * 2))\n        self.center_replace_patience = max(8, 2 + self.dim // 4)\n\n        # archive pruning\n        self.max_archive_base = max(2000, 50 * self.dim)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # determine bounds from func if available\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        rng_range = ub - lb\n        rng_center = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        X = []  # archive points (list of arrays)\n        F = []  # archive values (list of floats)\n        f_best = np.inf\n        x_best = None\n\n        # initial space-filling random draws\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper: pick distinct top centers\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if all(np.linalg.norm(x - c) > 1e-12 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.max_centers, len(X)))\n        if len(centers) == 0:\n            centers = [np.array(self.rng.uniform(lb, ub))]\n\n        # per-center anisotropic trust radii & momentum/history\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-12) for _ in centers]\n        stagnation = [0 for _ in centers]\n        last_disp = [np.zeros(n) for _ in centers]  # recent successful displacement (momentum)\n\n        # safe evaluation wrapper\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(np.asarray(x, dtype=float), lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # local fit: weighted linear + separable quadratic, but with stronger ridge\n        def fit_local_linear_and_sepquad(center, neighbors_X, neighbors_F):\n            dx = neighbors_X - center  # m x n\n            m = dx.shape[0]\n            if m < 3:\n                return None\n            dists = np.linalg.norm(dx, axis=1) + 1e-12\n            w = 1.0 / (dists)  # inverse-distance weighting\n            w = w / np.max(w)\n            W = np.sqrt(w)[:, None]\n\n            # linear: A_lin = [1, dx]\n            A_lin = np.hstack([np.ones((m,1)), dx])\n            try:\n                params_lin, *_ = np.linalg.lstsq(W * A_lin, W * neighbors_F, rcond=None)\n                a_lin = float(params_lin[0])\n                b_lin = params_lin[1:].flatten()\n            except Exception:\n                return None\n\n            # separable quadratic: include 0.5 * h_i * dx_i^2 columns\n            A_q = np.hstack([np.ones((m,1)), dx, 0.5 * (dx**2)])\n            try:\n                # solve with ridge on normal equations to stabilize\n                AtW = (W * A_q).T @ (W * A_q)\n                rhs = (W * A_q).T @ (W * neighbors_F)\n                ridge = self.ridge * np.eye(AtW.shape[0])\n                params_q = np.linalg.solve(AtW + ridge, rhs)\n                params_q = params_q.flatten()\n                # extract h diagonal (last n columns)\n                h_diag = params_q[1 + n: 1 + 2 * n]\n                # enforce positive minimal curvature\n                h_diag = np.maximum(h_diag, 1e-6)\n            except Exception:\n                h_diag = np.ones(n) * 1e-4\n            return a_lin, b_lin, h_diag\n\n        iter_count = 0\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            iter_count += 1\n\n            # occasional refresh of centers from archive (keep top)\n            if (iter_count % 6) == 0 and len(X) > 0:\n                topk = min(self.max_centers, max(1, len(X)//10))\n                new_centers = get_top_centers(topk)\n                merged = []\n                for c in (centers + new_centers):\n                    if not any(np.allclose(c, m, atol=1e-12) for m in merged):\n                        merged.append(np.array(c))\n                    if len(merged) >= self.max_centers:\n                        break\n                centers = merged[:self.max_centers]\n                if len(trust_radius) != len(centers):\n                    trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-12) for _ in centers]\n                    stagnation = [0 for _ in centers]\n                    last_disp = [np.zeros(n) for _ in centers]\n\n            improved_global = False\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            for cidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[cidx])\n                tr = trust_radius[cidx]\n\n                # determine if enough archive points for local modeling\n                enough_data = len(X) >= max(3 * n + 1, self.model_neighbor_multiplier * n)\n                if enough_data:\n                    X_arr = np.asarray(X)\n                    F_arr = np.asarray(F)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), max(3 * n + 1, self.model_neighbor_multiplier * n))\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n\n                    local = fit_local_linear_and_sepquad(center, X_nei, F_nei)\n                    if local is not None:\n                        a_loc, b_loc, h_loc = local\n                    else:\n                        a_loc = b_loc = h_loc = None\n\n                    proposals = []\n\n                    # separable quadratic minimizer (regularized)\n                    if b_loc is not None and h_loc is not None:\n                        delta = - b_loc / (h_loc + 1e-12)\n                        # Regularize step: shrink by factor depending on confidence (neighbor spread)\n                        neigh_spread = np.maximum(np.std(X_nei - center, axis=0), 1e-12)\n                        shrink = 1.0 / (1.0 + np.mean(neigh_spread / (tr + 1e-12)))\n                        delta = delta * np.clip(shrink, 0.25, 1.0)\n                        delta = np.clip(delta, -tr, tr)\n                        x_q = np.clip(center + delta, lb, ub)\n                        proposals.append(('sepquad', x_q))\n\n                    # correlated multivariate Gaussian jitter (use local cov + trust diag)\n                    try:\n                        cov_local = np.cov((X_nei - center).T)\n                        # blend with diag(tr^2) for anisotropic scaling\n                        blend = 0.5\n                        cov_tr_diag = np.diag((tr * 0.8) ** 2)\n                        cov = blend * cov_local + (1 - blend) * cov_tr_diag + 1e-12 * np.eye(n)\n                        L = np.linalg.cholesky(cov)\n                        for scale in (0.35, 0.9, 1.6):\n                            z = self.rng.randn(n)\n                            jitter = (L @ z) * scale\n                            xg = np.clip(center + jitter, lb, ub)\n                            proposals.append((f'corr_gauss_{scale}', xg))\n                    except Exception:\n                        # fallback to independent per-dim jitter\n                        for scale in (0.35, 0.9):\n                            jitter = self.rng.randn(n) * (tr * scale)\n                            proposals.append((f'gauss_fallback_{scale}', np.clip(center + jitter, lb, ub)))\n\n                    # momentum-like step: push along last successful displacement (if any)\n                    disp = last_disp[cidx]\n                    if np.linalg.norm(disp) > 1e-12:\n                        step = 0.9 * disp\n                        step = np.clip(step, -tr, tr)\n                        proposals.append(('momentum', np.clip(center + step, lb, ub)))\n\n                    # principal-direction multiscale line probes (use eigenvectors of local covariance)\n                    try:\n                        C = np.cov((X_nei - center).T)\n                        eigvals, eigvecs = np.linalg.eigh(C + 1e-12 * np.eye(n))\n                        # pick top 3 directions\n                        idx_e = np.argsort(-eigvals)[:min(3, n)]\n                        for idx_ei in idx_e:\n                            v = eigvecs[:, idx_ei]\n                            # probe multiple scales along +/- direction\n                            for s in (0.25, 0.6, 1.2):\n                                step_scale = s * (tr * (np.sqrt(max(eigvals[idx_ei], 1e-12)) / (np.sqrt(np.max(np.abs(eigvals)) + 1e-12) + 1e-12)))\n                                step_vec = v * step_scale\n                                proposals.append((f'eig_probe_{idx_ei}_{s}_p', np.clip(center + step_vec, lb, ub)))\n                                proposals.append((f'eig_probe_{idx_ei}_{s}_m', np.clip(center - step_vec, lb, ub)))\n                    except Exception:\n                        pass\n\n                    # rank proposals conservatively by distance to center (prefer small)\n                    # deduplicate\n                    unique_props = []\n                    prop_names = []\n                    for name, p in proposals:\n                        if not any(np.allclose(p, q, atol=1e-12) for q in unique_props):\n                            unique_props.append(p); prop_names.append(name)\n                    unique_props.sort(key=lambda z: np.linalg.norm(z - center))\n\n                    # evaluate proposals until improvement or out of work_allow\n                    for pidx, xprop in enumerate(unique_props):\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        # avoid re-evaluating identical last point\n                        if len(X) > 0 and np.allclose(xprop, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(xprop)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        fprop, xprop = out\n                        if fprop < f_best - 1e-12:\n                            # accept improvement\n                            improved_global = True\n                            disp_vec = xprop - center\n                            centers[cidx] = xprop.copy()\n                            last_disp[cidx] = 0.8 * disp_vec + 0.2 * last_disp[cidx]\n                            trust_radius[cidx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                    # if no improvement, attempt short multiscale directional line-search (few alphas)\n                    if (not improved_global) and work_allow > 0 and evals < budget:\n                        # pick a few promising directions from top candidates (or random)\n                        top_dirs = []\n                        # use differences of nearby points as candidate directions\n                        diffs = (X_nei - center)\n                        if diffs.shape[0] > 0:\n                            # pick strongest variance directions\n                            sv = np.std(diffs, axis=0)\n                            if np.linalg.norm(sv) > 1e-12:\n                                # generate a handful of normalized directions\n                                for _ in range(min(6, 3 + n // 3)):\n                                    v = self.rng.choice(diffs.shape[0], 1)[0]\n                                    dirv = diffs[v]\n                                    if np.linalg.norm(dirv) > 1e-12:\n                                        top_dirs.append(dirv / (np.linalg.norm(dirv) + 1e-12))\n                        # fill with random if not enough\n                        while len(top_dirs) < min(6, 3 + n // 3):\n                            v = self.rng.randn(n)\n                            top_dirs.append(v / (np.linalg.norm(v) + 1e-12))\n                        # perform small line probes\n                        for d in top_dirs:\n                            if work_allow <= 0 or evals >= budget or improved_global:\n                                break\n                            scales = [0.2, 0.6, 1.1]\n                            for alpha in scales:\n                                if work_allow <= 0 or evals >= budget or improved_global:\n                                    break\n                                step = alpha * (np.linalg.norm(tr) / np.sqrt(n + 1e-12))\n                                x_try = np.clip(center + d * step, lb, ub)\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    disp_vec = xtry - center\n                                    centers[cidx] = xtry.copy()\n                                    last_disp[cidx] = 0.7 * disp_vec + 0.3 * last_disp[cidx]\n                                    trust_radius[cidx] = np.minimum(tr * (1.25), self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n\n                    # tempered Cauchy escape if still nothing\n                    if (not improved_global) and work_allow > 0 and evals < budget and self.rng.rand() < self.cauchy_prob:\n                        scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                        jump = self.rng.standard_cauchy(size=n)\n                        jump = np.clip(jump, -6, 6)\n                        x_jump = np.clip(center + jump * scale, lb, ub)\n                        if not (len(X) > 0 and np.allclose(x_jump, X[-1], atol=1e-12)):\n                            out = safe_eval(x_jump)\n                            work_allow -= 1\n                            if out is not None:\n                                fjump, xjump = out\n                                if fjump < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xjump.copy()\n                                    last_disp[cidx] = xjump - center\n                                    trust_radius[cidx] = np.minimum(tr * 2.5, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                else:\n                                    # unsuccessful jump strongly shrinks trust\n                                    trust_radius[cidx] = np.maximum(tr * 0.6, self.trust_min * rng_range)\n\n                    # update trust/stagnation if still no improvement for this center\n                    if not improved_global:\n                        stagnation[cidx] += 1\n                        trust_radius[cidx] = np.maximum(tr * self.failure_shrink, self.trust_min * rng_range)\n                    else:\n                        # reduce stagnation counters of others modestly\n                        for j in range(len(stagnation)):\n                            if j != cidx and stagnation[j] > 0:\n                                stagnation[j] = max(0, stagnation[j] - 1)\n\n                else:\n                    # insufficient data: exploratory correlated jitter + directional probes\n                    base_step = np.linalg.norm(trust_radius[cidx]) / np.sqrt(float(n) + 1e-12)\n                    probes = min(8, work_allow)\n                    for _ in range(probes):\n                        v = self.rng.randn(n)\n                        v = v / (np.linalg.norm(v) + 1e-12)\n                        s = self.rng.choice(self.direction_scales)\n                        x_try = np.clip(center + v * s * base_step, lb, ub)\n                        if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xtry.copy()\n                            last_disp[cidx] = xtry - center\n                            trust_radius[cidx] = np.minimum(trust_radius[cidx] * 1.3, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                # replace center if stagnated too long\n                if stagnation[cidx] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # compute distances from archive points to nearest center\n                        d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        # pick among the farthest archived points to increase diversity\n                        cand_idx = np.argsort(d_to_centers)[-max(1, min(40, len(X_arr))):]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[cidx] = X_arr[pick].copy()\n                        trust_radius[cidx] = np.maximum(self.trust_init_frac * rng_range, 1e-12)\n                        stagnation[cidx] = 0\n                        last_disp[cidx] = np.zeros(n)\n\n            # archive pruning to keep size manageable\n            max_archive = max(self.max_archive_base, 30 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                if len(rest_idx) > 0:\n                    keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                    keep_idx = np.concatenate([keep_best, keep_rest])\n                else:\n                    keep_idx = keep_best\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasional small replacement of worst centers by recent bests\n            if len(X) > 0 and (iter_count % 7 == 0):\n                best_centers = get_top_centers(min(len(centers), max(1, len(X)//8)))\n                for i in range(min(len(best_centers), len(centers))):\n                    centers[i] = best_centers[i].copy()\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm MultiScaleCorrelatedEnsemble scored 0.312 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "fbec7c73-80af-4533-bf3e-ca16ae12cded", "operator": null, "metadata": {"aucs": [0.11479529543550115, 0.1584674383453809, 0.40268004950729086, 0.8263821640736655, 0.2193888730976601, 0.48505589670378835, 0.2511183165897888, 0.32065408491432934, 0.19725775431096326, 0.1489903409850436]}, "task_prompt": ""}
{"id": "385a3d1e-5977-4358-ac9d-82065052b7f4", "fitness": 0.35493525347090016, "name": "AES_CGLE", "description": "- Maintains a small ensemble of optimization \"centers\" with per-dimension anisotropic trust radii (trust_init_frac, trust_min, trust_max_frac) seeded by an initial uniform sample (init_ratio, min_init/max_init) so the algorithm explores diverse promising regions.  \n- Around each center it fits a lightweight weighted separable quadratic + linear surrogate (ridge regularization, distance-based weights, neighbor count adaptive via base_neighbor_multiplier*dim) and converts RMSE into a confidence score to decide how much to trust surrogate minima.  \n- Generates a rich set of proposals (surrogate minimizer and gradient-like step, PCA/principal-direction moves, Gaussian jitter, multiscale directional probes, coordinate tweaks, and occasional tempered Cauchy jumps) that are deduplicated and ranked by predicted improvement × confidence before evaluation, while limiting per-iteration evaluations (max_eval_per_iter) and always clipping to box bounds.  \n- Adapts trusts on success/failure (success_expand, failure_shrink), tracks stagnation to replace stuck centers, prunes/refreshes an archive (max_archive_base) to keep memory bounded, and enforces the hard evaluation budget and RNG reproducibility (seed).", "code": "import numpy as np\n\nclass AES_CGLE:\n    \"\"\"\n    Adaptive Ensemble Surrogate with Covariance-Guided Lévy Escapes (AES-CGLE)\n\n    Main idea:\n    - Maintain an ensemble of centers with anisotropic trust radii.\n    - Fit weighted separable-quadratic + linear models around centers and compute a simple model-confidence\n      (RMSE on neighbors) to decide how much to trust surrogate minimizers.\n    - Propose trusted surrogate minimizers, PCA/principal-direction steps, multiscale directional probes,\n      coordinate pattern tweaks, and tempered Cauchy jumps. Adapt trusts by success/failure and replace\n      stagnated centers using diverse archive samples. Always respect evaluation budget and box bounds.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # Initialization sizing\n        self.init_ratio = 0.13\n        self.min_init = max(10, 2 * self.dim)\n        self.max_init = min(400, int(0.35 * self.budget))\n\n        # ensemble settings\n        self.ensemble_size = max(2, min(6, self.dim // 2 + 1))\n        self.max_centers = max(3, min(12, self.ensemble_size * 2))\n        self.center_replace_patience = 9\n\n        # modeling\n        self.base_neighbor_multiplier = 6  # neighbors = multiplier * dim (can adapt)\n        self.ridge = 1e-6\n\n        # trust region (per-dim)\n        self.trust_init_frac = 0.5\n        self.trust_min = 1e-6\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.5\n        self.failure_shrink = 0.68\n\n        # search controls\n        self.max_eval_per_iter = 60\n        self.direction_scales = np.array([0.25, 0.5, 1.0, 2.0])\n        self.cauchy_prob = 0.11\n        self.cauchy_scale_frac = 0.45\n\n        # archive/memory\n        self.max_archive_base = max(2000, 50 * self.dim)\n\n        # RNG\n        self.rng = np.random.RandomState(self.seed)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # bounds (problem statement says bounds are -5..5, but respect func.bounds if present)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        X = []\n        F = []\n\n        f_best = np.inf\n        x_best = None\n\n        # initial samples\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper: pick distinct top centers\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if not any(np.linalg.norm(x - c) < 1e-10 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.max_centers, len(X)))\n        if len(centers) == 0:\n            centers = [np.array(self.rng.uniform(lb, ub))]\n\n        # per-center anisotropic trust radii\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n        stagnation = [0 for _ in centers]\n\n        # safe eval helper\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # fit weighted separable quadratic + linear and compute simple RMSE confidence\n        def fit_models_and_confidence(center, neighbors_X, neighbors_F):\n            # neighbors_* are arrays (m x n)\n            dx = neighbors_X - center\n            m = dx.shape[0]\n            if m < 3:\n                return None\n            # design: constant + linear + 0.5 * diag quadratic\n            M = np.ones((m, 1 + 2 * n))\n            M[:, 1:1 + n] = dx\n            M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n            y = neighbors_F\n            dists = np.linalg.norm(dx, axis=1)\n            w = 1.0 / (dists + 1e-12)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            b = W * y\n            try:\n                ridge = self.ridge * np.eye(M.shape[1])\n                params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ b, rcond=None)\n                params = params.flatten()\n                a = params[0]\n                b_lin = params[1:1 + n]\n                h_diag = params[1 + n:1 + 2 * n]\n                h_diag = np.maximum(h_diag, 1e-8)\n                # compute fitted values and RMSE as confidence proxy\n                yhat = M @ params\n                resid = y - yhat\n                rmse = np.sqrt(np.mean(resid ** 2)) + 1e-12\n                # lower rmse -> higher confidence, we'll map to a [0,1] weight\n                conf = 1.0 / (1.0 + rmse)  # simple monotone mapping\n                return dict(a=a, b=b_lin, h=h_diag, conf=conf)\n            except Exception:\n                return None\n\n        iter_since_refresh = 0\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            iter_since_refresh += 1\n\n            # refresh centers occasionally\n            if iter_since_refresh % 6 == 0 and len(X) > 0:\n                topk = min(self.max_centers, max(1, len(X)//12))\n                new_centers = get_top_centers(topk)\n                # merge preserving uniqueness\n                merged = []\n                for c in (centers + new_centers):\n                    if not any(np.linalg.norm(np.array(c) - np.array(m)) < 1e-10 for m in merged):\n                        merged.append(np.array(c))\n                    if len(merged) >= self.max_centers:\n                        break\n                centers = merged[:self.max_centers]\n                if len(trust_radius) != len(centers):\n                    trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n                    stagnation = [0 for _ in centers]\n\n            improved_global = False\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            # build array forms for neighbors lookup\n            X_arr = np.asarray(X) if len(X) > 0 else np.empty((0, n))\n            F_arr = np.asarray(F) if len(F) > 0 else np.empty((0,))\n\n            for cidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[cidx])\n                tr = trust_radius[cidx]\n\n                # choose neighbor count adaptively (more neighbors as we collect more points)\n                neighbor_mult = self.base_neighbor_multiplier + (len(X) // 500)\n                neighbors_needed = max(2 * n + 1, int(neighbor_mult * n))\n                if len(X) >= neighbors_needed:\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), neighbors_needed)\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n\n                    model = fit_models_and_confidence(center, X_nei, F_nei)\n                    proposals = []\n                    scores = []\n\n                    if model is not None:\n                        # surrogate minimizer (separable quadratic)\n                        delta = - model['b'] / (model['h'] + 1e-20)\n                        delta = np.clip(delta, -tr, tr)\n                        x_q = np.clip(center + delta, lb, ub)\n                        # predicted f value proxy\n                        dx_q = x_q - center\n                        pred_f = model['a'] + model['b'] @ dx_q + 0.5 * np.sum(model['h'] * (dx_q ** 2))\n                        # score: predicted improvement scaled by confidence and closeness\n                        score_q = (np.max([0.0, np.max(F_nei) - pred_f]) + 1e-12) * model['conf'] / (1.0 + np.linalg.norm(dx_q))\n                        proposals.append(x_q); scores.append(score_q)\n\n                        # linear gradient-ish step (scaled by trust and confidence)\n                        g = model['b']\n                        if np.linalg.norm(g) > 0:\n                            step_len = np.minimum(tr, tr * (1.0 / (1.0 + np.linalg.norm(g))))\n                            step = - np.sign(g) * step_len * 0.6\n                            x_lin = np.clip(center + step, lb, ub)\n                            score_lin = model['conf'] * 0.5 / (1.0 + np.linalg.norm(x_lin - center))\n                            proposals.append(x_lin); scores.append(score_lin)\n\n                    # PCA / principal directions proposals\n                    try:\n                        C = np.cov((X_nei - center).T)\n                        if C.shape == (n, n):\n                            eigvals, eigvecs = np.linalg.eigh(C + 1e-12 * np.eye(n))\n                            idx_e = np.argsort(-np.abs(eigvals))[:min(3, n)]\n                            for idx_ei in idx_e:\n                                v = eigvecs[:, idx_ei]\n                                for s in (0.5, 1.0):\n                                    step_vec = v * (s * np.linalg.norm(tr) * (abs(eigvals[idx_ei]) + 1e-12) / (np.max(np.abs(eigvals)) + 1e-12))\n                                    x_e = np.clip(center + step_vec, lb, ub)\n                                    proposals.append(x_e)\n                                    scores.append(0.3 / (1.0 + np.linalg.norm(step_vec)))\n                    except Exception:\n                        pass\n\n                    # Gaussian-mixture jitter proposals (small/medium)\n                    for sf in (0.25, 0.6):\n                        jitter = self.rng.randn(n) * (sf * tr)\n                        xg = np.clip(center + jitter, lb, ub)\n                        proposals.append(xg); scores.append(0.15 / (1.0 + np.linalg.norm(jitter)))\n\n                    # deduplicate proposals (within small tolerance)\n                    uniq = []\n                    uniq_scores = []\n                    for p, s in zip(proposals, scores):\n                        if not any(np.allclose(p, q, atol=1e-12) for q in uniq):\n                            uniq.append(p); uniq_scores.append(s)\n\n                    # sort by descending score (trusty proposals first). If model is absent or low confidence,\n                    # the PCA/gaussian proposals will naturally be earlier due to scores.\n                    order_prop = np.argsort(-np.array(uniq_scores))\n                    for idxp in order_prop:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        xprop = uniq[idxp]\n                        out = safe_eval(xprop)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        fprop, xprop = out\n                        if fprop < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xprop.copy()\n                            trust_radius[cidx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                    # if surrogate/proposals did not help, do multiscale directional probes around center\n                    if (not improved_global) and work_allow > 0 and evals < budget:\n                        num_dirs = int(np.clip(4 + n // 2, 4, 12))\n                        dirs = []\n                        for _ in range(num_dirs):\n                            v = self.rng.randn(n)\n                            nv = np.linalg.norm(v)\n                            if nv == 0:\n                                v = np.ones(n); nv = np.linalg.norm(v)\n                            dirs.append(v / nv)\n                        self.rng.shuffle(dirs)\n                        base_step = np.linalg.norm(tr) / np.sqrt(float(n) + 1e-12)\n                        for d in dirs:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            candidates = []\n                            for s in self.direction_scales:\n                                step = s * base_step\n                                candidates.append(np.clip(center + d * step, lb, ub))\n                                candidates.append(np.clip(center - d * step, lb, ub))\n                            self.rng.shuffle(candidates)\n                            for xt in candidates:\n                                if work_allow <= 0 or evals >= budget:\n                                    break\n                                if len(X) > 0 and np.allclose(xt, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(xt)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.25, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n                            # end candidates\n                        # end dirs\n\n                    # coordinate tweaks if still nothing\n                    if (not improved_global) and work_allow > 0 and evals < budget:\n                        coord_step = np.maximum(tr, 1e-12)\n                        coords = np.arange(n)\n                        max_coords = min(n, max(3, int(work_allow // 2)))\n                        if max_coords < n:\n                            coords = self.rng.choice(coords, size=max_coords, replace=False)\n                        for i in coords:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            for sign in (+1.0, -1.0):\n                                x_try = centers[cidx].copy()\n                                x_try[i] = x_try[i] + sign * coord_step[i]\n                                x_try = np.clip(x_try, lb, ub)\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.2, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n\n                    # occasional tempered Cauchy escape\n                    if (not improved_global) and work_allow > 0 and evals < budget and self.rng.rand() < self.cauchy_prob:\n                        scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                        jump = self.rng.standard_cauchy(size=n)\n                        jump = np.clip(jump, -8, 8)\n                        x_jump = np.clip(centers[cidx] + jump * scale, lb, ub)\n                        out = safe_eval(x_jump)\n                        if out is not None:\n                            work_allow -= 1\n                            fjump, xjump = out\n                            if fjump < f_best - 1e-12:\n                                improved_global = True\n                                centers[cidx] = xjump.copy()\n                                trust_radius[cidx] = np.minimum(tr * 2.0, self.trust_max_frac * rng_range)\n                                stagnation[cidx] = 0\n                            else:\n                                trust_radius[cidx] = np.maximum(tr * 0.82, self.trust_min * rng_range)\n\n                    # update trust/stagnation if still no improvement for this center\n                    if not improved_global:\n                        stagnation[cidx] += 1\n                        trust_radius[cidx] = np.maximum(tr * self.failure_shrink, self.trust_min * rng_range)\n                    else:\n                        # gentle decrease of others' stagnation to maintain adaptability\n                        for j in range(len(stagnation)):\n                            if j != cidx and stagnation[j] > 0:\n                                stagnation[j] = max(0, stagnation[j] - 1)\n\n                else:\n                    # insufficient data: exploratory probes from center\n                    base_step = np.linalg.norm(trust_radius[0]) / np.sqrt(float(n) + 1e-12)\n                    probes = min(6, work_allow)\n                    for _ in range(probes):\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        s = self.rng.choice(self.direction_scales)\n                        x_try = np.clip(center + d * s * base_step, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xtry.copy()\n                            trust_radius[cidx] = np.minimum(trust_radius[cidx] * 1.2, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                # replace center if stagnated\n                if stagnation[cidx] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # distance to nearest center\n                        d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        cand_idx = np.argsort(d_to_centers)[-max(1, min(30, len(X_arr))):]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[cidx] = X_arr[pick].copy()\n                        trust_radius[cidx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                        stagnation[cidx] = 0\n\n            # archive pruning\n            max_archive = max(self.max_archive_base, 20 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasionally refresh centers with best archive points\n            if len(X) > 0 and (iter_since_refresh % 5 == 0):\n                best_centers = get_top_centers(min(len(centers), max(1, len(X)//12)))\n                for i in range(min(len(best_centers), len(centers))):\n                    centers[i] = best_centers[i].copy()\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AES_CGLE scored 0.355 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "fbec7c73-80af-4533-bf3e-ca16ae12cded", "operator": null, "metadata": {"aucs": [0.17218304699222153, 0.19772079021774236, 0.41087692675624854, 0.8032632353876531, 0.3566661831332528, 0.45747483428590885, 0.2678277499241085, 0.3594072088334036, 0.350137106985664, 0.17379545219279857]}, "task_prompt": ""}
{"id": "2edd420d-dcac-45ac-a353-b14464a49d19", "fitness": "-inf", "name": "MemSubspaceSurrogateSearch", "description": "1) The algorithm combines memory-guided low-dimensional random-subspace probing with occasional quadratic surrogates and short 1‑D golden-section line-searches, all executed in a budget-safe, bound-aware loop so func() is never called beyond self.budget.  \n2) It keeps an LRU direction memory (memory_size=8) and builds k≈ceil(sqrt(n)) subspaces reusing recent memory vectors to bias probes (probes≈max(4,2k)), mirrors coefficients to cancel sampling bias, and occasionally perturbs candidates with DE-like archive differences and Cauchy jumps (levy_prob≈0.10, cauchy_scale≈0.5) for heavy‑tailed escapes.  \n3) Every model_every iterations (default 12) it fits a weighted, ridge-regularized quadratic surrogate in a random k‑dim subspace using nearby archive points (requires ~4k+6 samples), stabilizes the Hessian, solves for the subspace minimizer and limits the proposed step by a global scalar trust radius.  \n4) Global control is a single isotropic step size initialized as 0.38*mean_range with multiplicative expand/shrink factors (success_expand=1.25, failure_shrink=0.7) and a tiny ridge (1e-6) for numerical stability, plus archive pruning (max_archive ~ max(2000,50*n)) to keep memory compact.", "code": "import numpy as np\n\nclass MemSubspaceSurrogateSearch:\n    \"\"\"\n    Memory-guided Random-Subspace Surrogate Search (MSSS)\n\n    One-line: Combine mirrored memory-directed low-dim subspace probing and short line-search\n    with cheap random-subspace quadratic surrogates fitted around the current best, adaptive\n    scalar trust/step control, and occasional heavy-tailed escapes — budget-safe and archive-aware.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=8, model_every=12, max_eval_per_iter=60,\n                 subspace_k_frac=None, model_neighbor_multiplier=6,\n                 init_samples=6, levy_prob=0.10, cauchy_scale=0.5,\n                 success_expand=1.25, failure_shrink=0.7, ridge=1e-6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.model_every = int(model_every)\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        # default subspace dimension ~ ceil(sqrt(n)) or fraction if provided\n        self.subspace_k_frac = subspace_k_frac\n        self.model_neighbor_multiplier = int(model_neighbor_multiplier)\n        self.init_samples = int(init_samples)\n        self.levy_prob = float(levy_prob)\n        self.cauchy_scale = float(cauchy_scale)\n        self.success_expand = float(success_expand)\n        self.failure_shrink = float(failure_shrink)\n        self.ridge = float(ridge)\n        # rng\n        self.rng = np.random.default_rng(self.seed)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # bounds (Many BBOB tasks use -5..5)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == (): lb = np.full(n, lb)\n        if ub.shape == (): ub = np.full(n, ub)\n        rng_range = ub - lb\n        mean_range = np.mean(rng_range)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archive\n        X = []\n        F = []\n\n        # initial random space-filling-ish points (small)\n        init_n = max(1, min(self.init_samples, budget//10))\n        for _ in range(init_n):\n            if evals >= budget: break\n            x0 = self.rng.uniform(lb, ub)\n            f0 = float(func(x0))\n            evals += 1\n            X.append(x0.copy()); F.append(f0)\n\n        if len(X) == 0:\n            # ensure at least one evaluation\n            x0 = np.clip(np.zeros(n), lb, ub)\n            f0 = float(func(x0))\n            evals += 1\n            X.append(x0.copy()); F.append(f0)\n\n        # best trackers\n        idx_best = int(np.argmin(F))\n        f_best = float(F[idx_best]); x_best = X[idx_best].copy()\n\n        # directional memory (LRU of unit vectors)\n        dir_memory = []\n\n        # scalar step/trust (isotropic)\n        step = max(1e-12, 0.38 * mean_range)\n        min_step = 1e-9 * max(1.0, mean_range)\n\n        iter_count = 0\n\n        # safe eval helper\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None, None\n            x_c = np.clip(np.asarray(x, dtype=float), lb, ub)\n            try:\n                f = float(func(x_c))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X.append(x_c.copy()); F.append(f)\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # short budget-aware 1D golden-like bracketed search (few evals)\n        def short_line_search(x0, f0, d, init_alpha, max_evals=8):\n            nonlocal evals\n            if evals >= budget:\n                return None, None\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                return None, None\n            d_unit = d / dn\n            remaining = min(max_evals, budget - evals)\n            if remaining < 2:\n                return None, None\n            # initial bracket +/- init_alpha\n            a_val = f0; a = 0.0\n            b = init_alpha\n            xb = np.clip(x0 + b * d_unit, lb, ub)\n            out = safe_eval(xb)\n            if out[0] is None:\n                return None, None\n            fb, xb = out\n            remaining -= 1\n            if fb >= a_val:\n                # try other side\n                b = -init_alpha\n                xb = np.clip(x0 + b * d_unit, lb, ub)\n                out = safe_eval(xb)\n                if out[0] is None:\n                    return None, None\n                fb, xb = out\n                remaining -= 1\n                if fb >= a_val:\n                    return None, None\n            # small golden-section iterations inside [0,b] or [b,0]\n            left, right = min(a, b), max(a, b)\n            gr = (np.sqrt(5) - 1) / 2.0\n            c = right - gr * (right - left)\n            dpt = left + gr * (right - left)\n            xc = np.clip(x0 + c * d_unit, lb, ub)\n            out = safe_eval(xc)\n            if out[0] is None:\n                return None, None\n            fc, xc = out\n            remaining -= 1\n            xd = np.clip(x0 + dpt * d_unit, lb, ub)\n            out = safe_eval(xd)\n            if out[0] is None:\n                return None, None\n            fd, xd = out\n            remaining -= 1\n            best_f = a_val; best_x = x0.copy()\n            if fc < best_f:\n                best_f = fc; best_x = xc.copy()\n            if fd < best_f:\n                best_f = fd; best_x = xd.copy()\n            iters = 0\n            while iters < remaining and abs(right - left) > 1e-12:\n                iters += 1\n                if fc < fd:\n                    right = dpt\n                    dpt = c\n                    fd = fc\n                    c = right - gr * (right - left)\n                    xc = np.clip(x0 + c * d_unit, lb, ub)\n                    out = safe_eval(xc)\n                    if out[0] is None:\n                        break\n                    fc, xc = out\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    left = c\n                    c = dpt\n                    fc = fd\n                    dpt = left + gr * (right - left)\n                    xd = np.clip(x0 + dpt * d_unit, lb, ub)\n                    out = safe_eval(xd)\n                    if out[0] is None:\n                        break\n                    fd, xd = out\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # helper: fit tiny quadratic surrogate in random k-dim subspace around x_best\n        def fit_subspace_quadratic_and_propose(k):\n            nonlocal step\n            if len(X) < 4 * k + 6:\n                return None\n            X_arr = np.asarray(X)\n            F_arr = np.asarray(F)\n            dists = np.linalg.norm(X_arr - x_best, axis=1)\n            idx_sorted = np.argsort(dists)[:min(len(X_arr), max(len(X_arr), self.model_neighbor_multiplier * n, 4 * k + 6))]\n            X_nei = X_arr[idx_sorted]\n            F_nei = F_arr[idx_sorted]\n            # random orthonormal basis Q (n x k)\n            A = self.rng.standard_normal((n, k))\n            Q, _ = np.linalg.qr(A, mode='reduced')\n            Z = (X_nei - x_best) @ Q  # (m,k)\n            m = Z.shape[0]\n            # build quadratic features in subspace: [1, z, 0.5*z^2, cross_terms]\n            cross_count = k * (k - 1) // 2\n            p = 1 + k + k + cross_count\n            Phi = np.ones((m, p))\n            Phi[:, 1:1+k] = Z\n            Phi[:, 1+k:1+2*k] = 0.5 * (Z**2)\n            col = 1 + 2*k\n            for i in range(k):\n                for j in range(i+1, k):\n                    Phi[:, col] = Z[:, i] * Z[:, j]\n                    col += 1\n            y = F_nei\n            # proximity weights (in original space)\n            prox = 1.0 / (dists[idx_sorted] + 1e-12)\n            prox = prox / (np.max(prox) + 1e-12)\n            W = np.sqrt(prox)[:, None]\n            A_mat = W * Phi\n            b_vec = W * y\n            ridge_lambda = self.ridge + 1e-6 * (1.0 / max(1, m))\n            try:\n                lhs = A_mat.T @ A_mat + ridge_lambda * np.eye(p)\n                rhs = A_mat.T @ b_vec\n                params = np.linalg.solve(lhs, rhs).flatten()\n            except Exception:\n                try:\n                    params, *_ = np.linalg.lstsq(A_mat.T @ A_mat + ridge_lambda * np.eye(p), A_mat.T @ b_vec, rcond=None)\n                    params = params.flatten()\n                except Exception:\n                    return None\n            # extract quadratic\n            g_lin = params[1:1+k]\n            h_diag = params[1+k:1+2*k]\n            H = np.zeros((k,k))\n            np.fill_diagonal(H, h_diag)\n            col = 1 + 2*k\n            for i in range(k):\n                for j in range(i+1, k):\n                    c = params[col]\n                    H[i,j] = c; H[j,i] = c\n                    col += 1\n            H = 0.5 * (H + H.T)\n            # stabilize Hessian\n            try:\n                eig = np.linalg.eigvalsh(H)\n                min_eig = np.min(eig)\n            except Exception:\n                min_eig = -1.0\n            if min_eig <= 1e-8:\n                shift = (1e-8 - min_eig) + 1e-6\n                H_reg = H + shift * np.eye(k)\n            else:\n                H_reg = H.copy()\n            try:\n                z_star = -np.linalg.solve(H_reg, g_lin)\n            except Exception:\n                z_star = -np.linalg.pinv(H_reg) @ g_lin\n            # limit by scalar step: ||Q z|| <= step\n            step_vec = Q @ z_star\n            norm_step = np.linalg.norm(step_vec)\n            if norm_step > 0:\n                scale = min(1.0, step / (norm_step + 1e-16))\n            else:\n                scale = 1.0\n            z_limited = z_star * scale\n            x_model = np.clip(x_best + Q @ z_limited, lb, ub)\n            return x_model, Q, z_limited\n\n        # main loop\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            improved_round = False\n\n            # 1) Occasionally attempt surrogate model (every model_every iterations)\n            if (iter_count % self.model_every) == 0 and work_allow > 0:\n                # choose small subspace dimension (k)\n                if self.subspace_k_frac is None:\n                    k = min(max(2, int(np.ceil(np.sqrt(n)))), 8)\n                else:\n                    k = int(np.clip(int(np.ceil(self.subspace_k_frac * n)), 2, min(8, n)))\n                res = fit_subspace_quadratic_and_propose(k)\n                if res is not None:\n                    x_model, Q_model, z_lim = res\n                    out = safe_eval(x_model)\n                    work_allow -= 1\n                    if out[0] is None:\n                        break\n                    f_model, x_model = out\n                    if f_model < f_best - 1e-12:\n                        improved_round = True\n                        # record memory direction\n                        dmem = (x_model - x_best)\n                        dn = np.linalg.norm(dmem)\n                        if dn > 0:\n                            dir_unit = dmem / dn\n                            dir_memory.insert(0, dir_unit.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                        # expand step gently\n                        step = min(step * self.success_expand, 5.0 * mean_range)\n                    else:\n                        # shrink step on surrogate miss\n                        step = max(step * self.failure_shrink, min_step)\n\n            # 2) Mirrored memory-augmented subspace probing (MDARSS-like)\n            # prepare k-dim basis reusing some memory if available\n            k = min(max(1, int(np.ceil(np.sqrt(n)))), n)\n            use_mem = min(len(dir_memory), k//2)\n            basis_cols = []\n            for i in range(use_mem):\n                basis_cols.append(dir_memory[i].copy())\n            needed = k - len(basis_cols)\n            if needed > 0:\n                R = self.rng.standard_normal((n, needed))\n                if basis_cols:\n                    R = np.column_stack((np.column_stack(basis_cols), R))\n                Qfull, _ = np.linalg.qr(R, mode='reduced')\n                basis = Qfull[:, :k]\n            else:\n                Qfull, _ = np.linalg.qr(np.column_stack(basis_cols), mode='reduced')\n                basis = Qfull[:, :k]\n\n            probes = max(4, 2 * k)\n            half = (probes + 1)//2\n            coeff_list = []\n            for _ in range(half):\n                coeff_list.append(self.rng.standard_normal(k))\n            coeff_list = coeff_list + [(-c) for c in coeff_list]\n            coeff_list = coeff_list[:probes]\n            # shuffle probe order\n            self.rng.shuffle(coeff_list)\n\n            # per-iteration candidate evaluation\n            for coeffs in coeff_list:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                d = basis @ coeffs\n                dnrm = np.linalg.norm(d)\n                if dnrm == 0:\n                    continue\n                d = d / (dnrm + 1e-16)\n                # optionally bias by recent coordinate RMS (approx using last memory)\n                if len(dir_memory) > 0:\n                    coord_scale = 1.0 + 0.3 * np.std(np.vstack(dir_memory), axis=0)\n                    d = d * coord_scale\n                    d = d / (np.linalg.norm(d) + 1e-16)\n                alpha = self.rng.uniform(-step, step)\n                x_try = np.clip(x_best + alpha * d, lb, ub)\n\n                # occasional DE-like archive diff (small)\n                if len(X) >= 2 and self.rng.random() < 0.10:\n                    i1, i2 = self.rng.choice(len(X), size=2, replace=False)\n                    de = 0.4 * (X[i1] - X[i2])\n                    x_try = np.clip(x_try + 0.5 * de, lb, ub)\n\n                # occasional memory Cauchy jump\n                if dir_memory and (self.rng.random() < 0.12):\n                    u = dir_memory[self.rng.integers(len(dir_memory))]\n                    jump = np.tan(np.pi * (self.rng.random() - 0.5))\n                    x_try = np.clip(x_try + (self.cauchy_scale * jump * step) * u, lb, ub)\n\n                out = safe_eval(x_try)\n                work_allow -= 1\n                if out[0] is None:\n                    break\n                f_try, x_try = out\n                if f_try < f_best - 1e-12:\n                    # success: accept and attempt short line-search along this direction\n                    dsucc = x_try - x_best\n                    dn = np.linalg.norm(dsucc)\n                    if dn > 0:\n                        dir_unit = dsucc / dn\n                        dir_memory.insert(0, dir_unit.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                        # try short line-search from x_try along dir_unit\n                        if budget - evals >= 2:\n                            ls = short_line_search(x_try, f_try, dir_unit, init_alpha=abs(alpha) if abs(alpha) > 1e-12 else step, max_evals=min(10, budget - evals))\n                            if ls[0] is not None:\n                                f_ls, x_ls = ls\n                                if f_ls < f_try - 1e-12:\n                                    # accept improved result\n                                    x_try = x_ls.copy()\n                                    f_try = f_ls\n                    # update best and step\n                    if f_try < f_best - 1e-12:\n                        f_best = float(f_try); x_best = x_try.copy()\n                    step = min(step * self.success_expand, 5.0 * mean_range)\n                    improved_round = True\n                else:\n                    # maybe do a tiny local line-search occasionally\n                    if self.rng.random() < 0.03 and budget - evals >= 2:\n                        ls = short_line_search(x_best, f_best, d, init_alpha=step, max_evals=min(6, budget - evals))\n                        if ls[0] is not None:\n                            f_ls, x_ls = ls\n                            if f_ls < f_best - 1e-12:\n                                f_best = float(f_ls); x_best = x_ls.copy()\n                                improved_round = True\n                                step = min(step * self.success_expand, 5.0 * mean_range)\n                    # shrink slightly on failure\n                    step = max(step * self.failure_shrink, min_step)\n\n            # 3) occasional heavy-tailed global escape (Cauchy / Lévy style)\n            if (self.rng.random() < self.levy_prob) and (evals < budget):\n                jump = np.tan(np.pi * (self.rng.random(n) - 0.5))\n                scale = self.cauchy_scale * rng_range\n                x_jump = np.clip(x_best + 0.7 * step * jump * (scale / (np.linalg.norm(scale) + 1e-12)), lb, ub)\n                out = safe_eval(x_jump)\n                if out[0] is not None:\n                    work_allow = max(0, work_allow - 1)\n                    fj, xj = out\n                    if fj < f_best - 1e-12:\n                        f_best = float(fj); x_best = xj.copy()\n                        step = min(step * 1.6, 5.0 * mean_range)\n                        dir_mem = xj - x_best\n                        dn = np.linalg.norm(dir_mem)\n                        if dn > 0:\n                            du = dir_mem / dn\n                            dir_memory.insert(0, du.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                    else:\n                        step = max(step * 0.8, min_step)\n\n            # archive pruning to keep memory controlled\n            max_archive = max(2000, 50 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                keep_rest = rest[::max(1, len(rest)//(max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest]) if len(keep_rest) > 0 else keep_best\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # tiny safeguard on step\n            if step < min_step:\n                step = min_step\n\n            # early stop if very small objective achieved\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "fbec7c73-80af-4533-bf3e-ca16ae12cded", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6bb64e62-cbb9-483a-84d3-a097d870e957", "fitness": 0.10940429751590126, "name": "STRIDE", "description": "STRIDE is a multi-patch, hybrid sampler that maintains several local \"patch\" centers and selects which to sample using a UCB-like score (patch visits, improvement reward and patch_ucb_c) to balance exploration and exploitation. Sampling is biased by a compact low-rank directional bank B (rank≈ceil(sqrt(dim)/1.5)) and an anisotropic per-coordinate scale S (RMS-style smoothing with smooth_alpha≈0.15) so candidates are mixtures of per-dim Gaussian steps, low-rank directional moves, occasional DE-like difference mutations (p_de≈0.16) and heavy-tailed Cauchy jumps (p_cauchy≈0.12), with mirrored samples to reduce variance. Successful weighted mean steps update S and incrementally insert/boost normalized directions into B (bank_eta≈0.12) and occasional QR orthonormalization; good samples trigger cheap directional line-refinement probes and archive storage for recombination. Global step-size sigma is adapted multiplicatively from recent success fraction (target≈0.2), patches implement momentum/perturbation on stagnation and stochastic merge/split, while small population size (lambda ~ 6+√dim) and archive limits keep the method lightweight and robust.", "code": "import numpy as np\n\nclass STRIDE:\n    \"\"\"\n    STRIDE: Self-Triggered Directional Ensemble\n\n    Main ideas / novel mechanisms:\n    - Patch ensemble: maintain multiple \"patch centers\" (sub-populations) and choose which to sample from\n      using a UCB-like score that balances recent improvement and exploration (multi-armed bandit).\n    - Directional bank B: compact online low-rank collection of successful normalized directions\n      used to bias samples into promising subspaces (updated via incremental Gram-Schmidt).\n    - Per-coordinate adaptive scale (S) using exponential absolute-step smoothing (RMS-style but on steps),\n      enabling anisotropic sampling without full covariance.\n    - Mixed sampling: a three-way mixture (anisotropic Gaussian through S, directional low-rank moves via B,\n      heavy-tailed Cauchy/Levy jumps for escapes).\n    - Lightweight directional line-refinement: when a candidate yields clear improvement, perform\n      1-3 cheap probes along that direction to exploit the direction.\n    - Archive and DE-like difference mutations for recombination.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_sigma=None, bank_rank=None, lambda_base=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        self.seed = seed\n\n        # population size (small)\n        if lambda_base is None:\n            self.lambda_ = max(6, int(6 + np.sqrt(self.dim)))\n        else:\n            self.lambda_ = int(lambda_base)\n\n        # rank of directional bank\n        if bank_rank is None:\n            self.rank = max(1, int(np.ceil(np.sqrt(self.dim) / 1.5)))\n        else:\n            self.rank = min(max(1, int(bank_rank)), self.dim)\n\n        # initial global step-size\n        if init_sigma is None:\n            self.sigma0 = 0.2 * 10.0\n        else:\n            self.sigma0 = float(init_sigma)\n\n        # exploration parameters\n        self.p_cauchy = 0.12\n        self.cauchy_scale = 1.0\n        self.p_de = 0.16\n        self.F_de = 0.7\n\n        # per-coordinate smoothing\n        self.smooth_alpha = 0.15  # update rate for S\n        self.S_eps = 1e-9\n\n        # directional bank update rate\n        self.bank_eta = 0.12\n\n        # patch parameters\n        self.max_patches = 6\n        self.init_patches = 3\n        self.patch_ucb_c = 1.2  # exploration weight for UCB\n\n        # small line-refinement budget per trigger\n        self.max_line_probes = 3\n\n        # archive limit\n        self.archive_limit = 4000\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (use func.bounds if present else default)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == (): lb = np.full(n, lb)\n            if ub.shape == (): ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        domain_scale = np.mean(ub - lb)\n        sigma = max(1e-12, float(self.sigma0))\n\n        # per-coordinate scale (S) starts small\n        S = np.ones(n) * 0.05 * domain_scale\n\n        # directional bank B (n x r), strengths for each direction\n        r = self.rank\n        B = np.zeros((n, r))\n        B_strength = np.zeros(r)\n\n        # archive\n        archive_X = []\n        archive_F = []\n\n        # patches: each patch is dict with center, best_f, count, score_accum, visits\n        patches = []\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # create initial patches by sampling a few points across the domain\n        initial_patch_count = min(self.init_patches, max(1, budget // 50))\n        for _ in range(initial_patch_count):\n            x0 = np.random.uniform(lb, ub)\n            if evals >= budget: break\n            f0 = func(x0); evals += 1\n            archive_X.append(x0.copy()); archive_F.append(f0)\n            if f0 < f_best:\n                f_best = f0; x_best = x0.copy()\n            patches.append({\n                'center': x0.copy(),\n                'best_f': f0,\n                'visits': 1,\n                'ucb_reward': 0.0,\n                'last_improve': 0\n            })\n\n        # if no patches created (very small budget) create one at center of domain\n        if len(patches) == 0:\n            center = 0.5 * (lb + ub)\n            f0 = func(center); evals += 1\n            archive_X.append(center.copy()); archive_F.append(f0)\n            f_best = f0; x_best = center.copy()\n            patches.append({'center': center.copy(), 'best_f': f0, 'visits': 1, 'ucb_reward': 0.0, 'last_improve':0})\n\n        # ensure archive bounds\n        def push_archive(x, f):\n            archive_X.append(x.copy()); archive_F.append(f)\n            if len(archive_X) > self.archive_limit:\n                archive_X.pop(0); archive_F.pop(0)\n\n        # utility: pick a patch according to UCB\n        def pick_patch():\n            # compute UCB values\n            total_visits = sum(p['visits'] for p in patches) + 1e-12\n            ucb_vals = []\n            for p in patches:\n                # reward is improvement magnitude inverse of best_f (smaller is better)\n                impro = max(1e-12, (max(1e-12, 1.0 / (p['best_f'] + 1e-12))))\n                bonus = self.patch_ucb_c * np.sqrt(np.log(total_visits) / (1 + p['visits']))\n                ucb_vals.append(impro + bonus)\n            idx = int(np.argmax(ucb_vals))\n            patches[idx]['visits'] += 1\n            return idx\n\n        # ensure at least one direction in B by seeding with small random orthonormal vectors\n        rand = np.random.randn(n, r)\n        Q, _ = np.linalg.qr(rand)\n        B[:, :] = Q[:, :r] * 1e-3\n        B_strength[:] = 1e-6\n\n        # main loop: sample populations from selected patches\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n\n            # dynamically allow new patch creation occasionally\n            if (np.random.rand() < 0.07) and (len(patches) < self.max_patches) and (evals + 1 < budget):\n                # create a new patch by sampling a random point\n                xr = np.random.uniform(lb, ub)\n                fr = func(xr); evals += 1\n                push_archive(xr, fr)\n                patches.append({'center': xr.copy(), 'best_f': fr, 'visits':1, 'ucb_reward':0.0, 'last_improve':0})\n                if fr < f_best: f_best = fr; x_best = xr.copy()\n                # continue to next generation\n\n            # select patch to sample from\n            pidx = pick_patch()\n            patch = patches[pidx]\n            center = patch['center'].copy()\n\n            # sampling mixture coefficients (adaptive): increase directional bias if bank has strong directions\n            total_strength = np.sum(B_strength) + 1e-12\n            dir_bias = float(np.clip(0.2 + 0.6 * (total_strength / (total_strength + 1.0)), 0.05, 0.95))\n            # also allow random local exploration factor\n            rand_bias = 1.0 - dir_bias\n\n            arx = np.zeros((lam, n))\n            arz = np.zeros((lam, n))\n            arfit = np.full(lam, np.inf)\n\n            base_z = np.random.randn(lam, n)\n            # small per-dim noise scale derived from S\n            per_dim_scale = S + self.S_eps\n\n            for i in range(lam):\n                z = base_z[i].copy()\n                # anisotropic Gaussian through per-dim scale\n                y_gauss = z * per_dim_scale\n\n                # directional low-rank component: sample r coords from standard normal and project into B\n                zr = np.random.randn(r)\n                y_dir = np.dot(B, zr)  # linear combination of bank directions\n\n                # combine\n                y = rand_bias * y_gauss + dir_bias * y_dir\n\n                # occasional DE-like mutation: differences from archive points\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    a, b = np.random.choice(len(archive_X), size=2, replace=False)\n                    diff = archive_X[a] - archive_X[b]\n                    # scale differential by F_de and mask\n                    mask = (np.random.rand(n) < 0.5)\n                    y[mask] += self.F_de * diff[mask] / (domain_scale + 1e-12)\n\n                # occasional heavy-tailed jump\n                if np.random.rand() < self.p_cauchy:\n                    rdraw = np.random.standard_cauchy() * self.cauchy_scale\n                    zd = np.random.randn(n)\n                    zd = zd / (np.linalg.norm(zd) + 1e-12)\n                    y = rdraw * zd * np.mean(per_dim_scale)\n\n                # mirrored sampling to reduce variance\n                if (i % 2 == 1):\n                    y = -y\n\n                x = center + sigma * y\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates\n            for i in range(lam):\n                if evals >= budget: break\n                x = arx[i]\n                f = func(x); evals += 1\n                arfit[i] = f\n                push_archive(x, f)\n                if f < f_best:\n                    f_best = f; x_best = x.copy()\n                    # when we find a global improvement, perform a tiny local exploitation along the successful y\n                    # choose up to max_line_probes evaluations if budget allows\n                    if evals < budget:\n                        # direction is from center -> x\n                        d = x - center\n                        nd = np.linalg.norm(d)\n                        if nd > 1e-12:\n                            d_unit = d / nd\n                            # try short positive and negative steps along this direction\n                            probes = min(self.max_line_probes, budget - evals)\n                            for t in range(probes):\n                                alpha = 0.5 ** (t + 1)\n                                x_try = np.clip(x + alpha * nd * d_unit * 0.6, lb, ub)\n                                f_try = func(x_try); evals += 1\n                                push_archive(x_try, f_try)\n                                if f_try < f_best:\n                                    f_best = f_try; x_best = x_try.copy()\n                                    # promote this as new center immediately\n                                    x = x_try.copy()\n                # update patch best_f if improved\n                if f < patch['best_f']:\n                    patch['best_f'] = f\n                    patch['last_improve'] = evals\n                    # move center slightly toward found good sample (weighted)\n                    patch['center'] = 0.6 * patch['center'] + 0.4 * x\n\n            # selection: pick top mu by fitness\n            idx = np.argsort(arfit)\n            mu = max(1, lam // 2)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n            f_sel = arfit[sel]\n\n            # recombine center of the patch from selected candidates (weighted by rank)\n            ranks = np.arange(1, mu + 1)\n            weights = (mu + 1 - ranks).astype(float)\n            weights /= np.sum(weights)\n            new_center = np.sum(weights[:, None] * x_sel, axis=0)\n            patch['center'] = np.clip(0.8 * patch['center'] + 0.2 * new_center, lb, ub)\n\n            # compute weighted mean step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # update per-dim scale S using smoothed absolute step magnitude (RMS-like)\n            absstep = np.abs(y_w)\n            S = (1.0 - self.smooth_alpha) * S + self.smooth_alpha * (absstep * np.mean(S))\n            S = np.maximum(S, 1e-12)\n\n            # update directional bank B with normalized y_w as signal (if it's meaningful)\n            signal = y_w.copy()\n            snorm = np.linalg.norm(signal)\n            if snorm > 1e-12:\n                s = signal / (snorm + 1e-12)\n                # incremental update: try to insert/boost direction similar to s\n                # compute projections on existing bank\n                projs = np.abs(B.T @ s)  # similarity\n                # find least used direction index to replace or augment\n                replace_idx = int(np.argmin(B_strength))\n                # if s is similar to some existing direction, boost that one instead\n                best_sim_idx = int(np.argmax(projs))\n                if projs[best_sim_idx] > 0.85 and B_strength[best_sim_idx] > 0.0:\n                    # boost existing\n                    B_strength[best_sim_idx] = (1.0 - self.bank_eta) * B_strength[best_sim_idx] + self.bank_eta * (snorm + 1e-12)\n                    # nudge the basis vector slightly toward s\n                    u = B[:, best_sim_idx] + self.bank_eta * (s - (B[:, best_sim_idx] @ s) * B[:, best_sim_idx])\n                    u = u / (np.linalg.norm(u) + 1e-12)\n                    B[:, best_sim_idx] = u\n                else:\n                    # replace weakest direction with s\n                    B[:, replace_idx] = s.copy()\n                    B_strength[replace_idx] = (1.0 - self.bank_eta) * B_strength[replace_idx] + self.bank_eta * snorm\n\n                # occasionally orthonormalize bank to keep numerical stability\n                if np.random.rand() < 0.15:\n                    try:\n                        Q, _ = np.linalg.qr(B)\n                        B[:, :] = Q[:, :r]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # adapt sigma multiplicatively based on success fraction in selected set\n            # success defined as improvement vs patch best\n            success_frac = np.mean((f_sel < patch['best_f']).astype(float))\n            # smooth multiplicative update: wants success ~ 0.2\n            sigma *= np.exp(0.4 * (success_frac - 0.2))\n            sigma = float(np.clip(sigma, 1e-12, 100.0 * domain_scale))\n\n            # small patch-level momentum: if patch has not improved for long, push it away (encourage exploration)\n            stagn = evals - patch.get('last_improve', 0)\n            if stagn > max(50, 5 * n) and np.random.rand() < 0.25:\n                perturb = np.random.randn(n) * (0.8 * sigma * np.mean(S))\n                patch['center'] = np.clip(patch['center'] + perturb, lb, ub)\n\n            # occasionally merge close patches or split a prolific patch into two\n            if (np.random.rand() < 0.05) and (len(patches) > 1):\n                # merge closest pair if they are very close\n                dists = np.array([[np.linalg.norm(patches[i]['center'] - patches[j]['center']) for j in range(len(patches))] for i in range(len(patches))])\n                i, j = np.unravel_index(np.argmin(dists + np.eye(len(patches)) * 1e9), dists.shape)\n                if dists[i, j] < 0.05 * domain_scale:\n                    # merge j into i\n                    patches[i]['center'] = 0.6 * patches[i]['center'] + 0.4 * patches[j]['center']\n                    patches[i]['best_f'] = min(patches[i]['best_f'], patches[j]['best_f'])\n                    patches[i]['visits'] += patches[j]['visits']\n                    patches.pop(j)\n\n            # maintain archive compactness done in push_archive\n\n        # final return\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm STRIDE scored 0.109 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e0803be8-3b99-4e93-af4c-1f31db4bf8ee", "operator": null, "metadata": {"aucs": [0.05484024720859637, 0.11922397892961145, 0.16385329519562508, 0.08678102170711, 0.07385297159761894, 0.14546305173702145, 0.16164672002935976, 0.11042062841049116, 0.0649453000844894, 0.113015760259089]}, "task_prompt": ""}
{"id": "7001612a-f98b-47b3-b88d-77166bdf3b68", "fitness": 0.41663273730164685, "name": "LAROS", "description": "LAROS mixes per-coordinate adaptive scaling and a learned low-rank proposal subspace: a RMSProp-like accumulator G (rms_beta=0.90, eps=1e-7) yields diagonal scales D=(G+eps)^-0.4 while a small k-dimensional U (k≈2·log(dim)) is learned online with an Oja update (oja_eta=0.15) and occasional re-orthonormalization. Proposals combine the diagonal and subspace components via a logit-controlled blend (alpha stored as alpha_logit, updated additively with alpha_lr=0.12 toward alpha_target=0.30), with mirrored sampling for variance reduction and occasional heavy-tailed Cauchy jumps (p_cauchy=0.10, scale=1.2) and probabilistic DE-style recombination (p_de=0.25, F_de=0.7) using an archive (limit 2000). Mean and step recombination use log-linear weights (mu from lambda≈4+4·log(dim)) producing y_w and a weighted mean m, while momentum (momentum_decay=0.90) carries inertia and G is updated from weighted second moments to adapt per-coordinate variability. Sigma is multiplicatively adapted (sigma_lr=0.8) using a short success window (success_window≈max(8,λ)) and a tanh-moderated gain, and stagnation is handled by periodic Lévy-like burst searches (stagnation_check≈max(60,6·dim)) to escape local traps. Overall the design emphasizes a hybrid diagonal+low-rank search, aggressive online adaptation of mixing and scales, and multiple escape mechanisms (Cauchy/Levy, DE recombination, momentum) tuned for bounded continuous benchmarks.", "code": "import numpy as np\n\nclass LAROS:\n    \"\"\"\n    LAROS: Logit-Adaptive RMSProp Oja Search\n\n    Main idea:\n    - Per-coordinate RMSProp-like accumulator (G) to adapt coordinate scales (D = (G+eps)^-0.4).\n    - A small k-dimensional subspace U learned online via an Oja-type update from successful steps.\n    - Mixing between diagonal and low-rank proposals controlled by alpha; alpha is represented in logit\n      space and updated additively based on recent short-term success rate (different equation from OASLIS).\n    - DE-style archive recombination with higher probability and slightly different F factor.\n    - Mirrored sampling for variance reduction; occasional Cauchy/Lévy jumps to escape traps (different freq/scale).\n    - Momentum with a slightly different update form to carry inertia.\n    - Sigma adapts multiplicatively with a different gain and a bounding schedule.\n    - Many default hyper-parameters deliberately chosen differently from OASLIS.\n    - Intended for Many Affine BBOB-style noiseless continuous benchmarks with bounds [-5,5].\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, init_sigma=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size (different heuristic)\n        self.lambda_ = max(4, int(4 + np.round(4.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace dimension: a smaller, less aggressive choice than sqrt(dim)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(2.0 * np.log(max(2, self.dim + 1)))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # initial global step-size (different default scaling)\n        if init_sigma is None:\n            self.init_sigma = 0.15 * 10.0  # uses same domain scale idea but smaller multiplier\n        else:\n            self.init_sigma = float(init_sigma)\n\n        # stochastic operators probabilities and scales (different values)\n        self.p_cauchy = 0.10\n        self.cauchy_scale = 1.2\n        self.p_de = 0.25\n        self.F_de = 0.7\n        self.mirror = True\n\n        # Oja update rate (slightly larger)\n        self.oja_eta = 0.15\n\n        # momentum/inertia damping (slightly higher retention)\n        self.momentum_decay = 0.90\n\n        # logit-based alpha adaptation\n        # initial alpha represented as logit\n        init_alpha = 0.55\n        self.alpha_logit = np.log(init_alpha / (1.0 - init_alpha))\n        self.alpha_lr = 0.12  # additive update on logit (different equation)\n        self.alpha_target = 0.30  # target success rate (different target)\n\n        # RMSProp-like smoothing (different from AdaGrad)\n        self.rms_beta = 0.90\n        self.G_eps = 1e-7\n        # exponent on D different to change transformation (not exact inverse sqrt)\n        self.D_exponent = -0.4\n\n        # archive\n        self.archive_limit = 2000\n\n        # sigma adaptation gain (different multiplicative schedule)\n        self.sigma_lr = 0.8\n\n        # short-term window size for success counting\n        self.success_window = max(8, self.lambda_)\n\n        # stagnation parameters for Levy/inertia bursts (different periodicity)\n        self.stagnation_check = max(60, 6 * self.dim)\n        self.lévy_burst_count = 6  # number of trial jumps during a burst\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initialize\n        m = np.random.uniform(lb, ub)\n        sigma = max(1e-12, self.init_sigma)\n        G = np.ones(n) * 1e-3  # RMS accumulator initial value different\n        D = (G + self.G_eps) ** self.D_exponent\n\n        # initialize low-rank U via QR\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            U = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = rand / (np.linalg.norm(rand, axis=0, keepdims=True) + 1e-12)\n\n        v = np.zeros(n)  # momentum\n        # compute alpha from logit\n        def logit_to_alpha(logit):\n            s = 1.0 / (1.0 + np.exp(-logit))\n            return float(np.clip(s, 1e-4, 1.0 - 1e-4))\n        alpha = logit_to_alpha(self.alpha_logit)\n\n        archive_X = []\n        archive_F = []\n\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        # different weighting: log-linear weights (gives different recombination eq.)\n        ranks = np.arange(1, mu + 1)\n        weights = np.log(mu + 1.0) - np.log(ranks + 0.5)\n        weights = np.maximum(weights, 1e-12)\n        weights /= np.sum(weights)\n\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = fm; x_best = xm.copy()\n            fm_old = fm\n        else:\n            fm_old = np.inf\n\n        recent_successes = []\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # base gaussian draws\n            base_z = np.random.randn(current_lambda, n)\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # projections\n                proj_low = U @ (U.T @ z)\n                proj_high = z - proj_low\n\n                # updated per-coordinate scale\n                D = (G + self.G_eps) ** self.D_exponent\n\n                # subspace scaled differently: scale subspace by median(D) to change equation\n                sub_scale = float(np.median(D))\n\n                # compute alpha from stored logit (keeps eqn different from multiplicative exp)\n                alpha = logit_to_alpha(self.alpha_logit)\n\n                # diagonal component (elementwise)\n                y_diag = D * proj_high\n                # subspace component\n                y_sub = sub_scale * proj_low\n\n                # combine with a non-linear mixture: alpha controls a softmax-like blending\n                # use alpha as weight after applying a tanh to components to change mapping\n                y = (1.0 - alpha) * np.tanh(y_diag) + alpha * (0.8 * y_sub)\n\n                # occasional heavy-tailed jump with different sampling\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * sub_scale\n\n                # mirrored sampling\n                if self.mirror and (i % 2 == 1):\n                    y = -y\n\n                # momentum applied as a fraction proportional to sigma and v\n                x = m + sigma * y + 0.5 * v\n\n                # DE-style recombination (rand-1/bin style) applied probabilistically\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                    i1, i2, i3 = np.random.choice(len(archive_X), size=3, replace=False)\n                    # generate a rand/1 mutant\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2]) + 0.3 * (archive_X[i3] - m)\n                    # binomial crossover\n                    mask = (np.random.rand(n) < 0.5)\n                    x[mask] += de_mut[mask]\n\n                # clip\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # selection\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            m_old = m.copy()\n            fm_old = fm_old if 'fm_old' in locals() else np.inf\n\n            # recombine mean using log-linear weights (different eq)\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # compute a success measure relative to previous mean evaluation or best\n            sel_best_f = np.min(arfit[sel]) if sel.size > 0 else np.inf\n            # success if selected best improved over last mean evaluation or global best\n            success_flag = 1 if sel_best_f < fm_old - 1e-12 else 0\n            recent_successes.append(success_flag)\n            if len(recent_successes) > self.success_window:\n                recent_successes.pop(0)\n            success_rate = float(np.mean(recent_successes)) if recent_successes else 0.0\n\n            # update alpha logit additively (different from multiplicative exp rule)\n            # increase logit when success_rate > target, decrease otherwise\n            delta_logit = self.alpha_lr * (success_rate - self.alpha_target)\n            self.alpha_logit += float(delta_logit)\n            # recompute alpha optionally for internal use\n            alpha = logit_to_alpha(self.alpha_logit)\n\n            # update momentum: blend old momentum and new drift with different coefficients\n            v = self.momentum_decay * v + (1.0 - self.momentum_decay) * (sigma * y_w)\n\n            # RMSProp-like update of G using weighted second moments (different smoothing)\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            G = self.rms_beta * G + (1.0 - self.rms_beta) * (y2 + self.G_eps)\n            D = (G + self.G_eps) ** self.D_exponent\n\n            # sigma adaptation: multiplicative update with different gain and normalization\n            # target success 0.25 here; use tanh to moderate large jumps\n            adj = self.sigma_lr * (success_rate - 0.25) / max(0.08, np.sqrt(1.0 + n / 8.0))\n            sigma *= np.exp(adj)\n            domain_scale = np.mean(ub - lb)\n            sigma = float(np.clip(sigma, 1e-12, 50.0 * domain_scale))\n\n            # Oja update: use the raw weighted y_w as signal (not normalized), with a different normalization factor\n            signal = y_w.copy()\n            norm_signal = np.linalg.norm(signal) + 1e-12\n            if norm_signal > 1e-12:\n                signal_unit = signal / norm_signal\n                # update each basis vector with scaled Oja step\n                for j in range(U.shape[1]):\n                    u = U[:, j]\n                    proj = np.dot(u, signal_unit)\n                    u = u + self.oja_eta * proj * (signal_unit - proj * u)\n                    U[:, j] = u / (np.linalg.norm(u) + 1e-12)\n                # occasional re-orthonormalization (different probability)\n                if np.random.rand() < 0.15:\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # stagnation check and Lévy bursts (different detection frequency and behavior)\n            if (len(archive_F) >= self.stagnation_check) and (evals % self.stagnation_check == 0):\n                last_vals = archive_F[-self.stagnation_check:]\n                if len(last_vals) == self.stagnation_check and np.min(last_vals) >= f_best - 1e-12:\n                    old_sigma = sigma\n                    sigma *= 2.5\n                    # try a handful of Lévy-like directional escapes\n                    for _ in range(min(self.lévy_burst_count, budget - evals)):\n                        z = np.random.randn(n)\n                        r = np.random.standard_cauchy() * 1.8\n                        y = r * (z / (np.linalg.norm(z) + 1e-12)) * float(np.median(D))\n                        x_try = np.clip(m + sigma * y, lb, ub)\n                        f_try = func(x_try)\n                        evals += 1\n                        archive_X.append(x_try.copy()); archive_F.append(f_try)\n                        if f_try < f_best:\n                            f_best = f_try\n                            x_best = x_try.copy()\n                            # adopt new mean and reset momentum and recent successes\n                            m = x_try.copy()\n                            v = np.zeros(n)\n                            recent_successes = []\n                            # update fm_old as this becomes the new reference\n                            fm_old = f_try\n                    sigma = old_sigma\n\n            # set fm_old to the best known fitness of the current mean if we have evaluated it previously,\n            # else approximate it by the best selected (conservative)\n            # we keep fm_old as min(f_best, previous fm_old)\n            fm_old = min(f_best, fm_old)\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm LAROS scored 0.417 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e0803be8-3b99-4e93-af4c-1f31db4bf8ee", "operator": null, "metadata": {"aucs": [0.1274284870264455, 0.16586553369915402, 0.5544248365784205, 0.9388337253796734, 0.34306755126072064, 0.9015006198120405, 0.2882497493212295, 0.3763593710317412, 0.2895685719409752, 0.1810289269660681]}, "task_prompt": ""}
{"id": "f4bf2f11-713b-42a6-9ce3-eac642c87b2c", "fitness": "-inf", "name": "ATOM_SPIRE", "description": "ATOM_SPIRE maintains a small orthonormal \"atom\" subspace (k ≈ ceil(log2(dim)+1)) learned online and samples candidate directions by Dirichlet mixes of these atoms (adaptive concentration) combined with an orthogonal per-coordinate RMS-scaled Gaussian component (S and D with rms_beta=0.85) so the search mixes structured subspace moves and fine-grained anisotropic noise. It injects curved trajectories via a cheap rotated-momentum \"spiral\" (momentum v with momentum_decay=0.88), uses mirrored (antithetic) sampling and occasional heavy-tailed Cauchy jumps (p_cauchy≈0.12) plus DE-style elite recombination (p_de≈0.16) with an archive of elites to promote diversity and recombination. Adaptation is done by smoothed multiplicative sigma updates (init_sigma=1.5, soft 1/5-like rule with eta_sigma≈0.18), online projection-style atom updates (atom_eta≈0.12) with periodic QR re‑orthonormalization, and an RMS-like per-coordinate scaler to shape steps. Stagnation triggers Lévy/inertia bursts, archive pruning, and modest randomization of atoms; population size, mu/weights and ranking are tuned to favor a few elites while keeping a budget-aware lambda-driven sampling loop within bounds [-5,5].", "code": "import numpy as np\n\nclass ATOM_SPIRE:\n    \"\"\"\n    ATOM_SPIRE: Atomized Subspace & PIving-Spiral REstarts optimizer\n\n    Main ideas / novel mechanisms:\n    - Maintain a small set of normalized \"atoms\" (direction prototypes) learned online from successful steps.\n    - Sample candidate directions by mixing atoms using a Dirichlet draw (compositional sampling),\n      plus an orthogonal per-coordinate RMS-scaled Gaussian component to capture fine-grained adaption.\n    - Add a low-cost \"spiral\" perturbation derived from a rotated momentum (approximated by index shifts)\n      to produce curved trajectories that explore along manifolds.\n    - Update atoms via an online projection-update (atom_eta) distinct from classical Oja: atoms are\n      nudged toward the weighted successful composite directions and re-normalized.\n    - Use an RMS-like per-coordinate scaler (S) similar to RMSProp/Adam second-moment smoothing,\n      but applied to step-vectors for anisotropic scaling; D = 1 / (sqrt(S)+eps).\n    - Use Dirichlet mixing concentration to create sparse vs dense atom mixes (adaptive).\n    - Include mirrored sampling, DE-style elite recombination, periodic QR orthonormalization of atoms,\n      adaptive step-size (smoothed multiplicative rule), and occasional Lévy/inertia bursts when stagnating.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 atoms_k=None,\n                 init_sigma=None,\n                 atom_eta=0.12,\n                 rms_beta=0.85,\n                 dirichlet_conc=0.8,\n                 p_cauchy=0.12,\n                 p_de=0.16,\n                 mirror=True,\n                 momentum_decay=0.88):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.atom_eta = float(atom_eta)\n        self.rms_beta = float(rms_beta)\n        self.dirichlet_conc = float(dirichlet_conc)\n        self.p_cauchy = float(p_cauchy)\n        self.p_de = float(p_de)\n        self.mirror = bool(mirror)\n        self.momentum_decay = float(momentum_decay)\n\n        # population size (slightly different heuristic than many algorithms)\n        self.lambda_ = max(8, int(5 + np.floor(4.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # number of atoms (small subspace); use a log-scale to differ from sqrt heuristic\n        if atoms_k is None:\n            self.k = max(1, int(np.ceil(np.log2(max(2, self.dim)) + 1)))\n        else:\n            self.k = min(max(1, int(atoms_k)), self.dim)\n\n        # initial sigma relative to [-5,5] default if not provided\n        if init_sigma is None:\n            self.init_sigma = 0.15 * 10.0\n        else:\n            self.init_sigma = float(init_sigma)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (use func.bounds if available)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initialize mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = max(1e-12, self.init_sigma)\n\n        # RMS-like second moment per-coordinate (S) and derived diagonal scale D\n        S = np.ones(n) * 1e-2\n        eps = 1e-8\n        D = 1.0 / (np.sqrt(S) + eps)\n\n        # initialize atom matrix A (n x k) orthonormal via QR\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            A = Q[:, :self.k].copy()\n        except np.linalg.LinAlgError:\n            # fallback to normalized random vectors\n            A = rand.copy()\n            for j in range(self.k):\n                norm = np.linalg.norm(A[:, j]) + 1e-12\n                A[:, j] = A[:, j] / norm\n\n        # momentum (inertia) vector\n        v = np.zeros(n)\n\n        # Dirichlet concentration parameter for atom mixing (can change adaptively)\n        dir_conc = float(self.dirichlet_conc)\n\n        # archive of elites for recombination\n        archive_X = []\n        archive_F = []\n        archive_limit = max(500, 40 * n)  # bounded archive\n\n        # population params and weights (exponential ranking to diversify)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        ranks = np.arange(1, mu + 1)\n        # exponential rank weights (different weighting scheme)\n        weights = np.exp(-0.5 * (ranks - 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_best = fm\n            x_best = xm.copy()\n            fm_mean = fm\n        else:\n            fm_mean = np.inf\n\n        # short-term success memory to adapt mixing and sigma\n        recent = []\n        success_window = max(12, lam * 2)\n\n        # auxiliary parameters\n        atom_eta = float(self.atom_eta)\n        rms_beta = float(self.rms_beta)\n        p_cauchy = float(self.p_cauchy)\n        p_de = float(self.p_de)\n        mirror = bool(self.mirror)\n        momentum_decay = float(self.momentum_decay)\n\n        # stagnation detection counters\n        best_since = 0\n        stagnation_threshold = max(60, 6 * n)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # sample a Dirichlet base for each offspring (mix of atoms)\n            # Dirichlet concentration controls sparsity (small => sparse mix; large => dense mix)\n            for i in range(current_lambda):\n                # Dirichlet mixing over atoms\n                mix = np.random.dirichlet(np.ones(self.k) * dir_conc)\n\n                # composite atom direction (normalized)\n                d_atoms = A.dot(mix)\n                norm_da = np.linalg.norm(d_atoms) + 1e-12\n                d_atoms = d_atoms / norm_da\n\n                # Gaussian orthogonal complement: sample z and remove projection onto atom-subspace\n                z = np.random.randn(n)\n                # projection onto atom subspace (if A orthonormal this is A @ (A.T @ z))\n                # but A may drift; we nonetheless use A.T @ z for projection coefficients\n                coeffs = A.T.dot(z)  # k\n                proj = A.dot(coeffs)\n                z_orth = z - proj\n\n                # apply per-coordinate RMS scaling to the orthogonal component\n                d_coord = D * z_orth\n                # normalize coordinate part magnitude to match a typical scale\n                norm_dc = np.linalg.norm(d_coord) + 1e-12\n                d_coord = d_coord / norm_dc\n\n                # a cheap \"spiral\" perturbation derived from shifted/rotated momentum:\n                # create a circular-ish component by index shifts (low cost surrogate for Givens rotations)\n                # rotate momentum by small index shifts and combine sin/cos with decay\n                t = (evals + i) / max(1.0, budget)\n                spiral_scale = 0.6 * (0.5 + 0.5 * np.sin(2.0 * np.pi * t))\n                spiral = np.roll(v, 1) * (0.6 * np.sin(t * 3.0)) - np.roll(v, 2) * (0.4 * np.cos(t * 2.1))\n                # normalize spiral\n                s_norm = np.linalg.norm(spiral) + 1e-12\n                spiral = spiral / s_norm\n\n                # adaptive mixing coefficient between atoms and coordinate noise (alpha_atoms)\n                # we adapt by recent success: if successes high, favor atoms\n                success_rate = float(np.mean(recent)) if recent else 0.0\n                alpha_atoms = 0.5 * (1.0 + (success_rate - 0.2))  # simple affine mapping\n                alpha_atoms = float(np.clip(alpha_atoms, 0.05, 0.95))\n\n                # combine components into a final direction y (normalized)\n                y = alpha_atoms * d_atoms + (1.0 - alpha_atoms) * d_coord + 0.08 * spiral\n                y_norm = np.linalg.norm(y) + 1e-12\n                y = y / y_norm\n\n                # occasional heavy-tailed jump\n                if np.random.rand() < p_cauchy:\n                    r = np.random.standard_cauchy() * 1.2\n                    y = r * (y / (np.linalg.norm(y) + 1e-12))\n                    # de-scale to avoid overflow\n                    y = y / (1.0 + abs(r) * 0.5)\n\n                # mirrored sampling (antithetic)\n                if mirror and (i % 2 == 1):\n                    y = -y\n\n                # candidate with inertia contribution\n                inertia_term = 0.45 * v  # tuned weight for inertia\n                x = m + sigma * y + inertia_term\n\n                # DE-style elite recombination with some probability: pick two elites and blend\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    elite1 = archive_X[i1]\n                    elite2 = archive_X[i2]\n                    beta = np.random.rand()\n                    de_vec = beta * (elite1 - elite2)\n                    # random per-dim crossover mask\n                    mask = (np.random.rand(n) < 0.5)\n                    x[mask] += de_vec[mask] * np.random.uniform(0.4, 0.9)\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y  # store direction (approx (x-m)/sigma ignoring inertia)\n\n            # Evaluate candidates without exceeding budget\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                # archive and elite management\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > archive_limit:\n                    # drop worst entries to keep archive informative\n                    worst_idx = np.argmax(archive_F)\n                    archive_X.pop(worst_idx)\n                    archive_F.pop(worst_idx)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n                    best_since = 0\n                else:\n                    best_since += 1\n\n            # selection & recombination\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            m_old = m.copy()\n            # recombine mean by weighted convex combination of selected candidates\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean direction in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # success detection: consider whether any selected beat the previous mean fitness estimate\n            sel_best_f = np.min(arfit[sel])\n            # conservative check against fm_mean (last evaluated mean); if unknown use global best\n            compare_ref = fm_mean if (fm_mean is not None) else f_best\n            success_flag = 1 if sel_best_f < compare_ref - 1e-12 else 0\n\n            # update recent successes window\n            recent.append(success_flag)\n            if len(recent) > success_window:\n                recent.pop(0)\n\n            # adapt Dirichlet concentration to encourage sparser mixes when atoms succeed\n            # when atoms do well, increase concentration to create focused mixes\n            atom_success_rate = float(np.mean(recent)) if recent else 0.0\n            dir_conc *= np.exp(0.04 * (atom_success_rate - 0.18))\n            dir_conc = float(np.clip(dir_conc, 0.2, 3.0))\n\n            # Update momentum/inertia: carry forward and add new drift\n            v = momentum_decay * v + 0.95 * sigma * y_w\n\n            # Update RMS-like second moment S using weighted squared selected directions (in coordinate space)\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            S = rms_beta * S + (1.0 - rms_beta) * (y2 + eps)\n            D = 1.0 / (np.sqrt(S) + eps)\n\n            # adaptive global sigma (smoothed multiplicative rule, soft 1/5th-like)\n            success_rate = float(np.mean(recent)) if recent else 0.0\n            eta_sigma = 0.18  # learning rate for sigma adaption\n            sigma *= np.exp(eta_sigma * (success_rate - 0.2) / max(0.08, np.sqrt(1.0 + n / 15.0)))\n            domain_scale = np.mean(ub - lb)\n            sigma = float(np.clip(sigma, 1e-12, 80.0 * domain_scale))\n\n            # Update atoms A with the weighted successful composite direction\n            # We use a projection-update that nudges each atom toward y_w while reducing component already explained\n            signal = y_w.copy()\n            sig_norm = np.linalg.norm(signal) + 1e-12\n            signal = signal / sig_norm\n            if sig_norm > 1e-12:\n                for j in range(self.k):\n                    u = A[:, j]\n                    proj = np.dot(u, signal)\n                    # update atom: different formula than classic Oja, includes a damping term\n                    u_new = (1.0 - atom_eta) * u + atom_eta * (signal - proj * u) * proj\n                    u_new = u_new / (np.linalg.norm(u_new) + 1e-12)\n                    A[:, j] = u_new\n                # occasional re-orthonormalize to maintain numerical stability\n                if np.random.rand() < 0.15:\n                    try:\n                        Q, _ = np.linalg.qr(A)\n                        A = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # update fm_mean: re-evaluate or approximate previous mean fitness if lost (we keep fm_mean as last known)\n            # if new mean m changed significantly, compute fm_mean lazily if budget remains and improvement is likely\n            if evals < budget and np.linalg.norm(m - m_old) > 1e-8:\n                # small probability to evaluate new mean to keep fm_mean up to date (budget permitting)\n                if np.random.rand() < 0.15:\n                    xm = np.clip(m, lb, ub)\n                    fm_new = func(xm)\n                    evals += 1\n                    archive_X.append(xm.copy()); archive_F.append(fm_new)\n                    if fm_new < f_best:\n                        f_best = fm_new; x_best = xm.copy()\n                    fm_mean = fm_new\n\n            # stagnation detection and Lévy/inertia burst if stuck\n            if best_since >= stagnation_threshold:\n                # perform a modest number of heavy explorative jumps, adopt only if better\n                old_sigma = sigma\n                sigma *= 2.5\n                n_bursts = min(5, budget - evals)\n                for _ in range(n_bursts):\n                    z = np.random.randn(n)\n                    r = np.random.standard_cauchy() * 1.6\n                    y_le = r * (z / (np.linalg.norm(z) + 1e-12)) * np.mean(D)\n                    x_try = np.clip(m + sigma * y_le + 0.7 * v, lb, ub)\n                    if evals >= budget:\n                        break\n                    f_try = func(x_try)\n                    evals += 1\n                    archive_X.append(x_try.copy()); archive_F.append(f_try)\n                    if f_try < f_best:\n                        f_best = f_try; x_best = x_try.copy()\n                        # adopt new mean and reset momentum\n                        m = x_try.copy()\n                        v = np.zeros(n)\n                        fm_mean = f_try\n                        best_since = 0\n                sigma = old_sigma  # restore\n                # slightly randomize atoms to increase exploration directionality\n                for j in range(self.k):\n                    if np.random.rand() < 0.3:\n                        perturb = 0.03 * np.random.randn(n)\n                        A[:, j] += perturb\n                        A[:, j] /= (np.linalg.norm(A[:, j]) + 1e-12)\n\n            # enforce mean bounds\n            m = np.clip(m, lb, ub)\n\n        # return best found\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 267, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (7,1) (6,10) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "error": "In the code, line 267, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (7,1) (6,10) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "parent_ids": "e0803be8-3b99-4e93-af4c-1f31db4bf8ee", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "23e9cf3f-d0e0-406d-bd6d-72692cfe6f5b", "fitness": 0.2529725166888488, "name": "SPIRIT", "description": "The optimizer mixes a learned low‑rank search subspace U (k ≈ dim^0.6, updated online via a small Oja rate 0.04 and occasional re‑orthonormalization) with a diagonal coordinate rescaling D derived from an RMSProp‑like second moment S (beta=0.90, S_init small) and a momentum/inertia term (decay 0.90, scale 0.5), using an adaptive mixture weight alpha to interpolate subspace vs complement steps. It injects robust exploration through frequent mirrored Gaussian draws plus probabilistic heavy‑tailed Student‑t jumps (p_student=0.18, df=3, scale≈1.1) and larger occasional stagnation bursts (sigma×4, up to 8 t‑trials) to escape local traps. Population and selection are modestly enlarged with an exponential rank‑to‑weight scheme (tau=0.9) and archive‑based DE‑style recombination (p_de=0.22, F_de=0.7, cr_de=0.6, archive_limit=3000) to reuse past good solutions. Step‑size sigma is adapted multiplicatively from recent success rate (soft 1/5th‑style update, factor exponent 0.45 scaled by √(1+n/8)), while other hyperchoices (conservative init_sigma≈1.5, median scaling of subspace, mirrored pairing) bias the search toward reliable local refinement with intermittent bold jumps.", "code": "import numpy as np\n\nclass SPIRIT:\n    \"\"\"\n    SPIRIT: Subspace-Probabilistic Inertia + Robust t-sampling\n\n    Key ideas / differences from the referenced OASLIS:\n    - RMSProp-like second-moment accumulator (decay beta) instead of simple AdaGrad smoothing.\n    - Subspace dimension k ~ dim^0.6 (different scaling) and smaller Oja learning rate.\n    - Student-t (df=3) heavy-tailed jumps (different tail) with a different probability.\n    - Slightly larger population heuristic and alternative rank-to-weight mapping (exponential ranks).\n    - DE-style recombination uses archive-based rand/1 with higher F and crossover rate.\n    - Momentum/inertia with a different decay and contribution factor.\n    - Adaptive mixing alpha updated toward a different target with a distinct learning rate.\n    - Stagnation bursts use repeated Student-t explorations and adopt improvements.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, init_sigma=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        self.seed = seed\n\n        # population size: slightly larger scaling\n        self.lambda_ = max(6, int(5 + np.floor(2.5 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank dimension (different exponent)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(self.dim ** 0.6)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # initial sigma (global step)\n        if init_sigma is None:\n            # slightly more conservative global steps than examples: 0.15 * range\n            self.init_sigma = 0.15 * 10.0\n        else:\n            self.init_sigma = float(init_sigma)\n\n        # probabilities and scales (different values)\n        self.p_student = 0.18          # probability of Student-t heavy jump\n        self.student_df = 3.0          # degrees of freedom for heavy tails\n        self.student_scale = 1.1\n\n        self.p_de = 0.22               # archive DE recombination probability\n        self.F_de = 0.7                # DE/rand/1 scale\n        self.cr_de = 0.6               # DE crossover prob\n\n        self.mirror = True\n\n        # Oja parameters (smaller learning rate than original)\n        self.oja_eta = 0.04\n\n        # momentum / inertia\n        self.momentum_decay = 0.90\n        self.momentum_scale = 0.5\n\n        # alpha mixing (different init/target/learning rate)\n        self.alpha = 0.4\n        self.alpha_lr = 0.10\n        self.alpha_target = 0.30\n\n        # RMSProp-like second moment accumulator\n        self.rms_beta = 0.90\n        self.S_eps = 1e-7\n        self.S_init = 1e-3\n\n        # archive size\n        self.archive_limit = 3000\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (assume -5..5 if not provided)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initialize mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = max(1e-12, float(self.init_sigma))\n\n        # RMS second moment accumulator S and per-coordinate scale D = 1/sqrt(S + eps)\n        S = np.ones(n) * float(self.S_init)\n        D = 1.0 / np.sqrt(S + self.S_eps)\n\n        # low-rank subspace U orthonormal init\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            U = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            # fallback to normalized columns\n            U = rand.copy()\n            for j in range(U.shape[1]):\n                U[:, j] /= (np.linalg.norm(U[:, j]) + 1e-12)\n\n        # momentum/inertia vector\n        v = np.zeros(n)\n\n        # archive for DE-like recombination\n        archive_X = []\n        archive_F = []\n\n        # rank-to-weight scheme: exponential ranks (sharper than linear)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        ranks = np.arange(1, mu + 1)\n        tau = 0.9  # temperature controlling steepness\n        raw_w = np.exp(-tau * (ranks - 1))\n        weights = raw_w / np.sum(raw_w)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_best = fm\n            x_best = xm.copy()\n            last_mean_f = fm\n        else:\n            last_mean_f = np.inf\n\n        # recent success window for alpha adaptation\n        recent_successes = []\n        success_window = max(8, lam)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # produce base gaussian draws\n            base_z = np.random.randn(current_lambda, n)\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # project onto subspace and complement\n                proj_low = U @ (U.T @ z)\n                proj_high = z - proj_low\n\n                # diagonal-scaled high component (elementwise)\n                y_diag = D * proj_high\n\n                # subspace-scaled low component: use median(D) for scaling\n                medD = float(np.median(D))\n                y_sub = medD * proj_low\n\n                # combine using alpha\n                y = (1.0 - self.alpha) * y_diag + self.alpha * y_sub\n\n                # occasional Student-t heavy-tailed jump\n                if np.random.rand() < self.p_student:\n                    # direction is z normalized\n                    nz = np.linalg.norm(z) + 1e-20\n                    r = np.random.standard_t(self.student_df) * self.student_scale\n                    y = r * (z / nz) * medD\n\n                # mirrored sampling: pair every two samples\n                if self.mirror and (i % 2 == 1):\n                    y = -y\n\n                # candidate with inertia (momentum contributes as drift)\n                x = m + sigma * y + self.momentum_scale * v\n\n                # DE-style archive recombination (rand/1) with crossover\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                    # pick three distinct archive members\n                    i1, i2, i3 = np.random.choice(len(archive_X), size=3, replace=False)\n                    xa = archive_X[i1]; xb = archive_X[i2]; xc = archive_X[i3]\n                    de_mut = self.F_de * (xa - xb)\n                    # per-dim crossover\n                    mask = (np.random.rand(n) < self.cr_de)\n                    x[mask] = x[mask] + de_mut[mask]\n                    # small mix with xc to diversify\n                    x = x + 0.05 * (xc - x)\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates without exceeding budget\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    # FIFO removal\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # selection: best mu\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            m_old = m.copy()\n            # recombine mean with exponential rank weights\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # success measure: did any selected offspring improve global best?\n            sel_best_f = float(np.min(arfit[sel])) if len(sel) > 0 else np.inf\n            success_flag = 1 if sel_best_f < f_best else 0\n            recent_successes.append(1 if sel_best_f < f_best else 0)\n            if len(recent_successes) > success_window:\n                recent_successes.pop(0)\n            success_rate = float(np.mean(recent_successes)) if recent_successes else 0.0\n\n            # adapt alpha: increase subspace usage when recent success > target\n            # multiplicative update (different base/strength)\n            self.alpha *= np.exp(self.alpha_lr * (success_rate - self.alpha_target))\n            self.alpha = float(np.clip(self.alpha, 0.03, 0.95))\n\n            # update momentum\n            v = self.momentum_decay * v + (1.0 - self.momentum_decay) * (sigma * y_w)\n\n            # update RMS second moment S with weighted second moment of y_sel\n            if y_sel.size > 0:\n                y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            else:\n                y2 = np.zeros(n)\n            S = self.rms_beta * S + (1.0 - self.rms_beta) * (y2 + self.S_eps)\n            D = 1.0 / np.sqrt(S + self.S_eps)\n\n            # adapt sigma multiplicatively (smoothed 1/5th-like)\n            # target success 0.2 but with different scaling\n            adapt_factor = np.exp(0.45 * (success_rate - 0.2) / max(0.05, np.sqrt(1.0 + n / 8.0)))\n            sigma *= adapt_factor\n            domain_scale = np.mean(ub - lb)\n            sigma = float(np.clip(sigma, 1e-12, 80.0 * domain_scale))\n\n            # Online Oja update for U using normalized signal y_w\n            signal = y_w.copy()\n            sig_norm = np.linalg.norm(signal)\n            if sig_norm > 1e-12:\n                signal = signal / (sig_norm + 1e-12)\n                for j in range(U.shape[1]):\n                    u = U[:, j]\n                    proj = float(np.dot(u, signal))\n                    u = u + self.oja_eta * proj * (signal - proj * u)\n                    u = u / (np.linalg.norm(u) + 1e-12)\n                    U[:, j] = u\n                # occasional re-orthonormalize\n                if np.random.rand() < 0.15:\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # stagnation detection and inertia bursts (Student-t explorations)\n            stagnation_length = max(30, 3 * n)\n            if len(archive_F) >= stagnation_length and (evals % stagnation_length == 0):\n                recent = archive_F[-stagnation_length:]\n                if len(recent) == stagnation_length and np.min(recent) >= (f_best - 1e-12):\n                    # perform a burst of Student-t directed trials\n                    old_sigma = sigma\n                    sigma *= 4.0\n                    tries = min(8, budget - evals)\n                    for _ in range(tries):\n                        z = np.random.randn(n)\n                        nz = np.linalg.norm(z) + 1e-12\n                        r = np.random.standard_t(self.student_df) * (1.6 * self.student_scale)\n                        y = r * (z / nz) * float(np.median(D))\n                        x_try = np.clip(m + sigma * y, lb, ub)\n                        f_try = func(x_try)\n                        evals += 1\n                        archive_X.append(x_try.copy()); archive_F.append(f_try)\n                        if f_try < f_best:\n                            f_best = f_try\n                            x_best = x_try.copy()\n                            # adopt new mean and reset momentum\n                            m = x_try.copy()\n                            v = np.zeros(n)\n                            break\n                    sigma = old_sigma\n\n            # keep mean in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm SPIRIT scored 0.253 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e0803be8-3b99-4e93-af4c-1f31db4bf8ee", "operator": null, "metadata": {"aucs": [0.061334011380510756, 0.1453045921262831, 0.7206035328731966, 0.41356235410786624, 0.2899180852427047, 0.17260192937893837, 0.18442084803236491, 0.11238524575456166, 0.26404754505395756, 0.16554702293810453]}, "task_prompt": ""}
{"id": "61e0b794-2a70-4ab1-af6b-befa2f61a927", "fitness": 0.4137948336482868, "name": "AHLDE", "description": "AHLDE is a hybrid strategy that combines a CMA-like global step-size controller (ps and sigma with mu-eff based weights, lambda ~ 4+3log(n)) with per-coordinate adaptive scaling (an AdaGrad-like accumulator G producing diagonal multipliers D) and a learned low-rank subspace U (k ≈ ceil(sqrt(n))) to capture dominant search directions. Sampling mixes a diagonal (D·orthogonal) component and a low-rank component (U·z_low) with an adaptive mixing weight alpha (start 0.6, updated from projection energy), uses mirrored pairs, occasional heavy-tailed Cauchy jumps (p_cauchy ≈ 0.12) and DE-style archive differences (p_de ≈ 0.20, F_de=0.75) for exploration. Adaptation is driven by a CMA-like path-length update for sigma, exponential smoothing of second-moment statistics into G→D (beta_g=0.2), momentum/inertia (momentum_decay=0.85) for stability, and budget-aware selection/recombination with log-weights and mu ≈ lambda/2. The low-rank subspace is learned online via Oja updates (oja_eta≈0.06) and periodically refreshed by SVD on a success buffer (svd_every=25, buffer_max≈max(20,8k)) with re-orthonormalization, while an archive and stagnation detectors trigger sigma inflation and mild restarts to escape local traps.", "code": "import numpy as np\n\nclass AHLDE:\n    \"\"\"\n    AHLDE: Adaptive Hybrid Low-rank + Diagonal Evolution\n\n    Main idea:\n    - Maintain global sigma with a CMA-like path-length controller (ps).\n    - Maintain per-coordinate adaptive scales via an AdaGrad-like accumulator G -> D.\n    - Maintain a small low-rank subspace U (k ~ sqrt(n)) learned online via Oja and refreshed\n      via periodic SVD on a small buffer of successful steps.\n    - Sample steps mixing diagonal and low-rank components, use mirrored sampling,\n      occasionally apply Cauchy jumps and DE-style archive differences.\n    - Use log-based recombination weights, archive for exploration, momentum for inertia,\n      and stagnation detection to trigger bursts/restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population & selection\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # hyperparameters (sensible defaults)\n        self.p_cauchy = 0.12\n        self.cauchy_scale = 1.0\n        self.p_de = 0.20\n        self.F_de = 0.75\n        self.mirrored = True\n        self.momentum_decay = 0.85\n        self.oja_eta = 0.06\n        self.svd_every = 25  # apply SVD refresh every this many generations (if buffer sufficient)\n        self.buffer_max = max(20, 8 * self.k)\n        self.archive_limit = 4000\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_scale = np.mean(ub - lb)\n\n        # strategy parameters (CMA-like for sigma control)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.2 * domain_scale\n        ps = np.zeros(n)\n\n        # AdaGrad-like accumulator and derived diagonal scale D\n        G = np.ones(n) * 1e-2\n        eps = 1e-8\n        D = (G + eps) ** -0.5  # elementwise multiplier\n\n        # low-rank subspace U (n x k) via QR\n        rand_mat = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand_mat)\n            U = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = np.zeros((n, self.k))\n\n        # small buffer of recent successful (weighted) steps for SVD refresh\n        success_buffer = []\n\n        # momentum/inertia\n        v = np.zeros(n)\n\n        # adaptive mix between diag and subspace (alpha in [0,1]) start balanced\n        alpha = 0.6\n\n        # archive for DE differences\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = fm; x_best = xm.copy()\n\n        gen = 0\n        no_improve_gens = 0\n        last_improve_eval = evals\n\n        # main loop\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # produce samples\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # pre-sampled gaussian components\n            zs = np.random.randn(current_lambda, n)\n            zs_low = np.random.randn(current_lambda, self.k) if self.k > 0 else np.zeros((current_lambda, 0))\n\n            for i in range(current_lambda):\n                z = zs[i].copy()\n\n                # decompose z into subspace projection and orthogonal complement\n                if self.k > 0:\n                    proj_low = U @ (U.T @ z)\n                    proj_high = z - proj_low\n                else:\n                    proj_low = np.zeros(n)\n                    proj_high = z\n\n                # diag part applies elementwise D to orthogonal part\n                y_diag = D * proj_high\n\n                # subspace part from random low-dim sample\n                if self.k > 0:\n                    low = U @ zs_low[i]\n                    # scale subspace by mean diag scale (so comparable magnitude)\n                    y_sub = np.mean(D) * low\n                else:\n                    y_sub = np.zeros(n)\n\n                # combine components\n                y = (1.0 - alpha) * y_diag + alpha * y_sub\n\n                # occasional Cauchy jump\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * np.mean(D)\n\n                # mirrored sampling\n                if self.mirrored and (i % 2 == 1):\n                    y = -y\n\n                x = m + sigma * y + 0.35 * v\n\n                # DE-style archive difference mutation with crossover-like blending\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    # per-dim probabilistic add to preserve partial structure\n                    mask = (np.random.rand(n) < 0.5)\n                    x[mask] += de_mut[mask]\n\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # Evaluate candidates (careful budget accounting)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n                    last_improve_eval = evals\n\n            # selection and recombination\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space (approx)\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # update path ps: approximate invsqrt using diagonal D only\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n\n            # sigma update (CMA-like)\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = float(np.clip(sigma, 1e-12, 1e2 * domain_scale))\n\n            # update AdaGrad-like G and D with weighted second moment of selected y's\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            # exponential smoothing toward y2\n            beta_g = 0.2\n            G = (1.0 - beta_g) * G + beta_g * (y2 + eps)\n            D = (G + eps) ** -0.5\n\n            # update momentum/inertia\n            v = self.momentum_decay * v + 0.95 * sigma * y_w\n\n            # online Oja update of subspace using normalized y_w as signal\n            if self.k > 0:\n                sig = y_w.copy()\n                nrm = np.linalg.norm(sig)\n                if nrm > 1e-12:\n                    sig = sig / (nrm + 1e-12)\n                    for j in range(U.shape[1]):\n                        u = U[:, j]\n                        proj = np.dot(u, sig)\n                        u = u + self.oja_eta * proj * (sig - proj * u)\n                        u = u / (np.linalg.norm(u) + 1e-12)\n                        U[:, j] = u\n                    # occasional re-orthonormalize\n                    if np.random.rand() < 0.15:\n                        try:\n                            Q, _ = np.linalg.qr(U)\n                            U = Q[:, :self.k]\n                        except np.linalg.LinAlgError:\n                            pass\n\n            # store success into buffer (weighted mean step)\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.buffer_max:\n                success_buffer.pop(0)\n\n            # periodic SVD refresh for U from buffer to capture reliable principal directions\n            if (gen % self.svd_every == 0) and (len(success_buffer) >= self.k):\n                try:\n                    Y = np.vstack(success_buffer).T  # n x m\n                    Y = Y - np.mean(Y, axis=1, keepdims=True)\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        # mix old and new subspace for stability\n                        alpha_mix = 0.7\n                        U_refresh = U_new[:, :k_take]\n                        if U_refresh.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U_refresh.shape[1]))\n                            U_refresh = np.hstack([U_refresh, pad])\n                        U = alpha_mix * U_refresh + (1 - alpha_mix) * U\n                        # re-orthonormalize\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                except np.linalg.LinAlgError:\n                    pass\n\n            # adapt mixing alpha: increase low-rank weight when successes come from subspace directions\n            # heuristic: measure fraction of selected y energy in the subspace vs total\n            if self.k > 0:\n                total_energy = np.sum(y_sel ** 2)\n                if total_energy > 0:\n                    proj_energy = np.sum((U.T @ y_sel.T) ** 2)\n                    frac = np.sum(proj_energy) / (total_energy + 1e-20)\n                    # move alpha toward frac with small learning rate\n                    alpha += 0.05 * (min(1.0, frac) - alpha)\n                    alpha = float(np.clip(alpha, 0.05, 0.98))\n\n            # stagnation detection: if no improvement in long window, do sigma inflation and mild restart around best\n            if (evals - last_improve_eval) > max(200, 10 * n):\n                # bump sigma and nudge mean toward random good archive point\n                sigma *= 2.2\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                # clear buffer to allow fresh subspace learning\n                success_buffer = []\n                last_improve_eval = evals  # avoid repeating immediate\n                no_improve_gens = 0\n\n            # keep mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # loop until budget consumed\n            # end generation\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AHLDE scored 0.414 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e0803be8-3b99-4e93-af4c-1f31db4bf8ee", "operator": null, "metadata": {"aucs": [0.16815974679238233, 0.15257135404064903, 0.6764036793672586, 0.921279914220565, 0.32455773252593023, 0.9083746215996958, 0.21488780193299983, 0.29408842491990395, 0.34543747002099123, 0.13218759106249112]}, "task_prompt": ""}
{"id": "aa7c2df4-919b-4e2d-9b84-060254a53773", "fitness": "-inf", "name": "LMCS", "description": "The design is a hybrid CMA-style optimizer: it keeps CMA-like population sizing (λ ≈ 4+3 log n), recombination weights, evolution paths (ps, pc), rank-one/mu covariance updates, sigma adaptation with damping, and periodic eigen-decomposition (every ~10·n) to form B/D for sampling. It augments mirrored Gaussian sampling with a small LRU memory (mem_size=8) and Langevin-style momentum (alpha_m≈0.55) so recent normalized steps bias proposals, and it builds compact memory-primed subspaces (k ≈ n^(2/3) or n^0.6) for focused probes and intensifications. Per-coordinate RMS scaling (coord_var, coord_alpha≈0.18, init tiny ~1e-3) adaptively rescales directions, while occasional short budget-aware 1D golden-section-like line searches, subspace intensifications, and Cauchy jumps (p_cauchy≈0.12) give aggressive local refinement and escape. Everything is budget-aware via safe_eval and archive pruning, with small restarts/perturbations to recover from stagnation and conservative defaults (sigma init ≈0.3·range) to balance exploration and exploitation.", "code": "import numpy as np\nfrom collections import deque\n\nclass LMCS:\n    \"\"\"\n    LMCS: Langevin-Memory CMA Subspace optimizer\n\n    One-line: CMA-style mirrored sampling and covariance adaptation augmented by\n    Langevin-style momentum proposals in small memory-biased subspaces, short\n    budget-aware 1D intensifications, per-coordinate RMS scaling and occasional\n    Cauchy jumps for escape.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 mem_size=8, model_every=25, p_cauchy=0.12):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.mem_size = int(mem_size)\n        self.model_every = int(model_every)\n        self.p_cauchy = float(p_cauchy)\n        # CMA-like defaults\n        self.lambda_ = max(6, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n        # set RNG\n        if seed is None:\n            self.rng = np.random.RandomState()\n        else:\n            self.rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        # CMA-like weights and constants\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        c1 = 2 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1 - c1, 2 * (mu_eff - 2 + 1 / mu_eff) / ((n + 2) ** 2 + mu_eff))\n        damps = 1 + 2 * max(0, np.sqrt((mu_eff - 1) / (n + 1)) - 1) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # state\n        m = self.rng.uniform(lb, ub).astype(float)\n        sigma = 0.3 * np.mean(rng_range) if np.mean(rng_range) > 0 else 0.3\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eig_every = max(1, int(10 * n))\n        eigen_counter = 0\n        gen_count = 0\n\n        # per-coordinate RMS scaling (starts near 1)\n        coord_var = np.ones(n) * 1e-3\n        coord_alpha = 0.18\n\n        # directional memory (LRU) for momentum & Cauchy jumps\n        mem = deque(maxlen=self.mem_size)\n\n        # archive for modeling / diagnostics\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # safe evaluation wrapper (clips, obeys budget, updates archive)\n        def safe_eval(x):\n            nonlocal evals, f_opt, x_opt\n            if evals >= budget:\n                return None, None\n            x_c = np.clip(x, lb, ub).astype(float)\n            try:\n                f = float(func(x_c))\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_X.append(x_c.copy())\n            archive_F.append(f)\n            if f < f_opt - 1e-12:\n                f_opt = float(f)\n                x_opt = x_c.copy()\n            return f, x_c\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            out = safe_eval(xm)\n            if out[0] is not None:\n                pass\n\n        # short, very budget-aware 1D golden-section-like line search along direction d\n        def short_line_search(x0, f0, d, init_step, max_evals=6):\n            nonlocal evals\n            if evals >= budget:\n                return None, None\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            dunit = d / dnrm\n            remaining = budget - evals\n            if remaining <= 0:\n                return None, None\n            s = float(abs(init_step)) if init_step != 0 else 0.5 * np.mean(rng_range)\n            # coarse probes +s, -s\n            xa = np.clip(x0 + s * dunit, lb, ub)\n            out = safe_eval(xa)\n            if out[0] is None:\n                return None, None\n            fa, xa = out\n            if fa < f0:\n                a, b = 0.0, s\n                fa_val, fb_val = f0, fa\n            else:\n                xb = np.clip(x0 - s * dunit, lb, ub)\n                out = safe_eval(xb)\n                if out[0] is None:\n                    return None, None\n                fb, xb = out\n                if fb < f0:\n                    a, b = -s, 0.0\n                    fa_val, fb_val = fb, f0\n                else:\n                    return None, None\n            # limited golden-section refinement\n            gr = (np.sqrt(5) - 1) / 2\n            c = b - gr * (b - a)\n            d_alpha = a + gr * (b - a)\n            xc = np.clip(x0 + c * dunit, lb, ub)\n            out = safe_eval(xc)\n            if out[0] is None:\n                return None, None\n            fc, xc = out\n            xd = np.clip(x0 + d_alpha * dunit, lb, ub)\n            out = safe_eval(xd)\n            if out[0] is None:\n                return None, None\n            fd, xd = out\n            best_f = f0\n            best_x = x0.copy()\n            for val, px in ((fa_val, xa if 'xa' in locals() else None),\n                            (fb_val, xb if 'xb' in locals() else None),\n                            (fc, xc), (fd, xd)):\n                try:\n                    if val is not None and val < best_f:\n                        best_f = val; best_x = px.copy()\n                except Exception:\n                    pass\n            # iterate limitedly\n            remaining = budget - evals\n            iters = 0\n            max_iters = min(max_evals, remaining)\n            while iters < max_iters and abs(b - a) > 1e-12:\n                iters += 1\n                if fc < fd:\n                    b = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    c = b - gr * (b - a)\n                    xc = np.clip(x0 + c * dunit, lb, ub)\n                    out = safe_eval(xc)\n                    if out[0] is None:\n                        break\n                    fc, xc = out\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    a = c\n                    c = d_alpha\n                    fc = fd\n                    d_alpha = a + gr * (b - a)\n                    xd = np.clip(x0 + d_alpha * dunit, lb, ub)\n                    out = safe_eval(xd)\n                    if out[0] is None:\n                        break\n                    fd, xd = out\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # helper: form a small memory-primed subspace basis (k approx n^(2/3) but clipped)\n        def subspace_basis(mem_list, k_hint=None):\n            if k_hint is None:\n                k = max(1, int(np.ceil(n ** (2.0 / 3.0))))\n            else:\n                k = max(1, int(min(n, k_hint)))\n            use_mem = min(len(mem_list), max(0, k // 2))\n            cols = []\n            # recent memory entries first\n            for i in range(use_mem):\n                cols.append(np.array(mem_list[i], dtype=float))\n            need = k - len(cols)\n            if need > 0:\n                R = self.rng.randn(n, need)\n                if cols:\n                    R = np.column_stack((np.column_stack(cols), R))\n                try:\n                    Q, _ = np.linalg.qr(R)\n                    basis = Q[:, :k]\n                except Exception:\n                    # fallback random orthonormal\n                    R2 = self.rng.randn(n, k)\n                    Q, _ = np.linalg.qr(R2)\n                    basis = Q[:, :k]\n            else:\n                R = np.column_stack(cols)\n                try:\n                    Q, _ = np.linalg.qr(R)\n                    basis = Q[:, :k]\n                except Exception:\n                    R2 = self.rng.randn(n, k)\n                    Q, _ = np.linalg.qr(R2)\n                    basis = Q[:, :k]\n            return basis\n\n        # main loop: generation-based similar to DMLSA but augmented with Langevin-memory subspace probes\n        while evals < budget:\n            remaining = budget - evals\n            planned = min(lam, remaining)\n            candidates = []\n            zlist = []\n            pairs = (planned + 1) // 2\n            for _ in range(pairs):\n                zlist.append(self.rng.randn(n))\n            # compute momentum from memory (average of most recent few)\n            momentum = np.zeros(n)\n            if len(mem) > 0:\n                # weighted recent memory as momentum\n                mcount = min(len(mem), 3)\n                for i in range(mcount):\n                    momentum += (mem[i] * (1.0 / (i + 1)))\n                momentum = momentum / (mcount + 1e-20)\n                # normalize moderate magnitude\n                mnorm = np.linalg.norm(momentum)\n                if mnorm > 0:\n                    momentum = momentum / mnorm\n            # create candidates using mirrored sampling and add a Langevin momentum component\n            for z in zlist:\n                if len(candidates) >= planned:\n                    break\n                # transform with current decomposition\n                # ensure B and D shapes consistent: B @ (D * z)\n                y = (B * D) @ z\n                # per-coordinate scaling\n                scaled_y = y * np.sqrt(coord_var + 1e-12)\n                # Langevin-style: combine scaled Gaussian direction with momentum\n                alpha_m = 0.55\n                y_comb = (1 - alpha_m) * scaled_y + alpha_m * (momentum * (np.linalg.norm(scaled_y) + 1e-12))\n                x = m + sigma * y_comb\n                # occasionally add small Langevin noise\n                x = x + (sigma * 0.08) * self.rng.randn(n)\n                # small probability to add a memory-subspace perturbation (subspace Langevin)\n                if (len(mem) >= 1) and (self.rng.rand() < 0.25):\n                    basis = subspace_basis(list(mem), k_hint=max(1, min(n, int(np.ceil(n ** 0.6)))))\n                    coeffs = self.rng.randn(basis.shape[1])\n                    dir_sub = basis @ coeffs\n                    dn = np.linalg.norm(dir_sub)\n                    if dn > 0:\n                        dir_sub = dir_sub / dn\n                        lam_len = sigma * (0.6 + 0.8 * self.rng.rand())\n                        x = x + lam_len * dir_sub\n                x = np.clip(x, lb, ub)\n                candidates.append(x)\n                if len(candidates) < planned:\n                    # mirrored counterpart\n                    z2 = -z\n                    y2 = (B * D) @ z2\n                    scaled_y2 = y2 * np.sqrt(coord_var + 1e-12)\n                    y_comb2 = (1 - alpha_m) * scaled_y2 + alpha_m * (momentum * (np.linalg.norm(scaled_y2) + 1e-12))\n                    x2 = m + sigma * y_comb2 + (sigma * 0.08) * self.rng.randn(n)\n                    x2 = np.clip(x2, lb, ub)\n                    candidates.append(x2)\n\n            # evaluate candidates (budget-respecting)\n            arfit = np.full(len(candidates), np.inf)\n            for k_idx, x in enumerate(candidates):\n                if evals >= budget:\n                    break\n                out = safe_eval(x)\n                if out[0] is None:\n                    break\n                f_val, x_c = out\n                arfit[k_idx] = f_val\n\n            valid = np.isfinite(arfit)\n            if not np.any(valid):\n                break\n            eval_count = int(np.sum(valid))\n            arx = np.array(candidates)[:eval_count]\n            arfit = arfit[:eval_count]\n            idx = np.argsort(arfit)\n            sel_idx = idx[:min(mu, eval_count)]\n            x_sel = arx[sel_idx]\n            # recombination to update mean\n            m_old = m.copy()\n            m = np.sum(weights[:len(sel_idx)][:, None] * x_sel, axis=0)\n\n            # compute y_sel and updates\n            y_sel = (x_sel - m_old) / (sigma + 1e-20)\n            y_w = np.sum(weights[:len(sel_idx)][:, None] * y_sel, axis=0)\n\n            # update evolution paths\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n            norm_ps = np.linalg.norm(ps)\n            denom = np.sqrt(1 - (1 - cs) ** (2 * (gen_count + 1)))\n            hsig = 1.0 if (norm_ps / (denom + 1e-20) / chi_n) < (1.4 + 2 / (n + 1)) else 0.0\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n\n            # covariance update\n            rank_one = np.outer(pc, pc)\n            rank_mu = np.zeros((n, n))\n            for i in range(len(sel_idx)):\n                yi = y_sel[i][:, None]\n                rank_mu += weights[i] * (yi @ yi.T)\n            C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n\n            # per-coordinate RMS-like adaptation from selected y_sel\n            sec_mom = np.sum(weights[:len(sel_idx)][:, None] * (y_sel ** 2), axis=0)\n            coord_var = (1 - coord_alpha) * coord_var + coord_alpha * (sec_mom + 1e-12)\n            coord_var = np.clip(coord_var, 1e-12, 1e6)\n\n            # sigma adaptation\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1))\n            sigma = max(sigma, 1e-12)\n\n            # update directional memory from weighted step if meaningful\n            yw_norm = np.linalg.norm(y_w)\n            if yw_norm > 1e-12:\n                mem.appendleft((y_w / yw_norm).copy())\n\n            # occasional short line-intensification along promising memory direction\n            if len(mem) > 0 and (self.rng.rand() < 0.18) and evals < budget:\n                ddir = mem[0].copy()\n                base_step = sigma * (0.8 + 0.8 * self.rng.rand())\n                # small budget for line search\n                ls_budget = min(6, max(1, budget - evals))\n                out_ls = short_line_search(m, f_opt if f_opt < np.inf else arfit.min(), ddir, init_step=base_step, max_evals=ls_budget)\n                if out_ls[0] is not None:\n                    f_ls, x_ls = out_ls\n                    if f_ls < f_opt - 1e-12:\n                        f_opt = f_ls\n                        x_opt = x_ls.copy()\n                        # accept as new mean to intensify\n                        m = x_ls.copy()\n                        sigma = max(sigma * 0.9, 1e-12)\n                        # add direction to memory\n                        step_dir = x_ls - m_old\n                        dn = np.linalg.norm(step_dir)\n                        if dn > 0:\n                            mem.appendleft((step_dir / dn).copy())\n\n            # occasional Cauchy jump along a memory direction to escape\n            if len(mem) > 0 and (self.rng.rand() < self.p_cauchy) and evals < budget:\n                u = mem[self.rng.randint(len(mem))]\n                jump = np.tan(np.pi * (self.rng.rand() - 0.5))\n                x_jump = np.clip(m + (sigma * 1.0) * jump * u, lb, ub)\n                out = safe_eval(x_jump)\n                if out[0] is not None:\n                    fj, xj = out\n                    if fj < f_opt - 1e-12:\n                        f_opt = fj\n                        x_opt = xj.copy()\n                        m = xj.copy()\n                        sigma = max(sigma, 0.2 * np.mean(rng_range))\n\n            # occasional small subspace-focused probe using memory basis for intensification\n            if (gen_count % max(1, int(max(3, n // 2)))) == 0 and evals < budget and len(mem) >= 1:\n                basis = subspace_basis(list(mem), k_hint=max(1, int(np.ceil(n ** 0.6))))\n                # sample a few coefficients and test along that subspace\n                for _ in range(min(4, max(1, budget - evals))):\n                    coeffs = self.rng.normal(scale=1.0, size=basis.shape[1])\n                    dir_sub = basis @ coeffs\n                    dn = np.linalg.norm(dir_sub)\n                    if dn == 0:\n                        continue\n                    dir_sub = dir_sub / dn\n                    length = sigma * (0.4 + 0.8 * self.rng.rand())\n                    x_try = np.clip(m + length * dir_sub, lb, ub)\n                    out = safe_eval(x_try)\n                    if out[0] is None:\n                        break\n                    f_try, xt = out\n                    if f_try < f_opt - 1e-12:\n                        f_opt = f_try\n                        x_opt = xt.copy()\n                        m = xt.copy()\n                        sigma = min(sigma * 1.15, max(1e-12, 5.0 * np.mean(rng_range)))\n                        # store direction\n                        mem.appendleft(dir_sub.copy())\n\n            # eigen decomposition occasionally\n            eigen_counter += max(1, int(np.sum(valid)))\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # archive pruning to control memory\n            max_archive = max(2000, 50 * n)\n            if len(archive_X) > max_archive:\n                idx_sorted = np.argsort(archive_F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                if len(rest_idx) > 0:\n                    keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                    keep_idx = np.concatenate([keep_best, keep_rest])\n                else:\n                    keep_idx = keep_best\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            gen_count += 1\n\n            # small restart if stagnation (no improvement for long)\n            if gen_count % max(50, int(10 + 2 * n)) == 0:\n                # perturb around best if exists\n                if x_opt is not None and self.rng.rand() < 0.5:\n                    perturb = self.rng.randn(n) * (0.3 * np.mean(rng_range))\n                    m = np.clip(x_opt + perturb, lb, ub)\n                    out = safe_eval(m)\n                    if out[0] is not None:\n                        fm, xm = out\n                        if fm < f_opt - 1e-12:\n                            f_opt = fm\n                            x_opt = xm.copy()\n                        # clear memory to diversify\n                        mem.clear()\n                else:\n                    # mild global random walk to diversify\n                    m = np.clip(self.rng.uniform(lb, ub), lb, ub)\n\n            # quick bailout if extremely good\n            if f_opt <= 1e-12:\n                break\n\n        # final: return best seen (or current mean if none)\n        if x_opt is None:\n            # fall back to last m or any archive\n            if len(archive_X) > 0:\n                return float(archive_F[np.argmin(archive_F)]), np.array(archive_X[np.argmin(archive_F)], dtype=float)\n            else:\n                return float(np.inf), np.clip(m, lb, ub)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 76, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem = deque(maxlen=self.mem_size)", "error": "In the code, line 76, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem = deque(maxlen=self.mem_size)", "parent_ids": "e0803be8-3b99-4e93-af4c-1f31db4bf8ee", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "39c151f0-c5c6-4677-8eba-1211a8aa6dc4", "fitness": 0.39226597571933464, "name": "ACNE", "description": "1) ACNE maintains an archive of recent evaluated points (default ~4*dim) and builds a low-rank PCA subspace from that archive so search, modeling and steps are performed in a reduced, data-driven coordinate system.  \n2) It samples an ensemble of anisotropic probes in the subspace using variance inferred from archived projections and a small mixture of scale factors, maps probes to the full space, clips to bounds and records budget-safe evaluations.  \n3) Using collected probes (plus the center) it fits a weighted least-squares quadratic surrogate in the subspace (or a linear surrogate if data are scarce), extracts g and H, regularizes H and solves H s = −g for a Newton-like step followed by safeguarded line-search/backtracking.  \n4) The algorithm adapts a trust-region radius (expand on success, shrink on failure), prunes the archive for diversity via repulsive selection, uses randomized jitter/diversification and staged restarts on stagnation, and always respects the evaluation budget.", "code": "import numpy as np\n\nclass ACNE:\n    \"\"\"\n    Adaptive Covariance-guided Newton Ensemble (ACNE)\n\n    Main ideas / novel mechanisms:\n    - Maintain an archive of recent evaluated points; build a low-rank subspace\n      from the archive covariance (PCA). Work in that subspace for modeling.\n    - In each iteration sample an ensemble of anisotropic probes from the\n      covariance-inferred subspace (variance scaled by eigenvalues and a trust\n      signal). Collect probes into the archive.\n    - Fit a low-dimensional quadratic model (f ≈ c + g^T s + 0.5 s^T H s) in the\n      subspace by weighted least squares using only ensemble + center points.\n      Extract g and H, regularize H and solve H s = -g for a Newton-like step.\n    - Take a safeguarded line-search/backtracking along the model step; if it\n      fails, shrink trust-region radius and fall back to best probe.\n    - Adaptive trust-region radius and archive management (repulsive pruning),\n      randomized global diversifications and budget-safe evaluations.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_size=None, min_step=1e-9, init_radius=1.0):\n        \"\"\"\n        budget: max number of function evaluations\n        dim: problem dimensionality\n        seed: RNG seed (optional)\n        archive_size: maximum archive size (default ~4*dim)\n        min_step: minimal radius\n        init_radius: initial trust-region radius in absolute coordinates\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.archive_size = int(archive_size) if archive_size is not None else max(20, 4 * self.dim)\n        self.min_step = float(min_step)\n        self.init_radius = float(init_radius)\n\n    def __call__(self, func):\n        n = self.dim\n\n        # bounds: ensure arrays of length n\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        lb = lb.reshape(n,)\n        ub = ub.reshape(n,)\n\n        # safe evaluation wrapper and bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x.copy()\n\n        # initialize current point uniformly in domain (Many Affine BBOB: typically -5..5)\n        x_cur = self.rng.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.asarray(x_best, dtype=float)\n        x_cur = np.asarray(x_cur, dtype=float)\n        f_cur = float(f_cur)\n\n        # initial radius relative to domain size (absolute)\n        domain_size = np.mean(ub - lb)\n        radius = max(self.init_radius * domain_size, 1e-12)\n        radius = min(radius, max(1.0, domain_size))\n        radius = max(radius, self.min_step)\n\n        # archive: list of (x, f). Start with current\n        archive_x = [x_cur.copy()]\n        archive_f = [f_cur]\n\n        # stagnation control\n        no_improve = 0\n        stagnation_limit = max(12, int(10 + np.log1p(n)))\n        restarts = 0\n        max_restarts = 6\n\n        # helper: prune archive for diversity (repulsive removal)\n        def prune_archive(max_size):\n            if len(archive_x) <= max_size:\n                return\n            # compute pairwise distances to prefer keeping diverse and good points\n            X = np.vstack(archive_x)\n            best_idx = np.argsort(archive_f)[: max_size // 2]  # keep the better half\n            keep = set(best_idx.tolist())\n            # greedy add by maximizing min distance to kept points\n            remaining = [i for i in range(len(archive_x)) if i not in keep]\n            dists = np.linalg.norm(X[:, None, :] - X[None, :, :], axis=2)\n            while len(keep) < max_size:\n                # for each candidate compute distance to current kept set\n                cand_scores = []\n                for i in remaining:\n                    if not keep:\n                        cand_scores.append((i, np.inf))\n                    else:\n                        mind = np.min([dists[i, j] for j in keep])\n                        cand_scores.append((i, mind))\n                if not cand_scores:\n                    break\n                # pick candidate with largest min-distance\n                pick = max(cand_scores, key=lambda z: z[1])[0]\n                keep.add(pick)\n                remaining.remove(pick)\n            keep = sorted(list(keep))\n            # reduce archive\n            archive_x[:] = [archive_x[i] for i in keep]\n            archive_f[:] = [archive_f[i] for i in keep]\n\n        # helper: build PCA subspace from archive (n x k)\n        def build_subspace(k):\n            if len(archive_x) >= 2:\n                M = np.vstack(archive_x)  # (m, n)\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                # compute covariance (n x n) via small-m trick if m < n\n                try:\n                    U, S, Vt = np.linalg.svd(Mc, full_matrices=False)\n                    pcs = Vt.T  # n x r\n                    r = pcs.shape[1]\n                    take = min(k, r)\n                    basis = pcs[:, :take]\n                    if take < k:\n                        # add random orthonormal supplements\n                        R = self.rng.standard_normal((n, k - take))\n                        stacked = np.column_stack((basis, R))\n                        Q, _ = np.linalg.qr(stacked)\n                        return Q[:, :k]\n                    else:\n                        Q, _ = np.linalg.qr(basis)\n                        return Q[:, :k]\n                except np.linalg.LinAlgError:\n                    R = self.rng.standard_normal((n, k))\n                    Q, _ = np.linalg.qr(R)\n                    return Q[:, :k]\n            else:\n                R = self.rng.standard_normal((n, k))\n                Q, _ = np.linalg.qr(R)\n                return Q[:, :k]\n\n        # helper: fit quadratic model in subspace if enough samples\n        def fit_quadratic_subspace(points_s, points_f, center_f):\n            # points_s: list of k-dim displacement vectors s = B^T (x - x_center)\n            # returns (g (k,), H (k,k)) or (None, None) if ill-conditioned / insufficient\n            m = len(points_s)\n            if m < 1:\n                return None, None\n            # build design matrix with basis functions:\n            # phi = [1, s_i (k), 0.5*s_i^2 (k diag), s_i*s_j (i<j) -> k*(k-1)/2]\n            s0 = np.asarray(points_s)\n            k = s0.shape[1]\n            p = 1 + 2 * k + (k * (k - 1)) // 2\n            Phi = np.zeros((m, p))\n            for idx in range(m):\n                s = s0[idx]\n                row = []\n                row.append(1.0)\n                row.extend(s.tolist())\n                row.extend((0.5 * (s * s)).tolist())  # diag terms corresponding to 0.5*H_ii\n                # off-diagonal products\n                for i in range(k):\n                    for j in range(i + 1, k):\n                        row.append(s[i] * s[j])\n                Phi[idx, :] = np.asarray(row)\n            # target y = f - center_f (center shift reduces constant intercept scale issues)\n            y = np.asarray(points_f) - float(center_f)\n            # weights: prefer points with lower f (heuristic), also closer in s-norm\n            fstd = max(1e-8, np.std(points_f))\n            w = np.exp(-0.5 * (y / (fstd if fstd > 0 else 1.0)) ** 2)\n            # small distance weighting\n            s_norms = np.linalg.norm(s0, axis=1)\n            dscale = np.mean(s_norms) + 1e-12\n            w *= np.exp(- (s_norms / max(1e-8, 2.0 * dscale)))\n            W = np.sqrt(w)  # multiply rows by sqrt weights\n            A = (W[:, None] * Phi)\n            b = W * y\n            # regularized least-squares\n            reg = 1e-8 + 1e-6 * np.mean(np.abs(b))\n            try:\n                sol, *_ = np.linalg.lstsq(A.T @ A + reg * np.eye(p), A.T @ b, rcond=None)\n            except np.linalg.LinAlgError:\n                return None, None\n            # extract g and H\n            # sol indices: 0 -> c (constant), 1..k -> g, next k entries -> 0.5*H_ii, remainder -> H_ij (i<j)\n            g = np.asarray(sol[1:1 + k], dtype=float)\n            H = np.zeros((k, k), dtype=float)\n            # diag\n            offset = 1 + k\n            for i in range(k):\n                H[i, i] = float(sol[offset + i])  # these were 0.5*H_ii in design? Actually we used 0.5*s_i^2 so sol contains H_ii\n            # off-diagonals\n            out = offset + k\n            for i in range(k):\n                for j in range(i + 1, k):\n                    H[i, j] = float(sol[out])\n                    H[j, i] = H[i, j]\n                    out += 1\n            return g, H\n\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            # choose subspace dim k (small, grows slowly with n and archive info)\n            k = max(1, min(n, int(min(n, max(2, int(np.clip(int(np.sqrt(n)) + 1 + int(len(archive_x) / max(1, n/4)), 2, n)))))))\n            B = build_subspace(k)  # n x k\n\n            # create ensemble probes in subspace\n            # infer anisotropic variances from archive projection\n            var_coef = np.ones(k)\n            if len(archive_x) >= 3:\n                M = np.vstack(archive_x)\n                Sproj = (M - np.mean(M, axis=0)) @ B  # (m,k)\n                var_coef = np.var(Sproj, axis=0) + 1e-12\n                # boost directions that historically improved (archive of good points)\n                good_mask = np.array(archive_f) <= (np.percentile(archive_f, max(10, min(50, 10 + len(archive_x)//5))))\n                if np.any(good_mask):\n                    var_coef *= (1.0 + 0.5 * np.mean(np.var(Sproj[good_mask], axis=0) / (var_coef + 1e-12)))\n\n            # number of probes this round: try to budget at least small reserves for line-search (3 each)\n            probes = min(max(6, 4 * k), max(1, remaining - 3))\n            probes = max(1, probes)\n\n            # ensemble storage for modeling\n            s_list = []\n            f_list = []\n            x_list = []\n\n            improved_round = False\n            best_probe_idx = None\n            best_probe_f = np.inf\n\n            # center for model is current x_cur\n            x_center = x_cur.copy()\n            f_center = f_cur\n\n            # sample probes\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample coefficients anisotropically\n                coeffs = self.rng.normal(scale=np.sqrt(var_coef))\n                # scale to target radius with some randomness (mixture of scales)\n                scale_factor = self.rng.choice([0.25, 0.5, 1.0, 1.5], p=[0.2, 0.4, 0.3, 0.1])\n                s = coeffs * scale_factor\n                # map to full space\n                d = B @ s\n                nd = np.linalg.norm(d)\n                if nd == 0:\n                    continue\n                # normalize direction and scale to radius but preserve relative s magnitudes\n                if np.linalg.norm(s) > 0:\n                    d = d / (np.linalg.norm(d) + 1e-20) * (radius * (0.5 + 0.5 * self.rng.random()))\n                x_try = clip(x_center + d)\n                f_try, x_try = safe_eval(x_try)\n                if f_try is None:\n                    break\n                # store for modeling: s relative to subspace coords (projected from x_try - x_center)\n                s_proj = B.T @ (x_try - x_center)\n                s_list.append(s_proj)\n                f_list.append(f_try)\n                x_list.append(x_try.copy())\n                # track best probe\n                if f_try < best_probe_f:\n                    best_probe_f = f_try\n                    best_probe_idx = len(x_list) - 1\n                # immediate accept if strong improvement\n                if f_try < f_cur - 1e-12:\n                    # accept probe and attempt a short safeguarded line-refinement along d direction\n                    # choose step length proportional to improvement and radius\n                    # do a simple backtracking with factors\n                    factors = [1.0, 0.6, 0.3, 0.15, 0.075]\n                    accepted = False\n                    for fac in factors:\n                        if evals >= self.budget:\n                            break\n                        x_trial = clip(x_center + fac * d)\n                        f_trial, x_trial = safe_eval(x_trial)\n                        if f_trial is None:\n                            break\n                        if f_trial < f_cur - 1e-12:\n                            # accept and update\n                            x_old = x_cur.copy()\n                            x_cur = x_trial.copy()\n                            f_cur = float(f_trial)\n                            archive_x.insert(0, x_cur.copy())\n                            archive_f.insert(0, f_cur)\n                            if len(archive_x) > self.archive_size * 2:\n                                prune_archive(self.archive_size)\n                            # expand radius mildly on success\n                            radius = min(radius * 1.15 + 0.05 * domain_size, 5.0 * domain_size)\n                            improved_round = True\n                            no_improve = 0\n                            accepted = True\n                            break\n                    if accepted:\n                        # break the probe loop to focus on model next iteration\n                        break\n\n            # Add best probe into archive explicitly (if not already accepted)\n            if best_probe_idx is not None and not improved_round:\n                archive_x.insert(0, x_list[best_probe_idx].copy())\n                archive_f.insert(0, f_list[best_probe_idx])\n            # prune archive to keep size and diversity\n            prune_archive(self.archive_size)\n\n            # Attempt to build quadratic model in the k-dimensional subspace using points collected plus center\n            # We need at least p_min samples\n            # For modeling include the center and all collected probes projected s = B^T (x - x_center)\n            model_s = []\n            model_f = []\n            # include center\n            model_s.append(np.zeros(k))\n            model_f.append(f_center)\n            for si, fi in zip(s_list, f_list):\n                model_s.append(si)\n                model_f.append(fi)\n            # require minimal samples for unique fit (parms = 1 + k + k*(k+1)/2)\n            p_params = 1 + k + (k * (k + 1)) // 2\n            if len(model_s) >= max(3, min(len(model_s), p_params // 2)) and len(model_s) >= min(len(model_s), p_params):\n                # attempt fit if we have at least ~p_params/2 samples (allow overdetermined if possible)\n                if len(model_s) >= p_params:\n                    g, H = fit_quadratic_subspace(model_s, model_f, f_center)\n                else:\n                    # if insufficient samples for full quadratic, try to fit linear surrogate (g only)\n                    # simple linear least squares: f = c + g^T s\n                    S = np.vstack(model_s)\n                    Y = np.asarray(model_f) - f_center\n                    # weight by closeness\n                    w = np.exp(- np.linalg.norm(S, axis=1) / (np.mean(np.linalg.norm(S, axis=1)) + 1e-12))\n                    A = (w[:, None] * S)\n                    b = w * Y\n                    # solve for g by least squares\n                    try:\n                        g_lin, *_ = np.linalg.lstsq(A, b, rcond=None)\n                        g = g_lin\n                        H = np.eye(k) * (1e-4)  # tiny curvature regularizer\n                    except np.linalg.LinAlgError:\n                        g, H = None, None\n                if g is not None and H is not None:\n                    # regularize H to ensure positive-definite-ish\n                    lam = 1e-6 + 1e-3 * np.mean(np.abs(g))\n                    H_reg = 0.5 * (H + H.T) + lam * np.eye(k)\n                    # attempt to solve H_reg s = -g for Newton-like step\n                    try:\n                        s_star = np.linalg.solve(H_reg, -g)\n                    except np.linalg.LinAlgError:\n                        # fallback to pseudo-inverse\n                        try:\n                            s_star = -np.linalg.pinv(H_reg) @ g\n                        except Exception:\n                            s_star = None\n                    if s_star is not None and np.all(np.isfinite(s_star)):\n                        # map to full space step\n                        d_full = B @ s_star\n                        nd = np.linalg.norm(d_full)\n                        if nd > 1e-20:\n                            # cap step length to trust-region radius\n                            if nd > 4.0 * radius:\n                                d_full = d_full / nd * (4.0 * radius)\n                                nd = np.linalg.norm(d_full)\n                            # perform line-search/backtracking along d_full but budget-safe\n                            factors = [1.0, 0.7, 0.4, 0.2, 0.1]\n                            accepted_model = False\n                            for fac in factors:\n                                if evals >= self.budget:\n                                    break\n                                x_trial = clip(x_center + fac * d_full)\n                                f_trial, x_trial = safe_eval(x_trial)\n                                if f_trial is None:\n                                    break\n                                if f_trial < f_cur - 1e-12:\n                                    # accept model step\n                                    x_old = x_cur.copy()\n                                    x_cur = x_trial.copy()\n                                    f_cur = float(f_trial)\n                                    archive_x.insert(0, x_cur.copy())\n                                    archive_f.insert(0, f_cur)\n                                    # successful step increases radius\n                                    radius = min(radius * (1.0 + 0.2 * fac) + 0.05 * domain_size, 5.0 * domain_size)\n                                    improved_round = True\n                                    no_improve = 0\n                                    accepted_model = True\n                                    break\n                                # small probe bookkeeping: even if not improving, add to archive for model learning\n                                archive_x.insert(0, x_trial.copy())\n                                archive_f.insert(0, f_trial)\n                            if not accepted_model:\n                                # fallback: try best probe found previously (already added)\n                                radius = max(radius * 0.8, self.min_step)\n                                no_improve += 1\n                        else:\n                            # degenerate model step; shrink radius\n                            radius = max(radius * 0.8, self.min_step)\n                            no_improve += 1\n                    else:\n                        # couldn't compute s_star; shrink and continue\n                        radius = max(radius * 0.85, self.min_step)\n                        no_improve += 1\n                else:\n                    # model not fitted properly\n                    radius = max(radius * 0.9, self.min_step)\n                    no_improve += 1\n            else:\n                # insufficient data for modeling: fallback to exploring best probe or random jitter\n                if best_probe_idx is not None and best_probe_f < f_cur - 1e-12:\n                    # accept best probe\n                    x_cur = archive_x[0].copy()\n                    f_cur = archive_f[0]\n                    radius = min(radius * 1.08 + 0.02 * domain_size, 5.0 * domain_size)\n                    improved_round = True\n                    no_improve = 0\n                else:\n                    # random local jitter around current\n                    if (self.rng.random() < 0.25) and (evals < self.budget):\n                        d = self.rng.standard_normal(n)\n                        d /= (np.linalg.norm(d) + 1e-20)\n                        step_len = radius * (0.2 + 0.8 * self.rng.random())\n                        x_try = clip(x_cur + d * step_len)\n                        f_try, x_try = safe_eval(x_try)\n                        if f_try is None:\n                            break\n                        archive_x.insert(0, x_try.copy())\n                        archive_f.insert(0, f_try)\n                        if f_try < f_cur - 1e-12:\n                            x_cur = x_try.copy()\n                            f_cur = float(f_try)\n                            radius = min(radius * 1.07 + 0.01 * domain_size, 5.0 * domain_size)\n                            improved_round = True\n                            no_improve = 0\n                        else:\n                            radius = max(radius * 0.93, self.min_step)\n                            no_improve += 1\n                    else:\n                        radius = max(radius * 0.92, self.min_step)\n                        no_improve += 1\n\n            # ensure archive bounded and pruned\n            prune_archive(self.archive_size)\n\n            # Stagnation and restart logic\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final tiny-local polish until budget exhausted\n                    while evals < self.budget:\n                        d = self.rng.standard_normal(n)\n                        d /= (np.linalg.norm(d) + 1e-20)\n                        alpha = self.rng.uniform(-0.15 * radius, 0.15 * radius)\n                        x_try = clip(x_cur + alpha * d)\n                        f_try, x_try = safe_eval(x_try)\n                        if f_try is None:\n                            break\n                        if f_try < f_cur - 1e-12:\n                            x_cur = x_try.copy()\n                            f_cur = f_try\n                            archive_x.insert(0, x_cur.copy())\n                            archive_f.insert(0, f_cur)\n                        radius = max(radius * 0.98, self.min_step)\n                    break\n                else:\n                    # randomized restart around best\n                    if x_best is not None:\n                        scale = domain_size * min(1.0, 0.4 + 0.2 * restarts)\n                        x_restart = clip(x_best + self.rng.standard_normal(n) * scale)\n                        f_restart, x_restart = safe_eval(x_restart)\n                        if f_restart is None:\n                            break\n                        x_cur = x_restart.copy()\n                        f_cur = float(f_restart)\n                        archive_x.insert(0, x_cur.copy())\n                        archive_f.insert(0, f_cur)\n                    else:\n                        # random global restart\n                        x_restart = self.rng.uniform(lb, ub)\n                        f_restart, x_restart = safe_eval(x_restart)\n                        if f_restart is None:\n                            break\n                        x_cur = x_restart.copy()\n                        f_cur = float(f_restart)\n                        archive_x.insert(0, x_cur.copy())\n                        archive_f.insert(0, f_cur)\n                    # inflate radius to escape basins\n                    radius = min(max(radius * (1.5 + 0.1 * restarts), self.min_step), 5.0 * domain_size)\n                    no_improve = 0\n                    # partially forget archive (decay)\n                    keep_count = max(3, len(archive_x) // 3)\n                    archive_x = archive_x[:keep_count]\n                    archive_f = archive_f[:keep_count]\n\n            # tiny safeguard lower bound on radius\n            if radius < self.min_step:\n                radius = self.min_step\n\n            # early perfect solution check\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            x_best = x_cur.copy()\n            f_best = float(f_cur)\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ACNE scored 0.392 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "d3bac529-3666-4b22-8add-4fb85d47bfb4", "operator": null, "metadata": {"aucs": [0.13932182493335776, 0.18779207307888868, 0.45086352909409955, 0.8914590574954127, 0.43984749951042157, 0.5391274600165388, 0.27499340873203104, 0.4108610198908934, 0.4113752342576381, 0.1770186501840646]}, "task_prompt": ""}
{"id": "807c8fca-e631-47c7-af71-c592445126aa", "fitness": "-inf", "name": "SDMAC", "description": "SDMAC combines a short memory of recent successful unit-step directions (memory_size=20) with an online exponential-moving-average covariance (cov_lr≈0.15) to capture both local directional successes and global anisotropy, and it builds small orthonormal subspaces by mixing top covariance eigenvectors and PCA of memory for focused sampling (subspace dimension k ≈ sqrt(n)+1). Sampling in the subspace uses Gaussian coefficients scaled by the square roots of top covariance eigenvalues and biased by soft mem_scores (tanh-based score_bias), performs multiple multi-scale probes per round (probes ≈ max(6,4*k)) and immediately accepts improving moves. On an improving probe the algorithm updates mem_steps, inflates mem_scores, updates the covariance with the outer-product of the actual displacement, grows the absolute step multiplicatively (growth≈1.15) and occasionally does a cheap one-point parabolic line refinement (midpoint evaluation) to gain extra improvement. Failed rounds trigger gentle mem_scores decay, step shrinkage (shrink≈0.75) and stagnation-controlled randomized restarts around the best-known point (max_restarts), with strict budget accounting, bound clipping and a final tiny-direction polish when restarts are exhausted.", "code": "import numpy as np\n\nclass SDMAC:\n    \"\"\"\n    SDMAC: Stochastic Directional Memory with Adaptive Covariance\n\n    Main ideas:\n    - Keep a short memory of recent successful unit steps and softmax scores.\n    - Maintain an online covariance (EMA) of successful step vectors to capture\n      global anisotropy; combine its principal axes with memory PCA to build\n      small subspaces for directional sampling.\n    - In each round, perform multiple multi-scale probes inside the subspace,\n      sampling coefficients using eigenvalue-scaled Gaussians biased by memory scores.\n    - Immediately accept improving probes and attempt a cheap parabolic refinement\n      (at most one extra evaluation) to gain additional improvement.\n    - Adapt a single absolute step size multiplicatively with more aggressive\n      growth and moderate shrinkage; update covariance and memory on success,\n      decay scores on failures, and do randomized restarts after stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=20, init_step_factor=0.6, min_step=1e-9,\n                 cov_lr=0.15, growth=1.15, shrink=0.75, max_restarts=5):\n        \"\"\"\n        budget: maximum function evaluations\n        dim: dimension\n        seed: optional RNG seed\n        memory_size: number of recent step directions to keep\n        init_step_factor: fraction of domain mean used to initialize absolute step\n        min_step: minimal absolute step\n        cov_lr: EMA learning rate for covariance update on success (0..1)\n        growth: multiplicative step growth on success (>1)\n        shrink: multiplicative step shrink on failure (<1)\n        max_restarts: how many randomized restarts allowed before final polish\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.memory_size = int(memory_size)\n        self.init_step_factor = float(init_step_factor)\n        self.min_step_param = float(min_step)\n        self.cov_lr = float(cov_lr)\n        self.growth = float(growth)\n        self.shrink = float(shrink)\n        self.max_restarts = int(max_restarts)\n\n    def __call__(self, func):\n        n = self.dim\n        # read bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        lb = lb.reshape(n,)\n        ub = ub.reshape(n,)\n\n        evals = 0\n\n        def clip_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initial random starting point\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.asarray(x_best, dtype=float)\n        x_cur = np.asarray(x_cur, dtype=float)\n        f_cur = float(f_cur)\n\n        domain_size = np.mean(ub - lb)\n        step = max(self.init_step_factor * domain_size, 1e-12)\n        min_step = max(self.min_step_param * max(1.0, domain_size), 1e-12)\n\n        # memory of recent successful unit steps + soft scores\n        mem_steps = []   # list of (n,)\n        mem_scores = []  # floats (higher => preferred)\n\n        # online covariance estimate (EMA) of successful step vectors (outer product)\n        # initialize as small isotropic\n        cov = np.eye(n) * (0.08 * domain_size) ** 2\n        cov_eps = 1e-12\n\n        # control parameters\n        no_improve = 0\n        stagnation_limit = max(10, int(6 + np.log1p(n)))\n        restarts = 0\n\n        # small utility: build k-dim orthonormal basis combining cov-eigs and mem-PCA\n        def build_subspace(k):\n            # extract top eigenvectors of cov\n            try:\n                evals_cov, evecs = np.linalg.eigh(cov + cov_eps * np.eye(n))\n            except np.linalg.LinAlgError:\n                evecs = np.random.randn(n, n)\n                evecs, _ = np.linalg.qr(evecs)\n                evals_cov = np.ones(n)\n            # sort descending\n            idx = np.argsort(evals_cov)[::-1]\n            evecs = evecs[:, idx]\n            # memory PCA if available\n            if len(mem_steps) >= 2:\n                M = np.vstack(mem_steps)\n                Mc = M - M.mean(axis=0, keepdims=True)\n                try:\n                    U, S, Vt = np.linalg.svd(Mc, full_matrices=False)\n                    mem_pcs = Vt.T  # n x r\n                except np.linalg.LinAlgError:\n                    mem_pcs = np.empty((n, 0))\n            else:\n                mem_pcs = np.empty((n, 0))\n            # stack cov_evecs and mem pcs, add some random vectors for diversity\n            stacked = np.column_stack([evecs[:, :min(n, k)], mem_pcs[:, :min(mem_pcs.shape[1], k)]])\n            # append a few random directions to ensure full rank\n            if stacked.shape[1] < k:\n                R = np.random.randn(n, k - stacked.shape[1])\n                stacked = np.column_stack([stacked, R])\n            # orthonormalize and return first k columns\n            Q, _ = np.linalg.qr(stacked)\n            return Q[:, :k]\n\n        # cheap one-point parabolic refinement using f0 and f1; evaluate midpoint only if budget allows\n        def cheap_parabola_line(x0, f0, d, alpha1, f1):\n            # inputs: tested x1 = x0 + alpha1 * d with known f1\n            # attempt to evaluate x_mid = x0 + 0.5*alpha1*d and fit parabola through (0,f0),(alpha1/2,f_mid),(alpha1,f1)\n            if self.budget - evals <= 0:\n                return None, None\n            if abs(alpha1) < 1e-20 or np.linalg.norm(d) == 0:\n                return None, None\n            if self.budget - evals < 1:\n                return None, None\n            x_mid = clip_to_bounds(x0 + 0.5 * alpha1 * d)\n            f_mid, x_mid = safe_eval(x_mid)\n            if f_mid is None:\n                return None, None\n            # fit parabola through a=0, b=alpha1/2, c=alpha1\n            a = 0.0; fa = f0\n            b = 0.5 * alpha1; fb = f_mid\n            c = alpha1; fc = f1\n            # compute coefficients for parabola f(alpha) = A*alpha^2 + B*alpha + C\n            denom = (a - b) * (a - c) * (b - c)\n            if abs(denom) < 1e-20:\n                return None, None\n            A = (a * (fb - fc) + b * (fc - fa) + c * (fa - fb)) / denom\n            B = (a*a * (fc - fb) + b*b * (fa - fc) + c*c * (fb - fa)) / denom\n            # parabola minimizer\n            if A <= 0 or np.isclose(A, 0.0):\n                # not convex, return best of samples if better than f0\n                best_f = f0; best_x = x0.copy()\n                if f_mid < best_f:\n                    best_f = f_mid; best_x = x_mid.copy()\n                if f1 < best_f:\n                    # reconstruct x1\n                    x1 = clip_to_bounds(x0 + alpha1 * d)\n                    best_f = f1; best_x = x1.copy()\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            alpha_star = -B / (2.0 * A)\n            # accept only if within [min(alpha range)*0.3, max*3] to avoid wild steps\n            low = min(a, c); high = max(a, c)\n            width = high - low\n            if width <= 0:\n                return None, None\n            if alpha_star < low - 0.5 * width or alpha_star > high + 2.0 * width:\n                return None, None\n            # evaluate at alpha_star if budget allows\n            if self.budget - evals <= 0:\n                return None, None\n            x_s = clip_to_bounds(x0 + alpha_star * d)\n            f_s, x_s = safe_eval(x_s)\n            if f_s is None:\n                return None, None\n            if f_s < f0 - 1e-12:\n                return f_s, x_s\n            # otherwise, maybe return best sampled\n            best_f = f0; best_x = x0.copy()\n            if f_mid < best_f:\n                best_f = f_mid; best_x = x_mid.copy()\n            if f1 < best_f:\n                x1 = clip_to_bounds(x0 + alpha1 * d)\n                best_f = f1; best_x = x1.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # main loop: rounds of subspace probes until budget used\n        while evals < self.budget:\n            # adapt subspace dimension: use a modest size depending on n\n            k = min(n, max(2, int(round(np.sqrt(n) + 1))))\n            basis = build_subspace(k)  # n x k\n\n            # extract top k eigenvalues to scale coefficient sampling\n            try:\n                evals_cov, evecs = np.linalg.eigh(cov + cov_eps * np.eye(n))\n                evals_cov = np.maximum(evals_cov, 1e-16)\n                idx = np.argsort(evals_cov)[::-1]\n                top_vals = evals_cov[idx][:k]\n            except np.linalg.LinAlgError:\n                top_vals = np.ones(k)\n\n            # form sampling scales per basis coordinate\n            # bias scales by mem_scores if available (soft preference)\n            if mem_scores:\n                scores = np.array(mem_scores[:k], dtype=float)\n                # normalize scores to be roughly between 0 and 1 via logistic-like transform\n                score_bias = 1.0 + (np.tanh(np.array(scores) / (np.std(scores) + 1e-12)) * 0.5)\n                scales = np.sqrt(top_vals) * score_bias[:k]\n            else:\n                scales = np.sqrt(top_vals) + 1e-8\n\n            # number of probes: somewhat larger than ADES to improve exploration\n            probes = max(6, 4 * k)\n\n            improved_round = False\n            round_candidates = []\n\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample coefficients with variance proportional to scales^2\n                coeffs = np.random.randn(k) * (scales + 1e-12)\n                d = basis @ coeffs\n                nd = np.linalg.norm(d)\n                if nd == 0:\n                    continue\n                d = d / nd\n\n                # multi-scale alpha: small / medium / large with probabilities\n                r = np.random.rand()\n                if r < 0.25:\n                    mult = 0.25\n                elif r < 0.8:\n                    mult = 1.0\n                else:\n                    mult = 3.0\n                alpha = step * mult * (np.random.uniform(-1.0, 1.0))\n                x_try = clip_to_bounds(x_cur + alpha * d)\n                f_try, x_try = safe_eval(x_try)\n                if f_try is None:\n                    break\n                round_candidates.append((f_try, x_try.copy(), d.copy(), alpha))\n\n                # immediate accept and refinement\n                if f_try < f_cur - 1e-12:\n                    # try cheap parabola using midpoint refinement\n                    f_refined, x_refined = cheap_parabola_line(x_cur, f_cur, d, alpha, f_try)\n                    if f_refined is not None and f_refined < f_try - 1e-12:\n                        f_try = f_refined\n                        x_try = x_refined.copy()\n                    # accept\n                    delta_vec = x_try - x_cur\n                    nv = np.linalg.norm(delta_vec)\n                    if nv > 0:\n                        unit_step = (delta_vec / nv).copy()\n                        mem_steps.insert(0, unit_step)\n                        mem_scores.insert(0, 1.0)  # fresh positive score\n                        if len(mem_steps) > self.memory_size:\n                            mem_steps.pop()\n                            mem_scores.pop()\n                        # update covariance EMA with outer product of actual vector (absolute magnitude)\n                        vec = delta_vec.reshape(n, 1)\n                        cov = (1.0 - self.cov_lr) * cov + self.cov_lr * (vec @ vec.T)\n                    # accept move\n                    x_cur = x_try.copy()\n                    f_cur = float(f_try)\n                    improved_round = True\n                    no_improve = 0\n                    step = min(step * self.growth, 6.0 * domain_size)\n                    # small local polish occasionally\n                    if evals < self.budget and np.random.rand() < 0.3:\n                        jitter = np.random.randn(n)\n                        jitter /= (np.linalg.norm(jitter) + 1e-20)\n                        jitter *= 0.12 * step\n                        f_j, x_j = safe_eval(clip_to_bounds(x_cur + jitter))\n                        if f_j is not None and f_j < f_cur - 1e-12:\n                            dv = x_j - x_cur\n                            nv2 = np.linalg.norm(dv)\n                            if nv2 > 0:\n                                mem_steps.insert(0, (dv / nv2).copy())\n                                mem_scores.insert(0, 1.0)\n                                if len(mem_steps) > self.memory_size:\n                                    mem_steps.pop(); mem_scores.pop()\n                                # update cov\n                                v = dv.reshape(n, 1)\n                                cov = (1.0 - self.cov_lr) * cov + self.cov_lr * (v @ v.T)\n                            x_cur = x_j.copy(); f_cur = float(f_j)\n                    # bump top mem score if present\n                    if mem_scores:\n                        mem_scores[0] = mem_scores[0] + 0.5\n                else:\n                    # no immediate improvement => slightly penalize local score expectation\n                    if mem_scores:\n                        # decay a random small fraction to avoid losing all memory\n                        idx = min(len(mem_scores) - 1, np.random.randint(0, max(1, len(mem_scores))))\n                        mem_scores[idx] *= 0.95\n\n            # end probes\n\n            # if no immediate improvement, attempt to use top candidates for a single refinement\n            if (not improved_round) and round_candidates:\n                round_candidates.sort(key=lambda z: z[0])\n                # examine up to top 2\n                for cand in round_candidates[:2]:\n                    f_try, x_try, d_try, alpha_try = cand\n                    if self.budget - evals >= 1:\n                        f_refined, x_refined = cheap_parabola_line(x_cur, f_cur, d_try, alpha_try, f_try)\n                        if f_refined is not None and f_refined < f_cur - 1e-12:\n                            delta_vec = x_refined - x_cur\n                            nv = np.linalg.norm(delta_vec)\n                            if nv > 0:\n                                mem_steps.insert(0, (delta_vec / nv).copy())\n                                mem_scores.insert(0, 1.0)\n                                if len(mem_steps) > self.memory_size:\n                                    mem_steps.pop(); mem_scores.pop()\n                                v = delta_vec.reshape(n, 1)\n                                cov = (1.0 - self.cov_lr) * cov + self.cov_lr * (v @ v.T)\n                            x_cur = x_refined.copy(); f_cur = float(f_refined)\n                            improved_round = True\n                            no_improve = 0\n                            step = min(step * (1.0 + 0.08), 6.0 * domain_size)\n                            break\n\n            # step size and memory score Updates when round failed\n            if not improved_round:\n                no_improve += 1\n                step = max(step * self.shrink, min_step)\n                # decay all mem_scores gently\n                mem_scores = [s * 0.92 for s in mem_scores]\n            else:\n                # slight grow of top memory scores to keep them attractive\n                mem_scores = [min(s * 1.06, 20.0) for s in mem_scores]\n\n            # prune low-score memory entries\n            if mem_scores:\n                keep = [i for i, s in enumerate(mem_scores) if s > 1e-3]\n                if len(keep) != len(mem_scores):\n                    mem_steps = [mem_steps[i] for i in keep]\n                    mem_scores = [mem_scores[i] for i in keep]\n\n            # stagnation / restart\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                if restarts > self.max_restarts:\n                    # final polish: many tiny directional searches until budget exhausted\n                    while evals < self.budget:\n                        d = np.random.randn(n)\n                        d /= (np.linalg.norm(d) + 1e-20)\n                        alpha = np.random.uniform(-0.15 * step, 0.15 * step)\n                        f_try, x_try = safe_eval(clip_to_bounds(x_cur + alpha * d))\n                        if f_try is None:\n                            break\n                        if f_try < f_cur - 1e-12:\n                            dv = x_try - x_cur\n                            nv = np.linalg.norm(dv)\n                            if nv > 0:\n                                mem_steps.insert(0, (dv / nv).copy())\n                                mem_scores.insert(0, 1.0)\n                                if len(mem_steps) > self.memory_size:\n                                    mem_steps.pop()\n                                    mem_scores.pop()\n                            x_cur = x_try.copy(); f_cur = float(f_try)\n                        step = max(step * 0.95, min_step)\n                    break\n                else:\n                    # randomized restart around best-known\n                    if x_best is not None:\n                        radius = min(1.2 * domain_size, 0.7 * domain_size * (1 + restarts / 2.0))\n                        perturb = np.random.randn(n) * radius\n                        x_new = clip_to_bounds(x_best + perturb)\n                    else:\n                        x_new = np.random.uniform(lb, ub)\n                    f_new, x_new = safe_eval(x_new)\n                    if f_new is None:\n                        break\n                    x_cur = x_new.copy(); f_cur = float(f_new)\n                    # inflate step to escape basin\n                    step = min(max(step * 2.0, 0.8 * domain_size), 8.0 * domain_size)\n                    # gently fade memory\n                    mem_scores = [s * 0.25 for s in mem_scores]\n                    no_improve = 0\n\n            # early stop if perfect\n            if f_best <= 1e-12:\n                break\n\n        if x_best is None:\n            x_best = x_cur.copy()\n            f_best = float(f_cur)\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 229, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (4,) (3,) \nOn line: scales = np.sqrt(top_vals) * score_bias[:k]", "error": "In the code, line 229, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (4,) (3,) \nOn line: scales = np.sqrt(top_vals) * score_bias[:k]", "parent_ids": "d3bac529-3666-4b22-8add-4fb85d47bfb4", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "21a18d5d-274b-4c5e-82e3-ad4fe75b7bae", "fitness": "-inf", "name": "SSTQ", "description": "The SSTQ design keeps a bounded archive of evaluated (x,f) pairs (default size ~max(20,6*dim)) and builds an adaptive low-dimensional subspace from the archive covariance (top eigenvectors, orthonormalized by QR) to concentrate modeling and search. Inside that subspace it fits a cheap ridge-regularized quadratic surrogate (constant, linear, and symmetric quadratic terms with diagonal scaled appropriately, lambda scaled to var(F)) and solves a regularized linear system for the surrogate minimizer constrained by a multiplicative trust radius. When the surrogate is not trusted or fails, the algorithm mixes multiscale randomized probes (local Gaussian scaled by projected variances, anisotropic single-direction steps, occasional full-domain jumps; probe_per_iter ≈ max(6,4*k_sub)) and refines promising directions with a cheap asymmetric 3-point curvature probe using quadratic interpolation. Robust practical machinery enforces box clipping and strict budgeted evaluations, adapts trust multiplicatively on success/failure (e.g. increases ≈1.08–1.18 and shrinks ≈0.80–0.88), prunes the archive, and performs limited restarts/polishing when stagnation (stagnation_limit ≈ max(12,10+2·log1p(n))) is detected.", "code": "import numpy as np\n\nclass SSTQ:\n    \"\"\"\n    Stochastic Subspace Trust-Quadratic (SSTQ)\n\n    Main ideas:\n    - Keep a small archive of evaluated points (x, f). Build an adaptive low-dimensional\n      subspace from the empirical covariance (top eigenvectors).\n    - Fit a cheap quadratic surrogate in that subspace via ridge regression:\n        f(z) ≈ c + b^T z + 1/2 z^T Q z  where z are coordinates in subspace.\n      Solve for minimizer z* (regularized) and propose x = x_center + B z* within a trust radius.\n    - Mix surrogate proposals with randomized multiscale Gaussian probes (global & local).\n    - When surrogate proposes an improving direction, do a curvature probe (tiny line search / 3-point) to refine cheaply.\n    - Adapt trust radius multiplicatively on success/failure; perform randomized restarts when stagnant.\n    - Always budget safe (never call func more than budget) and respect box constraints.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 archive_size=None, init_trust=0.5, min_trust=1e-6):\n        \"\"\"\n        budget: maximum number of function evaluations\n        dim: dimensionality\n        seed: RNG seed\n        archive_size: maximum archive size (defaults to 6*dim)\n        init_trust: initial trust-radius factor relative to domain size\n        min_trust: minimum trust radius absolute lower bound\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        self.seed = seed\n        self.archive_size = archive_size if archive_size is not None else max(20, 6 * self.dim)\n        self.init_trust = float(init_trust)\n        self.min_trust = float(min_trust)\n\n    def __call__(self, func):\n        n = self.dim\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        lb = lb.reshape(n,)\n        ub = ub.reshape(n,)\n\n        # helpers\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x\n\n        # initialization: random start\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.asarray(x_best, dtype=float)\n        x_cur = x_cur.copy()\n        f_cur = float(f_cur)\n\n        # domain size (typical -5..5 -> 10)\n        domain_size = np.mean(ub - lb)\n        # trust radius (absolute)\n        trust = max(self.init_trust * domain_size, self.min_trust)\n        trust_max = max(2.0 * domain_size, trust)\n        trust_min = max(self.min_trust, 1e-8)\n\n        # archive of (x, f) tuples (most recent first)\n        archive_X = [x_cur.copy()]\n        archive_f = [f_cur]\n\n        # parameters\n        k_sub = max(2, int(min(n, max(2, int(np.ceil(np.sqrt(n) + 1))))))\n        probe_per_iter = max(6, 4 * k_sub)\n        no_improve = 0\n        stagnation_limit = max(12, int(10 + 2 * np.log1p(n)))\n        restarts = 0\n        max_restarts = 6\n\n        # helper: build k-dim subspace basis B (n x k) from archive covariance\n        def build_basis(k):\n            if len(archive_X) >= 2:\n                X = np.vstack(archive_X)  # (m, n)\n                mean = X.mean(axis=0)\n                C = ((X - mean).T @ (X - mean)) / max(1, X.shape[0] - 1)  # n x n\n                # small regularization for numerical stability\n                C += 1e-8 * np.eye(n)\n                try:\n                    # compute top-k eigenvectors\n                    vals, vecs = np.linalg.eigh(C)  # ascending eigenvalues\n                    idx = np.argsort(vals)[::-1]\n                    top = idx[:k]\n                    B = vecs[:, top]\n                    # ensure orthonormal\n                    Q, _ = np.linalg.qr(B)\n                    return Q[:, :k]\n                except np.linalg.LinAlgError:\n                    R = np.random.randn(n, k)\n                    Q, _ = np.linalg.qr(R)\n                    return Q[:, :k]\n            else:\n                R = np.random.randn(n, k)\n                Q, _ = np.linalg.qr(R)\n                return Q[:, :k]\n\n        # helper: fit quadratic surrogate in subspace coordinates z (k-dim)\n        # model: f ≈ c + b^T z + 0.5 z^T Q z\n        def fit_quadratic(B, center_x):\n            # collect points within a radius around center for fitting\n            X = np.vstack(archive_X)\n            F = np.asarray(archive_f, dtype=float)\n            # project to subspace coords\n            Z = (X - center_x) @ B  # (m, k)\n            m = Z.shape[0]\n            # Build design matrix: [1, z1,...,zk, quadratic_terms (k*(k+1)/2)]\n            qcount = k_sub * (k_sub + 1) // 2\n            D = np.empty((m, 1 + k_sub + qcount))\n            D[:, 0] = 1.0\n            D[:, 1:1 + k_sub] = Z\n            # quadratic terms: upper triangular (including diagonal)\n            col = 1 + k_sub\n            for i in range(k_sub):\n                for j in range(i, k_sub):\n                    D[:, col] = Z[:, i] * Z[:, j]\n                    col += 1\n            # ridge regression\n            lam = 1e-6 * (1.0 + np.var(F))\n            try:\n                # solve (D^T D + lam I) w = D^T F\n                A = D.T @ D\n                A += lam * np.eye(A.shape[0])\n                rhs = D.T @ F\n                w = np.linalg.solve(A, rhs)\n            except np.linalg.LinAlgError:\n                w = np.linalg.lstsq(D, F, rcond=None)[0]\n            # extract c, b, Q\n            c = float(w[0])\n            b = w[1:1 + k_sub].copy()\n            Q = np.zeros((k_sub, k_sub), dtype=float)\n            col = 1 + k_sub\n            for i in range(k_sub):\n                for j in range(i, k_sub):\n                    val = w[col]\n                    Q[i, j] = val\n                    Q[j, i] = val\n                    col += 1\n            # Q currently corresponds to quadratic terms exactly, but since f ≈ c + b^T z + sum_{i<=j} w_ij z_i z_j\n            # and we defined model as c + b^T z + 0.5 z^T Q z, we must convert:\n            # For i==j: w_ii z_i^2 => 0.5 * Q_ii z_i^2 => Q_ii = 2*w_ii\n            # For i<j: w_ij z_i z_j => 0.5 * (Q_ij + Q_ji) z_i z_j => Q_ij = w_ij\n            # So set diagonal double:\n            for i in range(k_sub):\n                Q[i, i] = 2.0 * Q[i, i]\n            return c, b, Q\n\n        # helper: minimize surrogate (regularized) inside trust radius\n        def surrogate_minimizer(c, b, Q, B, center_x, trust_radius):\n            # solve (Q + mu I) z = -b  for z*, with mu chosen to ensure ||z*||*||B|| <= trust_radius\n            # simple regularization schedule\n            mu = 1e-8\n            max_mu = 1e4\n            for _ in range(20):\n                try:\n                    Z = -np.linalg.solve(Q + mu * np.eye(Q.shape[0]), b)\n                except np.linalg.LinAlgError:\n                    mu = max(mu * 10.0, 1e-6)\n                    if mu > max_mu:\n                        return None\n                    continue\n                # map back to x and check radius\n                x_cand = center_x + B @ Z\n                step_norm = np.linalg.norm(x_cand - center_x)\n                if step_norm <= max(1e-12, trust_radius):\n                    return Z, x_cand\n                # otherwise increase mu to shrink step (makes solution smaller)\n                # scale mu proportional to square of ratio\n                mu *= (1.5 + (step_norm / (trust_radius + 1e-12)))\n                if mu > max_mu:\n                    return None\n            return None\n\n        # small 3-point curvature probe (different from ADES: adaptive asymmetric deltas)\n        def curvature_probe(x0, f0, d, base_step):\n            nonlocal evals\n            # choose asymmetrical deltas to detect tilt: a, b such that a != b\n            a = 0.6 * base_step\n            b = 1.1 * base_step\n            # ensure budget\n            if evals + 2 > self.budget:\n                return None, None\n            dp = d / (np.linalg.norm(d) + 1e-20)\n            xp = clip(x0 + a * dp)\n            fm, xp = safe_eval(xp)\n            if fm is None:\n                return None, None\n            xm = clip(x0 - b * dp)\n            fm2, xm = safe_eval(xm)\n            if fm2 is None:\n                return None, None\n            # fit a quadratic in alpha using points (-b, fm2), (0, f0), (+a, fm)\n            # compute coefficients for quadratic p(alpha) = A alpha^2 + B alpha + C\n            denom = (a + b) * (a * b)\n            if abs(denom) < 1e-20:\n                return None, None\n            # use formula for A and B from three points (stable enough)\n            A = (fm + fm2 - 2.0 * f0) / (a * a + 2.0 * a * b + b * b)\n            B = (fm / a - fm2 / b) / (1.0 + b / a + a / b)\n            if A <= 1e-14 or not np.isfinite(A):\n                # pick best of sampled\n                best_f = f0; best_x = x0.copy()\n                if fm < best_f:\n                    best_f, best_x = fm, xp.copy()\n                if fm2 < best_f:\n                    best_f, best_x = fm2, xm.copy()\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            alpha_star = -B / (2.0 * A)\n            # restrict alpha_star to sensible bracket [-2*b, 2*a]\n            limit = max(2.0 * b, 2.0 * a)\n            if abs(alpha_star) > limit:\n                # fallback to sampled\n                best_f = f0; best_x = x0.copy()\n                if fm < best_f:\n                    best_f, best_x = fm, xp.copy()\n                if fm2 < best_f:\n                    best_f, best_x = fm2, xm.copy()\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            # eval at alpha_star\n            if evals >= self.budget:\n                return None, None\n            xs = clip(x0 + alpha_star * dp)\n            fs, xs = safe_eval(xs)\n            if fs is None:\n                return None, None\n            if fs < f0 - 1e-12:\n                return fs, xs\n            # else pick best of sampled\n            best_f = f0; best_x = x0.copy()\n            if fm < best_f:\n                best_f, best_x = fm, xp.copy()\n            if fm2 < best_f:\n                best_f, best_x = fm2, xm.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # main loop\n        while evals < self.budget:\n            # adapt subspace dimension slightly\n            k = min(n, max(1, int(max(2, k_sub + int(np.round(np.random.randn() * 0.4))))))\n            B = build_basis(k)  # n x k\n            # choose center for surrogate: either best-so-far with probability, or current\n            if x_best is not None and np.random.rand() < 0.35:\n                center = x_best.copy()\n                f_center = f_best\n            else:\n                center = x_cur.copy()\n                f_center = f_cur\n\n            # try to fit surrogate if enough points\n            use_surrogate = len(archive_X) >= max(6, k + 3)\n            candidate_accepted = False\n\n            if use_surrogate:\n                # fit quadratic surrogate in subspace coordinates\n                c, b_coef, Qmat = fit_quadratic(B, center)\n                # regularize Q gently to avoid negative definite\n                # ensure symmetric\n                Qmat = 0.5 * (Qmat + Qmat.T)\n                # attempt minimizer inside trust radius\n                res = surrogate_minimizer(c, b_coef, Qmat, B, center, trust)\n                if res is not None:\n                    Zstar, x_cand = res\n                    # ensure candidate unique enough\n                    if evals < self.budget:\n                        f_cand, x_cand = safe_eval(x_cand)\n                        if f_cand is not None and f_cand < f_cur - 1e-12:\n                            # curvature probe along the surrogate step direction for refinement\n                            ddir = x_cand - center\n                            if np.linalg.norm(ddir) > 0 and (self.budget - evals) >= 3 and np.random.rand() < 0.8:\n                                f_ref, x_ref = curvature_probe(center, f_center, ddir, base_step=np.linalg.norm(ddir))\n                                if f_ref is not None and f_ref < f_cand - 1e-12:\n                                    f_cand = f_ref; x_cand = x_ref.copy()\n                            # accept move\n                            step_vec = x_cand - x_cur\n                            if np.linalg.norm(step_vec) > 0:\n                                archive_X.insert(0, x_cand.copy())\n                                archive_f.insert(0, f_cand)\n                                # trim archive\n                                if len(archive_X) > self.archive_size:\n                                    archive_X.pop()\n                                    archive_f.pop()\n                            x_cur = x_cand.copy(); f_cur = float(f_cand)\n                            # adapt trust\n                            trust = min(trust * 1.18, trust_max)\n                            candidate_accepted = True\n                            no_improve = 0\n                        else:\n                            # surrogate failed to improve; shrink trust moderately\n                            trust *= 0.80\n                            no_improve += 1\n                else:\n                    # cannot get surrogate minimizer (ill-conditioned etc.)\n                    use_surrogate = False\n\n            # If surrogate didn't produce a successful candidate, perform randomized multiscale probing\n            if not candidate_accepted:\n                # probes include: local gaussian within trust, anisotropic along basis, and occasional large jump\n                best_local_found = False\n                best_probe = (np.inf, None)\n                for p in range(probe_per_iter):\n                    if evals >= self.budget:\n                        break\n                    r = np.random.rand()\n                    if r < 0.6:\n                        # local Gaussian inside trust radius (scaled by trust)\n                        z = np.random.randn(k)\n                        # scale by eigenvalue-like heuristic: smaller along directions of small variance\n                        # approximate variances from archive projected\n                        if len(archive_X) >= 2:\n                            proj = (np.vstack(archive_X) - center) @ B\n                            varz = np.var(proj, axis=0) + 1e-12\n                        else:\n                            varz = np.ones(k)\n                        z = z * np.sqrt(varz)\n                        z = z / (np.linalg.norm(z) + 1e-20) * (np.random.rand() ** 0.5) * trust\n                        x_probe = center + B @ z\n                    elif r < 0.9:\n                        # anisotropic directional probe along a random basis column with a random step\n                        j = np.random.randint(0, k)\n                        dirn = B[:, j]\n                        step_len = trust * (0.3 + 1.4 * np.random.rand())\n                        if np.random.rand() < 0.5:\n                            step_len *= -1.0\n                        x_probe = center + dirn * step_len\n                    else:\n                        # large exploratory jump\n                        x_probe = np.random.uniform(lb, ub)\n                    x_probe = clip(x_probe)\n                    f_probe, x_probe = safe_eval(x_probe)\n                    if f_probe is None:\n                        break\n                    # store in archive quickly if good\n                    if f_probe < best_probe[0]:\n                        best_probe = (f_probe, x_probe.copy())\n                    # immediate accept if improves current\n                    if f_probe < f_cur - 1e-12:\n                        # small curvature probe along direction x_probe - center\n                        ddir = x_probe - center\n                        if np.linalg.norm(ddir) > 0 and (self.budget - evals) >= 3:\n                            f_ref, x_ref = curvature_probe(center, f_probe, ddir, base_step=np.linalg.norm(ddir))\n                            if f_ref is not None and f_ref < f_probe - 1e-12:\n                                f_probe = f_ref; x_probe = x_ref.copy()\n                        # accept\n                        x_cur = x_probe.copy()\n                        f_cur = float(f_probe)\n                        archive_X.insert(0, x_probe.copy())\n                        archive_f.insert(0, f_probe)\n                        if len(archive_X) > self.archive_size:\n                            archive_X.pop(); archive_f.pop()\n                        trust = min(trust * 1.12, trust_max)\n                        candidate_accepted = True\n                        best_local_found = True\n                        no_improve = 0\n                        break\n\n                # end probes\n                if not best_local_found and best_probe[1] is not None:\n                    # opportunistically try to exploit best probe via a tiny surrogate in its direction\n                    f_probe, x_probe = best_probe\n                    if f_probe < f_cur - 1e-12:\n                        x_cur = x_probe.copy(); f_cur = float(f_probe)\n                        archive_X.insert(0, x_probe.copy())\n                        archive_f.insert(0, f_probe)\n                        if len(archive_X) > self.archive_size:\n                            archive_X.pop(); archive_f.pop()\n                        trust = min(trust * 1.08, trust_max)\n                        candidate_accepted = True\n                        no_improve = 0\n                    else:\n                        # decay trust because no improvement\n                        trust *= 0.88\n                        no_improve += 1\n\n            # maintain best and pruning\n            if x_best is None or f_cur < f_best:\n                f_best = float(f_cur); x_best = x_cur.copy()\n\n            # prune archive by keeping best + recent mixture\n            if len(archive_X) > 1.5 * self.archive_size:\n                # keep best few and most recent\n                idxs = list(range(min(5, len(archive_X))))  # recent\n                # find best few\n                sorted_idx = np.argsort(np.asarray(archive_f))\n                for i in sorted_idx[:min(10, len(sorted_idx))]:\n                    if i not in idxs:\n                        idxs.append(i)\n                newX = [archive_X[i] for i in idxs][:self.archive_size]\n                newF = [archive_f[i] for i in idxs][:self.archive_size]\n                archive_X = newX\n                archive_f = newF\n\n            # stagnation & restart logic\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final polish: random small Gaussian around best\n                    while evals < self.budget:\n                        d = np.random.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-20)\n                        alpha = 0.08 * trust * np.random.rand()\n                        x_try = clip(x_best + d * (alpha * np.random.randn()))\n                        f_try, x_try = safe_eval(x_try)\n                        if f_try is None:\n                            break\n                        if f_try < f_best - 1e-12:\n                            f_best = float(f_try); x_best = x_try.copy()\n                    break\n                else:\n                    # restart around best with increasing radius\n                    radius = min(1.5 * domain_size, trust_max * (1.0 + restarts * 0.8))\n                    x_new = clip(x_best + np.random.randn(n) * radius)\n                    f_new, x_new = safe_eval(x_new)\n                    if f_new is None:\n                        break\n                    x_cur = x_new.copy(); f_cur = float(f_new)\n                    archive_X.insert(0, x_cur.copy()); archive_f.insert(0, f_cur)\n                    if len(archive_X) > self.archive_size:\n                        archive_X.pop(); archive_f.pop()\n                    # increase trust to escape\n                    trust = min(trust * 1.6, trust_max)\n                    no_improve = 0\n                    continue\n\n            # safeguard trust bounds\n            trust = max(min(trust, trust_max), trust_min)\n\n            # optional early exit\n            if f_best <= 1e-12:\n                break\n\n        # final fallback\n        if x_best is None:\n            x_best = x_cur.copy()\n            f_best = float(f_cur)\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 140, in fit_quadratic, the following error occurred:\nValueError: could not broadcast input array from shape (16,6) into shape (16,5)\nOn line: D[:, 1:1 + k_sub] = Z", "error": "In the code, line 140, in fit_quadratic, the following error occurred:\nValueError: could not broadcast input array from shape (16,6) into shape (16,5)\nOn line: D[:, 1:1 + k_sub] = Z", "parent_ids": "d3bac529-3666-4b22-8add-4fb85d47bfb4", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e883ddbe-2d8f-46e1-ab9a-5397c9551f93", "fitness": 0.4026298926852475, "name": "SMES", "description": "The algorithm mixes manifold-guided and isotropic exploration by keeping a short memory (memory_size=8) of recent successful unit directions with trust weights and sampling new proposals as weighted blends of those directions (80% chance) or pure Gaussian directions (20%), plus rare axis-aligned tweaks for exploitation. Step lengths use heavy-tailed jumps (Student‑t df=3 scaled by a step size) to allow occasional long leaps, with aggressive multiplicative growth on success (step_grow=1.20) and shrink on failure (step_shrink=0.75) and a domain‑scaled minimum step. Local exploitation is done via quadratic line refinement using three points (0, alpha, 2*alpha) and by trying midpoints of promising candidates to exploit curvature and combine directions. Robustness and budget-safety are enforced by clipping to bounds, strict eval counting, trust decay/update rules, stagnation-driven restarts (stagnation limit ~6+1.5*sqrt(n), max_restarts=6) and a final small-step polishing phase.", "code": "import numpy as np\n\nclass SMES:\n    \"\"\"\n    Stochastic Manifold Exploratory Search (SMES)\n\n    Main algorithmic parameters (tunable):\n    - budget: maximum function evaluations (required)\n    - dim: search dimension (required)\n    - seed: RNG seed (optional)\n    - memory_size: how many recent successful directions to keep (default 8)\n    - init_step_frac: initial step as fraction of mean domain (default 0.3)\n    - min_step: absolute minimal step magnitude (default 1e-9 scaled by domain)\n    - step_grow: multiplicative step growth on success (default 1.20)\n    - step_shrink: multiplicative step shrink on failure (default 0.75)\n    - probes_per_round: number of directional probes per main iteration (computed adaptively)\n    - trust_decay_success/failed: per-memory trust update multipliers\n    - stagnation_limit: iterations without improvement before a restart\n    Notes:\n    - The code enforces budget-safe evaluations via safe_eval.\n    - Uses a different set of parameter choices and equations vs the referenced ADES:\n      * heavy-tailed alpha sampling (Student-t) for occasional long leaps,\n      * quadratic refinement uses (0, alpha, 2*alpha) points (not +/-delta),\n      * more aggressive growth/shrink multipliers,\n      * different memory size and trust update rules.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=8, init_step_frac=0.3, min_step=1e-9,\n                 step_grow=1.20, step_shrink=0.75):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.memory_size = int(memory_size)\n        self._init_step_frac = float(init_step_frac)\n        self._min_step_param = float(min_step)\n        self.step_grow = float(step_grow)\n        self.step_shrink = float(step_shrink)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds (Many Affine BBOB typically -5..5)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        lb = lb.reshape(n,)\n        ub = ub.reshape(n,)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def clip_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initialize current point uniformly in domain\n        x_cur = self.rng.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.asarray(x_best, dtype=float)\n        x_cur = np.asarray(x_cur, dtype=float)\n        f_cur = float(f_cur)\n\n        # domain and step initialization\n        domain_size = np.mean(ub - lb)\n        step = max(self._init_step_frac * domain_size, 1e-12)\n        min_step = max(self._min_step_param * max(1.0, domain_size), 1e-12)\n\n        # memory and trust (for manifold / weighted sampling)\n        mem_steps = []   # list of unit vectors\n        mem_trust = []   # positive floats\n\n        # bookkeeping\n        no_improve_iters = 0\n        stagnation_limit = max(10, int(6 + np.sqrt(n) * 1.5))\n        restarts = 0\n        max_restarts = 6\n\n        # probes tuning: different heuristic than ADES\n        def probes_for_round():\n            # moderate number of probes scaling with sqrt(n)\n            return max(6, int(4 + 2 * np.sqrt(n)))\n\n        # Build a new proposal direction by mixing memory directions and random noise.\n        # This differs from ADES's PCA: we form a weighted linear blend (stochastic manifold).\n        def sample_direction():\n            if mem_steps and (self.rng.random() < 0.8):\n                # weighted combination of top-trust memory vectors\n                trusts = np.array(mem_trust, dtype=float)\n                trusts = trusts / (np.sum(trusts) + 1e-20)\n                # draw random coefficients biased by trust\n                k = min(len(mem_steps), max(1, int(1 + self.rng.integers(0, len(mem_steps)))))\n                idx = self.rng.choice(len(mem_steps), size=k, replace=False, p=trusts)\n                coeffs = self.rng.normal(loc=0.0, scale=1.0, size=k) * (0.5 + 0.5 * trusts[idx])\n                d = np.zeros(n)\n                for c, i in zip(coeffs, idx):\n                    d += c * mem_steps[i]\n                # add isotropic gaussian jitter to explore orthogonal complement\n                d += 0.3 * self.rng.standard_normal(n)\n            else:\n                # pure random direction\n                d = self.rng.standard_normal(n)\n            norm = np.linalg.norm(d)\n            if norm == 0:\n                # fallback random direction\n                d = self.rng.standard_normal(n)\n                norm = np.linalg.norm(d)\n            return d / (norm + 1e-20)\n\n        # Quadratic refinement using points at 0, alpha, 2*alpha along direction d.\n        # This is different from ADES (which used +/- delta).\n        def quad_refine(x0, f0, d, alpha):\n            # uses up to 2 extra evaluations: at x0 + alpha*d and x0 + 2*alpha*d,\n            # fits quadratic through (0,f0), (a,f1), (2a,f2) and returns improved point if found.\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            x1 = clip_to_bounds(x0 + alpha * d)\n            f1, x1 = safe_eval(x1)\n            if f1 is None:\n                return None, None\n            if evals >= self.budget:\n                # no budget left for second point; accept if improvement\n                if f1 < f0 - 1e-12:\n                    return f1, x1\n                return None, None\n            x2 = clip_to_bounds(x0 + 2.0 * alpha * d)\n            f2, x2 = safe_eval(x2)\n            if f2 is None:\n                return None, None\n            # fit quadratic f(a) = A a^2 + B a + C with C=f0\n            # Using points at a=alpha and a=2alpha:\n            # Solve for A,B quickly:\n            # f1 = A a^2 + B a + f0\n            # f2 = A (2a)^2 + B (2a) + f0 = 4 A a^2 + 2 B a + f0\n            a = alpha\n            denom = (2.0 * (a ** 2))\n            A = (f2 - 2.0 * f1 + f0) / (2.0 * a * a + 1e-30)\n            B = (f1 - f0 - A * a * a) / (a + 1e-30)\n            # if parabola convex (A > 0), compute minimizer\n            if A <= 0 or not np.isfinite(A):\n                # fallback to best sampled among f0,f1,f2\n                best_f = f0; best_x = x0.copy()\n                if f1 < best_f - 1e-12:\n                    best_f = f1; best_x = x1.copy()\n                if f2 < best_f - 1e-12:\n                    best_f = f2; best_x = x2.copy()\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            alpha_star = -B / (2.0 * A)\n            # only accept within reasonable multiple of alpha (say 0.0..4*alpha)\n            if alpha_star <= -4.0 * abs(a) or alpha_star >= 4.0 * abs(a):\n                # out of trust range; use best of samples\n                best_f = f0; best_x = x0.copy()\n                if f1 < best_f - 1e-12:\n                    best_f = f1; best_x = x1.copy()\n                if f2 < best_f - 1e-12:\n                    best_f = f2; best_x = x2.copy()\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            # finally evaluate at alpha_star if budget permits and it's different from sampled\n            if evals >= self.budget:\n                return None, None\n            x_s = clip_to_bounds(x0 + alpha_star * d)\n            f_s, x_s = safe_eval(x_s)\n            if f_s is None:\n                return None, None\n            if f_s < f0 - 1e-12:\n                return f_s, x_s\n            # fallback best of sampled\n            best_f = f0; best_x = x0.copy()\n            if f1 < best_f - 1e-12:\n                best_f = f1; best_x = x1.copy()\n            if f2 < best_f - 1e-12:\n                best_f = f2; best_x = x2.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # main loop\n        while evals < self.budget:\n            probes = probes_for_round()\n            improved_round = False\n            candidate_list = []  # store (f_try, x_try, d, alpha)\n\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                d = sample_direction()\n                # heavy-tailed alpha sampling to allow occasional long jumps\n                # Student-t with df=3 is used; scaled by step\n                alpha = self.rng.standard_t(df=3) * step\n                # cap extremes\n                alpha = float(np.clip(alpha, -5.0 * step, 5.0 * step))\n                # small probability to try axis-aligned coordinate tweaks for exploitation\n                if self.rng.random() < 0.06:\n                    axis = self.rng.integers(0, n)\n                    d = np.zeros(n); d[axis] = 1.0\n\n                x_try = clip_to_bounds(x_cur + alpha * d)\n                f_try, x_try = safe_eval(x_try)\n                if f_try is None:\n                    break\n                candidate_list.append((f_try, x_try.copy(), d.copy(), alpha))\n\n                if f_try < f_cur - 1e-12:\n                    # do quadratic refinement (0, alpha, 2*alpha)\n                    f_new, x_new = quad_refine(x_cur, f_cur, d, alpha if alpha != 0 else step * 0.7)\n                    if f_new is not None and f_new < f_try - 1e-12:\n                        f_try = f_new\n                        x_try = x_new.copy()\n                    # accept\n                    # grow step more aggressively than ADES\n                    step *= self.step_grow\n                    step = min(step, 10.0 * domain_size)\n                    # insert into memory\n                    delta_vec = x_try - x_cur\n                    norm_dv = np.linalg.norm(delta_vec)\n                    if norm_dv > 0:\n                        dir_unit = (delta_vec / norm_dv).copy()\n                        mem_steps.insert(0, dir_unit)\n                        mem_trust.insert(0, 1.0)  # high trust for new success\n                        # trim memory\n                        if len(mem_steps) > self.memory_size:\n                            mem_steps.pop()\n                            mem_trust.pop()\n                    # update current\n                    x_cur = x_try.copy()\n                    f_cur = float(f_try)\n                    improved_round = True\n                    no_improve_iters = 0\n                    # boost top trust and slightly decay others (different multipliers)\n                    mem_trust = [min(10.0, t * 0.95) for t in mem_trust]  # slight decay first\n                    if mem_trust:\n                        mem_trust[0] = mem_trust[0] + 0.6\n                    # small local jitter polishing with low probability\n                    if evals < self.budget and self.rng.random() < 0.25:\n                        j = self.rng.standard_normal(n)\n                        j /= (np.linalg.norm(j) + 1e-20)\n                        j *= 0.15 * step\n                        f_j, x_j = safe_eval(clip_to_bounds(x_cur + j))\n                        if f_j is not None and f_j < f_cur - 1e-12:\n                            delta_vec = x_j - x_cur\n                            norm_dv = np.linalg.norm(delta_vec)\n                            if norm_dv > 0:\n                                mem_steps.insert(0, (delta_vec / norm_dv).copy())\n                                mem_trust.insert(0, 1.0)\n                                if len(mem_steps) > self.memory_size:\n                                    mem_steps.pop(); mem_trust.pop()\n                            x_cur = x_j.copy(); f_cur = float(f_j)\n                else:\n                    # occasional attempt to refine an unpromising direction (small chance)\n                    if self.rng.random() < 0.045 and (self.budget - evals) >= 2:\n                        f_new, x_new = quad_refine(x_cur, f_cur, d, np.sign(alpha) * max(abs(alpha), 0.6 * step))\n                        if f_new is not None and f_new < f_cur - 1e-12:\n                            delta_vec = x_new - x_cur\n                            norm_dv = np.linalg.norm(delta_vec)\n                            if norm_dv > 0:\n                                mem_steps.insert(0, (delta_vec / norm_dv).copy())\n                                mem_trust.insert(0, 1.0)\n                                if len(mem_steps) > self.memory_size:\n                                    mem_steps.pop(); mem_trust.pop()\n                            x_cur = x_new.copy(); f_cur = float(f_new)\n                            improved_round = True\n                            no_improve_iters = 0\n                            step *= (1.0 + 0.09)  # mild increase\n            # end probes\n\n            # if no direct improvement, attempt to combine best candidates by trying their midpoints along their directions\n            if (not improved_round) and candidate_list:\n                candidate_list.sort(key=lambda z: z[0])\n                # try the best two candidates by performing a short combined move\n                for i in range(min(2, len(candidate_list))):\n                    f_try, x_try, d_try, alpha_try = candidate_list[i]\n                    # try midpoint between current and candidate\n                    if evals >= self.budget:\n                        break\n                    mid = clip_to_bounds(0.5 * (x_cur + x_try))\n                    f_mid, mid = safe_eval(mid)\n                    if f_mid is None:\n                        break\n                    if f_mid < f_cur - 1e-12:\n                        delta_vec = mid - x_cur\n                        norm_dv = np.linalg.norm(delta_vec)\n                        if norm_dv > 0:\n                            mem_steps.insert(0, (delta_vec / norm_dv).copy())\n                            mem_trust.insert(0, 1.0)\n                            if len(mem_steps) > self.memory_size:\n                                mem_steps.pop(); mem_trust.pop()\n                        x_cur = mid.copy(); f_cur = float(f_mid)\n                        improved_round = True\n                        no_improve_iters = 0\n                        step *= 1.10\n                        break\n\n            # step adaptation and memory trust update when round ended\n            if not improved_round:\n                no_improve_iters += 1\n                step *= self.step_shrink\n                # decay trust more when failing\n                mem_trust = [t * 0.90 for t in mem_trust]\n            else:\n                # small stabilization of trust values\n                mem_trust = [min(10.0, t * 1.03) for t in mem_trust]\n\n            # prune very low-trust entries\n            if mem_trust:\n                keep_idx = [i for i, t in enumerate(mem_trust) if t > 1e-3]\n                if len(keep_idx) != len(mem_trust):\n                    mem_steps = [mem_steps[i] for i in keep_idx]\n                    mem_trust = [mem_trust[i] for i in keep_idx]\n\n            # safeguard minimum step\n            if step < min_step:\n                step = min_step\n\n            # stagnation / restart logic (different heuristics than ADES)\n            if no_improve_iters >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final polishing: dense small isotropic probes until budget exhausted\n                    while evals < self.budget:\n                        d = self.rng.standard_normal(n)\n                        d /= (np.linalg.norm(d) + 1e-20)\n                        alpha = self.rng.normal(0.0, 0.12 * step)\n                        x_try = clip_to_bounds(x_cur + alpha * d)\n                        f_try, x_try = safe_eval(x_try)\n                        if f_try is None:\n                            break\n                        if f_try < f_cur - 1e-12:\n                            # register small improvement\n                            delta_vec = x_try - x_cur\n                            norm_dv = np.linalg.norm(delta_vec)\n                            if norm_dv > 0:\n                                mem_steps.insert(0, (delta_vec / norm_dv).copy())\n                                mem_trust.insert(0, 1.0)\n                                if len(mem_steps) > self.memory_size:\n                                    mem_steps.pop()\n                            x_cur = x_try.copy(); f_cur = float(f_try)\n                        step *= 0.95\n                        if step < min_step:\n                            break\n                    break\n                else:\n                    # restart around best known (exploit restarts with larger variance)\n                    if x_best is not None:\n                        radius = min(1.5, 1.0 + 0.8 * restarts) * domain_size * 0.5\n                        perturb = self.rng.normal(0.0, radius, size=n)\n                        x_cur = clip_to_bounds(x_best + perturb)\n                        f_new, x_cur = safe_eval(x_cur)\n                        if f_new is None:\n                            break\n                        f_cur = float(f_new)\n                    else:\n                        x_cur = self.rng.uniform(lb, ub)\n                        f_new, x_cur = safe_eval(x_cur)\n                        if f_new is None:\n                            break\n                        f_cur = float(f_new)\n                    # reset step to slightly larger value\n                    step = min(step * 1.8 + 0.1 * domain_size, 8.0 * domain_size)\n                    # partially forget memory to diversify\n                    mem_trust = [t * 0.25 for t in mem_trust]\n                    no_improve_iters = 0\n\n            # very small objective bail-out\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            x_best = x_cur.copy()\n            f_best = float(f_cur)\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm SMES scored 0.403 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "d3bac529-3666-4b22-8add-4fb85d47bfb4", "operator": null, "metadata": {"aucs": [0.14164619767666187, 0.15854202726112332, 0.9296305969138329, 0.7654109783806959, 0.2221902172167205, 0.9589980217935781, 0.23149530021332754, 0.21846583830945743, 0.2583849353788368, 0.14153481370824128]}, "task_prompt": ""}
{"id": "ccc8247e-3014-461d-96ea-6bdeecd91a79", "fitness": "-inf", "name": "HASTE_QN", "description": "HASTE-QN keeps an LRU memory of recent successful unit step directions, their absolute step lengths (alphas) and trust weights (memory_size default 20) to bias future search and to build a trust-weighted PCA subspace (via SVD) augmented by random directions for diversity. It allocates a modest number of probes per round with a bandit-like rule that favors high-trust memory directions or principal components, sampling alphas around past successful values and using variance-weighted coefficients to shape directions. When a probe improves, the algorithm performs budget-aware local refinement: a 3-point parabolic line fit (convexity checks and alpha limits) plus an optional curvature-informed alpha suggestion from a small recent_points buffer, and it adaptively grows/shrinks a global absolute step (grow ~1.12 on success, shrink ~0.86 on failure) while updating/decaying trusts. The design is budget-safe and box-clipped, includes occasional tiny polishing jitters, pruning of low-trust memories, stagnation-triggered randomized restarts and a final fine-grained polish, and various pragmatic parameter choices to trade exploration and exploitation.", "code": "import numpy as np\n\nclass HASTE_QN:\n    \"\"\"\n    HASTE-QN: Hybrid Adaptive Subspace Trust-weighted Ensemble with cheap Quasi-Newton line refinement.\n\n    Main features:\n    - Keep an LRU memory of successful unit step directions + last-successful step lengths + trust weights.\n    - Build k-dimensional orthonormal subspaces from a trust-weighted PCA of memory + random augmentations.\n    - Allocate probes per round by a bandit-like rule: more probes toward higher-trust directions and principal components.\n    - For each probe: do a single cheap evaluation; on improvement, run a 3-point parabolic refinement and a cheap curvature-based step suggestion (uses recent f values) to attempt a second refinement (both budget-aware).\n    - Adaptive global absolute step size with multiplicative growth/shrink and per-memory trust updates.\n    - Budget-safe, box-clipped, restarts, and final tiny-probe polishing.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=20, init_step_factor=0.5, min_step=1e-9):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.memory_size = int(memory_size)\n        self.init_step_factor = float(init_step_factor)\n        self.min_step_param = float(min_step)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # Bounds from func\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        lb = lb.reshape(n,)\n        ub = ub.reshape(n,)\n\n        domain_size = float(np.mean(ub - lb))\n        # initial step absolute scale\n        step = max(self.init_step_factor * domain_size, 1e-12)\n        min_step = max(self.min_step_param * max(1.0, domain_size), 1e-12)\n        max_step = 5.0 * domain_size\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # safe eval wrapper with clipping, returns (f,x) or (None,None) if budget exhausted\n        def clip_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x\n\n        # initialize current point\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.array(x_best, dtype=float)\n        f_cur = float(f_cur)\n\n        # memory structures\n        mem_dirs = []    # list of unit vectors (most recent first)\n        mem_alphas = []  # last helpful alpha along that direction (absolute)\n        mem_trust = []   # trust weights >0\n        # small circular buffer of recent (x,f) for cheap curvature estimates\n        recent_points = [(x_cur.copy(), f_cur)]\n\n        # parameters\n        stagnation_limit = max(12, int(8 + np.log1p(n)))\n        no_improve = 0\n        restarts = 0\n        max_restarts = 6\n\n        # subspace dimension heuristic\n        def subspace_dim():\n            # blend sqrt(n) with small constant; clamp\n            k = max(1, int(min(n, int(np.ceil(np.sqrt(n)))) ))\n            # slightly randomize to avoid deterministic pattern\n            if n >= 6 and np.random.rand() < 0.3:\n                k = min(n, k + np.random.randint(0, 2))\n            return k\n\n        # build a k-dimensional orthonormal basis from memory PCA + random\n        def build_basis(k):\n            if len(mem_dirs) >= 2:\n                M = np.vstack(mem_dirs)  # m x n (m recent direction vectors)\n                # weight rows by sqrt(trust) to bias PCA\n                trusts = np.array(mem_trust).reshape(-1,)\n                W = np.sqrt(np.maximum(trusts, 1e-8)).reshape(-1,1)\n                Mc = (M * W)  # weighted\n                try:\n                    # compute SVD of Mc (m x n), get right singular vectors (n)\n                    U, S, Vt = np.linalg.svd(Mc, full_matrices=False)\n                except np.linalg.LinAlgError:\n                    # fallback random\n                    R = np.random.randn(n, k)\n                    Q, _ = np.linalg.qr(R)\n                    return Q[:, :k]\n                pcs = Vt.T  # n x r\n                take = min(k, pcs.shape[1])\n                basis_cols = pcs[:, :take]\n                if take < k:\n                    R = np.random.randn(n, k - take)\n                    stacked = np.column_stack((basis_cols, R))\n                    Q, _ = np.linalg.qr(stacked)\n                    return Q[:, :k]\n                else:\n                    Q, _ = np.linalg.qr(basis_cols)\n                    return Q[:, :k]\n            else:\n                R = np.random.randn(n, k)\n                Q, _ = np.linalg.qr(R)\n                return Q[:, :k]\n\n        # cheap 3-point parabolic refinement along direction d from x0\n        def parabolic_refine(x0, f0, d, delta):\n            # returns (f_new, x_new) or (None,None)\n            nonlocal evals\n            if np.linalg.norm(d) == 0:\n                return None, None\n            d = d / (np.linalg.norm(d) + 1e-20)\n            remaining = self.budget - evals\n            if remaining <= 0:\n                return None, None\n            # evaluate +delta and -delta; order chosen to possibly save one eval if +delta is improvement\n            x_p = clip_to_bounds(x0 + delta * d)\n            f_p, x_p = safe_eval(x_p)\n            if f_p is None:\n                return None, None\n            remaining = self.budget - evals\n            x_m = clip_to_bounds(x0 - delta * d)\n            if remaining <= 0:\n                if f_p < f0 - 1e-12:\n                    return f_p, x_p\n                return None, None\n            f_m, x_m = safe_eval(x_m)\n            if f_m is None:\n                return None, None\n            # compute quadratic coefficients; accept minimizer if convex and within bounds\n            denom = 2.0 * (delta ** 2)\n            A = (f_p + f_m - 2.0 * f0) / denom\n            B = (f_p - f_m) / (2.0 * delta)\n            if not np.isfinite(A) or not np.isfinite(B):\n                # fallback pick best of sampled\n                best_f, best_x = (f_p, x_p) if f_p < f_m else (f_m, x_m)\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            if A <= 0 or np.isclose(A, 0.0):\n                # not convex -> pick better side if any\n                best_f, best_x = (f_p, x_p) if f_p < f_m else (f_m, x_m)\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            alpha_star = -B / (2.0 * A)\n            # acceptance threshold for magnitude\n            if abs(alpha_star) > 4.0 * delta:\n                # out of reasonable range\n                best_f, best_x = (f_p, x_p) if f_p < f_m else (f_m, x_m)\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            # evaluate at alpha_star\n            remaining = self.budget - evals\n            if remaining <= 0:\n                # fallback to best sampled\n                best_f, best_x = (f_p, x_p) if f_p < f_m else (f_m, x_m)\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            x_s = clip_to_bounds(x0 + alpha_star * d)\n            f_s, x_s = safe_eval(x_s)\n            if f_s is None:\n                return None, None\n            if f_s < f0 - 1e-12:\n                return f_s, x_s\n            # else return best of sampled if any\n            best_f, best_x = f0, x0.copy()\n            if f_p < best_f - 1e-12:\n                best_f, best_x = f_p, x_p.copy()\n            if f_m < best_f - 1e-12:\n                best_f, best_x = f_m, x_m.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # cheap curvature-informed scalar suggestion using last few recent_points\n        def curvature_suggest_alpha(d, x0, f0, baseline_alpha):\n            \"\"\"\n            Using recent_points try to estimate curvature along d and suggest an improved alpha.\n            Return suggested_alpha (abs) or None.\n            This uses finite-difference-like quadratic approximations from recent sample history if available.\n            \"\"\"\n            if np.linalg.norm(d) == 0:\n                return None\n            d = d / (np.linalg.norm(d) + 1e-20)\n            # try to find two distinct projections from recent_points to estimate curvature\n            if len(recent_points) < 2:\n                return None\n            # compute scalar projections of recent deltas onto d\n            vals = []\n            for (x_i, f_i) in recent_points[-6:]:\n                proj = np.dot(x_i - x0, d)\n                vals.append((proj, f_i))\n            # Need at least two distinct proj values (not both zero)\n            # Choose two most extreme projected points\n            if len(vals) < 2:\n                return None\n            vals_sorted = sorted(vals, key=lambda z: z[0])\n            left = vals_sorted[0]\n            right = vals_sorted[-1]\n            if np.isclose(left[0], right[0]):\n                return None\n            # Fit quadratic through (a_left, f_left), (0, f0), (a_right, f_right) approximately\n            a1, f1 = left\n            a2, f2 = right\n            # Solve for A,B: f(a) = A a^2 + B a + C, with C = f0\n            # A = (f1 + f2 - 2*f0) / (a1^2 + a2^2 - 2*0?) approximate if symmetric; handle degenerate\n            denom = (a1**2 + a2**2)\n            if np.isclose(denom, 0.0):\n                return None\n            A = (f1 + f2 - 2.0 * f0) / denom\n            B = 0.0\n            # If A <= 0 or not finite, skip\n            if not np.isfinite(A) or A <= 1e-14:\n                return None\n            # approximate minimizer alpha_star from these two points solving derivative ~ 0:\n            # Using a symmetric approximation doesn't give B; fallback use single-sided estimate: choose alpha near -B/(2A)\n            # We can approximate B by linear fit between (a1,f1) and (0,f0): B ≈ (f1 - f0 - A*a1*a1) / a1\n            try:\n                B1 = (f1 - f0 - A * (a1**2)) / (a1 + 1e-20)\n                B2 = (f2 - f0 - A * (a2**2)) / (a2 + 1e-20)\n                B = 0.5 * (B1 + B2)\n            except Exception:\n                B = 0.0\n            alpha_star = -B / (2.0 * A)\n            if not np.isfinite(alpha_star):\n                return None\n            # limit suggestion magnitude\n            if abs(alpha_star) > 6.0 * max(abs(baseline_alpha), 1.0 * step):\n                return None\n            # Require at least some departure\n            if abs(alpha_star) < 0.05 * max(abs(baseline_alpha), step * 0.1):\n                return None\n            return alpha_star\n\n        # main optimization loop\n        while evals < self.budget:\n            k = max(1, subspace_dim())\n            basis = build_basis(k)  # n x k\n\n            # compute per-basis coefficient variances from projection of memory if available\n            if len(mem_dirs) >= 2:\n                M = np.vstack(mem_dirs)  # m x n\n                proj = M @ basis  # m x k\n                var_coef = np.var(proj, axis=0) + 1e-12\n                # inflate variances for components with higher mean projection magnitude\n                var_coef *= (1.0 + 0.5 * np.mean(mem_trust) if mem_trust else 1.0)\n            else:\n                var_coef = np.ones(k, dtype=float)\n\n            # decide number of probes: scale with k but cap by remaining budget\n            probes = int(max(6, min(3*k, max(6, 4*k))))\n            probes = min(probes, max(1, self.budget - evals))\n            improved_round = False\n            candidate_buffer = []\n\n            # Bandit-like allocation: compute weights for memory directions vs random\n            mem_weight = np.array(mem_trust) if mem_trust else np.array([])\n            mem_weight = np.maximum(mem_weight, 0.0)\n            if mem_weight.size > 0:\n                mem_probs = mem_weight / (np.sum(mem_weight) + 1e-12)\n            else:\n                mem_probs = np.array([])\n\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # with some probability sample biased from memory, otherwise sample in PCA basis\n                if mem_probs.size > 0 and np.random.rand() < 0.45:\n                    # choose memory index stochastically by trust\n                    idx = np.random.choice(len(mem_dirs), p=mem_probs)\n                    dir_base = mem_dirs[idx].copy()\n                    # perturb within small subspace: mix with basis-projected noise\n                    noise_coeffs = (np.random.randn(k) * np.sqrt(var_coef)) * 0.2\n                    d = dir_base + (basis @ noise_coeffs) * 0.6\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                    # alpha sampled around last successful alpha for this memory if exists\n                    base_alpha = mem_alphas[idx] if idx < len(mem_alphas) else step\n                    alpha = np.random.normal(loc=0.7 * base_alpha, scale=0.6 * abs(base_alpha) + 0.5 * step)\n                    # clamp alpha\n                    if np.random.rand() < 0.08:\n                        alpha *= 2.0  # occasional long shot\n                else:\n                    # sample random coefficients in basis with variance weighting\n                    coeffs = np.random.randn(k) * np.sqrt(var_coef)\n                    d = basis @ coeffs\n                    nd = np.linalg.norm(d)\n                    if nd == 0:\n                        continue\n                    d = d / nd\n                    # alpha mixture: small/medium/large\n                    r = np.random.rand()\n                    if r < 0.6:\n                        alpha = np.random.uniform(-1.0 * step, 1.0 * step)\n                    elif r < 0.9:\n                        alpha = np.random.uniform(-2.5 * step, 2.5 * step)\n                    else:\n                        alpha = np.random.uniform(-5.0 * step, 5.0 * step)\n\n                # try probe\n                x_try = clip_to_bounds(x_cur + alpha * d)\n                f_try, x_try = safe_eval(x_try)\n                if f_try is None:\n                    break\n                candidate_buffer.append((f_try, x_try.copy(), d.copy(), alpha))\n\n                if f_try < f_cur - 1e-12:\n                    # success -> attempt parabolic refine\n                    delta = max(abs(alpha), 0.9 * step)\n                    f_line, x_line = parabolic_refine(x_cur, f_cur, d, delta=delta)\n                    if f_line is not None and f_line < f_try - 1e-12:\n                        f_try = f_line\n                        x_try = x_line.copy()\n                    # attempt curvature-informed suggestion (cheap)\n                    suggested = curvature_suggest_alpha(d, x_cur, f_cur, alpha)\n                    if suggested is not None and (self.budget - evals) >= 1:\n                        # evaluate suggested candidate\n                        x_sugg = clip_to_bounds(x_cur + suggested * d)\n                        f_sugg, x_sugg = safe_eval(x_sugg)\n                        if f_sugg is None:\n                            pass\n                        else:\n                            if f_sugg < f_try - 1e-12:\n                                f_try = f_sugg\n                                x_try = x_sugg.copy()\n                                alpha = suggested\n                    # accept move\n                    # update memory: insert direction and alpha and trust\n                    delta_vec = x_try - x_cur\n                    norm_dv = np.linalg.norm(delta_vec)\n                    if norm_dv > 0:\n                        dir_unit = (delta_vec / norm_dv).copy()\n                        mem_dirs.insert(0, dir_unit)\n                        mem_alphas.insert(0, abs(alpha))\n                        mem_trust.insert(0, 1.0)\n                        # trim memory\n                        if len(mem_dirs) > self.memory_size:\n                            mem_dirs.pop()\n                            mem_alphas.pop()\n                            mem_trust.pop()\n                    x_cur = x_try.copy()\n                    f_cur = float(f_try)\n                    improved_round = True\n                    no_improve = 0\n                    # step growth modest\n                    step = min(step * 1.12, max_step)\n                    # slightly boost trust of newest and decay others\n                    mem_trust = [min(t * 1.05, 50.0) for t in mem_trust]\n                    if mem_trust:\n                        mem_trust[0] = mem_trust[0] + 0.35\n                    # maintain recent_points\n                    recent_points.append((x_cur.copy(), f_cur))\n                    if len(recent_points) > 12:\n                        recent_points.pop(0)\n                else:\n                    # small chance to run local parabolic refinement anyway\n                    if np.random.rand() < 0.03 and (self.budget - evals) >= 3:\n                        delta = 0.8 * step\n                        f_line, x_line = parabolic_refine(x_cur, f_cur, d, delta=delta)\n                        if f_line is not None and f_line < f_cur - 1e-12:\n                            delta_vec = x_line - x_cur\n                            norm_dv = np.linalg.norm(delta_vec)\n                            if norm_dv > 0:\n                                mem_dirs.insert(0, (delta_vec / norm_dv).copy())\n                                mem_alphas.insert(0, abs(delta))\n                                mem_trust.insert(0, 1.0)\n                                if len(mem_dirs) > self.memory_size:\n                                    mem_dirs.pop(); mem_alphas.pop(); mem_trust.pop()\n                            x_cur = x_line.copy(); f_cur = float(f_line)\n                            improved_round = True\n                            no_improve = 0\n                            step = min(step * 1.08, max_step)\n                            recent_points.append((x_cur.copy(), f_cur))\n                            if len(recent_points) > 12:\n                                recent_points.pop(0)\n\n            # End probe loop for this round\n\n            # Use candidate buffer: try a refinement on top candidates if no immediate improvement\n            if (not improved_round) and candidate_buffer and (self.budget - evals) >= 3:\n                candidate_buffer.sort(key=lambda z: z[0])\n                for idx in range(min(3, len(candidate_buffer))):\n                    f_try, x_try, d_try, alpha_try = candidate_buffer[idx]\n                    # attempt parabolic refine anchored at x_cur\n                    f_line, x_line = parabolic_refine(x_cur, f_cur, d_try, delta=max(abs(alpha_try), 0.8 * step))\n                    if f_line is not None and f_line < f_cur - 1e-12:\n                        delta_vec = x_line - x_cur\n                        norm_dv = np.linalg.norm(delta_vec)\n                        if norm_dv > 0:\n                            mem_dirs.insert(0, (delta_vec / norm_dv).copy())\n                            mem_alphas.insert(0, max(abs(alpha_try), 0.5 * step))\n                            mem_trust.insert(0, 1.0)\n                            if len(mem_dirs) > self.memory_size:\n                                mem_dirs.pop(); mem_alphas.pop(); mem_trust.pop()\n                        x_cur = x_line.copy(); f_cur = float(f_line)\n                        improved_round = True\n                        no_improve = 0\n                        step = min(step * 1.09, max_step)\n                        recent_points.append((x_cur.copy(), f_cur))\n                        if len(recent_points) > 12:\n                            recent_points.pop(0)\n                        break\n\n            # Step-size and trust updates\n            if not improved_round:\n                no_improve += 1\n                step = max(step * 0.86, min_step)\n                # decay older trusts\n                mem_trust = [t * 0.96 for t in mem_trust]\n            else:\n                # small decay to avoid runaway trusts\n                mem_trust = [min(t * 1.02, 100.0) for t in mem_trust]\n\n            # prune very low trust memories\n            if mem_trust:\n                keep = [i for i,t in enumerate(mem_trust) if t > 1e-3]\n                if len(keep) != len(mem_trust):\n                    mem_dirs = [mem_dirs[i] for i in keep]\n                    mem_alphas = [mem_alphas[i] for i in keep]\n                    mem_trust = [mem_trust[i] for i in keep]\n\n            # local polishing occasionally when improving or near stagnation\n            if improved_round and (self.budget - evals) >= 6:\n                # try few tiny orthogonal jitters\n                for _ in range(3):\n                    if evals >= self.budget:\n                        break\n                    jitter = np.random.randn(n)\n                    jitter = jitter / (np.linalg.norm(jitter) + 1e-20)\n                    alpha = np.random.uniform(-0.15 * step, 0.15 * step)\n                    f_j, x_j = safe_eval(clip_to_bounds(x_cur + alpha * jitter))\n                    if f_j is None:\n                        break\n                    if f_j < f_cur - 1e-12:\n                        delta_vec = x_j - x_cur\n                        norm_dv = np.linalg.norm(delta_vec)\n                        if norm_dv > 0:\n                            mem_dirs.insert(0, (delta_vec / norm_dv).copy())\n                            mem_alphas.insert(0, abs(alpha))\n                            mem_trust.insert(0, 1.0)\n                            if len(mem_dirs) > self.memory_size:\n                                mem_dirs.pop(); mem_alphas.pop(); mem_trust.pop()\n                        x_cur = x_j.copy(); f_cur = float(f_j)\n                        recent_points.append((x_cur.copy(), f_cur))\n                        if len(recent_points) > 12:\n                            recent_points.pop(0)\n\n            # stagnation: restart or final polish\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                no_improve = 0\n                if restarts > max_restarts:\n                    # final polish: many tiny probes around best until budget exhausted\n                    while evals < self.budget:\n                        d = np.random.randn(n)\n                        d /= (np.linalg.norm(d) + 1e-20)\n                        alpha = np.random.uniform(-0.12 * step, 0.12 * step)\n                        f_try, x_try = safe_eval(clip_to_bounds(x_cur + alpha * d))\n                        if f_try is None:\n                            break\n                        if f_try < f_cur - 1e-12:\n                            x_cur = x_try.copy(); f_cur = float(f_try)\n                            # update memory mildly\n                            dv = x_cur - x_best if x_best is not None else x_cur - x_cur\n                            if np.linalg.norm(dv) > 1e-20:\n                                mem_dirs.insert(0, dv / (np.linalg.norm(dv) + 1e-20))\n                                mem_trust.insert(0, 0.5)\n                                mem_alphas.insert(0, abs(alpha))\n                                if len(mem_dirs) > self.memory_size:\n                                    mem_dirs.pop(); mem_trust.pop(); mem_alphas.pop()\n                        step = max(step * 0.95, min_step)\n                        if step <= min_step:\n                            break\n                    break\n                else:\n                    # restart around best found so far (diversify)\n                    if x_best is not None:\n                        radius = min(1.0, 0.6 * (1.0 + restarts / 2.0)) * domain_size\n                        perturb = np.random.randn(n) * radius\n                        x_new = clip_to_bounds(x_best + perturb)\n                        f_new, x_new = safe_eval(x_new)\n                        if f_new is None:\n                            break\n                        x_cur = x_new.copy(); f_cur = float(f_new)\n                    else:\n                        # random restart\n                        x_cur = np.random.uniform(lb, ub)\n                        f_new, x_cur = safe_eval(x_cur)\n                        if f_new is None:\n                            break\n                        f_cur = float(f_new)\n                    # enlarge step to escape basin\n                    step = min(2.0 * step + 0.2 * domain_size, max_step)\n                    # partially decay memory trust to avoid sticking to stale directions\n                    mem_trust = [t * 0.35 for t in mem_trust]\n\n            # early exit if best sufficiently small\n            if f_best <= 1e-12:\n                break\n\n        # final fallback\n        if x_best is None:\n            x_best = x_cur.copy()\n            f_best = float(f_cur)\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 992, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: 'a' and 'p' must have same size", "error": "In the code, line 992, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: 'a' and 'p' must have same size", "parent_ids": "d3bac529-3666-4b22-8add-4fb85d47bfb4", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "da30a045-c6cf-4a01-8d9a-ffa2fc659abd", "fitness": "-inf", "name": "EDCLM", "description": "The algorithm mixes a space-filling max–min initialization with a CMA-style engine: mirrored sampling, weighted recombination, and adaptive sigma/diagonal+rank updates (ps, pc, C, eigen updates) but uses per-coordinate RMS scaling (coord_var with alpha=0.18) to handle anisotropy cheaply. It fits lightweight ensemble diagonal–quadratic local models on nearby archive points via distance-weighted ridge regression (linear + per-coordinate curvature), uses the model step (clamped by per-dim trust) and a principal-direction correction to propose expensive but informed candidates. A small directional memory (deque, mem_size=6) and moderate-probability heavy-tailed Cauchy/Levy jumps (p_jump≈0.18, cauchy_scale≈0.6) inject global exploration and seed jumps, while multiscale orthogonal directional probes act as a fallback when models or CMA steps fail. Practical engineering touches include archive pruning, occasional eigen decompositions for stability, capped per-iteration evaluations, and modest default population sizing to keep the method robust across dimensions and limited budgets.", "code": "import numpy as np\nfrom collections import deque\n\nclass EDCLM:\n    \"\"\"\n    Ensemble Directional CMA with Levy Memory (EDCLM)\n    One-line: space-filling init + CMA-style mirrored sampling with per-coordinate RMS scaling,\n    ensemble diagonal-quadratic local models and principal-direction correction, small directional memory\n    with occasional Cauchy/Levy jumps, and multiscale directional fallback probes.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_samples_ratio=0.12, min_init=None,\n                 mem_size=6, p_jump=0.18, cauchy_scale=0.6,\n                 model_every=15, max_eval_per_iter=80):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.init_samples_ratio = float(init_samples_ratio)\n        self.min_init = int(min_init) if min_init is not None else max(8, 2 * self.dim)\n        self.mem_size = int(mem_size)\n        self.p_jump = float(p_jump)\n        self.cauchy_scale = float(cauchy_scale)\n        self.model_every = int(model_every)\n        self.max_eval_per_iter = int(max_eval_per_iter)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n\n        # Basic CMA-like population sizes and weights\n        lam = max(4, int(4 + np.floor(3 * np.log(max(1, n)))))\n        mu = max(1, lam // 2)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # Covariance adaptation constants (CMA-ish)\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        c1 = 2 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1 - c1, 2 * (mu_eff - 2 + 1 / mu_eff) / ((n + 2) ** 2 + mu_eff))\n        damps = 1 + 2 * max(0, np.sqrt((mu_eff - 1) / (n + 1)) - 1) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # Initialize dynamic state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.25 * np.mean(rng_range)\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eig_every = max(1, int(12 * n))\n        eigen_counter = 0\n        gen_count = 0\n\n        # per-coordinate RMS scaling\n        coord_var = np.ones(n) * 1e-6\n        coord_alpha = 0.18\n\n        # directional memory\n        mem = deque(maxlen=self.mem_size)\n\n        # archive\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # Safe eval wrapper that respects budget and archives\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            if evals >= budget:\n                return None\n            x_clipped = np.clip(np.asarray(x, dtype=float), lb, ub)\n            try:\n                f = func(x_clipped)\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_X.append(x_clipped.copy()); archive_F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x_clipped.copy()\n            return float(f), x_clipped\n\n        # space-filling greedy initialization (max-min on a pool)\n        max_init = max(1, min(int(0.4 * budget), int(self.init_samples_ratio * budget + 0.5)))\n        init_budget = max(1, min(max_init, self.min_init))\n        pool_size = max(50, init_budget * 6)\n        pool = np.random.uniform(lb, ub, size=(pool_size, n))\n        chosen = []\n        for i in range(init_budget):\n            if i == 0:\n                idx = np.random.randint(0, pool_size)\n                chosen.append(pool[idx])\n            else:\n                cur = np.array(chosen)\n                # compute min distance from each pool point to chosen set\n                dists = np.min(np.linalg.norm(pool - cur[:, None, :], axis=2), axis=0)\n                idx = int(np.argmax(dists))\n                chosen.append(pool[idx])\n        for x in chosen:\n            if evals >= budget:\n                break\n            out = safe_eval(x)\n            if out is None:\n                break\n\n        if evals >= budget:\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # main optimization loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            improved = False\n\n            # Attempt ensemble diagonal-quadratic local model if archive large enough\n            neighbors_needed = max(2 * n + 1, 6 * n)\n            if len(archive_X) >= neighbors_needed and work_allow > 0:\n                X_arr = np.asarray(archive_X)\n                F_arr = np.asarray(archive_F)\n                dists = np.linalg.norm(X_arr - x_best, axis=1)\n                idxs = np.argsort(dists)[:min(len(dists), neighbors_needed)]\n                X_nei = X_arr[idxs]; F_nei = F_arr[idxs]\n                dx = X_nei - x_best\n                mrows = dx.shape[0]\n                # model f ≈ a + b^T dx + 0.5 * sum h_i dx_i^2\n                M = np.ones((mrows, 1 + 2 * n))\n                M[:, 1:1 + n] = dx\n                M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n                y = F_nei\n                # distance weight kernel\n                bandwidth = np.median(dists[idxs]) + 1e-12\n                w = np.exp(- (dists[idxs] ** 2) / (2.0 * (bandwidth ** 2 + 1e-12)))\n                w = w / (np.max(w) + 1e-12)\n                W = np.sqrt(w)[:, None]\n                A = W * M\n                b = W * y\n                ridge = 1e-6 * (1.0 + np.mean(np.abs(y)))\n                try:\n                    params, *_ = np.linalg.lstsq(A.T @ A + ridge * np.eye(A.shape[1]), A.T @ b, rcond=None)\n                    params = params.flatten()\n                    b_lin = params[1:1 + n]\n                    h_diag = params[1 + n:1 + 2 * n]\n                    # regularize Hessian diag (positive)\n                    h_reg = np.copy(h_diag)\n                    h_reg[h_reg < 1e-8] = 1e-8\n                    delta = - b_lin / (h_reg + 1e-20)\n                    # clamp by per-dim trust derived from coord_var and range\n                    trust = np.maximum(0.5 * np.sqrt(coord_var) * np.maximum(rng_range, 1e-12), 1e-9)\n                    delta_limited = np.clip(delta, -trust, trust)\n                    x_model = np.clip(x_best + delta_limited, lb, ub)\n                    # principal-direction correction (capture a mixed direction)\n                    try:\n                        Wdx = (W * dx).T\n                        cov_local = Wdx @ Wdx.T + 1e-12 * np.eye(n)\n                        vals, vecs = np.linalg.eigh(cov_local)\n                        principal = vecs[:, -1]\n                        proj = dx @ principal\n                        if np.ptp(proj) > 1e-12:\n                            s = np.sum(w * proj * y) / (np.sum(w * proj * proj) + 1e-12)\n                            step_dir = -np.sign(s) * np.minimum(np.linalg.norm(trust), np.linalg.norm(trust) * 0.7)\n                            x_corr = np.clip(x_best + principal * step_dir, lb, ub)\n                        else:\n                            x_corr = x_model\n                    except Exception:\n                        x_corr = x_model\n\n                    # evaluate model candidates in order: model then correction\n                    for x_cand in (x_model, x_corr):\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        # avoid duplicate last eval\n                        if len(archive_X) > 0 and np.allclose(x_cand, archive_X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_cand)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_cand, x_cand = out\n                        if f_cand < f_best - 1e-12:\n                            improved = True\n                            # expand sigma/trust slightly on success\n                            sigma = max(sigma * 1.12, 1e-12)\n                            # update directional memory with the model step (normalized)\n                            ystep = (x_cand - x_best)\n                            yn = np.linalg.norm(ystep)\n                            if yn > 1e-12:\n                                mem.append((ystep / yn).copy())\n                            break\n                    if not improved:\n                        # shrink sigma/trust gently when model fails\n                        sigma *= 0.88\n                except Exception:\n                    # model fitting failed; skip\n                    pass\n\n            # If model not applied or not improved, do CMA-like mirrored generation as main engine\n            if not improved and work_allow > 0 and evals < budget:\n                # plan number of offspring this generation up to lam but not exceeding work_allow\n                planned = min(lam, work_allow)\n                # mirrored sampling\n                if planned % 2 == 0:\n                    half = planned // 2\n                    zs = np.random.randn(half, n)\n                    zs = np.vstack([zs, -zs])\n                else:\n                    pairs = (planned + 1) // 2\n                    zs = np.random.randn(pairs, n)\n                candidates = []\n                for z in zs[:planned]:\n                    # transform using B*(D * z) ~ B @ (D * z)\n                    y = (B * D) @ z\n                    scaled_y = y * np.sqrt(coord_var)\n                    x = m + sigma * scaled_y\n                    # occasional memory Cauchy jump\n                    if mem and (np.random.rand() < self.p_jump):\n                        u = mem[np.random.randint(len(mem))]\n                        jump = np.tan(np.pi * (np.random.rand() - 0.5))\n                        x = x + (sigma * self.cauchy_scale) * jump * u\n                    x = np.clip(x, lb, ub)\n                    candidates.append(x)\n                # evaluate candidates until budget or planned exhausted\n                arfit = np.full(len(candidates), np.inf)\n                for k, x in enumerate(candidates):\n                    if evals >= budget or work_allow <= 0:\n                        break\n                    out = safe_eval(x)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    f, xret = out\n                    arfit[k] = f\n\n                # selection & recombination\n                valid = np.isfinite(arfit)\n                if np.any(valid):\n                    arx = np.array(candidates)[valid]\n                    arfit2 = arfit[valid]\n                    idxs = np.argsort(arfit2)\n                    sel = idxs[:min(mu, len(idxs))]\n                    x_sel = arx[sel]\n                    # recompute mean\n                    m_old = m.copy()\n                    if len(sel) > 0:\n                        wlen = min(len(sel), len(weights))\n                        m = np.sum(weights[:wlen, None] * x_sel[:wlen], axis=0)\n                        y_sel = (x_sel - m_old[np.newaxis, :]) / (sigma + 1e-20)\n                        y_w = np.sum(weights[:wlen, None] * y_sel[:wlen], axis=0)\n                    else:\n                        y_w = np.zeros(n)\n\n                    # update ps/pc using invsqrtC\n                    ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n                    norm_ps = np.linalg.norm(ps)\n                    denom = np.sqrt(1 - (1 - cs) ** (2 * (gen_count + 1)))\n                    hsig = 1.0 if (norm_ps / (denom + 1e-20) / chi_n) < (1.4 + 2 / (n + 1)) else 0.0\n                    pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n\n                    # covariance update\n                    rank_one = np.outer(pc, pc)\n                    rank_mu = np.zeros((n, n))\n                    if len(sel) > 0:\n                        for i in range(len(sel)):\n                            yi = y_sel[i][:, None]\n                            rank_mu += (weights[i] * (yi @ yi.T))\n                    C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n\n                    # per-coordinate RMS update from selected y_sel if any\n                    if len(sel) > 0:\n                        sec_mom = np.sum(weights[:len(sel)][:, None] * (y_sel ** 2), axis=0)\n                        coord_var = (1 - coord_alpha) * coord_var + coord_alpha * (sec_mom + 1e-12)\n                        coord_var = np.clip(coord_var, 1e-12, 1e6)\n\n                    # sigma adaptation\n                    sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1))\n                    if sigma < 1e-12:\n                        sigma = 1e-12\n\n                    # update directional memory with last weighted step\n                    yw_norm = np.linalg.norm(y_w)\n                    if yw_norm > 1e-12:\n                        mem.append((y_w / yw_norm).copy())\n\n                # count this as a generation\n                gen_count += 1\n                eigen_counter += int(np.sum(valid))\n\n            # If still not improved after both model and CMA block, do multiscale directional probing (fallback)\n            if not improved and evals < budget and work_allow > 0:\n                # create a few orthogonalized directions and multiscale probes as in No.5 idea\n                nd = min(8 + n // 4, 16)\n                Dlist = []\n                tries = 0\n                while len(Dlist) < nd and tries < nd * 6:\n                    v = np.random.randn(n)\n                    for u in Dlist:\n                        v = v - np.dot(u, v) * u\n                    nv = np.linalg.norm(v)\n                    if nv > 1e-12:\n                        Dlist.append(v / nv)\n                    tries += 1\n                base_step = np.maximum(0.5 * np.sqrt(coord_var) * np.maximum(rng_range, 1e-12), 1e-9)\n                scales = [0.25, 0.5, 1.0, 2.0]\n                for d in Dlist:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    for s in scales:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        step = s * np.mean(base_step)\n                        x_try = np.clip(x_best + d * step, lb, ub)\n                        if len(archive_X) > 0 and np.allclose(x_try, archive_X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_try, x_try = out\n                        if f_try < f_best - 1e-12:\n                            improved = True\n                            # nudge sigma\n                            sigma = max(sigma * 1.08, 1e-12)\n                            # update mem\n                            move = x_try - x_best\n                            mn = np.linalg.norm(move)\n                            if mn > 1e-12:\n                                mem.append((move / mn).copy())\n                            # update x_best done in safe_eval\n                            break\n                    if improved:\n                        break\n\n            # occasional memory-guided heavy-tailed jump (Cauchy)\n            if evals < budget and (np.random.rand() < self.p_jump):\n                if mem:\n                    u = mem[np.random.randint(len(mem))]\n                else:\n                    u = np.random.randn(n)\n                    u = u / (np.linalg.norm(u) + 1e-20)\n                jump = np.tan(np.pi * (np.random.rand() - 0.5))\n                x_jump = np.clip(x_best + (sigma * self.cauchy_scale) * jump * u, lb, ub)\n                out = safe_eval(x_jump)\n                if out is not None:\n                    f_jump, x_jump = out\n                    if f_jump < f_best - 1e-12:\n                        # accept and slightly enlarge sigma\n                        sigma = max(sigma * 1.2, 1e-12)\n                        # register move in mem\n                        move = x_jump - x_best\n                        mn = np.linalg.norm(move)\n                        if mn > 1e-12:\n                            mem.append((move / mn).copy())\n\n            # eigen decomposition occasionally to update B, D, invsqrtC\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(n); B = np.eye(n); D = np.ones(n); invsqrtC = np.eye(n)\n\n            # prune archive to reasonable size\n            max_archive = max(2000, 60 * n)\n            if len(archive_X) > max_archive:\n                idxs = np.argsort(archive_F)\n                keep_best = idxs[:300]\n                rest = idxs[300:]\n                stride = max(1, len(rest) // (max_archive - 300))\n                keep_rest = rest[::stride]\n                keep = np.concatenate([keep_best, keep_rest])\n                archive_X = [archive_X[i] for i in keep]\n                archive_F = [archive_F[i] for i in keep]\n\n            # small termination heuristic\n            if f_best <= 1e-12 or evals >= budget:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 73, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem = deque(maxlen=self.mem_size)", "error": "In the code, line 73, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem = deque(maxlen=self.mem_size)", "parent_ids": "d3bac529-3666-4b22-8add-4fb85d47bfb4", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "b5fa999a-6327-424d-8812-a4aa999bdb88", "fitness": "-inf", "name": "NEON", "description": "NEON combines a low-rank learned subspace (small k ~ log(dim)) with per-coordinate adaptive scales S and a global CSA-like sigma adapted via a path-length ps to balance directional exploitation and isotropic exploration. It samples candidates from a mixture of coordinate-wise Gaussian noise and projections onto learned template directions U (updated with an Oja/Sanger-like rule), augmented by occasional tempered Cauchy jumps and DE-style archive-difference injections for heavy tails and novelty. A small population (lam) with mirrored sampling, a geometric multi-scale bandit over relative sigmas (scale_pool_rel) and MAD-based smoothing for S provides multi-scale exploration and online selection of effective step sizes, while an archive and success memory guide diversity and template updates. Stagnation handling (sigma boost, pivot toward diverse archive points, partial U reinitialization) and strict bound clipping ensure escape from local traps and safe, budget-respecting evaluations.", "code": "import numpy as np\n\nclass NEON:\n    \"\"\"\n    NEON optimizer for continuous bound-constrained problems (noisy or noiseless).\n    - budget: total number of function evaluations allowed\n    - dim: dimensionality of search space\n    Optional kwargs:\n    - seed: RNG seed\n    - k: number of learned template directions (subspace rank)\n    - lam: population size override\n    - mirrored: use antithetic sampling for variance reduction\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, k=None, lam=None, mirrored=True):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population sizing: small, scales slowly with dim\n        if lam is None:\n            self.lam = max(6, int(6 + np.floor(1.8 * np.log(max(1, self.dim)))))\n        else:\n            self.lam = int(lam)\n        self.mu = max(1, int(np.ceil(self.lam / 3.0)))  # parents used for recombination\n\n        # learned templates k (low-dimensional directional templates)\n        if k is None:\n            self.k = max(1, int(np.ceil(np.log1p(self.dim))))  # small log-scale subspace\n        else:\n            self.k = min(max(1, int(k)), self.dim)\n\n        # exploration hyperparameters\n        self.mirrored = bool(mirrored)\n        self.p_cauchy = 0.07        # probability of a tempered Cauchy jump\n        self.cauchy_scale = 1.5     # scale multiplier for Cauchy jump\n        self.p_de = 0.30            # probability to use archive-difference injection\n        self.F_de = 0.85            # multiplier for archive differences\n        self.max_archive = 5000\n\n        # learning rates\n        self.c_scale = 0.12         # coordinate-scale smoothing\n        self.eta_oja = 0.08         # online template learning rate (Oja-like)\n        self.bandit_eta = 0.25      # bandit update strength for multi-scale selection\n\n        # internal bandit scales (geometric pool around base sigma)\n        self.n_scales = 6\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n\n        # base global sigma (a fraction of search range)\n        base_sigma = 0.18 * np.mean(ub - lb)\n        base_sigma = max(base_sigma, 1e-12)\n        sigma = base_sigma\n\n        # coordinate-wise scales (multiplicative factors on standard normals)\n        S = np.ones(n)\n\n        # learned template directions: random orthonormal initial basis (n x k)\n        if self.k >= 1:\n            R = np.random.randn(n, self.k)\n            try:\n                U, _ = np.linalg.qr(R)\n                U = U[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        # bandit over multiple scales: geometric around sigma\n        scale_pool_rel = np.geomspace(0.35, 3.0, num=self.n_scales)\n        scale_weights = np.ones(self.n_scales)  # multiplicative weights\n\n        # archive for DE-like differences and for novelty selection\n        archive_X = []\n        archive_F = []\n\n        # success memory (for template learning and novelty measurement)\n        success_mem = []\n        success_mem_max = max(12, 8 * self.k)\n\n        # path length variables (for sigma control)\n        ps = np.zeros(n)\n        weights = np.arange(self.mu, 0, -1.0)\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n        cs = 0.3 / (n + 0.3)\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n        damps = 1.0 + cs + 0.3 * np.sqrt(mu_eff / max(1.0, n))\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation at mean (counts towards budget)\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            f = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(f)\n            f_opt = f\n            x_opt = xm.copy()\n\n        last_impr_eval = evals\n        stagnation_window = max(6, int(2.0 * n))\n\n        # main optimization loop\n        while evals < budget:\n            # number of candidates we can still evaluate this generation (respect budget)\n            remaining = budget - evals\n            lam = min(self.lam, remaining)\n\n            # prepare candidate arrays\n            cand_x = np.zeros((lam, n))\n            cand_y = np.zeros((lam, n))  # normalized step y (such that x = m + sigma_rel * y)\n            cand_scale_idx = np.zeros(lam, dtype=int)\n\n            # sample per-candidate scale indices from bandit (softmax-like sampling)\n            probs = scale_weights / np.sum(scale_weights)\n            # To avoid degenerate probabilities, add small smoothing\n            probs = (probs + 1e-12) / np.sum(probs + 1e-12)\n\n            base_z = np.random.randn(lam, n)\n            for i in range(lam):\n                z = base_z[i].copy()\n\n                # choose a scale index (bandit)\n                idx = np.random.choice(self.n_scales, p=probs)\n                cand_scale_idx[i] = idx\n                sigma_rel = sigma * scale_pool_rel[idx]\n\n                # construct composed direction:\n                # coordinate-wise scaled gaussian part + learned template projection\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U @ z_low\n                    # randomized mixing coefficient: emphasize templates at higher dims slightly\n                    beta = np.random.beta(1.2, 3.0)  # favors smaller values but occasionally larger\n                    # combine: S * z contributes per-coordinate; low contributes directional template\n                    y = (S * z) * (1.0 - beta) + (np.median(S) * low) * beta\n                else:\n                    y = S * z\n\n                # occasional tempered Cauchy/Levy-like jump for heavy tails\n                if np.random.rand() < self.p_cauchy:\n                    # draw a t-distributed magnitude via Cauchy; temper to avoid huge values\n                    r = np.random.standard_cauchy()\n                    r = np.tanh(0.6 * r) * self.cauchy_scale  # temper the extreme tails\n                    dirn = z / (np.linalg.norm(z) + 1e-20)\n                    y = r * dirn * (np.median(S) + 1e-10)\n\n                # mirrored/antithetic sampling to reduce variance\n                if self.mirrored and (i % 2 == 1):\n                    y = -y\n\n                # apply archive-difference mutation sometimes (novelty-driven DE)\n                x = m + sigma_rel * y\n                if (len(archive_X) >= 2) and (np.random.rand() < self.p_de):\n                    # choose two archive points, prefer diverse ones by distance\n                    idxs = np.random.choice(len(archive_X), size=3, replace=False)\n                    # pick farthest pair among them\n                    sub = np.array(archive_X)[idxs]\n                    dists = np.sum((sub[:, None, :] - sub[None, :, :]) ** 2, axis=2)\n                    # find max dist pair (i<j)\n                    tri = np.triu_indices(3, k=1)\n                    pair_idx = np.argmax(dists[tri])\n                    pairs = list(zip(tri[0], tri[1]))\n                    a, b = pairs[pair_idx]\n                    de_dir = np.array(archive_X)[idxs[a]] - np.array(archive_X)[idxs[b]]\n                    x = x + self.F_de * de_dir * np.random.uniform(0.6, 1.1)  # stochastic scaling\n\n                # strict clipping to bounds\n                x = np.clip(x, lb, ub)\n\n                cand_x[i] = x\n                cand_y[i] = y\n\n            # evaluate candidates (respecting budget)\n            cand_f = np.full(lam, np.inf)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = cand_x[i]\n                fi = func(xi)\n                evals += 1\n                cand_f[i] = fi\n\n                # archive management\n                archive_X.append(xi.copy())\n                archive_F.append(fi)\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n\n                # update best\n                if fi < f_opt:\n                    f_opt = fi\n                    x_opt = xi.copy()\n                    last_impr_eval = evals\n\n            # selection: pick mu best of this generation\n            order = np.argsort(cand_f)\n            sel = order[:self.mu]\n            x_sel = cand_x[sel]\n            y_sel = cand_y[sel]\n            f_sel = cand_f[sel]\n\n            # recombine new mean (descending linear weights)\n            w = np.arange(self.mu, 0, -1.0)\n            w = w / np.sum(w)\n            m_old = m.copy()\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # weighted step in y-space\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # path length update for sigma adaptation (CSA-like)\n            # normalize by coordinate scales to be scale-invariant\n            invS = 1.0 / (np.sqrt(S) + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invS)\n            norm_ps = np.linalg.norm(ps)\n            # update base sigma (global center)\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = np.clip(sigma, 1e-12, 2.0 * np.mean(ub - lb) + 1e-12)\n\n            # update coordinate-wise scales S using robust MAD-like on selected y's\n            if y_sel.size > 0:\n                med = np.median(y_sel, axis=0)\n                mad = np.median(np.abs(y_sel - med), axis=0)\n                approx_std = 1.4826 * (mad + 1e-12)\n                # exponential smoothing\n                S = (1.0 - self.c_scale) * S + self.c_scale * (approx_std + 1e-12)\n                # keep scales within reasonable bounds\n                S = np.clip(S, 1e-8, 1e4)\n\n            # update learned templates U with an Oja-like online rule using y_w\n            if self.k > 0 and np.linalg.norm(y_w) > 0:\n                ynorm = y_w / (np.linalg.norm(y_w) + 1e-20)\n                # sequential Sanger/Oja-like updates with mild deflation\n                for j in range(self.k):\n                    uj = U[:, j]\n                    proj = np.dot(uj, ynorm)\n                    # Sanger variant that encourages uj to align with ynorm but keep orthogonality\n                    uj += self.eta_oja * (ynorm * proj - (proj ** 2) * uj)\n                    U[:, j] = uj\n                # orthonormalize columns occasionally to prevent degeneration\n                if np.random.rand() < 0.25:\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass  # keep U as is\n\n            # record success vector (for occasional PCA-like analyses and novelty)\n            success_mem.append(y_w.copy())\n            if len(success_mem) > success_mem_max:\n                success_mem.pop(0)\n\n            # Bandit (scale pool) reward update:\n            # Reward scales that were used by winners in this generation proportional to improvement\n            # compute generation best improvement vs last mean-eval if available\n            gen_best = np.min(cand_f)\n            # we attribute reward only if generation improved the global best or improved over m_old\n            # approximate m_old function value by nearest archive element or previous f_opt (conservative)\n            ref_val = f_opt if f_opt < np.inf else np.mean(archive_F) if archive_F else np.inf\n            # but more concretely, reward relative improvement among candidates w.r.t their parent's value\n            # we'll compute for each selected candidate whether it improved relative to the current best archive average\n            if np.isfinite(gen_best):\n                # normalized improvement magnitude (bounded)\n                improv = max(0.0, (ref_val - gen_best) / (abs(ref_val) + 1e-12))\n                # give credit to scale indices used by the top mu candidates\n                for j in sel:\n                    idx = cand_scale_idx[j]\n                    # multiplicative weight update\n                    scale_weights[idx] *= np.exp(self.bandit_eta * improv / (1.0 + 0.5 * np.sum((scale_weights == 0))))\n                # renormalize to keep numbers stable\n                scale_weights = np.maximum(scale_weights, 1e-8)\n\n            # Stagnation policy: diversify if no improvement for many evaluations\n            if (evals - last_impr_eval) > stagnation_window:\n                # enlarge sigma and randomly pivot mean towards a diverse archive point\n                sigma *= 2.0\n                if len(archive_X) > 0:\n                    # choose archive point with some probability biased by novelty (distance to mean)\n                    arch = np.array(archive_X)\n                    dists = np.linalg.norm(arch - m, axis=1)\n                    probs_arch = dists + 1e-12\n                    probs_arch /= np.sum(probs_arch)\n                    pick = np.random.choice(len(archive_X), p=probs_arch)\n                    # jump partly towards it with random interpolation\n                    alpha = np.random.uniform(0.3, 0.8)\n                    m = (1.0 - alpha) * m + alpha * arch[pick]\n                # clear some memory to encourage new templates\n                success_mem = []\n                # reinit some columns of U randomly to escape local degeneracy\n                if self.k > 0:\n                    idxs = np.random.choice(self.k, size=max(1, int(0.4 * self.k)), replace=False)\n                    for j in idxs:\n                        v = np.random.randn(n)\n                        U[:, j] = v / (np.linalg.norm(v) + 1e-12)\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n                last_impr_eval = evals  # avoid repeated immediate interventions\n\n            # enforce mean bounds\n            m = np.clip(m, lb, ub)\n\n            # small safeguard: keep scale weights normalized-ish\n            scale_weights = scale_weights / (np.mean(scale_weights) + 1e-12)\n\n        # end main loop\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "63bfe055-c94c-489c-a0d2-92204f5e88aa", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4e8342c3-7c48-4bc9-a323-07a8d4893fba", "fitness": "-inf", "name": "ARMS", "description": "ARMS combines robust per-coordinate scaling, a learned low-rank search subspace, archive-based differential mutations and occasional heavy-tailed Lévy jumps to balance local search and global exploration. It uses a relatively large population (lambda_ and mu biased up), quadratic selection weights, a conservative MAD-based coordinate scale D (initialized 0.7+0.3) with slow learning rate c_d=0.05, and a multiplicative median-success sigma adaptation (sigma_up=1.25, sigma_down=0.80, success_target≈0.2) starting from sigma=0.25*range. Subspace moves are learned from a normalized success_buffer via periodic SVD to form an orthonormal U of dimension k≈ceil(sqrt(n)), and each offspring mixes coordinate-wise Gaussian steps with U-based low-rank steps (alpha~Beta(2,3)); with p_levy=0.15 for frequent Lévy-like jumps and p_de=0.5 to add archive-difference DE perturbations. An archive of evaluated solutions and a stagnation handler (inflate sigma, pull mean toward top archived points, clear buffer) provide diversification and restart-like behavior while strict bound clipping and mirrored sampling are available for stability and symmetry.", "code": "import numpy as np\n\nclass ARMS:\n    \"\"\"\n    ARMS - Adaptive Rank-Mixture Search\n\n    One-line idea:\n      Combine robust per-coordinate scaling, a small learned subspace,\n      archive-based DE differences, and occasional heavy-tailed jumps,\n      using a median/quantile-success sigma adaptation (1/5-like) and\n      periodic low-rank re-learning from a success buffer.\n\n    Main algorithm parameters (defaults and semantic differences from AMSDLR):\n      - lambda_ : population size (here ~ max(12, 6 + 3*log(n))) -> larger baseline\n      - mu      : number of selected parents (~ ceil(lambda_/2)) -> more parents\n      - k       : subspace dimension (default ~ ceil(sqrt(n))) -> larger subspace than log-scale\n      - sigma   : global step-size (initialized to 0.25 * range) -> larger startup step\n      - D       : per-coordinate scale (MAD-based), learning rate c_d = 0.05 (slower)\n      - p_levy  : probability of heavy-tailed Lévy-like jump (here 0.15) -> more frequent\n      - levy_scale : heavier jumps by default (2.0)\n      - p_de    : probability of using archive-difference mutation (here 0.5) -> more frequent\n      - F_de    : DE factor for difference (here 0.6)\n      - mirrored: default False (different variance handling)\n      - sigma adaptation: median-success multiplicative rule (increase/decrease), not path-length CSA\n      - subspace_update_every: recompute top-k SVD from success_buffer every few generations\n      - success_buffer_size: keep recent successful y steps for subspace learning\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None,\n                 lambda_scale=3.0, init_sigma_frac=0.25, c_d=0.05,\n                 p_levy=0.15, levy_scale=2.0, p_de=0.5, F_de=0.6, mirrored=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population sizing (different shape than AMSDLR)\n        self.lambda_ = max(12, int(6 + lambda_scale * np.log(max(1, self.dim))))\n        self.mu = max(1, int(np.ceil(self.lambda_ / 2.0)))\n\n        # subspace dimension: prefer sqrt(n) by default (different from log1p)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(max(1, self.dim)))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # scales and probabilities\n        self.init_sigma_frac = float(init_sigma_frac)\n        self.c_d = float(c_d)\n        self.p_levy = float(p_levy)\n        self.levy_scale = float(levy_scale)\n        self.p_de = float(p_de)\n        self.F_de = float(F_de)\n        self.mirrored = bool(mirrored)\n\n        # subspace and buffer sizes / frequencies (different defaults)\n        self.success_buffer_size = max(8 * self.k, 20)\n        self.subspace_update_every = max(2, int(2 + np.ceil(self.dim / 8.0)))\n\n        # archive limits\n        self.max_archive = 2000\n\n        # sigma adaptation multipliers (median-success style)\n        self.sigma_up = 1.25\n        self.sigma_down = 0.80\n        self.success_target = 0.2  # target success rate ~1/5\n\n    def __call__(self, func):\n        n = self.dim\n        budget = self.budget\n\n        # bounds (Many Affine BBOB: typically -5..5)\n        # func.bounds.lb/ub can be scalars or arrays\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n\n        if lb.shape == ():\n            lb = np.full(n, float(lb))\n        if ub.shape == ():\n            ub = np.full(n, float(ub))\n\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n\n        # selection weights (use quadratic emphasis on top ranks, different shape)\n        ranks = np.arange(1, mu + 1)\n        weights = (mu + 1 - ranks) ** 2\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # initial state\n        m = np.random.uniform(lb, ub)                       # initialize mean uniformly within bounds\n        sigma = max(1e-12, self.init_sigma_frac * np.mean(ub - lb))\n        # per-coordinate robust scale (start slightly conservative)\n        D = 0.7 + 0.3 * np.ones(n)\n\n        # small orthonormal subspace (n x k)\n        if self.k >= 1:\n            R = np.random.randn(n, self.k)\n            try:\n                U, _ = np.linalg.qr(R)\n                U = U[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        # success buffer for subspace learning\n        success_buffer = []\n\n        # archive for DE differences\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation at mean (counts to budget)\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            f = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(f)\n            f_opt = f\n            x_opt = xm.copy()\n\n        # stagnation controls\n        last_improvement_eval = evals\n        stagnation_window = max(6, 6 * n)  # larger window before stronger interventions\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n\n            # draw base normals\n            base_z = np.random.randn(current_lambda, n)\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # subspace component\n                if self.k > 0 and U.shape[1] > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U @ z_low\n                else:\n                    low = np.zeros(n)\n\n                # randomized mixing alpha drawn from Beta to skew toward small local steps\n                # Beta(2,3) tends to favor local Gaussian while allowing some subspace contribution\n                alpha = np.random.beta(2.0, 3.0)\n\n                # mix coordinate-wise scaled gaussian and low-rank direction (NOT the same formula as AMSDLR)\n                # note: D scales coordinates multiplicatively\n                y_local = D * z\n                y_sub = np.mean(D) * low\n                y = (1.0 - alpha) * y_local + alpha * y_sub\n\n                # occasional heavier Lévy-style jump (more frequent and larger here)\n                if np.random.rand() < self.p_levy:\n                    # use a Cauchy magnitude modulated by levy_scale and a random direction\n                    r = np.random.standard_cauchy() * self.levy_scale\n                    dirn = z / (np.linalg.norm(z) + 1e-20)\n                    y = r * dirn * (np.median(D) + 1e-10)\n\n                # optional mirrored sampling (can be toggled off)\n                if self.mirrored and (i % 2 == 1):\n                    y = -y\n\n                x = m + sigma * y\n\n                # DE-style archive difference mutation with higher probability\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # strict clipping to bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection (choose best mu)\n            idx = np.argsort(arfit)\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            # recombine new mean with the quadratic weights\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # sigma update using a median-success (quantile) multiplicative rule:\n            # count successes within this generation relative to previous mean fitness estimate\n            # (we use archive_F last known f_opt as baseline for success counting)\n            baseline = f_opt\n            # In practice, count offspring that improved over baseline (conservative)\n            n_success = np.sum(arfit < baseline)\n            success_rate = float(n_success) / max(1.0, float(current_lambda))\n\n            if success_rate > self.success_target:\n                sigma *= self.sigma_up\n            else:\n                sigma *= self.sigma_down\n\n            # safeguard sigma in reasonable bounds\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 5.0 * np.mean(ub - lb) + 1e-12)\n\n            # Update per-coordinate D using MAD-like robust update (different learning c_d)\n            if y_sel.size > 0:\n                med = np.median(y_sel, axis=0)\n                mad = np.median(np.abs(y_sel - med), axis=0)\n                approx_std = 1.4826 * (mad + 1e-12)\n                D = (1.0 - self.c_d) * D + self.c_d * (approx_std + 1e-12)\n                # keep D bounded away from zero to avoid division issues\n                D = np.maximum(D, 1e-8)\n\n            # store normalized weighted step into success_buffer but only if it was useful (some improvement)\n            if np.any(arfit < baseline):\n                # normalize y_w to unit scale to avoid huge components dominating\n                norm_yw = np.linalg.norm(y_w) + 1e-20\n                success_buffer.append((y_w / norm_yw).copy())\n            else:\n                # occasionally still keep the step to retain diversity\n                if np.random.rand() < 0.15:\n                    norm_yw = np.linalg.norm(y_w) + 1e-20\n                    success_buffer.append((y_w / norm_yw).copy())\n\n            # trim buffer\n            if len(success_buffer) > self.success_buffer_size:\n                success_buffer.pop(0)\n\n            # update low-rank subspace occasionally using SVD of buffered vectors (different freq/technique)\n            if (len(success_buffer) >= max(4, self.k)) and (evals % self.subspace_update_every == 0):\n                # build Y: n x m (each column is an included step)\n                Y = np.vstack(success_buffer).T  # n x m\n                # center columns (though they are normalized, subtract mean for stability)\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    # compute compact SVD; pick top-k left singular vectors as new U\n                    U_new, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                    # ensure we have at least k columns\n                    if U_new.shape[1] >= self.k:\n                        U = U_new[:, :self.k].copy()\n                    else:\n                        # pad with random orthonormal columns if necessary\n                        R = np.random.randn(n, self.k - U_new.shape[1])\n                        Q, _ = np.linalg.qr(R)\n                        U = np.hstack([U_new, Q[:, :self.k - U_new.shape[1]]])[:, :self.k]\n                except np.linalg.LinAlgError:\n                    # if SVD fails, keep previous U\n                    pass\n\n            # stagnation intervention: if no improvement for long, diversify aggressively\n            if (evals - last_improvement_eval) > stagnation_window:\n                # inflate sigma, jump mean towards a random archive good point, and clear buffer\n                sigma *= 3.0\n                if len(archive_X) > 0:\n                    # pick a reasonably good archived point (top 10%)\n                    topk = max(1, int(0.1 * len(archive_X)))\n                    order = np.argsort(archive_F)\n                    pick_idx = np.random.choice(order[:topk])\n                    # move mean towards that archived point plus a randomized perturbation\n                    m = 0.6 * m + 0.4 * (archive_X[pick_idx] + 0.1 * (np.random.randn(n) * np.mean(D)))\n                    # clip\n                    m = np.clip(m, lb, ub)\n                # reset buffer to allow fresh learning\n                success_buffer = []\n\n            # enforce mean bounds every generation\n            m = np.clip(m, lb, ub)\n\n            # loop continues until budget exhausted\n\n        # return best found\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 219, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (6,1) (3,2) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "error": "In the code, line 219, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (6,1) (3,2) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "parent_ids": "63bfe055-c94c-489c-a0d2-92204f5e88aa", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "eb0fcc86-3b27-4bb0-a346-ec89b8e5849f", "fitness": "-inf", "name": "ORBIDE", "description": "1) ORBIDE builds a diagonal+low-rank preconditioner: a per-coordinate Adagrad-like accumulator G (beta_G=0.12) gives S = 1/sqrt(G) for coordinate-wise scaling, while a learned low-rank orthonormal basis U (r = max(1,floor(sqrt(dim)))) and a small latent transform B inject structured, correlated moves — B is derived from projected eigen-energies and U is updated by streaming power-iteration on a rolling success covariance C.  \n2) Sampling mixes exploration modes: mirrored Gaussian sampling around the mean with randomized blending between diagonal and low-rank steps, occasional heavy-tailed Cauchy jumps (p_cauchy=0.06, scale=1.5) for long escapes, and archive-difference DE-style mutations (p_de=0.30, F_de=0.85) scaled by a crowding factor to encourage diverse moves.  \n3) Selection and control use an evolution-strategy style batch (lambda ≈ max(6, 6+1.5 log dim), mu ≈ lambda/3) with exponential recombination weights, a path-length-like sigma controller (cs,damps,chi_n) initialized with sigma0_frac=0.20 and safeguarded, plus a stagnation policy that inflates sigma (factor 2.2) and partially restarts the mean toward archived points.  \n4) Practical robustness comes from a bounded archive (max_archive=5000), a success buffer sized buffer_max = max(10*r,12) to keep C lightweight, QR orthonormalization for U, clipping to bounds, and conservative parameter choices (eta_U=0.6, small QR perturbations) to stabilize learning while remaining exploratory.", "code": "import numpy as np\n\nclass ORBIDE:\n    \"\"\"\n    ORBIDE - Orthogonalized Random-Basis Implicit Differential Evolution\n\n    Key ideas (concise):\n    - Precondition samples by a multiplicative per-coordinate scale S and a learned low-rank orthonormal basis U,\n      combined via a small latent transform B. This yields a diagonal+low-rank preconditioner without storing a full covariance.\n    - U is learned from a rolling success buffer using a streaming (power-iteration style) eigen-update on the sum of outer products.\n    - Per-coordinate adaptation follows an Adagrad-like accumulator on successful step magnitudes, producing S = 1/sqrt(G).\n    - Candidates sometimes use archive-difference mutations scaled by their local crowding (to favor exploration near diverse regions).\n    - Occasional heavy-tailed (Cauchy) jumps enable long-range escape.\n    - Mirrored sampling, linear/exponential recombination weights, path-length-like sigma control, and a stagnation restart policy.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, latent_r=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # population sizing\n        self.lambda_ = max(6, int(np.ceil(6.0 + 1.5 * np.log(max(1, self.dim)))))  # somewhat larger than minimal\n        self.mu = max(1, int(np.ceil(self.lambda_ / 3.0)))\n\n        # latent subspace dimension (choose sqrt-scale for a different behavior)\n        if latent_r is None:\n            self.r = max(1, int(np.floor(np.sqrt(max(1, self.dim)))))\n        else:\n            self.r = min(max(1, int(latent_r)), self.dim)\n\n        # adaptation hyper-parameters\n        self.sigma0_frac = 0.20   # initial sigma = frac * mean(range)\n        self.p_cauchy = 0.06      # occasional heavy-tailed jump\n        self.cauchy_scale = 1.5\n        self.p_de = 0.30          # probability of archive-difference mutation\n        self.F_de = 0.85\n        self.mirrored = True\n\n        # streaming adaptation rates\n        self.beta_G = 0.12        # Adagrad-like accumulator rate (for G)\n        self.eta_U = 0.6          # learning rate for streaming eigen-direction power-step (intermediate)\n        self.buffer_max = max(10 * self.r, 12)\n\n        # stagnation and archive controls\n        self.max_archive = 5000\n        self.stagnation_factor = 2.2\n        self.restart_fraction = 0.5\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # read bounds; Many Affine BBOB uses -5..5 typically, but use provided\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # init mean and state\n        m = np.random.uniform(lb, ub)\n        sigma = max(1e-12, self.sigma0_frac * np.mean(ub - lb))\n\n        # per-coordinate accumulator G and scale S = 1/sqrt(G)\n        G = np.ones(n) * 1e-6  # prevent division by zero\n        S = 1.0 / (np.sqrt(G) + 1e-12)\n\n        # low-rank orthonormal basis U (n x r) and small latent transform B (r x r)\n        # initialize U as random orthonormal columns\n        if self.r >= 1:\n            R = np.random.randn(n, self.r)\n            try:\n                Q, _ = np.linalg.qr(R)\n                U = Q[:, :self.r]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.r))\n            B = np.eye(self.r)\n        else:\n            U = np.zeros((n, 0))\n            B = np.zeros((0, 0))\n\n        # path-like state for sigma control (different constants)\n        ps = np.zeros(n)\n        # weight design: exponential descending weights (different from AMSDLR)\n        weights = np.exp(-np.arange(self.mu))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        cs = 0.45 / (n + 0.45)\n        damps = 1.0 + 1.5 * cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0/(4.0*n) + 1.0/(21.0*n**2))\n\n        # success buffer to construct streaming covariance C = sum y y^T\n        success_buffer = []\n        # maintain a cumulative C matrix for streaming updates (n x n), updated incrementally from buffer\n        C = np.zeros((n, n))\n\n        # archive\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # Evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            f = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(f)\n            f_opt = float(f)\n            x_opt = xm.copy()\n            last_improvement = evals\n\n        stagnation_window = max(6, int(2.0 * n))\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            # ensure mirrored pairs when used\n            if self.mirrored and (lam % 2 == 1) and (lam > 1):\n                lam -= 1\n\n            arx = np.zeros((lam, n))\n            arz = np.zeros((lam, n))\n            arfit = np.full(lam, np.inf)\n\n            # prepare random seeds for this batch\n            z_batch = np.random.randn(lam, n)\n            z_latent = np.random.randn(lam, self.r) if self.r > 0 else np.zeros((lam, 0))\n\n            for i in range(lam):\n                z = z_batch[i].copy()\n                z_r = z_latent[i].copy() if self.r > 0 else np.zeros(0)\n\n                # build preconditioned step: diagonal + low-rank mix with randomized alpha\n                alpha = np.random.uniform(0.05, 0.85)\n                step_diag = S * z    # coordinate-wise scaled Gaussian\n                step_low = np.zeros(n)\n                if self.r > 0:\n                    step_low = U @ (B @ z_r)\n                y = (1.0 - alpha) * step_diag + alpha * step_low\n\n                # occasional heavy-tailed Cauchy jump (long escapes)\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    # direction biased by S\n                    dirn = z / (np.linalg.norm(z) + 1e-20)\n                    y = r * dirn * (np.median(S) + 1e-10)\n\n                # mirrored pairing\n                if self.mirrored and (i % 2 == 1):\n                    y = -y\n\n                x = m + sigma * y\n\n                # crowding-aware archive-difference mutation:\n                # pick two archive points with preference for diverse choices relative to x (inverse similarity)\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    # compute distances to archive (if not too big)\n                    A = np.asarray(archive_X)\n                    dists = np.linalg.norm(A - x, axis=1) + 1e-12\n                    # choose first index with probability proportional to d (more distant more likely)\n                    probs = dists / np.sum(dists)\n                    i1 = np.random.choice(len(archive_X), p=probs)\n                    # choose second index uniformly but distinct\n                    choices = list(range(len(archive_X)))\n                    choices.remove(i1)\n                    i2 = np.random.choice(choices)\n                    de_vec = self.F_de * (archive_X[i1] - archive_X[i2])\n                    # scale down DE mutation if x is very crowded near archive (crowding factor)\n                    crowd = np.mean(1.0 / (dists + 1e-6))\n                    crowd_factor = 1.0 / (1.0 + 0.1 * crowd)\n                    x = x + de_vec * crowd_factor\n\n                # strict clipping\n                x = np.clip(x, lb, ub)\n\n                # evaluate candidate if we still have budget\n                if evals >= budget:\n                    break\n                f = func(x)\n                evals += 1\n\n                arx[i] = x\n                arz[i] = y\n                arfit[i] = f\n\n                # archive maintenance\n                archive_X.append(x.copy())\n                archive_F.append(float(f))\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n                    last_improvement = evals\n\n            # selection among evaluated (fillers may be inf)\n            valid = ~np.isinf(arfit)\n            if np.any(valid):\n                idx = np.argsort(arfit)\n                sel_idx = idx[:self.mu]\n                x_sel = arx[sel_idx]\n                y_sel = arz[sel_idx]\n\n                # recombine mean using exponential weights\n                weights_eff = weights.copy()\n                if len(sel_idx) < self.mu:\n                    weights_eff = weights_eff[:len(sel_idx)]\n                    weights_eff = weights_eff / np.sum(weights_eff)\n                m_old = m.copy()\n                m = np.sum(weights_eff[:, None] * x_sel, axis=0)\n                y_w = np.sum(weights_eff[:, None] * y_sel, axis=0)\n\n                # update per-coordinate accumulator G (Adagrad-style on successful weighted y)\n                # Note: use squared step magnitudes (y_w**2 scaled by sigma^2)\n                G = (1.0 - self.beta_G) * G + self.beta_G * ( (sigma * y_w) ** 2 )\n                S = 1.0 / (np.sqrt(G) + 1e-12)\n\n                # streaming update of covariance surrogate C and success buffer\n                success_buffer.append(y_w.copy())\n                C += np.outer(y_w, y_w)\n                if len(success_buffer) > self.buffer_max:\n                    # evict oldest buffer item and subtract its contribution from C\n                    old = success_buffer.pop(0)\n                    C -= np.outer(old, old)\n\n                # streaming power-iteration style update of U (if enough data)\n                if (self.r > 0) and (len(success_buffer) >= max(2, self.r)):\n                    # one power-step: U <- orthonormalize( (I - gamma) * U + gamma * C @ U / ||C @ U|| )\n                    # compute C@U\n                    CU = C @ U  # n x r\n                    # normalize columns to avoid scale blowup\n                    norms = np.linalg.norm(CU, axis=0) + 1e-20\n                    CU_normalized = CU / norms\n                    # blend and orthonormalize\n                    U = (1.0 - self.eta_U) * U + self.eta_U * CU_normalized\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.r]\n                    except np.linalg.LinAlgError:\n                        pass\n\n                    # small latent transform B: diagonal from projected eigenenergies (stabilizing low-rank magnitude)\n                    # compute small r x r matrix R = U^T C U, get eigenvalues and set B = diag(1/sqrt(eig+eps))\n                    Rt = U.T @ (C @ U)\n                    try:\n                        eigs = np.linalg.eigvalsh(Rt)\n                        eigs = np.maximum(eigs, 1e-12)\n                        B = np.diag(1.0 / (np.sqrt(eigs) + 1e-12))\n                    except np.linalg.LinAlgError:\n                        B = np.eye(self.r)\n\n                # path-like sigma control (but using S scaled coordinates)\n                invS = 1.0 / (S + 1e-12)\n                ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invS)\n                norm_ps = np.linalg.norm(ps)\n                sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n                # safeguard sigma\n                sigma = np.clip(sigma, 1e-12, 2.0 * np.mean(ub - lb) + 1e-12)\n\n                # stagnation intervention\n                if (evals - last_improvement) > stagnation_window:\n                    # inflate sigma and partially restart mean toward a diverse archive point\n                    sigma *= self.stagnation_factor\n                    if len(archive_X) > 0:\n                        pick = np.random.randint(len(archive_X))\n                        m = (1.0 - self.restart_fraction) * m + self.restart_fraction * np.array(archive_X[pick])\n                    # refresh S and success buffer softly\n                    G = (1.0 - 0.5 * self.beta_G) * G + 0.5 * self.beta_G * np.ones_like(G) * (np.mean(G) + 1e-8)\n                    S = 1.0 / (np.sqrt(G) + 1e-12)\n                    success_buffer = []\n                    C = np.zeros((n, n))\n                    # small random perturbation of U to encourage exploration\n                    if self.r > 0:\n                        Rn = np.random.randn(n, self.r) * 0.03 * np.mean(ub - lb)\n                        try:\n                            Q, _ = np.linalg.qr(U + Rn)\n                            U = Q[:, :self.r]\n                        except np.linalg.LinAlgError:\n                            pass\n\n                # ensure mean stays in bounds\n                m = np.clip(m, lb, ub)\n\n            else:\n                # no valid candidates (shouldn't happen normally), perform a small random restart sample\n                m = np.random.uniform(lb, ub)\n                if evals < budget:\n                    xm = np.clip(m, lb, ub)\n                    f = func(xm)\n                    evals += 1\n                    archive_X.append(xm.copy())\n                    archive_F.append(float(f))\n                    if f < f_opt:\n                        f_opt = float(f)\n                        x_opt = xm.copy()\n                        last_improvement = evals\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "63bfe055-c94c-489c-a0d2-92204f5e88aa", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "379631a2-5b6f-4091-a561-23531d796034", "fitness": 0.18664087451138478, "name": "LISAR", "description": "LISAR is a hybrid, CMA-like heuristic that uses a modest population (lambda ≈ 6 + 3 log n, mu ≈ lambda/2) and a small learned low-rank subspace (k ≈ floor(sqrt(n)/2) ) to balance global exploration and focused search. Mutations combine per-coordinate scaling D, low-rank subspace draws, occasional tempered Lévy (Cauchy) long jumps, Gaussian scaling mixtures, and DE-style differences sampled from an archive to introduce both heavy-tailed and directional diversity. Adaptation uses exponential-ranked recombination weights, a path-length ps (normalized by D) to multiplicatively update sigma, and a robust winsorized-coordinate std to slowly update D; successful steps are stored in a buffer to drive statistics. Subspace learning extracts eigenvectors of the Gram matrix of recent successes to update U, and a stagnation policy cools sigma, jitters D and partially restarts the mean toward the archive centroid to recover from stalls, with strict bound clipping and an evaluation-budget-aware generation loop.", "code": "import numpy as np\n\nclass LISAR:\n    \"\"\"\n    LISAR - Log-Indexed Subspace Adaptive Restart\n\n    Key design choices deliberately different from the provided AMSDLR:\n    - lambda_: larger/population scaling ~ 6 + 3*log(n)\n    - mu: roughly half the population (promotes stronger selection)\n    - k: subspace dimension = floor(sqrt(n)/2) (different scaling than log(n) or sqrt(n))\n    - sigma: initial step-size = 0.25 * median(range)\n    - D update: winsorized std -> smoothed with c_d = 0.05 (smaller, slower)\n    - p_levy: moderate 0.12, levy_scale = 0.9 (different frequency/scale)\n    - p_de: 0.25, F_de = 0.6 (different probability and strength)\n    - recombination weights: exponential-ranked (not linear)\n    - path-length control: different cs and damps formulas\n    - subspace learning: covariance / gram eig on winsorized success steps\n    - stagnation policy: cool sigma and partial restart toward archive centroid\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population sizing (different scale)\n        self.lambda_ = max(6, int(np.ceil(6.0 + 3.0 * np.log(max(1, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace dimension (different formula: modest sqrt-scaling)\n        if subspace_k is None:\n            self.k = max(1, int(np.floor(np.sqrt(max(1, self.dim)) / 2.0)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # adaptation and mutation settings (different from AMSDLR)\n        self.c_d = 0.05           # slower per-coordinate scale adaptation (winsorized std)\n        self.p_levy = 0.12        # probability of Lévy-like long jump\n        self.levy_scale = 0.9     # scale of heavy-tailed jump\n        self.p_de = 0.25          # DE-like difference probability\n        self.F_de = 0.6           # DE scaling factor\n        self.mirrored = False     # do not rely on strict antithetic pairing here\n        self.max_archive = 2000\n\n        # other knobs\n        self.buffer_factor = 8    # how many successes to keep per k (multiplied)\n        self.subspace_every = 5   # recompute subspace every few generations (in evals modulo sense)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = self.budget\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n\n        # exponential-ranked recombination weights (different from linear)\n        ranks = np.arange(mu, dtype=float)  # 0..mu-1\n        # sharper emphasis on best members\n        decay = max(1.0, mu / 3.0)\n        weights = np.exp(-ranks / decay)\n        weights = weights[::-1]  # reverse so best has highest weight (we will sort by best)\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length constants (different formula)\n        cs = 0.25 / (np.sqrt(n) + 0.2)\n        damps = 1.0 + 1.2 * cs + 0.1 * np.sqrt(mu_eff / max(1.0, n))\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        # start mean uniformly at random in bounds (diverse init)\n        m = np.random.uniform(lb, ub)\n        sigma = 0.25 * np.median(ub - lb)   # different starting multiplier\n        D = np.ones(n)                      # per-coordinate multipliers (neutral 1)\n        # initialise low-rank orthonormal basis U (n x k)\n        if self.k >= 1:\n            R = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(R)\n                U = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        ps = np.zeros(n)\n\n        # success buffer for subspace learning\n        success_buffer = []\n        buffer_max = max(6 * self.k, self.buffer_factor * self.k, 12)\n\n        # archive for DE and centroid / elite\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation: sample a handful (1) at mean for initial anchor\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            f = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(f)\n            f_opt = f\n            x_opt = xm.copy()\n\n        # stagnation control differently tuned\n        last_improvement_eval = evals\n        stagnation_window = max(8, int(2.0 * n))  # longer window than AMSDLR\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n\n            # produce base normals for this generation\n            base_z = np.random.randn(current_lambda, n)\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # randomized mixing alpha from a Beta to vary bias toward coordinate or subspace\n                alpha = np.random.beta(2.0, 5.0)  # typically small -> favor low-rank sometimes\n                if self.k > 0 and np.random.rand() < 0.95:\n                    z_low = np.random.randn(self.k)\n                    low = U @ z_low\n                    # introduce a modest per-coordinate rescaling factor and a small heteroskedastic noise\n                    y_coord = (D * z) * (1.0 + 0.3 * np.random.randn(n) * 0.0)  # no extra noise by default\n                    y = alpha * y_coord + (1.0 - alpha) * (np.mean(D) * low)\n                else:\n                    y = D * z\n\n                # occasional moderate Gaussian mixture (wider step)\n                if np.random.rand() < 0.08:\n                    y = 1.5 * y\n\n                # Lévy-like occasional heavy tail (tempered)\n                if np.random.rand() < self.p_levy:\n                    # generate moderate Cauchy and temper it with normal factor\n                    r = np.random.standard_cauchy() * self.levy_scale\n                    # temper extreme tails\n                    r = np.tanh(r) * 5.0\n                    dirn = z / (np.linalg.norm(z) + 1e-20)\n                    y = r * dirn * (np.median(D) + 1e-10)\n\n                # apply DE-like archive difference sometimes\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    # mix DE difference into y-space scaled by 1/sigma\n                    y = y + (de_mut / (sigma + 1e-20))\n\n                x = m + sigma * y\n\n                # strict clipping into bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates (respect budget)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                # maintain archive\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection: sort by fitness and take mu best\n            idx = np.argsort(arfit)\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            m_old = m.copy()\n            # recombine new mean: apply exponential weights to positions (best first)\n            # Note: weights were constructed such that higher weight corresponds to better rank\n            # However since we sorted idx ascending (best first), align weights accordingly.\n            w = weights.copy()\n            # If mu < len(weights) slice accordingly\n            if len(w) > mu:\n                w = w[-mu:]\n            w = w / np.sum(w)\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # path ps update uses D (not sqrt(D)) as a different normalization\n            invD = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invD)\n            norm_ps = np.linalg.norm(ps)\n\n            # sigma update: similar multiplicative rule with different damping\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 3.0 * np.mean(ub - lb) + 1e-12)\n\n            # Update per-coordinate D using winsorized std of selected y's (robust alternative)\n            if y_sel.size > 0:\n                ys = np.array(y_sel)\n                # compute winsorized coordinate-wise std (10%-90% clipping)\n                low_q = 0.10\n                high_q = 0.90\n                q_low = np.quantile(ys, low_q, axis=0)\n                q_high = np.quantile(ys, high_q, axis=0)\n                ys_clipped = np.minimum(np.maximum(ys, q_low[None, :]), q_high[None, :])\n                coord_std = np.std(ys_clipped, axis=0, ddof=0) + 1e-12\n                # combine with previous D using exponential smoothing\n                D = (1.0 - self.c_d) * D + self.c_d * coord_std\n\n            # store success step (y_w scaled) into success buffer (centered)\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace occasionally using small-gram eig (covariance of successes)\n            if (len(success_buffer) >= max(3, self.k)) and (evals % self.subspace_every == 0):\n                Y = np.vstack(success_buffer).T  # n x m\n                # center columns\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    G = Y.T @ Y\n                    G = (G + G.T) / 2.0\n                    eigvals, eigvecs = np.linalg.eigh(G)\n                    order = np.argsort(eigvals)[::-1]\n                    take = min(self.k, len(order))\n                    U_new = np.zeros((n, self.k))\n                    for j in range(take):\n                        v = eigvecs[:, order[j]]\n                        lamj = eigvals[order[j]]\n                        if lamj > 1e-14:\n                            U_new[:, j] = (Y @ v) / np.sqrt(lamj)\n                        else:\n                            U_new[:, j] = np.random.randn(n)\n                    # orthonormalize\n                    try:\n                        Q, _ = np.linalg.qr(U_new)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n                except np.linalg.LinAlgError:\n                    pass\n\n            # stagnation handling: different policy - partial restart toward archive centroid and cool sigma\n            if (evals - last_improvement_eval) > stagnation_window:\n                # cool down sigma (exploit) and move mean toward centroid of top 10% archive\n                sigma *= 0.6\n                if len(archive_X) > 5:\n                    sorted_idx = np.argsort(archive_F)\n                    topk = max(1, int(0.1 * len(sorted_idx)))\n                    top_idx = sorted_idx[:topk]\n                    centroid = np.mean(np.array(archive_X)[top_idx], axis=0)\n                    # move partially toward centroid with a random interpolation factor\n                    beta = np.random.uniform(0.3, 0.7)\n                    m = (1.0 - beta) * m + beta * centroid\n                # jitter per-coordinate scales to encourage new directions\n                D = D * (1.0 + 0.2 * (np.random.rand(n) - 0.5))\n                # clear buffer to relearn subspace\n                success_buffer = []\n                last_improvement_eval = evals  # avoid repeated triggers immediately\n\n            # ensure mean is still inside bounds\n            m = np.clip(m, lb, ub)\n\n        # return best found\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm LISAR scored 0.187 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "63bfe055-c94c-489c-a0d2-92204f5e88aa", "operator": null, "metadata": {"aucs": [0.11497602870784585, 0.1553137438294161, 0.24644555358952347, 0.14378205398695854, 0.20829256960608122, 0.21168434387862567, 0.24933239839492616, 0.2142693240757103, 0.18092487524534562, 0.14138785379941488]}, "task_prompt": ""}
{"id": "792cad44-1b70-4f9e-be4a-99117c215645", "fitness": 0.31991872785498704, "name": "HALCLM", "description": "HALC-LM is a hybrid strategy that combines CMA-like path-length sigma adaptation with a robust per-coordinate MAD scale (D) and a learned low-rank search subspace U (SVD on a buffer of recent successful weighted steps) to focus search along promising directions. It mixes multiple exploration mechanisms — mirrored (antithetic) sampling for variance reduction, occasional Lévy/Cauchy heavy-tailed jumps and archive-based DE differences (p_de, F_de) — while keeping population sizing conservative (lambda ≈ 4+3·log(n), mu ≈ lambda/2) and subspace size k ≈ ceil(sqrt(n)) for scalability. Key parameter choices encourage robustness and adaptivity: sigma starts at 0.18·range, D is smoothed with c_d=0.18 via MAD estimates, archive and buffer sizes limit memory, and recombination uses log-weights with mu_eff for stable adaptation. Stagnation handling escalates exploration by inflating sigma, raising p_de/p_levy, nudging the mean toward archived bests and relearning U, while clipping enforces bounds throughout.", "code": "import numpy as np\n\nclass HALCLM:\n    \"\"\"\n    HALC-LM: Hybrid Adaptive Low-rank CMA-DE with Lévy and Mirrored sampling.\n\n    One-line: Combine robust per-coordinate MAD scaling with a learned low-rank search\n    subspace, mirrored sampling, archive DE differences and occasional Lévy jumps,\n    controlled by a CMA-like path-length sigma adaptation and adaptive stagnation responses.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population sizing similar to CMA heuristics but slightly larger for robustness\n        self.lambda_ = max(6, int(4 + np.floor(3.0 * np.log(max(1, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace dimension default: moderate low-rank ~ sqrt(n) but not exceeding n\n        if subspace_k is None:\n            self.k = min(self.dim, max(1, int(np.ceil(np.sqrt(self.dim)))))\n        else:\n            self.k = min(self.dim, max(1, int(subspace_k)))\n\n        # exploration parameters (can adapt on stagnation)\n        self.p_de0 = 0.22\n        self.F_de = 0.8\n        self.p_levy0 = 0.10\n        self.mirrored = True\n\n        # robustness parameters\n        self.c_d = 0.18   # MAD smoothing for per-coordinate scale\n        self.buffer_max = max(20, 8 * self.k)\n        self.subspace_update_every = max(2, int(5))  # update frequency (evaluations multiples checked)\n        self.max_archive = 3000\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (respect func.bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # recombination weights (log-based)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length control constants (standard CMA-like)\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        m = np.random.uniform(lb, ub)                      # initial mean in bounds\n        sigma = 0.18 * np.mean(ub - lb)                    # starting global step-size\n        D = np.ones(n)                                     # per-coordinate std estimate (robust)\n        ps = np.zeros(n)                                   # path for sigma\n        pc = np.zeros(n)                                   # optional rank-one path (used in subspace influence)\n\n        # small orthonormal subspace (n x k)\n        if self.k >= 1:\n            R = np.random.randn(n, self.k)\n            try:\n                U, _ = np.linalg.qr(R)\n                U = U[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        # buffer of recent successful weighted y-steps for subspace learning\n        success_buffer = []\n\n        # archive for DE differences\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # dynamic exploration probabilities (can increase on stagnation)\n        p_de = float(self.p_de0)\n        p_levy = float(self.p_levy0)\n\n        # stagnation tracking\n        last_improvement_eval = 0\n        stagnation_threshold = max(20, int(5 * n))  # number of evaluations before escalating exploration\n\n        # initial evaluation at mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            f = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(f)\n            f_opt = float(f)\n            x_opt = xm.copy()\n            last_improvement_eval = evals\n\n        gen = 0\n        # main loop\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # sample base normals for mirrored sampling\n            base_z = np.random.randn(current_lambda, n)\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))  # store y such that x = m + sigma*y\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # low-rank contribution\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U @ z_low  # shape (n,)\n                    # randomized mixing coefficient to diversify steps\n                    alpha = np.random.uniform(0.45, 0.9)\n                    y = alpha * (D * z) + (1.0 - alpha) * (np.mean(D) * low)\n                else:\n                    y = D * z\n\n                # mirrored (antithetic) sampling\n                if self.mirrored and (i % 2 == 1):\n                    y = -y\n\n                # occasional Lévy/Cauchy heavy-tailed jump\n                if np.random.rand() < p_levy:\n                    r = np.random.standard_cauchy() * 1.0\n                    dirn = z / (np.linalg.norm(z) + 1e-20)\n                    y = r * dirn * (np.median(D) + 1e-10)\n\n                x = m + sigma * y\n\n                # occasionally add DE-style archive difference mutation\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates, careful not to exceed budget\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection and recombination\n            # sort by fitness (lower is better)\n            idx = np.argsort(arfit)\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]  # steps in y-space\n\n            # recombine to produce new mean\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space (used for adaptations)\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # update path for sigma using approximate invsqrt (1/D)\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n\n            # sigma adaptation (exponential)\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e2 * np.mean(ub - lb) + 1e-12)\n\n            # update per-coordinate scales D via MAD-like robust estimate on selected y's\n            if y_sel.size > 0:\n                med = np.median(y_sel, axis=0)\n                mad = np.median(np.abs(y_sel - med), axis=0)\n                approx_std = 1.4826 * (mad + 1e-12)\n                # exponential smoothing\n                D = (1.0 - self.c_d) * D + self.c_d * (approx_std + 1e-12)\n\n            # store weighted success step for low-rank learning\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.buffer_max:\n                success_buffer.pop(0)\n\n            # occasionally update low-rank subspace U using SVD on buffer (economical)\n            if (len(success_buffer) >= max(2, self.k)) and ((gen % self.subspace_update_every) == 0):\n                Y = np.vstack(success_buffer).T  # shape n x m\n                # center columns\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    # economy SVD (good for small k)\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        # if needed, pad with random orthonormal directions\n                        if U.shape[1] < self.k:\n                            pad = np.random.randn(n, self.k - U.shape[1])\n                            try:\n                                Qp, _ = np.linalg.qr(pad)\n                                U = np.hstack([U, Qp[:, : self.k - U.shape[1]]])\n                            except np.linalg.LinAlgError:\n                                pass\n                except np.linalg.LinAlgError:\n                    # keep old U on failure\n                    pass\n\n            # stagnation detection: increase exploration if no improvement recently\n            if (evals - last_improvement_eval) > stagnation_threshold:\n                # escalate exploration: inflate sigma, raise DE / Levy probabilities, nudge mean towards archive best\n                sigma *= 1.8\n                p_de = min(0.6, p_de * 1.6)\n                p_levy = min(0.4, p_levy * 1.6)\n                if len(archive_X) > 0:\n                    # pick a good archived point (best in archive)\n                    best_idx = int(np.argmin(archive_F))\n                    m = 0.6 * m + 0.4 * archive_X[best_idx]\n                # clear buffer to relearn subspace\n                success_buffer = []\n                last_improvement_eval = evals  # avoid repeated rapid escalation\n\n            # mild recovery of exploration probabilities toward baseline if improving\n            if (evals - last_improvement_eval) < (stagnation_threshold // 4):\n                # slowly decay p_de and p_levy back to baseline\n                p_de = max(self.p_de0, p_de * 0.95)\n                p_levy = max(self.p_levy0, p_levy * 0.95)\n\n            # enforce mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # loop continues until budget exhausted\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HALCLM scored 0.320 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "63bfe055-c94c-489c-a0d2-92204f5e88aa", "operator": null, "metadata": {"aucs": [0.1917718116131636, 0.15603721768754952, 0.3609297364521591, 0.6834658189214602, 0.3193769286048609, 0.5608778855253325, 0.23364197941837006, 0.3306384455875494, 0.2247286399584627, 0.13771881478096215]}, "task_prompt": ""}
{"id": "46d3854f-6b9a-4d5d-a392-1f1c4fa475c5", "fitness": "-inf", "name": "ARDLE", "description": "ARDLE is a hybrid CMA-ES/DE heuristic that uses Lehmer-skewed recombination weights and classic CMA-style ps/pc evolution paths with rank-one plus rank-mu covariance updates (invsqrtC, periodic eigendecomposition controlled by eigen_every_factor * n) to provide adaptive, rotation-aware search. Exploration is augmented by archive-driven differential mutations (probability p_de, archive of past samples with pruning to limit memory) and occasional Lévy jumps (p_levy, levy_beta, plus rare mean jumps on stagnation) to inject long-range moves. Step-size control is multiplicative and robust: an initial sigma = 0.28 * range is adapted by a ps-based rule using chi_n, cs and damps plus a median-success feedback term (target_s = 0.2, adapt_strength scaled by dimension) while F_de is annealed over the budget to reduce aggressive DE moves. Population and weight choices bias exploitation while keeping diversity: lambda_ scales like 6 + 4*log(dim) (mu = lambda_/2), Lehmer-like weight exponent 1.15 to emphasize top parents, carefully chosen c1/cmu/cc for stability, plus bounds clipping and numerical safeguards to keep the search well-behaved.", "code": "import numpy as np\n\nclass ARDLE:\n    \"\"\"\n    ARDLE: Adaptive Rotational Differential Lehmer Evolution\n    One-line: Lehmer-weighted CMA-ES style covariance adaptation with archive-driven DE mutations and occasional Lévy jumps,\n    combining ps/pc-based sigma control (chi_n) with a median-success feedback for robust affine-invariant exploration.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 p_de=0.22, p_levy=0.04, levy_beta=1.5, eigen_every_factor=5):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.p_de = float(p_de)\n        self.p_levy = float(p_levy)\n        self.levy_beta = float(levy_beta)\n        self.eigen_every_factor = int(eigen_every_factor)\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n        # population sizing (moderately larger baseline like ARDE_Enhanced)\n        self.lambda_ = max(6, int(6 + 4 * np.log(max(1, self.dim))))\n        self.mu = max(2, self.lambda_ // 2)\n\n    def _levy_step(self, n, beta):\n        # Mantegna's algorithm for symmetric Levy stable step-size\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = self.rng.normal(0, sigma_u, size=n)\n        v = self.rng.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1 / beta) + 1e-20)\n        return step\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # Lehmer-like recombination weights (skewed)\n        mu = self.mu\n        raw = (np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))) ** 1.15\n        weights = raw / (np.sum(raw) + 1e-20)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # adaptation constants (blend of ARDCE-style and ARDE-style choices)\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        c1 = 2.0 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1 - c1, 2 * (mu_eff - 2 + 1.0 / mu_eff) / ((n + 2) ** 2 + mu_eff))\n        damps = 1 + 2 * max(0, np.sqrt((mu_eff - 1) / (n + 1)) - 1) + cs\n\n        # expectation of ||N(0,I)||\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        m = self.rng.uniform(lb, ub)\n        sigma = 0.28 * np.mean(ub - lb)\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eigen_eval_counter = 0\n        eig_every = max(1, int(self.eigen_every_factor * n))\n\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            try:\n                fm = float(func(xm))\n            except Exception:\n                fm = np.inf\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_opt = fm; x_opt = xm.copy()\n            f_parent = fm\n        else:\n            f_parent = np.inf\n\n        # additional parameters for median-success sigma nudging\n        target_s = 0.2\n        adapt_strength = 0.55 / (1.0 + 0.04 * n)  # dimension-scaled sensitivity\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            # sample\n            arz = self.rng.standard_normal(size=(lam, n))\n            BD = B * D[np.newaxis, :]\n            ary = arz @ BD.T\n            arx = m + sigma * ary\n\n            # adaptive DE factor decreasing over time\n            F_de = 0.6 + 0.4 * (1.0 - evals / max(1, budget))\n\n            # apply archive-driven DE and occasional Levy jumps\n            for k in range(lam):\n                if (self.rng.random() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    arx[k] = arx[k] + de_mut\n                if self.rng.random() < self.p_levy:\n                    levy = self._levy_step(n, self.levy_beta)\n                    arx[k] = arx[k] + 0.45 * sigma * levy\n                arx[k] = np.clip(arx[k], lb, ub)\n\n            # evaluate candidates\n            arfit = np.full(lam, np.inf)\n            for k in range(lam):\n                if evals >= budget:\n                    break\n                xk = arx[k]\n                try:\n                    fk = float(func(xk))\n                except Exception:\n                    fk = np.inf\n                evals += 1\n                arfit[k] = fk\n                archive_X.append(xk.copy()); archive_F.append(fk)\n                if fk < f_opt:\n                    f_opt = fk; x_opt = xk.copy()\n\n            # ensure we had some valid evaluations\n            valid_mask = np.isfinite(arfit)\n            if not np.any(valid_mask):\n                break\n\n            # selection\n            sel_count = min(mu, int(np.sum(valid_mask)))\n            idx = np.argsort(arfit)[:sel_count]\n            x_sel = arx[idx]\n            y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n\n            # recombination with Lehmer-like weights (slice if fewer parents)\n            if sel_count < mu:\n                w_sub = weights[:sel_count]\n                w_sub = w_sub / (np.sum(w_sub) + 1e-20)\n                m_old = m.copy()\n                m = np.sum(w_sub[:, np.newaxis] * x_sel, axis=0)\n                y_w = np.sum(w_sub[:, np.newaxis] * y_sel, axis=0)\n            else:\n                m_old = m.copy()\n                m = np.sum(weights[:, np.newaxis] * x_sel, axis=0)\n                y_w = np.sum(weights[:, np.newaxis] * y_sel, axis=0)\n\n            # compute success rate relative to previous parent fitness\n            if np.isfinite(f_parent):\n                successes = np.sum(arfit < f_parent)\n                s_rate = successes / max(1, lam)\n            else:\n                finite_vals = arfit[np.isfinite(arfit)]\n                if finite_vals.size == 0:\n                    s_rate = 0.0\n                else:\n                    med = np.median(finite_vals)\n                    successes = np.sum(arfit < med)\n                    s_rate = successes / max(1, lam)\n\n            # update evolution paths\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n            norm_ps = np.linalg.norm(ps)\n            # hsig similar to CMA-ES criterion (using generation estimate)\n            # approximate generation count by lam for denominator term\n            denom = np.sqrt(1.0 - (1 - cs) ** (2 * max(1, lam)))\n            hsig = 1.0 if (norm_ps / (denom + 1e-20) / chi_n) < (1.4 + 2.0 / (n + 1)) else 0.0\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n\n            # covariance update\n            rank_one = np.outer(pc, pc)\n            rank_mu = np.zeros((n, n))\n            # choose weights used for rank_mu\n            w_for = (w_sub if sel_count < mu else weights)\n            for i in range(sel_count):\n                yi = y_sel[i][:, None]\n                rank_mu += w_for[i] * (yi @ yi.T)\n            C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n\n            # sigma adaptation: combine ps-based rule and median-success feedback multiplicatively\n            # (small combined exponent to keep stability)\n            sigma_factor_ps = np.exp((cs / (damps + 1e-20)) * (norm_ps / (chi_n + 1e-20) - 1.0))\n            sigma_factor_succ = np.exp(adapt_strength * (s_rate - target_s))\n            sigma *= sigma_factor_ps * sigma_factor_succ\n\n            # occasional mean Levy jump on detected stagnation (few improvements recently)\n            if len(archive_F) >= max(12, n) and (np.mean(archive_F[-min(50, len(archive_F)):]) - f_opt) < 1e-12:\n                if self.rng.random() < 0.015:\n                    levy = self._levy_step(n, self.levy_beta)\n                    m = np.clip(m + 0.9 * sigma * levy / (1.0 + np.linalg.norm(levy)), lb, ub)\n\n            # update parent fitness at new mean if changed significantly\n            if evals < budget and np.linalg.norm(m - m_old) > 1e-12:\n                xm = np.clip(m, lb, ub)\n                try:\n                    fm = float(func(xm))\n                except Exception:\n                    fm = np.inf\n                evals += 1\n                archive_X.append(xm.copy()); archive_F.append(fm)\n                if fm < f_opt:\n                    f_opt = fm; x_opt = xm.copy()\n                f_parent = fm\n            else:\n                f_parent = f_parent  # keep previous\n\n            # eigen decomposition periodically\n            eigen_eval_counter += lam\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # safeguards\n            sigma = max(sigma, 1e-12 * np.maximum(1.0, np.mean(ub - lb)))\n            if not np.isfinite(sigma):\n                sigma = 1e-8 * np.mean(ub - lb)\n\n            # prune archive occasionally to limit memory\n            if len(archive_X) > max(2000, 100 * n):\n                idx_sorted = np.argsort(archive_F)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                keep_rest = rest[::max(1, len(rest) // (max(0, (max(2000, 100 * n) - 200))))]\n                keep_idx = np.concatenate([keep_best, keep_rest]).astype(int)\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # early exit if perfect\n            if f_opt <= 1e-12:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "63bfe055-c94c-489c-a0d2-92204f5e88aa", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "fb27e687-c3c6-4f7d-b10a-a28581fccb3d", "fitness": "-inf", "name": "AdaptiveMixtureExplorer", "description": "The algorithm maintains an ensemble of adaptive \"attractors\" (n_attractors ≈ min(10, dim/2+1)) each with a per-dimension variance (diag_var), a scalar scale, and a mean; these are seeded from good initial random samples and the top archive points and are periodically nudged toward the global best for exploitation. Proposals are a mixture of local Gaussian sampling, PCA‑guided directional probes (pca_window=12) using recent successful steps, deterministic coordinate/pattern probes along high‑variance dimensions, pairwise mixture/crossover sampling between attractors, and occasional heavy‑tailed Cauchy jumps (cauchy_prob=0.10, cauchy_scale_frac=0.35) to escape basins. Adaptation is online: successful steps update means with a small learning rate (adapt_lr=0.2), variances via EMA (var_update_alpha=0.15), and scales grow/shrink by success_increase=1.25 / failure_decrease=0.75; attractors that stagnate (center_replace_patience=9) are rejuvenated from diverse archive points or random samples. A bounded archive with periodic pruning, per-iteration and per-attractor evaluation caps (max_eval_per_iter), and strict budget enforcement balance exploration and exploitation across the noisy-free continuous [-5,5] search space.", "code": "import numpy as np\n\nclass AdaptiveMixtureExplorer:\n    \"\"\"\n    Adaptive Mixture Explorer (AMX)\n\n    Key ideas:\n    - Maintain an ensemble of \"attractors\" (local Gaussian samplers) with per-dimension variances\n      and a scalar step-size. Each attractor adapts using successful steps (evolution-path-like)\n      to learn promising directions and anisotropic scaling.\n    - Proposals come from: local Gaussian sampling, PCA-guided directional probes (from recent\n      successful steps), deterministic coordinate/pattern search, mixture cross-sampling between\n      attractors, and occasional heavy-tailed Lévy/Cauchy jumps for escapes.\n    - Attractors that stagnate are rejuvenated by picking diverse archive points or random samples.\n    - Archive of evaluated points pruned periodically to keep memory bounded.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, n_attractors=None, init_ratio=0.08):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # initialization sizing\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(12, 2 * self.dim)\n        self.max_init = min(600, int(0.4 * self.budget))\n\n        # ensemble sizing\n        if n_attractors is None:\n            self.n_attractors = max(2, min(10, self.dim // 2 + 1))\n        else:\n            self.n_attractors = max(1, int(n_attractors))\n\n        # sampling & adaptation parameters\n        self.max_eval_per_iter = max(40, min(160, 6 * self.n_attractors))\n        self.attractor_init_frac = 0.4  # initial step-size fraction of range\n        self.success_increase = 1.25\n        self.failure_decrease = 0.75\n        self.adapt_lr = 0.2  # learning rate for mean updates\n        self.var_update_alpha = 0.15  # EMA for per-dimension variance\n        self.pca_window = 12  # number of recent successful step vectors to compute PCA\n        self.cauchy_prob = 0.10\n        self.cauchy_scale_frac = 0.35\n        self.center_replace_patience = 9\n\n        # archive limit\n        self.max_archive = None  # set in call\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds expected to be [-5,5] for BBOB, but read them generically\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        # safeguard\n        rng_range[rng_range <= 0] = 1.0\n\n        self.max_archive = max(2000, 50 * n)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archive\n        X = []    # list of evaluated points (np arrays)\n        F = []    # list of function values (floats)\n\n        f_best = np.inf\n        x_best = None\n\n        # initial sampling (space-filling-ish uniform random)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for i in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            fx = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(fx))\n            if fx < f_best:\n                f_best = float(fx); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper: get top distinct points as starting attractors\n        def get_top_points(k):\n            if len(X) == 0:\n                return []\n            idx = np.argsort(F)\n            centers = []\n            for ii in idx:\n                x = np.array(X[ii])\n                if not any(np.linalg.norm(x - c) < 1e-12 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        # initialize attractors\n        k = min(self.n_attractors, len(X))\n        seeds = get_top_points(k)\n        while len(seeds) < self.n_attractors:\n            seeds.append(self.rng.uniform(lb, ub))\n        seeds = [np.array(s) for s in seeds[:self.n_attractors]]\n\n        # per-attractor state\n        attractors = []\n        for s in seeds:\n            # variance per-dimension initialized to (frac * range)^2\n            var = (self.attractor_init_frac * rng_range) ** 2\n            step = self.attractor_init_frac * rng_range.copy()  # per-dimension step baseline\n            # but we keep a scalar scale to moderate exploration intensity\n            scale = np.mean(step)\n            attractors.append({\n                'mean': s.copy(),\n                'diag_var': var.copy(),  # per-dim variance\n                'scale': scale,\n                'age': 0,\n                'stagnation': 0,\n                'success_steps': [],  # list of recent successful step vectors (for PCA)\n            })\n\n        # safe evaluation wrapper that enforces budget\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            xc = np.clip(x, lb, ub)\n            fx = func(xc)\n            evals += 1\n            X.append(xc.copy()); F.append(float(fx))\n            if fx < f_best - 1e-12:\n                f_best = float(fx); x_best = xc.copy()\n            return float(fx), xc\n\n        # PCA helper on success steps to find principal directions\n        def principal_directions(step_list, max_components=3):\n            if len(step_list) < 2:\n                return None, None\n            S = np.vstack(step_list)\n            # center\n            Sc = S - np.mean(S, axis=0)\n            cov = np.cov(Sc.T)\n            try:\n                vals, vecs = np.linalg.eigh(cov)\n                idx = np.argsort(vals)[::-1]\n                vals = vals[idx]; vecs = vecs[:, idx]\n                k = min(max_components, vecs.shape[1])\n                return vecs[:, :k], vals[:k]\n            except Exception:\n                return None, None\n\n        iter_count = 0\n        # main loop\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            # occasionally refresh attractors from archive to encourage diversity\n            if iter_count % 8 == 0 and len(X) > 0:\n                # keep best half of attractors and refill rest with top diverse archive points\n                num_keep = max(1, self.n_attractors // 2)\n                # rank attractors by their local mean quality (value in archive nearest to mean)\n                mean_vals = []\n                X_arr = np.asarray(X)\n                F_arr = np.asarray(F)\n                for a in attractors:\n                    # find nearest archive point to attractor.mean\n                    dists = np.linalg.norm(X_arr - a['mean'], axis=1)\n                    idx = np.argmin(dists)\n                    mean_vals.append((F_arr[idx], dists[idx], a))\n                mean_vals.sort(key=lambda t: (t[0], t[1]))\n                keep = [t[2] for t in mean_vals[:num_keep]]\n                # fill remaining by selecting good and far archive points\n                new_attractors = keep.copy()\n                # candidates: top 30% of archive by objective\n                topk = max(1, int(0.3 * len(X_arr)))\n                idx_sorted = np.argsort(F_arr)[:topk]\n                # compute distance to kept attractors and pick diverse ones\n                taken = 0\n                tries = 0\n                while len(new_attractors) < self.n_attractors and tries < 200:\n                    tries += 1\n                    cand_idx = self.rng.choice(idx_sorted)\n                    cand_x = X_arr[cand_idx].copy()\n                    if not any(np.linalg.norm(cand_x - a['mean']) < 1e-9 for a in new_attractors):\n                        # create attractor around cand_x\n                        var = (self.attractor_init_frac * rng_range) ** 2\n                        scale = np.mean(self.attractor_init_frac * rng_range)\n                        new_attractors.append({'mean': cand_x, 'diag_var': var.copy(),\n                                               'scale': scale, 'age': 0, 'stagnation': 0,\n                                               'success_steps': []})\n                # fill with random if still missing\n                while len(new_attractors) < self.n_attractors:\n                    cand_x = self.rng.uniform(lb, ub)\n                    var = (self.attractor_init_frac * rng_range) ** 2\n                    scale = np.mean(self.attractor_init_frac * rng_range)\n                    new_attractors.append({'mean': cand_x, 'diag_var': var.copy(),\n                                           'scale': scale, 'age': 0, 'stagnation': 0,\n                                           'success_steps': []})\n                attractors = new_attractors\n\n            improved_global = False\n\n            # iterate attractors in random order\n            order = list(range(len(attractors)))\n            self.rng.shuffle(order)\n            for aidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n\n                a = attractors[aidx]\n                a['age'] += 1\n\n                # compute local proposal budget (at most few evaluations per attractor each iter)\n                local_budget = max(1, work_allow // max(1, len(attractors)))\n                # ensure not exceeding global per-iteration budget\n                local_budget = min(local_budget, work_allow)\n\n                # derive local covariance (diagonal) scaled by scalar scale\n                local_std = np.sqrt(a['diag_var']) * (a['scale'] / (np.mean(np.sqrt(a['diag_var'])) + 1e-12))\n\n                # 1) Mixture sampling: sample up to local_budget/2 from local Gaussian\n                n_gauss = max(1, int(np.ceil(local_budget * 0.5)))\n                for _ in range(n_gauss):\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    z = self.rng.randn(n) * local_std\n                    x_try = np.clip(a['mean'] + z, lb, ub)\n                    # avoid duplicate last eval\n                    if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                        continue\n                    out = safe_eval(x_try)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    ftry, xtry = out\n                    if ftry < f_best - 1e-12:\n                        improved_global = True\n                    # local improvement relative to attractor mean: we approximate value at mean by nearest archive point\n                    # fallback: if attractor mean not present in archive, we consider any improvement against global as improvement\n                    # find nearest archive value to attractor mean\n                    near_val = None\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        dists = np.linalg.norm(X_arr - a['mean'], axis=1)\n                        near_idx = np.argmin(dists)\n                        near_val = F[near_idx]\n                    # accept if better than nearest value or better than attractor.mean using global best heuristic\n                    accept_local = False\n                    if near_val is None or ftry < near_val - 1e-12:\n                        accept_local = True\n                    if accept_local:\n                        # update mean (small learning rate for stability)\n                        prev_mean = a['mean'].copy()\n                        a['mean'] = (1.0 - self.adapt_lr) * a['mean'] + self.adapt_lr * xtry\n                        # record successful step\n                        step_vec = xtry - prev_mean\n                        if np.linalg.norm(step_vec) > 1e-12:\n                            a['success_steps'].append(step_vec.copy())\n                            if len(a['success_steps']) > self.pca_window:\n                                a['success_steps'].pop(0)\n                        # update diag variance (EMA on squared step scaled)\n                        sq = (step_vec ** 2)\n                        a['diag_var'] = (1 - self.var_update_alpha) * a['diag_var'] + self.var_update_alpha * (sq + 1e-12)\n                        # increase scale a bit\n                        a['scale'] = np.minimum(a['scale'] * self.success_increase, np.max(rng_range) * 2.0)\n                        a['stagnation'] = 0\n                        # update global best if needed (safe_eval already updated)\n                        continue  # continue sampling for this attractor\n\n                # 2) PCA-guided deterministic probes along principal directions from recent successful steps\n                if work_allow > 0 and len(a['success_steps']) >= 2:\n                    vecs, vals = principal_directions(a['success_steps'], max_components=min(3, n))\n                    if vecs is not None:\n                        # for each principal direction probe +/-, try scaled steps\n                        for j in range(vecs.shape[1]):\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            v = vecs[:, j]\n                            # scale by eigenvalue magnitude and attractor scale\n                            lam = np.sqrt(max(0.0, vals[j])) + 1e-12\n                            for s in (1.0, 0.5, 1.5):\n                                x_try = np.clip(a['mean'] + s * a['scale'] * lam * v, lb, ub)\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                # local accept heuristic as above\n                                near_val = None\n                                if len(X) > 0:\n                                    X_arr = np.asarray(X); dists = np.linalg.norm(X_arr - a['mean'], axis=1)\n                                    near_idx = np.argmin(dists); near_val = F[near_idx]\n                                if near_val is None or ftry < near_val - 1e-12:\n                                    prev_mean = a['mean'].copy()\n                                    a['mean'] = (1.0 - self.adapt_lr) * a['mean'] + self.adapt_lr * xtry\n                                    step_vec = a['mean'] - prev_mean\n                                    if np.linalg.norm(step_vec) > 1e-12:\n                                        a['success_steps'].append(step_vec.copy())\n                                        if len(a['success_steps']) > self.pca_window:\n                                            a['success_steps'].pop(0)\n                                    sq = (step_vec ** 2)\n                                    a['diag_var'] = (1 - self.var_update_alpha) * a['diag_var'] + self.var_update_alpha * (sq + 1e-12)\n                                    a['scale'] = np.minimum(a['scale'] * self.success_increase, np.max(rng_range) * 2.0)\n                                    a['stagnation'] = 0\n                                    break\n\n                # 3) Deterministic coordinate/pattern probes along top variance dims\n                if work_allow > 0:\n                    # pick top dimensions by diag_var\n                    dims_order = np.argsort(-a['diag_var'])\n                    max_coords = min(n, 1 + max(2, int(work_allow // 4)))\n                    for di in dims_order[:max_coords]:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        step_mag = np.sqrt(a['diag_var'][di]) * a['scale'] / (np.mean(np.sqrt(a['diag_var'])) + 1e-12)\n                        for sign in (+1.0, -1.0):\n                            x_try = a['mean'].copy()\n                            x_try[di] = x_try[di] + sign * step_mag\n                            x_try = np.clip(x_try, lb, ub)\n                            if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                continue\n                            out = safe_eval(x_try)\n                            work_allow -= 1\n                            if out is None:\n                                break\n                            ftry, xtry = out\n                            if ftry < f_best - 1e-12:\n                                improved_global = True\n                            near_val = None\n                            if len(X) > 0:\n                                X_arr = np.asarray(X)\n                                dists = np.linalg.norm(X_arr - a['mean'], axis=1)\n                                near_idx = np.argmin(dists)\n                                near_val = F[near_idx]\n                            if near_val is None or ftry < near_val - 1e-12:\n                                prev_mean = a['mean'].copy()\n                                a['mean'] = (1.0 - self.adapt_lr) * a['mean'] + self.adapt_lr * xtry\n                                step_vec = a['mean'] - prev_mean\n                                if np.linalg.norm(step_vec) > 1e-12:\n                                    a['success_steps'].append(step_vec.copy())\n                                    if len(a['success_steps']) > self.pca_window:\n                                        a['success_steps'].pop(0)\n                                sq = (step_vec ** 2)\n                                a['diag_var'] = (1 - self.var_update_alpha) * a['diag_var'] + self.var_update_alpha * (sq + 1e-12)\n                                a['scale'] = np.minimum(a['scale'] * self.success_increase, np.max(rng_range) * 2.0)\n                                a['stagnation'] = 0\n                                break\n\n                # 4) Mixture crossover sampling with another random attractor (exploit combination)\n                if work_allow > 0 and len(attractors) > 1:\n                    other_idx = self.rng.choice([i for i in range(len(attractors)) if i != aidx])\n                    b = attractors[other_idx]\n                    alpha = self.rng.beta(2.0, 2.0)  # favors center mixing\n                    mix_mean = alpha * a['mean'] + (1.0 - alpha) * b['mean']\n                    mix_std = 0.5 * (np.sqrt(a['diag_var']) + np.sqrt(b['diag_var']))\n                    if work_allow > 0:\n                        z = self.rng.randn(n) * mix_std * (0.5 * (a['scale'] + b['scale']) / (np.mean(mix_std) + 1e-12))\n                        x_try = np.clip(mix_mean + z, lb, ub)\n                        if not (len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12)):\n                            out = safe_eval(x_try)\n                            work_allow -= 1\n                            if out is None:\n                                pass\n                            else:\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                near_val = None\n                                if len(X) > 0:\n                                    X_arr = np.asarray(X); dists = np.linalg.norm(X_arr - a['mean'], axis=1)\n                                    near_idx = np.argmin(dists); near_val = F[near_idx]\n                                if near_val is None or ftry < near_val - 1e-12:\n                                    prev_mean = a['mean'].copy()\n                                    a['mean'] = (1.0 - self.adapt_lr) * a['mean'] + self.adapt_lr * xtry\n                                    step_vec = a['mean'] - prev_mean\n                                    if np.linalg.norm(step_vec) > 1e-12:\n                                        a['success_steps'].append(step_vec.copy())\n                                        if len(a['success_steps']) > self.pca_window:\n                                            a['success_steps'].pop(0)\n                                    sq = (step_vec ** 2)\n                                    a['diag_var'] = (1 - self.var_update_alpha) * a['diag_var'] + self.var_update_alpha * (sq + 1e-12)\n                                    a['scale'] = np.minimum(a['scale'] * self.success_increase, np.max(rng_range) * 2.0)\n                                    a['stagnation'] = 0\n\n                # 5) Occasional heavy-tailed jump for escape\n                if work_allow > 0 and self.rng.rand() < self.cauchy_prob:\n                    scale = self.cauchy_scale_frac * rng_range\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -10, 10)\n                    x_jump = np.clip(a['mean'] + jump * scale, lb, ub)\n                    if not (len(X) > 0 and np.allclose(x_jump, X[-1], atol=1e-12)):\n                        out = safe_eval(x_jump)\n                        work_allow -= 1\n                        if out is not None:\n                            fjump, xjump = out\n                            if fjump < f_best - 1e-12:\n                                improved_global = True\n                            # accept & update mean if jump improved local neighbourhood as before\n                            near_val = None\n                            if len(X) > 0:\n                                X_arr = np.asarray(X); dists = np.linalg.norm(X_arr - a['mean'], axis=1)\n                                near_idx = np.argmin(dists); near_val = F[near_idx]\n                            if near_val is None or fjump < near_val - 1e-12:\n                                prev_mean = a['mean'].copy()\n                                a['mean'] = (1.0 - self.adapt_lr) * a['mean'] + self.adapt_lr * xjump\n                                step_vec = a['mean'] - prev_mean\n                                if np.linalg.norm(step_vec) > 1e-12:\n                                    a['success_steps'].append(step_vec.copy())\n                                    if len(a['success_steps']) > self.pca_window:\n                                        a['success_steps'].pop(0)\n                                sq = (step_vec ** 2)\n                                a['diag_var'] = (1 - self.var_update_alpha) * a['diag_var'] + self.var_update_alpha * (sq + 1e-12)\n                                a['scale'] = np.minimum(a['scale'] * (self.success_increase ** 1.5), np.max(rng_range) * 3.0)\n                                a['stagnation'] = 0\n                            else:\n                                # if jump didn't improve, shrink scale moderately\n                                a['scale'] = a['scale'] * self.failure_decrease\n\n                # if no success in this attractor during the actions above, increase stagnation and shrink\n                if a['stagnation'] >= self.center_replace_patience:\n                    # replace this attractor using a diverse archived point or random sample\n                    if len(X) > 0:\n                        X_arr = np.asarray(X); F_arr = np.asarray(F)\n                        # prefer moderately good but far-from-other-attractors points\n                        dists_to_attractors = np.min([np.linalg.norm(X_arr - at['mean'], axis=1) for at in attractors], axis=0)\n                        # pick from top 20 distant among top 40% good points\n                        top_good = max(1, int(0.4 * len(F_arr)))\n                        idx_good = np.argsort(F_arr)[:top_good]\n                        dist_good = dists_to_attractors[idx_good]\n                        order_idx = idx_good[np.argsort(-dist_good)]\n                        pick_idx = int(self.rng.choice(order_idx[:max(1, min(30, len(order_idx)))]))\n                        a['mean'] = X_arr[pick_idx].copy()\n                        a['diag_var'] = (self.attractor_init_frac * rng_range) ** 2\n                        a['scale'] = np.mean(self.attractor_init_frac * rng_range)\n                        a['age'] = 0\n                        a['stagnation'] = 0\n                        a['success_steps'] = []\n                    else:\n                        a['mean'] = self.rng.uniform(lb, ub)\n                        a['diag_var'] = (self.attractor_init_frac * rng_range) ** 2\n                        a['scale'] = np.mean(self.attractor_init_frac * rng_range)\n                        a['age'] = 0\n                        a['stagnation'] = 0\n                        a['success_steps'] = []\n\n                # If no local acceptance in this cycle, increase stagnation and shrink scale slightly\n                # Determine approximate local improvement: nearest archive value to mean\n                local_near_val = None\n                if len(X) > 0:\n                    X_arr = np.asarray(X)\n                    dists = np.linalg.norm(X_arr - a['mean'], axis=1)\n                    local_near_val = F[np.argmin(dists)]\n                # Define 'had_recent_success' if success_steps non-empty or scale increased this iteration\n                if len(a['success_steps']) == 0:\n                    a['stagnation'] += 1\n                    a['scale'] = np.maximum(a['scale'] * self.failure_decrease, 1e-12 * np.max(rng_range))\n                else:\n                    # small decay on stagnation to reward attractors with successes\n                    a['stagnation'] = max(0, a['stagnation'] - 1)\n\n            # prune archive when too large\n            if len(X) > self.max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                # sample representative from rest to fill remaining slots\n                if len(rest_idx) > 0:\n                    sample_stride = max(1, len(rest_idx) // (self.max_archive - 200))\n                    keep_rest = rest_idx[::sample_stride]\n                    keep_idx = np.concatenate([keep_best, keep_rest])[:self.max_archive]\n                else:\n                    keep_idx = keep_best[:self.max_archive]\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasional soft attraction of some attractors to global best (exploitation nudges)\n            if x_best is not None and len(attractors) > 0:\n                for i in range(min(len(attractors), max(1, int(len(X) // 25)))):\n                    if self.rng.rand() < 0.18:\n                        # nudge attractor mean slightly toward global best\n                        a = attractors[i]\n                        alpha = self.rng.uniform(0.05, 0.25)\n                        a['mean'] = (1 - alpha) * a['mean'] + alpha * x_best\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "e9f3e383-6b8c-4beb-9ac3-012fb83c40e5", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "b3e09fda-9492-4c7a-af3f-78beeb164d5d", "fitness": 0.368151900827406, "name": "AdaptiveCooperativeSubspaceEnsemble", "description": "The algorithm runs an ensemble of adaptive subspace explorers (ensemble_size ~ ceil(dim/1.5), larger initial sampling init_ratio=0.20 with min_init = max(12,3*dim) and per-iteration cap max_eval_per_iter=40) to cooperatively search the box-limited space under a fixed budget. Each center fits cheap local models (a weighted separable/diagonal quadratic with ridge=1e-4 and a simple linear fit) using neighbors (model_neighbor_multiplier=6 → neighbors ≈ 6*dim) and extracts PCA directions (top ≈15% components) to propose low-dimensional moves. Proposal generation is hybrid: surrogate minimizers clipped to a per-center trust radius, PCA-guided steps, a linear-gradient step (-0.6 scaling), DE-style recombination (donor = a + 0.8*(b-a)), jitter/crossover with global-best, multi-scale directional probes (scales [0.125,0.35,0.75,1.5]), coordinate tweaks, and occasional Cauchy heavy-tailed jumps (prob 0.08, scale_frac 0.6) for exploration. Adaptivity/maintenance: trust radii initialized as 0.3*rng_range and adapted with success_expand=1.8 / failure_shrink=0.5, stagnation counters with center_replace_patience=6 trigger center reseeding from archive, archive pruning to max_archive=max(1500,40*dim), and strict budget-aware safe_eval to never exceed the allotted evaluations.", "code": "import numpy as np\n\nclass AdaptiveCooperativeSubspaceEnsemble:\n    \"\"\"\n    Adaptive Cooperative Subspace Ensemble (ACSE)\n\n    One-line: Maintain a moderately larger ensemble of adaptive subspace explorers that\n    combine compact separable-quadratic surrogates, PCA-guided low-dimensional moves,\n    DE-style recombination between centers, multi-scale directional probes and occasional\n    heavy-tailed jumps. Uses different default parameter settings from the provided algorithm.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_ratio=0.20,\n                 ensemble_override=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # initialization sizing (different: larger initial sample fraction and minimum)\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(12, 3 * self.dim)\n        self.max_init = min(600, int(0.3 * self.budget))\n\n        # ensemble sizing (different heuristic)\n        if ensemble_override is None:\n            # slightly larger ensemble for small-to-moderate dims, capped\n            self.ensemble_size = max(3, min(12, int(np.ceil(self.dim / 1.5))))\n        else:\n            self.ensemble_size = max(1, int(ensemble_override))\n\n        # modeling & neighbors (different multiplier and ridge)\n        self.model_neighbor_multiplier = 6  # neighbors = multiplier * dim (reduced)\n        self.ridge = 1e-4  # slightly stronger regularization\n\n        # trust region settings (different initial fraction & adaptation rates)\n        self.trust_init_frac = 0.3\n        self.trust_min = 1e-8\n        self.trust_max_frac = 1.5\n        self.success_expand = 1.8\n        self.failure_shrink = 0.5\n\n        # probing (different scales and per-iteration cap)\n        self.direction_scales = np.array([0.125, 0.35, 0.75, 1.5])\n        self.max_eval_per_iter = 40\n        self.cauchy_prob = 0.08\n        self.cauchy_scale_frac = 0.60\n\n        # center management (different patience and archive cap)\n        self.center_replace_patience = 6\n        self.max_archive = None  # set later based on dim\n\n    def __call__(self, func):\n        n = int(self.dim)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, float(lb))\n        if ub.shape == ():\n            ub = np.full(n, float(ub))\n        rng_range = ub - lb\n        self.max_archive = max(1500, 40 * n)\n\n        budget = int(self.budget)\n        evals = 0\n\n        X = []\n        F = []\n\n        f_best = np.inf\n        x_best = None\n\n        # initial sampling (space-filling-ish but random)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            fx = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(fx))\n            if fx < f_best:\n                f_best = float(fx); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper: choose top distinct centers\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if not any(np.linalg.norm(x - c) < 1e-10 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.ensemble_size, len(X)))\n        while len(centers) < self.ensemble_size:\n            centers.append(self.rng.uniform(lb, ub))\n        centers = [np.array(c) for c in centers[:self.ensemble_size]]\n\n        # per-center trust and stagnation trackers\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-12) for _ in centers]\n        stagnation = [0 for _ in centers]\n\n        # safe evaluation wrapper\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            xc = np.clip(x, lb, ub)\n            fx = func(xc)\n            evals += 1\n            X.append(xc.copy()); F.append(float(fx))\n            if fx < f_best - 1e-12:\n                f_best = float(fx); x_best = xc.copy()\n            return float(fx), xc\n\n        # separable-diagonal quadratic + linear around center (weighted)\n        def fit_diagonal_quad(center, neighbors_X, neighbors_F):\n            dx = neighbors_X - center\n            m = dx.shape[0]\n            # build design: [1, dx_i, 0.5*dx_i^2] per dimension (diagonal quadratic)\n            M = np.ones((m, 1 + 2 * n))\n            M[:, 1:1 + n] = dx\n            M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n            y = neighbors_F\n            # weight by exponential kernel on distance to emphasize locality\n            dists = np.linalg.norm(dx, axis=1)\n            med = np.median(dists) + 1e-12\n            w = np.exp(-dists / (med + 1e-12))\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            bvec = W * y\n            try:\n                ridge = self.ridge * np.eye(M.shape[1])\n                params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ bvec, rcond=None)\n                params = params.flatten()\n                a = params[0]\n                b_lin = params[1:1 + n]\n                h_diag = params[1 + n:1 + 2 * n]\n                # avoid tiny curvature\n                h_reg = np.copy(h_diag)\n                h_reg[np.abs(h_reg) < 1e-8] = np.sign(h_reg[np.abs(h_reg) < 1e-8]) * 1e-8\n                # if exactly zero, set small positive curvature\n                h_reg[h_reg == 0] = 1e-8\n                return a, b_lin, h_reg\n            except Exception:\n                return None\n\n        # PCA directions helper\n        def pca_directions(neighbors_X, center, max_components=None):\n            dx = neighbors_X - center\n            if dx.shape[0] <= 1:\n                return None\n            cov = np.cov(dx.T)\n            try:\n                vals, vecs = np.linalg.eigh(cov)\n                idx = np.argsort(vals)[::-1]\n                vals = vals[idx]; vecs = vecs[:, idx]\n                if max_components is None:\n                    max_components = max(1, int(0.15 * n))\n                k = min(max_components, vecs.shape[1])\n                return vecs[:, :k], vals[:k]\n            except Exception:\n                return None\n\n        iter_count = 0\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            # occasional refresh of centers using archive diversity\n            if iter_count % 7 == 0 and len(X) > 0:\n                topk = min(len(centers), max(1, len(X) // 12))\n                new_centers = get_top_centers(topk)\n                merged = centers + new_centers\n                uniq = []\n                for c in merged:\n                    if not any(np.linalg.norm(c - u) < 1e-9 for u in uniq):\n                        uniq.append(c)\n                    if len(uniq) >= self.ensemble_size:\n                        break\n                while len(uniq) < self.ensemble_size:\n                    uniq.append(self.rng.uniform(lb, ub))\n                centers = [np.array(c) for c in uniq[:self.ensemble_size]]\n                if len(trust_radius) != len(centers):\n                    trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-12) for _ in centers]\n                    stagnation = [0 for _ in centers]\n\n            improved_global = False\n\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            # allocate a small per-center budget slice (soft)\n            for cidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n\n                center = np.array(centers[cidx])\n                tr = trust_radius[cidx]\n\n                # neighbors required for model fitting\n                neighbors_needed = max(n + 3, self.model_neighbor_multiplier * n)\n                if len(X) >= neighbors_needed:\n                    X_arr = np.asarray(X)\n                    F_arr = np.asarray(F)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), neighbors_needed)\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n\n                    # fit diagonal quadratic and linear\n                    quad = fit_diagonal_quad(center, X_nei, F_nei)\n                    try:\n                        A_lin = np.hstack([np.ones((X_nei.shape[0], 1)), X_nei - center])\n                        params_lin, *_ = np.linalg.lstsq(A_lin, F_nei, rcond=None)\n                        b_lin_only = params_lin[1:].flatten()\n                    except Exception:\n                        b_lin_only = None\n\n                    pca = pca_directions(X_nei, center)\n                    proposals = []\n\n                    # surrogate quadratic minimizer (diagonal)\n                    if quad is not None:\n                        a_q, b_q, h_q = quad\n                        delta_q = -b_q / (h_q + 1e-20)\n                        # scale to trust region (different clipping)\n                        delta_q = np.clip(delta_q, -1.2 * tr, 1.2 * tr)\n                        x_q = np.clip(center + delta_q, lb, ub)\n                        proposals.append(('quad_min', x_q))\n\n                    # linear gradient sign descent (different step)\n                    if b_lin_only is not None:\n                        step = -0.6 * b_lin_only / (np.linalg.norm(b_lin_only) + 1e-12) * np.linalg.norm(tr)\n                        x_lin = np.clip(center + step, lb, ub)\n                        proposals.append(('lin_grad', x_lin))\n\n                    # PCA-guided low-dim subspace moves (different scaling)\n                    if pca is not None:\n                        vecs, vals = pca\n                        for i_pc in range(vecs.shape[1]):\n                            v = vecs[:, i_pc]\n                            for s in (0.4, 0.9, 1.6):\n                                step = s * (np.linalg.norm(tr) / max(1.0, np.sqrt(float(n)))) * v\n                                x_p_plus = np.clip(center + step, lb, ub)\n                                x_p_minus = np.clip(center - step, lb, ub)\n                                proposals.append((f'pca_{i_pc}_+', x_p_plus))\n                                proposals.append((f'pca_{i_pc}_-', x_p_minus))\n\n                    # DE-style recombination between two random centers (cooperation)\n                    if len(centers) > 1:\n                        # choose two other centers distinct from cidx\n                        others = [i for i in range(len(centers)) if i != cidx]\n                        a, b = self.rng.choice(others, size=min(2, len(others)), replace=False)\n                        cr = 0.7\n                        Fd = self.rng.rand(n) < cr\n                        donor = centers[a] + 0.8 * (centers[b] - centers[a])\n                        trial = np.copy(center)\n                        trial[Fd] = donor[Fd]\n                        trial = np.clip(trial, lb, ub)\n                        proposals.append(('de_recomb', trial))\n\n                    # add jittered variants and ensure uniqueness\n                    unique_props = []\n                    for name, p in proposals:\n                        if not any(np.allclose(p, q, atol=1e-12) for q in unique_props):\n                            unique_props.append(p)\n                        # jitter variant scaled by trust (different jitter amplitude)\n                        jitter = (self.rng.randn(n) * 0.06) * np.maximum(tr, 1e-12)\n                        pj = np.clip(p + jitter, lb, ub)\n                        if not any(np.allclose(pj, q, atol=1e-12) for q in unique_props):\n                            unique_props.append(pj)\n\n                    # add crossover with global best occasionally\n                    if x_best is not None and np.linalg.norm(center - x_best) > 1e-12:\n                        alpha = self.rng.uniform(0.1, 0.6)\n                        x_cross = np.clip(alpha * center + (1 - alpha) * x_best, lb, ub)\n                        if not any(np.allclose(x_cross, q, atol=1e-12) for q in unique_props):\n                            unique_props.insert(0, x_cross)\n\n                    # sort proposals by closeness to center (prefer local)\n                    unique_props.sort(key=lambda z: np.linalg.norm(z - center))\n\n                    # evaluate proposals until improvement or work_allow exhausted\n                    local_improved = False\n                    for xprop in unique_props:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        out = safe_eval(xprop)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        fprop, xprop = out\n                        if fprop < f_best - 1e-12:\n                            local_improved = True\n                            improved_global = True\n                            centers[cidx] = xprop.copy()\n                            # expand trust more aggressively on success\n                            trust_radius[cidx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                    # if no improvement from surrogates & recombination, do structured directional exploration\n                    if not local_improved and work_allow > 0 and evals < budget:\n                        num_dirs = int(np.clip(3 + n // 3, 4, 10))\n                        dirs = []\n                        for _ in range(num_dirs):\n                            v = self.rng.randn(n)\n                            nv = np.linalg.norm(v)\n                            if nv == 0:\n                                v = np.ones(n); nv = np.linalg.norm(v)\n                            dirs.append(v / nv)\n                        self.rng.shuffle(dirs)\n                        base_step = np.linalg.norm(tr) / max(1.0, np.sqrt(float(n)))\n                        for d in dirs:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            cand = []\n                            for s in self.direction_scales:\n                                cand.append(np.clip(center + d * (s * base_step), lb, ub))\n                                cand.append(np.clip(center - d * (s * base_step), lb, ub))\n                            self.rng.shuffle(cand)\n                            for x_try in cand:\n                                if work_allow <= 0 or evals >= budget:\n                                    break\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    local_improved = True\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.4, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n                            # end cand\n                        # end dirs\n\n                    # coordinate micro-tweaks (different approach: random subset)\n                    if not local_improved and work_allow > 0 and evals < budget:\n                        coord_step = np.maximum(tr, 1e-12)\n                        max_coords = min(n, max(2, int(max(2, work_allow // 3))))\n                        coords = np.arange(n)\n                        if max_coords < n:\n                            coords = self.rng.choice(coords, size=max_coords, replace=False)\n                        for i in coords:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            sign = 1.0 if self.rng.rand() < 0.5 else -1.0\n                            x_try = centers[cidx].copy()\n                            x_try[i] = x_try[i] + sign * coord_step[i]\n                            x_try = np.clip(x_try, lb, ub)\n                            if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                continue\n                            out = safe_eval(x_try)\n                            work_allow -= 1\n                            if out is None:\n                                break\n                            ftry, xtry = out\n                            if ftry < f_best - 1e-12:\n                                local_improved = True\n                                improved_global = True\n                                centers[cidx] = xtry.copy()\n                                trust_radius[cidx] = np.minimum(tr * 1.25, self.trust_max_frac * rng_range)\n                                stagnation[cidx] = 0\n                                break\n\n                    # occasional Cauchy heavy-tailed jump (different prob/scale)\n                    if (not local_improved and work_allow > 0 and evals < budget\n                            and self.rng.rand() < self.cauchy_prob):\n                        scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-12)\n                        jump = self.rng.standard_cauchy(size=n)\n                        jump = np.clip(jump, -12, 12)\n                        x_jump = np.clip(centers[cidx] + jump * scale, lb, ub)\n                        out = safe_eval(x_jump)\n                        if out is not None:\n                            work_allow -= 1\n                            fjump, xjump = out\n                            if fjump < f_best - 1e-12:\n                                local_improved = True\n                                improved_global = True\n                                centers[cidx] = xjump.copy()\n                                trust_radius[cidx] = np.minimum(tr * 2.2, self.trust_max_frac * rng_range)\n                                stagnation[cidx] = 0\n                            else:\n                                # penalize trust a bit on failed big jump\n                                trust_radius[cidx] = np.maximum(tr * 0.6, self.trust_min * rng_range)\n\n                    # update stagnation/trust if no improvement for this center\n                    if not local_improved:\n                        stagnation[cidx] += 1\n                        trust_radius[cidx] = np.maximum(tr * self.failure_shrink, self.trust_min * rng_range)\n                    else:\n                        # mild reset of other centers' stagnation to encourage cooperative diversification\n                        for j in range(len(stagnation)):\n                            if j != cidx and stagnation[j] > 0:\n                                stagnation[j] = max(0, stagnation[j] - 1)\n\n                else:\n                    # not enough data: exploratory random subspace probes\n                    base_step = np.linalg.norm(trust_radius[cidx]) / max(1.0, np.sqrt(float(n)))\n                    for _ in range(min(6, work_allow)):\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        s = self.rng.choice(self.direction_scales)\n                        x_try = np.clip(center + d * s * base_step, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xtry.copy()\n                            trust_radius[cidx] = np.minimum(trust_radius[cidx] * 1.3, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                # replace center if stagnated for too long\n                if stagnation[cidx] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # compute distance to all centers and pick a point far from existing centers\n                        dists_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        # pick from a tail of distant but reasonably good points (balance distance and quality)\n                        idx_rank = np.argsort(dists_to_centers)\n                        tail = idx_rank[-max(1, min(30, len(X_arr))):]\n                        # prefer among tail points those with good function values (choose top by F among tail)\n                        tail_sorted_by_f = sorted(tail, key=lambda i: F[i])[:max(1, min(10, len(tail)))]\n                        pick = int(self.rng.choice(tail_sorted_by_f))\n                        centers[cidx] = X_arr[pick].copy()\n                        trust_radius[cidx] = np.maximum(self.trust_init_frac * rng_range, 1e-12)\n                        stagnation[cidx] = 0\n\n            # prune archive if too large\n            if len(X) > self.max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:300]\n                rest_idx = idx_sorted[300:]\n                # sample rest to keep diversity\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (self.max_archive - 300))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasional nudge of some centers to the global best to exploit recent improvements\n            if x_best is not None and len(centers) > 0:\n                for i in range(min(len(centers), max(1, len(X)//25))):\n                    if self.rng.rand() < 0.15:\n                        centers[i] = x_best.copy()\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveCooperativeSubspaceEnsemble scored 0.368 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e9f3e383-6b8c-4beb-9ac3-012fb83c40e5", "operator": null, "metadata": {"aucs": [0.1584686434828657, 0.17316865573523055, 0.47547303908733984, 0.9077843940429182, 0.2350917854018778, 0.5842481644949491, 0.28633097874266145, 0.43711696522884713, 0.2713147296774393, 0.1525216523799312]}, "task_prompt": ""}
{"id": "0b7889bd-3f66-4150-8fdc-1c8cd53592b9", "fitness": "-inf", "name": "HybridAdaptiveSubspaceLevySurrogate", "description": "The algorithm maintains a small ensemble of adaptive \"centers\" (ensemble_size ≈ max(2,min(6,dim//3+1))) seeded by a space‑filling Latin‑hybrid initialization and a large archive (max_archive = max(2000,50*dim)), and each center has its own trust radius (trust_init_frac=0.45, expand_factor=1.7, shrink_factor=0.6, trust_max_frac=2.5) that is expanded on success and shrunk on failure with replacement after stagnation (replace_patience=7). Around each center it fits a cheap separable quadratic surrogate via weighted ridge least‑squares with Gaussian distance weights (sigma = median normalized distance, ridge=1e‑7) and optionally a weighted linear gradient estimate, and extracts local PCA directions (k ~ sqrt(dim) or 0.3*dim) from neighbors (neighbors ≈ nei_multiplier*dim with nei_multiplier=7) to define structured subspace proposals. A rich proposal generator produces quad‑minimizer variants, gradient steps, PCA subspace combinations, memory‑direction probes, jittered offspring, mirrored/boundary samples and occasional heavy‑tailed (Cauchy/Levy) jumps, then ranks candidates by surrogate prediction (z‑scored) minus a novelty bonus (lambda_novel=0.6) and evaluates only the top few per center constrained by per‑iteration and total budget (max_eval_per_iter). Global mechanisms include simulated‑annealing style uphill acceptance (T0 = 0.05*span_mean, T_decay=0.995), archive pruning, periodic center refresh toward best solutions, and occasional diversification to balance exploitation and heavy‑tailed exploration.", "code": "import numpy as np\n\nclass HybridAdaptiveSubspaceLevySurrogate:\n    \"\"\"\n    Hybrid Adaptive Subspace + Levy Surrogate (HASLS)\n\n    One-line: Maintain an ensemble of adaptive subspace centers; fit fast\n    local separable quadratic surrogates with Gaussian distance weights;\n    produce many proposals (PCA subspace, memory directions, surrogate minimizers,\n    Levy jumps, mirrored boundary samples), rank them by surrogate prediction\n    and novelty, then evaluate only the top candidates within per-iteration budget.\n    Adapt trust radii per-center on success/failure and allow occasional\n    uphill moves via a decaying temperature.\n\n    __init__(self, budget, dim, seed=None, ensemble_size=None, init_frac=0.12, max_eval_per_iter=80)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, ensemble_size=None,\n                 init_frac=0.12, max_eval_per_iter=80):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(seed)\n\n        # initialization sizing\n        self.init_frac = float(init_frac)\n        self.min_init = max(12, 2 * self.dim)\n        self.max_init = min(600, int(0.5 * self.budget))\n\n        # ensemble sizing (different from original)\n        if ensemble_size is None:\n            self.ensemble_size = max(2, min(6, max(2, self.dim // 3 + 1)))\n        else:\n            self.ensemble_size = max(1, int(ensemble_size))\n\n        # neighbor and surrogate parameters\n        self.nei_multiplier = 7  # neighbors = multiplier * dim (slightly different)\n        self.ridge = 1e-7\n\n        # trust region behaviour\n        self.trust_init_frac = 0.45\n        self.trust_min = 1e-7\n        self.trust_max_frac = 2.5\n        self.expand_factor = 1.7\n        self.shrink_factor = 0.60\n\n        # candidate generation\n        self.direction_scales = np.array([0.2, 0.4, 0.8, 1.6])\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        self.cauchy_prob = 0.14\n        self.cauchy_scale_frac = 0.40\n\n        # ensemble management\n        self.replace_patience = 7\n        self.max_archive = None  # set later based on dim/budget\n\n        # temperature for occasional uphill acceptance (simulated-annealing style)\n        self.T0 = None\n        self.T_decay = 0.995\n\n    def __call__(self, func):\n        n = int(self.dim)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        span = ub - lb\n        span_mean = float(np.mean(span))\n        self.max_archive = max(2000, 50 * n)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archive\n        X = []\n        F = []\n\n        # best so far\n        f_best = np.inf\n        x_best = None\n\n        # initial temperature based on initial function scale guess (use span)\n        self.T0 = 0.05 * (span_mean if span_mean > 0 else 1.0)\n        T = self.T0\n\n        # Determine initial sampling budget\n        init_budget = int(np.clip(self.init_frac * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n\n        # space-filling-ish initialization (Latin-hybrid: uniform + jittered grid)\n        for i in range(init_budget):\n            # employ jittered uniform sampling\n            x = self.rng.uniform(lb, ub)\n            fx = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(fx))\n            if fx < f_best:\n                f_best = float(fx); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # Helper: safe evaluation\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            xc = np.clip(x, lb, ub)\n            fx = func(xc)\n            evals += 1\n            X.append(xc.copy()); F.append(float(fx))\n            if fx < f_best - 1e-12:\n                f_best = float(fx); x_best = xc.copy()\n            return float(fx), xc\n\n        # create initial centers using top-ranked distinct points + random fill\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if all(np.linalg.norm(x - c) > 1e-9 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.ensemble_size, len(X)))\n        while len(centers) < self.ensemble_size:\n            centers.append(self.rng.uniform(lb, ub))\n        centers = [np.array(c) for c in centers[:self.ensemble_size]]\n\n        # per-center state: trust radii, stagnation, last successful direction (memory)\n        trust = [np.maximum(self.trust_init_frac * span, 1e-9) for _ in centers]\n        stagnation = [0 for _ in centers]\n        last_dir = [None for _ in centers]  # memory of last good direction vector\n\n        # local surrogate: separable quadratic with Gaussian kernel weights (different weighting)\n        def fit_local_separable_quadratic(center, X_nei, F_nei, tr):\n            dx = X_nei - center\n            dists = np.linalg.norm(dx / (tr + 1e-12), axis=1)  # normalized by trust\n            # Gaussian kernel width adaptively set to median distance or 1.0\n            sigma = np.median(dists) if dx.shape[0] > 0 else 1.0\n            sigma = max(sigma, 1e-6)\n            w = np.exp(-(dists ** 2) / (2.0 * sigma ** 2)) + 1e-12\n            W = np.sqrt(w)[:, None]\n            m = dx.shape[0]\n            # design matrix: [1, dx, 0.5*dx^2]\n            M = np.ones((m, 1 + 2 * n))\n            M[:, 1:1 + n] = dx\n            M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n            b = F_nei\n            # weighted ridge least squares (different linear algebra)\n            try:\n                A = (W * M).T @ (W * M) + self.ridge * np.eye(M.shape[1])\n                rhs = (W * M).T @ (W * b)\n                params = np.linalg.solve(A, rhs).flatten()\n                a = params[0]\n                b_lin = params[1:1 + n]\n                h_diag = params[1 + n:1 + 2 * n]\n                # regularize curvature to avoid negative / tiny curvature problems\n                h_diag = np.where(np.abs(h_diag) < 1e-8, np.sign(h_diag) * 1e-8 + 1e-8, h_diag)\n                return a, b_lin, h_diag\n            except Exception:\n                return None\n\n        # PCA directions (centered on best or local neighbors); returns top-k PCs\n        def pca_components(X_nei, center, max_k=None):\n            dx = X_nei - center\n            if dx.shape[0] < 2:\n                return None\n            # small SVD on dx\n            try:\n                U, S, Vt = np.linalg.svd(dx, full_matrices=False)\n                # Vt rows are principal directions; choose k adaptively: sqrt(n) or 20% of dim\n                if max_k is None:\n                    max_k = max(1, int(np.sqrt(n)))\n                k = min(max_k, Vt.shape[0])\n                return Vt[:k].T, S[:k] ** 2 / max(1, dx.shape[0] - 1)  # vectors, approx eigenvalues\n            except Exception:\n                return None\n\n        iter_count = 0\n        # main loop\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            # periodic refresh of centers from archive: keep best and diverse ones\n            if iter_count % 5 == 0 and len(X) > 0:\n                topk = min(len(centers), max(1, len(X) // 12))\n                new_centers = get_top_centers(topk)\n                merged = centers + new_centers\n                uniq = []\n                for c in merged:\n                    if not any(np.linalg.norm(c - u) < 1e-9 for u in uniq):\n                        uniq.append(c)\n                    if len(uniq) >= self.ensemble_size:\n                        break\n                while len(uniq) < self.ensemble_size:\n                    uniq.append(self.rng.uniform(lb, ub))\n                centers = [np.array(c) for c in uniq[:self.ensemble_size]]\n                # reset stats\n                trust = [np.maximum(self.trust_init_frac * span, 1e-9) for _ in centers]\n                stagnation = [0 for _ in centers]\n                last_dir = [None for _ in centers]\n                T = max(T * self.T_decay, 1e-12)\n\n            improved_any = False\n            # randomize order each iter\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            for cidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n\n                center = centers[cidx].copy()\n                tr = trust[cidx].copy()\n\n                # find neighbors\n                neighbors_needed = max(2 * n + 1, self.nei_multiplier * n)\n                have_neighbors = len(X) >= neighbors_needed\n\n                # proposal list (we'll produce many and then surrogate-rank them)\n                proposals = []\n                proposal_meta = []  # store origin tags\n\n                if have_neighbors:\n                    X_arr = np.asarray(X)\n                    F_arr = np.asarray(F)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), neighbors_needed)\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n\n                    # fit local separable quadratic and linear around center (distinct method)\n                    quad = fit_local_separable_quadratic(center, X_nei, F_nei, tr)\n                    # compute a simple linear gradient estimate via weighted least-squares around center\n                    try:\n                        W_lin = np.exp(-(dists[idx_nei] / (np.median(dists[idx_nei]) + 1e-12)) ** 2)\n                        A_lin = np.hstack([np.ones((X_nei.shape[0], 1)), (X_nei - center)])\n                        wA = (W_lin[:, None] * A_lin)\n                        params_lin = np.linalg.lstsq(wA, W_lin * F_nei, rcond=None)[0]\n                        grad_est = params_lin[1:].flatten()\n                    except Exception:\n                        grad_est = None\n\n                    # PCA components around center\n                    pca = pca_components(X_nei, center, max_k=max(1, int(0.3 * n)))\n\n                    # SURROGATE-MINIMIZER: if quad exists, compute predicted minimizer\n                    if quad is not None:\n                        a_q, b_q, h_q = quad\n                        delta_q = - b_q / (h_q + 1e-20)\n                        # limit by trust: scale if necessary\n                        clamp = np.minimum(1.0, np.abs(delta_q) / (tr + 1e-12))\n                        delta_q = delta_q * np.minimum(1.0, 1.0 / (np.max(clamp) + 1e-12))\n                        # create a few scaled variants\n                        for scale in [0.25, 0.5, 1.0]:\n                            dx = np.clip(scale * delta_q, -tr, tr)\n                            cand = np.clip(center + dx, lb, ub)\n                            proposals.append(cand); proposal_meta.append(('quad_scale', scale))\n\n                    # Linear sign-step / gradient descent proposals\n                    if grad_est is not None:\n                        for alpha in [0.4, 0.8, 1.2]:\n                            step = -alpha * (grad_est / (np.linalg.norm(grad_est) + 1e-12)) * np.linalg.norm(tr) / np.sqrt(n)\n                            cand = np.clip(center + step, lb, ub)\n                            proposals.append(cand); proposal_meta.append(('grad_step', alpha))\n\n                    # PCA guided subspace probes: random linear combos in top subspace and axis-aligned\n                    if pca is not None:\n                        vecs, vals = pca\n                        k = vecs.shape[1]\n                        # multiply sparse combinations in subspace\n                        for _ in range(min(12, 3 * k)):\n                            coeffs = self.rng.randn(k)\n                            coeffs = coeffs / (np.linalg.norm(coeffs) + 1e-12)\n                            length = self.rng.choice([0.25, 0.5, 1.0, 1.8])\n                            step = vecs @ (coeffs * length * (np.linalg.norm(tr) / np.sqrt(n)))\n                            proposals.append(np.clip(center + step, lb, ub)); proposal_meta.append(('pca_combo', length))\n                        # single PC +/-\n                        for i_pc in range(k):\n                            v = vecs[:, i_pc]\n                            for s in (0.5, 1.0, 1.6):\n                                step = s * (np.linalg.norm(tr) / np.sqrt(n)) * v\n                                proposals.append(np.clip(center + step, lb, ub)); proposal_meta.append((f'pca_{i_pc}_+', s))\n                                proposals.append(np.clip(center - step, lb, ub)); proposal_meta.append((f'pca_{i_pc}_-', s))\n\n                    # Memory-direction proposals: bias along last successful direction\n                    if last_dir[cidx] is not None:\n                        ld = last_dir[cidx]\n                        for s in (0.5, 1.0):\n                            proposals.append(np.clip(center + s * np.linalg.norm(tr) * ld, lb, ub)); proposal_meta.append(('memory_dir', s))\n                            proposals.append(np.clip(center - s * np.linalg.norm(tr) * ld, lb, ub)); proposal_meta.append(('memory_dir', -s))\n\n                    # add jittered versions of top proposals (ensemble effect)\n                    base_samples = min(20, max(8, len(proposals)))\n                    for i in range(base_samples):\n                        # pick a parent (if none, sample random)\n                        if len(proposals) > 0:\n                            parent = proposals[self.rng.randint(0, len(proposals))]\n                        else:\n                            parent = center\n                        jitter = self.rng.randn(n) * 0.02 * (tr + 1e-12)\n                        proposals.append(np.clip(parent + jitter, lb, ub)); proposal_meta.append(('jitter', i))\n\n                    # Levy (heavy-tailed) proposals sometimes\n                    if self.rng.rand() < 0.5:\n                        for _ in range(6):\n                            jump = self.rng.standard_cauchy(size=n)\n                            jump = np.clip(jump, -6, 6)\n                            scale = self.cauchy_scale_frac * np.maximum(span, 1e-9)\n                            cand = np.clip(center + jump * scale * self.rng.rand(), lb, ub)\n                            proposals.append(cand); proposal_meta.append(('levy', 0))\n\n                else:\n                    # Few neighbors: exploratory random subspace proposals from center\n                    for _ in range(min(30, work_allow * 2)):\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        s = self.rng.choice(self.direction_scales)\n                        step = d * s * (np.linalg.norm(tr) / np.sqrt(n))\n                        proposals.append(np.clip(center + step, lb, ub)); proposal_meta.append(('rand_probe', s))\n                    # add a few boundary-reflected random points\n                    for _ in range(6):\n                        x = self.rng.uniform(lb, ub)\n                        # reflect near edges\n                        mirror = np.where((x - lb) < (ub - x), lb + (x - lb) * 0.5, ub - (ub - x) * 0.5)\n                        proposals.append(np.clip(mirror, lb, ub)); proposal_meta.append(('mirror', 0))\n\n                # Make proposals unique (within tolerance) and compute surrogate predictions (cheap)\n                unique = []\n                uniq_meta = []\n                for p, mtag in zip(proposals, proposal_meta):\n                    if not any(np.allclose(p, q, atol=1e-12) for q in unique):\n                        unique.append(p)\n                        uniq_meta.append(mtag)\n                proposals = unique\n                proposal_meta = uniq_meta\n\n                # If we have a local quad surrogate, compute predicted f for all proposals; otherwise estimate by distance\n                pred_vals = []\n                if have_neighbors and quad is not None:\n                    a_q, b_q, h_q = quad\n                    # compute dx relative to center and surrogate f = a + b^T dx + 0.5 * sum(h * dx^2)\n                    for p in proposals:\n                        dx = p - center\n                        pred = a_q + float(np.dot(b_q, dx)) + 0.5 * float(np.dot(h_q, dx * dx))\n                        pred_vals.append(pred)\n                else:\n                    # cheap heuristic prediction: use nearest archive neighbor value plus distance penalty\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        for p in proposals:\n                            dists = np.linalg.norm(X_arr - p, axis=1)\n                            idx = np.argmin(dists)\n                            pred = F[idx] + 0.05 * np.min(dists) * (1.0 + self.rng.rand())\n                            pred_vals.append(pred)\n                    else:\n                        pred_vals = [0.0 for _ in proposals]\n\n                # novelty score: larger distance from archive gets bonus to encourage exploration\n                novelty = []\n                if len(X) > 0:\n                    X_arr = np.asarray(X)\n                    for p in proposals:\n                        dmin = float(np.min(np.linalg.norm(X_arr - p, axis=1)))\n                        novelty.append(dmin)\n                else:\n                    novelty = [np.linalg.norm(p - center) for p in proposals]\n\n                # combine predicted value and novelty into ranking score (lower is better)\n                pred_arr = np.array(pred_vals)\n                novelty_arr = np.array(novelty)\n                # standardize\n                if pred_arr.size > 0:\n                    pred_z = (pred_arr - pred_arr.mean()) / (pred_arr.std() + 1e-12)\n                else:\n                    pred_z = pred_arr\n                nov_z = (novelty_arr - novelty_arr.mean()) / (novelty_arr.std() + 1e-12) if novelty_arr.size > 0 else novelty_arr\n                # score = surrogate_pred - lambda * novelty (we prefer low surrogate prediction and some novelty)\n                lambda_novel = 0.6\n                scores = pred_z - lambda_novel * nov_z\n                order_idx = np.argsort(scores)\n\n                # Evaluate top candidates sequentially while respecting work_allow and budget.\n                evaluated_any = False\n                # choose a cap for how many actual evaluations to do for this center this iter\n                max_actual = min(max(3, int(work_allow // 3)), max(1, work_allow))\n                # also ensure not evaluating too many if many proposals ranked similarly\n                to_eval_idx = order_idx[:min(len(order_idx), max_actual)]\n\n                for idx in to_eval_idx:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    p = proposals[int(idx)]\n                    # avoid evaluating duplicates of last call\n                    if len(X) > 0 and np.allclose(p, X[-1], atol=1e-12):\n                        continue\n                    out = safe_eval(p)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    f_p, x_p = out\n                    evaluated_any = True\n                    # acceptance logic: accept if better than center or sometimes uphill with probability depending on T\n                    # compute \"current center value\" as the best known archive point at exact center (or approximate by nearest)\n                    if len(X) > 1:\n                        # find nearest archive to center to approximate its current function\n                        X_arr = np.asarray(X)\n                        dists_c = np.linalg.norm(X_arr - center, axis=1)\n                        idxc = int(np.argmin(dists_c))\n                        f_center = F[idxc]\n                    else:\n                        f_center = f_best\n\n                    # if this candidate is better than best or better than center by margin, accept as new center\n                    if (f_p < f_best - 1e-12) or (f_p <= f_center - 1e-12) or (self.rng.rand() < np.exp(-(f_p - f_center) / max(1e-12, T))):\n                        # accept\n                        centers[cidx] = x_p.copy()\n                        # update last success direction\n                        dir_vec = x_p - center\n                        norm_dir = np.linalg.norm(dir_vec)\n                        if norm_dir > 1e-12:\n                            last_dir[cidx] = dir_vec / norm_dir\n                        # expand trust modestly\n                        trust[cidx] = np.minimum(tr * self.expand_factor, self.trust_max_frac * span)\n                        stagnation[cidx] = 0\n                        improved_any = True\n                        # break out after success to give other centers a chance\n                        break\n                    else:\n                        # rejection -> shrink trust slightly\n                        trust[cidx] = np.maximum(tr * self.shrink_factor, self.trust_min * span)\n                        stagnation[cidx] += 1\n\n                # if no candidate evaluated (rare), spend a tiny exploratory eval (random)\n                if not evaluated_any and work_allow > 0 and evals < budget:\n                    x_try = np.clip(center + (self.rng.randn(n) * 0.05) * (np.linalg.norm(tr) / np.sqrt(n)), lb, ub)\n                    out = safe_eval(x_try)\n                    work_allow -= 1\n                    if out is not None:\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            centers[cidx] = xtry.copy()\n                            trust[cidx] = np.minimum(tr * self.expand_factor, self.trust_max_frac * span)\n                            stagnation[cidx] = 0\n                            improved_any = True\n                        else:\n                            stagnation[cidx] += 1\n                            trust[cidx] = np.maximum(tr * self.shrink_factor, self.trust_min * span)\n\n                # occasional heavy-tailed jump independent of surrogate ranking\n                if (self.rng.rand() < self.cauchy_prob) and work_allow > 0 and evals < budget:\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -8, 8)\n                    scale = self.cauchy_scale_frac * np.maximum(span, 1e-9)\n                    x_jump = np.clip(centers[cidx] + jump * scale * self.rng.rand(), lb, ub)\n                    out = safe_eval(x_jump)\n                    work_allow -= 1\n                    if out is not None:\n                        f_j, x_j = out\n                        if f_j < f_best - 1e-12:\n                            centers[cidx] = x_j.copy()\n                            trust[cidx] = np.minimum(tr * 2.0, self.trust_max_frac * span)\n                            stagnation[cidx] = 0\n                            last_dir[cidx] = (x_j - center) / (np.linalg.norm(x_j - center) + 1e-12)\n                            improved_any = True\n                        else:\n                            trust[cidx] = np.maximum(tr * 0.85, self.trust_min * span)\n                            stagnation[cidx] += 1\n\n                # replace center if stagnated too long\n                if stagnation[cidx] >= self.replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # choose candidate moderately good but far from existing centers to increase exploration\n                        # compute distance to existing centers\n                        dists_to_centers = np.min(np.vstack([np.linalg.norm(X_arr - c, axis=1) for c in centers]), axis=0)\n                        # pick among top distant indices but not extremely bad function values\n                        candidate_indices = np.argsort(dists_to_centers)[-min(30, len(X_arr)):]\n                        # among these pick one with reasonably low rank (top 50 percentile)\n                        subset = candidate_indices\n                        if len(subset) > 0:\n                            pick = int(self.rng.choice(subset))\n                            centers[cidx] = X_arr[pick].copy()\n                            trust[cidx] = np.maximum(self.trust_init_frac * span, 1e-9)\n                            stagnation[cidx] = 0\n                            last_dir[cidx] = None\n                    else:\n                        centers[cidx] = self.rng.uniform(lb, ub)\n                        trust[cidx] = np.maximum(self.trust_init_frac * span, 1e-9)\n                        stagnation[cidx] = 0\n                        last_dir[cidx] = None\n\n            # prune archive if too large\n            if len(X) > self.max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:300]\n                rest_idx = idx_sorted[300:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (self.max_archive - 300))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # cool temperature a bit\n            T = max(T * self.T_decay, 1e-12)\n\n            # occasional nudge: move some centers towards global best to consolidate exploitation\n            if x_best is not None and len(centers) > 0:\n                for i in range(min(len(centers), max(1, len(X) // 25))):\n                    if self.rng.rand() < 0.15:\n                        centers[i] = 0.2 * centers[i] + 0.8 * x_best\n\n            # if nothing improved for a while, diversify: sprinkle a few random points into centers\n            if not improved_any and iter_count % 12 == 0:\n                for i in range(len(centers)):\n                    if self.rng.rand() < 0.2:\n                        centers[i] = self.rng.uniform(lb, ub)\n                        trust[i] = np.maximum(self.trust_init_frac * span, 1e-9)\n                        stagnation[i] = 0\n                        last_dir[i] = None\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "e9f3e383-6b8c-4beb-9ac3-012fb83c40e5", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a6974d82-c816-483d-8448-ea03de798ea7", "fitness": 0.3453835872862343, "name": "EnsembleLevyAdaptiveSurrogate", "description": "Ensemble Lévy Adaptive Surrogate Search (ELASS) maintains a small ensemble of trust-centered solutions initialized by jittered space‑filling samples (init_ratio=0.18, larger min_init/max_init) with per‑center trust radii and stagnation counters to drive center replacement and occasional nudges toward the global best. Around each center it fits cheap weighted linear surrogates (ridge regularization) and estimates local isotropic curvature from residuals to propose quasi‑Newton scaled gradient steps, augmented by PCA‑guided low‑dim moves, coordinate tweaks, and multi‑scale directional probes (wide direction_scales) for robust local search. To preserve global exploration it uses heavier-tailed Lévy/Cauchy jumps with higher probability and scale (cauchy_prob=0.18, cauchy_scale_frac=0.6), probabilistic Gaussian refinements around the best, and aggressive trust updates (success_expand=2.0, failure_shrink=0.5) with archive pruning to control memory. The design trades more aggressive expansion, larger initialization, and more frequent heavy jumps for adaptive local modeling and diverse multi-scale proposals to work across a wide range of continuous landscapes.", "code": "import numpy as np\n\nclass EnsembleLevyAdaptiveSurrogate:\n    \"\"\"\n    Ensemble Lévy Adaptive Surrogate Search (ELASS)\n\n    One-line: Maintain an ensemble of adaptive-trust centers, fit cheap weighted linear surrogates,\n    estimate local curvature from neighbor residuals, propose scaled gradient-quasi-Newton steps,\n    combine with PCA low-dim moves, multi-scale directional probes, Gaussian refinement around best,\n    and occasional heavy-tailed Lévy/Cauchy jumps. Parameters & equations differ from the\n    provided EPASS algorithm (different ensemble sizing, initialization ratios, neighbor counts,\n    trust update rules, scales, and surrogate step computation).\n\n    Main algorithmic parameters (different choices from the provided algorithm):\n      - init_ratio: 0.18 (vs 0.12)\n      - min_init: max(12, 3*dim) (vs max(10, 2*dim))\n      - max_init: min(0.3*budget, 500)\n      - ensemble_size: max(3, min(10, dim//3 + 2)) (different formula)\n      - model_neighbor_multiplier: 6 (fewer neighbors)\n      - ridge: 1e-4 (more regularization)\n      - trust_init_frac: 0.3 (smaller initial trust)\n      - success_expand: 2.0, failure_shrink: 0.5 (more aggressive)\n      - direction_scales: [0.125,0.25,0.5,1.0,2.0,4.0] (wider multi-scale probes)\n      - max_eval_per_iter: 40 (tight per-iteration budget)\n      - cauchy_prob: 0.18, cauchy_scale_frac: 0.60 (more frequent/heavier jumps)\n      - center_replace_patience: 12 (replace after longer stagnation)\n      - max_archive: max(1500, 30*dim) (different archive size)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # initialization sizing (different)\n        self.init_ratio = 0.18\n        self.min_init = max(12, 3 * self.dim)\n        self.max_init = min(500, int(0.3 * self.budget))\n\n        # ensemble sizing (different formula)\n        self.ensemble_size = max(3, min(10, self.dim // 3 + 2))\n\n        # modeling & neighbors (different)\n        self.model_neighbor_multiplier = 6\n        self.ridge = 1e-4\n\n        # trust region settings (more aggressive)\n        self.trust_init_frac = 0.3\n        self.trust_min = 1e-8\n        self.trust_max_frac = 3.0\n        self.success_expand = 2.0\n        self.failure_shrink = 0.5\n\n        # probing (wider scale set)\n        self.direction_scales = np.array([0.125, 0.25, 0.5, 1.0, 2.0, 4.0])\n        self.max_eval_per_iter = 40\n\n        # Lévy/Cauchy jump settings (more likely/heavy)\n        self.cauchy_prob = 0.18\n        self.cauchy_scale_frac = 0.60\n\n        # center management & archive\n        self.center_replace_patience = 12\n        self.max_archive = None  # set in call\n\n        # additional refinement probability around best\n        self.best_local_refine_prob = 0.25\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # bounds from func (usually -5, +5 on BBOB)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        self.max_archive = max(1500, 30 * n)\n\n        budget = int(self.budget)\n        evals = 0\n\n        X = []  # evaluated points\n        F = []  # their function values\n\n        f_best = np.inf\n        x_best = None\n\n        # initial sampling budget (different clipping)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n\n        # modest space-filling initialization (random + some Latin-like jitter)\n        for i in range(init_budget):\n            # stratify a bit across dimensions: jittered uniform\n            x = self.rng.uniform(lb, ub)\n            fx = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(fx))\n            if fx < f_best:\n                f_best = float(fx); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper to pick distinct top centers\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if all(np.linalg.norm(x - c) > 1e-9 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.ensemble_size, len(X)))\n        while len(centers) < self.ensemble_size:\n            centers.append(self.rng.uniform(lb, ub))\n        centers = [np.array(c) for c in centers[:self.ensemble_size]]\n\n        # per-center trust and stagnation\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n        stagnation = [0 for _ in centers]\n\n        # safe evaluation wrapper\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            xc = np.clip(x, lb, ub)\n            fx = func(xc)\n            evals += 1\n            X.append(xc.copy()); F.append(float(fx))\n            if fx < f_best - 1e-12:\n                f_best = float(fx); x_best = xc.copy()\n            return float(fx), xc\n\n        # weighted linear surrogate around center -> returns intercept, grad\n        def fit_weighted_linear(center, neighbors_X, neighbors_F):\n            dx = neighbors_X - center\n            m = dx.shape[0]\n            if m < 2:\n                return None\n            # weights inverse distance (clipped)\n            dists = np.linalg.norm(dx, axis=1)\n            w = 1.0 / (dists + 1e-12)\n            # normalize weights\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = np.hstack([np.ones((m, 1)), dx])\n            Aw = W * A\n            bw = W * neighbors_F\n            try:\n                ridge = self.ridge * np.eye(A.shape[1])\n                params, *_ = np.linalg.lstsq(Aw.T @ Aw + ridge, Aw.T @ bw, rcond=None)\n                params = params.flatten()\n                intercept = params[0]\n                grad = params[1:1 + n]\n                # residuals used later for curvature estimate\n                pred = A @ params\n                resid = neighbors_F - pred\n                return intercept, grad, resid, dists\n            except Exception:\n                return None\n\n        # PCA directions via SVD (robust)\n        def pca_directions(neighbors_X, center, max_components=None):\n            dx = neighbors_X - center\n            if dx.shape[0] <= 1:\n                return None\n            try:\n                U, S, Vt = np.linalg.svd(dx - dx.mean(axis=0), full_matrices=False)\n                if max_components is None:\n                    max_components = min(n, max(1, int(0.25 * n)))\n                k = min(max_components, Vt.shape[0])\n                vecs = Vt[:k].T  # columns are components\n                vals = (S[:k] ** 2) / max(1, dx.shape[0] - 1)\n                return vecs, vals\n            except Exception:\n                return None\n\n        iter_count = 0\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            # occasionally refresh centers using archive best & diverse ones\n            if iter_count % 8 == 0 and len(X) > 0:\n                topk = min(len(centers), max(1, len(X) // 12))\n                new_centers = get_top_centers(topk)\n                merged = centers + new_centers\n                uniq = []\n                for c in merged:\n                    if not any(np.linalg.norm(c - u) < 1e-9 for u in uniq):\n                        uniq.append(c)\n                    if len(uniq) >= self.ensemble_size:\n                        break\n                while len(uniq) < self.ensemble_size:\n                    uniq.append(self.rng.uniform(lb, ub))\n                centers = [np.array(c) for c in uniq[:self.ensemble_size]]\n                if len(trust_radius) != len(centers):\n                    trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n                    stagnation = [0 for _ in centers]\n\n            improved_global = False\n\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            for cidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n\n                center = np.array(centers[cidx])\n                tr = trust_radius[cidx]\n\n                # determine neighbor requirement (different multiplier)\n                neighbors_needed = max(2 * n + 1, self.model_neighbor_multiplier * n)\n                if len(X) >= neighbors_needed:\n                    X_arr = np.asarray(X)\n                    F_arr = np.asarray(F)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X_arr), neighbors_needed)\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n\n                    # fit weighted linear surrogate\n                    lin = fit_weighted_linear(center, X_nei, F_nei)\n                    # PCA directions\n                    pca = pca_directions(X_nei, center)\n\n                    proposals = []\n\n                    # surrogate quasi-Newton-like step: scaled gradient step using curvature estimate\n                    if lin is not None:\n                        intercept, grad, resid, neigh_dists = lin\n                        g = np.array(grad)\n                        gnorm = np.linalg.norm(g)\n                        if gnorm > 0:\n                            # estimate local isotropic curvature from residuals:\n                            # curvature ~ median(|resid| / (dist^2 + eps))\n                            denom = neigh_dists ** 2 + 1e-12\n                            curv = np.median(np.abs(resid) / denom) if denom.size > 0 else 1e-6\n                            curv = float(max(curv, 1e-8))\n                            # compute step: delta = - (trust * scale) * g / (curv + small)\n                            scale = np.minimum(np.linalg.norm(tr) / (gnorm + 1e-12), 1.75)\n                            delta = - (scale * g) / (curv + 1e-8)\n                            # clip per-dim to trust region\n                            delta = np.clip(delta, -tr, tr)\n                            x_sur = np.clip(center + delta, lb, ub)\n                            proposals.append(('sur', x_sur))\n\n                            # also try a conservative gradient-only step\n                            delta2 = -0.5 * (tr / (np.maximum(np.abs(g), 1e-12))) * np.sign(g)\n                            delta2 = np.clip(delta2, -tr, tr)\n                            x_grad = np.clip(center + delta2, lb, ub)\n                            proposals.append(('grad_sign', x_grad))\n\n                    # PCA-guided proposals (use top PCs, different scale schedule)\n                    if pca is not None:\n                        vecs, vals = pca\n                        for i_pc in range(vecs.shape[1]):\n                            v = vecs[:, i_pc]\n                            for s in (0.4, 0.9, 1.8):\n                                step = s * (np.linalg.norm(tr) / (np.sqrt(float(n)) + 1e-12)) * v\n                                x_p_plus = np.clip(center + step, lb, ub)\n                                x_p_minus = np.clip(center - step, lb, ub)\n                                proposals.append((f'pca_{i_pc}_+', x_p_plus))\n                                proposals.append((f'pca_{i_pc}_-', x_p_minus))\n\n                    # include a directed recombination with global best (if available)\n                    if x_best is not None and np.linalg.norm(center - x_best) > 1e-9:\n                        beta = self.rng.uniform(0.1, 0.6)\n                        x_recomb = np.clip(beta * x_best + (1 - beta) * center + self.rng.randn(n) * 0.01 * rng_range, lb, ub)\n                        proposals.insert(0, ('recomb', x_recomb))\n\n                    # add jittered variants and uniqueness\n                    unique_props = []\n                    for name, p in proposals:\n                        if not any(np.allclose(p, q, atol=1e-12) for q in unique_props):\n                            unique_props.append(p)\n                        # jitter variant\n                        jitter = (self.rng.randn(n) * 0.06) * tr\n                        pj = np.clip(p + jitter, lb, ub)\n                        if not any(np.allclose(pj, q, atol=1e-12) for q in unique_props):\n                            unique_props.append(pj)\n\n                    # sort proposals by predicted improvement heuristic (close + directed to best)\n                    def score_prop(z):\n                        s = np.linalg.norm(z - center)\n                        if x_best is not None:\n                            s += 0.5 * np.linalg.norm(z - x_best)\n                        return s\n                    unique_props.sort(key=score_prop)\n\n                    # evaluate proposals until success or work_allow exhausted\n                    for xprop in unique_props:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        out = safe_eval(xprop)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        fprop, xprop = out\n                        if fprop < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xprop.copy()\n                            # more aggressive expansion on success\n                            trust_radius[cidx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                    # directional multi-scale fallback (wider scales)\n                    if not improved_global and work_allow > 0 and evals < budget:\n                        num_dirs = int(np.clip(3 + n // 3, 4, 10))\n                        dirs = []\n                        for _ in range(num_dirs):\n                            v = self.rng.randn(n)\n                            nv = np.linalg.norm(v)\n                            if nv == 0:\n                                v = np.ones(n); nv = np.linalg.norm(v)\n                            dirs.append(v / nv)\n                        self.rng.shuffle(dirs)\n                        base_step = np.linalg.norm(tr) / (np.sqrt(float(n)) + 1e-12)\n                        for d in dirs:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            cand = []\n                            for s in self.direction_scales:\n                                cand.append(np.clip(center + d * (s * base_step), lb, ub))\n                                cand.append(np.clip(center - d * (s * base_step), lb, ub))\n                            self.rng.shuffle(cand)\n                            for x_try in cand:\n                                if work_allow <= 0 or evals >= budget:\n                                    break\n                                # avoid duplicate immediate repeats\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.8, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n                            # end cand\n                        # end dirs\n\n                    # coordinate tweaks (use fraction of trust)\n                    if not improved_global and work_allow > 0 and evals < budget:\n                        coord_step = np.maximum(tr * 0.5, 1e-12)\n                        coords = np.arange(n)\n                        max_coords = min(n, max(4, int(work_allow // 3)))\n                        if max_coords < n:\n                            coords = self.rng.choice(coords, size=max_coords, replace=False)\n                        for i in coords:\n                            if work_allow <= 0 or evals >= budget:\n                                break\n                            for sign in (+1.0, -1.0):\n                                x_try = centers[cidx].copy()\n                                x_try[i] = x_try[i] + sign * coord_step[i]\n                                x_try = np.clip(x_try, lb, ub)\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                work_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_global = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.5, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n\n                    # occasional Lévy/Cauchy jump (more frequent and larger)\n                    if not improved_global and work_allow > 0 and evals < budget and self.rng.rand() < self.cauchy_prob:\n                        scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                        jump = self.rng.standard_cauchy(size=n)\n                        jump = np.clip(jump, -12, 12)\n                        x_jump = np.clip(centers[cidx] + jump * scale, lb, ub)\n                        out = safe_eval(x_jump)\n                        if out is not None:\n                            work_allow -= 1\n                            fjump, xjump = out\n                            if fjump < f_best - 1e-12:\n                                improved_global = True\n                                centers[cidx] = xjump.copy()\n                                trust_radius[cidx] = np.minimum(tr * 2.5, self.trust_max_frac * rng_range)\n                                stagnation[cidx] = 0\n                            else:\n                                # penalize trust a bit\n                                trust_radius[cidx] = np.maximum(tr * 0.7, self.trust_min * rng_range)\n\n                    # probabilistic local refinement around global best (if improvement happened elsewhere)\n                    if not improved_global and x_best is not None and work_allow > 0 and self.rng.rand() < self.best_local_refine_prob:\n                        sigma = 0.15 * np.maximum(rng_range, 1e-9) * (0.5 + self.rng.rand())\n                        x_local = np.clip(x_best + self.rng.randn(n) * sigma, lb, ub)\n                        out = safe_eval(x_local)\n                        work_allow -= 1\n                        if out is not None:\n                            ftry, xtry = out\n                            if ftry < f_best - 1e-12:\n                                improved_global = True\n                                # move one of the centers to this improved best (diversify)\n                                c_replace = self.rng.randint(len(centers))\n                                centers[c_replace] = xtry.copy()\n                                trust_radius[c_replace] = np.minimum(trust_radius[c_replace] * 1.4, self.trust_max_frac * rng_range)\n                                stagnation[c_replace] = 0\n\n                    # update stagnation/trust if no improvement for this center\n                    if not improved_global:\n                        stagnation[cidx] += 1\n                        trust_radius[cidx] = np.maximum(tr * self.failure_shrink, self.trust_min * rng_range)\n                    else:\n                        # small positive effect on other centers' stagnation reset\n                        for j in range(len(stagnation)):\n                            if j != cidx and stagnation[j] > 0:\n                                stagnation[j] = max(0, stagnation[j] - 1)\n\n                else:\n                    # not enough data: exploratory random probes from center with multiple scales\n                    base_step = np.linalg.norm(trust_radius[0]) / (np.sqrt(float(n)) + 1e-12)\n                    probes = min(8, work_allow)\n                    for _ in range(probes):\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        s = self.rng.choice(self.direction_scales)\n                        x_try = np.clip(center + d * s * base_step, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_global = True\n                            centers[cidx] = xtry.copy()\n                            trust_radius[cidx] = np.minimum(trust_radius[cidx] * 1.6, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                # replace center if stagnated for long\n                if stagnation[cidx] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # compute distance to current centers\n                        dists_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        # prefer moderately good and far points: score = rank(F) - 0.5*distance_rank\n                        idx_by_F = np.argsort(F)\n                        rankF = np.empty_like(idx_by_F); rankF[idx_by_F] = np.arange(len(idx_by_F))\n                        rankD = np.argsort(dists_to_centers)\n                        rankD_inv = np.empty_like(rankD); rankD_inv[rankD] = np.arange(len(rankD))\n                        score = rankF - 0.4 * rankD_inv\n                        cand_idx = np.argsort(score)[:max(1, min(30, len(X_arr)))]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[cidx] = X_arr[pick].copy()\n                        trust_radius[cidx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                        stagnation[cidx] = 0\n\n            # prune archive to keep memory manageable\n            if len(X) > self.max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:250]\n                rest_idx = idx_sorted[250:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (self.max_archive - 250))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # gently nudge some centers to the current best to concentrate search (probabilistic)\n            if x_best is not None and len(centers) > 0:\n                for i in range(min(len(centers), max(1, len(X)//25))):\n                    if self.rng.rand() < 0.18:\n                        centers[i] = x_best.copy()\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm EnsembleLevyAdaptiveSurrogate scored 0.345 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e9f3e383-6b8c-4beb-9ac3-012fb83c40e5", "operator": null, "metadata": {"aucs": [0.1369083605205451, 0.1766330711924029, 0.5850057572137071, 0.6563281205903098, 0.2246941762929653, 0.7159093112992936, 0.24931415725303052, 0.3400067334774557, 0.21925410090003894, 0.1497820841225942]}, "task_prompt": ""}
{"id": "27595738-6831-4757-a958-268513f57bc9", "fitness": "-inf", "name": "EnsembleSurrogateBanditTrust", "description": "The algorithm maintains an ensemble of candidate centers with per-center trust radii (initialized from trust_init_frac and bounded by trust_max_frac) and builds an archive from a small space-filling initial sample (init_ratio, min_init/max_init) while seeding missing centers randomly or from top archived points. It fits cheap distance-weighted separable quadratic surrogates (linear + diagonal Hessian, ridge regularization) on a neighborhood sized by model_neighbor_multiplier * dim and also computes PCA directions to capture anisotropy for low-dimensional searches. A bandit-style allocation (score = predicted local improvement / (1+uncertainty) + small trust bias, normalized into probs and limited by max_eval_per_iter) directs per-iteration evaluations among centers, and proposals include surrogate minimizers, PCA-guided Gaussian samples, linear sign steps, multi-scale directional probes, coordinate tweaks, and occasional heavy-tailed Cauchy jumps (cauchy_prob, cauchy_scale_frac). Adaptation and robustness come from trust updates on success/failure (success_expand, failure_shrink), stagnation counters with center replacement after center_replace_patience, archive pruning to limit memory, occasional nudges toward the global best, and a safe_eval wrapper that enforces bounds and the strict evaluation budget.", "code": "import numpy as np\n\nclass EnsembleSurrogateBanditTrust:\n    \"\"\"\n    Ensemble Surrogate Bandit Trust search (EAS-BTS)\n\n    Main ideas:\n    - Small space-filling init, maintain an archive and an ensemble of centers with per-center trust radii.\n    - Fit cheap distance-weighted separable quadratic + linear surrogates on nearest neighbors.\n    - Allocate per-iteration evaluations across centers via a bandit-style score: predicted local improvement / (1 + local uncertainty).\n    - Propose surrogate minimizers (trust-clamped), PCA-guided low-dim Gaussian searches inside trust, multi-scale directional probes and coordinate tweaks as fallback.\n    - Occasional heavy-tailed Cauchy jumps to escape basins and center replacement on stagnation.\n    - Strict safe_eval wrapper enforces budget and bounds.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # initialization sizing\n        self.init_ratio = 0.12\n        self.min_init = max(10, 2 * self.dim)\n        self.max_init = min(400, int(0.4 * self.budget))\n\n        # ensemble sizing\n        self.ensemble_size = max(2, min(8, self.dim // 2 + 1))\n\n        # modeling\n        self.model_neighbor_multiplier = 8\n        self.ridge = 1e-6\n\n        # trust region\n        self.trust_init_frac = 0.5\n        self.trust_min = 1e-8\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.6\n        self.failure_shrink = 0.66\n\n        # probing / misc\n        self.direction_scales = np.array([0.25, 0.5, 1.0, 2.0])\n        self.max_eval_per_iter = 60\n        self.cauchy_prob = 0.12\n        self.cauchy_scale_frac = 0.45\n\n        # center management\n        self.center_replace_patience = 8\n\n        # RNG\n        self.rng = np.random.RandomState(self.seed)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        # allow scalar bounds\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archive\n        X = []\n        F = []\n\n        # best\n        f_best = np.inf\n        x_best = None\n\n        # initial sampling\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # initialize centers as top distinct points\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if all(np.linalg.norm(x - c) > 1e-9 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.ensemble_size, len(X)))\n        while len(centers) < self.ensemble_size:\n            centers.append(self.rng.uniform(lb, ub))\n        centers = [np.array(c) for c in centers[:self.ensemble_size]]\n\n        # per-center trust radii and stagnation counters\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n        stagnation = [0 for _ in centers]\n\n        # safe eval wrapper\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            xc = np.clip(x, lb, ub)\n            f = func(xc)\n            evals += 1\n            X.append(xc.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = xc.copy()\n            return float(f), xc\n\n        # fit separable quad: returns (a, b_lin, h_diag, residual_std) or None\n        def fit_separable_quad(center, neighbors_X, neighbors_F):\n            dx = neighbors_X - center\n            m = dx.shape[0]\n            M = np.ones((m, 1 + 2 * n))\n            M[:, 1:1 + n] = dx\n            M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n            y = neighbors_F\n            dists = np.linalg.norm(dx, axis=1)\n            # weights (closer => larger)\n            w = 1.0 / (dists + 1e-12)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            bvec = W * y\n            try:\n                ridge = self.ridge * np.eye(M.shape[1])\n                params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ bvec, rcond=None)\n                params = params.flatten()\n                a = params[0]\n                b_lin = params[1:1 + n]\n                h_diag = params[1 + n:1 + 2 * n]\n                # regularize curvature to positive floor\n                h_reg = np.copy(h_diag)\n                h_reg[h_reg < 1e-8] = 1e-8\n                # compute residual std as proxy for uncertainty\n                y_pred = M @ params\n                resid = y - y_pred\n                sigma = np.sqrt(np.maximum(1e-12, np.mean(resid ** 2)))\n                return a, b_lin, h_reg, sigma\n            except Exception:\n                return None\n\n        # PCA directions on neighbors, return (vecs, vals)\n        def pca_directions(neighbors_X, center, max_components=None):\n            dx = neighbors_X - center\n            if dx.shape[0] <= 1:\n                return None\n            cov = np.cov(dx.T)\n            try:\n                vals, vecs = np.linalg.eigh(cov)\n                idx = np.argsort(vals)[::-1]\n                vals = vals[idx]; vecs = vecs[:, idx]\n                if max_components is None:\n                    max_components = min(n, max(1, int(0.2 * n)))\n                k = min(max_components, vecs.shape[1])\n                return vecs[:, :k], vals[:k]\n            except Exception:\n                return None\n\n        # main loop\n        iter_since_refresh = 0\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            iter_since_refresh += 1\n\n            # occasionally refresh centers to include top archive points for diversity\n            if iter_since_refresh % 6 == 0 and len(X) > 0:\n                topk = min(len(centers), max(1, len(X) // 10))\n                new_centers = get_top_centers(topk)\n                merged = centers + new_centers\n                uniq = []\n                for c in merged:\n                    if not any(np.linalg.norm(c - u) < 1e-9 for u in uniq):\n                        uniq.append(c)\n                    if len(uniq) >= self.ensemble_size:\n                        break\n                while len(uniq) < self.ensemble_size:\n                    uniq.append(self.rng.uniform(lb, ub))\n                centers = [np.array(c) for c in uniq[:self.ensemble_size]]\n                # ensure trust/stagnation lists align\n                trust_radius = [trust_radius[i] if i < len(trust_radius) else np.maximum(self.trust_init_frac * rng_range, 1e-9) for i in range(len(centers))]\n                stagnation = [0 for _ in centers]\n\n            # compute bandit scores (predicted improvement / (1 + uncertainty)) for each center\n            scores = []\n            for cidx, center in enumerate(centers):\n                # default small exploration score\n                score = 1e-6\n                # must have enough neighbors to fit surrogate\n                neighbors_needed = max(2 * n + 1, self.model_neighbor_multiplier * n)\n                if len(X) >= neighbors_needed:\n                    X_arr = np.asarray(X)\n                    F_arr = np.asarray(F)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), neighbors_needed)\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n                    quad = fit_separable_quad(center, X_nei, F_nei)\n                    if quad is not None:\n                        a_q, b_q, h_q, sigma = quad\n                        # predicted minimizer and predicted value\n                        delta_q = -b_q / (h_q + 1e-20)\n                        delta_q = np.clip(delta_q, -trust_radius[cidx], trust_radius[cidx])\n                        x_q = np.clip(center + delta_q, lb, ub)\n                        # predicted f at model minimizer (use quadratic formula)\n                        dx = x_q - center\n                        f_pred = a_q + np.dot(b_q, dx) + 0.5 * np.dot(h_q, dx * dx)\n                        # center f_est: use nearest neighbor value as proxy if center not evaluated\n                        # find closest archived value\n                        if len(X) > 0:\n                            nearest_idx = np.argmin(np.linalg.norm(np.asarray(X) - center, axis=1))\n                            f_center = float(F[nearest_idx])\n                        else:\n                            f_center = f_best\n                        # compute score: predicted gain scaled by uncertainty and trust size\n                        pred_gain = max(0.0, f_center - f_pred)\n                        # encourage exploring larger trust regions a bit\n                        trust_size = np.linalg.norm(trust_radius[cidx]) / (1.0 + np.sqrt(n))\n                        score = (pred_gain + 1e-12) / (1.0 + sigma) + 0.01 * trust_size\n                else:\n                    # if no model, small exploration score proportional to trust\n                    score = 0.01 * (np.linalg.norm(trust_radius[cidx]) + 1e-12)\n                # decay by stagnation to prefer active centers\n                score = score / (1.0 + 0.2 * stagnation[cidx])\n                scores.append(score)\n\n            # normalize scores to probabilities for allocating work_allow across centers\n            scores = np.maximum(np.array(scores, dtype=float), 1e-12)\n            probs = scores / np.sum(scores)\n\n            # allocate integer budget slices across centers (at least 1 to top ones)\n            alloc = np.floor(probs * work_allow).astype(int)\n            # ensure we allocate remaining by greedy assignment\n            remaining_alloc = int(work_allow - np.sum(alloc))\n            if remaining_alloc > 0:\n                order = np.argsort(-probs)\n                for idx in order[:remaining_alloc]:\n                    alloc[idx] += 1\n\n            # iterate centers and use allocated evals\n            for cidx in range(len(centers)):\n                if evals >= budget:\n                    break\n                if alloc[cidx] <= 0:\n                    # small chance for random global exploration if nothing allocated\n                    if self.rng.rand() < 0.02 and evals < budget:\n                        xg = self.rng.uniform(lb, ub)\n                        out = safe_eval(xg)\n                        if out is None:\n                            break\n                        alloc[cidx] = 0\n                    continue\n\n                # local budget for this center\n                local_allow = alloc[cidx]\n                center = np.array(centers[cidx])\n                tr = trust_radius[cidx]\n\n                improved_here = False\n\n                neighbors_needed = max(2 * n + 1, self.model_neighbor_multiplier * n)\n                if len(X) >= neighbors_needed:\n                    X_arr = np.asarray(X)\n                    F_arr = np.asarray(F)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), neighbors_needed)\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n\n                    # fit surrogate\n                    quad = fit_separable_quad(center, X_nei, F_nei)\n                    # linear fit (around center) for fallback direction\n                    try:\n                        A_lin = np.hstack([np.ones((X_nei.shape[0], 1)), X_nei - center])\n                        params_lin, *_ = np.linalg.lstsq(A_lin, F_nei, rcond=None)\n                        b_lin_only = params_lin[1:].flatten()\n                    except Exception:\n                        b_lin_only = None\n\n                    # PCA directions\n                    pca = pca_directions(X_nei, center)\n\n                    # 1) Surrogate minimizer proposal (highest priority)\n                    if quad is not None and local_allow > 0 and evals < budget:\n                        a_q, b_q, h_q, sigma = quad\n                        delta_q = -b_q / (h_q + 1e-20)\n                        delta_q = np.clip(delta_q, -tr, tr)\n                        x_q = np.clip(center + delta_q, lb, ub)\n                        out = safe_eval(x_q)\n                        local_allow -= 1\n                        if out is None:\n                            break\n                        f_q, x_q = out\n                        if f_q < f_best - 1e-12:\n                            improved_here = True\n                            centers[cidx] = x_q.copy()\n                            trust_radius[cidx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            # update remaining/allocation logic\n                        else:\n                            # shrink trust on failure\n                            trust_radius[cidx] = np.maximum(tr * self.failure_shrink, self.trust_min * rng_range)\n\n                    # 2) PCA-guided low-dimensional Gaussian sampling (exploit anisotropy)\n                    if not improved_here and local_allow > 0 and pca is not None and evals < budget:\n                        vecs, vals = pca\n                        # number of samples proportional to min(local_allow, #components*2)\n                        samples = min(local_allow, max(1, vecs.shape[1] * 2))\n                        for s_i in range(samples):\n                            if local_allow <= 0 or evals >= budget:\n                                break\n                            # choose a PC index (favor top components)\n                            pc_idx = self.rng.choice(vecs.shape[1], p=(vals / (np.sum(vals) + 1e-12)))\n                            direction = vecs[:, pc_idx]\n                            # sample gaussian in subspace spanned by one PC with scale from trust radius\n                            scale = (np.linalg.norm(tr) / np.sqrt(float(n))) * self.rng.uniform(0.3, 1.2)\n                            x_try = np.clip(center + direction * (self.rng.randn() * scale), lb, ub)\n                            out = safe_eval(x_try)\n                            local_allow -= 1\n                            if out is None:\n                                break\n                            ftry, xtry = out\n                            if ftry < f_best - 1e-12:\n                                improved_here = True\n                                centers[cidx] = xtry.copy()\n                                trust_radius[cidx] = np.minimum(tr * 1.5, self.trust_max_frac * rng_range)\n                                stagnation[cidx] = 0\n                                break\n\n                    # 3) Linear sign-based step / ensemble perturbations\n                    if not improved_here and local_allow > 0 and b_lin_only is not None and evals < budget:\n                        step = -0.5 * np.sign(b_lin_only) * tr\n                        x_lin = np.clip(center + step, lb, ub)\n                        out = safe_eval(x_lin)\n                        local_allow -= 1\n                        if out is None:\n                            break\n                        f_lin, x_lin = out\n                        if f_lin < f_best - 1e-12:\n                            improved_here = True\n                            centers[cidx] = x_lin.copy()\n                            trust_radius[cidx] = np.minimum(tr * 1.3, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n\n                    # 4) fallback directional multi-scale probes\n                    if not improved_here and local_allow > 0 and evals < budget:\n                        num_dirs = int(np.clip(4 + n // 2, 4, 12))\n                        dirs = []\n                        for _ in range(num_dirs):\n                            v = self.rng.randn(n)\n                            nv = np.linalg.norm(v)\n                            if nv == 0:\n                                v = np.ones(n); nv = np.linalg.norm(v)\n                            dirs.append(v / nv)\n                        self.rng.shuffle(dirs)\n                        base_step = np.linalg.norm(tr) / np.sqrt(float(n) + 1e-12)\n                        for d in dirs:\n                            if local_allow <= 0 or evals >= budget:\n                                break\n                            cand = []\n                            for s in self.direction_scales:\n                                step = s * base_step\n                                cand.append(np.clip(center + d * step, lb, ub))\n                                cand.append(np.clip(center - d * step, lb, ub))\n                            self.rng.shuffle(cand)\n                            for x_try in cand:\n                                if local_allow <= 0 or evals >= budget:\n                                    break\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                local_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_here = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.25, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n                            # end cand\n                        # end dirs\n\n                    # 5) coordinate search small tweaks\n                    if not improved_here and local_allow > 0 and evals < budget:\n                        coord_step = np.maximum(tr, 1e-12)\n                        coords = np.arange(n)\n                        max_coords = min(n, max(2, int(local_allow // 2)))\n                        if max_coords < n:\n                            coords = self.rng.choice(coords, size=max_coords, replace=False)\n                        for i in coords:\n                            if local_allow <= 0 or evals >= budget:\n                                break\n                            for sign in (+1.0, -1.0):\n                                x_try = centers[cidx].copy()\n                                x_try[i] = x_try[i] + sign * coord_step[i]\n                                x_try = np.clip(x_try, lb, ub)\n                                if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                    continue\n                                out = safe_eval(x_try)\n                                local_allow -= 1\n                                if out is None:\n                                    break\n                                ftry, xtry = out\n                                if ftry < f_best - 1e-12:\n                                    improved_here = True\n                                    centers[cidx] = xtry.copy()\n                                    trust_radius[cidx] = np.minimum(tr * 1.2, self.trust_max_frac * rng_range)\n                                    stagnation[cidx] = 0\n                                    break\n                            if improved_here:\n                                break\n\n                    # 6) occasional Cauchy jump (probability slightly decays with progress)\n                    if (not improved_here) and (local_allow > 0) and (evals < budget):\n                        prob = self.cauchy_prob * (1.0 - float(evals) / float(max(1, budget)))\n                        if self.rng.rand() < prob:\n                            scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                            jump = self.rng.standard_cauchy(size=n)\n                            jump = np.clip(jump, -10, 10)\n                            x_jump = np.clip(centers[cidx] + jump * scale, lb, ub)\n                            out = safe_eval(x_jump)\n                            local_allow -= 1\n                            if out is None:\n                                break\n                            fjump, xjump = out\n                            if fjump < f_best - 1e-12:\n                                improved_here = True\n                                centers[cidx] = xjump.copy()\n                                trust_radius[cidx] = np.minimum(tr * 2.0, self.trust_max_frac * rng_range)\n                                stagnation[cidx] = 0\n                            else:\n                                trust_radius[cidx] = np.maximum(tr * 0.8, self.trust_min * rng_range)\n\n                else:\n                    # not enough data: exploratory random directional probes\n                    base_step = np.linalg.norm(trust_radius[0]) / np.sqrt(float(n) + 1e-12)\n                    for _ in range(min(6, local_allow)):\n                        if evals >= budget:\n                            break\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        s = self.rng.choice(self.direction_scales)\n                        x_try = np.clip(center + d * s * base_step, lb, ub)\n                        out = safe_eval(x_try)\n                        local_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_here = True\n                            centers[cidx] = xtry.copy()\n                            trust_radius[cidx] = np.minimum(trust_radius[cidx] * 1.2, self.trust_max_frac * rng_range)\n                            stagnation[cidx] = 0\n                            break\n\n                # update stagnation/trust if no improvement for this center\n                if not improved_here:\n                    stagnation[cidx] += 1\n                    trust_radius[cidx] = np.maximum(trust_radius[cidx] * self.failure_shrink, self.trust_min * rng_range)\n                else:\n                    # slight reduction of stagnation for others to encourage cross-benefits\n                    for j in range(len(stagnation)):\n                        if j != cidx and stagnation[j] > 0:\n                            stagnation[j] = max(0, stagnation[j] - 1)\n\n                # replace center if stagnated long\n                if stagnation[cidx] >= self.center_replace_patience and len(X) > 0:\n                    X_arr = np.asarray(X)\n                    # distance to nearest center\n                    d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                    # choose among top distant candidates\n                    cand_idx = np.argsort(d_to_centers)[-max(1, min(20, len(X_arr))):]\n                    pick = int(self.rng.choice(cand_idx))\n                    centers[cidx] = X_arr[pick].copy()\n                    trust_radius[cidx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                    stagnation[cidx] = 0\n\n            # end per-center allocations\n\n            # prune archive to keep memory manageable\n            max_archive = max(2000, 50 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasional nudge some centers to global best\n            if x_best is not None and len(centers) > 0:\n                for i in range(len(centers)):\n                    if self.rng.rand() < 0.1 and np.linalg.norm(centers[i] - x_best) > 1e-9:\n                        centers[i] = x_best.copy()\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 999, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: probabilities do not sum to 1", "error": "In the code, line 999, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: probabilities do not sum to 1", "parent_ids": "e9f3e383-6b8c-4beb-9ac3-012fb83c40e5", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "c8a75e5b-c80f-4b25-84e9-f5f3d1a2d26d", "fitness": "-inf", "name": "ARDE_SS", "description": "ARDE_SS is a hybrid CMA-ES/DE-style optimizer: it maintains a small population size scaling with log(dim), uses Lehmer-like recombination weights, evolution paths (ps, pc), covariance matrix C with periodic eigendecomposition (frequency ∝ dim), and median-success sigma adaptation (target_s ≈ 0.2, damped) with an initial sigma set to 0.28·mean(range). It augments the backbone with archive-driven exploration: differential mutations sampled from archive differences, occasional Mantegna-like Lévy jumps for long-range moves, residual-guided multiscale directional probes and coordinate tweaks to escape stagnation and exploit local structure. It fits cheap random-subspace quadratic surrogates (subspace size = min(subspace_k_max, ceil(subspace_frac·dim))) on nearby archive points with proximity-weighted ridge regression, regularizes the subspace Hessian, solves for an analytic minimizer and trusts it only within a sigma-scaled radius before evaluating. Practical budget- and robustness-focused choices include strict bound clipping, a global eval counter (never exceeding budget), archive pruning (keep best + diverse tail), many numerical fallbacks (regularization, pinv, try/except) and tuned parameters (eig_every_factor, model_neighbor_multiplier, small ridge) to handle Many-Affine BBOB noiseless problems.", "code": "import numpy as np\n\nclass ARDE_SS:\n    \"\"\"\n    ARDE_SS (Adaptive Rotational Differential Evolution with Subspace Surrogates)\n\n    One-line: Hybrid CMA-ES / DE backbone with Lehmer-like recombination and median-success sigma adaptation,\n    augmented by cheap random-subspace quadratic surrogates (k <= 8) fitted on nearby archive points to\n    propose analytic subspace minimizers, plus residual-guided probes, coordinate tweaks and occasional\n    Lévy/DE jumps to escape basins. Budget-safe and bound-aware for Many-Affine BBOB tasks.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 p_de=0.22, p_levy=0.06, levy_beta=1.5,\n                 subspace_frac=0.33, subspace_k_max=8,\n                 model_neighbor_multiplier=6,\n                 eig_every_factor=5):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.p_de = float(p_de)\n        self.p_levy = float(p_levy)\n        self.levy_beta = float(levy_beta)\n        self.subspace_frac = float(subspace_frac)\n        self.subspace_k_max = int(subspace_k_max)\n        self.model_neighbor_multiplier = int(model_neighbor_multiplier)\n        self.eigen_every_factor = int(eig_every_factor)\n\n        # population sizing similar to ARDE_Enhanced but modest\n        self.lambda_ = max(6, int(6 + 4 * np.log(max(2, self.dim))))\n        self.mu = max(2, self.lambda_ // 2)\n\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _levy_step(self, n, beta):\n        # Mantegna-like symmetric Levy step\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = self.rng.normal(0, sigma_u, size=n)\n        v = self.rng.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1 / beta))\n        return step\n\n    def __call__(self, func):\n        n = int(self.dim)\n        budget = int(self.budget)\n\n        # bounds (Many Affine BBOB: [-5,5] but respect func)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n\n        # Lehmer-like weights (skewed) for recombination\n        mu = self.mu\n        raw = (np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))) ** 1.1\n        weights = raw / np.sum(raw)\n        mu_eff = (np.sum(weights) ** 2) / np.sum(weights ** 2)\n\n        # adaptation parameters\n        cc = 0.7 / np.sqrt(n + mu_eff)\n        cs = 0.3 / (1.0 + mu_eff / (n + 1.0))\n        c1 = 1.2 / (((n + 1.5) ** 1.6) + 0.5 * mu_eff)\n        cmu = min(0.85 * (1 - c1), 0.5 * mu_eff / (mu_eff + 2.0) * (1 - c1))\n        damps = 1.0 + cs + 2.0 * max(0.0, np.sqrt(mu_eff / (n + 1.0)) - 1.0)\n\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize dynamic state\n        m = self.rng.uniform(lb, ub)\n        sigma = 0.28 * np.mean(rng_range)\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eigen_eval_counter = 0\n        eig_every = max(1, int(self.eigen_every_factor * n))\n\n        # archive for DE differences and surrogate neighbors\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # safe eval helper\n        def safe_eval(x):\n            nonlocal evals, budget, archive_X, archive_F, f_opt, x_opt\n            if evals >= budget:\n                return None\n            x_c = np.clip(np.asarray(x, dtype=float), lb, ub)\n            f = func(x_c)\n            evals += 1\n            archive_X.append(x_c.copy())\n            archive_F.append(float(f))\n            if f < f_opt:\n                f_opt = float(f)\n                x_opt = x_c.copy()\n            return float(f), x_c\n\n        # initial evaluation of mean\n        if evals < budget:\n            out = safe_eval(np.clip(m, lb, ub))\n            if out is not None:\n                f_parent = out[0]\n            else:\n                f_parent = np.inf\n        else:\n            f_parent = np.inf\n\n        # main loop: generate generations until budget exhausted\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(self.lambda_, remaining)\n\n            # sample from N(0,C) via B*D\n            arz = self.rng.standard_normal(size=(current_lambda, n))\n            BD = B * D[np.newaxis, :]\n            ary = arz @ BD.T\n            arx = m + sigma * ary\n\n            # Adaptive DE factor that decays (favor exploration early)\n            F_de = 0.6 + 0.4 * (1.0 - evals / max(1, budget))\n\n            # Apply DE and Lévy perturbations per-offspring\n            for k in range(current_lambda):\n                if (self.rng.random() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    arx[k] = arx[k] + de_mut\n                if self.rng.random() < self.p_levy:\n                    levy = self._levy_step(n, self.levy_beta)\n                    arx[k] = arx[k] + 0.5 * sigma * levy\n                arx[k] = np.clip(arx[k], lb, ub)\n\n            # Evaluate offspring (one by one), keep arfit\n            arfit = np.full(current_lambda, np.inf)\n            for k in range(current_lambda):\n                if evals >= budget:\n                    break\n                out = safe_eval(arx[k])\n                if out is None:\n                    break\n                arfit[k] = out[0]\n\n            # If no evaluations, break\n            if np.sum(np.isfinite(arfit)) == 0:\n                break\n\n            # selection (take best sel_mu)\n            sel_mu = min(mu, int(np.sum(np.isfinite(arfit))))\n            idx = np.argsort(arfit)[:sel_mu]\n            x_sel = arx[idx]\n            y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n\n            # compute success rate relative to parent fitness\n            if np.isfinite(f_parent):\n                successes = np.sum(arfit < f_parent)\n                s_rate = successes / max(1, current_lambda)\n            else:\n                finite = arfit[np.isfinite(arfit)]\n                median_f = np.median(finite) if finite.size > 0 else np.inf\n                s_rate = np.sum(finite < median_f) / max(1, finite.size)\n\n            # recombine to new mean (weights subset if necessary)\n            if sel_mu < mu:\n                w_sub = weights[:sel_mu]\n                w_sub = w_sub / np.sum(w_sub)\n                m_old = m.copy()\n                m = np.sum(w_sub[:, np.newaxis] * x_sel, axis=0)\n                y_w = np.sum(w_sub[:, np.newaxis] * y_sel, axis=0)\n            else:\n                m_old = m.copy()\n                m = np.sum(weights[:, np.newaxis] * x_sel, axis=0)\n                y_w = np.sum(weights[:, np.newaxis] * y_sel, axis=0)\n\n            # evolution path updates\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n            norm_ps = np.linalg.norm(ps)\n\n            # success-driven hsig\n            hsig = 1.0 if (s_rate > 0.15) else 0.0\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n\n            # rank-one and rank-mu updates\n            rank_one = np.outer(pc, pc)\n            rank_mu = np.zeros((n, n))\n            for i in range(sel_mu):\n                yi = y_sel[i][:, None]\n                rank_mu += ( (w_sub[i] if sel_mu < mu else weights[i]) * (yi @ yi.T) )\n\n            C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n\n            # median-success / target-success sigma adaptation\n            target_s = 0.2\n            adapt_strength = 0.6 / (1.0 + 0.05 * n)\n            sigma *= np.exp(adapt_strength * (s_rate - target_s) / (damps + 1e-20))\n            sigma = max(sigma, 1e-12)\n\n            # Optionally perform a cheap random-subspace quadratic surrogate fit around x_opt / m\n            # Only if we have enough archive points\n            k_sub = int(min(self.subspace_k_max, max(1, int(np.ceil(self.subspace_frac * n)))))\n            neighbors_needed = max(4 * k_sub + 6, self.model_neighbor_multiplier * n)\n            if len(archive_X) >= neighbors_needed and evals < budget:\n                # pick neighbors near the current best (x_opt if available, else m)\n                anchor = x_opt.copy() if x_opt is not None else m.copy()\n                X_arr = np.asarray(archive_X)\n                F_arr = np.asarray(archive_F)\n                dists = np.linalg.norm(X_arr - anchor, axis=1)\n                idx_sorted = np.argsort(dists)[:min(len(X_arr), neighbors_needed)]\n                X_nei = X_arr[idx_sorted]\n                F_nei = F_arr[idx_sorted]\n                # build a random orthonormal subspace basis Q (n x k_sub)\n                A = self.rng.normal(size=(n, k_sub))\n                # QR orthonormalization\n                Q, _ = np.linalg.qr(A, mode='reduced')\n                Z = (X_nei - anchor) @ Q  # neighbor coords in subspace\n                m_samples = Z.shape[0]\n                # build design matrix phi for quadratic (1 + k + k diag + cross)\n                cross_count = k_sub * (k_sub - 1) // 2\n                p = 1 + k_sub + k_sub + cross_count\n                Phi = np.ones((m_samples, p))\n                Phi[:, 1:1 + k_sub] = Z\n                Phi[:, 1 + k_sub:1 + 2 * k_sub] = 0.5 * (Z ** 2)\n                col = 1 + 2 * k_sub\n                for i in range(k_sub):\n                    for j in range(i + 1, k_sub):\n                        Phi[:, col] = Z[:, i] * Z[:, j]\n                        col += 1\n                y = F_nei\n                # proximity weights (in original space)\n                prox = 1.0 / (dists[idx_sorted] + 1e-12)\n                prox = prox / (np.max(prox) + 1e-12)\n                W = np.sqrt(prox)[:, None]\n                A_mat = W * Phi\n                b_vec = W * y\n                ridge = 1e-6 + 1e-4 * (1.0 / max(1, m_samples))\n                try:\n                    lhs = A_mat.T @ A_mat + ridge * np.eye(p)\n                    rhs = A_mat.T @ b_vec\n                    params = np.linalg.solve(lhs, rhs).flatten()\n                    a0 = params[0]\n                    g_lin = params[1:1 + k_sub]\n                    h_diag = params[1 + k_sub:1 + 2 * k_sub]\n                    # reconstruct Hessian in subspace\n                    H = np.zeros((k_sub, k_sub))\n                    np.fill_diagonal(H, h_diag)\n                    col = 1 + 2 * k_sub\n                    for i in range(k_sub):\n                        for j in range(i + 1, k_sub):\n                            c = params[col]; col += 1\n                            H[i, j] = c; H[j, i] = c\n                    H = 0.5 * (H + H.T)\n                    # regularize Hessian to PD for minimizer solve\n                    try:\n                        eigvals = np.linalg.eigvalsh(H)\n                        min_eig = np.min(eigvals)\n                    except Exception:\n                        min_eig = -1.0\n                    if min_eig <= 1e-8:\n                        shift = (1e-8 - min_eig) + 1e-6\n                        H_reg = H + shift * np.eye(k_sub)\n                    else:\n                        H_reg = H.copy()\n                    # solve for z*: H_reg * z* = -g_lin\n                    try:\n                        z_star = -np.linalg.solve(H_reg, g_lin)\n                    except np.linalg.LinAlgError:\n                        z_star = -np.linalg.pinv(H_reg) @ g_lin\n                    # clip step by trust radius equivalent -> use sigma-scaled trust\n                    step_vec = Q @ z_star\n                    step_norm = np.linalg.norm(step_vec)\n                    trust_radius = max(1e-12, 2.0 * sigma * np.linalg.norm(rng_range) / np.sqrt(n + 1e-12))\n                    if step_norm > 0:\n                        scale = min(1.0, trust_radius / (step_norm + 1e-16))\n                    else:\n                        scale = 1.0\n                    z_limited = z_star * scale\n                    x_model = np.clip(anchor + Q @ z_limited, lb, ub)\n                    # evaluate surrogate minimizer candidate\n                    if evals < budget:\n                        out = safe_eval(x_model)\n                        if out is not None:\n                            # if improved, slightly inflate sigma to exploit\n                            if out[0] < f_parent - 1e-12:\n                                sigma = min(sigma * 1.5, 10.0 * sigma)\n                                # update parent fitness baseline\n                                f_parent = out[0]\n                            else:\n                                # modest shrink if not successful\n                                sigma = max(sigma * 0.85, 1e-12)\n                except Exception:\n                    # skip surrogate on any numerical issue\n                    pass\n\n            # residual-guided multiscale probes around anchor (x_opt or m)\n            if len(archive_X) >= 6 and evals < budget:\n                anchor = x_opt.copy() if x_opt is not None else m.copy()\n                X_arr = np.asarray(archive_X); F_arr = np.asarray(archive_F)\n                dists = np.linalg.norm(X_arr - anchor, axis=1)\n                idx_local = np.argsort(dists)[:min(len(X_arr), max(8, 6 * n // 5))]\n                localX = X_arr[idx_local]; localF = F_arr[idx_local]\n                medf = np.median(localF)\n                res = np.abs(localF - medf) + 1e-12\n                weights_res = res / np.sum(res)\n                dir_vec = np.sum(((localX - anchor).T * weights_res).T, axis=0)\n                if np.linalg.norm(dir_vec) > 1e-12:\n                    dir_unit = dir_vec / np.linalg.norm(dir_vec)\n                    for s in (0.3, 0.8, 1.4):\n                        if evals >= budget:\n                            break\n                        x_try = np.clip(anchor + dir_unit * (s * sigma), lb, ub)\n                        out = safe_eval(x_try)\n                        if out is None:\n                            break\n\n            # periodic eigen-decomposition to update B, D, invsqrtC\n            eigen_eval_counter += current_lambda\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # occasionally perform a small coordinate pattern search around x_opt if stagnation likely\n            if evals < budget and len(archive_F) >= max(20, n) and (np.mean(archive_F[-min(len(archive_F), 50):]) - f_opt) < 1e-12:\n                # try a few coordinate tweaks\n                anchor = x_opt.copy() if x_opt is not None else m.copy()\n                coords = np.arange(n)\n                self.rng.shuffle(coords)\n                for i in coords[:min(n, 4)]:\n                    if evals >= budget:\n                        break\n                    for sgn in (+1.0, -1.0):\n                        step = sgn * 0.5 * sigma * max(1e-3, rng_range[i])\n                        x_try = np.clip(anchor.copy(), lb, ub)\n                        x_try[i] = np.clip(x_try[i] + step, lb[i], ub[i])\n                        out = safe_eval(x_try)\n                        if out is None:\n                            break\n\n            # Update f_parent baseline if mean changed meaningfully\n            if evals < budget and np.linalg.norm(m - m_old) > 1e-12:\n                out = safe_eval(np.clip(m, lb, ub))\n                if out is not None:\n                    f_parent = out[0]\n\n            # small archive pruning for memory control: keep best + diverse tail\n            max_archive = max(2000, 50 * n)\n            if len(archive_X) > max_archive:\n                Xarr = np.asarray(archive_X); Farr = np.asarray(archive_F)\n                idx_sorted = np.argsort(Farr)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                # subsample rest to fill\n                step = max(1, len(rest_idx) // (max_archive - 200))\n                keep_rest = rest_idx[::step]\n                keep_idx = np.concatenate([keep_best, keep_rest])[:max_archive]\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n        # final fallback if nothing evaluated (shouldn't happen)\n        if x_opt is None:\n            x_opt = np.clip(self.rng.uniform(lb, ub), lb, ub)\n            f_opt = func(x_opt)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "e9f3e383-6b8c-4beb-9ac3-012fb83c40e5", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d0f1f6fe-5082-456c-a464-0a4e9d995953", "fitness": 0.2000064985028558, "name": "ASTRE", "description": "The algorithm mixes multiple mutation modes (isotropic, coordinate-wise scaled by per-coordinate RMS and a learned low-dimensional subspace) with occasional differential-archive steps and Lévy/Cauchy jumps for heavy-tail exploration; sampling proportions and hyperparameters are biased toward subspace moves (p_subspace=0.5, p_isotropic=0.3, p_coord=0.2, p_de≈0.18, p_levy≈0.08) and the trust-region radius R is initialized to 0.25 of the domain range. Population size is modest and budget-aware (lambda = max(6, 6+4 log(dim)), mu = lambda//3) and recombination uses a temperature-annealed softmax on negative ranks (temp from 1.2→0.2, logits scaled by temp*sqrt(n)) to form a weighted mean shift. Adaptation combines an exponentially-weighted buffer PCA to learn a k-dimensional basis (k≈sqrt(dim)/1.2) and singular-value scales, a small L-BFGS-like secant memory for directional curvature, and per-coordinate RMSProp-style scaling (beta≈0.15) to adapt sensitivities. Trust-region and mean updates are driven by a predicted improvement from a linear fit on the buffer vs actual improvement (rho = actual/predicted) to expand/shrink R and accept or partially apply the recombined mean, with stagnation handling (inflate R, nudge to archive point, clear buffers) and strict bound clipping throughout.", "code": "import numpy as np\n\nclass ASTRE:\n    \"\"\"\n    Adaptive Subspace-Trust-Region Evolution (ASTRE)\n    - Dual-mode mixing: isotropic, coordinate-wise, and learned subspace directions.\n    - Per-coordinate RMS scaling (like RMSProp) to adapt sensitivities.\n    - Incremental subspace learned from recent successful steps (weighted PCA).\n    - Small L-BFGS-like secant memory to estimate directional curvature and damp steps.\n    - Trust-region radius R adapts by an improvement ratio (actual vs predicted).\n    - Candidates recombined by a temperature-controlled softmax of fitness (budget-aware).\n    - Mutations include differential-archive differences and occasional Lévy/Cauchy jumps.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        # population size heuristic (different from standard CMA)\n        self.lambda_ = max(6, int(6 + np.floor(4.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 3)\n        # subspace dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.clip(np.ceil(self.dim ** 0.5 / 1.2), 1, self.dim)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # secant memory (L-BFGS-like pairs)\n        self.secant_m = min(8, 2 * self.k + 2)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (support scalar bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initialization\n        m = np.random.uniform(lb, ub)  # center\n        range_mean = np.mean(ub - lb)\n        R = 0.25 * range_mean  # trust-region radius (plays role of sigma)\n        R_min = 1e-8\n        R_max = 0.5 * np.max(ub - lb) + 1e-12\n\n        # per-coordinate RMS scale (s) and its EMA of squared steps\n        s = np.ones(n)\n        s2_ema = np.ones(n)\n\n        # learned subspace basis B (n x k) and eigen-scales lambda_sub (k)\n        # initialize with random orthonormal basis\n        if self.k > 0:\n            rand = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand)\n                B = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                B = np.zeros((n, self.k))\n        else:\n            B = np.zeros((n, 0))\n        lambda_sub = np.ones(self.k)\n\n        # success buffer for subspace PCA (store recent steps y = (x-m)/R)\n        buffer = []\n        buffer_max = max(8 * self.k, 20)\n\n        # archive for DE and restarts\n        archive_X = []\n        archive_F = []\n\n        # secant memory lists\n        secant_dx = []\n        secant_df = []\n\n        # control schedules\n        temp_init = 1.2  # initial temperature for softmax recombination\n        temp_final = 0.2\n        t_eval = 0\n\n        # mixing probabilities for sampling components\n        p_isotropic = 0.30\n        p_subspace = 0.50\n        p_coordinate = 0.20\n\n        # mutation probs\n        p_de = 0.18\n        p_levy = 0.08\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(float(fm))\n            f_opt = float(fm)\n            x_opt = xm.copy()\n\n        # helper: approximate gradient via finite differences from buffer (cheap linear fit)\n        def approx_grad_from_buffer(buf, fbuf):\n            # fit linear model f ~ a + g^T y using recent pairs of (y, f)\n            if len(buf) < 3:\n                return np.zeros(n)\n            Y = np.vstack(buf)  # m x n (y = x - m scaled later)\n            # center Ys and fs\n            Yc = Y - np.mean(Y, axis=0, keepdims=True)\n            fc = np.array(fbuf) - np.mean(fbuf)\n            # regularized least squares to get gradient estimate in x-space\n            # we fit delta_f ≈ g^T delta_x   => g in R^n\n            # Yc here are steps in real space, so gradient estimate:\n            reg = 1e-6 + 1e-2 * np.mean(np.var(Yc, axis=0))\n            try:\n                # solve (Yc^T Yc + reg I) g = Yc^T fc\n                A = Yc.T @ Yc + reg * np.eye(n)\n                b = Yc.T @ fc\n                g = np.linalg.solve(A, b)\n                return g\n            except np.linalg.LinAlgError:\n                return np.zeros(n)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            # dynamic population with remaining budget awareness\n            lam = min(self.lambda_, max(2, remaining))\n            # temperature annealing (linearly by evals)\n            t_frac = evals / max(1, budget)\n            temp = temp_init * (1.0 - t_frac) + temp_final * t_frac\n\n            # generate candidate population\n            arx = np.zeros((lam, n))\n            armut = []  # mutation metadata (for predicted improvement)\n            for i in range(lam):\n                # sample mixture mode\n                mode = np.random.rand()\n                if mode < p_isotropic:\n                    # isotropic Gaussian\n                    z = np.random.randn(n)\n                    step = (R * z) / np.sqrt(n)  # isotropic scaled by R\n                    mut_mode = 'iso'\n                elif mode < p_isotropic + p_subspace and self.k > 0:\n                    # subspace directional sampling: linear combination of basis with random coeffs\n                    zsub = np.random.randn(self.k)\n                    sub = (B @ (np.sqrt(lambda_sub + 1e-12) * zsub))\n                    # scale to appropriate magnitude relative to coordinate scale\n                    step = R * (0.7 * sub + 0.3 * (np.random.randn(n) * (s / (np.mean(s) + 1e-12))))\n                    mut_mode = 'sub'\n                else:\n                    # coordinate-wise scaled gaussian\n                    z = np.random.randn(n)\n                    step = R * z * (s / (np.mean(s) + 1e-12))\n                    mut_mode = 'coord'\n\n                # occasional Levy/Cauchy jump for heavy tails\n                if np.random.rand() < p_levy:\n                    c = np.random.standard_cauchy()\n                    # directional heavy jump using normalized step\n                    nrm = np.linalg.norm(step) + 1e-12\n                    step = (np.sign(c) * (np.abs(c) ** 0.7)) * (step / nrm) * (R * 4.0)\n                    mut_mode += '+levy'\n\n                x = m + step\n\n                # occasional DE-style archive difference\n                if np.random.rand() < p_de and len(archive_X) >= 2:\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    diff = archive_X[i1] - archive_X[i2]\n                    x = x + 0.6 * diff  # scale DE step modestly\n                    mut_mode += '+de'\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n                arx[i] = x\n                armut.append((step.copy(), mut_mode))\n\n            # evaluate sequentially until budget exhausted\n            arfit = np.full(lam, np.inf, dtype=float)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                fx = float(func(x))\n                evals += 1\n                arfit[i] = fx\n                archive_X.append(x.copy())\n                archive_F.append(fx)\n                if fx < f_opt:\n                    f_opt = fx\n                    x_opt = x.copy()\n\n            # selection: softmax-weighted recombination based on negative ranks & temperature\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            # compute ranks and softmax weights (prefer lower f)\n            sorted_idx = valid_idx[np.argsort(arfit[valid_idx])]\n            ranks = np.arange(1, len(sorted_idx) + 1)\n            # softmax on negative ranks scaled by temperature -> higher weight to top individuals\n            logits = - (ranks - 1) / max(1e-12, temp * np.sqrt(n))\n            exps = np.exp(logits - np.max(logits))\n            soft_w = exps / np.sum(exps)\n            # keep top mu candidates but allow soft weights\n            take = min(self.mu, len(sorted_idx))\n            chosen = sorted_idx[:take]\n            weights = soft_w[:take]  # already sum to <1 generally, normalize\n            if weights.sum() <= 0:\n                weights = np.ones_like(weights) / len(weights)\n            else:\n                weights = weights / np.sum(weights)\n\n            x_sel = arx[chosen]\n            f_sel = arfit[chosen]\n            steps = np.array([armut[j][0] for j in chosen])  # steps = x - m\n            # recombined mean shift\n            delta_m = np.sum(weights[:, None] * steps, axis=0)\n            m_new = m + delta_m\n\n            # predict improvement using a tiny linear model from recent buffer/fitness (in x-space)\n            # construct small local buffer of (x - m, f)\n            local_buf_len = min(len(buffer), 12)\n            buf_y = [b for b in buffer[-local_buf_len:]]\n            # convert buffer stored as (x - prev_m)/prev_R maybe - but we stored raw steps, so treat as steps\n            # To get f buffer associated, take most recent archive_F entries with corresponding X near those steps:\n            # approximate by using last |buf| evaluations in archive\n            if len(buf_y) > 0 and len(archive_F) >= len(buf_y):\n                fbuf_local = archive_F[-len(buf_y):]\n            else:\n                fbuf_local = None\n            grad_hat = approx_grad_from_buffer(buf_y, fbuf_local if fbuf_local is not None else [])\n            predicted_improv = -np.dot(grad_hat, delta_m)  # predicted decrease (positive is good)\n\n            # actual improvement\n            f_m = archive_F[-1] if len(archive_F) > 0 else f_opt\n            actual_improv = max(0.0, (f_m - np.mean(f_sel))) if len(f_sel) > 0 else 0.0\n\n            # trust-region update using ratio rho = actual / (predicted + eps)\n            eps = 1e-8\n            rho = actual_improv / (abs(predicted_improv) + eps)\n            # update rule:\n            if predicted_improv > 1e-12 and rho > 0.8:\n                # very successful: enlarge region moderately\n                R = min(R * (1.2 + 0.8 * (rho - 0.8)), R_max)\n            elif rho > 0.4:\n                # successful\n                R = min(R * 1.05, R_max)\n            elif rho < 0.1:\n                # unsuccessful: shrink (conservative)\n                R = max(R * 0.7, R_min)\n            else:\n                # slight decay\n                R = max(R * 0.95, R_min)\n\n            # apply secant memory update using movement and mean-of-f_sel\n            # compute representative step and f change\n            if len(f_sel) > 0:\n                rep_step = delta_m.copy()\n                rep_df = np.mean(f_sel) - (archive_F[-1] if len(archive_F) > 0 else f_opt)\n                # store in secant memory (limited)\n                if len(secant_dx) >= self.secant_m:\n                    secant_dx.pop(0); secant_df.pop(0)\n                secant_dx.append(rep_step.copy())\n                secant_df.append(rep_df)\n\n            # update per-coordinate RMS s using selected steps (RMSProp style)\n            # use weighted second moments of chosen steps (in coordinate space)\n            if len(steps) > 0:\n                y2 = np.sum(weights[:, None] * (steps ** 2), axis=0)\n                beta = 0.15\n                s2_ema = (1.0 - beta) * s2_ema + beta * (y2 + 1e-12)\n                s = np.sqrt(s2_ema + 1e-12)\n                # prevent collapse\n                s = np.clip(s, 1e-6, 1e6)\n\n            # store successful (positive actual improvement) steps into buffer for subspace learning\n            if actual_improv > 1e-12 and len(steps) > 0:\n                # store steps normalized by R to be scale-invariant\n                avg_step = delta_m.copy() / (R + 1e-12)\n                buffer.append(avg_step.copy())\n                if len(buffer) > buffer_max:\n                    buffer.pop(0)\n\n            # update subspace B and lambda_sub using weighted PCA on buffer (exponentially weighted)\n            if len(buffer) >= max(3, self.k) and (evals % max(1, min(10, n)) == 0):\n                # assemble matrix M where rows are steps (m x n)\n                M = np.vstack(buffer)  # m x n\n                # weight recent more strongly\n                mcnt = M.shape[0]\n                weights_buf = np.exp(-np.linspace(0, 1.0, mcnt) * 2.5)  # decreasing weights\n                weights_buf = weights_buf / np.sum(weights_buf)\n                # compute weighted covariance in the step space\n                M_centered = M - np.average(M, axis=0, weights=weights_buf)\n                # form sqrt-weighted matrix for SVD (m x n)\n                Mw = (M_centered.T * np.sqrt(weights_buf)).T  # m x n\n                try:\n                    # compute top-k SVD on Mw (small m often)\n                    U_s, svals, Vt = np.linalg.svd(Mw, full_matrices=False)\n                    # Vt is n x n' where first rows are principal directions in coordinate space\n                    # take top k columns of Vt.T as basis vectors\n                    k_take = min(self.k, Vt.shape[0])\n                    if k_take > 0:\n                        B_new = Vt[:k_take].T  # n x k_take\n                        # pad/truncate to self.k\n                        if B_new.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - B_new.shape[1]))\n                            B = np.hstack([B_new, pad])\n                            # scales from singular values normalized by sqrt(mcnt)\n                            svals_norm = svals[:B_new.shape[1]] / (np.sqrt(max(1, mcnt)) + 1e-12)\n                            lambda_sub = np.concatenate([svals_norm, np.ones(self.k - len(svals_norm))])\n                        else:\n                            B = B_new[:, :self.k]\n                            svals_norm = svals[:self.k] / (np.sqrt(max(1, mcnt)) + 1e-12)\n                            lambda_sub = svals_norm.copy()\n                except np.linalg.LinAlgError:\n                    pass\n\n            # modest mean update: accept new mean only if it doesn't degrade too much,\n            # otherwise move partially towards recombined mean (trusting ratio)\n            # compute mean fitness of chosen vs current mean (last eval of mean stored earlier)\n            mean_fitness_selected = np.mean(f_sel) if len(f_sel) > 0 else f_opt\n            current_mean_f = archive_F[-1] if len(archive_F) > 0 else f_opt\n            # acceptance rule\n            if mean_fitness_selected <= current_mean_f or rho > 0.3:\n                m = m_new.copy()\n            else:\n                # partial move: weighted by rho\n                alpha = max(0.05, min(0.6, rho))\n                m = (1.0 - alpha) * m + alpha * m_new\n\n            # record successful step into buffer even if not large improvement for diversity\n            if len(steps) > 0 and np.random.rand() < 0.15:\n                simple_step = delta_m.copy() / (R + 1e-12)\n                buffer.append(simple_step)\n                if len(buffer) > buffer_max:\n                    buffer.pop(0)\n\n            # stagnation detection: if no improvement for long window, trigger injection\n            worst_ages = 50 + 5 * n\n            if len(archive_F) > worst_ages:\n                # if best hasn't improved in last worst_ages evals\n                best_index = int(np.argmin(archive_F))\n                if evals - best_index > worst_ages:\n                    # inject: increase R, nudge mean to random archive point, clear buffers\n                    R = min(R * 2.0, R_max)\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.5 * m + 0.5 * archive_X[pick]\n                    buffer = []\n                    secant_dx = []; secant_df = []\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # shorten lambda in last iterations to ensure budget is respected next loop\n            # (the while condition will handle budget, but proactively reduce lambda a bit)\n            t_eval += lam\n\n            # safety clamp radius\n            R = float(np.clip(R, R_min, R_max))\n\n            # loop continues until budget exhausted\n\n        # final return\n        if x_opt is None:\n            # fallback: best in archive\n            if len(archive_X) > 0:\n                idx = np.argmin(archive_F)\n                x_opt = archive_X[idx].copy()\n                f_opt = float(archive_F[idx])\n            else:\n                x_opt = np.clip(m, lb, ub)\n                f_opt = float(func(x_opt)) if evals < budget else float(f_opt)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ASTRE scored 0.200 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "72f4ba5b-2276-4c3f-800f-269ed722d539", "operator": null, "metadata": {"aucs": [0.09516783933180994, 0.1641577704936723, 0.2633507051395003, 0.18640784293903734, 0.2164321817238205, 0.22233013097885912, 0.23442064265606388, 0.24030095098789384, 0.23145316650185266, 0.146043754276048]}, "task_prompt": ""}
{"id": "dcb0a239-de9d-4da9-bf67-e2c50cf57842", "fitness": 0.17376745263072974, "name": "FALRE", "description": "FALRE combines per-coordinate adaptive scaling (RMSProp-like EMA v→D with rms_rho=0.85 and small eps) and a learned low‑rank subspace U (k≈ceil(√n)) extracted from a success_buffer via SVD to mix diagonal and low‑rank search directions (alpha_mix=0.7 favors diagonal, S scales low‑rank contributions). It uses a small, budget‑frugal population (lambda heuristic), mirrored sampling, rank‑weighted recombination into a momentumed mean update (alpha_m=0.9, mom_beta=0.6), and a smooth success‑rate sigma controller targeting p_target=0.25 with asymmetric up/down gains; initial sigma is 0.18·range and sigma is clipped to sensible bounds. Exploration is augmented by occasional heavy‑tailed moves (Cauchy with p_cauchy=0.08, Levy-like jumps α=1.8), archive‑based DE‑style perturbations (p_de=0.18, de_F=0.6), and stagnation remedies (inflate sigma, mix mean with archive and reset buffers). Practical safeguards include bounds clipping, archive/buffer size limits, periodic orthonormalization/reconditioning of U, and conservative numerical damping for singular values and variances to keep the method stable under limited budgets.", "code": "import numpy as np\n\nclass FALRE:\n    \"\"\"\n    Factorized Adaptive Low-Rank Evolution (FALRE)\n\n    Key ideas:\n    - Per-coordinate adaptive scaling using an RMSProp-like EMA on squared steps.\n    - A learned low-rank subspace U (k ~ ceil(sqrt(n))) updated from a buffer of recent successful steps via SVD.\n    - Success-rate based step-size control (1/5-style but smooth), momentumed mean updates.\n    - Mirrored sampling supported, occasional Levy/Cauchy jumps for heavy-tailed exploration, archive-based simple DE-like perturbations.\n    - Budget-respecting evaluation loop; bounds handled via clipping.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None,\n                 mirrored=True, p_cauchy=0.08, p_de=0.18, de_F=0.6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size heuristic (different from original)\n        # slightly smaller lam to be more budget-frugal on medium dims\n        self.lambda_ = max(4, int(3 + np.floor(2.5 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace dimension defaults to ceil(sqrt(n)) but can be overridden\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # behavior knobs\n        self.mirrored = bool(mirrored)\n        self.p_cauchy = float(p_cauchy)\n        self.p_de = float(p_de)\n        self.de_F = float(de_F)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds handling (accept scalar or array)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n\n        # rank-based exponential weights (different from log-weights)\n        ranks = np.arange(1, mu + 1)\n        nu = max(1.0, mu / 2.0)\n        raw_w = np.exp(-ranks / nu)\n        weights = raw_w / np.sum(raw_w)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # step-size control parameters (success-rate feedback)\n        p_target = 0.25  # desired success rate\n        sigma_gain_up = 0.6  # sensitivity to exceed target\n        sigma_gain_down = 0.8  # sensitivity to be below target\n        sigma_min = 1e-12\n        sigma_max = 1e2 * np.max(ub - lb) + 1e-12\n\n        # per-coordinate RMSProp-like parameters (different from EMA in original)\n        rms_rho = 0.85\n        eps = 1e-12\n        rms_scale = 1.0  # global multiplier for D\n\n        # momentum for mean recombination\n        mom_beta = 0.6\n        momentum = np.zeros(n)\n\n        # initialize mean, sigma\n        m = np.random.uniform(lb, ub)\n        sigma = 0.18 * np.mean(ub - lb)  # different init coefficient\n\n        # diagonal \"learning rates\" via RMS of steps (D ~ sqrt(v))\n        v = np.ones(n)  # EMA of squared y components\n        D = np.sqrt(v + eps)\n\n        # low-rank subspace init\n        if self.k >= 1:\n            R = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(R)\n                U = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n        S = np.ones(self.k) * 0.6  # initial low-rank scales\n\n        # buffers & archive\n        success_buffer = []  # store recent successful y's (in normalized y-space)\n        buffer_max = max(12 * self.k, 30)\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of mean (counts towards budget)\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_opt = float(fm)\n            x_opt = xm.copy()\n\n        # counters / reconditioning schedule\n        recond_every = max(40, 6 * n)\n        recond_counter = 0\n\n        # main loop: produce batches but evaluate one-by-one to respect budget\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # ensure even number for mirrored sampling if desired\n            if self.mirrored and (current_lambda % 2 == 1):\n                if current_lambda > 1:\n                    current_lambda -= 1\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n\n            # generate population\n            idx = 0\n            while idx < current_lambda:\n                # draw independent diag and low-rank normals\n                z_diag = np.random.randn(n)\n                z_low = np.random.randn(self.k) if self.k > 0 else np.zeros(0)\n\n                # combine - mixing formula intentionally different:\n                # scale diagonal by D, low-rank by S and an adaptive mixing factor alpha_mix\n                alpha_mix = 0.7  # put more weight on diagonal, different constant\n                low_contrib = (U @ (S * z_low)) if self.k > 0 else 0.0\n                y = (1.0 - alpha_mix) * (D * z_diag) + alpha_mix * low_contrib\n\n                # occasional Cauchy heavy tail scaled by median D\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy() * 0.8\n                    # scale Cauchy step by median coordinate scale\n                    medianD = np.median(D)\n                    if np.isfinite(r):\n                        y = r * (z_diag / (np.linalg.norm(z_diag) + 1e-20)) * medianD\n\n                # form candidate and optionally apply DE-like archive difference perturbation\n                x = m + sigma * y\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    x = x + self.de_F * (archive_X[i1] - archive_X[i2])\n\n                x = np.clip(x, lb, ub)\n                arx[idx] = x\n                arz[idx] = y\n                idx += 1\n\n                # mirrored partner\n                if self.mirrored and idx < current_lambda:\n                    y2 = -y\n                    x2 = np.clip(m + sigma * y2, lb, ub)\n                    if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        x2 = x2 + self.de_F * (archive_X[i1] - archive_X[i2])\n                        x2 = np.clip(x2, lb, ub)\n                    arx[idx] = x2\n                    arz[idx] = y2\n                    idx += 1\n\n            # evaluate candidates sequentially\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > 8000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n\n            # selection among evaluated\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            idx_sorted = np.argsort(arfit[valid_idx])\n            sel_idx_rel = valid_idx[idx_sorted[:min(mu, len(idx_sorted))]]\n            sel_count = len(sel_idx_rel)\n            if sel_count == 0:\n                break\n\n            x_sel = arx[sel_idx_rel]\n            y_sel = arz[sel_idx_rel]\n            # recombination using the exponential weights defined earlier (weights length mu)\n            w = weights[:sel_count].copy()\n            w = w / np.sum(w)\n\n            # compute weighted recombined vector (x-space and y-space)\n            x_recomb = np.sum(w[:, None] * x_sel, axis=0)\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # momentum update on mean (different than classic CMA)\n            step = x_recomb - m\n            momentum = mom_beta * momentum + (1.0 - mom_beta) * step\n            m_old = m.copy()\n            alpha_m = 0.9  # stronger pull to recombined candidate\n            m = m + alpha_m * step + 0.6 * momentum\n            m = np.clip(m, lb, ub)\n\n            # compute success-rate relative to previous mean objective (use best in batch vs fm)\n            # approximate parent fitness as best known at m_old or last fm in archive\n            parent_f = archive_F[-1] if len(archive_F) > 0 else np.inf\n            # count offspring strictly better than parent_f\n            successes = np.sum(arfit[valid_idx] < parent_f)\n            success_rate = successes / float(len(valid_idx))\n\n            # adapt sigma using smoothed success-rate feedback (different equation)\n            if success_rate > p_target:\n                # multiplicative increase dependent on surplus\n                sigma *= (1.0 + sigma_gain_up * (success_rate - p_target))\n            else:\n                sigma *= np.exp(-sigma_gain_down * (p_target - success_rate) / max(1.0, np.sqrt(n)))\n            sigma = float(np.clip(sigma, sigma_min, sigma_max))\n\n            # update per-coordinate RMSProp-like accumulator using weighted squared y's\n            # Note: use y_sel (in normalized step-space) to estimate second moments\n            y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n            v = rms_rho * v + (1.0 - rms_rho) * (y2 + eps)\n            D = np.sqrt(v) * rms_scale\n            D = np.maximum(D, 1e-8)\n\n            # store successful weighted y (in normalized y-space) into success_buffer\n            # use only genuine improvements to the known parent to bias subspace learning\n            if successes > 0:\n                # compute weighted average of only those improved individuals\n                improved_idx = valid_idx[np.where(arfit[valid_idx] < parent_f)[0]]\n                if improved_idx.size > 0:\n                    # map to selection indices to get their y's or compute average of improved y's\n                    y_improved = np.mean(arz[improved_idx], axis=0)\n                    success_buffer.append(y_improved.copy())\n                    if len(success_buffer) > buffer_max:\n                        success_buffer.pop(0)\n\n            # occasionally add the global best step to buffer (small nudges)\n            if (np.random.rand() < 0.06) and (x_opt is not None):\n                success_buffer.append(((x_opt - m) / (sigma + 1e-20)).copy())\n                if len(success_buffer) > buffer_max:\n                    success_buffer.pop(0)\n\n            # update low-rank subspace from buffer but with different cadence\n            recond_counter += current_lambda\n            if (len(success_buffer) >= max(3, self.k)) and (recond_counter >= max(4, int(0.25 * n))):\n                recond_counter = 0\n                Y = np.vstack(success_buffer).T  # n x m\n                # center\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        # scale singulars differently: normalize by sqrt(num samples) and add damping\n                        S = (svals[:k_take] / (np.sqrt(max(1, Y.shape[1])) + 1e-12)) * 0.9\n                        # pad if necessary\n                        if U.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U.shape[1]))\n                            U = np.hstack([U, pad])\n                            S = np.concatenate([S, np.ones(self.k - len(S)) * 0.5])\n                except np.linalg.LinAlgError:\n                    # skip if SVD failed\n                    pass\n\n            # occasional orthonormalization of U to keep numerical stability\n            if self.k > 0 and (np.random.rand() < 0.08):\n                try:\n                    Q, _ = np.linalg.qr(U)\n                    U = Q[:, :self.k]\n                except np.linalg.LinAlgError:\n                    pass\n\n            # small Levy-like mutation to escape stagnation (different schedule)\n            if (np.random.rand() < 0.035):\n                # draw a Levy-like step from a symmetric Pareto-ish distribution\n                alpha = 1.8\n                u = np.random.rand(n)\n                levy = np.sign(np.random.randn(n)) * (1.0 / (u ** (1.0 / alpha) + 1e-20))\n                jump = 0.5 * np.median(D) * levy / (np.linalg.norm(levy) / np.sqrt(n) + 1e-20)\n                m = np.clip(m + jump, lb, ub)\n\n            # simple stagnation check: if no improvement for many evals, inflate sigma and mix with random archive members\n            if evals > 200:\n                best_idx = np.argmin(archive_F) if len(archive_F) > 0 else None\n                best_age = (evals - best_idx) if best_idx is not None else 0\n                if best_age > max(40, 8 * n):\n                    sigma *= 1.8\n                    # mix mean toward a randomly selected archive entry\n                    if len(archive_X) > 0:\n                        pick = np.random.randint(len(archive_X))\n                        m = 0.5 * m + 0.5 * archive_X[pick]\n                    # clear buffer for fresh subspace learning\n                    success_buffer = []\n                    v = np.ones(n)  # reset per-coordinate variances a bit\n\n            # ensure mean bounds\n            m = np.clip(m, lb, ub)\n\n            # ensure sigma bounds\n            sigma = float(np.clip(sigma, sigma_min, sigma_max))\n\n            # end loop condition handled by while\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm FALRE scored 0.174 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "72f4ba5b-2276-4c3f-800f-269ed722d539", "operator": null, "metadata": {"aucs": [0.09529010942776683, 0.11352426384604797, 0.22487675429155485, 0.14589113211856486, 0.16098412547654462, 0.24124463537900476, 0.2152889830010607, 0.1934250554807131, 0.21696865567552215, 0.13018081161051742]}, "task_prompt": ""}
{"id": "3d835d35-9b8a-4e75-927a-467ef4c25c4d", "fitness": 0.20401960035902894, "name": "ASTRO_DE", "description": "ASTRO_DE runs a trust-region style search (radius r initialized to 0.15 of the box and adapted by a recent success-rate heuristic) with momentum (mmt, beta_m=0.9) and per-coordinate adaptive scaling v (Adam-like second moment, b2=0.9) to handle anisotropy and preserve directional memory. It learns a low-rank search subspace U (k ≈ ceil(sqrt(n))) from recent successful normalized steps kept in a success_buffer and updates U/S via periodic thin SVD, using the subspace to generate efficient proposals alongside a small orthogonal complement. Candidate generation is a stochastic mixture (p_sub≈0.45 subspace proposals, p_levy≈0.06 occasional heavy-tailed Cauchy jumps, p_de≈0.18 archive-based DE differences, and diagonal preconditioned Gaussian steps), followed by weighted recombination of the top μ solutions to update the mean and update v/mmt; all proposals are clipped to bounds. An archive and adaptive population size control diversity, and occasional reconditioning builds an approximate inverse-square-root preconditioner from the diagonal plus low-rank model to improve sampling and escape stagnation.", "code": "import numpy as np\n\nclass ASTRO_DE:\n    \"\"\"\n    ASTRO-DE: Adaptive Subspace Trust-Region with Momentum and Differential Evolution.\n\n    Main ideas:\n    - Maintain a trust-region radius r (like sigma) that adapts by success-rate (target ~ 1/5).\n    - Per-coordinate adaptive scaling via an Adam-like second moment (v) to treat anisotropy.\n    - Keep a momentum vector (mmt) that biases search towards promising directions.\n    - Learn a low-rank subspace U (k ~ ceil(sqrt(n))) from recent successful steps (incremental PCA / SVD on buffer)\n      and use it to propose efficient directional perturbations.\n    - Candidate generation is a mixture: subspace-proposals, coordinate-wise proposals, occasional Levy (heavy-tailed) jumps,\n      and archive-based DE difference perturbations.\n    - Recombine top mu candidates to update the mean. Buffer-based subspace updates and periodic reconditioning of an approximate\n      preconditioner are used.\n    - All proposals are clipped to provided bounds and the evaluator is strictly limited by the budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population size heuristic; will be adapted slightly with remaining budget\n        self.base_lambda = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.base_lambda // 2)\n        # subspace dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n        self.verbose = verbose\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (BBOB typically -5..5); robust handling if given scalars\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # state\n        lam = self.base_lambda\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        eps = 1e-12\n\n        # initialize mean randomly in box\n        m = np.random.uniform(lb, ub)\n        # trust-region radius (relative to box size)\n        r = 0.15 * np.mean(ub - lb)\n        r_min = 1e-8\n        r_max = 2.0 * np.mean(ub - lb)  # limit\n        # momentum (directional memory)\n        mmt = np.zeros(n)\n        beta_m = 0.9  # momentum smoothing\n\n        # per-coordinate second moment like Adam (v -> used to scale coordinates)\n        v = np.ones(n) * 1e-2\n        b2 = 0.9\n        inv_scale = 1.0 / (np.sqrt(v) + 1e-12)\n\n        # low-rank subspace U (n x k) and singular magnitudes S (k), initialized random orthonormal\n        if self.k > 0:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand_mat)\n                U = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n            S = np.ones(self.k) * 0.5\n        else:\n            U = np.zeros((n, 0))\n            S = np.zeros(0)\n\n        # buffers and archive\n        success_buffer = []           # recent successful y (in normalized step space)\n        buffer_max = max(20, 10 * self.k, 50)\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # behavior probabilities\n        p_sub = 0.45       # sample from subspace\n        p_levy = 0.06      # heavy-tailed jump\n        p_de = 0.18        # differential evolution mutation\n        F_de = 0.6\n        levy_scale = 0.8\n\n        # success tracking for trust-region adaptation\n        recent_successes = []\n        success_window = 20\n        target_success_rate = 0.2  # desired fraction of improved offspring\n\n        # periodic reconditioning frequency\n        recond_every = max(40, 6 * n)\n        recond_counter = 0\n        approx_invsqrt = np.eye(n)  # approximate inv sqrt preconditioner for a y->z transform\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_opt = float(fm)\n            x_opt = xm.copy()\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            # adapt population size modestly based on remaining budget and dim\n            lam = max(2, min(self.base_lambda, int(np.ceil(self.base_lambda * (1.0 - 0.2 * (evals / max(1, budget)))))))\n            lam = min(lam, remaining)\n            if lam <= 0:\n                break\n\n            # ensure even number for mirrored pairing sometimes\n            if lam % 2 == 1 and lam > 1:\n                lam -= 1\n\n            candidates = np.zeros((lam, n))\n            proposals = np.zeros((lam, n))  # raw step y (m->x) scaled by r\n            # generate lam candidates\n            for i in range(lam):\n                # mixture of proposal mechanisms\n                rand = np.random.rand()\n                # base gaussian in scaled coordinate space\n                if (self.k > 0) and (rand < p_sub):\n                    # subspace sample: combine low-rank and small orth complement\n                    z_sub = np.random.randn(self.k)\n                    sub_part = (U @ (S * z_sub)) if self.k > 0 else 0.0\n                    # small complement noise\n                    comp_noise = 0.6 * (np.random.randn(n) * inv_scale)\n                    y = sub_part + comp_noise\n                    mode = \"sub\"\n                elif rand < p_sub + p_levy:\n                    # Levy-like heavy tail jump\n                    # Use Cauchy for heavy tail on a random direction\n                    dirn = np.random.randn(n)\n                    dirn = dirn / (np.linalg.norm(dirn) + eps)\n                    rlev = np.random.standard_cauchy() * levy_scale\n                    y = rlev * dirn\n                    mode = \"levy\"\n                else:\n                    # coordinate-wise scaled gaussian (preconditioned by inv_scale)\n                    y = np.random.randn(n) * inv_scale\n                    mode = \"diag\"\n\n                # incorporate momentum bias (helps exploit directionality)\n                y = y + 0.6 * mmt\n\n                # differential-evolution style archive difference occasionally\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_vec = archive_X[i1] - archive_X[i2]\n                    y = y + F_de * (de_vec / (np.linalg.norm(de_vec) + eps))\n\n                # scale by trust-region radius to create candidate\n                x = m + r * y\n                x = np.clip(x, lb, ub)\n                candidates[i] = x\n                proposals[i] = y\n\n            # evaluate candidates one-by-one to respect strict budget\n            fitness = np.full(lam, np.inf)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = candidates[i]\n                f = func(x)\n                evals += 1\n                fitness[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                # keep archive bounded to avoid memory explosion\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n\n            # selection among evaluated candidates\n            valid_idx = np.where(np.isfinite(fitness))[0]\n            if valid_idx.size == 0:\n                break\n            idx_sorted = valid_idx[np.argsort(fitness[valid_idx])]\n            sel_count = min(mu, len(idx_sorted))\n            sel_idx = idx_sorted[:sel_count]\n            x_sel = candidates[sel_idx]\n            y_sel = proposals[sel_idx]  # in normalized step-space (unitless)\n\n            # compute how many improved relative to current mean evaluation\n            # We compare to the archived best f_opt before generation; count improvements among evaluated that beat previous mean value at start of generation\n            improvements = 0\n            prev_best = f_opt  # best after evaluations so far (this includes improvements inside the batch though)\n            # To approximate success we count those that are strictly better than the best before starting batch:\n            # But we didn't store the best prior; approximate by taking min of archive_F except last lam (simple heuristic)\n            if len(archive_F) > lam:\n                prev_best_est = min(archive_F[:-lam])\n            else:\n                prev_best_est = min(archive_F)\n            for i in valid_idx:\n                if fitness[i] < prev_best_est - 1e-12:\n                    improvements += 1\n\n            recent_successes.append(1 if improvements > 0 else 0)\n            if len(recent_successes) > success_window:\n                recent_successes.pop(0)\n\n            # recombine new mean: log-weighted recombination similar to CMA but normalized to sel_count\n            if sel_count > 0:\n                w = weights[:sel_count].copy()\n                w = w / np.sum(w)\n                m_old = m.copy()\n                # recombine in parameter space (not step-space) to better respect bounds\n                m = np.sum(w[:, None] * x_sel, axis=0)\n                # compute weighted average step y_w\n                y_w = np.sum(w[:, None] * y_sel, axis=0)\n                # update momentum\n                mmt = beta_m * mmt + (1.0 - beta_m) * (m - m_old)\n                # update second moment v (Adam-like) on steps y scaled by r: we accumulate squared step magnitudes\n                y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n                v = (b2 * v) + (1.0 - b2) * (y2 + eps)\n                inv_scale = 1.0 / (np.sqrt(v) + eps)\n\n                # store successful y (normalized) into buffer if improvement observed\n                if improvements > 0:\n                    success_buffer.append(y_w.copy())\n                else:\n                    # small chance to still promote diversity\n                    if np.random.rand() < 0.05:\n                        success_buffer.append(y_w.copy())\n                if len(success_buffer) > buffer_max:\n                    success_buffer.pop(0)\n\n                # update low-rank subspace from buffer occasionally (SVD)\n                if (len(success_buffer) >= max(3, self.k)) and (recond_counter % max(1, int(np.ceil(n/4)))) == 0:\n                    try:\n                        Y = np.vstack(success_buffer).T  # n x m\n                        # center columns\n                        Y = Y - np.mean(Y, axis=1, keepdims=True)\n                        # thin SVD\n                        U_new, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                        take = min(self.k, U_new.shape[1])\n                        if take > 0:\n                            U = U_new[:, :take]\n                            # relative singular scales\n                            S = (svals[:take] / (np.sqrt(max(1, Y.shape[1])) + eps)).copy()\n                            if U.shape[1] < self.k:\n                                # pad with small orthonormal columns\n                                pad_cols = self.k - U.shape[1]\n                                pad = np.zeros((n, pad_cols))\n                                U = np.hstack([U, pad])\n                                S = np.concatenate([S, np.ones(pad_cols) * 1e-2])\n                    except Exception:\n                        pass\n\n            # trust-region adaptation based on recent success rate\n            if len(recent_successes) >= success_window:\n                succ_rate = np.mean(recent_successes)\n                if succ_rate > target_success_rate * 1.2:\n                    r = min(r * 1.2, r_max)\n                elif succ_rate < target_success_rate * 0.8:\n                    r = max(r * 0.85, r_min)\n                # occasionally reset momentum if stuck\n                if succ_rate < 0.02:\n                    mmt *= 0.3\n                    # small nudge towards a random archive point to escape stagnation\n                    if len(archive_X) > 0:\n                        pick = np.random.randint(len(archive_X))\n                        m = 0.5 * m + 0.5 * archive_X[pick]\n\n            # periodic approximate reconditioning to produce an approximate inverse sqrt preconditioner\n            recond_counter += lam\n            if recond_counter >= recond_every:\n                recond_counter = 0\n                try:\n                    # approximate covariance in step-space: diag(v^{-1}) + low-rank U S^2 U^T\n                    diag_part = np.diag(1.0 / (v + eps))\n                    cov_approx = diag_part.copy()\n                    if self.k > 0:\n                        cov_approx += (U @ np.diag((S ** 2) + eps) @ U.T)\n                    cov_approx = 0.5 * (cov_approx + cov_approx.T)\n                    # eigen-decompose\n                    eigvals, eigvecs = np.linalg.eigh(cov_approx)\n                    eigvals = np.maximum(eigvals, 1e-12)\n                    # invsqrt\n                    approx_invsqrt = (eigvecs * (1.0 / np.sqrt(eigvals))) @ eigvecs.T\n                except Exception:\n                    approx_invsqrt = np.eye(n)\n\n            # Ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # if budget exhausted, break\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ASTRO_DE scored 0.204 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "72f4ba5b-2276-4c3f-800f-269ed722d539", "operator": null, "metadata": {"aucs": [0.16468811799725547, 0.16715951046177868, 0.23927629710290788, 0.22497476090415303, 0.19980033538519482, 0.20975915721791827, 0.22594391906201794, 0.2726394592165625, 0.18168599253520823, 0.1542684537072927]}, "task_prompt": ""}
{"id": "429c8be1-c324-449c-89aa-d039b319f288", "fitness": 0.14787313441836497, "name": "BAPC", "description": "BAPC is a hybrid Gaussian search that mixes a per-coordinate diagonal scale D with an online low-rank subspace (U, s) to shape mutations, uses mirrored sampling and exponential recombination weights to form a new mean, and adapts global step-size sigma by a population success-rate rule rather than CMA-style paths to respect limited budgets. Key heuristics are a slightly larger population size, subspace dimension k ≈ n^(2/3), an initial sigma = 0.3·box size, and exponential weights with effective mu for robust recombination. Adaptation uses an RMSprop-like EMA for D (beta_D=0.88) so D ∝ sqrt(second moment), an Oja-style incremental PCA for U with slow decay (eta_oja≈0.06) and EMA-updated subspace scales s, while sigma is updated via lr_sigma = 0.6/√n toward a target success p_target=0.25. Exploration is boosted by occasional heavy-tailed Cauchy jumps (p_cauchy≈0.09), archive-guided DE nudges (p_de≈0.15), and stagnation controls that inflate sigma and nudge the mean toward archived good points.", "code": "import numpy as np\n\nclass BAPC:\n    \"\"\"\n    Block-Adaptive Projection Covariance (BAPC)\n\n    Main ideas (novel compared to provided algorithm):\n    - Population-size and weights follow a different heuristic (slightly larger lambda).\n    - Sigma adapted by a population success-rate rule (target success probability) instead of CMA ps path.\n    - Per-coordinate scaling D updated with an RMSprop-like exponential moving second moment (different constants).\n    - Low-rank subspace (U, s) is adapted online via an Oja-like rule (incremental PCA) instead of periodic SVD.\n    - Subspace dimension k defaults to n^(2/3) (different scaling vs sqrt(n)).\n    - Occasional heavy-tailed Cauchy jumps and archive-guided nudges retained.\n    - Mirrored sampling kept for variance reduction. All evaluations respect the provided budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size heuristic (different from original)\n        self.lambda_ = max(6, int(6 + np.floor(4.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace dimension: use n^(2/3) as a different scaling heuristic\n        if subspace_k is None:\n            self.k = min(max(1, int(np.ceil(self.dim ** (2.0 / 3.0)))), self.dim)\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # behaviour hyper-parameters (clear, tunable)\n        self.p_cauchy = 0.09\n        self.cauchy_scale = 1.0\n        self.p_de = 0.15\n        self.F_de = 0.6\n        self.mirrored = True\n\n        # sigma success-rate control target & learning rate (different constant choices)\n        self.p_target = 0.25\n        self.lr_sigma = 0.6 / np.sqrt(max(1, self.dim))  # learning rate for sigma\n\n        # D (diag) RMSprop-like parameters\n        self.beta_D = 0.88\n        self.eps = 1e-12\n\n        # Oja rule learning rate for subspace (decays slowly)\n        self.eta_oja = 0.06\n\n        # stagnation control\n        self.stag_iter_threshold = max(30, 3 * self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds from func (Many Affine BBOB uses [-5,5] but support general)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initialize mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n\n        # initial sigma: 0.3 of box size average (different scale)\n        sigma = 0.3 * np.mean(ub - lb)\n        sigma = max(sigma, 1e-12)\n\n        # per-coordinate scale D and its RMSprop accumulator\n        D = np.ones(n)\n        vD = np.ones(n) * 1e-6\n\n        # initialize subspace U (n x k) orthonormal and scales s\n        if self.k > 0:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand_mat)\n                U = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n            s = np.ones(self.k) * 0.6  # initial subspace scales (different)\n        else:\n            U = np.zeros((n, 0))\n            s = np.zeros(0)\n\n        # recombination weights (exponential decay different from log weighting)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        # smoother exponential weights\n        exp_basis = np.exp(-np.arange(mu) / max(1.0, (mu / 3.0)))\n        weights = exp_basis / np.sum(exp_basis)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # archive to store evaluated points for DE-like moves and nudges\n        archive_X = []\n        archive_F = []\n\n        # book-keeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # evaluate initial mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(float(fm))\n            f_opt = float(fm)\n            x_opt = xm.copy()\n            f_mean = float(fm)  # baseline for success-rate checks\n        else:\n            f_mean = np.inf\n\n        # small random seed-dependent jitter schedule for Oja learning decay\n        decay_counter = 0\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # keep mirrored parity\n            if self.mirrored and current_lambda % 2 == 1:\n                current_lambda -= 1\n                if current_lambda <= 0:\n                    current_lambda = 1\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n\n            k_idx = 0\n            while k_idx < current_lambda and evals + (current_lambda - k_idx) > 0:\n                # sample gaussian components\n                z_diag = np.random.randn(n)\n                z_sub = np.random.randn(self.k) if self.k > 0 else np.zeros(0)\n\n                # low-rank contribution\n                low = (U @ (s * z_sub)) if self.k > 0 else 0.0\n\n                # mix diagonal and low-rank with a different mixing coefficient\n                mix_low = 0.8 * np.median(D) if self.k > 0 else 0.0\n                y = D * z_diag + mix_low * low\n\n                # occasional Cauchy heavy-tail jump\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    norm_z = np.linalg.norm(z_diag) + 1e-20\n                    y = r * (z_diag / norm_z) * np.mean(D)\n\n                x = m + sigma * y\n\n                # occasional DE-like archive mutation\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    x = x + self.F_de * (archive_X[i1] - archive_X[i2])\n\n                x = np.clip(x, lb, ub)\n                arx[k_idx] = x\n                arz[k_idx] = y\n                k_idx += 1\n\n                # mirrored partner\n                if self.mirrored and k_idx < current_lambda:\n                    y2 = -y\n                    x2 = m + sigma * y2\n                    if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        x2 = x2 + self.F_de * (archive_X[i1] - archive_X[i2])\n                    x2 = np.clip(x2, lb, ub)\n                    arx[k_idx] = x2\n                    arz[k_idx] = y2\n                    k_idx += 1\n\n            # evaluate candidates one-by-one to respect budget\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = float(f)\n                archive_X.append(x.copy())\n                archive_F.append(float(f))\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n\n            # selection\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            order = np.argsort(arfit[valid_idx])\n            sel_idx_rel = valid_idx[order[:min(mu, len(order))]]\n            sel_count = len(sel_idx_rel)\n            if sel_count == 0:\n                break\n\n            x_sel = arx[sel_idx_rel]\n            y_sel = arz[sel_idx_rel]\n\n            # compute recombined y_w in normalized-y space (weights truncated if fewer selected)\n            w = weights[:sel_count].copy()\n            w = w / np.sum(w)\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # recombine mean by shifting in y-space (keeps consistent scaling)\n            m_old = m.copy()\n            m = m + sigma * y_w\n            m = np.clip(m, lb, ub)\n\n            # success-rate update for sigma:\n            # define a success as candidate better than previous mean evaluation f_mean\n            successes = np.sum(arfit[valid_idx] < f_mean)\n            p_succ = successes / max(1.0, len(valid_idx))\n            # exponential update (different equation from CMA)\n            sigma *= np.exp(self.lr_sigma * (p_succ - self.p_target))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e2 * np.mean(ub - lb) + 1e-12)\n\n            # update baseline mean evaluation f_mean: set to best of previous baseline and new recombined mean if evaluated\n            # we did evaluate initial m_old earlier; we don't evaluate m after recombination to save budget.\n            # update f_mean conservatively: if any offspring improved over f_mean, set to min offspring\n            if np.any(arfit[valid_idx] < f_mean):\n                f_mean = float(np.min(arfit[valid_idx]))\n            # else degrade baseline slightly with noise to avoid stagnation\n            else:\n                f_mean = f_mean + 1e-12\n\n            # update diagonal D with RMSprop-like exponential second moment using selected y's\n            y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n            vD = self.beta_D * vD + (1.0 - self.beta_D) * (y2 + self.eps)\n            # adapt D differently: scale proportional to sqrt(vD) but normalized to median 1\n            D_new = np.sqrt(vD + self.eps)\n            # normalize D to keep typical scale ~1 (prevent runaway)\n            med = np.median(D_new) + 1e-20\n            D = D_new / med\n\n            # Oja-style online update of subspace U and scales s using the recombined y_w\n            if self.k > 0:\n                decay_counter += 1\n                eta = self.eta_oja / (1.0 + 0.03 * decay_counter)  # slow decay\n                # project y_w onto current subspace and orthogonal residual\n                proj = U.T @ y_w  # k\n                recon = U @ proj\n                resid = y_w - recon\n                # update each u_j using Oja rule with normalization towards orthogonality\n                for j in range(self.k):\n                    uj = U[:, j]\n                    # Oja increment: uj <- uj + eta * (y_w * proj_j - proj_j^2 * uj)\n                    pj = proj[j]\n                    increment = eta * (y_w * pj - (pj ** 2) * uj)\n                    uj = uj + increment\n                    # re-normalize uj to unit norm (small safeguard)\n                    nrm = np.linalg.norm(uj) + 1e-20\n                    U[:, j] = uj / nrm\n                # occasionally orthonormalize full U for stability\n                if (decay_counter % max(10, int(np.sqrt(n)))) == 0:\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n                # update subspace scales s via EMA of squared projections (different constants)\n                proj2 = proj ** 2\n                beta_s = 0.25\n                s2 = (1.0 - beta_s) * (s ** 2) + beta_s * (proj2 + 1e-20)\n                s = np.sqrt(s2)\n                # keep s not too small\n                s = np.maximum(s, 0.05)\n\n            # stagnation detection: if no improvement in a while, inflate sigma and nudge mean\n            if len(archive_F) > 0:\n                idx_best = int(np.argmin(archive_F))\n                best_age = evals - (len(archive_F) - 1 - idx_best)  # approximate age\n                if evals > 100 and (best_age > self.stag_iter_threshold):\n                    sigma *= 1.8\n                    # nudge mean toward a random good archive point\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.55 * m + 0.45 * archive_X[pick]\n                    m = np.clip(m, lb, ub)\n                    # slight reset of D and s to explore new directions\n                    D = np.maximum(D * (0.8 + 0.4 * np.random.rand(n)), 0.1)\n                    s = np.maximum(s * (0.8 + 0.4 * np.random.rand(self.k)), 0.05)\n                    decay_counter = 0\n\n            # update best after moving mean: if mean was evaluated previously in archive, sync f_mean else keep\n            # (we do not evaluate mean here to preserve budget)\n\n            # ensure bounds on mean\n            m = np.clip(m, lb, ub)\n\n            # break if budget exhausted\n            if evals >= budget:\n                break\n\n        # final return best found\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm BAPC scored 0.148 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "72f4ba5b-2276-4c3f-800f-269ed722d539", "operator": null, "metadata": {"aucs": [0.0692049663623946, 0.14752687666834352, 0.21627371242190008, 0.12516366333248652, 0.14921453129913886, 0.16551134186916594, 0.18934588295298327, 0.1674541024678169, 0.13863327001475334, 0.1104029967946667]}, "task_prompt": ""}
{"id": "e157d1cc-26d6-4b5f-b0e2-deda091f662a", "fitness": 0.4910643073804807, "name": "HybridDiagSubspaceDE", "description": "This hybrid optimizer combines cheap diagonal preconditioning (per-coordinate std D updated by an EMA of selected y^2, diag-dominant sampling with alpha=0.9 and initial sigma=0.22·avg_range) with a learned low-rank subspace (U,S learned by SVD on a rolling success_buffer of recent weighted steps, k≈⌈√n⌉, buffer_max=max(10·k,30)) so search mixes coordinate-wise and correlated directions. Sampling is diversified by mirrored pairs, occasional heavy‑tailed Cauchy jumps (p_cauchy=0.10) and DE-style archive differences (p_de=0.18, F_de=0.7) to inject global moves and preserve an archive of evaluated points. Adaptation is CMA-like: evolution paths ps/pc, weighted recombination with log-weights, mu_eff-based step-size control and sigma update, while periodic SVD/eigendecomposition (eig_every=max(50,10·n)) builds an approximate covariance (diag + U diag(S^2) U^T) and invsqrtC for more informed updates. Practical robustness features include mirrored sampling, bounds clipping, archive capping, stagnation detection with modest restarts/diversification, and safeguards for numerical reconditioning.", "code": "import numpy as np\n\nclass HybridDiagSubspaceDE:\n    \"\"\"\n    Hybrid Diagonal+Subspace CMA-DE optimizer with mirrored sampling, occasional Cauchy jumps,\n    DE-style archive differences, CMA-like sigma adaptation and periodic reconditioning.\n    One-line: Cheap diagonal preconditioning + learned low-rank subspace + robust exploration hooks.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population heuristic\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n        # subspace dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        span = ub - lb\n        avg_range = float(np.mean(span))\n\n        # strategy params\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length control constants\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        cc = (4.0 + mu_eff / n) / (n + 4.0 + 2.0 * mu_eff / n)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        m = np.random.uniform(lb, ub)               # mean\n        sigma = 0.22 * avg_range                    # step-size\n        D = np.ones(n)                              # diag std approx\n        # low-rank subspace U (n x k) and scales S\n        if self.k >= 1:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand_mat)\n                U = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n        S = np.ones(self.k)\n\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n\n        # buffers / archive\n        success_buffer = []\n        buffer_max = max(10 * self.k, 30)\n        archive_X = []\n        archive_F = []\n\n        # exploration controls\n        p_cauchy = 0.10\n        cauchy_scale = 1.0\n        p_de = 0.18\n        F_de = 0.7\n        mirrored = True\n\n        # reconditioning frequency\n        eig_every = max(50, 10 * n)\n        eigen_counter = 0\n        B_full = np.eye(n)\n        D_full = np.ones(n)\n        invsqrtC_full = np.eye(n)\n        have_full = False\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_opt = float(fm)\n            x_opt = xm.copy()\n            last_improvement_eval = evals\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # enforce mirrored pairing if desired\n            if mirrored and (current_lambda % 2 == 1) and current_lambda > 1:\n                current_lambda -= 1\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))  # stored y steps\n\n            # sampling\n            for k_idx in range(current_lambda):\n                # diag component\n                z = np.random.randn(n)\n                # low-rank component\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U @ (S * z_low)\n                else:\n                    low = 0.0\n                # mix weights: allow strong diag but keep low-rank contribution\n                alpha = 0.9  # diag dominance factor\n                y = (D * z) * alpha + (0.7 * np.mean(D)) * low\n                # occasional heavy-tailed jump along random direction\n                if np.random.rand() < p_cauchy:\n                    r = np.random.standard_cauchy() * cauchy_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * np.mean(D)\n                # mirrored option: for odd index flip sign of previous y\n                if mirrored and (k_idx % 2 == 1):\n                    y = -prev_y\n                x = m + sigma * y\n\n                # DE-like difference mutation\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                x = np.clip(x, lb, ub)\n                arx[k_idx] = x\n                arz[k_idx] = y\n                prev_y = y.copy()\n\n            # evaluate candidates\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            idx_sorted = valid_idx[np.argsort(arfit[valid_idx])]\n            sel_count = min(mu, idx_sorted.size)\n            sel_idx = idx_sorted[:sel_count]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            # normalized weights for possibly smaller selection\n            w = weights[:sel_count].copy()\n            w = w / np.sum(w)\n\n            # recombine mean\n            m_old = m.copy()\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # approximate invsqrtC for ps update: prefer full if available else diag-based\n            if have_full:\n                invsqrt = invsqrtC_full\n            else:\n                invsqrt = np.diag(1.0 / (D + 1e-20))\n\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrt @ y_w)\n            norm_ps = np.linalg.norm(ps)\n\n            # hsig for covariance path (conservative)\n            hsig = 1.0 if (norm_ps / np.sqrt(1.0 - (1.0 - cs) ** (2.0 * max(1, evals / max(1, lam)))) / chi_n) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # adapt sigma\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e3 * avg_range + 1e-12)\n\n            # update diagonal D via EMA of selected y^2\n            c_d = 0.22\n            y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n\n            # push weighted successful steps into buffer to learn subspace\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace via SVD on buffer periodically\n            if (len(success_buffer) >= self.k) and (evals % max(1, min(10, n)) == 0):\n                Y = np.vstack(success_buffer).T  # n x m\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    U_new, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        # singular scales normalized by sqrt(count) to be stable\n                        S = (svals[:k_take] / (np.sqrt(max(1, Y.shape[1])) + 1e-12)).copy()\n                        if U.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U.shape[1]))\n                            U = np.hstack([U, pad])\n                            S = np.concatenate([S, np.ones(self.k - len(S))])\n                except np.linalg.LinAlgError:\n                    pass\n\n            # occasionally recondition: build approximate covariance diag(D^2)+U diag(S^2) U^T and eigendecompose\n            eigen_counter += current_lambda\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                try:\n                    cov_approx = np.diag(D ** 2)\n                    if self.k > 0:\n                        cov_approx = cov_approx + (U @ np.diag(S ** 2) @ U.T)\n                    cov_approx = 0.5 * (cov_approx + cov_approx.T)\n                    Dvals, B = np.linalg.eigh(cov_approx)\n                    Dvals = np.maximum(Dvals, 1e-20)\n                    D_full = np.sqrt(Dvals)\n                    B_full = B\n                    invsqrtC_full = (B_full * (1.0 / D_full)) @ B_full.T\n                    have_full = True\n                except np.linalg.LinAlgError:\n                    have_full = False\n                    B_full = np.eye(n); D_full = np.ones(n); invsqrtC_full = np.eye(n)\n\n            # small covariance-scale corruption protection\n            if np.any(np.isnan(D)) or (np.any(D <= 0)):\n                D = np.ones(n) * max(1e-6, np.median(D))\n\n            # stagnation detection (no improvement long time)\n            stagnation_threshold = max(50, 10 * n)\n            if (evals - last_improvement_eval) > stagnation_threshold:\n                # modest restart: inflate sigma, nudge mean to a random archive point (if available)\n                sigma *= 1.8\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.65 * m + 0.35 * archive_X[pick]\n                # diversify D and small reset of ps/pc\n                D = 0.9 * D + 0.1 * (1.0 + 0.1 * np.random.randn(n))\n                ps *= 0.5\n                pc *= 0.5\n                success_buffer = []\n                last_improvement_eval = evals\n\n            # keep mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # budget loop guard\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDiagSubspaceDE scored 0.491 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "72f4ba5b-2276-4c3f-800f-269ed722d539", "operator": null, "metadata": {"aucs": [0.19949550405510597, 0.15993025363572888, 0.4066037230448071, 0.9645423065597963, 0.6979805259178585, 0.921906106986193, 0.27283119745052997, 0.3332748493407066, 0.772199275252358, 0.1818793315617222]}, "task_prompt": ""}
{"id": "eb4a5ffc-9856-4177-8a6b-beedbd924987", "fitness": "-inf", "name": "BLARDE", "description": "BLARDE is a hybrid CMA/DE-style optimizer that augments a compact, orthonormal low-rank directional basis (k ≈ ceil(sqrt(n))) with a bandit (UCB) over per-arm counts/rewards and adaptive per-arm step sizes to focus targeted directional probes and perform cheap parabolic refinements (including small opposition samples) when probes improve the best found point. Population search uses mirrored Gaussian offspring mapped by the CMA eigenbasis (B and D_eig), with archive-based DE perturbations and occasional Mantegna-like Levy jumps to introduce long-range moves, while evaluated offspring are collected into an archive for DE and selection. Adaptation is CMA-inspired (weighted recombination, ps/pc cumulation, sigma adaptation via norm(ps)/chi_n, rank-one and rank-mu covariance updates, periodic eigendecomposition) plus lightweight mechanisms for basis re-orthonormalization, replacing poorly performing arms, and per-arm step grow/shrink rules. The algorithm is budget-aware (limits directional probes), uses conservative default scales (sigma0 ≈ 0.25·mean_range, step0 ≈ 0.35·mean_range), tuned probabilities for DE/probing/Levy (p_de≈0.28, p_dir_probe≈0.22, p_levy≈0.10), and soft restarts when sigma collapses to maintain robustness.", "code": "import numpy as np\n\nclass BLARDE:\n    \"\"\"\n    BLARDE: Bandit-guided Levy-Adaptive Rank-Differential Evolution\n\n    One-line: Hybrid CMA/DE that augments population sampling with a compact bandit-guided\n    low-rank basis of directional probes (UCB-selected arms with adaptive per-arm steps),\n    uses mirrored Gaussian offspring + DE archive perturbations, performs cheap parabolic\n    refinement on successful directional probes, and updates covariance with selected offspring.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 subspace_k=None, memory_size=8, ucb_beta=1.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # low-rank subspace size (approx sqrt(n))\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n        self.memory_size = int(memory_size)\n        self.ucb_beta = float(ucb_beta)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb, dtype=float)\n        if ub.shape == ():\n            ub = np.full(n, ub, dtype=float)\n        rng = ub - lb\n        mean_range = max(1e-12, np.mean(rng))\n\n        # --- internal helpers ---\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            if evals >= budget:\n                return None, None\n            x = clip(np.asarray(x, dtype=float))\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = f\n                x_best = x.copy()\n            return f, x\n\n        # --- initialize CMA-like state (inspired by LARDE) ---\n        lam = max(4, int(5 + np.floor(5.0 * np.log(max(1, n)))))  # slightly larger\n        mu = max(1, lam // 2)\n        weights = np.log(np.arange(1, mu + 1) + 0.5)[::-1]\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n        # adaptation constants (blended LARDE defaults)\n        cc = 2.0 / (n + 2.0)\n        cs = 0.35 * (mu_eff / (n + mu_eff + 1))\n        c1 = 1.0 / max(1.0, (n + 3.0) ** 2)\n        cmu = 0.6 * (1.0 - c1) * min(1.0, mu_eff / (2.0 * n))\n        damps = 1.0 + 0.3 * cs + 0.2 * np.sqrt(max(0.0, mu_eff / n))\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # dynamic CMA state\n        m = np.random.uniform(lb, ub)\n        sigma = max(1e-12, 0.25 * mean_range)\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D_eig = np.ones(n)\n        invsqrtC = np.eye(n)\n        eigen_eval_counter = 0\n        eig_every = max(1, int(8 * n))\n\n        # archive for DE-like ops\n        archive_X = []\n        archive_F = []\n\n        # bandit-guided low-rank basis (from No.1)\n        p = min(self.k, n)\n        R = np.random.randn(n, p)\n        Q, _ = np.linalg.qr(R)\n        D_basis = Q[:, :p].copy()  # orthonormal basis columns\n        counts = np.zeros(p, dtype=int)\n        rewards = np.zeros(p, dtype=float)\n        steps = np.full(p, 0.35 * mean_range)\n        min_step = 1e-12 * mean_range\n        recent_moves = []\n\n        # control probs\n        p_de = 0.28\n        F_de = 0.8\n        p_dir_probe = 0.22   # fraction of budget used for targeted directional probes per generation\n        p_levy = 0.10\n\n        # cheap parabolic interpolation along a direction\n        def parabolic_vertex(alpha_vals, f_vals, x0, d):\n            if len(alpha_vals) != 3 or len(f_vals) != 3:\n                return None, None, None\n            a1, a2, a3 = alpha_vals\n            f1, f2, f3 = f_vals\n            denom = (a1 - a2) * (a1 - a3) * (a2 - a3)\n            if abs(denom) < 1e-16:\n                return None, None, None\n            A = ((f1 * (a2 - a3) + f2 * (a3 - a1) + f3 * (a1 - a2))) / denom\n            B_ = ((f1 * (a3 ** 2 - a2 ** 2) + f2 * (a1 ** 2 - a3 ** 2) + f3 * (a2 ** 2 - a1 ** 2))) / denom\n            if abs(A) < 1e-16:\n                return None, None, None\n            alpha_star = -B_ / (2 * A)\n            # clamp modestly\n            low = min(alpha_vals) - 2.0 * max(1.0, abs(min(alpha_vals)))\n            high = max(alpha_vals) + 2.0 * max(1.0, abs(max(alpha_vals)))\n            alpha_star = float(np.clip(alpha_star, low, high))\n            x_star = clip(x0 + alpha_star * d)\n            f_star, _ = safe_eval(x_star)\n            if f_star is None:\n                return None, None, None\n            return f_star, x_star, alpha_star\n\n        # UCB selector\n        def select_ucb():\n            total = counts.sum()\n            if total == 0:\n                untried = np.where(counts == 0)[0]\n                if len(untried) > 0:\n                    return int(np.random.choice(untried))\n                return int(np.random.randint(0, p))\n            avg = np.zeros_like(rewards)\n            nz = counts > 0\n            avg[nz] = rewards[nz] / counts[nz]\n            bonus = np.sqrt(np.log(1 + total) / (1 + counts))\n            scores = avg + self.ucb_beta * bonus\n            return int(np.argmax(scores))\n\n        # evaluate initial mean once\n        if evals < budget:\n            xm = clip(m)\n            fm, xm = safe_eval(xm)\n            if fm is None:\n                return float(f_best), np.array(x_best, dtype=float)\n            archive_X.append(xm.copy()); archive_F.append(fm)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            # decide how many directional probes to try this iteration (budget-aware)\n            max_dir = min(p * 2, max(0, int(round(p_dir_probe * lam))))\n            max_dir = min(max_dir, remaining)\n            # directional probing phase\n            for _ in range(max_dir):\n                if evals >= budget:\n                    break\n                idx = select_ucb()\n                d = D_basis[:, idx].copy()\n                dn = np.linalg.norm(d)\n                if dn == 0:\n                    v = np.random.randn(n)\n                    v /= (np.linalg.norm(v) + 1e-20)\n                    D_basis[:, idx] = v\n                    d = v\n                # sample alpha around current sigma-scaled step\n                alpha0 = np.random.normal(loc=0.0, scale=steps[idx] * (sigma / (0.25 * mean_range + 1e-20)))\n                if abs(alpha0) < 1e-16:\n                    alpha0 = steps[idx] * (0.5 - np.random.rand())\n                x_try = clip(m + alpha0 * d)\n                f_try, x_ret = safe_eval(x_try)\n                if f_try is None:\n                    break\n                counts[idx] += 1\n                # small opposition sample sometimes\n                if (np.random.rand() < 0.12) and (evals < budget):\n                    alpha_op = -alpha0 * (0.4 + 0.6 * np.random.rand())\n                    x_op = clip(m + alpha_op * d)\n                    f_op, _ = safe_eval(x_op)\n                    if f_op is None:\n                        break\n                    counts[idx] += 1\n                    if f_op < f_try:\n                        f_try = f_op\n                        x_ret = clip(m + alpha_op * d)\n                # reward is improvement over current best\n                reward = max(0.0, f_best - f_try)\n                rewards[idx] += reward\n                # accept improvement by moving mean gently toward probe success\n                if f_try < f_best - 1e-12:\n                    # soft move of mean toward success (help CMA)\n                    m = 0.85 * m + 0.15 * x_ret\n                    # update basis direction toward move\n                    mv = x_ret - m\n                    mvn = np.linalg.norm(mv)\n                    if mvn > 0:\n                        unit_mv = mv / mvn\n                        gamma = 0.30\n                        D_basis[:, idx] = (1 - gamma) * D_basis[:, idx] + gamma * unit_mv\n                        D_basis[:, idx] /= (np.linalg.norm(D_basis[:, idx]) + 1e-20)\n                        # store recent move\n                        recent_moves.insert(0, unit_mv.copy())\n                        if len(recent_moves) > self.memory_size:\n                            recent_moves.pop()\n                    # step grow\n                    steps[idx] = min(steps[idx] * 1.20, 5.0 * mean_range)\n                    # parabolic refinement if budget permits\n                    if (budget - evals) >= 2:\n                        a1 = alpha0\n                        a2 = 0.0\n                        a3 = alpha0 * 1.6\n                        alpha_vals = [a2, a1, a3]\n                        f_vals = []\n                        for a in alpha_vals:\n                            xa = clip((m if a != a1 else (m)) + a * d)\n                            # careful: evaluate around the previous mean prior to soft move; use x_ret's direction\n                            fa, _ = safe_eval(xa)\n                            if fa is None:\n                                break\n                            f_vals.append(fa)\n                        if len(f_vals) == 3:\n                            f_par, x_par, a_par = parabolic_vertex(alpha_vals, f_vals, m, d)\n                            if f_par is not None and f_par < f_best - 1e-12:\n                                # accept parabolic improved point: nudge mean\n                                m = 0.9 * m + 0.1 * x_par\n                                # update basis toward parabolic move\n                                mv2 = x_par - m\n                                mv2n = np.linalg.norm(mv2)\n                                if mv2n > 0:\n                                    D_basis[:, idx] = (1 - 0.25) * D_basis[:, idx] + 0.25 * (mv2 / mv2n)\n                                    D_basis[:, idx] /= (np.linalg.norm(D_basis[:, idx]) + 1e-20)\n                                steps[idx] = min(steps[idx] * 1.12, 5.0 * mean_range)\n                else:\n                    # shrink step\n                    steps[idx] = max(steps[idx] * 0.80, min_step)\n                # occasional global Gaussian perturbation acceptance\n                if np.random.rand() < 0.04 and evals < budget:\n                    sigma_g = 0.4 * mean_range * (0.3 + np.random.rand())\n                    xg = clip(m + np.random.randn(n) * sigma_g)\n                    fg, _ = safe_eval(xg)\n                    if fg is None:\n                        break\n                    if fg < f_best - 1e-12:\n                        m = 0.9 * m + 0.1 * xg\n                        # adapt a random poor basis toward global move\n                        worst_idx = int(np.argmin(rewards + 1e-12))\n                        mv = xg - m\n                        mvn = np.linalg.norm(mv)\n                        if mvn > 0:\n                            D_basis[:, worst_idx] = (1 - 0.25) * D_basis[:, worst_idx] + 0.25 * (mv / mvn)\n                            D_basis[:, worst_idx] /= (np.linalg.norm(D_basis[:, worst_idx]) + 1e-20)\n                            steps[worst_idx] = max(steps[worst_idx], 0.6 * sigma_g)\n\n            # re-orthonormalize basis occasionally\n            if np.random.rand() < 0.12:\n                try:\n                    Q, _ = np.linalg.qr(D_basis)\n                    D_basis = Q[:, :p].copy()\n                except Exception:\n                    pass\n\n            # population offspring generation (CMA/DE-style)\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n            current_lambda = min(lam, remaining)\n            # mirrored sampling\n            if current_lambda % 2 == 0:\n                half = current_lambda // 2\n                arz_half = np.random.randn(half, n)\n                arz = np.vstack([arz_half, -arz_half])\n            else:\n                arz = np.random.randn(current_lambda, n)\n            # map via B*(D_eig * z)\n            ary = arz @ (B * D_eig).T\n            arx = m + sigma * ary\n\n            # apply DE archive perturbations + occasional Levy jumps\n            for k in range(current_lambda):\n                xk = arx[k].copy()\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    xk = xk + de_mut\n                if np.random.rand() < p_levy:\n                    # Mantegna-like step\n                    alpha = 1.5\n                    sigma_u = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                               (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n                    u = np.random.normal(0, sigma_u, size=n)\n                    v = np.random.normal(0, 1.0, size=n)\n                    levy = (u / (np.abs(v) ** (1.0 / alpha))) * (0.5 * sigma)\n                    xk = xk + levy\n                arx[k] = clip(xk)\n\n            # Evaluate offspring sequentially\n            arfit = np.full(current_lambda, np.inf)\n            for k in range(current_lambda):\n                if evals >= budget:\n                    break\n                xk = arx[k]\n                fk, xk = safe_eval(xk)\n                if fk is None:\n                    break\n                arfit[k] = fk\n                archive_X.append(xk.copy()); archive_F.append(fk)\n                # maintain archive bounded\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n\n            # selection & recombination if any offspring evaluated\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                # nothing evaluated, continue\n                continue\n            idx_sorted = np.argsort(arfit)\n            sel = idx_sorted[:min(mu, len(idx_sorted))]\n            x_sel = arx[sel]\n            y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n            m_old = m.copy()\n            # update mean (weighted)\n            m = np.sum(weights[:len(sel)][:, None] * x_sel, axis=0)\n\n            # update cumulation and sigma\n            y_w = np.sum(weights[:len(sel)][:, None] * y_sel, axis=0)\n            # invsqrtC timing: if eig not recently recomputed, invsqrtC may be identity; safe multiply\n            try:\n                ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrtC @ y_w)\n            except Exception:\n                ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * y_w\n            norm_ps = np.linalg.norm(ps)\n            sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n            sigma = max(sigma, 1e-12)\n\n            # hsig and pc\n            denom = np.sqrt(1.0 - (1.0 - cs) ** (2.0 * max(1.0, evals / max(1, current_lambda))))\n            hsig = 1.0 if (norm_ps / (denom + 1e-20) / chi_n) < (1.5 + 1.0 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # covariance rank-mu update\n            rank_mu = np.zeros((n, n))\n            for i, ii in enumerate(sel):\n                yi = y_sel[i][:, None]\n                rank_mu += weights[i] * (yi @ yi.T)\n            rank_one = np.outer(pc, pc)\n            C = (1.0 - c1 - cmu) * C + c1 * (rank_one + (1.0 - hsig) * cc * (2.0 - cc) * C) + cmu * rank_mu\n            # numerical stability\n            C = np.triu(C) + np.triu(C, 1).T\n            C += 1e-12 * np.eye(n)\n\n            # occasionally update eigendecomposition\n            eigen_eval_counter += max(1, len(valid_idx))\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D_eig = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D_eig)) @ B.T\n                except Exception:\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D_eig = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # rotate/replace poor basis arms occasionally based on low reward/count ratio\n            if np.random.rand() < 0.08:\n                score = np.full(p, -1e9)\n                nz = counts > 0\n                score[nz] = rewards[nz] / counts[nz]\n                worst = np.argsort(score)[:max(1, p // 4)]\n                for wi in worst:\n                    rnd = np.random.randn(n)\n                    if (len(recent_moves) > 0) and (np.random.rand() < 0.6):\n                        candidate = 0.5 * rnd + 0.5 * recent_moves[np.random.randint(len(recent_moves))]\n                    else:\n                        candidate = rnd\n                    candidate /= (np.linalg.norm(candidate) + 1e-20)\n                    D_basis[:, wi] = candidate\n                    counts[wi] = 0\n                    rewards[wi] = 0.0\n                    steps[wi] = 0.35 * mean_range\n\n            # small stagnation rescue: if sigma becomes tiny or no improvement for long, do soft restart\n            if sigma < 1e-8 * mean_range:\n                # soft restart around best\n                if x_best is not None:\n                    m = clip(x_best + np.random.randn(n) * (0.6 * mean_range))\n                    sigma = max(sigma * 5.0, 0.1 * mean_range)\n                    # refresh basis partially\n                    for i in range(max(1, p // 2)):\n                        v = np.random.randn(n)\n                        v /= (np.linalg.norm(v) + 1e-20)\n                        D_basis[:, i] = v\n                        counts[i] = 0\n                        rewards[i] = 0.0\n                        steps[i] = 0.35 * mean_range\n\n            # quick exit if extremely good\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            # fallback sample\n            x_best = clip(np.random.uniform(lb, ub))\n            f_best, _ = safe_eval(x_best)\n            if f_best is None:\n                f_best = np.inf\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "72f4ba5b-2276-4c3f-800f-269ed722d539", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "91db634f-d3ec-4a2d-b83b-ab0c40db47ed", "fitness": 0.40336372204928556, "name": "MALM", "description": "The algorithm maintains a small ensemble of adaptive \"anchors\" (centers with trust radii, counts and cumulative improvement) and selects anchors by a softmax over per-anchor performance (temperature annealing and occasional soft replacement) to balance exploration vs exploitation. Around each anchor it fits a cheap ridge-regularized linear surrogate from a rolling memory of recent samples to estimate a local gradient and propose scaled negative-gradient steps with orthogonal jitter (falling back to isotropic Gaussian proposals when the model is unreliable). It uses budget-aware intensification primitives — a 3-point parabolic line fit for local refinement and occasional global covariance jumps sampled from stored successful displacements — while updating anchors (blend moves, radius expansion/shrinkage) based on success or failure. All evaluations are budget-guarded via safe_eval, with stagnation detection triggering soft restarts and local intensification; parameters (anchors≈6, memory≈64, small ridge, low global_jump_prob and moderate refine_prob) bias the method toward robust, conservative adaptation across diverse continuous landscapes.", "code": "import numpy as np\n\nclass MALM:\n    \"\"\"\n    Mixture of Adaptive Local Models (MALM)\n\n    Main ideas:\n    - Keep a small set of anchor points (local models). Each anchor has a center, trust radius,\n      and simple statistics (counts, cumulative improvement).\n    - Maintain a rolling memory of recent evaluated samples to fit cheap linear surrogates\n      (ridge regression) around each anchor to predict a descent direction (approx gradient).\n    - Propose steps by moving along the predicted negative gradient direction scaled by the\n      anchor trust radius (plus small isotropic noise). If no reliable local model exists,\n      sample from per-anchor isotropic Gaussian or from a global covariance built from\n      recent successful moves.\n    - After successful moves, gently move anchor centers toward better points, increase\n      trust radius and weight; on failures shrink radius. Occasionally refine a promising\n      direction with a budget-aware 3-point parabolic fit.\n    - Soft restarts when stagnation detected; all evaluations are guarded by a safe_eval\n      counter so we never exceed the budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 anchors=6, memory=64, init_radius=1.0, ridge=1e-6,\n                 global_jump_prob=0.04, refine_prob=0.18, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.anchors_k = int(max(1, anchors))\n        self.memory_size = int(max(10, memory))\n        self.init_radius = float(init_radius)\n        self.ridge = float(ridge)\n        self.global_jump_prob = float(global_jump_prob)\n        self.refine_prob = float(refine_prob)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds (support func-provided)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # ---------- Initialization ----------\n        # start with a random center\n        x0 = np.random.uniform(lb, ub)\n        f0, _ = safe_eval(x0)\n        if f0 is None:\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # anchors: each anchor has center, trust_radius, counts, cum_reward\n        anchors = []\n        for k in range(self.anchors_k):\n            # spread anchors around initial point with moderate perturbation\n            center = clip(x0 + np.random.randn(n) * (0.5 * self.init_radius))\n            anchor = {\n                'center': center,\n                'radius': max(1e-6, self.init_radius * (0.8 + 0.4 * np.random.rand())),\n                'counts': 1,\n                'cum_imp': 0.0,   # cumulative improvement when anchor proposals succeeded\n            }\n            anchors.append(anchor)\n\n        # recent samples memory: list of (x, f)\n        recent = [(x0.copy(), float(f0))]\n\n        # store successful moves (displacements) for global covariance estimation\n        successes = []\n\n        # control parameters\n        stagnation = 0\n        stagnation_limit = max(30, int(6 + np.sqrt(n) * 4))\n        iterations = 0\n\n        # helper: fit linear surrogate f ≈ a + g^T (x - center) for nearby points using ridge\n        def fit_local_linear(center, radius):\n            # collect neighbors within radius\n            X = []\n            y = []\n            for (xx, ff) in recent:\n                d = np.linalg.norm(xx - center)\n                if d <= max(radius * 1.25, 1e-12):\n                    X.append(xx - center)\n                    y.append(ff)\n            if len(y) < 3:\n                return None  # insufficient data\n            X = np.asarray(X)  # m x n\n            y = np.asarray(y)  # m\n            # solve ridge regression for gradient g, model: f = a + X g\n            # stack a column of ones to get intercept, solve for [a; g]\n            A = np.hstack([np.ones((X.shape[0], 1)), X])\n            # normal eqn: (A^T A + lambda I) w = A^T y\n            ATA = A.T @ A\n            lam = max(self.ridge, 1e-8) * np.trace(ATA) / (ATA.shape[0] + 1.0)\n            ATA += lam * np.eye(ATA.shape[0])\n            ATy = A.T @ y\n            try:\n                w = np.linalg.solve(ATA, ATy)\n            except np.linalg.LinAlgError:\n                return None\n            a = w[0]\n            g = w[1:]\n            gnorm = np.linalg.norm(g)\n            if gnorm < 1e-12:\n                return None\n            return g, a, len(y)\n\n        # cheap parabolic fit along a direction using three alpha values (centered)\n        def parabolic_fit_line(x0, d, alphas):\n            # alphas: list of three scalars (must be distinct)\n            if len(alphas) != 3:\n                return None, None\n            pts = []\n            fs = []\n            for a in alphas:\n                if evals >= self.budget:\n                    return None, None\n                x = clip(x0 + a * d)\n                f, _ = safe_eval(x)\n                if f is None:\n                    return None, None\n                pts.append(x)\n                fs.append(f)\n            f1, f2, f3 = fs\n            a1, a2, a3 = alphas\n            denom = (a1 - a2) * (a1 - a3) * (a2 - a3)\n            if abs(denom) < 1e-16:\n                return None, None\n            A = ( (f1*(a2 - a3) + f2*(a3 - a1) + f3*(a1 - a2)) ) / denom\n            B = ( (f1*(a3**2 - a2**2) + f2*(a1**2 - a3**2) + f3*(a2**2 - a1**2)) ) / denom\n            if abs(A) < 1e-16:\n                return None, None\n            alpha_star = -B / (2.0 * A)\n            # clamp to tested convex region extended slightly\n            lo = min(alphas) - 0.5 * (max(alphas) - min(alphas) + 1e-6)\n            hi = max(alphas) + 0.5 * (max(alphas) - min(alphas) + 1e-6)\n            alpha_star = float(np.clip(alpha_star, lo, hi))\n            x_star = clip(x0 + alpha_star * d)\n            f_star, _ = safe_eval(x_star)\n            if f_star is None:\n                return None, None\n            return f_star, x_star\n\n        # main loop\n        while evals < self.budget:\n            iterations += 1\n\n            # choose an anchor by softmax over per-anchor average improvement per proposal\n            scores = np.array([ (a['cum_imp'] / (a['counts'] + 1e-12)) for a in anchors ])\n            # add small exploration temperature\n            temp = 0.2 + 0.8 * np.exp(-0.001 * iterations)\n            exp_scores = np.exp(np.clip(scores / (np.std(scores)+1e-8) / (temp + 1e-12), -50, 50))\n            probs = exp_scores / np.sum(exp_scores)\n            idx = np.random.choice(len(anchors), p=probs)\n            anchor = anchors[idx]\n\n            # fit local linear surrogate if possible\n            fit = fit_local_linear(anchor['center'], anchor['radius'])\n            use_model = (fit is not None) and (np.random.rand() < 0.92)  # usually trust model if available\n\n            if use_model:\n                g, a0, mcount = fit\n                # predicted descent direction (negative gradient)\n                d_pred = -g\n                dnorm = np.linalg.norm(d_pred)\n                if dnorm < 1e-12:\n                    # fallback to isotropic sample\n                    d_unit = np.random.randn(n)\n                    d_unit /= (np.linalg.norm(d_unit) + 1e-20)\n                    proposal_dir = d_unit\n                else:\n                    proposal_dir = d_pred / dnorm\n                # adaptive step scale: combine trust radius and model confidence (more data -> bigger)\n                conf = np.tanh(0.5 * np.log(1.0 + mcount))\n                step_scale = anchor['radius'] * (0.4 + 0.6 * conf)\n                # add small orthogonal jitter to avoid staying on a ridge\n                jitter = np.random.randn(n) * (0.08 * anchor['radius'])\n                # project jitter to orthogonal component to keep main direction\n                jitter -= proposal_dir * (proposal_dir @ jitter)\n                proposal_vector = proposal_dir * step_scale + jitter\n                x_prop = clip(anchor['center'] + proposal_vector)\n                f_prop, x_prop = safe_eval(x_prop)\n                if f_prop is None:\n                    break\n                # record sample\n                recent.insert(0, (x_prop.copy(), float(f_prop)))\n                if len(recent) > self.memory_size:\n                    recent.pop()\n                anchor['counts'] += 1\n\n                # evaluate improvement relative to center's function (we need f(center) from recent or evaluate)\n                # find f_center in recent memory or evaluate center (costly but occasional)\n                f_center = None\n                for (xx, ff) in recent:\n                    if np.allclose(xx, anchor['center'], atol=1e-12):\n                        f_center = ff\n                        break\n                if f_center is None:\n                    # evaluate center if budget allows\n                    f_center, _ = safe_eval(anchor['center'])\n                    if f_center is None:\n                        break\n                    recent.insert(0, (anchor['center'].copy(), float(f_center)))\n                    if len(recent) > self.memory_size:\n                        recent.pop()\n\n                imp = max(0.0, f_center - f_prop)\n                anchor['cum_imp'] += imp\n\n                if f_prop < f_center - 1e-12:\n                    # success: move anchor center gently toward the proposal\n                    blend = 0.25 + 0.5 * np.tanh(imp / (abs(f_center)+1e-12))\n                    anchor['center'] = clip((1.0 - blend) * anchor['center'] + blend * x_prop)\n                    # expand radius moderately\n                    anchor['radius'] = min(anchor['radius'] * (1.15 + 0.2 * conf), 5.0 * np.mean(ub - lb))\n                    stagnation = 0\n                    # store successful displacement\n                    successes.append((x_prop - anchor['center']).copy())\n                    if len(successes) > self.memory_size:\n                        successes.pop(0)\n                else:\n                    # failure: shrink radius\n                    anchor['radius'] = max(anchor['radius'] * 0.80, 1e-8)\n                    stagnation += 1\n\n                # occasionally try a parabolic refinement along proposal vector (centered at previous center)\n                if (np.random.rand() < self.refine_prob) and (self.budget - evals >= 2):\n                    # use small alphas around 0: [-0.8, 0.0, +0.8] scaled to step_scale\n                    d_unit = proposal_vector.copy()\n                    d_unit /= (np.linalg.norm(d_unit) + 1e-20)\n                    alphas = [-0.8 * step_scale, 0.0, 0.8 * step_scale]\n                    pf, px = parabolic_fit_line(anchor['center'], d_unit, alphas)\n                    if pf is not None:\n                        recent.insert(0, (px.copy(), float(pf)))\n                        if len(recent) > self.memory_size:\n                            recent.pop()\n                        # accept if better than best-known center\n                        if pf < f_center - 1e-12:\n                            anchor['center'] = px.copy()\n                            anchor['radius'] = min(anchor['radius'] * 1.12, 5.0 * np.mean(ub - lb))\n                            stagnation = 0\n\n            else:\n                # fallback exploration: propose isotropic gaussian around anchor center using anchor.radius\n                noise = np.random.randn(n)\n                noise /= (np.linalg.norm(noise) + 1e-20)\n                scale = anchor['radius'] * (0.6 + 0.8 * np.random.rand())\n                x_prop = clip(anchor['center'] + noise * scale)\n                f_prop, x_prop = safe_eval(x_prop)\n                if f_prop is None:\n                    break\n                recent.insert(0, (x_prop.copy(), float(f_prop)))\n                if len(recent) > self.memory_size:\n                    recent.pop()\n                anchor['counts'] += 1\n                # compute improvement using nearest sample to center\n                f_center = None\n                for (xx, ff) in recent:\n                    if np.allclose(xx, anchor['center'], atol=1e-12):\n                        f_center = ff\n                        break\n                if f_center is None:\n                    f_center, _ = safe_eval(anchor['center'])\n                    if f_center is None:\n                        break\n                    recent.insert(0, (anchor['center'].copy(), float(f_center)))\n                    if len(recent) > self.memory_size:\n                        recent.pop()\n                imp = max(0.0, f_center - f_prop)\n                anchor['cum_imp'] += imp\n                if f_prop < f_center - 1e-12:\n                    blend = 0.2\n                    anchor['center'] = clip((1.0 - blend) * anchor['center'] + blend * x_prop)\n                    anchor['radius'] = min(anchor['radius'] * 1.12, 5.0 * np.mean(ub - lb))\n                    stagnation = 0\n                    successes.append((x_prop - anchor['center']).copy())\n                    if len(successes) > self.memory_size:\n                        successes.pop(0)\n                else:\n                    anchor['radius'] = max(anchor['radius'] * 0.85, 1e-8)\n                    stagnation += 1\n\n            # occasional global covariance jump sampled from successes\n            if (np.random.rand() < self.global_jump_prob) and (len(successes) >= 2) and (evals < self.budget):\n                # compute empirical covariance of recent successful displacements\n                S = np.vstack(successes)\n                C = np.cov(S.T) + 1e-8 * np.eye(n)\n                # scale by average anchor radius\n                avg_r = np.mean([a['radius'] for a in anchors])\n                scale = 0.6 * avg_r\n                try:\n                    L = np.linalg.cholesky(C)\n                    z = L @ np.random.randn(n) * scale\n                except np.linalg.LinAlgError:\n                    z = np.random.randn(n) * scale\n                xg = clip(x_best + z if x_best is not None else anchors[0]['center'] + z)\n                fg, xg = safe_eval(xg)\n                if fg is None:\n                    break\n                recent.insert(0, (xg.copy(), float(fg)))\n                if len(recent) > self.memory_size:\n                    recent.pop()\n                if fg < f_best - 1e-12:\n                    # improve: move worst-performing anchor slightly toward global winner\n                    worst_idx = int(np.argmin([ (a['cum_imp'] / (a['counts']+1e-12)) for a in anchors ]))\n                    anchors[worst_idx]['center'] = clip((1.0 - 0.4) * anchors[worst_idx]['center'] + 0.4 * xg)\n                    anchors[worst_idx]['radius'] = max(anchors[worst_idx]['radius'], np.linalg.norm(z) * 0.6)\n                    stagnation = 0\n                    successes.append((xg - anchors[worst_idx]['center']).copy())\n                    if len(successes) > self.memory_size:\n                        successes.pop(0)\n\n            # soft anchor replacement if some anchors very poor\n            if iterations % max(10, self.anchors_k * 2) == 0:\n                perf = np.array([ (a['cum_imp'] / (a['counts'] + 1e-12)) for a in anchors ])\n                # replace the bottom 20% anchors randomly with some probability\n                cutoff = np.percentile(perf, 25)\n                for i, a in enumerate(anchors):\n                    if perf[i] <= cutoff and np.random.rand() < 0.35:\n                        # reinitialize around best-known point if available\n                        center_seed = x_best if (x_best is not None and np.random.rand() < 0.8) else anchors[i]['center']\n                        anchors[i]['center'] = clip(center_seed + np.random.randn(n) * (0.6 * np.mean(ub - lb)))\n                        anchors[i]['radius'] = max(anchors[i]['radius'] * 0.8, self.init_radius * 0.7)\n                        anchors[i]['counts'] = 1\n                        anchors[i]['cum_imp'] = 0.0\n\n            # stagnation handling: soft restart around best if no progress\n            if stagnation >= stagnation_limit:\n                stagnation = 0\n                # create new anchors clustered near best with expanded radii\n                if x_best is not None:\n                    for i in range(len(anchors)):\n                        anchors[i]['center'] = clip(x_best + np.random.randn(n) * (1.2 * np.mean(ub - lb) * (0.4 + 0.6 * np.random.rand())))\n                        anchors[i]['radius'] = max(anchors[i]['radius'] * (1.6 + 0.8 * np.random.rand()), 1e-6)\n                        anchors[i]['counts'] = 1\n                        anchors[i]['cum_imp'] = 0.0\n                    # small local intensification around best\n                    local_tries = min(12, self.budget - evals)\n                    for _ in range(local_tries):\n                        if evals >= self.budget:\n                            break\n                        dx = np.random.randn(n) * (0.03 * np.mean(ub - lb))\n                        xt = clip(x_best + dx)\n                        ft, xt = safe_eval(xt)\n                        if ft is None:\n                            break\n                        recent.insert(0, (xt.copy(), float(ft)))\n                        if len(recent) > self.memory_size:\n                            recent.pop()\n\n            # keep recent length in check\n            if len(recent) > self.memory_size:\n                recent = recent[:self.memory_size]\n            if len(successes) > self.memory_size:\n                successes = successes[-self.memory_size:]\n\n            # termination if found extremely good solution\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm MALM scored 0.403 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "f2f29081-d1c2-4d76-a1c0-a17d70eca292", "operator": null, "metadata": {"aucs": [0.1574815014416363, 0.17266392539763842, 0.6459057214608658, 0.9140872876315527, 0.3475356444474338, 0.8334945324368019, 0.25123496902031095, 0.262460245750622, 0.27670508843530606, 0.172068304470688]}, "task_prompt": ""}
{"id": "72589b93-6472-4587-b010-b7b93eb7abf0", "fitness": 0.16831524804132192, "name": "ADEC", "description": "The algorithm maintains a low-rank directional ensemble D (p = ceil(n^0.6)) inside the [-5,5] box and performs searches mainly along these p orthonormal directions to bias exploration to a compact subspace. Each direction has online statistics (counts, an EMA of improvement rewards with alpha=0.2, step sizes initialized to 0.25*range_scale and adapted multiplicatively—grow ×1.25 on success, shrink ×0.70 on failure—and a directional variance) and arms are selected with a Thompson-style noisy sample (noise ∝ sqrt(log(1+total)/(1+counts)) scaled by exploration_scale) to balance exploration/exploitation. Local refinement uses mid-point curvature probing and a three-point parabolic fit around tested alphas, while successful moves are stored in recent_moves and used to soft-update directions or recombine when replacing poor arms; the basis is occasionally re-orthonormalized for stability. Budget-safe evaluations, bounds clipping, occasional curvature-aware global Gaussian jumps biased by dir_variances, and a stagnation/restart policy (soft restarts near the best, limited restarts, and final local polishing) ensure robustness and strict adherence to the evaluation budget.", "code": "import numpy as np\n\nclass ADEC:\n    \"\"\"\n    Adaptive Directional Ensemble with Curvature Estimation (ADEC)\n\n    Key ideas (differences to the provided BLRDS):\n    - Use a slightly different low-rank subspace size (p ~ n^0.6) to bias towards\n      fewer directions in high-dim problems.\n    - Use a Thompson-style noisy sampling for arm selection (sampled estimate of\n      expected improvement) instead of pure UCB.\n    - Multiplicative step-size adaptation with different growth/shrink rates.\n    - Exponential moving average (EMA) for per-arm reward estimates (robust to outliers).\n    - Parabolic refinement uses midpoint + endpoint evaluations (different alpha schedule).\n    - Replace bad arms by recombining recent successful moves with random directions.\n    - Maintain a simple directional variance vector to bias occasional global Gaussian jumps\n      towards promising subspace axes (curvature-aware exploration).\n    - Budget-respecting safe_eval and bounds clipping.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 subspace_exp=0.6, memory_size=10, exploration_scale=1.0):\n        \"\"\"\n        Parameters:\n        - budget: maximum number of allowed function evaluations\n        - dim: problem dimension\n        - seed: RNG seed for reproducibility (optional)\n        - subspace_exp: exponent to compute subspace size p = max(2, ceil(n^subspace_exp))\n                        (different from sqrt used in BLRDS)\n        - memory_size: how many recent successful move directions to remember\n        - exploration_scale: multiplies the exploration/noise used in arm selection\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.subspace_exp = float(subspace_exp)\n        self.memory_size = int(memory_size)\n        self.exploration_scale = float(exploration_scale)\n\n    def __call__(self, func):\n        n = self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initialize current point uniformly in domain\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, _ = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # subspace dimension p (different exponent than sqrt)\n        p = min(max(2, int(np.ceil(n ** self.subspace_exp))), n)\n\n        # initialize orthonormal directions (n x p)\n        R = np.random.randn(n, p)\n        Q, _ = np.linalg.qr(R)\n        D = Q[:, :p].copy()\n\n        # per-direction statistics: EMA reward, counts, step-sizes\n        counts = np.zeros(p, dtype=int)\n        ema_rewards = np.zeros(p, dtype=float)     # EMA of improvements\n        ema_alpha = 0.2                             # smoothing for EMA\n        range_scale = np.mean(ub - lb)\n        steps = np.full(p, 0.25 * range_scale)     # initial step sizes (different factor)\n        min_step = 1e-7 * max(1.0, range_scale)\n\n        # recent successful unit move directions\n        recent_moves = []\n\n        # additional controls\n        probes_per_round = max(3, p * 2)  # try a bit more probes than BLRDS\n        iteration = 0\n        rotate_every = max(15, p * 4)     # replace/rotate arms more frequently\n        stagnation = 0\n        stagnation_limit = max(10, int(6 + np.log(1 + n) * 3))\n        restarts = 0\n        max_restarts = 5\n\n        # directional variance (bias for global gaussian jumps)\n        dir_variances = np.ones(p, dtype=float) * (0.25 * range_scale)\n\n        # Thompson-style selection: sample noisy estimates based on EMA and counts\n        def select_arm_thompson():\n            total = counts.sum()\n            # Baseline exploration: if many arms untried, prefer them\n            untried = np.where(counts == 0)[0]\n            if len(untried) > 0 and np.random.rand() < 0.5:\n                return int(np.random.choice(untried))\n            # sample for each arm: mean = ema_rewards, variance ~ exploration_scale/(1+counts)\n            noise_std = self.exploration_scale * np.sqrt(np.log(1.0 + total + 1.0) / (1.0 + counts))\n            samples = ema_rewards + noise_std * np.random.randn(p)\n            return int(np.argmax(samples))\n\n        # simple parabolic fit from three points (alpha_left, alpha_mid, alpha_right)\n        def parabolic_refine(x0, d, a_left, f_left, a_mid, f_mid, a_right, f_right):\n            # Fit quadratic through (a_left,f_left), (a_mid,f_mid), (a_right,f_right)\n            # Use Lagrange interpolation formula to get vertex\n            a1, a2, a3 = a_left, a_mid, a_right\n            f1, f2, f3 = f_left, f_mid, f_right\n            denom = (a1 - a2) * (a1 - a3) * (a2 - a3)\n            if abs(denom) < 1e-18:\n                return None, None\n            A = ((f1 * (a2 - a3) + f2 * (a3 - a1) + f3 * (a1 - a2))) / denom\n            B = ((f1 * (a3 ** 2 - a2 ** 2) + f2 * (a1 ** 2 - a3 ** 2) + f3 * (a2 ** 2 - a1 ** 2))) / denom\n            if abs(A) < 1e-18:\n                return None, None\n            a_star = -B / (2.0 * A)\n            # clamp to a reasonable range around tested alphas\n            amin = min(a1, a2, a3) - 0.5 * abs(a2 - a1) - 0.5 * abs(a3 - a2)\n            amax = max(a1, a2, a3) + 0.5 * abs(a2 - a1) + 0.5 * abs(a3 - a2)\n            a_star = float(np.clip(a_star, amin, amax))\n            x_star = clip(x0 + a_star * d)\n            f_star, _ = safe_eval(x_star)\n            if f_star is None:\n                return None, None\n            return f_star, x_star\n\n        # main loop\n        while evals < self.budget:\n            iteration += 1\n            # number of probes is budget-aware\n            probes = min(probes_per_round, max(1, self.budget - evals))\n            improved_round = False\n\n            for _ in range(probes):\n                if evals >= self.budget:\n                    break\n\n                idx = select_arm_thompson()\n                d = D[:, idx].copy()\n                dn = np.linalg.norm(d)\n                if dn == 0 or np.isnan(dn):\n                    # regenerate direction\n                    v = np.random.randn(n)\n                    v /= (np.linalg.norm(v) + 1e-20)\n                    D[:, idx] = v\n                    d = D[:, idx].copy()\n\n                # sample alpha around current step but allow directional sign flip with probability\n                sign = 1.0 if np.random.rand() < 0.7 else -1.0\n                alpha = sign * steps[idx] * (0.6 + 0.8 * np.random.rand())  # in [0.6,1.4]*step, mostly forward\n                x_trial = clip(x_cur + alpha * d)\n                f_trial, x_ret = safe_eval(x_trial)\n                if f_trial is None:\n                    break\n                counts[idx] += 1\n\n                # also evaluate a midpoint for curvature estimation with small prob (or if improvement)\n                mid_alpha = 0.5 * alpha\n                x_mid = clip(x_cur + mid_alpha * d)\n                f_mid, _ = safe_eval(x_mid)\n                if f_mid is None:\n                    break\n                counts[idx] += 1\n\n                # compute improvement reward (positive improvements only)\n                reward = max(0.0, f_cur - f_trial)\n                # update EMA of rewards\n                ema_rewards[idx] = (1.0 - ema_alpha) * ema_rewards[idx] + ema_alpha * reward\n\n                # Accept improvement greedily\n                if f_trial < f_cur - 1e-12:\n                    x_prev = x_cur.copy()\n                    f_prev = f_cur\n                    x_cur = x_ret.copy()\n                    f_cur = f_trial\n                    improved_round = True\n                    stagnation = 0\n\n                    # record move direction\n                    move_vec = x_cur - x_prev\n                    mvnorm = np.linalg.norm(move_vec)\n                    if mvnorm > 0:\n                        unit_mv = move_vec / mvnorm\n                        recent_moves.insert(0, unit_mv.copy())\n                        if len(recent_moves) > self.memory_size:\n                            recent_moves.pop()\n                        # stronger soft-update of the direction than BLRDS\n                        gamma = 0.5\n                        D[:, idx] = (1 - gamma) * D[:, idx] + gamma * unit_mv\n                        D[:, idx] /= (np.linalg.norm(D[:, idx]) + 1e-20)\n\n                    # increase step moderately (different factor)\n                    steps[idx] = min(steps[idx] * 1.25, 6.0 * range_scale)\n\n                    # attempt parabolic refinement using left=alpha, mid=mid_alpha, right= -0.5*alpha (different schedule)\n                    # evaluate right point if budget allows\n                    right_alpha = -0.5 * alpha\n                    x_right = clip(x_cur + right_alpha * d)\n                    f_right, _ = safe_eval(x_right)\n                    if f_right is None:\n                        break\n                    counts[idx] += 1\n                    # Use the evaluations for parabolic interior points: (right_alpha, mid_alpha, alpha)\n                    res = parabolic_refine(x_cur, d, right_alpha, f_right, mid_alpha, f_mid, alpha, f_trial)\n                    if res is not None:\n                        f_par, x_par = res\n                        if f_par is not None and f_par < f_cur - 1e-12:\n                            # accept refined point\n                            mv = x_par - x_cur\n                            x_cur = x_par.copy()\n                            f_cur = f_par\n                            # mild adapt of direction\n                            mvn = np.linalg.norm(mv)\n                            if mvn > 0:\n                                unit_mv = mv / mvn\n                                D[:, idx] = (1 - 0.2) * D[:, idx] + 0.2 * unit_mv\n                                D[:, idx] /= (np.linalg.norm(D[:, idx]) + 1e-20)\n                            steps[idx] = min(steps[idx] * 1.10, 6.0 * range_scale)\n                            # update ema reward for this arm (bonus)\n                            ema_rewards[idx] = (1.0 - ema_alpha) * ema_rewards[idx] + ema_alpha * (f_prev - f_cur)\n\n                else:\n                    # no improvement: shrink step more aggressively (different rates)\n                    steps[idx] = max(steps[idx] * 0.70, min_step)\n                    stagnation += 1\n                    # decay ema_reward slightly to penalize unsuccessful arm\n                    ema_rewards[idx] = (1.0 - ema_alpha) * ema_rewards[idx]\n\n                # Occasional acceptance of small worsening moves with probability scaled by progress\n                if (not improved_round) and (np.random.rand() < 0.008):\n                    if f_trial <= f_cur + 1e-5:\n                        x_cur = x_ret.copy()\n                        f_cur = f_trial\n\n                # directional variance adaptation: increase variance for arm if it helped, decay otherwise\n                if ema_rewards[idx] > 0:\n                    dir_variances[idx] = min(dir_variances[idx] * 1.12, 2.0 * range_scale)\n                else:\n                    dir_variances[idx] = max(dir_variances[idx] * 0.92, 0.05 * min_step + 1e-12)\n\n                # occasional curvature-aware global jump: sample in subspace spanned by top-k arms\n                if (np.random.rand() < 0.025) and (self.budget - evals) > 0:\n                    # choose top arms by EMA reward\n                    k = max(1, min(p, int(np.ceil(0.25 * p))))\n                    top_idx = np.argsort(-ema_rewards)[:k]\n                    # construct Gaussian jump using combination of top directions\n                    coeffs = np.random.randn(k)\n                    coeffs = coeffs / (np.linalg.norm(coeffs) + 1e-20)\n                    jump = np.zeros(n)\n                    for j, c in enumerate(coeffs):\n                        jump += c * D[:, top_idx[j]]\n                    # scale jump by weighted average of dir_variances of chosen arms\n                    scale = np.mean(dir_variances[top_idx]) * (0.4 + 0.6 * np.random.rand())\n                    xg = clip(x_cur + jump * scale)\n                    fg, _ = safe_eval(xg)\n                    if fg is None:\n                        break\n                    if fg < f_cur - 1e-12:\n                        mv = xg - x_cur\n                        x_cur = xg.copy()\n                        f_cur = fg\n                        # push the worst arm toward this global move\n                        worst = int(np.argmin(ema_rewards + 1e-12))\n                        mvn = np.linalg.norm(mv)\n                        if mvn > 0:\n                            D[:, worst] = (1 - 0.3) * D[:, worst] + 0.3 * (mv / mvn)\n                            D[:, worst] /= (np.linalg.norm(D[:, worst]) + 1e-20)\n                            steps[worst] = max(steps[worst], 0.8 * scale)\n\n            # re-orthonormalize basis occasionally for stability\n            if iteration % 11 == 0:\n                Q, _ = np.linalg.qr(D)\n                D = Q[:, :p].copy()\n\n            # rotate/replace poor arms more frequently (different policy)\n            if iteration % rotate_every == 0:\n                # score arms by ema_rewards (lower is worse); prefer to replace bottom quarter\n                order = np.argsort(ema_rewards)\n                n_replace = max(1, p // 4)\n                for wi in order[:n_replace]:\n                    # combine random with one recent move sometimes\n                    rnd = np.random.randn(n)\n                    if recent_moves and (np.random.rand() < 0.7):\n                        candidate = 0.6 * rnd + 0.4 * recent_moves[np.random.randint(len(recent_moves))]\n                    else:\n                        candidate = rnd\n                    candidate /= (np.linalg.norm(candidate) + 1e-20)\n                    D[:, wi] = candidate\n                    counts[wi] = 0\n                    ema_rewards[wi] = 0.0\n                    steps[wi] = 0.25 * range_scale\n                    dir_variances[wi] = 0.25 * range_scale\n\n            # stagnation and restart policy (different thresholds and behavior)\n            if stagnation >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final local polishing: very small perturbations around best\n                    small = 0.01 * range_scale\n                    trials = min(12, self.budget - evals)\n                    for _ in range(trials):\n                        if evals >= self.budget:\n                            break\n                        xt = clip(x_best + np.random.randn(n) * small)\n                        ft, _ = safe_eval(xt)\n                        if ft is None:\n                            break\n                    # if no improvement after extended stagnation, exit\n                    if stagnation > stagnation_limit * 4:\n                        break\n                else:\n                    # soft restart: sample new current near best with moderate perturbation\n                    if x_best is not None:\n                        perturb = 0.5 * range_scale\n                        x_cur = clip(x_best + np.random.randn(n) * perturb)\n                        fc, _ = safe_eval(x_cur)\n                        if fc is None:\n                            break\n                        f_cur = fc\n                    # reinitialize half of directions randomly\n                    half = max(1, p // 2)\n                    for i in range(half):\n                        v = np.random.randn(n)\n                        v /= (np.linalg.norm(v) + 1e-20)\n                        D[:, i] = v\n                        counts[i] = 0\n                        ema_rewards[i] = 0.0\n                        steps[i] = min(2.0 * range_scale, steps[i] * 1.8)\n                        dir_variances[i] = 0.5 * range_scale\n                    stagnation = 0\n\n            # keep steps lower-bounded\n            steps = np.maximum(steps, min_step)\n\n            # early exit if extremely good\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ADEC scored 0.168 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "f2f29081-d1c2-4d76-a1c0-a17d70eca292", "operator": null, "metadata": {"aucs": [0.06440130122072185, 0.15879854175109953, 0.3089697184072081, 0.18348549578313644, 0.17160667421609777, 0.14299219473625946, 0.17323829081077402, 0.2023417812555155, 0.14702658597142082, 0.1302918962609858]}, "task_prompt": ""}
{"id": "122adab0-0a9c-4384-8b28-cbece946e202", "fitness": 0.15444419634316695, "name": "ASE_ER", "description": "The algorithm maintains a low-rank orthonormal search basis D (default p ≈ ceil(√dim)) and treats each basis vector as an \"arm\" with Thompson-sampling style selection using per-arm counts, sum_impr and sumsq_impr to estimate potential improvements. Each arm has an adaptive step distribution (mu_step initialized ≈0.28·range, sigma_step ≈0.22·range) that is updated on successes and shrunk on failures, with a tiny floor to avoid collapse. A decaying rank‑1 curvature accumulator C (beta_curv≈0.15) plus a recent_moves memory guide anisotropic global proposals, entropy‑guided resampling/reinitialization of low‑potential arms, and soft restarts when stagnation occurs, while occasional Metropolis-like uphill acceptance and curvature‑guided jumps help escape traps. Local parabolic refinement along accepted directions and final budget‑aware polishing around the best point are used for efficient local improvement, and all evaluations are clipped to the problem bounds and strictly budget‑limited.", "code": "import numpy as np\n\nclass ASE_ER:\n    \"\"\"\n    Adaptive Subspace Evolution with Entropy-Guided Resampling (ASE-ER)\n\n    Key ideas implemented:\n    - Maintain a low-rank orthonormal basis D (n x p) of directional \"arms\".\n    - Use a Thompson-sampling style selection: each arm keeps statistics of observed\n      improvements; selection samples a candidate score from an approx. posterior.\n    - Each arm maintains an adaptive step-size distribution (mu_step, sigma_step).\n    - On successful moves, update step distributions and gently update the basis\n      direction toward the successful move. Keep a small recent_moves memory.\n    - Maintain a low-rank curvature accumulator C (n x n, rank-1 updates decayed)\n      used to construct anisotropic proposals for resampling/restarts.\n    - Replace low-entropy / low-potential arms periodically by draws guided by\n      curvature and memory (\"entropy-guided resampling\").\n    - Allow occasional global curvature-guided jumps and small probabilistic uphill\n      acceptance (Metropolis-like) to escape shallow traps.\n    - All evaluations clipped to bounds and strictly budget-aware.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 subspace_size=None, memory_size=10, beta_curv=0.15,\n                 entropy_replace_frac=0.25, ucb_like_beta=1.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        # basis size\n        self.p = int(subspace_size) if subspace_size is not None else max(2, int(np.ceil(np.sqrt(self.dim))))\n        self.memory_size = int(memory_size)\n        self.beta_curv = float(beta_curv)              # curvature decay / update strength\n        self.entropy_replace_frac = float(entropy_replace_frac)\n        self.ucb_like_beta = float(ucb_like_beta)      # controls exploration in sampling\n        # minimal step floor multiplier\n        self._min_step_mul = 1e-8\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds from func or default [-5,5]\n        lb = np.asarray(getattr(func, 'bounds', type('b', (), {'lb': -5.0})).lb, dtype=float)\n        ub = np.asarray(getattr(func, 'bounds', type('b', (), {'ub':  5.0})).ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = f\n                x_best = x.copy()\n            return f, x\n\n        # initialize point\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, _ = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # initialize orthonormal basis D (n x p)\n        p = min(self.p, n)\n        R = np.random.randn(n, p)\n        Q, _ = np.linalg.qr(R)\n        D = Q[:, :p].copy()\n\n        # per-arm statistics for Thompson-like selection\n        counts = np.zeros(p, dtype=int)\n        sum_impr = np.zeros(p, dtype=float)   # sum of positive improvements observed\n        sumsq_impr = np.zeros(p, dtype=float) # sum of squared improvements (for variance)\n        # per-arm step distribution params (mean and std)\n        range_mean = float(np.mean(ub - lb))\n        mu_step = np.full(p, 0.28 * range_mean)\n        sigma_step = np.full(p, 0.22 * range_mean)\n\n        min_step = self._min_step_mul * max(1.0, range_mean)\n\n        # curvature accumulator (symmetric matrix) approximated as dense but decayed\n        # for computational simplicity; used to shape global proposals\n        C = 1e-6 * np.eye(n)\n\n        recent_moves = []\n\n        # control params\n        rotate_every = max(20, p * 6)\n        iteration = 0\n        stagnation = 0\n        stagn_limit = max(12, 6 + int(np.log(1 + n) * 3))\n        restarts = 0\n        max_restarts = 5\n\n        # helper: compute arm posterior sample (Thompson-like)\n        def sample_arm_score(i):\n            # posterior mean ~ avg improvement; posterior std ~ empirical std / sqrt(counts+1)\n            if counts[i] == 0:\n                # optimistic prior: encourage exploration, give wide variance\n                prior_mean = 0.1 * range_mean\n                prior_var = (0.8 * range_mean) ** 2\n                return np.random.normal(prior_mean, np.sqrt(prior_var))\n            mu = sum_impr[i] / max(1, counts[i])\n            # sample variance (unbiased estimate)\n            if counts[i] > 1:\n                var = max(1e-12, (sumsq_impr[i] - (sum_impr[i]**2)/counts[i]) / (counts[i]-1))\n            else:\n                var = (0.5 * range_mean) ** 2\n            # posterior stdev ~ sqrt(var / counts) tempered by ucb_like_beta\n            post_std = np.sqrt(var / (counts[i])) * (1.0 + 1.0/self.ucb_like_beta)\n            post_std = max(post_std, 1e-8 * range_mean)\n            return np.random.normal(mu, post_std)\n\n        # helper: arm entropy-like measure from step distribution (higher sigma -> higher entropy)\n        def arm_entropy(i):\n            s = max(sigma_step[i], 1e-12)\n            return 0.5 * np.log(2 * np.pi * np.e * s * s)\n\n        # parabolic refine (three-point quick fit) along direction d around alpha0\n        def refine_parabola(x0, d, alpha0):\n            # pick three alphas centered around alpha0\n            a = alpha0\n            deltas = np.array([-0.6, 0.0, 0.6]) * max(1.0, abs(a)) * 0.5\n            alphas = a + deltas\n            fvals = []\n            pts = []\n            for a_i in alphas:\n                xt = clip(x0 + a_i * d)\n                f_i, _ = safe_eval(xt)\n                if f_i is None:\n                    return None, None, None\n                fvals.append(f_i); pts.append(xt.copy())\n            # fit parabola in alpha: f = A a^2 + B a + C, get vertex\n            a1, a2, a3 = alphas\n            f1, f2, f3 = fvals\n            denom = (a1 - a2)*(a1 - a3)*(a2 - a3)\n            if abs(denom) < 1e-18:\n                return None, None, None\n            A = ((f1*(a2 - a3) + f2*(a3 - a1) + f3*(a1 - a2))) / denom\n            B = ((f1*(a3*a3 - a2*a2) + f2*(a1*a1 - a3*a3) + f3*(a2*a2 - a1*a1))) / denom\n            if abs(A) < 1e-18:\n                return None, None, None\n            a_star = -B / (2 * A)\n            # clamp reasonably\n            low = min(alphas) - 2.0 * max(1.0, abs(min(alphas)))\n            high = max(alphas) + 2.0 * max(1.0, abs(max(alphas)))\n            a_star = float(np.clip(a_star, low, high))\n            x_star = clip(x0 + a_star * d)\n            f_star, _ = safe_eval(x_star)\n            if f_star is None:\n                return None, None, None\n            return f_star, x_star, a_star\n\n        # main loop\n        while evals < self.budget:\n            iteration += 1\n\n            # select arm via Thompson-like sampling (sampled potential improvement)\n            samples = np.array([sample_arm_score(i) for i in range(p)])\n            arm_idx = int(np.argmax(samples))\n\n            d = D[:, arm_idx].copy()\n            dn = np.linalg.norm(d)\n            if dn < 1e-20:\n                d = np.random.randn(n)\n                d /= (np.linalg.norm(d) + 1e-20)\n                D[:, arm_idx] = d.copy()\n\n            # propose alpha from arm's step distribution (truncated to bounds via clip on x)\n            a = np.random.normal(mu_step[arm_idx], sigma_step[arm_idx])\n            # if step tiny, draw from small uniform\n            if abs(a) < 1e-12:\n                a = mu_step[arm_idx] * (0.2 + 0.8 * np.random.rand())\n\n            x_try = clip(x_cur + a * d)\n            f_try, x_ret = safe_eval(x_try)\n            if f_try is None:\n                break\n\n            # compute improvement (can be negative)\n            impro = (f_cur - f_try)\n            # record stats: only count positive improvements in sum_impr for bias toward improvement\n            counts[arm_idx] += 1\n            if impro > 0:\n                sum_impr[arm_idx] += impro\n                sumsq_impr[arm_idx] += impro * impro\n            else:\n                # keep a tiny pseudo-observation to stabilize variance estimate\n                sumsq_impr[arm_idx] += 0.0\n\n            accepted = False\n\n            if f_try < f_cur - 1e-12:\n                # accept improvement\n                x_prev = x_cur.copy()\n                f_prev = f_cur\n                x_cur = x_ret.copy()\n                f_cur = f_try\n                accepted = True\n                stagnation = 0\n\n                # update recent moves and basis direction gently toward move\n                mv = x_cur - x_prev\n                mvn = np.linalg.norm(mv)\n                if mvn > 0:\n                    umv = mv / mvn\n                    recent_moves.insert(0, umv.copy())\n                    if len(recent_moves) > self.memory_size:\n                        recent_moves.pop()\n                    # soft adjust direction\n                    gamma = 0.25\n                    D[:, arm_idx] = (1 - gamma) * D[:, arm_idx] + gamma * umv\n                    D[:, arm_idx] /= (np.linalg.norm(D[:, arm_idx]) + 1e-20)\n\n                    # update curvature matrix with rank-1 outer product (normalized)\n                    u = umv.reshape(-1, 1)\n                    C = (1 - self.beta_curv) * C + self.beta_curv * (u @ u.T)\n\n                # adapt step distribution toward observed magnitude\n                obs_step = abs(a)\n                alpha = 0.20\n                mu_step[arm_idx] = (1 - alpha) * mu_step[arm_idx] + alpha * obs_step\n                # update sigma as EWMA of deviation\n                sigma_step[arm_idx] = np.sqrt(max(1e-12, (1 - alpha) * (sigma_step[arm_idx]**2) + alpha * ((obs_step - mu_step[arm_idx])**2 + 1e-12)))\n                # modestly increase sigma to encourage useful exploration\n                sigma_step[arm_idx] = min(sigma_step[arm_idx] * 1.08, 5.0 * range_mean)\n\n                # try cheap parabolic refine around this direction if budget allows\n                remaining = self.budget - evals\n                if remaining >= 2:\n                    f_par, x_par, a_par = refine_parabola(x_prev, D[:, arm_idx], a)\n                    if f_par is not None and f_par < f_cur - 1e-12:\n                        # accept refined\n                        x_cur = x_par.copy()\n                        f_cur = f_par\n                        # update step stats again toward refined magnitude\n                        obs_step2 = abs(a_par)\n                        mu_step[arm_idx] = (1 - 0.18) * mu_step[arm_idx] + 0.18 * obs_step2\n                        sigma_step[arm_idx] = np.sqrt(max(1e-12, (1 - 0.18) * sigma_step[arm_idx]**2 + 0.18 * (obs_step2 - mu_step[arm_idx])**2))\n\n            else:\n                # not improving: small downhill acceptance may happen via Metropolis-like rule\n                stagnation += 1\n                # shrink step\n                sigma_step[arm_idx] = max(sigma_step[arm_idx] * 0.82, min_step)\n                mu_step[arm_idx] = max(mu_step[arm_idx] * 0.92, min_step)\n\n                # small probabilistic uphill acceptance to escape\n                if np.random.rand() < 0.02:\n                    # temperature decays with iteration and stagnation\n                    T = 0.05 * range_mean / (1.0 + 0.01 * iteration + 0.05 * stagnation)\n                    delta = f_try - f_cur\n                    prob = np.exp(-max(0.0, delta) / (T + 1e-12))\n                    if np.random.rand() < prob:\n                        x_cur = x_ret.copy()\n                        f_cur = f_try\n                        accepted = True\n                        stagnation = 0\n\n            # if we accepted a move, reward is positive improvement; otherwise zero reward\n            # (sum_impr and sumsq handled above)\n            # occasional global curvature-guided jump\n            if np.random.rand() < 0.035 and (self.budget - evals) > 0:\n                # sample from mvn with covariance proportional to C plus isotropic\n                # construct low-rank-ish covariance: use eigenvector approximation via power iteration for one principal vector\n                try:\n                    # approximate top eigenvector by power iteration\n                    v = np.random.randn(n)\n                    v /= (np.linalg.norm(v) + 1e-20)\n                    for _ in range(3):\n                        v = C @ v\n                        v /= (np.linalg.norm(v) + 1e-20)\n                    lam = float((v @ (C @ v)))\n                    cov = 0.5 * range_mean * (lam + 1e-8) * np.eye(n) + 0.2 * range_mean * np.outer(v, v)\n                    z = np.random.multivariate_normal(np.zeros(n), cov)\n                except Exception:\n                    z = np.random.randn(n) * (0.6 * range_mean)\n                xg = clip(x_best + z if x_best is not None else x_cur + z)\n                fg, _ = safe_eval(xg)\n                if fg is None:\n                    break\n                if fg < f_cur - 1e-12:\n                    # accept global move and push worst arm toward this direction\n                    mv = xg - x_cur\n                    x_cur = xg.copy(); f_cur = fg\n                    worst = int(np.argmin(sum_impr / (counts + 1e-6)))\n                    mvn = np.linalg.norm(mv)\n                    if mvn > 0:\n                        D[:, worst] = 0.6 * D[:, worst] + 0.4 * (mv / mvn)\n                        D[:, worst] /= (np.linalg.norm(D[:, worst]) + 1e-20)\n                        mu_step[worst] = max(mu_step[worst], 0.8 * np.linalg.norm(z))\n                        sigma_step[worst] = max(sigma_step[worst], 0.6 * np.linalg.norm(z))\n                        counts[worst] = 0\n                        sum_impr[worst] = 0.0\n                        sumsq_impr[worst] = 0.0\n\n            # periodic orthonormalization for numerical stability\n            if iteration % 9 == 0:\n                Q, _ = np.linalg.qr(D)\n                D = Q[:, :p].copy()\n\n            # entropy-guided resampling/rotation of poor arms\n            if iteration % rotate_every == 0:\n                entropies = np.array([arm_entropy(i) for i in range(p)])\n                # lower entropy arms are too \"cold\" (exploitative); also consider low mean improvement\n                mean_impr = np.zeros(p)\n                nonzero = counts > 0\n                mean_impr[nonzero] = sum_impr[nonzero] / counts[nonzero]\n                # combined score: prioritise replacing arms with low (entropy + positive potential)\n                combined = (entropies / (np.max(entropies) + 1e-12)) + 0.8 * (mean_impr / (np.max(np.abs(mean_impr)) + 1e-12))\n                # lower combined means candidates for replacement\n                idx_replace = np.argsort(combined)[:max(1, int(np.ceil(self.entropy_replace_frac * p)))]\n                for ri in idx_replace:\n                    # build candidate from recent moves + curvature principal direction + random\n                    cand = 0.4 * np.random.randn(n)\n                    if len(recent_moves) > 0 and np.random.rand() < 0.7:\n                        cand += 0.7 * recent_moves[np.random.randint(len(recent_moves))]\n                    # add curvature principal direction if exists\n                    try:\n                        v = np.linalg.eigvalsh(C)[-1]\n                        # if curvature trivial, ignore\n                    except Exception:\n                        v = None\n                    candidate = cand\n                    candidate /= (np.linalg.norm(candidate) + 1e-20)\n                    D[:, ri] = candidate\n                    counts[ri] = 0\n                    sum_impr[ri] = 0.0\n                    sumsq_impr[ri] = 0.0\n                    mu_step[ri] = 0.3 * range_mean\n                    sigma_step[ri] = 0.25 * range_mean\n\n            # stagnation and soft restart policy\n            if stagnation >= stagn_limit:\n                restarts += 1\n                stagnation = 0\n                if restarts > max_restarts:\n                    # final polishing: local gaussian probes around best with tiny scale\n                    sigma_local = 0.02 * range_mean\n                    local_tries = min(20, self.budget - evals)\n                    for _ in range(local_tries):\n                        if evals >= self.budget:\n                            break\n                        xt = clip(x_best + np.random.randn(n) * sigma_local)\n                        ft, _ = safe_eval(xt)\n                        if ft is None:\n                            break\n                    break  # end optimization after polishing\n                else:\n                    # soft restart: sample new x_cur near x_best guided by curvature\n                    if x_best is not None:\n                        try:\n                            # sample from covariance ~ C + isotropic\n                            cov = C + (0.2 * range_mean) * np.eye(n)\n                            z = np.random.multivariate_normal(np.zeros(n), cov)\n                        except Exception:\n                            z = np.random.randn(n) * (0.4 * range_mean)\n                        x_new = clip(x_best + z)\n                        f_new, _ = safe_eval(x_new)\n                        if f_new is None:\n                            break\n                        x_cur = x_new.copy(); f_cur = f_new\n                    # reinitialize half of basis for exploration\n                    for i in range(max(1, p // 2)):\n                        v = np.random.randn(n)\n                        v /= (np.linalg.norm(v) + 1e-20)\n                        if len(recent_moves) > 0 and np.random.rand() < 0.5:\n                            v = 0.6 * v + 0.4 * recent_moves[np.random.randint(len(recent_moves))]\n                            v /= (np.linalg.norm(v) + 1e-20)\n                        D[:, i] = v\n                        counts[i] = 0\n                        sum_impr[i] = 0.0\n                        sumsq_impr[i] = 0.0\n                        mu_step[i] = 0.35 * range_mean\n                        sigma_step[i] = 0.3 * range_mean\n\n            # floor steps\n            mu_step = np.maximum(mu_step, min_step)\n            sigma_step = np.maximum(sigma_step, min_step)\n\n            # quick termination if good enough\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ASE_ER scored 0.154 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "f2f29081-d1c2-4d76-a1c0-a17d70eca292", "operator": null, "metadata": {"aucs": [0.09466547916316193, 0.14819437067976493, 0.23124500224431688, 0.13474311417564666, 0.10973553455564111, 0.22299939362309784, 0.17481155333947218, 0.18281540233710603, 0.12757309306946296, 0.11765902024399877]}, "task_prompt": ""}
{"id": "612fea34-5dd2-4561-8ea9-9c44405c994c", "fitness": 0.17895718957426637, "name": "ARSS", "description": "The algorithm maintains a low–rank orthonormal search subspace D (p ≈ max(2, ceil(n^(1/3)))) that is re‑orthonormalized and periodically rotates/replaces the weakest directions using EWMA reward, counts and a small memory of recent successful moves to keep directions relevant. Arms (directions) are chosen by a soft bandit: a softmax over EWMA rewards plus an exploration bonus (temperature ~0.6), with per‑arm Gaussian step lengths and multiplicative step‑size adaptation (grow=1.5 on success, shrink=0.6 on failure, initial step ≈0.25·range, bounded by min/max). Successful probes produce momentum‑style soft updates of the chosen basis vector (learn_rate ~0.40, small retention), and the optimizer uses local enhancements: opposition sampling (≈15%), cheap quadratic/parabolic refinement from three nearby alphas, occasional global Gaussian jumps (≈4%), and tiny simulated‑annealing acceptance to escape plateaus. All evaluations go through safe_eval which enforces bounds and the budget, EWMA smoothing (α≈0.20) drives online arm quality, and a stagnation counter triggers gentle soft restarts and a final small‑step polishing phase (limited restarts) to recover from stagnation.", "code": "import numpy as np\n\nclass ARSS:\n    \"\"\"\n    Adaptive Rotating Subspace Search (ARSS)\n\n    - Maintain a low-rank orthonormal basis of p directions (p ≈ max(2, ceil(n^(1/3)))).\n    - Select arms via a softmax over an EWMA reward + exploration term (soft bandit).\n    - Per-arm multiplicative step-size adaptation (grow on success, shrink on failure).\n    - Soft (momentum-like) updates of direction vectors toward successful moves.\n    - Cheap quadratic refinement (poly fit of degree 2) when budget allows.\n    - Occasional opposition sampling, global Gaussian jumps, and partial/soft restarts.\n    - All function calls are guarded by safe_eval that respects the budget and bounds.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 subspace_size=None, memory_size=12,\n                 temp=0.6, grow=1.5, shrink=0.6, learn_rate=0.40):\n        \"\"\"\n        budget: maximum number of function evaluations\n        dim: dimensionality of the problem\n        seed: optional RNG seed\n        subspace_size: optional number of basis directions p\n        memory_size: number of recent successful move directions to retain\n        temp: softmax temperature for bandit sampling (lower -> greedier)\n        grow: multiplicative step increase on success\n        shrink: multiplicative step decrease on failure\n        learn_rate: how strongly to push a basis direction toward a successful move\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # choose p differently than the original: use cube root scaling (smaller subspace)\n        self.p = subspace_size if subspace_size is not None else max(2, int(np.ceil(self.dim ** (1.0 / 3.0))))\n        self.memory_size = int(memory_size)\n        self.temp = float(max(1e-6, temp))\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.learn_rate = float(learn_rate)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds (Many Affine BBOB typically uses [-5,5])\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initialize current point\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, _ = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # build orthonormal basis (n x p)\n        p = min(self.p, n)\n        R = np.random.randn(n, p)\n        Q, _ = np.linalg.qr(R)\n        D = Q[:, :p].copy()\n\n        # per-arm stats: counts and EWMA rewards (not cumulative), and steps\n        counts = np.zeros(p, dtype=int)\n        ewma = np.zeros(p, dtype=float)  # exponentially-weighted mean reward\n        ewma_alpha = 0.20  # how fast EWMA reacts to new rewards (different paramization)\n        steps = np.full(p, 0.25 * np.mean(ub - lb))  # starting step smaller than original\n        max_step = 6.0 * np.mean(ub - lb)\n        min_step = 1e-9 * max(1.0, np.mean(ub - lb))\n\n        recent_moves = []\n\n        # control params\n        max_probes_per_round = max(2, 2 * p)\n        stagnation_iters = 0\n        stagnation_limit = max(10, int(6 + np.log(1 + n)))\n        restarts = 0\n        max_restarts = 5\n        rotate_every = max(10, p * 3)\n        iteration = 0\n\n        def softmax_sample(scores, temperature):\n            # numerical stable softmax sampling\n            scaled = scores / float(temperature)\n            mx = np.max(scaled)\n            ex = np.exp(scaled - mx)\n            probs = ex / (np.sum(ex) + 1e-20)\n            return np.random.choice(len(scores), p=probs)\n\n        # simple quadratic vertex fit using numpy.polyfit degree 2\n        def quad_vertex(alpha_vals, f_vals, x0, d):\n            if len(alpha_vals) != len(f_vals) or len(alpha_vals) < 3:\n                return None, None, None\n            try:\n                coefs = np.polyfit(alpha_vals, f_vals, 2)  # [A, B, C]\n                A, B = coefs[0], coefs[1]\n                if abs(A) < 1e-16:\n                    return None, None, None\n                alpha_star = -B / (2.0 * A)\n            except Exception:\n                return None, None, None\n            # clamp alpha_star to somewhat reasonable range near samples\n            low = min(alpha_vals) - 2.0 * max(1.0, abs(min(alpha_vals)))\n            high = max(alpha_vals) + 2.0 * max(1.0, abs(max(alpha_vals)))\n            alpha_star = float(np.clip(alpha_star, low, high))\n            x_star = clip(x0 + alpha_star * d)\n            f_star, _ = safe_eval(x_star)\n            if f_star is None:\n                return None, None, None\n            return f_star, x_star, alpha_star\n\n        # main optimization loop\n        while evals < self.budget:\n            iteration += 1\n            probes = min(max_probes_per_round, max(1, (self.budget - evals)))\n            improved_in_round = False\n\n            for _probe in range(probes):\n                if evals >= self.budget:\n                    break\n\n                # form per-arm scores = ewma_reward + exploration / sqrt(1+counts)\n                exploration = 0.6 * np.mean(ub - lb)\n                scores = ewma + exploration / (np.sqrt(1.0 + counts))\n                # sample an index from softmax of scores (temperature controls greediness)\n                idx = softmax_sample(scores, self.temp)\n\n                # ensure direction normalized\n                d = D[:, idx]\n                dn = np.linalg.norm(d)\n                if dn == 0:\n                    v = np.random.randn(n)\n                    v /= (np.linalg.norm(v) + 1e-20)\n                    D[:, idx] = v\n                    d = D[:, idx]\n\n                # sample alpha from gaussian (centered) with scale = steps[idx]\n                alpha0 = np.random.normal(loc=0.0, scale=steps[idx])\n                if abs(alpha0) < 1e-16:\n                    alpha0 = steps[idx] * (0.4 - np.random.rand() * 0.8)\n\n                x_try = clip(x_cur + alpha0 * d)\n                f_try, x_ret = safe_eval(x_try)\n                if f_try is None:\n                    break\n                counts[idx] += 1\n\n                # occasional opposition sample with different scaling\n                if np.random.rand() < 0.15 and evals < self.budget:\n                    alpha_op = -alpha0 * (0.6 + 0.4 * np.random.rand())\n                    x_op = clip(x_cur + alpha_op * d)\n                    f_op, _ = safe_eval(x_op)\n                    if f_op is None:\n                        break\n                    counts[idx] += 1\n                    if f_op < f_try:\n                        f_try = f_op\n                        x_ret = clip(x_cur + alpha_op * d)\n\n                # reward = improvement, update EWMA\n                reward = max(0.0, f_cur - f_try)\n                ewma[idx] = (1.0 - ewma_alpha) * ewma[idx] + ewma_alpha * reward\n\n                if f_try < f_cur - 1e-12:\n                    # accept improvement\n                    x_prev = x_cur.copy()\n                    f_prev = f_cur\n                    x_cur = x_ret.copy()\n                    f_cur = f_try\n                    improved_in_round = True\n                    stagnation_iters = 0\n\n                    # record move direction\n                    move_vec = x_cur - x_prev\n                    mvnorm = np.linalg.norm(move_vec)\n                    if mvnorm > 0:\n                        unit_move = move_vec / mvnorm\n                        recent_moves.insert(0, unit_move.copy())\n                        if len(recent_moves) > self.memory_size:\n                            recent_moves.pop()\n\n                        # soft (momentum-like) update towards the move plus a small retention of previous direction\n                        gamma = self.learn_rate\n                        retention = 0.15\n                        D[:, idx] = (1.0 - gamma - retention) * D[:, idx] + gamma * unit_move + retention * (D[:, idx])\n                        D[:, idx] /= (np.linalg.norm(D[:, idx]) + 1e-20)\n\n                    # increase step more aggressively than original to explore faster\n                    steps[idx] = min(steps[idx] * self.grow, max_step)\n\n                    # cheap quadratic refinement if budget available: probe two additional nearby alphas\n                    remaining = self.budget - evals\n                    if remaining >= 2:\n                        # choose three alpha sample points centered near alpha0\n                        a_center = alpha0\n                        a1 = a_center * 0.6\n                        a2 = 0.0\n                        a3 = a_center * 1.6\n                        alpha_vals = [a1, a2, a3]\n                        f_vals = []\n                        ok = True\n                        for a in alpha_vals:\n                            xa = clip(x_prev + a * d)\n                            fa, _ = safe_eval(xa)\n                            if fa is None:\n                                ok = False\n                                break\n                            f_vals.append(fa)\n                        if ok and len(f_vals) == 3:\n                            f_par, x_par, a_par = quad_vertex(alpha_vals, f_vals, x_prev, d)\n                            if f_par is not None and f_par < f_cur - 1e-12:\n                                # accept parabolic improvement\n                                x_cur = x_par.copy()\n                                f_cur = f_par\n                                mv = x_cur - x_prev\n                                mn = np.linalg.norm(mv)\n                                if mn > 0:\n                                    unit_mv = mv / mn\n                                    # milder update from parabolic refinement\n                                    D[:, idx] = (1.0 - 0.28) * D[:, idx] + 0.28 * unit_mv\n                                    D[:, idx] /= (np.linalg.norm(D[:, idx]) + 1e-20)\n                                steps[idx] = min(steps[idx] * 1.22, max_step)\n\n                else:\n                    # failed probe: shrink step\n                    steps[idx] = max(steps[idx] * self.shrink, min_step)\n                    stagnation_iters += 1\n\n                # tiny probability to accept a slightly worse move if near-equal (simulated annealing style)\n                if (not improved_in_round) and (np.random.rand() < 0.008) and (f_try is not None):\n                    if f_try <= f_cur + 1e-7:\n                        x_cur = x_ret.copy()\n                        f_cur = f_try\n\n                # occasional global Gaussian exploration with larger sigma\n                if np.random.rand() < 0.04 and (self.budget - evals) > 0:\n                    sigma = 0.7 * np.mean(ub - lb) * (0.4 + 0.9 * np.random.rand())\n                    xg = clip(x_cur + np.random.randn(n) * sigma)\n                    fg, _ = safe_eval(xg)\n                    if fg is None:\n                        break\n                    if fg < f_cur - 1e-12:\n                        mv = xg - x_cur\n                        x_cur = xg.copy()\n                        f_cur = fg\n                        # adapt one of the weaker directions towards this global successful move\n                        worst_idx = int(np.argmin(ewma + 1e-12))\n                        mvn = np.linalg.norm(mv)\n                        if mvn > 0:\n                            D[:, worst_idx] = (1.0 - 0.30) * D[:, worst_idx] + 0.30 * (mv / mvn)\n                            D[:, worst_idx] /= (np.linalg.norm(D[:, worst_idx]) + 1e-20)\n                            steps[worst_idx] = max(steps[worst_idx], 0.85 * sigma)\n\n            # re-orthonormalize occasionally for numerical stability\n            if iteration % 9 == 0:\n                try:\n                    Q, _ = np.linalg.qr(D)\n                    D = Q[:, :p].copy()\n                except Exception:\n                    # if QR fails for some reason, re-generate a few random directions\n                    for i in range(p):\n                        v = np.random.randn(n)\n                        D[:, i] = v / (np.linalg.norm(v) + 1e-20)\n\n            # rotate / replace poor directions periodically, but with different logic than original\n            if iteration % rotate_every == 0:\n                # rank arms by EWMA reward per attempt (fallback to random for untried)\n                score = np.copy(ewma)\n                untried = (counts == 0)\n                if np.any(untried):\n                    # give small random bonus to untried to encourage testing\n                    score[untried] += 0.5 * np.random.rand(np.sum(untried))\n                worst = np.argsort(score)[:max(1, p // 3)]\n                for wi in worst:\n                    rnd = np.random.randn(n)\n                    if len(recent_moves) > 0 and np.random.rand() < 0.65:\n                        candidate = 0.6 * rnd + 0.4 * recent_moves[np.random.randint(len(recent_moves))]\n                    else:\n                        candidate = rnd\n                    candidate /= (np.linalg.norm(candidate) + 1e-20)\n                    D[:, wi] = candidate\n                    counts[wi] = 0\n                    ewma[wi] = 0.0\n                    steps[wi] = 0.22 * np.mean(ub - lb)\n\n            # stagnation and soft restart policy (gentler than original)\n            if stagnation_iters >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final polishing: very small steps local search around best\n                    steps = np.maximum(steps * 0.30, min_step)\n                    local_tries = min(12, self.budget - evals)\n                    for _ in range(local_tries):\n                        if evals >= self.budget:\n                            break\n                        dx = np.random.randn(n) * (0.015 * np.mean(ub - lb))\n                        xt = clip(x_best + dx)\n                        ft, _ = safe_eval(xt)\n                        if ft is None:\n                            break\n                    # if still stuck, break\n                    if stagnation_iters > stagnation_limit * 3:\n                        break\n                else:\n                    # soft restart: sample around best if available, refresh some directions\n                    if x_best is not None:\n                        perturb_scale = 0.45 * np.mean(ub - lb)\n                        x_new = clip(x_best + np.random.randn(n) * perturb_scale)\n                        fc, _ = safe_eval(x_new)\n                        if fc is None:\n                            break\n                        x_cur = x_new.copy()\n                        f_cur = fc\n                    # refresh half of the directions randomly but keep some from recent_moves\n                    for i in range(max(1, p // 2)):\n                        if len(recent_moves) > 0 and np.random.rand() < 0.5:\n                            cand = 0.7 * np.random.randn(n) + 0.3 * recent_moves[np.random.randint(len(recent_moves))]\n                        else:\n                            cand = np.random.randn(n)\n                        cand /= (np.linalg.norm(cand) + 1e-20)\n                        D[:, i] = cand\n                        counts[i] = 0\n                        ewma[i] = 0.0\n                        steps[i] = min(2.0 * np.mean(ub - lb), steps[i] * 1.8)\n                    stagnation_iters = 0\n\n            # keep steps bounded\n            steps = np.maximum(steps, min_step)\n\n            # quick termination if near-optimal\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ARSS scored 0.179 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "f2f29081-d1c2-4d76-a1c0-a17d70eca292", "operator": null, "metadata": {"aucs": [0.02048666667015786, 0.14720252078012708, 0.3070767098558401, 0.1649752502969125, 0.1353907925583654, 0.3291461123687969, 0.21054699226203422, 0.18351727123887784, 0.1828814129361349, 0.10834816677541681]}, "task_prompt": ""}
{"id": "8f7a20ab-ba91-49a5-8b81-1e8664115b04", "fitness": 0.24938025060987862, "name": "HSBS", "description": "The HSBS heuristic is a hybrid search that maintains a low-rank orthonormal set of p directions (p = max(2, ceil(sqrt(n))) by default) treated as bandit “arms” with per-arm counts, rewards and adaptive step sizes (initial steps = 0.35 * range, grow by ≈×1.18 on success, shrink by ×0.80 on failure, and lower-bounded by min_step), and selects arms via UCB (avg reward + ucb_beta * sqrt(log(total)/counts)). It augments bandit-driven single-direction probes (chosen ≈72% of the time, with occasional opposite probes at ≈14% and rare global Gaussian jumps ≈3%) with occasional random k‑dim subspace probes (k≈ceil(sqrt(n))) built by QR that reuse a small LRU-like direction memory (memory_size) to bias subspaces. Promising moves trigger cheap 1‑D refinements: a 3‑point parabolic vertex fit and a budget-limited golden-ish line search (short_line_search), plus small local polishing when improvements occur. Robustness and budget-awareness come from strict safe_eval and bounds clipping, periodic re-orthonormalization, rotation/replacement of poor arms (every rotate_every ≈ max(20,6p)), stagnation detection with soft restarts (and eventual final polishing), and randomized perturbations/partial reinitialization to escape local plateaus.", "code": "import numpy as np\n\nclass HSBS:\n    \"\"\"\n    Hybrid Subspace-Bandit Search (HSBS)\n\n    Combines:\n    - low-rank orthonormal direction set treated as bandit arms (UCB) with per-arm adaptive steps,\n    - occasional random k-dim subspace probing that reuses direction memory,\n    - cheap 1-D refinements (parabolic + golden-ish) on promising probes,\n    - direction memory (LRU), periodic rotation / replacement, global Gaussian jumps and soft restarts,\n    - strict budget-aware safe_eval and bounds clipping.\n\n    Interface:\n        HSBS(budget=10000, dim=10, seed=None, memory_size=8, ucb_beta=1.0)\n    Call:\n        f_best, x_best = HSBS(...)(func)\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, memory_size=8, ucb_beta=1.0, subspace_factor=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        self.memory_size = int(memory_size)\n        self.ucb_beta = float(ucb_beta)\n        self.subspace_factor = subspace_factor  # if None, use k = ceil(sqrt(n))\n    \n    def __call__(self, func):\n        n = self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # helpers\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initialization\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, _ = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # low-rank directional basis (p arms)\n        p = self.subspace_factor if self.subspace_factor is not None else max(2, int(np.ceil(np.sqrt(n))))\n        p = min(p, n)\n        R = np.random.randn(n, p)\n        Q, _ = np.linalg.qr(R)\n        D = Q[:, :p].copy()\n\n        # per-direction bandit stats\n        counts = np.zeros(p, dtype=int)\n        rewards = np.zeros(p, dtype=float)\n        steps = np.full(p, 0.35 * np.mean(ub - lb))\n        min_step = 1e-8 * max(1.0, np.mean(ub - lb))\n\n        # memory of good unit directions\n        dir_memory = []\n\n        # control params\n        max_probes_per_round = max(2, 2 * p)\n        stagnation_iters = 0\n        stagnation_limit = max(12, int(8 + np.log(1 + n) * 4))\n        restarts = 0\n        max_restarts = 6\n        rotate_every = max(20, p * 6)\n        iteration = 0\n\n        rng = np.random.RandomState(self.seed)\n\n        # UCB selector\n        def select_ucb():\n            total = counts.sum()\n            if total == 0:\n                untried = np.where(counts == 0)[0]\n                if len(untried) > 0:\n                    return int(rng.choice(untried))\n                return int(rng.randint(0, p))\n            avg = np.zeros_like(rewards)\n            nz = counts > 0\n            avg[nz] = rewards[nz] / counts[nz]\n            bonus = np.sqrt(np.log(1 + total) / (1 + counts))\n            scores = avg + self.ucb_beta * bonus\n            return int(np.argmax(scores))\n\n        # parabolic vertex refinement using 3 points\n        def parabolic_refine(x0, f0, d, alpha_vals, f_vals):\n            # alpha_vals length 3, f_vals length 3\n            a1, a2, a3 = alpha_vals\n            f1, f2, f3 = f_vals\n            denom = (a1 - a2) * (a1 - a3) * (a2 - a3)\n            if abs(denom) < 1e-16:\n                return None, None\n            A = ( (f1*(a2 - a3) + f2*(a3 - a1) + f3*(a1 - a2)) ) / denom\n            B = ( (f1*(a3**2 - a2**2) + f2*(a1**2 - a3**2) + f3*(a2**2 - a1**2)) ) / denom\n            if abs(A) < 1e-16:\n                return None, None\n            alpha_star = -B / (2 * A)\n            # clamp modestly to avoid crazy jumps\n            low = min(alpha_vals) - 2.0 * max(1.0, abs(min(alpha_vals)))\n            high = max(alpha_vals) + 2.0 * max(1.0, abs(max(alpha_vals)))\n            alpha_star = float(np.clip(alpha_star, low, high))\n            x_star = clip(x0 + alpha_star * d)\n            f_star, _ = safe_eval(x_star)\n            if f_star is None:\n                return None, None\n            return f_star, x_star\n\n        # cheap golden-ish line search limited by remaining budget\n        def short_line_search(x0, f0, d, init_step, max_evals=10):\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                return None, None\n            d_u = d / dn\n            remaining = max(0, self.budget - evals)\n            if remaining <= 0:\n                return None, None\n            # try positive then negative\n            a_pos = init_step\n            xp = clip(x0 + a_pos * d_u)\n            fp, _ = safe_eval(xp)\n            if fp is None:\n                return None, None\n            remaining -= 1\n            if fp < f0:\n                best_f, best_x = fp, xp.copy()\n            else:\n                a_pos = -init_step\n                xp = clip(x0 + a_pos * d_u)\n                fp2, _ = safe_eval(xp)\n                if fp2 is None:\n                    return None, None\n                remaining -= 1\n                if fp2 < f0:\n                    best_f, best_x = fp2, xp.copy()\n                else:\n                    return None, None\n            # golden-like shrink search between 0 and a_pos\n            a_low, a_high = 0.0, a_pos\n            gr = (np.sqrt(5) - 1) / 2\n            iters = 0\n            while remaining > 0 and iters < max_evals and abs(a_high - a_low) > 1e-12:\n                iters += 1\n                a1 = a_high - gr * (a_high - a_low)\n                a2 = a_low + gr * (a_high - a_low)\n                x1 = clip(x0 + a1 * d_u); f1, _ = safe_eval(x1)\n                if f1 is None:\n                    break\n                remaining -= 1\n                x2 = clip(x0 + a2 * d_u); f2, _ = safe_eval(x2)\n                if f2 is None:\n                    break\n                remaining -= 1\n                if f1 < f2:\n                    a_high = a2\n                    if f1 < best_f:\n                        best_f, best_x = f1, x1.copy()\n                else:\n                    a_low = a1\n                    if f2 < best_f:\n                        best_f, best_x = f2, x2.copy()\n            if best_f < f0:\n                return best_f, best_x\n            return None, None\n\n        # main loop\n        while evals < self.budget:\n            iteration += 1\n            probes = min(max_probes_per_round, max(1, (self.budget - evals)))\n            improved_in_round = False\n\n            # decide mix: mostly bandit-directed probes but sometimes subspace probes\n            if rng.rand() < 0.72:\n                # Bandit-guided directional probes\n                for _ in range(probes):\n                    if evals >= self.budget:\n                        break\n                    idx = select_ucb()\n                    d = D[:, idx].copy()\n                    dn = np.linalg.norm(d)\n                    if dn == 0:\n                        v = rng.randn(n)\n                        v /= (np.linalg.norm(v) + 1e-20)\n                        D[:, idx] = v\n                        d = v\n                    # sample alpha (gaussian centered 0 with direction step)\n                    alpha0 = rng.normal(loc=0.0, scale=steps[idx])\n                    if abs(alpha0) < 1e-16:\n                        alpha0 = steps[idx] * (0.5 - rng.rand())\n                    x_try = clip(x_cur + alpha0 * d)\n                    f_try, x_ret = safe_eval(x_try)\n                    if f_try is None:\n                        break\n                    counts[idx] += 1\n                    # opposition sampling sometimes\n                    if rng.rand() < 0.14 and evals < self.budget:\n                        alpha_op = -alpha0 * (0.4 + 0.6 * rng.rand())\n                        x_op = clip(x_cur + alpha_op * d)\n                        f_op, _ = safe_eval(x_op)\n                        if f_op is None:\n                            break\n                        counts[idx] += 1\n                        if f_op < f_try:\n                            f_try = f_op\n                            x_ret = clip(x_cur + alpha_op * d)\n                    # reward for bandit = improvement amount\n                    reward = max(0.0, f_cur - f_try)\n                    rewards[idx] += reward\n\n                    if f_try < f_cur - 1e-12:\n                        # accept\n                        prev = x_cur.copy()\n                        f_prev = f_cur\n                        x_cur = x_ret.copy()\n                        f_cur = f_try\n                        improved_in_round = True\n                        stagnation_iters = 0\n                        # record move direction\n                        mv = x_cur - prev\n                        mvn = np.linalg.norm(mv)\n                        if mvn > 0:\n                            unit_mv = mv / mvn\n                            dir_memory.insert(0, unit_mv.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                            # soft update the arm direction toward the move\n                            gamma = 0.28\n                            D[:, idx] = (1 - gamma) * D[:, idx] + gamma * unit_mv\n                            D[:, idx] /= (np.linalg.norm(D[:, idx]) + 1e-20)\n                        # grow step for this arm\n                        steps[idx] = min(steps[idx] * 1.18, 5.0 * np.mean(ub - lb))\n                        # cheap parabolic refinement if budget allows\n                        remaining = self.budget - evals\n                        if remaining >= 2:\n                            # pick three alphas near alpha0 including center 0\n                            a_cand = [0.0, alpha0 * 0.6, alpha0 * 1.4]\n                            f_vals = []\n                            for a in a_cand:\n                                x_a = clip(prev + a * d)\n                                f_a, _ = safe_eval(x_a)\n                                if f_a is None:\n                                    break\n                                f_vals.append(f_a)\n                            if len(f_vals) == 3:\n                                res = parabolic_refine(prev, f_prev, d, a_cand, f_vals)\n                                if res is not None:\n                                    f_par, x_par = res\n                                    if f_par is not None and f_par < f_cur - 1e-12:\n                                        # accept parabolic refinement\n                                        x_cur = x_par.copy()\n                                        f_cur = f_par\n                                        # small update to direction and step\n                                        mv2 = x_cur - prev\n                                        mvn2 = np.linalg.norm(mv2)\n                                        if mvn2 > 0:\n                                            D[:, idx] = (1 - 0.22) * D[:, idx] + 0.22 * (mv2 / mvn2)\n                                            D[:, idx] /= (np.linalg.norm(D[:, idx]) + 1e-20)\n                                        steps[idx] = min(steps[idx] * 1.12, 5.0 * np.mean(ub - lb))\n                    else:\n                        # no improvement -> shrink this arm's step\n                        steps[idx] = max(steps[idx] * 0.80, min_step)\n                        stagnation_iters += 1\n                        # small probabilistic acceptance to escape plateau\n                        if rng.rand() < 0.012 and f_try is not None and f_try <= f_cur + 1e-6:\n                            x_cur = x_ret.copy()\n                            f_cur = f_try\n\n                    # occasional global Gaussian jump\n                    if rng.rand() < 0.03 and (self.budget - evals) > 0:\n                        sigma = 0.4 * np.mean(ub - lb) * (0.4 + 0.6 * rng.rand())\n                        xg = clip(x_cur + rng.randn(n) * sigma)\n                        fg, _ = safe_eval(xg)\n                        if fg is None:\n                            break\n                        if fg < f_cur - 1e-12:\n                            # incorporate move and update a weak arm toward that move\n                            mvg = xg - x_cur\n                            mvgn = np.linalg.norm(mvg)\n                            x_cur = xg.copy()\n                            f_cur = fg\n                            if mvgn > 0:\n                                worst = int(np.argmin(rewards + 1e-12))\n                                D[:, worst] = (1 - 0.25) * D[:, worst] + 0.25 * (mvg / mvgn)\n                                D[:, worst] /= (np.linalg.norm(D[:, worst]) + 1e-20)\n                                steps[worst] = max(steps[worst], 0.8 * sigma)\n            else:\n                # Random small subspace probing (mix ideas from ARSS)\n                k = max(1, int(np.ceil(np.sqrt(n))))\n                use_mem = min(len(dir_memory), k // 2)\n                basis_cols = []\n                if use_mem > 0:\n                    mem_sel = np.array(dir_memory[:use_mem])\n                    basis_cols.append(mem_sel.T)\n                needed = k - sum(b.shape[1] for b in basis_cols) if basis_cols else k\n                R = rng.randn(n, needed)\n                if basis_cols:\n                    R = np.column_stack(basis_cols + [R])\n                Qs, _ = np.linalg.qr(R)\n                basis = Qs[:, :k]\n                probes_sub = max(3, 2 * k)\n                global_step = 0.45 * np.mean(ub - lb)\n                for _ in range(min(probes_sub, self.budget - evals)):\n                    coeffs = rng.randn(k)\n                    d = basis @ coeffs\n                    nd = np.linalg.norm(d)\n                    if nd == 0:\n                        continue\n                    d = d / nd\n                    alpha = rng.uniform(-global_step, global_step)\n                    x_try = clip(x_cur + alpha * d)\n                    f_try, x_ret = safe_eval(x_try)\n                    if f_try is None:\n                        break\n                    if f_try < f_cur - 1e-12:\n                        # record direction, small line search, accept\n                        dir_succ = (x_ret - x_cur)\n                        dn = np.linalg.norm(dir_succ)\n                        if dn > 0:\n                            dir_succ /= dn\n                            dir_memory.insert(0, dir_succ.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                        # short 1-D search along dir_succ\n                        ls_rem = self.budget - evals\n                        if ls_rem >= 2:\n                            res = short_line_search(x_cur, f_cur, dir_succ, init_step=alpha if alpha != 0 else global_step, max_evals=min(10, ls_rem))\n                            if res is not None:\n                                f_ls, x_ls = res\n                                if f_ls is not None and f_ls < f_try - 1e-12:\n                                    f_try, x_ret = f_ls, x_ls.copy()\n                        x_cur = x_ret.copy()\n                        f_cur = f_try\n                        improved_in_round = True\n                        stagnation_iters = 0\n                    else:\n                        # small chance to do short line from current center\n                        if rng.rand() < 0.06 and (self.budget - evals) >= 3:\n                            res = short_line_search(x_cur, f_cur, d, init_step=global_step, max_evals=6)\n                            if res is not None:\n                                f_ls, x_ls = res\n                                if f_ls is not None and f_ls < f_cur - 1e-12:\n                                    x_cur = x_ls.copy()\n                                    f_cur = f_ls\n                                    dir_succ = (x_ls - x_cur)\n                                    if np.linalg.norm(dir_succ) > 0:\n                                        dir_succ = dir_succ / np.linalg.norm(dir_succ)\n                                        dir_memory.insert(0, dir_succ.copy())\n                                        if len(dir_memory) > self.memory_size:\n                                            dir_memory.pop()\n                                    improved_in_round = True\n                                    stagnation_iters = 0\n\n            # re-orthonormalize occasionally\n            if iteration % 7 == 0:\n                Q, _ = np.linalg.qr(D)\n                D = Q[:, :p].copy()\n\n            # rotate/replace poor arms occasionally\n            if iteration % rotate_every == 0:\n                score = np.full(p, -1e9)\n                nonzero = counts > 0\n                score[nonzero] = rewards[nonzero] / counts[nonzero]\n                worst = np.argsort(score)[:max(1, p // 4)]\n                for wi in worst:\n                    rnd = rng.randn(n)\n                    if len(dir_memory) > 0 and rng.rand() < 0.6:\n                        cand = 0.5 * rnd + 0.5 * dir_memory[rng.randint(len(dir_memory))]\n                    else:\n                        cand = rnd\n                    cand /= (np.linalg.norm(cand) + 1e-20)\n                    D[:, wi] = cand\n                    counts[wi] = 0\n                    rewards[wi] = 0.0\n                    steps[wi] = 0.35 * np.mean(ub - lb)\n\n            # local polishing if improvement observed\n            if improved_in_round:\n                extras = min(6, max(1, int(np.ceil(np.log(1 + n)))))\n                for _ in range(extras):\n                    if evals >= self.budget:\n                        break\n                    d = rng.randn(n)\n                    d /= (np.linalg.norm(d) + 1e-20)\n                    a = rng.uniform(-0.5 * np.mean(ub - lb) * 0.2, 0.5 * np.mean(ub - lb) * 0.2)\n                    f_try, x_ret = safe_eval(clip(x_cur + a * d))\n                    if f_try is None:\n                        break\n                    if f_try < f_cur - 1e-12:\n                        dir_succ = (x_ret - x_cur)\n                        x_cur = x_ret.copy()\n                        f_cur = f_try\n                        if np.linalg.norm(dir_succ) > 0:\n                            dir_succ = dir_succ / np.linalg.norm(dir_succ)\n                            dir_memory.insert(0, dir_succ.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n\n            # adapt stagnation and restart\n            if not improved_in_round:\n                stagnation_iters += 1\n            else:\n                stagnation_iters = 0\n\n            if stagnation_iters >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final polishing: reduce steps and try small local tries around best\n                    steps = np.maximum(steps * 0.45, min_step)\n                    local_tries = min(12, self.budget - evals)\n                    for _ in range(local_tries):\n                        if evals >= self.budget:\n                            break\n                        dx = rng.randn(n) * (0.02 * np.mean(ub - lb))\n                        xt = clip(x_best + dx) if x_best is not None else clip(x_cur + dx)\n                        ft, _ = safe_eval(xt)\n                        if ft is None:\n                            break\n                    if stagnation_iters > stagnation_limit * 3:\n                        break\n                else:\n                    # soft restart around best\n                    if x_best is not None:\n                        perturb = rng.randn(n) * (0.5 * np.mean(ub - lb))\n                        x_cur = clip(x_best + perturb)\n                        fc, _ = safe_eval(x_cur)\n                        if fc is None:\n                            break\n                        f_cur = fc\n                    # partially reinitialize directions\n                    for i in range(max(1, p // 2)):\n                        v = rng.randn(n)\n                        v /= (np.linalg.norm(v) + 1e-20)\n                        D[:, i] = v\n                        counts[i] = 0\n                        rewards[i] = 0.0\n                        steps[i] = min(1.0 * np.mean(ub - lb), steps[i] * 2.0)\n                    dir_memory = dir_memory[:max(0, self.memory_size // 2)]\n                    stagnation_iters = 0\n\n            # clamp steps\n            steps = np.maximum(steps, min_step)\n\n            # early termination if extremely good\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HSBS scored 0.249 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "f2f29081-d1c2-4d76-a1c0-a17d70eca292", "operator": null, "metadata": {"aucs": [0.09080226482201736, 0.15666992563159965, 0.3662057767918129, 0.32708261503078595, 0.25148395398685597, 0.41157534006322716, 0.23709106649427614, 0.27515616398115916, 0.2515742702924634, 0.12616112900458842]}, "task_prompt": ""}
{"id": "77ab90a0-5260-412c-800f-0a98ad3951f2", "fitness": "-inf", "name": "DARDE", "description": "DARDE is a hybrid CMA/DE heuristic that uses Lehmer-like recombination weights and mirrored BD*z sampling (lambda scaling with dim, initial sigma = 0.25*mean(range)) while applying per-coordinate RMS scaling (coord_var) to tailor step sizes by dimension. It injects diversity with occasional DE-style archive-difference mutations, Lévy global perturbations, and memory-directed Cauchy jumps from a small LRU directional memory (mem_size), while mirrored sampling reduces estimator variance. Covariance and step-size adaptation follow CMA-ES principles (ps, pc, rank-one + rank-mu updates) with infrequent eigen decompositions controlled by eigen_every_factor, plus safeguards on sigma and C to keep stability. An archive supports selection, DE choices and occasional diagonal quadratic model proposals (model_every) for directed exploitation, and all proposals are clipped to bounds and budget-aware; parameters like p_de, p_levy and p_mem_jump trade exploration vs steady local adaptation.", "code": "import numpy as np\nfrom collections import deque\n\nclass DARDE:\n    \"\"\"\n    Directional ARDE (DARDE)\n    One-line: Lehmer-weighted CMA/DE hybrid with per-coordinate RMS scaling, small directional memory,\n    mirrored sampling and occasional Cauchy/Lévy directional jumps plus archive-driven diagonal-model proposals.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 p_de=0.25, p_levy=0.06, levy_beta=1.5,\n                 mem_size=6, p_mem_jump=0.18, cauchy_scale=0.6,\n                 eigen_every_factor=5, model_every=20):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.p_de = float(p_de)\n        self.p_levy = float(p_levy)\n        self.levy_beta = float(levy_beta)\n        self.mem_size = int(mem_size)\n        self.p_mem_jump = float(p_mem_jump)\n        self.cauchy_scale = float(cauchy_scale)\n        self.eigen_every_factor = int(eigen_every_factor)\n        self.model_every = int(model_every)\n        # RNG\n        if seed is None:\n            self.rng = np.random.default_rng()\n        else:\n            self.rng = np.random.default_rng(seed)\n\n        # population sizes (Lehmer-like weights later)\n        self.lambda_ = max(6, int(6 + 4 * np.log(max(1, self.dim))))\n        self.mu = max(2, self.lambda_ // 2)\n\n    def _levy_step(self, n, beta):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = self.rng.normal(0, sigma_u, size=n)\n        v = self.rng.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1 / beta))\n        return step\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n\n        # Lehmer-like recombination weights\n        mu = self.mu\n        raw = (np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))) ** 1.15\n        weights = raw / np.sum(raw)\n        mu_eff = (np.sum(weights) ** 2) / np.sum(weights ** 2)\n\n        # adaptation constants (moderate choices)\n        cc = 0.6 / np.sqrt(n + mu_eff)\n        cs = 0.3 / (1.0 + mu_eff / (n + 1.0))\n        c1 = 1.0 / (((n + 1.3) ** 1.7) + 0.5 * mu_eff)\n        cmu = min(0.8 * (1 - c1), 0.5 * mu_eff / (mu_eff + 2.0) * (1 - c1))\n        damps = 1.0 + cs + 2.0 * max(0.0, np.sqrt(mu_eff / (n + 1.0)) - 1.0)\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize state\n        m = self.rng.uniform(lb, ub)\n        sigma = 0.25 * np.mean(rng_range)\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eigen_eval_counter = 0\n        eig_every = max(1, int(self.eigen_every_factor * n))\n\n        # per-coordinate RMS scaling\n        coord_var = np.ones(n) * 1e-6\n        coord_alpha = 0.2\n\n        # directional memory (LRU)\n        mem = deque(maxlen=self.mem_size)\n\n        # archive\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation at mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            try:\n                fm = float(func(xm))\n            except Exception:\n                fm = np.inf\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_opt = fm\n            x_opt = xm.copy()\n\n        gen_count = 0\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(self.lambda_, remaining)\n\n            # mirrored sampling: generate ceil(lambda/2) zs, mirror them\n            half = (current_lambda + 1) // 2\n            zs = [self.rng.standard_normal(n) for _ in range(half)]\n            zs = zs + [(-z) for z in zs]\n            zs = zs[:current_lambda]\n\n            arx = np.empty((current_lambda, n))\n            # precompute BD\n            BD = B * D[np.newaxis, :]\n            for k, z in enumerate(zs):\n                # map z via BD and per-coordinate scaling\n                y = BD.T @ z  # yields length n\n                # apply per-coordinate RMS scaling\n                y = y * np.sqrt(coord_var)\n                x = m + sigma * y\n\n                # occasional DE mutation\n                if (self.rng.random() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                    F_de = 0.6 + 0.4 * (1.0 - evals / max(1, budget))  # decaying F\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + 0.7 * de_mut\n\n                # occasional Levy added globally\n                if self.rng.random() < self.p_levy:\n                    levy = self._levy_step(n, self.levy_beta)\n                    x = x + 0.5 * sigma * levy\n\n                # occasional memory-directed Cauchy jump\n                if mem and (self.rng.random() < self.p_mem_jump):\n                    u = mem[self.rng.integers(len(mem))]\n                    jump = np.tan(np.pi * (self.rng.random() - 0.5))\n                    x = x + (sigma * self.cauchy_scale) * jump * u\n\n                arx[k] = np.clip(x, lb, ub)\n\n            # evaluate offspring until budget exhausted\n            arfit = np.full(current_lambda, np.inf)\n            for k in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[k]\n                try:\n                    f = float(func(x))\n                except Exception:\n                    f = np.inf\n                evals += 1\n                arfit[k] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n\n            # selection and recombination (ensure some valid)\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            sel_mu = min(self.mu, valid_idx.size)\n            idx_sorted = np.argsort(arfit[valid_idx])\n            selected = valid_idx[idx_sorted[:sel_mu]]\n            x_sel = arx[selected]\n            y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n\n            # recompute weights subset if fewer selections\n            if sel_mu < mu:\n                w_sub = weights[:sel_mu].copy()\n                w_sub = w_sub / np.sum(w_sub)\n                w_for = w_sub\n            else:\n                w_for = weights\n\n            m_old = m.copy()\n            m = np.sum(w_for[:, None] * x_sel, axis=0)\n            y_w = np.sum(w_for[:, None] * y_sel, axis=0)\n\n            # update evolution paths\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n            norm_ps = np.linalg.norm(ps)\n            # success rate approximated by fraction better than previous mean fitness\n            # if we have a parent fitness in archive nearest to m_old, use it; else fallback to median\n            if archive_F:\n                f_parent = np.min(archive_F[-min(len(archive_F), max(10, current_lambda)):])\n            else:\n                f_parent = np.inf\n            successes = np.sum(arfit < f_parent)\n            s_rate = successes / max(1, current_lambda)\n            hsig = 1.0 if (s_rate > 0.15) else 0.0\n\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n\n            # covariance update (rank-one + rank-mu)\n            rank_one = np.outer(pc, pc)\n            rank_mu = np.zeros((n, n))\n            for i in range(y_sel.shape[0]):\n                yi = y_sel[i][:, None]\n                rank_mu += w_for[i] * (yi @ yi.T)\n            C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n\n            # per-coordinate RMS update from selected y_sel\n            sec_mom = np.sum(w_for[:, None] * (y_sel ** 2), axis=0)\n            coord_var = (1 - coord_alpha) * coord_var + coord_alpha * (sec_mom + 1e-12)\n            coord_var = np.clip(coord_var, 1e-12, 1e12)\n\n            # sigma adaptation: median-success vs target\n            target_s = 0.2\n            adapt_strength = 0.6 / (1.0 + 0.05 * n)\n            sigma *= np.exp(adapt_strength * (s_rate - target_s) / (damps + 1e-20))\n            sigma = max(sigma, 1e-12)\n\n            # update directional memory with normalized y_w or best offspring step\n            yw_norm = np.linalg.norm(y_w)\n            if yw_norm > 1e-12:\n                mem.append((y_w / yw_norm).copy())\n            else:\n                # also try to add last best step\n                best_idx = np.argmin(arfit)\n                if np.isfinite(arfit[best_idx]):\n                    step_vec = arx[best_idx] - m_old\n                    dn = np.linalg.norm(step_vec)\n                    if dn > 0:\n                        mem.append((step_vec / dn).copy())\n\n            # occasionally try a diagonal quadratic model from best archive points\n            gen_count += 1\n            if (gen_count % self.model_every == 0) and (len(archive_X) >= (n + 2)) and (evals < budget):\n                k_best = min(len(archive_X), 2 * n + 5)\n                idxb = np.argsort(archive_F)[:k_best]\n                Xm = np.array([archive_X[i] for i in idxb])\n                Fm = np.array([archive_F[i] for i in idxb])\n                A = np.hstack([Xm ** 2, Xm, np.ones((Xm.shape[0], 1))])\n                reg = 1e-6\n                try:\n                    sol, *_ = np.linalg.lstsq(A.T @ A + reg * np.eye(A.shape[1]), A.T @ Fm, rcond=None)\n                    sol = sol.flatten()\n                    a = sol[:n]\n                    b = sol[n:2 * n]\n                    a_safe = a.copy()\n                    small = np.abs(a_safe) < 1e-8\n                    a_safe[small] = 1e-8 * np.sign(a_safe[small] + 1e-12) + 1e-8\n                    x_star = -0.5 * b / (a_safe + 1e-20)\n                    # propose modest step toward x_star from current mean\n                    x_prop = np.clip(m + 0.6 * (x_star - m), lb, ub)\n                    if evals < budget:\n                        try:\n                            f_prop = float(func(x_prop))\n                        except Exception:\n                            f_prop = np.inf\n                        evals += 1\n                        archive_X.append(x_prop.copy())\n                        archive_F.append(f_prop)\n                        if f_prop < f_opt:\n                            f_opt = f_prop\n                            x_opt = x_prop.copy()\n                            # accept as new mean to focus\n                            m = x_prop.copy()\n                except Exception:\n                    pass\n\n            # periodic eigen decomposition to update B, D, invsqrtC\n            eigen_eval_counter += max(1, current_lambda)\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # small safeguard for sigma\n            if sigma < 1e-12:\n                sigma = 1e-12\n\n            # optional mean levy jump on stagnation (few improvements recently)\n            if (self.rng.random() < 0.01) and (len(archive_F) >= max(10, n)):\n                recent = archive_F[-min(len(archive_F), 50):]\n                if np.mean(recent) - f_opt < 1e-12:\n                    levy = self._levy_step(n, self.levy_beta)\n                    m = np.clip(m + 0.8 * sigma * levy, lb, ub)\n\n        # ensure returned x_opt is not None\n        if x_opt is None:\n            # fallback find best in archive\n            if archive_F:\n                idx = int(np.argmin(archive_F))\n                x_opt = np.array(archive_X[idx], dtype=float)\n                f_opt = float(archive_F[idx])\n            else:\n                x_opt = np.clip(m, lb, ub)\n                try:\n                    f_opt = float(func(x_opt))\n                except Exception:\n                    f_opt = np.inf\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 88, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem = deque(maxlen=self.mem_size)", "error": "In the code, line 88, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem = deque(maxlen=self.mem_size)", "parent_ids": "f2f29081-d1c2-4d76-a1c0-a17d70eca292", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "00fb28ac-276d-4f1d-ba3f-3b1cbe7cdd73", "fitness": 0.2337576388893216, "name": "ALeSB", "description": "ALeSB maintains a central estimate m with a global step-size sigma and per-coordinate scales D, generates a small population each generation and recombines the best candidates using soft exponential weights (lambda_base, mu, weights) to update m. Candidate generation is a mixture of four operators — local Gaussian (p_local=0.48) with per-coordinate scaling and a learned low-rank correlated drift from an incremental covariance M (subspace R refreshed every few generations), Lévy-like heavy tails (Student-t, levy_df=1.8, p_levy=0.12) for long jumps, bridge moves between archive elites (p_bridge=0.20) to exploit good regions, and coordinate-focused bandit-selected refinements (p_coord=0.20) whose coordinate sampling probabilities self-adapt from improvement rewards. Sigma is adapted multiplicatively toward a target success rate (~0.20) using a sliding recent window, D is updated from median absolute deviations of selected normalized steps, and M is updated incrementally from the recombination direction to learn principal search subspaces. An archive of evaluated points and stagnation handling (inflating sigma, jittering D, reseeding m, weakening M) ensure diversification, while bounds clamping, safeguards on sigma/D, and limited-rank subspace keep stability and low computational cost.", "code": "import numpy as np\n\nclass ALeSB:\n    \"\"\"\n    ALeSB (Adaptive Lévy-Subspace Bandit Search)\n\n    Core ideas:\n      - Maintain a current center (m), a global scale sigma and per-coordinate scales D.\n      - Propose candidates via a mixture of operators:\n          * Local Gaussian in (D)-scaled coordinates plus low-rank correlated directions\n          * Lévy-like heavy-tailed proposals (Student-t) for long jumps\n          * Bridge moves that jump to midpoints of archive elites with small perturbations\n          * Coordinate-focused small moves where a bandit selects promising coordinates to refine\n      - Learn a modest low-rank correlated subspace M (incremental covariance) whose top eigenvectors\n        are used to generate correlated steps. Update M slowly to keep cost low.\n      - Adapt sigma using a generation-wise success-rate control (target ~20%) with multiplicative updates.\n      - Adapt per-coordinate exploration probabilities (bandit) using improvement-based rewards.\n      - Detect stagnation (no improvement for a while) and perform strong diversification: inflate sigma,\n        randomize D and re-seed the center from archive or random point.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 pop_factor=6, subspace_r=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size heuristic (small to moderate)\n        self.lambda_base = max(6, int(pop_factor + np.floor(1.8 * np.log(max(1, self.dim)))))\n        # number of recombined parents\n        self.mu = max(1, self.lambda_base // 3)\n\n        # per-coordinate bandit initial preferences\n        self.bandit_probs = np.ones(self.dim) / self.dim\n\n        # per-coordinate adaptation inertia\n        self.c_d = 0.12\n\n        # incremental covariance learning rate\n        self.alpha_M = 0.05\n\n        # subspace rank\n        if subspace_r is None:\n            self.r = max(1, int(np.ceil(0.6 * np.sqrt(self.dim))))\n        else:\n            self.r = min(max(1, int(subspace_r)), self.dim)\n\n        # operator probabilities\n        self.p_local = 0.48\n        self.p_levy = 0.12\n        self.p_bridge = 0.20\n        self.p_coord = 0.20\n\n        # Lévy (Student-t) degree of freedom (heavy tail)\n        self.levy_df = 1.8\n\n        # sigma success-rate adaptation\n        self.target_success = 0.20\n        self.sigma_adapt_rate = 0.15  # multiplicative intensity\n\n        # stagnation parameters\n        self.stag_eval_thresh = max(10, 4 * self.dim)  # fallback threshold\n        self.stag_sigma_mult = 3.0\n\n        # archive\n        self.archive_max = 2000\n\n        # how often to refresh subspace eigenvectors\n        self.subspace_update_every = 6\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        avg_range = float(np.mean(domain_range))\n\n        # population size (capped by remaining budget)\n        lam = self.lambda_base\n        mu = min(self.mu, lam)\n\n        # recombination weights (soft exponential)\n        ranks = np.arange(mu)\n        weights = np.exp(-ranks / max(1.0, (mu / 2.5)))\n        weights = weights / np.sum(weights)\n\n        # initial state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.18 * avg_range  # moderately conservative start\n        D = np.ones(n)            # per-coordinate scale factors\n        M = np.zeros((n, n), dtype=float)  # incremental covariance-like matrix\n        # initial random orthonormal subspace R (n x r)\n        if self.r >= 1:\n            rand_mat = np.random.randn(n, self.r)\n            try:\n                Q, _ = np.linalg.qr(rand_mat)\n                R = Q[:, :self.r]\n            except np.linalg.LinAlgError:\n                R = np.zeros((n, self.r))\n        else:\n            R = np.zeros((n, 0))\n\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n\n        # Evaluate initial center\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            if fm < f_opt:\n                f_opt = fm\n                x_opt = xm.copy()\n                last_improvement_eval = evals\n\n        generation = 0\n        # sliding window success history for sigma adaptation\n        recent_successes = []\n        recent_window = 40\n\n        while evals < budget:\n            generation += 1\n            remaining = budget - evals\n            lam_cur = min(lam, remaining)\n\n            # prepare storage\n            cand_X = np.zeros((lam_cur, n))\n            cand_Y = np.zeros((lam_cur, n))  # y = (x - m) / sigma\n            cand_F = np.full(lam_cur, np.inf)\n            op_types = []\n\n            # generate candidates\n            for j in range(lam_cur):\n                op = np.random.rand()\n                y = np.zeros(n)\n\n                # sample a small correlated vector from low-rank subspace if available\n                if self.r > 0 and R.size > 0:\n                    z_sub = np.random.randn(self.r)\n                    corr_part = R @ z_sub\n                else:\n                    corr_part = 0.0\n\n                if op < self.p_local:\n                    # local Gaussian: per-coordinate scaling + correlated part\n                    z = np.random.randn(n)\n                    # mixing coefficient for correlated subspace scaled adaptively by median(D)\n                    gamma = 0.55 * np.median(D) / (1.0 + 0.25 * self.r)\n                    y = D * z + gamma * corr_part\n                    op_types.append(\"local\")\n                elif op < self.p_local + self.p_levy:\n                    # heavy-tailed Lévy-like: Student-t scaled by D\n                    t = np.random.standard_t(self.levy_df, size=n)\n                    # normalize vector length and scale by median(D)\n                    tnorm = np.linalg.norm(t) + 1e-20\n                    y = (t / tnorm) * (1.1 * np.median(D) * (np.log1p(np.abs(t)) / (1.0 + np.log1p(np.abs(t)))))\n                    # add a small correlated drift sometimes\n                    if self.r > 0 and np.random.rand() < 0.5:\n                        y += 0.6 * corr_part\n                    op_types.append(\"levy\")\n                elif op < self.p_local + self.p_levy + self.p_bridge and len(archive_X) >= 2:\n                    # bridge: choose two archive elites and jump to their midpoint + small noise\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    a = archive_X[i1]\n                    b = archive_X[i2]\n                    mid = 0.5 * (a + b)\n                    # directional push from center to midpoint with some perturbation\n                    dir_mid = mid - m\n                    y = dir_mid / (sigma + 1e-20) + 0.3 * D * np.random.randn(n)\n                    # add small correlated component toward principal subspace\n                    if self.r > 0 and np.random.rand() < 0.6:\n                        y += 0.25 * corr_part\n                    op_types.append(\"bridge\")\n                else:\n                    # coordinate-focused move selected by bandit\n                    # sample a mini-set of coordinates according to bandit probabilities\n                    k_coords = max(1, int(0.12 * n))  # refine a small fraction\n                    coords = np.random.choice(n, size=k_coords, replace=False, p=self.bandit_probs)\n                    y = np.zeros(n)\n                    # small gaussian moves on selected coords; rest zero (axis refinement)\n                    y_coords = np.random.randn(k_coords) * 0.6\n                    y[coords] = (D[coords] * y_coords)\n                    # occasionally add a correlated small nudge\n                    if self.r > 0 and np.random.rand() < 0.35:\n                        y += 0.12 * corr_part\n                    op_types.append(\"coord\")\n\n                # form candidate and clip\n                x = m + sigma * y\n                x = np.clip(x, lb, ub)\n\n                cand_X[j] = x\n                cand_Y[j] = y\n\n            # evaluate candidates sequentially until budget\n            for j in range(lam_cur):\n                if evals >= budget:\n                    break\n                x = cand_X[j]\n                f = func(x)\n                evals += 1\n                cand_F[j] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > self.archive_max:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection & recombination\n            idx = np.argsort(cand_F)\n            sel_idx = idx[:min(mu, np.sum(np.isfinite(cand_F)))]\n            if sel_idx.size == 0:\n                # no valid evaluations (shouldn't happen), do small random perturbation and continue\n                m_old = m.copy()\n                m = np.clip(m + 0.1 * sigma * np.random.randn(n), lb, ub)\n                continue\n\n            x_sel = cand_X[sel_idx]\n            y_sel = cand_Y[sel_idx]\n\n            m_old = m.copy()\n            # recombine in x-space using weights\n            # if fewer than mu candidates, re-normalize weights accordingly\n            w = weights[:len(sel_idx)]\n            w = w / np.sum(w)\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # effective average y step (in normalized coordinates)\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # success definition: improvement of best candidate relative to previous center\n            # note: fm_old is last evaluated center objective (from earlier)\n            fm_old = archive_F[-(len(cand_F) - 0) - 1] if (len(archive_F) > 0) else np.inf\n            # instead use last known f_opt before generation to judge success: compare if best candidate improved original center\n            # find best candidate fitness\n            best_cand_f = np.min(cand_F[np.isfinite(cand_F)])\n            success = 1 if best_cand_f < f_opt else 0  # improvement on global opt (strong)\n            # but also track \"local\" generation success: did new center improve relative to previous center evaluation\n            # previous center f is approximated as the archive entry for m_old (closest)\n            # find archive F of m_old if present else fallback to last_improvement\n            # We instead define gen_success by comparing candidate best to the center evaluation recorded earlier (archive end)\n            # Find the archive fitness corresponding to the initial center in this gen (it's the last entry before generating candidates)\n            # That archived center fitness equals archive_F[-(lam_cur + 1)] ideally; to keep safe, consider previous best center fitness f_center\n            # We'll approximate by the best fitness recorded before generation started:\n            f_center_before = f_opt  # conservative; if candidates improve global f_opt, success True\n            gen_success = 1 if best_cand_f < f_center_before else 0\n\n            # update recent_successes\n            recent_successes.append(gen_success)\n            if len(recent_successes) > recent_window:\n                recent_successes.pop(0)\n            sr = float(np.mean(recent_successes)) if len(recent_successes) > 0 else 0.0\n\n            # sigma adaptation (multiplicative)\n            if sr > self.target_success:\n                sigma *= np.exp(self.sigma_adapt_rate * (sr - self.target_success))\n            else:\n                sigma /= np.exp(self.sigma_adapt_rate * (self.target_success - sr))\n\n            # safety bounds on sigma\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e3 * avg_range + 1e-12)\n\n            # robust per-coordinate adaptation using median absolute deviation from selected y's\n            if x_sel.shape[0] >= 1:\n                med = np.median(y_sel, axis=0)\n                mad = np.median(np.abs(y_sel - med), axis=0)\n                approx_std = 1.4826 * (mad + 1e-20)\n                # blend into D with inertia\n                D = (1.0 - self.c_d) * D + self.c_d * (approx_std + 1e-12)\n            else:\n                # small jitter\n                D = D * (1.0 + 1e-6 * np.random.randn(n))\n\n            # clamp D\n            D = np.maximum(D, 1e-12)\n            D = np.minimum(D, 1e3 * np.ones_like(D))\n\n            # incremental covariance M update using normalized y_w (directional)\n            norm_yw = np.linalg.norm(y_w) + 1e-20\n            if norm_yw > 0:\n                v = y_w / norm_yw\n                M = (1.0 - self.alpha_M) * M + self.alpha_M * np.outer(v, v)\n\n            # refresh subspace R occasionally using top-r eigenvectors of M\n            if (generation % self.subspace_update_every) == 0 and self.r > 0:\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(M)\n                    if eigvecs.shape[1] >= self.r:\n                        R = eigvecs[:, -self.r:]\n                except np.linalg.LinAlgError:\n                    # skip on failure\n                    pass\n\n            # update bandit probabilities for coordinate-focused moves:\n            # For each coordinate move candidate that improved, assign reward to its coords\n            # We'll scan candidates that were of \"coord\" type and yield improvements vs center value\n            # approximate center fitness as the best known before generation (f_center_before)\n            coord_rewards = np.zeros(n)\n            for idx_j in sel_idx:\n                j = int(idx_j)\n                if op_types[j] == \"coord\":\n                    f_j = cand_F[j]\n                    # reward is relative improvement over previous best center (clamped)\n                    reward = max(0.0, (f_center_before - f_j) / (abs(f_center_before) + 1e-12))\n                    # attribute reward to coordinates changed (non-zero entries in y)\n                    changed = np.abs(cand_Y[j]) > 1e-12\n                    coord_rewards[changed] += reward / (np.sum(changed) + 1e-12)\n\n            # exponential smoothing into bandit_probs\n            if np.sum(coord_rewards) > 0:\n                # normalize rewards into a distribution\n                rnorm = coord_rewards + 1e-20\n                rnorm = rnorm / np.sum(rnorm)\n                # learning rate for bandit\n                bandit_lr = 0.15\n                self.bandit_probs = (1 - bandit_lr) * self.bandit_probs + bandit_lr * rnorm\n                # ensure no zero probs\n                self.bandit_probs = np.maximum(self.bandit_probs, 1e-6)\n                self.bandit_probs = self.bandit_probs / np.sum(self.bandit_probs)\n\n            # stagnation detection: if no improvement for a while, diversify\n            if (evals - last_improvement_eval) > self.stag_eval_thresh:\n                # inflate sigma and jitter D and maybe reseed center from archive or random\n                sigma *= self.stag_sigma_mult\n                # jitter D strongly to encourage exploration\n                D = np.maximum(1e-12, D * (1.0 + 0.8 * (np.random.rand(n) - 0.5)))\n                # reseed center\n                if len(archive_X) > 0 and np.random.rand() < 0.8:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.5 * m + 0.5 * archive_X[pick]\n                else:\n                    m = np.random.uniform(lb, ub)\n                # reset recent successes moderately\n                recent_successes = recent_successes[-5:]\n                # weaken M to encourage new subspace discovery\n                M *= 0.3\n                last_improvement_eval = evals  # avoid immediate repeated stagnation triggers\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # continue next generation until budget exhausted\n\n        # return best found\n        if x_opt is None:\n            xm = np.clip(m, lb, ub)\n            f_final = func(xm)\n            return float(f_final), np.array(xm, dtype=float)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ALeSB scored 0.234 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "a99b834f-9e23-4db3-a765-918a3d6daefd", "operator": null, "metadata": {"aucs": [0.11875662360382222, 0.19391494689401423, 0.4106539498025471, 0.19348264683067562, 0.20392491539942903, 0.40802626913469675, 0.23354651001270055, 0.21638818442889174, 0.20093224477964744, 0.15795009800679105]}, "task_prompt": ""}
{"id": "d5b01052-0a00-4f5b-ba4d-f28296a9afbd", "fitness": "-inf", "name": "LISA", "description": "LISA builds a mixed exploration–exploitation sampler by combining axis-wise adaptive Gaussian steps (per-coordinate RMS scales D updated with EWMA) with a learned low-rank correlated subspace U (incrementally updated covariance S and periodic eigendecomposition, k ≈ 0.5√n) so candidates are mixtures of diagonal and correlated directions. Global step-size sigma is controlled by an online sliding-window success-rate controller (target ≈20%) rather than path-length, while population sizing uses a small-to-moderate λ≈6+1.6√n and μ≈λ/4 with linear recombination weights to form the new mean in x-space and a weighted normalized y for subspace updates. Exploration boosters include occasional Mantegna Lévy jumps (α=1.5, p_levy≈0.12) for heavy tails and DE-style archive perturbations (p_de≈0.18, large archive up to 6000) to reuse past solutions, with bounds enforced by clipping. Robustness/stagnation measures include clamping sigma and D, modest stochastic jitter, inflation and archive-guided mean nudges after repeated failures, and bounded initial sigma (≈0.14·range) to bias early local search.", "code": "import numpy as np\n\nclass LISA:\n    \"\"\"\n    LISA (Lévy-Informed Subspace Adaptive Search)\n\n    Main idea (short):\n      - Maintain per-coordinate scale estimates (via EWMA of squared steps) and a\n        low-rank subspace U learned from recent successful step directions (small S).\n      - Sample candidates as a mixture of scaled axis-wise Gaussian steps and a low-rank\n        correlated component. Occasionally perform Lévy jumps (Mantegna) and DE-style\n        archive perturbations to boost exploration.\n      - Adapt global sigma using an online success-rate (1/5-ish) controller over a\n        sliding window rather than path-length. Diagonal scales (D) use EWMA variance.\n      - Subspace S (covariance-like) is updated from successful y's with a forgetting\n        factor; eigenvectors (top-k) are refreshed periodically.\n\n    Main tunable parameters (defaults below). Comments inside __init__ document them.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 lam=None, mu=None, subspace_k=None,\n                 p_levy=0.12, p_de=0.18, F_de_base=0.7,\n                 beta_D=0.14, alpha_S=0.12, subspace_every=5,\n                 success_window=20, success_target=0.20,\n                 sigma_init_factor=0.14):\n        \"\"\"\n        Parameters (high-level):\n         - budget, dim: problem budget and dimension\n         - seed: optional RNG seed\n         - lam, mu: population size and number of recombined parents (auto if None)\n         - subspace_k: rank of low-rank subspace (auto if None)\n         - p_levy: probability to perform a Lévy jump for a candidate\n         - p_de, F_de_base: probability and base factor for DE-style archive mutation\n         - beta_D: EWMA weight for diagonal scale variance (0..1)\n         - alpha_S: forgetting rate for incremental subspace covariance S\n         - subspace_every: how often (generations) to recompute eigenvectors\n         - success_window: sliding window (evaluations) to track success rate\n         - success_target: target success proportion (like 1/5 rule)\n         - sigma_init_factor: initial sigma as factor of range (different from provided)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population sizing (different heuristics from provided code)\n        if lam is None:\n            # slightly larger baseline relative to sqrt(dim)\n            self.lambda_ = max(6, int(6 + 1.6 * np.sqrt(self.dim)))\n        else:\n            self.lambda_ = int(lam)\n        self.mu = self.lambda_ // 4 if mu is None else max(1, int(mu))\n\n        # subspace rank (choose smaller than sqrt(n) to reduce cost)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim) * 0.5)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # mutation and adaptation probabilities / rates\n        self.p_levy = float(p_levy)\n        self.p_de = float(p_de)\n        self.F_de_base = float(F_de_base)\n        self.beta_D = float(beta_D)\n        self.alpha_S = float(alpha_S)\n        self.subspace_every = int(subspace_every)\n\n        # success-rate controller\n        self.success_window = int(success_window)\n        self.success_target = float(success_target)\n\n        # sigma initial multiplier (range * factor)\n        self.sigma_init_factor = float(sigma_init_factor)\n\n        # archive for DE-style perturbations\n        self.archive_max = 6000\n\n        # Mantegna Lévy parameters (1 < alpha <= 2; alpha closer to 1 is heavier tail)\n        self.levy_alpha = 1.5  # different heavy-tail exponent\n        self.levy_sigma_m = 0.7  # scale parameter used in Mantegna\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (Many Affine BBOB often -5..5)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        search_range = ub - lb\n        mean_range = np.mean(search_range)\n\n        lam = min(self.lambda_, budget)  # don't sample more than budget\n        mu = min(self.mu, lam)\n\n        # weights: linear decreasing weights (different from exponential)\n        ranks = np.arange(mu)\n        weights = (mu - ranks).astype(float)\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # initial state\n        m = np.random.uniform(lb, ub)                      # initial mean\n        sigma = max(1e-12, self.sigma_init_factor * mean_range)  # initial global step-size\n        # D stores per-coordinate RMS (std-like) estimated via EWMA of squared y's.\n        D = np.ones(n) * (0.5 * mean_range / max(1.0, np.sqrt(n)))  # initial moderate scales\n        S = np.zeros((n, n), dtype=float)  # small incremental \"covariance\" for subspace\n        if self.k >= 1:\n            rand_mat = np.random.randn(n, self.k)\n            # orthonormalize\n            try:\n                U, _ = np.linalg.qr(rand_mat)\n                U = U[:, :self.k]\n            except Exception:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # Evaluate initial mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            f = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(f)\n            f_opt = f\n            x_opt = xm.copy()\n\n        # helper: Mantegna Lévy sample for one scalar component scale\n        def mantegna_step(dim):\n            # returns a vector of length dim sampled from symmetric alpha-stable via Mantegna\n            # implementation following Mantegna (1983) approximation for alpha in (1,2]\n            alpha = self.levy_alpha\n            beta = 0.0\n            # generate u ~ N(0, sigma_u^2), v ~ N(0,1)\n            sigma_u = ( np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                       ( np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2) ) ) ** (1 / alpha)\n            u = np.random.randn(dim) * sigma_u\n            v = np.random.randn(dim)\n            step = u / (np.abs(v) ** (1.0 / alpha) + 1e-20)\n            return step\n\n        # sliding window success tracking\n        recent_successes = []\n        success_count = 0\n\n        generation = 0\n        # main loop\n        while evals < budget:\n            generation += 1\n            remaining = budget - evals\n            # propose up to lam (but not exceeding remaining eval budget)\n            cur_lam = min(lam, remaining)\n\n            # sample standard normals for axis component\n            Z_axis = np.random.randn(cur_lam, n)\n            # sample low-rank coefficients if needed\n            if self.k > 0:\n                Z_low = np.random.randn(cur_lam, self.k)\n            else:\n                Z_low = np.zeros((cur_lam, 0))\n\n            arx = np.zeros((cur_lam, n))\n            arz = np.zeros((cur_lam, n))  # normalized y = (x-m)/sigma\n\n            for i in range(cur_lam):\n                z_a = Z_axis[i]\n                # low-rank contribution\n                if self.k > 0:\n                    z_l = Z_low[i]\n                    low = U.dot(z_l)  # correlated direction\n                    # different mixing: scale by trimmed mean of D and depend on k\n                    gamma = 0.45 * np.mean(D) / (1.0 + 0.5 * np.sqrt(self.k + 1.0))\n                    y = D * z_a + gamma * low\n                else:\n                    y = D * z_a\n\n                # with some probability perform a Lévy jump replacing y\n                if np.random.rand() < self.p_levy:\n                    levy = mantegna_step(n)\n                    # normalize and scale to median coordinate scale\n                    lev_norm = np.linalg.norm(levy) + 1e-20\n                    medianD = np.median(D)\n                    y = (levy / lev_norm) * (1.2 * medianD) * (1.0 + 0.6 * np.random.rand())\n\n                # candidate in original space\n                x = m + sigma * y\n\n                # DE-style archive perturbation: rand/1 scaled by a randomized factor,\n                # occasionally add a \"current-to-rand\" style mixture.\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                    a, b, c = np.random.choice(len(archive_X), size=3, replace=False)\n                    F = self.F_de_base * (0.7 + 0.6 * np.random.rand())\n                    de_mut = F * (archive_X[a] - archive_X[b])\n                    # small current-to-rand blending\n                    CR = 0.2 + 0.6 * np.random.rand()\n                    x = x + de_mut + CR * (archive_X[c] - m)\n\n                # strictly enforce bounds\n                x = np.clip(x, lb, ub)\n                arx[i] = x\n                arz[i] = y\n\n            # Evaluate candidates (sequentially)\n            arfit = np.full(cur_lam, np.inf)\n            for i in range(cur_lam):\n                if evals >= budget:\n                    break\n                xi = arx[i]\n                fi = func(xi)\n                evals += 1\n                arfit[i] = fi\n                archive_X.append(xi.copy())\n                archive_F.append(fi)\n                if len(archive_X) > self.archive_max:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi\n                    x_opt = xi.copy()\n\n            # Selection: take top-mu by fitness (minimize)\n            idx_sorted = np.argsort(arfit)\n            sel_idx = idx_sorted[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n            f_sel = arfit[sel_idx]\n\n            # compute recombined mean (weighted average in x-space)\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # recombined y (effectively the step in normalized coords)\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # Determine success: compare best of generation to previous mean value.\n            # For simplicity, count success if the best candidate improves best-known (f_opt) OR\n            # best candidate improved over mean estimate by some threshold.\n            # We keep a sliding window of successes (1 if any selected candidate improved previous mean)\n            improved = False\n            if np.min(f_sel) < f_opt:\n                improved = True\n            else:\n                # check improvement relative to evaluating old mean (cheap check: compare to archive)\n                # approximate previous mean fitness: evaluate if we have it in archive; otherwise skip\n                # This branch avoids extra function evaluations.\n                pass\n\n            recent_successes.append(1 if improved else 0)\n            if len(recent_successes) > self.success_window:\n                recent_successes.pop(0)\n            success_rate = float(np.sum(recent_successes) / max(1, len(recent_successes)))\n\n            # Update global sigma via success-rate controller (increase if > target, decrease if <)\n            adapt_factor = np.exp(0.08 * (success_rate - self.success_target))\n            sigma *= adapt_factor\n            # clamp sigma to reasonable bounds\n            sigma = np.clip(sigma, 1e-12, 1e2 * mean_range + 1e-12)\n\n            # Update diagonal scales D via EWMA of squared y's from selected individuals:\n            # estimate per-coordinate RMS: sqrt((1-beta)*D^2 + beta * weighted_mean(y^2))\n            if y_sel.shape[0] >= 1:\n                y2_weighted = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n                D = np.sqrt((1.0 - self.beta_D) * (D ** 2) + self.beta_D * (y2_weighted + 1e-20))\n                # ensure positivity and bounds\n                D = np.clip(D, 1e-12, 1e2 * mean_range)\n            else:\n                # jitter to escape degenerate case\n                D = D * (1.0 + 1e-8 * np.random.randn(n))\n\n            # Update incremental low-rank S from normalized direction y_w (unitized)\n            ny = np.linalg.norm(y_w) + 1e-20\n            v = y_w / ny\n            S = (1.0 - self.alpha_S) * S + self.alpha_S * np.outer(v, v)\n\n            # Occasionally recompute eigenvectors for top-k subspace\n            if (self.k > 0) and ((generation % self.subspace_every) == 0):\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(S)\n                    # pick largest k\n                    if eigvecs.shape[1] >= self.k:\n                        U = eigvecs[:, -self.k:].copy()\n                except Exception:\n                    pass  # keep previous U\n\n            # stagnation handling: if many successive failures, slightly inflate sigma and randomize mean a bit\n            fail_window = max(5, self.success_window)\n            if len(recent_successes) >= fail_window and sum(recent_successes[-fail_window:]) == 0:\n                # modest boost to escape\n                sigma *= 1.8\n                # nudge mean toward a random archive element if available\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.7 * m + 0.3 * archive_X[pick]\n                # also reset small components of S to encourage new directions\n                S *= 0.6\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # loop continues until budget exhausted\n\n        # final return: best found\n        if x_opt is None:\n            xm = np.clip(m, lb, ub)\n            f_final = func(xm)\n            return float(f_final), np.array(xm, dtype=float)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "a99b834f-9e23-4db3-a765-918a3d6daefd", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "bb5431ea-d13a-47a4-afb9-8d953d29c548", "fitness": 0.3765507350671596, "name": "ARLSS", "description": "ARLSS is a hybrid population-based search that maintains a central mean m, a global step sigma and per-coordinate scales D (initial D=0.8, sigma initialized to 0.2×search‑width) and generates a lambda-sized batch each generation with mirrored sampling and mu-best recombination to update the mean. Candidate generation is governed by a 4-armed UCB bandit that adaptively chooses among axis-wise Gaussian exploitation, learned low-rank directional exploration (B learned from a FIFO success buffer via SVD; success_buf_size ≈ 6·k), Lévy-like heavy-tailed jumps (Cauchy component), and DE-style archive differential mutations (archive up to max(500,40·k)); arm statistics start conservatively (counts=1, tiny rewards) and are credited via an archetype-matching heuristic. Step-size sigma is adapted with a path-length-like update using a smoothed ps vector and chi_n normalization, while per-dimension scales D are robustly updated by exponential smoothing of median absolute selected steps; stagnation triggers sigma inflation (×2.5), bandit reset, archive nudging of the mean, and success-buffer thinning to re‑explore. Several heuristics control complexity: lambda ≈ max(6, ~2.2√n), subspace rank k≈0.6√n, subspace_refresh frequency, small exploration probability (3%), and bounded D/sigma to keep sampling stable.", "code": "import numpy as np\n\nclass ARLSS:\n    \"\"\"\n    ARLSS (Adaptive Rotating Levy-Subspace Search)\n\n    Key ideas:\n      - Maintain a mean m, global step sigma and per-coordinate scales D.\n      - Keep an archive of evaluated points.\n      - Maintain a small buffer of recent successful normalized steps to learn a low-rank\n        directional basis B (via incremental QR/PCA on the success matrix).\n      - At each candidate generation use a small multi-armed bandit (UCB) to select among:\n          * axis-wise Gaussian exploitation,\n          * low-rank directional exploration,\n          * Lévy-like heavy-tailed jumps,\n          * DE-style archive differential mutation.\n      - Adapt sigma via a path-length like rule and D via exponential smoothing of successful\n        absolute step magnitudes. The bandit learns which arm tends to produce improvements.\n      - Handle stagnation by resetting bandit statistics, inflating sigma and injecting\n        random samples from the domain.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 lambda_factor=1.0, subspace_rank=None,\n                 subspace_refresh=6, c_d=0.12, cs=0.28,\n                 alpha_success=0.08, p_learn=0.8):\n        \"\"\"\n        budget: number of function evaluations allowed\n        dim: problem dimension\n        seed: optional RNG seed\n        lambda_factor: scaling of population size (lambda = max(6, int(lambda_factor * sqrt(dim)*2)))\n        subspace_rank: optional rank for low-rank directional subspace\n        subspace_refresh: how many generations between subspace recomputations\n        c_d: per-dimension scale learning rate\n        cs: path-length smoothing constant for sigma\n        alpha_success: learning rate for updating success buffer covariance tracking\n        p_learn: fraction of population guided by learned mechanisms (vs pure random)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size heuristics\n        lam_guess = max(6, int(lambda_factor * max(6, np.floor(2.2 * np.sqrt(self.dim)))))\n        self.lambda_ = lam_guess\n        self.mu = max(1, self.lambda_ // 3)\n\n        # subspace rank\n        if subspace_rank is None:\n            self.k = max(1, int(np.ceil(0.6 * np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_rank)), self.dim)\n\n        self.subspace_refresh = int(max(1, subspace_refresh))\n        # adaptation rates\n        self.c_d = float(c_d)\n        self.cs = float(cs)\n        self.damps = 1.0 + self.cs + 0.4\n        self.alpha_success = float(alpha_success)\n        self.p_learn = float(p_learn)\n\n        # bandit arms: 0=axis Gaussian, 1=low-rank subspace, 2=levy heavy tail, 3=DE from archive\n        self.n_arms = 4\n        # initial DE factor base\n        self.F_de_base = 0.7\n\n        # stagnation controls\n        self.stag_sigma_mult = 2.5\n        self.stagnation_base = max(10, int(2.5 * self.dim / max(1, self.k)))\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # read bounds from func if available, otherwise default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initialize mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = 0.2 * np.mean(ub - lb)  # initial global step\n        D = np.ones(n) * 0.8            # per-dim scale (std-like)\n        ps = np.zeros(n)                # path for sigma\n\n        # success buffer for learning subspace: store recent normalized successful steps (rows)\n        success_buf_size = max(4, 6 * self.k)\n        success_buf = np.zeros((0, n))  # will grow until capacity, then FIFO\n        # current learned low-rank orthonormal basis B (n x k)\n        if self.k > 0:\n            rand_m = np.random.randn(n, self.k)\n            try:\n                B, _ = np.linalg.qr(rand_m)\n                B = B[:, :self.k]\n            except np.linalg.LinAlgError:\n                B = np.zeros((n, self.k))\n        else:\n            B = np.zeros((n, 0))\n\n        # archive for DE-like mutations: keep (x,f), bounded size\n        archive_X = []\n        archive_F = []\n        archive_max = max(500, 40 * self.k)\n\n        # bandit statistics\n        arm_counts = np.ones(self.n_arms, dtype=float)   # start with 1 for each arm (pseudo count)\n        arm_rewards = np.full(self.n_arms, 1e-6, dtype=float)  # cumulative reward\n\n        # recombination weights (for mu-best recombination)\n        ranks = np.arange(self.mu)\n        weights = np.exp(-ranks / max(1.0, (self.mu / 3.0)))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # constants for sigma adaptation\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n\n        # evaluate initial mean (one eval)\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            if fm < f_opt:\n                f_opt = fm; x_opt = xm.copy(); last_improvement_eval = evals\n\n        generation = 0\n        stagnation_thresh = self.stagnation_base\n\n        # main loop\n        while evals < budget:\n            generation += 1\n            remaining = budget - evals\n            current_lambda = min(self.lambda_, remaining)\n\n            # prepare storage for generated candidates\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))  # normalized step y = (x-m)/sigma\n\n            # precompute arm UCB scores\n            total_pulls = np.sum(arm_counts)\n            # average rewards\n            avg_rewards = arm_rewards / (arm_counts + 1e-12)\n            # UCB exploration term\n            ucb_terms = np.sqrt(2.0 * np.log(max(1.0, total_pulls)) / (arm_counts + 1e-12))\n            ucb_scores = avg_rewards + ucb_terms\n\n            for i in range(current_lambda):\n                # choose an arm with UCB (stochastic tie-break)\n                # with small prob choose purely random global (to avoid bandit lock)\n                if np.random.rand() < 0.03:\n                    arm = np.random.randint(self.n_arms)\n                else:\n                    # pick argmax with random tie-break\n                    candidates = np.flatnonzero(np.isclose(ucb_scores, ucb_scores.max()))\n                    arm = np.random.choice(candidates)\n\n                # generate a candidate depending on arm\n                # base random axis-wise gaussian\n                z = np.random.randn(n)\n\n                if arm == 0:\n                    # axis-wise Gaussian exploitation (small, precise)\n                    y = D * z * 0.9\n                elif arm == 1 and self.k > 0 and success_buf.shape[0] > 0:\n                    # low-rank directional mixture: combine B directions with small axis noise\n                    coeff = np.random.randn(self.k)\n                    low = B @ coeff\n                    # scale low direction by median D and by subspace adapt factor\n                    sub_scale = 0.8 * np.median(D) * (0.9 + 0.2 * np.random.rand())\n                    y = 0.5 * D * z + sub_scale * low / (np.linalg.norm(low) + 1e-12)\n                elif arm == 2:\n                    # Lévy-like heavy tail: use Cauchy scaled to D-median and sigma\n                    # produce a direction from Cauchy and combine with some axis noise\n                    r = np.random.standard_cauchy(size=1)[0]\n                    zc = np.sign(r) * (np.log1p(abs(r)) + 0.2)\n                    # random unit direction\n                    rnd_dir = np.random.randn(n)\n                    rnd_dir /= (np.linalg.norm(rnd_dir) + 1e-12)\n                    y = (0.6 * D * z) + (1.8 * np.median(D) * zc) * rnd_dir\n                elif arm == 3 and len(archive_X) >= 2:\n                    # DE-style archive differential mutation\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    F_de = self.F_de_base * (0.7 + 0.6 * np.random.rand())\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    # small Gaussian around DE mutated candidate\n                    y = D * z * 0.6 + (de_mut / max(1e-12, sigma))\n                else:\n                    # fallback: pure axis gaussian\n                    y = D * z\n\n                # occasional mirrored flip to reduce sampling variance (half of population)\n                if i % 2 == 1:\n                    y = -y\n\n                x = m + sigma * y\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n                # update local arm UCB info (increment count immediately)\n                arm_counts[arm] += 1.0\n                # small decay of UCB scores to favor recent changes\n                # (we don't immediately add reward; reward is added after evaluation)\n\n                # recompute ucb_scores for next pick (lightweight)\n                total_pulls = np.sum(arm_counts)\n                avg_rewards = arm_rewards / (arm_counts + 1e-12)\n                ucb_terms = np.sqrt(2.0 * np.log(max(1.0, total_pulls)) / (arm_counts + 1e-12))\n                ucb_scores = avg_rewards + ucb_terms\n\n            # Evaluate candidates (sequentially)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n\n                # add to archive (bounded)\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > archive_max:\n                    archive_X.pop(0); archive_F.pop(0)\n\n                # update best\n                if f < f_opt:\n                    # reward = improvement amount\n                    improvement = max(0.0, f_opt - f)\n                    f_opt = f; x_opt = x.copy(); last_improvement_eval = evals\n                    # assign full improvement reward to the arm that generated this candidate:\n                    # attempt to identify arm: recompute approximate y that matches arz[i]\n                    # we don't track arm per-candidate explicitly, but approximate by evaluating which arm would produce arz[i]\n                    # Simpler: credit arms by proximity to archetypal outputs (cheap heuristic): compute dot product\n                    # We'll instead credit the last arm chosen in generation loop: estimate by re-evaluating UCB pick order.\n                    # For reliability we credit the arm with maximum contribution to arz[i] among arms:\n                    # compute four hypothetical archetypal y_arm and pick closest.\n                    # construct archetypes:\n                    arche = []\n                    # arche 0\n                    z0 = arz[i] / (D + 1e-12)\n                    arche.append(D * (np.sign(z0) * np.minimum(abs(z0), 5.0)))\n                    # arche 1\n                    if self.k > 0:\n                        # project arz onto B\n                        proj = np.zeros(n)\n                        coeff = B.T @ arz[i]\n                        proj = B @ coeff\n                        arche.append(0.5 * D * np.random.randn(n) + 0.2 * proj)\n                    else:\n                        arche.append(D * np.random.randn(n))\n                    # arche 2\n                    rnd_dir = np.random.randn(n); rnd_dir /= (np.linalg.norm(rnd_dir) + 1e-12)\n                    arche.append(0.6 * D * np.random.randn(n) + 1.0 * rnd_dir)\n                    # arche 3\n                    arche.append(0.6 * D * np.random.randn(n))\n                    # pick closest archetype to the actual arz[i]\n                    dists = [np.linalg.norm(arz[i] - a) for a in arche]\n                    chosen_arm = int(np.argmin(dists))\n                    arm_rewards[chosen_arm] += improvement + 1e-8\n                else:\n                    # small negative or zero improvement: give tiny reward to the generating arm so counts make sense\n                    # We'll again approximate chosen_arm same as above\n                    arche = []\n                    z0 = arz[i] / (D + 1e-12)\n                    arche.append(D * (np.sign(z0) * np.minimum(abs(z0), 5.0)))\n                    if self.k > 0:\n                        coeff = B.T @ arz[i]; proj = B @ coeff\n                        arche.append(0.5 * D * np.random.randn(n) + 0.2 * proj)\n                    else:\n                        arche.append(D * np.random.randn(n))\n                    rnd_dir = np.random.randn(n); rnd_dir /= (np.linalg.norm(rnd_dir) + 1e-12)\n                    arche.append(0.6 * D * np.random.randn(n) + 1.0 * rnd_dir)\n                    arche.append(0.6 * D * np.random.randn(n))\n                    dists = [np.linalg.norm(arz[i] - a) for a in arche]\n                    chosen_arm = int(np.argmin(dists))\n                    # give tiny reward proportional to small negative improvement (encourage exploration)\n                    arm_rewards[chosen_arm] += max(1e-6, (f_opt - f) * 1e-4)\n\n                # update success buffer and per-dim scales D if candidate improved over mean (or was among top mu later)\n                # For simplicity: if candidate improved global best or is within top 30% of current batch, consider it successful\n                # We'll perform selection below to pick top mu and use them to update D and success buffer.\n\n            # Selection (lower fitness is better)\n            idx = np.argsort(arfit)\n            sel_idx = idx[:self.mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            m_old = m.copy()\n            # recombine in x-space\n            # if less than mu available (some inf), clip weights\n            effective_mu = x_sel.shape[0]\n            if effective_mu >= 1:\n                w = weights[:effective_mu]\n                w = w / np.sum(w)\n                m = np.sum(w[:, None] * x_sel, axis=0)\n                # weighted mean step in y-space\n                y_w = np.sum(w[:, None] * y_sel, axis=0)\n            else:\n                # nothing selected (edge case), do a random step\n                rnd = np.random.randn(n)\n                y_w = D * rnd\n                m = m + 0.5 * sigma * y_w\n\n            # update subspace statistics using normalized y_w\n            norm_yw = np.linalg.norm(y_w) + 1e-20\n            v = y_w / norm_yw\n            # add to success buffer if this generation produced improvements (relative)\n            # Use as success if any of selected had fitness <= current best + small tol\n            selected_fits = arfit[sel_idx]\n            if np.any(selected_fits <= f_opt + 1e-12) or (np.median(selected_fits) < np.median(archive_F[-min(len(archive_F), 20):])):\n                # push normalized vector into success buffer\n                vec = v.copy()[None, :]\n                if success_buf.shape[0] < success_buf_size:\n                    success_buf = np.vstack([success_buf, vec])\n                else:\n                    # FIFO: drop oldest\n                    success_buf = np.vstack([success_buf[1:], vec])\n\n            # occasionally recompute B via PCA / SVD of success_buf\n            if (generation % self.subspace_refresh) == 0 and success_buf.shape[0] >= max(2, self.k):\n                try:\n                    # center success buffer (should be already zero-mean but ensure)\n                    Sb = success_buf - np.mean(success_buf, axis=0)\n                    # SVD on small matrix (rows x n), compute top-k right singular vectors\n                    U_s, svals, Vt = np.linalg.svd(Sb, full_matrices=False)\n                    V = Vt.T\n                    if V.shape[1] >= self.k:\n                        B = V[:, :self.k].copy()\n                except np.linalg.LinAlgError:\n                    pass\n\n            # path-length style update for sigma\n            inv_sqrtD = 1.0 / (np.sqrt(D) + 1e-12)\n            ps = (1.0 - self.cs) * ps + np.sqrt(self.cs * (2.0 - self.cs) * mu_eff) * (y_w * inv_sqrtD)\n            norm_ps = np.linalg.norm(ps)\n            sigma *= np.exp((self.cs / self.damps) * (norm_ps / chi_n - 1.0))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e3 * np.mean(ub - lb) + 1e-12)\n\n            # robust per-dim scale update using selected y's absolute values\n            if y_sel.shape[0] >= 1:\n                med_abs = np.median(np.abs(y_sel), axis=0)\n                approx_std = 1.4826 * (med_abs + 1e-20)\n                # exponential smoothing to keep inertia\n                D = (1.0 - self.c_d) * D + self.c_d * (approx_std + 1e-12)\n            else:\n                # jitter\n                D = D * (1.0 + 1e-6 * np.random.randn(n))\n\n            D = np.clip(D, 1e-12, 1e3)\n\n            # stagnation detection and handling\n            if (evals - last_improvement_eval) > stagnation_thresh:\n                # inject global exploration burst\n                sigma *= self.stag_sigma_mult\n                # reset bandit (encourage exploration)\n                arm_counts = np.ones(self.n_arms, dtype=float)\n                arm_rewards = np.full(self.n_arms, 1e-6, dtype=float)\n                # nudge mean towards a random archive element or random point if archive empty\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.5 * m + 0.5 * archive_X[pick]\n                else:\n                    m = np.random.uniform(lb, ub)\n                # clear success buffer gently\n                success_buf = success_buf[max(0, success_buf.shape[0] - 2):, :]\n                # enlarge D slightly to encourage exploration\n                D = D * (1.0 + 0.25)\n\n                # lengthen stagnation threshold for next time\n                stagnation_thresh = int(stagnation_thresh * 1.5) + 5\n\n            # enforce mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # safety stop\n            if evals >= budget:\n                break\n\n        # final return\n        if x_opt is None:\n            xm = np.clip(m, lb, ub)\n            f_final = func(xm)\n            return float(f_final), np.array(xm, dtype=float)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ARLSS scored 0.377 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "a99b834f-9e23-4db3-a765-918a3d6daefd", "operator": null, "metadata": {"aucs": [0.20328938841723987, 0.18658228266366972, 0.5078834458956442, 0.8786371300537188, 0.3595849585666203, 0.6491342784738561, 0.27046496283410537, 0.2659012687548795, 0.2735789215779003, 0.17045071343396134]}, "task_prompt": ""}
{"id": "a5ee28c0-4bdb-4cae-94e4-31209b1973d3", "fitness": "-inf", "name": "AMSE_LD", "description": "The algorithm is an adaptive, hybrid evolutionary strategy that mixes a low-rank learned subspace (S, U) with multiplicative per-coordinate scaling D and a small population (lambda sized by log(n), mu ≈ lambda/2) where the subspace rank k is set slightly larger than sqrt(n) to capture dominant directions. Sampling blends Gaussian per-coordinate perturbations (y = D*z) with occasional low-rank contributions (gamma·U·z_low), mirrored antithetic draws, intermittent heavy-tailed radial Cauchy hops (p_cauchy≈0.12) and DE-style archive mutations (p_de≈0.18) to promote global moves and reuse past good points. Adaptation uses a path-length update for sigma (with small cs and damping) but computes the path in D-scaled coordinates, while the diagonal D is updated multiplicatively from MAD-based approximate standard deviations (c_d small = 0.02) and S is incrementally learned (alpha_S≈0.12) with eigendecomposition every few generations to refresh U. Robustness features include a small initial sigma (0.08·range), archive management, strict bound clipping, stagnation handling via sigma inflation (×1.5), partial coordinate randomization mixing archive/uniform anchors, and conservative safeguards on D and sigma to avoid collapse or explosion.", "code": "import numpy as np\n\nclass AMSE_LD:\n    \"\"\"\n    AMSE-LD (Adaptive Mirror-Subspace Evolution with Levy-DE hops)\n\n    Key features (different parameterization / formula choices from the provided OSDAS):\n      - Different population sizing and subspace rank heuristic\n      - Multiplicative diagonal adaptation (geometric blending) rather than linear blending\n      - Faster incremental subspace learning rate, different gamma mixing formula (mean-based)\n      - Path-length uses division by D (not sqrt(D)) and different cs/damping constants\n      - Different initial sigma and DE/Cauchy probabilities\n      - Different stagnation handling (partial coordinate randomization + sigma inflation)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population sizing (different scaling)\n        self.lambda_ = max(6, int(4 + np.floor(3.0 * np.log(max(1, self.dim + 1)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace rank (slightly larger than sqrt(n) factor)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim) * 1.1)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # algorithm hyperparameters (changed)\n        self.c_d = 0.02            # smaller blending for multiplicative update (more inertia)\n        self.alpha_S = 0.12        # faster incremental covariance learning\n        self.p_cauchy = 0.12       # probability for heavy-tailed Levy/Cauchy jump\n        self.p_de = 0.18           # probability of applying DE-style archive mutation\n        self.F_de_base = 0.85      # base DE factor\n        self.mirrored = True\n        self.subspace_update_every = 10  # eigen-decompose S every few generations\n        self.stagnation_mult = 1.5  # sigma multiplier on stagnation\n        self.archive_max = 1000     # smaller archive cap\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (Many Affine BBOB standard -5..5 but use func-provided bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n\n        # recombination weights: linear decreasing (different from exponential)\n        weights = np.linspace(1.0, 0.1, mu)\n        weights = np.maximum(weights, 1e-12)\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length constants (different formula & scaling)\n        cs = 0.15 * mu_eff / (n + mu_eff + 1.0)\n        damps = 1.0 + 1.2 * cs\n        # slightly different chi_n approximation\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * (n ** 2 + 1.0)))\n\n        # initial state\n        m = self.rng.uniform(lb, ub)                      # initial mean\n        sigma = 0.08 * np.mean(ub - lb)                   # different initial step-size (smaller)\n        D = np.ones(n)                                    # per-coordinate scales (std-like)\n        ps = np.zeros(n)                                  # evolution path for sigma\n\n        # incremental covariance S and subspace U\n        S = np.zeros((n, n), dtype=float)\n        if self.k >= 1:\n            rand_mat = self.rng.randn(n, self.k)\n            # orthonormalize via QR\n            try:\n                Q, _ = np.linalg.qr(rand_mat)\n                U = Q[:, :self.k].copy()\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        # lightweight archive for DE-style mutations\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n\n        # Evaluate initial mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            f = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(f)\n            if f < f_opt:\n                f_opt = f\n                x_opt = xm.copy()\n                last_improvement_eval = evals\n\n        # stagnation threshold (different heuristic)\n        stagnation_thresh = max(12, int(4 * n / max(1, self.k)))\n\n        generation = 0\n        # main loop\n        while evals < budget:\n            generation += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # base gaussian draws\n            base_z = self.rng.randn(current_lambda, n)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))  # y = (x - m) / sigma\n\n            for idx in range(current_lambda):\n                z = base_z[idx].copy()\n\n                # low-rank contribution\n                if self.k > 0:\n                    z_low = self.rng.randn(self.k)\n                    low = U @ z_low\n                    # different mixing weight: proportional to mean(D), scaled with a mild factor\n                    gamma = 0.4 * np.mean(D) / (1.0 + 0.2 * self.k)\n                    y = D * z + gamma * low\n                else:\n                    y = D * z\n\n                # occasional heavy-tailed Levy/Cauchy jump: radial scaled Cauchy (different scale)\n                if self.rng.rand() < self.p_cauchy:\n                    r = self.rng.standard_cauchy()\n                    nz = np.linalg.norm(z) + 1e-20\n                    # use log-like soft-scaling but slightly larger influence than OSDAS\n                    scale = np.log1p(1.0 + abs(r)) * (0.8 + 0.6 * self.rng.rand())\n                    y = np.sign(r) * (z / nz) * scale * np.mean(D)\n\n                # mirrored sampling (we flip every second sample)\n                if self.mirrored and (idx % 2 == 1):\n                    y = -y\n\n                x = m + sigma * y\n\n                # DE rand/1-style archive mutation sometimes (if archive has at least 2 points)\n                if (self.rng.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                    # randomized F_de drawn around base\n                    F_de = self.F_de_base * (0.6 + 0.8 * self.rng.rand())\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # strict clipping to bounds\n                x = np.clip(x, lb, ub)\n\n                arx[idx] = x\n                arz[idx] = y\n\n            # Evaluate candidates sequentially (respect budget)\n            arfit = np.full(current_lambda, np.inf)\n            for idx in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[idx]\n                f = func(x)\n                evals += 1\n                arfit[idx] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > self.archive_max:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection & recombination (lower fitness is better)\n            idx_sorted = np.argsort(arfit)\n            sel_idx = idx_sorted[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            m_old = m.copy()\n            # weighted recombination in x-space\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # incremental covariance S update (normalized outer product, different normalization)\n            norm_yw = np.linalg.norm(y_w) + 1e-20\n            v = y_w / norm_yw\n            # incorporate scaled outer product but normalized to keep scale stable\n            S = (1.0 - self.alpha_S) * S + self.alpha_S * np.outer(v, v)\n\n            # refresh low-rank subspace occasionally (eigendecomposition of S)\n            if (generation % self.subspace_update_every) == 0 and self.k > 0:\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(S)\n                    if eigvecs.shape[1] >= self.k:\n                        # take top-k eigenvectors (largest eigenvalues)\n                        U = eigvecs[:, -self.k:].copy()\n                except np.linalg.LinAlgError:\n                    pass\n\n            # path update: divide by D (different from sqrt(D) scaling)\n            invD = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invD)\n            norm_ps = np.linalg.norm(ps)\n\n            # sigma update (slightly different damping usage)\n            sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e3 * np.mean(ub - lb) + 1e-12)\n\n            # multiplicative diagonal adaptation using MAD -> approximate std\n            if y_sel.shape[0] >= 1:\n                med = np.median(y_sel, axis=0)\n                mad = np.median(np.abs(y_sel - med), axis=0)\n                approx_std = 1.4826 * (mad + 1e-20)\n                # multiplicative update: D <- D * exp(c_d*(approx_std / D - 1))\n                ratio = approx_std / (D + 1e-20)\n                D = D * np.exp(self.c_d * (ratio - 1.0))\n            else:\n                # small random jitter\n                D = D * (1.0 + 1e-6 * self.rng.randn(n))\n\n            # bound D\n            D = np.clip(D, 1e-12, 1e3)\n\n            # stagnation detection and handling (different policy)\n            if (evals - last_improvement_eval) > stagnation_thresh:\n                # moderate sigma inflation\n                sigma *= self.stagnation_mult\n                # partially randomize a subset of coordinates of the mean to escape plateaus\n                if len(archive_X) > 0:\n                    # choose 10%-40% of dims to randomize\n                    frac = min(0.4, max(0.1, 0.1 + 0.1 * self.rng.rand()))\n                    k_dims = max(1, int(np.floor(frac * n)))\n                    dims = self.rng.choice(n, size=k_dims, replace=False)\n                    # replace those coordinates with random mix of archive and uniform\n                    if self.rng.rand() < 0.6:\n                        pick = self.rng.randint(len(archive_X))\n                        anchor = archive_X[pick]\n                        m[dims] = 0.5 * m[dims] + 0.5 * anchor[dims]\n                    else:\n                        m[dims] = self.rng.uniform(lb[dims], ub[dims])\n                # slightly decay S to encourage new directions\n                S *= 0.6\n                # small perturbation to ps to avoid stalling\n                ps += 0.1 * self.rng.randn(n)\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n            if evals >= budget:\n                break\n\n        # result\n        if x_opt is None:\n            xm = np.clip(m, lb, ub)\n            f_final = func(xm)\n            return float(f_final), np.array(xm, dtype=float)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 194, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,1) (4,10) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "error": "In the code, line 194, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,1) (4,10) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "parent_ids": "a99b834f-9e23-4db3-a765-918a3d6daefd", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9b902e67-fe2a-4d90-b3d3-7d1f075043e4", "fitness": 0.17929506413066632, "name": "HLCDE", "description": "1) Hybrid covariance: samples combine a diagonal scale D and a learned low-rank subspace U (rank k ≈ 0.85√n) with per-mode energies svals/S_low, mixing them by a gamma factor to produce anisotropic steps that are cheap to represent.  \n2) CMA-like population and step control: population size λ scales modestly with log(dim), log-based recombination weights produce a weighted mean and a path ps for multiplicative sigma adaptation (path-length control), while D is adapted by exponential blending toward the weighted second moment of selected steps.  \n3) Robust exploration primitives: mirrored sampling for variance reduction, occasional heavy-tailed Cauchy jumps (directional scaling by y) and DE-style archive difference mutations provide large, diverse moves, and an archive of past points is kept and biased for nudges.  \n4) Low-rank learning and recovery: successful weighted steps are buffered and periodically SVD’d to refresh U and svals (incremental S_low updates between refreshes), with stagnation detection that inflates sigma, nudges the mean toward good archived points, and partially resets low-rank memory; practical clamping of D, sigma and energies keeps behavior numerically stable.", "code": "import numpy as np\n\nclass HLCDE:\n    \"\"\"\n    HLCDE (Hybrid Low-rank + Diagonal Covariance with DE & Cauchy)\n\n    One-line: Mix diagonal scaling and a learned low-rank correlated subspace,\n    use mirrored sampling, path-length sigma control, robust D updates, periodic\n    SVD-based subspace refresh, DE-style archive differences and occasional\n    heavy-tailed jumps to robustly optimize bounded continuous problems.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # Population sizing similar to CMA-family but moderate\n        self.lambda_ = max(6, int(6 + np.floor(3.0 * np.log(max(1, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace rank default ~0.85*sqrt(n)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim) * 0.85)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # adaptation hyper-parameters (sensible defaults)\n        self.c_d = 0.18          # diagonal blending rate\n        self.cs = None           # path-length constant (computed per problem dim & mu_eff)\n        self.damps = None\n        self.alpha_sub = 0.08    # incremental subspace covariance learning\n        self.subspace_update_every = max(3, int(3))  # generations between SVD refresh\n        self.mirrored = True\n        self.p_cauchy = 0.10\n        self.p_de = 0.22\n        self.F_de_base = 0.7\n        self.archive_max = 5000\n        self.stagnation_mult = 1.8\n        self.eps = 1e-12\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initialize dynamic state\n        lam = self.lambda_\n        mu = self.mu\n\n        # ensure mu <= lam\n        mu = min(mu, lam)\n\n        # recombination weights (log-based) will be recomputed per-generation when mu changes\n        def make_weights(mu_gen):\n            w = np.log(mu_gen + 0.5) - np.log(np.arange(1, mu_gen + 1))\n            w = w / np.sum(w)\n            mu_eff = 1.0 / np.sum(w ** 2)\n            return w, mu_eff\n\n        weights, mu_eff_global = make_weights(mu)\n\n        # path-length settings based on mu_eff and n\n        cs = (mu_eff_global + 2) / (n + mu_eff_global + 5.0)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff_global - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial mean, sigma\n        m = np.random.uniform(lb, ub)\n        sigma = 0.18 * np.mean(ub - lb)\n\n        # diagonal scales D (std-like)\n        D = np.ones(n)\n\n        # low-rank subspace U (n x k) and low-rank eigenvalues (energy per mode)\n        if self.k >= 1:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                U, _ = np.linalg.qr(rand_mat)\n                U = U[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n        # low-rank energy diag (positive)\n        svals = np.ones(self.k)\n\n        # incremental low-rank covariance (in U-basis)\n        S_low = np.eye(self.k) * 1e-6 if self.k > 0 else np.zeros((0, 0))\n\n        # paths and buffers\n        ps = np.zeros(n)\n        success_buffer = []  # store recent weighted steps y_w for SVD\n        buffer_max = max(8 * self.k, 20)\n\n        # archive\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n        generation = 0\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            if fm < f_opt:\n                f_opt = fm\n                x_opt = xm.copy()\n                last_improvement_eval = evals\n\n        # main loop\n        while evals < budget:\n            generation += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # if current_lambda changed and smaller than mu, recompute weights/mu_eff for this gen\n            mu_gen = max(1, min(mu, current_lambda))\n            weights, mu_eff = make_weights(mu_gen)\n            # recompute path constants with mu_eff (small change ok)\n            cs = (mu_eff + 2) / (n + mu_eff + 5.0)\n            damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n\n            # prepare storage\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))   # stored y = (x - m)/sigma\n\n            # mirrored sampling: generate half and mirror\n            half = (current_lambda + 1) // 2\n            # base gaussian for diagonal part\n            base_z = np.random.randn(half, n)\n            # low-rank normals for low-rank part\n            if self.k > 0:\n                base_z_low = np.random.randn(half, self.k)\n            else:\n                base_z_low = np.zeros((half, 0))\n\n            # mixing weight for low-rank part (adaptive): proportional to median(D) scaled by subspace energy\n            gamma = 0.7 * np.median(D) if self.k > 0 else 0.0\n            # clamp gamma modestly\n            gamma = max(0.02 * np.mean(D), min(gamma, 2.0 * np.mean(D)))\n\n            for i in range(half):\n                z = base_z[i].copy()\n                # low-rank draw\n                if self.k > 0:\n                    zlow = base_z_low[i].copy()\n                    low = U @ (np.sqrt(np.maximum(svals, self.eps)) * zlow)\n                    y = D * z + gamma * low\n                else:\n                    y = D * z\n\n                # first sample\n                idx1 = 2 * i\n                if idx1 < current_lambda:\n                    arx[idx1] = np.clip(m + sigma * y, lb, ub)\n                    arz[idx1] = y\n\n                # mirrored counterpart\n                idx2 = 2 * i + 1\n                if (idx2 < current_lambda):\n                    arx[idx2] = np.clip(m - sigma * y, lb, ub)\n                    arz[idx2] = -y\n\n            # apply occasional heavy-tailed and DE mutations and evaluate\n            arfit = np.full(current_lambda, np.inf)\n            for k_idx in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[k_idx].copy()\n                y = arz[k_idx].copy()\n\n                # occasionally replace by scaled Cauchy jump (in direction of y to keep anisotropy)\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy()\n                    # scale cauchy to avoid excessive explosions; base on median(D) and sigma\n                    c_scale = (0.8 + 0.8 * np.tanh(abs(r) / 3.0)) * np.median(D) * sigma\n                    dir_norm = np.linalg.norm(y) + 1e-20\n                    x = np.clip(m + c_scale * (y / dir_norm), lb, ub)\n\n                # sometimes apply DE-like archive difference mutation\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    F_de = self.F_de_base * (0.8 + 0.4 * np.random.rand())\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = np.clip(x + de_mut, lb, ub)\n\n                # evaluate\n                f = func(x)\n                evals += 1\n                arfit[k_idx] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                # bound archive\n                if len(archive_X) > self.archive_max:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                # update best\n                if f < f_opt:\n                    f_opt = f\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection & recombination (lower fitness better)\n            idx_sort = np.argsort(arfit)\n            sel_idx = idx_sort[:mu_gen]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]  # y = (x - m)/sigma approximately\n\n            m_old = m.copy()\n            # weighted recombination in x-space (new mean)\n            # guard if some arfit are inf (because budget exhausted) -> ensure shapes match\n            # use weights[:mu_gen]\n            w_used = weights[:mu_gen]\n            m = np.sum(w_used[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(w_used[:, None] * y_sel, axis=0)\n\n            # update path ps using diag-approx inverse sqrt (1/D)\n            invsqrt_approx = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invsqrt_approx)\n            norm_ps = np.linalg.norm(ps)\n\n            # sigma adaptation\n            sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n            # keep sigma in reasonable bounds relative to domain\n            dom_scale = np.mean(ub - lb)\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e3 * dom_scale + 1e-12)\n\n            # robust diagonal adaptation (exponential blend of second moment of selected y)\n            y2 = np.sum(w_used[:, None] * (y_sel ** 2), axis=0)\n            D2 = (1.0 - self.c_d) * (D ** 2) + self.c_d * (y2 + 1e-20)\n            D = np.sqrt(np.maximum(D2, 1e-12))\n\n            # update low-rank residual energy using projection of y_w onto U\n            if self.k > 0:\n                proj = U.T @ y_w  # k-vector\n                # incremental S_low update (outer product in U-basis)\n                S_low = (1.0 - self.alpha_sub) * S_low + self.alpha_sub * np.outer(proj, proj)\n                # set svals to diagonal energies (clamped)\n                svals = np.maximum(np.diag(S_low), 1e-12)\n\n            # push weighted successful step into buffer for subspace learning\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # periodic subspace refresh via SVD of buffered y's\n            if (self.k > 0) and ((generation % self.subspace_update_every) == 0) and (len(success_buffer) >= self.k):\n                # build matrix n x m_buf\n                Y = np.vstack(success_buffer).T  # n x m_buf\n                # center\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    U_new, svals_full, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        # energy: scaled singular values squared (approx variance in those modes)\n                        svals = np.maximum((svals_full[:k_take] ** 2) / max(1.0, np.sum(svals_full[:k_take] ** 2)), 1e-12)\n                        # resize S_low accordingly\n                        S_low = np.diag(svals)\n                except np.linalg.LinAlgError:\n                    # ignore and keep previous U\n                    pass\n\n            # stagnation detection & recovery\n            stagnation_thresh = max(8, int(3 * n / max(1, self.k)))\n            if (evals - last_improvement_eval) > stagnation_thresh:\n                sigma *= self.stagnation_mult\n                if len(archive_X) > 0:\n                    # nudge mean toward a random good archive point (prob biased to better)\n                    # sample from archive with bias to lower function values\n                    probs = np.exp(-0.5 * (np.array(archive_F) - min(archive_F)))\n                    probs = probs / (np.sum(probs) + 1e-20)\n                    pick = np.random.choice(len(archive_X), p=probs)\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                # reset some low-rank memory to encourage new directions\n                if self.k > 0:\n                    S_low *= 0.4\n                    svals = np.maximum(np.diag(S_low), 1e-12)\n                success_buffer = []\n                last_improvement_eval = evals  # avoid repeated immediate triggers\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # loop continues until budget exhausted\n\n        # return best found\n        # in case nothing improved, ensure we return something meaningful\n        if x_opt is None:\n            xm = np.clip(m, lb, ub)\n            f_final = func(xm)\n            return float(f_final), np.array(xm, dtype=float)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HLCDE scored 0.179 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "a99b834f-9e23-4db3-a765-918a3d6daefd", "operator": null, "metadata": {"aucs": [0.12697797756764484, 0.18392277369372556, 0.2284553111173132, 0.15739644935522124, 0.17165304734681008, 0.2182731920452362, 0.21660536358312532, 0.17915589329671744, 0.1702393536684199, 0.14027127963244934]}, "task_prompt": ""}
{"id": "9ad6f5e1-8f81-4b0a-8491-eb8a7a15f593", "fitness": 0.24997958739981935, "name": "SMARS", "description": "SMARS probes randomized low-dimensional affine subspaces (k ≈ ceil(√n), probes = max(4,2k)) and uses mirrored Gaussian sampling in those subspaces combined with an S-like subspace covariance S and cumulation path ps (cs=0.32, cov_beta=0.18) to adapt preferred search directions and scale (step initialized as 0.38·mean(range), min_step tiny). It applies per-coordinate RMS scaling (coord_var, coord_alpha) to favor coordinates with recent movement, keeps a small LRU memory of successful unit directions (memory_size=8) and augments exploration with occasional archive-difference (DE-style) perturbations and Cauchy Lévy-like jumps (mem_jump_prob, p_cauchy, cauchy_scale) to escape local traps. Budget-aware exploitation is provided by short 1D/mirrored line searches after successes and rare diagonal quadratic fits from a top-archive (model_every) to propose cautious steps toward estimated minima, while an archive with pruning stores past evaluations for modeling and DE diffs. Step-size is adapted by cumulation norm and empirical success rate (succ_grow/succ_shrink), and many design choices bias conservative, robust search (small cov_beta, clipped cauchy, mirrored sampling, budget-aware search limits).", "code": "import numpy as np\n\nclass SMARS:\n    \"\"\"\n    Subspace-Memory Adaptive Random Search (SMARS)\n\n    One-line: Probe randomized low-dimensional affine subspaces with a compact subspace covariance\n    and cumulation path, use mirrored sampling and per-coordinate RMS scaling, keep an LRU memory\n    of successful directions, augment exploration with occasional DE-diffs and Cauchy memory jumps,\n    perform budget-aware short 1D refinement along successes, and occasionally try a cheap diagonal\n    quadratic model from the archive for focused exploitation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=8, model_every=20, mem_jump_prob=0.18, cauchy_scale=0.6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.model_every = int(model_every)\n        self.mem_jump_prob = float(mem_jump_prob)\n        self.cauchy_scale = float(cauchy_scale)\n        self.rng = np.random.default_rng(seed)\n\n        # subspace / probing defaults (SCLS-like)\n        self.p_de = 0.22\n        self.F_de = 0.8\n        self.p_cauchy = 0.12\n\n        # cumulation & covariance-adapt (in subspace)\n        self.cs = 0.32\n        self.cov_beta = 0.18\n\n        # step adapt multipliers\n        self.succ_grow = 1.12\n        self.succ_shrink = 0.86\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # bounds handling (BBOB typical -5..5)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n\n        # initial point and archive\n        x_cur = self.rng.uniform(lb, ub)\n        evals = 0\n        X_archive = []\n        F_archive = []\n\n        # safe evaluation wrapper\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None\n            x_clipped = np.clip(x, lb, ub)\n            try:\n                f = float(func(x_clipped))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x_clipped.copy())\n            F_archive.append(f)\n            return float(f), x_clipped\n\n        out = safe_eval(x_cur)\n        if out is None:\n            return float(np.inf), np.zeros(n)\n        f_cur, x_cur = out\n        f_best = float(f_cur)\n        x_best = x_cur.copy()\n\n        # initialization\n        step = 0.38 * max(1e-12, np.mean(domain_range))\n        min_step = 1e-9 * max(1.0, np.mean(domain_range))\n\n        # subspace dimension k and small covariance S (in subspace coords)\n        k = max(1, int(np.ceil(np.sqrt(n))))\n        probes = max(4, 2 * k)\n        S = np.eye(k)\n        ps = np.zeros(k)\n        B_sub = np.eye(k)\n        D_sub = np.ones(k)\n        invsqrtS = np.eye(k)\n\n        # per-coordinate RMS scaling (from MDARSS)\n        coord_var = np.ones(n) * 1e-3\n        coord_alpha = 0.18\n        coord_eps = 1e-12\n\n        # LRU memory of successful unit directions in full space\n        dir_memory = []\n\n        # bookkeeping\n        iter_count = 0\n\n        # short, budget-aware golden-like 1D search (small number of evals)\n        def short_line_search(x0, f0, d, init_step, max_evals=8):\n            nonlocal evals\n            if evals >= self.budget:\n                return None\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None\n            d = d / dnrm\n            remaining = min(self.budget - evals, max_evals)\n            if remaining <= 0:\n                return None\n            # bracket +/- init_step\n            a = 0.0\n            fa = f0\n            b = init_step\n            xb = np.clip(x0 + b * d, lb, ub)\n            out = safe_eval(xb)\n            if out is None:\n                return None\n            fb, xb = out\n            remaining -= 1\n            if fb >= fa:\n                b = -init_step\n                xb = np.clip(x0 + b * d, lb, ub)\n                out = safe_eval(xb)\n                if out is None:\n                    return None\n                fb, xb = out\n                remaining -= 1\n                if fb >= fa:\n                    return None\n            # limited golden search in [0,b]\n            gr = (np.sqrt(5) - 1) / 2\n            left = a\n            right = b\n            c = right - gr * (right - left)\n            dd = left + gr * (right - left)\n            xc = np.clip(x0 + c * d, lb, ub)\n            out = safe_eval(xc)\n            if out is None:\n                return None\n            fc, xc = out\n            remaining -= 1\n            xd = np.clip(x0 + dd * d, lb, ub)\n            out = safe_eval(xd)\n            if out is None:\n                return None\n            fd, xd = out\n            remaining -= 1\n            best_f = fa\n            best_x = x0.copy()\n            for val,xx in ((fc,xc),(fd,xd)):\n                if val < best_f:\n                    best_f = val; best_x = xx.copy()\n            iters = 0\n            max_iters = remaining\n            while iters < max_iters and abs(right - left) > 1e-12:\n                iters += 1\n                if fc < fd:\n                    right = dd\n                    dd = c\n                    fd = fc\n                    c = right - gr * (right - left)\n                    xc = np.clip(x0 + c * d, lb, ub)\n                    out = safe_eval(xc)\n                    if out is None:\n                        break\n                    fc, xc = out\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    left = c\n                    c = dd\n                    fc = fd\n                    dd = left + gr * (right - left)\n                    xd = np.clip(x0 + dd * d, lb, ub)\n                    out = safe_eval(xd)\n                    if out is None:\n                        break\n                    fd, xd = out\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None\n\n        # periodic diagonal quadratic model proposal using top archive points\n        def try_diagonal_model():\n            nonlocal f_best, x_best, evals\n            if len(X_archive) < (n + 3):\n                return False\n            ksel = min(len(X_archive), 2 * n + 8)\n            idx = np.argsort(F_archive)[:ksel]\n            Xm = np.array([X_archive[i] for i in idx])\n            Fm = np.array([F_archive[i] for i in idx])\n            A = np.hstack([Xm ** 2, Xm, np.ones((Xm.shape[0], 1))])\n            reg = 1e-6\n            try:\n                sol, *_ = np.linalg.lstsq(A.T @ A + reg * np.eye(A.shape[1]), A.T @ Fm, rcond=None)\n                sol = sol.flatten()\n                a_diag = sol[:n]\n                b_lin = sol[n:2 * n]\n                a_safe = a_diag.copy()\n                # regularize tiny curvatures\n                a_safe[np.abs(a_safe) < 1e-8] = np.sign(a_safe[np.abs(a_safe) < 1e-8]) * 1e-8 + 1e-8\n                x_star = -0.5 * b_lin / (a_safe + 1e-20)\n                # propose a cautious step from current best towards x_star\n                x_prop = np.clip(x_best + 0.5 * (x_star - x_best), lb, ub)\n                out = safe_eval(x_prop)\n                if out is None:\n                    return False\n                f_prop, x_prop = out\n                if f_prop < f_best - 1e-12:\n                    f_best = f_prop\n                    x_best = x_prop.copy()\n                    return True\n            except Exception:\n                return False\n            return False\n\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n            # adapt k/probes slightly\n            k = max(1, int(np.clip(int(np.ceil(np.sqrt(n))), 1, n)))\n            probes = max(4, 2 * k)\n\n            # build subspace basis using some memory columns\n            use_mem = min(len(dir_memory), k // 2)\n            basis_cols = []\n            if use_mem > 0:\n                for i in range(use_mem):\n                    basis_cols.append(dir_memory[i].copy())\n            needed = k - len(basis_cols)\n            if needed > 0:\n                R = self.rng.standard_normal(size=(n, needed))\n                if basis_cols:\n                    R = np.column_stack((np.column_stack(basis_cols), R))\n                Q, _ = np.linalg.qr(R, mode='reduced')\n                basis = Q[:, :k]\n            else:\n                Q, _ = np.linalg.qr(np.column_stack(basis_cols), mode='reduced')\n                basis = Q[:, :k]\n\n            # ensure S has correct size\n            if S.shape[0] != k:\n                S_new = np.eye(k)\n                m = min(S.shape[0], k)\n                S_new[:m, :m] = S[:m, :m]\n                S = S_new\n                ps = np.zeros(k)\n                B_sub = np.eye(k)\n                D_sub = np.ones(k)\n                invsqrtS = np.eye(k)\n\n            # eigendecompose subspace covariance S for sampling/invsqrt\n            try:\n                vals, vecs = np.linalg.eigh(S)\n                vals = np.maximum(vals, 1e-20)\n                D_sub = np.sqrt(vals)\n                B_sub = vecs\n                invsqrtS = (B_sub * (1.0 / D_sub)) @ B_sub.T\n            except np.linalg.LinAlgError:\n                B_sub = np.eye(k); D_sub = np.ones(k); invsqrtS = np.eye(k)\n\n            # mirrored sampling of coefficients\n            half = (probes + 1) // 2\n            coeffs_list = [self.rng.standard_normal(k) for _ in range(half)]\n            coeffs_list = coeffs_list + [(-c) for c in coeffs_list]\n            coeffs_list = coeffs_list[:probes]\n\n            successes = 0\n            success_y = []\n\n            for coeffs in coeffs_list:\n                if evals >= self.budget:\n                    break\n                # generate direction in full space\n                d = basis @ coeffs\n                dnrm = np.linalg.norm(d)\n                if dnrm == 0:\n                    continue\n                d = d / dnrm\n\n                # apply per-coordinate RMS scaling\n                coord_scale = np.sqrt(coord_var + coord_eps)\n                d = d * coord_scale\n                d_norm = np.linalg.norm(d)\n                if d_norm == 0:\n                    continue\n                d = d / d_norm\n\n                # sample alpha\n                alpha = self.rng.uniform(-step, step)\n                x_try = x_cur + alpha * d\n\n                # DE-style archive diff sometimes\n                if (len(X_archive) >= 2) and (self.rng.random() < self.p_de):\n                    i1, i2 = self.rng.choice(len(X_archive), size=2, replace=False)\n                    de = self.F_de * (X_archive[i1] - X_archive[i2])\n                    x_try = x_try + 0.5 * de\n\n                # occasional Cauchy memory jump\n                if dir_memory and (self.rng.random() < self.mem_jump_prob):\n                    u = dir_memory[self.rng.integers(len(dir_memory))]\n                    jump = np.tan(np.pi * (self.rng.random() - 0.5))\n                    x_try = x_try + (self.cauchy_scale * jump * step) * u\n\n                # occasional independent Cauchy global jump\n                if self.rng.random() < self.p_cauchy:\n                    cj = self.rng.standard_cauchy(size=n)\n                    cj = np.clip(cj, -10, 10)\n                    x_try = x_try + 0.12 * step * cj\n\n                x_try = np.clip(x_try, lb, ub)\n                out = safe_eval(x_try)\n                if out is None:\n                    break\n                f_try, x_try = out\n\n                if f_try < f_cur - 1e-12:\n                    # accept\n                    prev_x = x_cur.copy()\n                    x_cur = x_try.copy()\n                    f_cur = f_try\n                    successes += 1\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n\n                    # compute subspace coordinate y (project coeffs into normalized subspace coords)\n                    # build y by projecting coeffs into S^{-1/2} basis (approx)\n                    # convert coeffs (k) to unit direction z then scale by alpha/step\n                    z = coeffs.copy()\n                    zn = np.linalg.norm(z)\n                    if zn > 0:\n                        z = z / zn\n                        y = (alpha / (step + 1e-20)) * z\n                    else:\n                        y = np.zeros(k)\n                    success_y.append(y.copy())\n\n                    # update dir memory\n                    dir_succ = x_cur - prev_x\n                    dn = np.linalg.norm(dir_succ)\n                    if dn > 0:\n                        dir_unit = dir_succ / dn\n                        dir_memory.insert(0, dir_unit.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n\n                    # update per-coordinate RMS-like var\n                    y_rel = (x_cur - prev_x) / (abs(alpha) + 1e-20)\n                    sec = y_rel ** 2\n                    coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (sec + 1e-12)\n\n                    # short line search along success direction (budget-aware)\n                    if self.budget - evals >= 2:\n                        ls_max = min(10, self.budget - evals)\n                        ls_out = short_line_search(x_cur, f_cur, dir_unit if dn > 0 else d, init_step=abs(alpha) if abs(alpha) > 1e-12 else step, max_evals=ls_max)\n                        if ls_out is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls is not None and f_ls < f_cur - 1e-12:\n                                x_cur = x_ls.copy()\n                                f_cur = f_ls\n                                if f_ls < f_best:\n                                    f_best = f_ls\n                                    x_best = x_ls.copy()\n\n                else:\n                    # small chance to try focused short line search from current point along d\n                    if (self.rng.random() < 0.03) and (self.budget - evals) >= 3:\n                        ls_out = short_line_search(x_cur, f_cur, d, init_step=step, max_evals=4)\n                        if ls_out is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls is not None and f_ls < f_cur - 1e-12:\n                                prev_x = x_cur.copy()\n                                x_cur = x_ls.copy()\n                                f_cur = f_ls\n                                if f_ls < f_best:\n                                    f_best = f_ls\n                                    x_best = x_ls.copy()\n                                # store direction\n                                dn2 = np.linalg.norm(x_cur - prev_x)\n                                if dn2 > 0:\n                                    dir_unit = (x_cur - prev_x) / dn2\n                                    dir_memory.insert(0, dir_unit.copy())\n                                    if len(dir_memory) > self.memory_size:\n                                        dir_memory.pop()\n                                # approximate y for cov update\n                                z = coeffs.copy()\n                                zn = np.linalg.norm(z)\n                                if zn > 0:\n                                    z = z / zn\n                                    y = (alpha / (step + 1e-20)) * z\n                                else:\n                                    y = np.zeros(k)\n                                success_y.append(y.copy())\n                                successes += 1\n\n            # end coeffs loop\n\n            attempted = max(1, len(coeffs_list))\n            success_rate = successes / attempted\n\n            # cumulation update using average successful y if available\n            if success_y:\n                y_mean = np.mean(np.vstack(success_y), axis=0)\n                try:\n                    ps = (1.0 - self.cs) * ps + np.sqrt(self.cs * (2.0 - self.cs)) * (invsqrtS @ y_mean)\n                except Exception:\n                    ps = (1.0 - self.cs) * ps + np.sqrt(self.cs * (2.0 - self.cs)) * y_mean\n            else:\n                ps = (1.0 - self.cs) * ps\n\n            # step-size adaptation: cumulation norm + success-rate multipliers\n            chi_k = np.sqrt(max(1, k)) * (1.0 - 1.0 / (4.0 * max(1, k)) + 1.0 / (21.0 * max(1, k) ** 2))\n            norm_ps = np.linalg.norm(ps)\n            # gentle cumulation-driven change\n            step *= np.exp(0.25 * (norm_ps / (chi_k + 1e-20) - 1.0) / max(1.0, k))\n            # success-rate driven\n            if success_rate > 0.2:\n                step *= self.succ_grow\n            elif success_rate < 0.05:\n                step *= self.succ_shrink\n            step = max(step, min_step)\n\n            # update subspace covariance S with successes\n            if success_y:\n                y_mean = np.mean(np.vstack(success_y), axis=0)\n                rank = np.outer(y_mean, y_mean)\n                S = (1.0 - self.cov_beta) * S + self.cov_beta * (rank + 1e-12 * np.eye(k))\n                # recompute eigendecomp next loop\n\n            # occasional diagonal model attempt\n            if (iter_count % self.model_every == 0) and (self.budget - evals > 0):\n                try_diagonal_model()\n\n            # small archive pruning\n            max_archive = max(2000, 50 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                keep_rest = rest[::max(1, len(rest) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # rare Lévy-style nudge around best when stagnating\n            if (self.rng.random() < 0.02) and (self.budget - evals > 0):\n                jump = np.tan(np.pi * (self.rng.random(n) - 0.5))\n                x_nudge = np.clip(x_best + 0.6 * step * self.cauchy_scale * jump, lb, ub)\n                out = safe_eval(x_nudge)\n                if out is not None:\n                    f_nudge, x_nudge = out\n                    if f_nudge < f_best - 1e-12:\n                        f_best = f_nudge\n                        x_best = x_nudge.copy()\n                        x_cur = x_best.copy()\n                        f_cur = f_best\n\n            # tiny safeguard\n            if step < min_step:\n                step = min_step\n\n            # exit early if very good\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm SMARS scored 0.250 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "a99b834f-9e23-4db3-a765-918a3d6daefd", "operator": null, "metadata": {"aucs": [0.06345008779400918, 0.16783870562223835, 0.2877138491103295, 0.9381443781130197, 0.15363268540603592, 0.19792709763547245, 0.17309850440968555, 0.17813084127031442, 0.225861550651126, 0.11399817398596268]}, "task_prompt": ""}
{"id": "1a0dd32a-f7ef-4f35-bab6-8cdbd5245aae", "fitness": 0.21103528050333872, "name": "DASS", "description": "The algorithm runs a small ensemble of \"actors\" (ensemble size scaled to dimension) each with anisotropic per‑dimension trust radii and step scales (initialized from rng range and init_ratio), keeping a bounded archive seeded by quasi‑random sampling and a safe_eval that clips to bounds and enforces the budget. Each actor builds cheap low‑dim quadratic models via central finite‑difference secants along local principal components (max 3 secant dirs, secant_eps_base=0.12) and proposes the quadratic minimizer clipped by trust, plus PC‑guided line probes. Exploration is mixed in via anisotropic Gaussian sampling, differential‑style recombination with archive/other centers, and occasional heavy Cauchy jumps (cauchy_prob=0.05) while proposals are ranked by surrogate predictions or conservatively by distance. Adaptation and resilience come from multiplicative trust/step updates on success/failure (success_expand≈1.45, failure_shrink≈0.55, step_inc≈1.4, step_dec≈0.6), actor replacement after stagnation (replace_patience=10) and periodic archive‑driven actor refresh and pruning to maintain diversity.", "code": "import numpy as np\n\nclass DASS:\n    \"\"\"\n    Directional Adaptive Subspace Search (DASS)\n\n    Key ideas:\n    - Maintain a small ensemble (\"actors\") each with anisotropic trust radii and per-dimension step scales.\n    - Use cheap stochastic secant approximations (central finite differences along a few directions)\n      to estimate directional gradients and curvatures, assemble low-dimensional quadratic steps in a subspace.\n    - Mix subspace-quadratic minimizers, anisotropic Gaussian sampling, differential-style recombination,\n      PC-guided line probes and occasional Cauchy escapes for exploration.\n    - Adapt trust radii and step scales on success/failure, replace stagnated actors with entropy-driven archive picks.\n    - Strict safe_eval to ensure the budget is never exceeded and domain is clipped to bounds.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 ensemble_size=None, init_ratio=0.12, max_eval_per_iter=40):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n\n        # ensemble sizing\n        if ensemble_size is None:\n            # prefer 4-8 actors depending on dimension\n            self.ensemble_size = max(3, min(8, self.dim // 2 + 2))\n        else:\n            self.ensemble_size = int(ensemble_size)\n\n        # initialization ratio (for initial archive seeding)\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(8, 2 * self.dim)\n        self.max_init = min(800, int(0.4 * self.budget))\n\n        # trust adaptation parameters\n        self.trust_init_frac = 0.5\n        self.trust_min_frac = 1e-6\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.45\n        self.failure_shrink = 0.55\n\n        # per-dimension step scale (like learning rates)\n        self.step_init_frac = 0.2\n        self.step_inc = 1.4\n        self.step_dec = 0.6\n        self.step_min_frac = 1e-8\n\n        # secant curvature estimation\n        self.max_secant_dirs = min(3, self.dim)  # number of subspace directions to estimate per actor\n        self.secant_eps_base = 0.12  # relative to trust or range\n\n        # recombination & escapes\n        self.recomb_prob = 0.2\n        self.cauchy_prob = 0.05\n        self.cauchy_scale_frac = 0.9\n        self.cauchy_clip = 8.0\n\n        # evaluations per outer loop\n        self.max_eval_per_iter = int(max_eval_per_iter)\n\n        # archive and replacement\n        self.max_archive = max(1000, 40 * self.dim)\n        self.replace_patience = 10\n        self.max_centers = max(3, min(12, self.ensemble_size * 2))\n\n    def __call__(self, func):\n        n = self.dim\n\n        # bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archive\n        X = []\n        F = []\n        f_best = np.inf\n        x_best = None\n\n        # initial quasi-random sampling (uniform)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper: unique top centers from archive in order of fitness\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if not any(np.allclose(x, c, atol=1e-12) for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        # initialise actors\n        k0 = min(self.ensemble_size, max(1, len(X)))\n        centers = get_top_centers(k0)\n        if len(centers) < k0:\n            for _ in range(k0 - len(centers)):\n                centers.append(np.array(self.rng.uniform(lb, ub)))\n\n        # per-actor state\n        tr_init = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n        trust_radius = [tr_init.copy() for _ in centers]  # per-dim trust\n        step_scale = [np.maximum(self.step_init_frac * rng_range, 1e-12) for _ in centers]  # per-dim steps\n        stagn = [0 for _ in centers]\n        actor_vals = []\n        # fill actor_vals from archive if available\n        for c in centers:\n            # find nearest archived value or set inf\n            if len(X) > 0:\n                dists = np.linalg.norm(np.asarray(X) - c, axis=1)\n                idx = int(np.argmin(dists))\n                actor_vals.append(float(F[idx]))\n            else:\n                actor_vals.append(np.inf)\n\n        # safe evaluation wrapper: clips to bounds and prevents budget overflow\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # small helper: PCA of top neighbors (use SVD)\n        def local_pcs(center, num_pc=2, m_nei= max(8, 4 * n)):\n            if len(X) < 3:\n                # random orthonormal fallback\n                R = np.linalg.qr(self.rng.randn(n, max(1, num_pc)))[0][:, :max(1, num_pc)]\n                return R\n            X_arr = np.asarray(X)\n            dists = np.linalg.norm(X_arr - center, axis=1)\n            idxs = np.argsort(dists)[:min(len(X_arr), m_nei)]\n            dx = X_arr[idxs] - center\n            if dx.shape[0] <= 1:\n                # fallback random\n                R = np.linalg.qr(self.rng.randn(n, max(1, num_pc)))[0][:, :max(1, num_pc)]\n                return R\n            try:\n                U, S, Vt = np.linalg.svd(dx, full_matrices=False)\n                pcs = Vt.T[:, :min(num_pc, Vt.shape[0])]\n                if pcs.shape[1] == 0:\n                    pcs = np.linalg.qr(self.rng.randn(n, 1))[0]\n                return pcs\n            except Exception:\n                R = np.linalg.qr(self.rng.randn(n, max(1, num_pc)))[0][:, :max(1, num_pc)]\n                return R\n\n        iter_count = 0\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            iter_count += 1\n\n            # occasionally refresh actor list from archive top and diversify\n            if (iter_count % 7) == 0 and len(X) > 0:\n                topk = min(self.max_centers, max(1, len(X) // 15))\n                new_centers = get_top_centers(topk)\n                merged = []\n                for c in (centers + new_centers):\n                    if not any(np.allclose(c, m, atol=1e-12) for m in merged):\n                        merged.append(np.array(c))\n                    if len(merged) >= self.max_centers:\n                        break\n                centers = merged[:self.max_centers]\n                # ensure per-actor arrays match\n                trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n                step_scale = [np.maximum(self.step_init_frac * rng_range, 1e-12) for _ in centers]\n                stagn = [0 for _ in centers]\n                actor_vals = [np.inf for _ in centers]\n\n            improved_any = False\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            for idx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[idx])\n                tr = np.array(trust_radius[idx])\n                step = np.array(step_scale[idx])\n                cur_val = actor_vals[idx] if idx < len(actor_vals) else np.inf\n\n                proposals = []\n                surrogate_scores = []  # lower is better; np.nan when not available\n\n                # 1) Subspace secant quadratic minimizer (consume small number of evals)\n                # choose directions via local PCs (or random) up to max_secant_dirs, orthonormalize\n                num_dirs = min(self.max_secant_dirs, n)\n                pcs = local_pcs(center, num_dirs, m_nei = max(10, 3 * n))\n                # ensure orthonormal columns\n                if pcs.shape[1] > num_dirs:\n                    pcs = pcs[:, :num_dirs]\n                # scale for finite differences\n                eps_dirs = []\n                g_dir = np.zeros(pcs.shape[1])\n                h_dir = np.zeros(pcs.shape[1])\n                needed_eval_for_secant = 2 * pcs.shape[1]\n                # Only perform secant if sufficient work_allow remains (keep at least 1 eval for final proposals)\n                if pcs.shape[1] > 0 and work_allow > needed_eval_for_secant:\n                    for j in range(pcs.shape[1]):\n                        v = pcs[:, j]\n                        # choose eps proportional to trust magnitude in direction\n                        eps = self.secant_eps_base * max(np.dot(np.abs(tr), np.abs(v)), 1e-9)\n                        # compute central differences: f(x+eps v), f(x-eps v)\n                        x_plus = np.clip(center + eps * v, lb, ub)\n                        outp = safe_eval(x_plus)\n                        work_allow -= 1\n                        if outp is None:\n                            break\n                        f_plus, _ = outp\n                        x_minus = np.clip(center - eps * v, lb, ub)\n                        outm = safe_eval(x_minus)\n                        work_allow -= 1\n                        if outm is None:\n                            break\n                        f_minus, _ = outm\n                        # central gradient and curvature along v\n                        g = (f_plus - f_minus) / (2.0 * eps)\n                        h = (f_plus + f_minus - 2.0 * cur_val) / (eps ** 2 + 1e-20)\n                        # regularize curvature toward small positive to avoid huge steps\n                        h_reg = h if np.isfinite(h) else 0.0\n                        # if curvature negative (nonconvex along direction), treat conservatively by flooring to small positive\n                        if h_reg <= 1e-8:\n                            h_reg = 1e-8\n                        g_dir[j] = g\n                        h_dir[j] = h_reg\n                        eps_dirs.append(eps)\n                    # build low-dim quadratic minimizer in subspace: alpha = -g / h\n                    alphas = - g_dir / (h_dir + 1e-20)\n                    # compose delta in original space\n                    delta = np.zeros(n)\n                    for j in range(pcs.shape[1]):\n                        delta += alphas[j] * pcs[:, j]\n                    # clip per-dim by trust radius\n                    delta = np.clip(delta, -tr, tr)\n                    x_sub = np.clip(center + delta, lb, ub)\n                    proposals.append(x_sub)\n                    # surrogate predicted value from quadratic approx along directions (sum g*alpha + 0.5*h*alpha^2)\n                    pred = cur_val + np.dot(g_dir, alphas) + 0.5 * np.dot(h_dir, alphas ** 2)\n                    surrogate_scores.append(float(pred))\n                else:\n                    # if not enough budget or no directions, fallback: small PC-guided probe without secant evals\n                    if pcs.shape[1] > 0:\n                        v0 = pcs[:, 0]\n                        x_pc = np.clip(center - v0 * np.mean(tr) * 0.6, lb, ub)\n                        proposals.append(x_pc); surrogate_scores.append(np.nan)\n\n                # 2) anisotropic Gaussian sampling around center (diagonal covariance using step scales)\n                # draw a small set (2) of anisotropic samples\n                for _ in range(2):\n                    noise = self.rng.randn(n) * (step * (0.9 + 0.4 * self.rng.rand(n)))\n                    x_gauss = np.clip(center + noise, lb, ub)\n                    proposals.append(x_gauss)\n                    surrogate_scores.append(np.nan)\n\n                # 3) differential-style recombination with archive or other actors\n                if (len(X) > 2 or len(centers) > 1) and (self.rng.rand() < self.recomb_prob):\n                    # pick two donors from archive or other centers biased to be distant\n                    cand_pool = X if len(X) >= 3 else centers\n                    # randomly select two distinct\n                    i1 = self.rng.randint(len(cand_pool))\n                    i2 = self.rng.randint(len(cand_pool))\n                    while i2 == i1 and len(cand_pool) > 1:\n                        i2 = self.rng.randint(len(cand_pool))\n                    a = np.array(cand_pool[i1])\n                    b = np.array(cand_pool[i2])\n                    scale = 0.5 + self.rng.rand()  # differential scale in [0.5,1.5]\n                    child = np.clip(center + scale * (a - b) + self.rng.randn(n) * (0.2 * np.mean(tr)), lb, ub)\n                    proposals.append(child); surrogate_scores.append(np.nan)\n\n                # 4) PC-guided line probes (along top 1-2 PCs from archive)\n                pcs2 = local_pcs(center, num_pc=2, m_nei=max(8, 3*n))\n                base = np.mean(tr)\n                for s in (0.3, 0.8, 1.5):\n                    for j in range(min(pcs2.shape[1], 2)):\n                        v = pcs2[:, j]\n                        proposals.append(np.clip(center + v * (s * base), lb, ub)); surrogate_scores.append(np.nan)\n                        proposals.append(np.clip(center - v * (s * base), lb, ub)); surrogate_scores.append(np.nan)\n\n                # 5) occasional Cauchy jump / far exploration\n                if self.rng.rand() < self.cauchy_prob:\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -self.cauchy_clip, self.cauchy_clip)\n                    scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                    x_jump = np.clip(center + jump * scale, lb, ub)\n                    proposals.append(x_jump); surrogate_scores.append(np.nan)\n\n                # deduplicate proposals\n                unique = []\n                uniq_scores = []\n                for i, p in enumerate(proposals):\n                    if not any(np.allclose(p, q, atol=1e-12) for q in unique):\n                        unique.append(p)\n                        uniq_scores.append(surrogate_scores[i] if i < len(surrogate_scores) else np.nan)\n\n                # ranking: use surrogate predictions when available, else prefer closer-to-center conservative moves\n                order_props = list(range(len(unique)))\n                finite_preds = [i for i, s in enumerate(uniq_scores) if not (s is None or np.isnan(s))]\n                nonfinite = [i for i in order_props if i not in finite_preds]\n                finite_preds.sort(key=lambda i: uniq_scores[i])\n                nonfinite.sort(key=lambda i: np.linalg.norm(unique[i] - center))\n                ranked = finite_preds + nonfinite\n\n                improved_local = False\n                # Evaluate proposals in ranked order (but do not exceed work_allow)\n                for ridx in ranked:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    xprop = unique[ridx]\n                    out = safe_eval(xprop)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    fprop, xprop = out\n                    if fprop < actor_vals[idx] - 1e-12 or fprop < f_best - 1e-12:\n                        # success for this actor: move actor to new point\n                        improved_any = True\n                        improved_local = True\n                        centers[idx] = xprop.copy()\n                        actor_vals[idx] = float(fprop)\n                        # expand trust and step scales\n                        trust_radius[idx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                        step_scale[idx] = np.minimum(step * self.step_inc, self.trust_max_frac * rng_range)\n                        stagn[idx] = 0\n                        # if this was global improvement, update best already done in safe_eval\n                        break\n\n                # If no improvement, attempt a short targeted line-search using stored pcs direction (cheap)\n                if (not improved_local) and work_allow > 0:\n                    # use top PC or random direction and a few small multiplier steps (no secant)\n                    if pcs.shape[1] > 0:\n                        dir0 = -pcs[:, 0]\n                    else:\n                        dir0 = - (self.rng.randn(n))\n                        dir0 = dir0 / (np.linalg.norm(dir0) + 1e-12)\n                    base_step = np.mean(step)\n                    for mult in (0.5, 1.1, 2.0):\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        x_try = np.clip(center + dir0 * (mult * base_step), lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < actor_vals[idx] - 1e-12 or ftry < f_best - 1e-12:\n                            improved_any = True\n                            improved_local = True\n                            centers[idx] = xtry.copy()\n                            actor_vals[idx] = float(ftry)\n                            trust_radius[idx] = np.minimum(tr * 1.2, self.trust_max_frac * rng_range)\n                            step_scale[idx] = np.minimum(step * self.step_inc, self.trust_max_frac * rng_range)\n                            stagn[idx] = 0\n                            break\n\n                # If still not improved, shrink trust and reduce step scales\n                if not improved_local:\n                    stagn[idx] += 1\n                    trust_radius[idx] = np.maximum(tr * self.failure_shrink, self.trust_min_frac * rng_range)\n                    step_scale[idx] = np.maximum(step * self.step_dec, self.step_min_frac * rng_range)\n\n                # Replace stagnated actor by entropy-driven pick from archive (farthest from existing centers)\n                if stagn[idx] >= self.replace_patience and len(X) > 0:\n                    X_arr = np.asarray(X)\n                    # compute distance to nearest center\n                    if len(centers) > 0:\n                        d_to_centers_all = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                    else:\n                        d_to_centers_all = np.linalg.norm(X_arr - center, axis=1)\n                    # pick from top-farthest subset to encourage exploration\n                    topk = max(1, min(40, len(X_arr)))\n                    cand_idx = np.argsort(d_to_centers_all)[-topk:]\n                    pick = int(self.rng.choice(cand_idx))\n                    centers[idx] = X_arr[pick].copy()\n                    trust_radius[idx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                    step_scale[idx] = np.maximum(self.step_init_frac * rng_range, 1e-12)\n                    stagn[idx] = 0\n                    actor_vals[idx] = float(F[pick])\n\n            # archive pruning to keep memory bounded\n            if len(X) > self.max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                if len(rest_idx) > 0:\n                    step = max(1, len(rest_idx) // (self.max_archive - 200))\n                    keep_rest = rest_idx[::step]\n                else:\n                    keep_rest = np.array([], dtype=int)\n                keep_idx = np.concatenate([keep_best, keep_rest])[:self.max_archive]\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # periodic actor refresh from top archive to maintain diversity\n            if len(X) > 0 and (iter_count % 6 == 0):\n                best_centers = get_top_centers(min(len(centers), max(1, len(X) // 12)))\n                for i in range(min(len(best_centers), len(centers))):\n                    centers[i] = best_centers[i].copy()\n                    # try to set actor_vals to corresponding F if possible\n                    # find nearest archived index\n                    if len(X) > 0:\n                        dists = np.linalg.norm(np.asarray(X) - centers[i], axis=1)\n                        j = int(np.argmin(dists))\n                        actor_vals[i] = float(F[j])\n                        stagn[i] = 0\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm DASS scored 0.211 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "7f1408a6-3d25-4e48-bd31-d2dacd374f35", "operator": null, "metadata": {"aucs": [0.1071988504660768, 0.18143560393848557, 0.24391339827446612, 0.2726074362919423, 0.22687027830281437, 0.25484173026604096, 0.24335574831485984, 0.2355965733072447, 0.1967692773798536, 0.14776390849160326]}, "task_prompt": ""}
{"id": "3d80c0de-4b9d-4706-b0ec-6d8323209953", "fitness": 0.358819913061431, "name": "DSAP", "description": "The DSAP optimizer runs a small sqrt(dim)-sized ensemble of adaptive centers (ensemble capped ≈8) seeded with a deliberately small initial sampling budget (init_ratio=0.08) and maintains per-center trust radii (trust_init_frac=0.4) and per-dimension learning rates (lr_init_frac=0.10) together with momentum to balance local exploitation and global exploration. For local modeling it builds robust weighted linear fits using an exponential distance kernel and PCA on weighted displacements to estimate directional curvatures, then converts those directional curvatures into per-dimension curvature proxies for cheap diagonal/quadratic steps. Proposal generation is opportunistic and diverse: cheap 3-point quadratic line fits, momentum+mirror moves, distance-biased recombination (recomb_prob=0.20), surrogate-guided probes along principal/gradient directions, and occasional tempered heavy-tailed Levy jumps (levy_prob=0.03), with proposals ranked by surrogate predictions or move size. The algorithm enforces the function-evaluation budget strictly, caps per-iteration evaluations, prunes and refreshes an archive of points (max_archive_base ≈ max(1200,30*dim)), replaces stagnating centers from distant archive points, and uses conservative expansion/shrink rules (expand=1.25, shrink=0.7) plus lr adaption to control step sizes.", "code": "import numpy as np\n\nclass DSAP:\n    \"\"\"\n    Dynamic Subspace Adaptive Patchwork (DSAP)\n\n    Key ideas (differences vs. the provided EMALP):\n    - Smaller, dimension-scaled ensemble (sqrt-based) and lower initial exploratory budget (init_ratio=0.08).\n    - Robust distance-weighting (exponential kernel) for local linear fits and curvature estimation.\n    - Curvature converted to per-dimension proxies using absolute loadings (not squared), and a slightly different floor.\n    - Trust-region / learning-rate update rules use different expansion/shrink factors (expand=1.25, shrink=0.7).\n    - Different probabilities and scales for recombination and heavy-tailed escapes.\n    - Adds a cheap quadratic line minimizer (fit to 3 points) opportunistically before doing full proposals.\n    - Archive management and budget enforcement similar in spirit but with different caps.\n    - Purposefully different numeric defaults/equations while retaining ensemble + local-model + adaptive-trust structure.\n\n    Main tunable parameters (exposed in __init__):\n      - budget: total function evaluations allowed (required)\n      - dim: problem dimension (required)\n      - ensemble_size: optional override for number of centers\n      - init_ratio: fraction of budget used for initial seeding (default 0.08)\n      - curv_rank: rank used for local directional curvature estimation (default min(4, dim))\n      - seed: RNG seed (optional)\n\n    The algorithm strictly respects the budget and clips to func.bounds if present,\n    otherwise uses [-5,5] per-dimension.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, ensemble_size=None, init_ratio=0.08,\n                 curv_rank=None, seed=None, max_eval_per_iter=40):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n\n        # ensemble sizing: proportional to sqrt(dim), clipped\n        if ensemble_size is None:\n            k = max(2, int(np.ceil(np.sqrt(self.dim))))\n            self.ensemble_size = min(max(2, k), 8)\n        else:\n            self.ensemble_size = int(ensemble_size)\n\n        # initial sampling ratio (smaller than EMALP)\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(6, self.dim)\n        self.max_init = min(600, int(0.25 * self.budget))\n\n        # neighbors multiplier for local models\n        self.nei_mult = 8\n\n        # curvature estimation rank\n        if curv_rank is None:\n            self.curv_rank = min(4, self.dim)\n        else:\n            self.curv_rank = min(int(curv_rank), self.dim)\n\n        # trust region params (different expansion/shrink)\n        self.trust_init_frac = 0.4\n        self.trust_min_frac = 1e-5\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.25\n        self.failure_shrink = 0.7\n\n        # learning-rate (per-dim) adaptation\n        self.lr_init_frac = 0.10\n        self.lr_inc = 1.5\n        self.lr_dec = 0.5\n        self.lr_min_frac = 1e-10\n\n        # momentum / mirror behavior\n        self.momentum_decay = 0.6\n        self.mirror_strength = 0.4\n\n        # recombination & heavy-tailed escapes\n        self.recomb_prob = 0.20\n        self.levy_prob = 0.03\n        self.levy_scale_frac = 0.6\n        self.levy_clip = 6.0\n\n        self.max_eval_per_iter = int(max_eval_per_iter)\n\n        # archive and center management\n        self.max_archive_base = max(1200, 30 * self.dim)\n        self.center_replace_patience = 8\n        self.max_centers = max(2, min(12, self.ensemble_size * 2))\n\n    def __call__(self, func):\n        n = self.dim\n\n        # bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # storage / archive\n        X = []\n        F = []\n        f_best = np.inf\n        x_best = None\n\n        # initial quasi-random exploration (uniform)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper: distinct sorted top centers by fitness\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if not any(np.linalg.norm(x - c) < 1e-12 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.max_centers, len(X)))\n        if len(centers) == 0:\n            centers = [np.array(self.rng.uniform(lb, ub))]\n\n        # per-center state\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n        lr = [np.maximum(self.lr_init_frac * rng_range, 1e-12) for _ in centers]\n        velocity = [np.zeros(n) for _ in centers]\n        stagn = [0 for _ in centers]\n\n        # safe evaluation helper (enforces budget and bounds)\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            fval = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(fval))\n            if fval < f_best - 1e-12:\n                f_best = float(fval); x_best = x_c.copy()\n            return float(fval), x_c\n\n        # robust local fit: weighted linear + directional quadratic residuals (different weighting)\n        def fit_local_robust(center, X_nei, F_nei, rank):\n            dx = X_nei - center\n            m = dx.shape[0]\n            if m < 4:\n                return None\n            # robust distance kernel: exponential of (dist / median)^{1.5}\n            dists = np.linalg.norm(dx, axis=1) + 1e-12\n            med = max(np.median(dists), 1e-12)\n            w = np.exp(- (dists / (1.6 * med)) ** 1.5)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n\n            # weighted linear model f ~ a + b^T dx (ridge)\n            A = np.hstack([np.ones((m, 1)), dx])\n            Ar = (W * A)\n            br = (W * F_nei)\n            try:\n                ridge = 1e-7 * np.eye(A.shape[1])\n                params, *_ = np.linalg.lstsq(Ar.T @ Ar + ridge, Ar.T @ br, rcond=None)\n                a = float(params[0]); b = params[1:].flatten()\n            except Exception:\n                return None\n\n            resid = F_nei - (a + dx.dot(b))\n\n            # PCA on weighted dx\n            try:\n                Dx = W * dx\n                U, S, Vt = np.linalg.svd(Dx, full_matrices=False)\n                pcs = Vt.T[:, :max(1, min(rank, Vt.shape[0]))]\n            except Exception:\n                pcs = np.eye(n, 1)\n\n            # estimate curvature along pcs using resid ~ 0.5 * h * p^2 (no linear term)\n            h_dir = np.zeros(pcs.shape[1])\n            for j in range(pcs.shape[1]):\n                v = pcs[:, j]\n                p = dx.dot(v)\n                P = (0.5 * (p ** 2))[:, None]  # m x 1\n                try:\n                    Pr = (W * P)\n                    # mild ridge\n                    c, *_ = np.linalg.lstsq(Pr.T @ Pr + 1e-9 * np.eye(1), Pr.T @ (W.flatten() * resid), rcond=None)\n                    h_dir[j] = float(c[0]) if c.size > 0 else 0.0\n                except Exception:\n                    h_dir[j] = 0.0\n\n            # convert directional curvatures to per-dim curvature proxy using absolute loadings\n            h_per_dim = np.zeros(n)\n            for j in range(pcs.shape[1]):\n                v = pcs[:, j]\n                h_per_dim += np.abs(h_dir[j]) * np.abs(v)\n            # floor and force positive-ish curvature\n            h_per_dim = np.maximum(h_per_dim, 5e-9)\n            return a, b, h_per_dim, pcs, h_dir\n\n        iter_count = 0\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            iter_count += 1\n\n            # periodic refresh of centers from archive bests\n            if (iter_count % 10) == 0 and len(X) > 0:\n                topk = min(self.max_centers, max(1, len(X) // 16))\n                new_centers = get_top_centers(topk)\n                merged = []\n                for c in (centers + new_centers):\n                    if not any(np.allclose(c, m, atol=1e-12) for m in merged):\n                        merged.append(np.array(c))\n                    if len(merged) >= self.max_centers:\n                        break\n                centers = merged[:self.max_centers]\n                if len(trust_radius) != len(centers):\n                    trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n                    lr = [np.maximum(self.lr_init_frac * rng_range, 1e-12) for _ in centers]\n                    velocity = [np.zeros(n) for _ in centers]\n                    stagn = [0 for _ in centers]\n\n            improved_any = False\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            for idx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[idx])\n                tr = np.array(trust_radius[idx])\n                lr_vec = np.array(lr[idx])\n                vel = np.array(velocity[idx])\n\n                # collect neighbors\n                X_arr = np.asarray(X) if len(X) > 0 else np.empty((0, n))\n                F_arr = np.asarray(F) if len(F) > 0 else np.empty((0,))\n                enough_data = len(X) >= max(4 * n, self.nei_mult * n)\n                local_model = None\n                if enough_data:\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), max(4 * n, self.nei_mult * n))\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n                    local_model = fit_local_robust(center, X_nei, F_nei, self.curv_rank)\n\n                proposals = []\n                surrogate_scores = []\n\n                # quick quadratic-line minimizer before heavy proposals:\n                # Try 3 points along random direction within trust to see if a cheap minimizer is found\n                dir_try = self.rng.randn(n)\n                dir_try /= (np.linalg.norm(dir_try) + 1e-12)\n                scales = [0.0, 0.5, 1.0]\n                pts = []\n                vals = []\n                for s in scales:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    xq = np.clip(center + dir_try * (s * np.mean(tr)), lb, ub)\n                    out = safe_eval(xq)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    fq, xq = out\n                    pts.append(s); vals.append(fq)\n                    if fq < f_best - 1e-12:\n                        # immediate success\n                        improved_any = True\n                        centers[idx] = xq.copy()\n                        velocity[idx] = self.momentum_decay * vel + 0.7 * (xq - center)\n                        trust_radius[idx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                        lr[idx] = np.minimum(np.array(lr_vec) * self.lr_inc, self.trust_max_frac * rng_range)\n                        stagn[idx] = 0\n                        # skip other proposals for this center\n                        pts = None\n                        break\n                if pts is None:\n                    continue\n                if len(pts) == 3:\n                    # fit quadratic f(s) ~ A s^2 + B s + C, find minimizer s* = -B/(2A) if A>0\n                    try:\n                        coefs = np.polyfit(pts, vals, 2)  # [A, B, C]\n                        A, B = coefs[0], coefs[1]\n                        if A > 1e-12:\n                            sstar = -B / (2.0 * A)\n                            if  -1.5 <= sstar <= 2.5:\n                                xstar = np.clip(center + dir_try * (sstar * np.mean(tr)), lb, ub)\n                                proposals.append(xstar)\n                                surrogate_scores.append(np.nan)\n                    except Exception:\n                        pass\n\n                # 1) diagonal conservative quadratic minimizer from local model\n                if local_model is not None:\n                    a_loc, b_loc, h_loc, pcs, h_dir = local_model\n                    # delta = -b / (h + lam) with small ridge lam depending on tr and lr\n                    lam = 1e-9 + 0.5 * np.mean(lr_vec) / (np.mean(tr) + 1e-12)\n                    delta = - b_loc / (h_loc + lam)\n                    # clip more conservatively: 0.8 * tr\n                    delta = np.clip(delta, -0.8 * tr, 0.8 * tr)\n                    x_q = np.clip(center + delta, lb, ub)\n                    proposals.append(x_q)\n                    # surrogate predicted value (quadratic)\n                    pred = a_loc + delta.dot(b_loc) + 0.5 * np.sum(h_loc * (delta ** 2))\n                    surrogate_scores.append(float(pred))\n                else:\n                    surrogate_scores.append(np.inf)\n\n                # 2) momentum + mirror step (different blend)\n                if x_best is not None:\n                    mirror = center + self.mirror_strength * (center - x_best)\n                else:\n                    mirror = center.copy()\n                noise = self.rng.randn(n) * (0.35 * tr)\n                x_m = np.clip(0.5 * mirror + 0.5 * center + vel * 0.8 + noise, lb, ub)\n                proposals.append(x_m); surrogate_scores.append(np.nan)\n\n                # 3) recombination (biased by distance) with stronger chance\n                if (len(centers) > 1) and (self.rng.rand() < self.recomb_prob):\n                    dists_cent = np.array([np.linalg.norm(center - np.array(c)) for c in centers])\n                    prob = dists_cent + 1e-9\n                    prob[idx] = 0.0\n                    if prob.sum() > 0:\n                        prob = prob / prob.sum()\n                        j = int(self.rng.choice(len(centers), p=prob))\n                    else:\n                        j = self.rng.randint(len(centers))\n                    partner = np.array(centers[j])\n                    alpha = 0.2 + 0.8 * self.rng.rand()\n                    child = np.clip(alpha * center + (1 - alpha) * partner + self.rng.randn(n) * (0.15 * tr), lb, ub)\n                    proposals.append(child); surrogate_scores.append(np.nan)\n\n                # 4) surrogate-guided line probes along principal local pcs or gradient direction\n                if local_model is not None:\n                    _, b_loc, _, pcs, _ = local_model\n                    if np.linalg.norm(b_loc) > 1e-12:\n                        g_dir = -b_loc / (np.linalg.norm(b_loc) + 1e-12)\n                    else:\n                        g_dir = pcs[:, 0] if pcs.shape[1] > 0 else self.rng.randn(n)\n                        g_dir = g_dir / (np.linalg.norm(g_dir) + 1e-12)\n                    base = np.mean(tr)\n                    for s in (0.3, 0.8, 1.6):\n                        x1 = np.clip(center + g_dir * (s * base), lb, ub)\n                        x2 = np.clip(center - g_dir * (s * base), lb, ub)\n                        proposals.append(x1); surrogate_scores.append(np.nan)\n                        proposals.append(x2); surrogate_scores.append(np.nan)\n                else:\n                    for _ in range(3):\n                        z = np.clip(center + self.rng.randn(n) * (0.5 * tr), lb, ub)\n                        proposals.append(z); surrogate_scores.append(np.nan)\n\n                # 5) tempered heavy-tailed jump\n                if self.rng.rand() < self.levy_prob:\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -self.levy_clip, self.levy_clip)\n                    scale = self.levy_scale_frac * np.maximum(rng_range, 1e-9)\n                    x_jump = np.clip(center + jump * scale, lb, ub)\n                    proposals.append(x_jump); surrogate_scores.append(np.nan)\n\n                # deduplicate proposals\n                unique = []\n                uniq_scores = []\n                for i, p in enumerate(proposals):\n                    if not any(np.allclose(p, q, atol=1e-12) for q in unique):\n                        unique.append(p)\n                        uniq_scores.append(surrogate_scores[i] if i < len(surrogate_scores) else np.nan)\n\n                # rank proposals: prefer surrogate predicted low, otherwise small moves weighted by trust and lr\n                order_props = list(range(len(unique)))\n                finite_preds = [i for i, s in enumerate(uniq_scores) if not (s is None or np.isnan(s))]\n                nonfinite = [i for i in order_props if i not in finite_preds]\n                finite_preds.sort(key=lambda i: uniq_scores[i])\n                nonfinite.sort(key=lambda i: np.linalg.norm((unique[i] - center) / (tr + 1e-12)))\n                ranked = finite_preds + nonfinite\n\n                improved_local = False\n                for ridx in ranked:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    xprop = unique[ridx]\n                    out = safe_eval(xprop)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    fprop, xprop = out\n                    if fprop < f_best - 1e-12:\n                        improved_any = True\n                        improved_local = True\n                        # update velocity (slightly different blend)\n                        new_vel = self.momentum_decay * vel + 0.85 * (xprop - center)\n                        velocity[idx] = new_vel\n                        centers[idx] = xprop.copy()\n                        trust_radius[idx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                        lr[idx] = np.minimum(np.array(lr_vec) * self.lr_inc, self.trust_max_frac * rng_range)\n                        stagn[idx] = 0\n                        break\n\n                # opportunistic short quadratic 1D search along best found direction if no improvement\n                if (not improved_local) and work_allow > 0 and local_model is not None:\n                    _, b_loc, _, pcs, _ = local_model\n                    if np.linalg.norm(b_loc) > 1e-12:\n                        dir0 = -b_loc / (np.linalg.norm(b_loc) + 1e-12)\n                    else:\n                        dir0 = pcs[:, 0] if pcs.shape[1] > 0 else self.rng.randn(n)\n                        dir0 = dir0 / (np.linalg.norm(dir0) + 1e-12)\n                    avg_lr = np.mean(np.maximum(lr_vec, 1e-12))\n                    steps = [0.6, 1.2, 2.0]\n                    vals = []\n                    pts = []\n                    for s in steps:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        x_try = np.clip(center + dir0 * (s * avg_lr), lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        pts.append(s); vals.append(ftry)\n                        if ftry < f_best - 1e-12:\n                            improved_any = True\n                            improved_local = True\n                            centers[idx] = xtry.copy()\n                            velocity[idx] = self.momentum_decay * vel + 0.75 * (xtry - center)\n                            trust_radius[idx] = np.minimum(tr * 1.20, self.trust_max_frac * rng_range)\n                            lr[idx] = np.minimum(np.array(lr_vec) * self.lr_inc, self.trust_max_frac * rng_range)\n                            stagn[idx] = 0\n                            break\n                    if (not improved_local) and len(pts) >= 3:\n                        # quadratic fit and extra probe at minimizer candidate\n                        try:\n                            coefs = np.polyfit(pts, vals, 2)\n                            A, B = coefs[0], coefs[1]\n                            if A > 1e-12:\n                                sstar = -B / (2.0 * A)\n                                if -1.0 <= sstar <= 3.0 and work_allow > 0:\n                                    xstar = np.clip(center + dir0 * (sstar * avg_lr), lb, ub)\n                                    out = safe_eval(xstar)\n                                    work_allow -= 1\n                                    if out is not None:\n                                        fsta, xsta = out\n                                        if fsta < f_best - 1e-12:\n                                            improved_any = True\n                                            improved_local = True\n                                            centers[idx] = xsta.copy()\n                                            velocity[idx] = self.momentum_decay * vel + 0.7 * (xsta - center)\n                                            trust_radius[idx] = np.minimum(tr * 1.30, self.trust_max_frac * rng_range)\n                                            lr[idx] = np.minimum(np.array(lr_vec) * self.lr_inc, self.trust_max_frac * rng_range)\n                                            stagn[idx] = 0\n                        except Exception:\n                            pass\n\n                # if still no improvement: shrink trust & lr, damp velocity\n                if not improved_local:\n                    stagn[idx] += 1\n                    trust_radius[idx] = np.maximum(tr * self.failure_shrink, self.trust_min_frac * rng_range)\n                    lr[idx] = np.maximum(np.array(lr_vec) * self.lr_dec, self.lr_min_frac * rng_range)\n                    velocity[idx] = self.momentum_decay * vel\n\n                # replace stagnated center with a far archive point to encourage exploration\n                if stagn[idx] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # distance to nearest center\n                        d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        topk = max(1, min(60, len(X_arr)))\n                        cand_idx = np.argsort(d_to_centers)[-topk:]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[idx] = X_arr[pick].copy()\n                        trust_radius[idx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                        lr[idx] = np.maximum(self.lr_init_frac * rng_range, 1e-12)\n                        velocity[idx] = np.zeros(n)\n                        stagn[idx] = 0\n\n            # prune archive to bound memory\n            max_archive = max(self.max_archive_base, 25 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:300]\n                rest_idx = idx_sorted[300:]\n                if len(rest_idx) > 0:\n                    step = max(1, len(rest_idx) // (max_archive - 300))\n                    keep_rest = rest_idx[::step]\n                else:\n                    keep_rest = np.array([], dtype=int)\n                keep_idx = np.concatenate([keep_best, keep_rest])[:max_archive]\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasional mild center refresh\n            if len(X) > 0 and (iter_count % 7 == 0):\n                best_centers = get_top_centers(min(len(centers), max(1, len(X) // 10)))\n                for i in range(min(len(best_centers), len(centers))):\n                    centers[i] = best_centers[i].copy()\n\n            # terminate early if no progress for many iterations (soft)\n            # (we still respect budget; this is just a small loop-level measure)\n            # Not forcibly breaking to conserve interface simplicity\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm DSAP scored 0.359 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "7f1408a6-3d25-4e48-bd31-d2dacd374f35", "operator": null, "metadata": {"aucs": [0.17857790819515762, 0.31283533389377616, 0.394579005068448, 0.7776843960912935, 0.33912023536405245, 0.4856138549495499, 0.2527774139323137, 0.3539416468377592, 0.32359483014882484, 0.1694745061331352]}, "task_prompt": ""}
{"id": "b697cb05-a965-4c74-96bf-da6b7c89d5ea", "fitness": "-inf", "name": "DECALR", "description": "1) The algorithm maintains a small ensemble of \"centers\" each with a discrete ladder-level that multiplicatively scales a base step (base_sigma_frac=0.12, ladder_factor=2.0, ladder_levels≈5) so centers can switch between exploitation and exploration.  \n2) Sampling is anisotropic: each center uses a diagonal covariance (diag_scale, adapted with cov_beta=0.18) plus a tiny low-rank subspace (curv_rank ≤ 2) built from recent successful steps (weights decayed by dir_weight_decay≈0.85), and proposals mix Gaussian, mirror, coordinate-focused probes and occasional tempered Lévy jumps (levy_prob=0.05).  \n3) Local, archive-driven modeling fits 1‑D quadratics by projecting nearby archive points onto candidate directions to predict minimizers without extra evaluations, and proposals are ranked by predicted improvement before being evaluated.  \n4) Adaptation rules move ladders down on success and up on failure (success_lower/failure_rise), update diag/low-rank structure on improvements, use stagnation counters and center-replacement after patience (≈12), and rely on safe_eval, archive pruning (max_archive), and per-iteration evaluation caps (max_eval_per_iter) to robustly respect the budget.", "code": "import numpy as np\n\nclass DECALR:\n    \"\"\"\n    DECAL-R: Directional Ensemble with Covariance-Adaptive Ladder Resets\n\n    Key ideas / differences from EMALP:\n    - Each center maintains a ladder-level that controls a multiplicative step-size (sigma).\n    - Per-center covariance model is diagonal scaling + a small set of low-rank direction vectors (rank <= 2).\n      Sampling mixes both parts to produce anisotropic proposals.\n    - Directional quadratic fitting is done by projecting archive neighbors onto candidate directions\n      to estimate curvature and a directional minimizer without extra function evaluations.\n    - A probabilistic \"ladder\" increases sigma on repeated failures and lowers sigma after successes.\n    - Centers record recent successful steps to form low-rank directions; these are used to bias search along promising directions.\n    - Occasional mirror/affine reflections and tempered Lévy restarts are used to escape traps.\n    - Archive pruning and strict safe_eval ensure the budget is never exceeded.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 ensemble_size=None, init_ratio=0.12,\n                 max_eval_per_iter=40, curv_rank=2, ladder_levels=5):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n\n        # ensemble sizing\n        if ensemble_size is None:\n            self.ensemble_size = max(3, min(7, self.dim // 2 + 1))\n        else:\n            self.ensemble_size = int(ensemble_size)\n\n        # initialization\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(8, 2 * self.dim)\n        self.max_init = min(500, int(0.35 * self.budget))\n\n        # sampling / covariance adaptation\n        self.curv_rank = min(curv_rank, self.dim)  # low-rank component size per center\n        self.base_sigma_frac = 0.12  # base fraction of box range\n        self.ladder_levels = int(max(1, ladder_levels))  # number of discrete step levels\n        self.ladder_factor = 2.0  # multiplicative change per ladder level (sigma * ladder_factor**level)\n\n        # adaptation speeds\n        self.cov_beta = 0.18  # how fast diag covariance adapts to successful steps\n        self.dir_weight_decay = 0.85  # decay weight for low-rank direction accumulation\n\n        # ladder dynamics\n        self.success_lower = 1  # on success, move ladder down by 1 (toward exploitation)\n        self.failure_rise = 1   # on failure, move ladder up by 1 (toward exploration)\n        self.max_stagn = 14\n\n        # other mechanisms\n        self.mirror_alpha = 0.5\n        self.levy_prob = 0.05\n        self.levy_scale = 0.9\n        self.levy_clip = 10.0\n\n        # per-iteration budget\n        self.max_eval_per_iter = int(max_eval_per_iter)\n\n        # archive and memory controls\n        self.max_archive = max(800, 60 * self.dim)\n        self.center_replace_patience = 12\n\n    def __call__(self, func):\n        n = self.dim\n\n        # bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archive\n        X = []\n        F = []\n        f_best = np.inf\n        x_best = None\n\n        # initial quasi-random exploration\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper: distinct sorted top centers\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if not any(np.linalg.norm(x - c) <= 1e-12 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        # initialize centers from top archive or random\n        centers = get_top_centers(min(self.ensemble_size, len(X)))\n        while len(centers) < self.ensemble_size:\n            centers.append(np.array(self.rng.uniform(lb, ub)))\n\n        # per-center state\n        # ladder level (0 = most exploitative (small sigma), higher = more exploratory)\n        ladder = [max(0, self.ladder_levels // 2) for _ in centers]\n        # diagonal covariance scaling (per-dim positive)\n        diag_scale = [np.ones(n) for _ in centers]\n        # low-rank directions: list of (u_vectors list, weights list), each u is shape (n,)\n        lowrank_dirs = [[[], []] for _ in centers]  # [ [u_list], [w_list] ]\n        # stagnation counter\n        stagn = [0 for _ in centers]\n\n        # safe evaluation wrapper\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # get nearest archive f for a given center (approx center fitness)\n        def center_f_est(center):\n            if len(X) == 0:\n                return np.inf\n            X_arr = np.asarray(X)\n            d = np.linalg.norm(X_arr - center, axis=1)\n            idx = int(np.argmin(d))\n            return F[idx]\n\n        # fit directional quadratic using archive projected onto direction (no extra evals)\n        def fit_directional_quadratic(center, direction, X_nei, F_nei):\n            # project neighbors onto direction\n            p = (X_nei - center).dot(direction)\n            # require diversity in projections\n            if X_nei.shape[0] < 6 or np.std(p) < 1e-9:\n                return None  # not enough info\n            # fit weighted quadratic: f ~ a + b p + 0.5 c p^2\n            w = 1.0 / (1.0 + np.abs(p))  # a light weight favoring close projections\n            W = np.sqrt(w)[:, None]\n            P = np.vstack([np.ones_like(p), p, 0.5 * (p ** 2)]).T  # m x 3\n            try:\n                Pr = (W * P)\n                br = (W.flatten() * F_nei)\n                params, *_ = np.linalg.lstsq(Pr.T @ Pr + 1e-10 * np.eye(3), Pr.T @ br, rcond=None)\n                a = float(params[0]); b = float(params[1]); c = float(params[2])  # c is curvature\n                return a, b, c\n            except Exception:\n                return None\n\n        # sample anisotropic multivariate gaussian with diag + low-rank components\n        def sample_from_center(idx, center):\n            # ladder scalar\n            level = ladder[idx]\n            sigma_scalar = self.base_sigma_frac * (self.ladder_factor ** level)\n            # per-dim sigma vector\n            sigma_vec = sigma_scalar * rng_range * np.sqrt(np.maximum(diag_scale[idx], 1e-12))\n            # diagonal component\n            z_diag = sigma_vec * self.rng.randn(n)\n            # low-rank component\n            u_list, w_list = lowrank_dirs[idx]\n            z_lr = np.zeros(n)\n            if len(u_list) > 0:\n                for u, w in zip(u_list, w_list):\n                    z_lr += np.sqrt(max(0.0, w)) * u * self.rng.randn()\n            return np.clip(center + z_diag + z_lr, lb, ub)\n\n        # main loop\n        iter_count = 0\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            iter_count += 1\n\n            # periodic center refresh from spread archive to maintain coverage\n            if (iter_count % 7) == 0 and len(X) > 0:\n                topk = min(len(centers), max(1, len(X) // 12))\n                new_centers = get_top_centers(topk)\n                merged = []\n                for c in (centers + new_centers):\n                    if not any(np.allclose(c, m, atol=1e-12) for m in merged):\n                        merged.append(np.array(c))\n                    if len(merged) >= len(centers):\n                        break\n                # if too few, fill randomly from archive spread\n                while len(merged) < len(centers):\n                    merged.append(X[self.rng.randint(len(X))].copy())\n                centers = merged[:len(centers)]\n                # reset some state modestly to avoid stale covariance\n                for i in range(len(centers)):\n                    diag_scale[i] = np.maximum(diag_scale[i], 1e-6)\n                    # decay dir weights slightly\n                    for j in range(len(lowrank_dirs[i][1])):\n                        lowrank_dirs[i][1][j] *= 0.9\n\n            improved_any = False\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            # precompute archive arrays for local fits\n            X_arr = np.asarray(X) if len(X) > 0 else np.empty((0, n))\n            F_arr = np.asarray(F) if len(F) > 0 else np.empty((0,))\n\n            for idx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n\n                center = np.array(centers[idx])\n                # guess center fitness from archive (may be approximate)\n                c_f = center_f_est(center)\n\n                # find neighbors for local fits\n                if len(X) >= max(10, 6 * n):\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), max(30, 8 * n))\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]; F_nei = F_arr[idx_nei]\n                else:\n                    X_nei = X_arr; F_nei = F_arr\n\n                # proposals container\n                proposals = []\n                predicted = []  # predicted improvement (larger positive = more promising)\n\n                # 1) covariance-adaptive sample (primary mode)\n                x_cov = sample_from_center(idx, center)\n                proposals.append(x_cov); predicted.append(np.nan)\n\n                # 2) directional minimizer along main low-rank or gradient proxy\n                # build candidate directions: use strongest low-rank dir if available, else top PC of neighbors\n                dir_candidates = []\n                u_list, w_list = lowrank_dirs[idx]\n                if len(u_list) > 0:\n                    # choose highest weight direction\n                    max_j = int(np.argmax(w_list))\n                    dir_candidates.append(u_list[max_j])\n                # also use principal component of neighbors if available\n                if X_nei.shape[0] >= min(3, n):\n                    try:\n                        Dx = X_nei - center\n                        U, S, Vt = np.linalg.svd(Dx, full_matrices=False)\n                        p0 = Vt.T[:, 0]\n                        dir_candidates.append(p0)\n                    except Exception:\n                        pass\n\n                # ensure uniqueness and normalized\n                dirs = []\n                for d in dir_candidates:\n                    if np.linalg.norm(d) < 1e-12:\n                        continue\n                    d = d / (np.linalg.norm(d) + 1e-12)\n                    if not any(np.allclose(d, dd, atol=1e-9) or np.allclose(d, -dd, atol=1e-9) for dd in dirs):\n                        dirs.append(d)\n\n                # for each direction attempt to predict minimizer via archive projection\n                for d in dirs:\n                    fit = fit_directional_quadratic(center, d, X_nei, F_nei)\n                    if fit is not None:\n                        a, b, ccurv = fit\n                        # predicted minimizer along t is t* = -b / ccurv (but if curvature <= 0 then treat cautiously)\n                        if abs(ccurv) > 1e-12:\n                            tstar = -b / (ccurv + 1e-16)\n                            # clip tstar to a few sigmas determined by current ladder\n                            sigma_scalar = self.base_sigma_frac * (self.ladder_factor ** ladder[idx])\n                            tclip = 4.0 * sigma_scalar * np.mean(rng_range)\n                            tstar = np.clip(tstar, -tclip, tclip)\n                            x_dir = np.clip(center + d * tstar, lb, ub)\n                            proposals.append(x_dir)\n                            # predicted improvement: a + b t + 0.5 c t^2 relative to current center approx a\n                            pred_delta = -(b * tstar + 0.5 * ccurv * (tstar ** 2))\n                            predicted.append(float(pred_delta))\n                        else:\n                            # if flat or negative curvature, still try small step toward -b\n                            small_t = -0.5 * b\n                            x_dir = np.clip(center + d * small_t, lb, ub)\n                            proposals.append(x_dir)\n                            predicted.append(np.nan)\n                    else:\n                        # fallback modest step along d\n                        small_step = 0.5 * self.base_sigma_frac * rng_range\n                        x_dir = np.clip(center + d * small_step, lb, ub)\n                        proposals.append(x_dir)\n                        predicted.append(np.nan)\n\n                # 3) mirror step across archive mean to generate a diverse candidate\n                if len(X) > 3:\n                    mean_archive = np.mean(X_arr, axis=0)\n                    mirror = center + self.mirror_alpha * (center - mean_archive)\n                    # add small anisotropic jitter\n                    jitter = self.rng.randn(n) * (0.4 * self.base_sigma_frac * rng_range)\n                    x_mirror = np.clip(mirror + jitter, lb, ub)\n                    proposals.append(x_mirror); predicted.append(np.nan)\n\n                # 4) coordinate-focus probe: focus on dims with large diag_scale\n                focus_idx = np.argsort(-np.abs(diag_scale[idx]))[:max(1, n // 6)]\n                z = np.zeros(n)\n                z[focus_idx] = self.rng.randn(len(focus_idx)) * (self.base_sigma_frac * rng_range[focus_idx] * 0.8 * (1.0 + ladder[idx] * 0.3))\n                z_others = self.rng.randn(n) * (0.15 * self.base_sigma_frac * rng_range)\n                x_focus = np.clip(center + z + z_others, lb, ub)\n                proposals.append(x_focus); predicted.append(np.nan)\n\n                # 5) occasional tempered Lévy jump\n                if self.rng.rand() < self.levy_prob:\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -self.levy_clip, self.levy_clip)\n                    x_jump = np.clip(center + jump * (self.levy_scale * rng_range), lb, ub)\n                    proposals.append(x_jump); predicted.append(np.nan)\n\n                # Deduplicate proposals preserving order\n                unique = []\n                uniq_pred = []\n                for p, q in zip(proposals, predicted):\n                    if not any(np.allclose(p, u, atol=1e-12) for u in unique):\n                        unique.append(p)\n                        uniq_pred.append(q)\n\n                # Rank proposals: prefer predicted positive improvements, then closer moves\n                idxs = list(range(len(unique)))\n                pred_idxs = [i for i in idxs if not (uniq_pred[i] is None or (isinstance(uniq_pred[i], float) and np.isnan(uniq_pred[i])))]\n                other_idxs = [i for i in idxs if i not in pred_idxs]\n                pred_idxs.sort(key=lambda i: -uniq_pred[i])  # larger predicted improvement first\n                other_idxs.sort(key=lambda i: np.linalg.norm(unique[i] - center))\n                ranked = pred_idxs + other_idxs\n\n                # evaluate proposals until a success (better than center estimate) or work_allow exhausted\n                improved_local = False\n                for ridx in ranked:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    xprop = unique[ridx]\n                    out = safe_eval(xprop)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    fprop, xprop = out\n                    # treat improvement relative to global best to encourage global progress\n                    if fprop < f_best - 1e-12:\n                        # global improvement success\n                        improved_any = True\n                        improved_local = True\n                        # update center toward found good point\n                        step = xprop - center\n                        # update diag covariance to increase weight on successful step dims\n                        diag_scale[idx] = (1.0 - self.cov_beta) * np.array(diag_scale[idx]) + self.cov_beta * (step ** 2) / (np.mean(step ** 2) + 1e-12)\n                        # insert low-rank direction (normalized) and weight proportional to improvement\n                        snorm = np.linalg.norm(step)\n                        if snorm > 1e-12:\n                            u_new = step / snorm\n                            w_new = max(1e-6, (c_f - fprop) if (c_f - fprop) > 0 else 1e-6)\n                            u_list, w_list = lowrank_dirs[idx]\n                            # decay existing weights\n                            lowrank_dirs[idx][1] = [w * self.dir_weight_decay for w in w_list]\n                            # append or replace smallest weight vector\n                            if len(u_list) < self.curv_rank:\n                                u_list.append(u_new); lowrank_dirs[idx][1].append(w_new)\n                            else:\n                                min_j = int(np.argmin(lowrank_dirs[idx][1]))\n                                u_list[min_j] = u_new; lowrank_dirs[idx][1][min_j] = w_new\n                        # move center exactly to xprop\n                        centers[idx] = xprop.copy()\n                        # move ladder toward exploitation (down) on success\n                        ladder[idx] = max(0, ladder[idx] - self.success_lower)\n                        stagn[idx] = 0\n                        break\n                    # else partial local improvement relative to center estimate\n                    center_estimate = c_f\n                    if fprop < center_estimate - 1e-12:\n                        # treat as local success even if not global best\n                        improved_any = True\n                        improved_local = True\n                        step = xprop - center\n                        diag_scale[idx] = (1.0 - self.cov_beta) * np.array(diag_scale[idx]) + self.cov_beta * (step ** 2) / (np.mean(step ** 2) + 1e-12)\n                        snorm = np.linalg.norm(step)\n                        if snorm > 1e-12:\n                            u_new = step / snorm\n                            w_new = max(1e-6, (center_estimate - fprop))\n                            u_list, w_list = lowrank_dirs[idx]\n                            lowrank_dirs[idx][1] = [w * self.dir_weight_decay for w in w_list]\n                            if len(u_list) < self.curv_rank:\n                                u_list.append(u_new); lowrank_dirs[idx][1].append(w_new)\n                            else:\n                                min_j = int(np.argmin(lowrank_dirs[idx][1]))\n                                u_list[min_j] = u_new; lowrank_dirs[idx][1][min_j] = w_new\n                        centers[idx] = xprop.copy()\n                        ladder[idx] = max(0, ladder[idx] - self.success_lower)\n                        stagn[idx] = 0\n                        break\n\n                # If no evaluated proposal improved, perform a short directional trust search if budget permits\n                if (not improved_local) and (work_allow > 0) and (len(dirs) > 0):\n                    # use the best direction (first one) and try small geometric steps\n                    d = dirs[0]\n                    sigma_scalar = self.base_sigma_frac * (self.ladder_factor ** ladder[idx])\n                    step_scales = [0.6, 1.0, 1.8]\n                    for s in step_scales:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        step = s * sigma_scalar * np.mean(rng_range)\n                        x_try = np.clip(center + d * step, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < c_f - 1e-12:\n                            improved_any = True\n                            improved_local = True\n                            # moderate covariance update\n                            st = xtry - center\n                            diag_scale[idx] = (1.0 - self.cov_beta) * np.array(diag_scale[idx]) + self.cov_beta * (st ** 2) / (np.mean(st ** 2) + 1e-12)\n                            snorm = np.linalg.norm(st)\n                            if snorm > 1e-12:\n                                u_new = st / snorm\n                                w_new = max(1e-6, (c_f - ftry))\n                                u_list, w_list = lowrank_dirs[idx]\n                                lowrank_dirs[idx][1] = [w * self.dir_weight_decay for w in w_list]\n                                if len(u_list) < self.curv_rank:\n                                    u_list.append(u_new); lowrank_dirs[idx][1].append(w_new)\n                                else:\n                                    min_j = int(np.argmin(lowrank_dirs[idx][1]))\n                                    u_list[min_j] = u_new; lowrank_dirs[idx][1][min_j] = w_new\n                            centers[idx] = xtry.copy()\n                            ladder[idx] = max(0, ladder[idx] - self.success_lower)\n                            stagn[idx] = 0\n                            break\n\n                # if still no improvement -> increase ladder level (explore more), damp diag and decay directions\n                if not improved_local:\n                    stagn[idx] += 1\n                    ladder[idx] = min(self.ladder_levels - 1, ladder[idx] + self.failure_rise)\n                    # slightly increase diag_scale to allow larger anisotropic jumps\n                    diag_scale[idx] = np.minimum(np.array(diag_scale[idx]) * 1.08, 1e6)\n                    # decay low-rank direction weights\n                    lowrank_dirs[idx][1] = [w * 0.95 for w in lowrank_dirs[idx][1]]\n                    # small random nudging to center to encourage exploration (no eval)\n                    centers[idx] = np.clip(centers[idx] + self.rng.randn(n) * (0.02 * rng_range), lb, ub)\n                    # replace stagnated center from archive far point\n                    if stagn[idx] >= self.center_replace_patience and len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # distance to nearest center\n                        d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        topk = max(1, min(40, len(X_arr)))\n                        cand_idx = np.argsort(d_to_centers)[-topk:]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[idx] = X_arr[pick].copy()\n                        diag_scale[idx] = np.ones(n)\n                        lowrank_dirs[idx] = [[], []]\n                        ladder[idx] = self.ladder_levels // 2\n                        stagn[idx] = 0\n\n            # prune archive to keep memory bounded (keep best and spread)\n            if len(X) > self.max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:max(200, self.dim * 10)]\n                rest_idx = idx_sorted[len(keep_best):]\n                if len(rest_idx) > 0:\n                    step = max(1, len(rest_idx) // (self.max_archive - len(keep_best)))\n                    keep_rest = rest_idx[::step]\n                else:\n                    keep_rest = np.array([], dtype=int)\n                keep_idx = np.concatenate([keep_best, keep_rest])[:self.max_archive]\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasional global mix: replace first few centers with top archive to re-anchor\n            if len(X) > 0 and (iter_count % 5 == 0):\n                best_centers = get_top_centers(min(len(centers), max(1, len(X) // 10)))\n                for i in range(min(len(best_centers), len(centers))):\n                    centers[i] = best_centers[i].copy()\n\n            # small termination safety\n            if not improved_any and iter_count > 3 and all(l == self.ladder_levels - 1 for l in ladder) and evals >= 0.9 * budget:\n                # near budget and all exploring; take few random final tries if budget allows\n                for _ in range(min(5, budget - evals)):\n                    x = self.rng.uniform(lb, ub)\n                    out = safe_eval(x)\n                    if out is None:\n                        break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 243, in __call__, the following error occurred:\nIndexError: index 804 is out of bounds for axis 0 with size 800\nOn line: X_nei = X_arr[idx_nei]; F_nei = F_arr[idx_nei]", "error": "In the code, line 243, in __call__, the following error occurred:\nIndexError: index 804 is out of bounds for axis 0 with size 800\nOn line: X_nei = X_arr[idx_nei]; F_nei = F_arr[idx_nei]", "parent_ids": "7f1408a6-3d25-4e48-bd31-d2dacd374f35", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6a3a0463-0669-47d8-b32f-3321d054d36d", "fitness": "-inf", "name": "EDACS", "description": "EDACS maintains a small ensemble of centers with per-center trust radii, per-dimension learning rates, velocities (momentum) and temperatures to explore multiple basins in parallel while adapting local step scales. It fits a cheap weighted linear surrogate with SVD-based low-rank principal directions to estimate a gradient-like linear term and directional curvature proxies, which drive a curvature-scaled quasi-Newton step and PCA line probes (with robust directional curvature-to-dimension mapping) for informed local moves. Exploration is diversified through DE-style mutation/crossover among centers, anisotropic Gaussian sampling, occasional heavy-tailed (Levy) jumps, and adaptive annealed acceptance to escape local minima, while successes expand trust and lr and failures shrink them (with stagnation-based center replacement from the archive). Practical controls include budget-aware safe_eval and initial quasi-random seeding, archive pruning and periodic refresh of centers, caps on per-iteration evaluations, and several tuned defaults (ensemble size, curv_rank, lr/trust multipliers, temperature decay) to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EDACS:\n    \"\"\"\n    Ensemble Directional Annealing with Curvature Scaling (EDACS)\n\n    Main idea:\n      - Maintain a small ensemble of centers with per-center trust radii, per-dimension learning rates,\n        velocities and temperatures.\n      - Fit a cheap weighted linear surrogate in a local neighborhood to estimate a gradient (linear term)\n        and low-rank directional curvature proxies (via SVD of centered neighbor displacements).\n      - Use curvature-scaled quasi-Newton steps, DE-style mutation, PCA line-probes and anisotropic Gaussian\n        sampling to generate proposals.\n      - Accept proposals when they improve the center (or global best), otherwise accept with annealed probability\n        to escape local minima. Adapt trust radii and learning rates on success/failure and replace stagnated centers.\n      - Careful budget accounting (safe_eval) and archive pruning to bound memory.\n\n    Key parameters (tunables):\n      - budget, dim: required\n      - ensemble_size: number of centers (default: min(7, max(3, dim//2 + 1)))\n      - init_ratio: fraction of budget used for initial seeding (default 0.20)\n      - curv_rank: number of directions to estimate curvature (default min(3, dim))\n      - trust_init_frac, trust_min_frac, trust_max_frac: fractions of search range to initialize/clamp trust radii\n      - lr_init_frac, lr_inc, lr_dec: per-dimension learning-rate init and adapt factors\n      - temp_init, temp_decay: annealing temperature initial and per-iter decay\n      - recomb_prob, de_F: DE-style mutation probability and differential weight\n      - levy_prob, levy_scale_frac: occasional heavy-tail jumps\n      - max_eval_per_iter: cap on evals per main loop\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, ensemble_size=None,\n                 init_ratio=0.20, curv_rank=None, max_eval_per_iter=40,\n                 recomb_prob=0.14, de_F=0.8,\n                 levy_prob=0.05, levy_scale_frac=0.9,\n                 temp_init=1.0, temp_decay=0.995):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(self.seed)\n\n        # ensemble sizing\n        if ensemble_size is None:\n            self.ensemble_size = max(3, min(7, self.dim // 2 + 1))\n        else:\n            self.ensemble_size = int(ensemble_size)\n\n        # initialization & local model\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(8, 2 * self.dim)\n        self.max_init = min(800, int(0.4 * self.budget))\n        self.curv_rank = min(self.dim, self.dim if curv_rank is None else int(curv_rank))\n\n        # trust region fractions and learning rates\n        self.trust_init_frac = 0.5\n        self.trust_min_frac = 1e-7\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.6\n        self.failure_shrink = 0.5\n\n        self.lr_init_frac = 0.18\n        self.lr_inc = 1.4\n        self.lr_dec = 0.55\n        self.lr_min_frac = 1e-9\n\n        # velocity / temperature (annealing)\n        self.momentum = 0.75\n        self.temp_init = float(temp_init)\n        self.temp_decay = float(temp_decay)\n\n        # recombination & escapes\n        self.recomb_prob = float(recomb_prob)\n        self.de_F = float(de_F)\n        self.levy_prob = float(levy_prob)\n        self.levy_scale_frac = float(levy_scale_frac)\n        self.levy_clip = 12.0\n\n        # eval control\n        self.max_eval_per_iter = int(max_eval_per_iter)\n\n        # archive & center management\n        self.max_archive_base = max(1200, 30 * self.dim)\n        self.center_replace_patience = 8\n        self.max_centers = max(3, min(12, self.ensemble_size * 2))\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds handling (Many BBOB uses [-5,5], but use func.bounds if provided)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archive\n        X = []\n        F = []\n        f_best = np.inf\n        x_best = None\n\n        # initial quasi-random seeding (slightly larger than EMALP)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper to get distinct top centers (returns centers and their function values)\n        def get_top_centers_with_f(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            centers_f = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if not any(np.linalg.norm(x - c) <= 1e-12 for c in centers):\n                    centers.append(x)\n                    centers_f.append(float(F[idx]))\n                if len(centers) >= k:\n                    break\n            return list(zip(centers, centers_f))\n\n        # initialize centers from top archive entries\n        init_centers = get_top_centers_with_f(min(self.max_centers, len(X)))\n        if len(init_centers) == 0:\n            # fallback random\n            init_centers = [(np.array(self.rng.uniform(lb, ub)), None) for _ in range(self.ensemble_size)]\n        centers = [c for c, _ in init_centers]\n        center_f = [cf if cf is not None else np.nan for _, cf in init_centers]\n        # ensure we have desired number of centers\n        while len(centers) < self.ensemble_size:\n            x = np.array(self.rng.uniform(lb, ub))\n            centers.append(x)\n            center_f.append(np.nan)\n\n        # per-center state\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n        lr = [np.maximum(self.lr_init_frac * rng_range, 1e-12) for _ in centers]\n        velocity = [np.zeros(n) for _ in centers]\n        stagn = [0 for _ in centers]\n        temps = [self.temp_init for _ in centers]\n\n        # safe evaluation wrapper\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # fit a weighted linear surrogate and low-rank directional curvatures (different conversion than EMALP)\n        def fit_local_proxy(center, X_nei, F_nei, rank):\n            # m x n, m\n            dx = X_nei - center\n            m = dx.shape[0]\n            if m < 4:\n                return None\n            dists = np.linalg.norm(dx, axis=1) + 1e-12\n            w = 1.0 / (dists + 1e-12)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n\n            A = np.hstack([np.ones((m, 1)), dx])  # m x (n+1)\n            try:\n                Ar = (W * A)\n                br = (W * F_nei)\n                ridge = 1e-8 * np.eye(A.shape[1])\n                params, *_ = np.linalg.lstsq(Ar.T @ Ar + ridge, Ar.T @ br, rcond=None)\n                a = float(params[0]); b = params[1:].flatten()\n            except Exception:\n                return None\n\n            resid = F_nei - (a + dx.dot(b))\n\n            # SVD on weighted dx\n            try:\n                Dx = (W * dx)\n                U, S, Vt = np.linalg.svd(Dx, full_matrices=False)\n                pcs = Vt.T[:, :max(1, min(rank, Vt.shape[0]))]\n            except Exception:\n                pcs = np.eye(n, 1)\n\n            # estimate directional curvature via robust regression of residual on p^2\n            h_dir = np.zeros(pcs.shape[1])\n            for j in range(pcs.shape[1]):\n                v = pcs[:, j]\n                p = dx.dot(v)\n                P = 0.5 * (p ** 2)  # scalar per neighbor\n                # robust weighted linear fit of resid ~ alpha + h * 0.5 p^2  -> h estimate\n                try:\n                    # solve for h : minimize ||W*(resid - h*P)||^2\n                    denom = np.sum((W.flatten() * P) ** 2) + 1e-10\n                    num = np.sum(W.flatten() * P * (W.flatten() * resid))\n                    h_est = num / denom\n                    h_dir[j] = float(h_est)\n                except Exception:\n                    h_dir[j] = 0.0\n\n            # convert directional curvatures to per-dim proxies (different mapping: signed sqrt-weighted)\n            h_per_dim = np.zeros(n)\n            for j in range(pcs.shape[1]):\n                v = pcs[:, j]\n                # take signed sqrt of magnitude to soften extremes\n                val = np.sign(h_dir[j]) * np.sqrt(np.abs(h_dir[j]) + 1e-12)\n                h_per_dim += val * (v ** 2)\n            # convert to non-negative curvature magnitude proxy for scaling denominator\n            h_mag = np.maximum(np.abs(h_per_dim), 1e-9)\n            return a, b, h_mag, pcs, h_dir\n\n        iter_count = 0\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            iter_count += 1\n\n            # periodic refresh: merge archive bests to maintain diversity\n            if (iter_count % 10) == 0 and len(X) > 0:\n                topk = min(self.max_centers, max(1, len(X) // 16))\n                new_centers_with_f = get_top_centers_with_f(topk)\n                merged = []\n                merged_f = []\n                for c, cf in (list(zip(centers, center_f)) + new_centers_with_f):\n                    if not any(np.allclose(c, m, atol=1e-12) for m in merged):\n                        merged.append(np.array(c))\n                        merged_f.append(float(cf) if cf is not None and not np.isnan(cf) else np.nan)\n                    if len(merged) >= self.max_centers:\n                        break\n                centers = merged[:self.max_centers]\n                center_f = merged_f[:self.max_centers]\n                # align other arrays\n                trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n                lr = [np.maximum(self.lr_init_frac * rng_range, 1e-12) for _ in centers]\n                velocity = [np.zeros(n) for _ in centers]\n                stagn = [0 for _ in centers]\n                temps = [self.temp_init for _ in centers]\n\n            improved_any = False\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            # precompute arrays for neighborhood picks\n            X_arr = np.asarray(X) if len(X) > 0 else np.empty((0, n))\n            F_arr = np.asarray(F) if len(F) > 0 else np.empty((0,))\n\n            for idx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[idx])\n                tr = np.array(trust_radius[idx])\n                lr_vec = np.array(lr[idx])\n                vel = np.array(velocity[idx])\n                temp = temps[idx]\n\n                # ensure center_f is set (try to find in archive)\n                if np.isnan(center_f[idx]) and len(X) > 0:\n                    dists_to_archive = np.linalg.norm(X_arr - center, axis=1)\n                    min_idx = int(np.argmin(dists_to_archive))\n                    center_f[idx] = float(F_arr[min_idx])\n\n                # collect neighbors for local model (if enough data)\n                local_model = None\n                if len(X) >= max(4 * n, 4 * self.curv_rank):\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), max(4 * n, 6 * self.curv_rank))\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]; F_nei = F_arr[idx_nei]\n                    local_model = fit_local_proxy(center, X_nei, F_nei, self.curv_rank)\n\n                proposals = []\n                meta = []  # metadata for proposals (type, surrogate_pred or None)\n\n                # 1) curvature-scaled quasi-Newton proposal (conservative)\n                if local_model is not None:\n                    a_loc, b_loc, h_loc, pcs, h_dir = local_model\n                    # scale step by per-dim lr and curvature: delta = - lr * b / (1 + sqrt(h))\n                    denom = 1.0 + np.sqrt(h_loc + 1e-12)\n                    delta = - (lr_vec * b_loc) / denom\n                    # clamp by trust radius\n                    delta = np.clip(delta, -tr, tr)\n                    x_q = np.clip(center + delta, lb, ub)\n                    # surrogate prediction using quadratic approximation\n                    pred = a_loc + delta.dot(b_loc) + 0.5 * np.sum(h_loc * (delta ** 2))\n                    proposals.append(x_q); meta.append(('quad', float(pred)))\n                else:\n                    # if no model, propose a scaled gradient-free quasi-step using velocity\n                    delta = vel * 0.9 + self.rng.randn(n) * (0.25 * tr)\n                    x_q = np.clip(center + delta, lb, ub)\n                    proposals.append(x_q); meta.append(('vx', None))\n\n                # 2) DE-style mutation + crossover among centers (differential proposals)\n                if len(centers) >= 3 and self.rng.rand() < self.recomb_prob:\n                    # pick three distinct centers different from idx\n                    ids = list(range(len(centers)))\n                    ids.remove(idx)\n                    a_id = self.rng.choice(ids)\n                    ids2 = ids.copy(); ids2.remove(a_id)\n                    b_id = self.rng.choice(ids2)\n                    c_id = self.rng.choice([i for i in ids2 if i != b_id] + [b_id])\n                    a = centers[a_id]; b = centers[b_id]; c = centers[c_id]\n                    mutant = np.clip(a + self.de_F * (b - c), lb, ub)\n                    # crossover with center\n                    cross = center.copy()\n                    cr = 0.7\n                    mask = self.rng.rand(n) < cr\n                    cross[mask] = mutant[mask]\n                    # add localized jitter scaled by trust\n                    cross = np.clip(cross + self.rng.randn(n) * (0.18 * tr), lb, ub)\n                    proposals.append(cross); meta.append(('de', None))\n\n                # 3) anisotropic Gaussian local exploration (drawn from trust and lr)\n                std = np.maximum(0.5 * tr, 1e-12) * (0.6 + self.rng.rand())\n                gauss = np.clip(center + self.rng.randn(n) * std, lb, ub)\n                proposals.append(gauss); meta.append(('gauss', None))\n\n                # 4) PCA line probes using local_model principal components or random directions\n                if local_model is not None:\n                    _, b_loc, _, pcs, _ = local_model\n                    # choose direction as negative linear term projected to top pc or b itself\n                    if np.linalg.norm(b_loc) > 1e-12:\n                        g_dir = -b_loc / (np.linalg.norm(b_loc) + 1e-12)\n                    else:\n                        g_dir = pcs[:, 0] if pcs.shape[1] > 0 else self.rng.randn(n)\n                        g_dir = g_dir / (np.linalg.norm(g_dir) + 1e-12)\n                    base = np.mean(tr)\n                    for s in (0.35, 0.85, 1.6):\n                        step = s * base * (0.7 + 0.6 * self.rng.rand())\n                        x1 = np.clip(center + g_dir * step, lb, ub)\n                        x2 = np.clip(center - g_dir * step, lb, ub)\n                        proposals.append(x1); meta.append(('line', None))\n                        proposals.append(x2); meta.append(('line', None))\n                else:\n                    # random directional probes\n                    for _ in range(2):\n                        d = self.rng.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-12)\n                        step = np.mean(tr) * (0.4 + self.rng.rand())\n                        proposals.append(np.clip(center + d * step, lb, ub)); meta.append(('randline', None))\n\n                # 5) occasional heavy-tail jump\n                if self.rng.rand() < self.levy_prob:\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -self.levy_clip, self.levy_clip)\n                    scale = self.levy_scale_frac * np.maximum(rng_range, 1e-9)\n                    x_jump = np.clip(center + jump * scale, lb, ub)\n                    proposals.append(x_jump); meta.append(('levy', None))\n\n                # deduplicate proposals (preserve corresponding meta)\n                unique = []\n                uniq_meta = []\n                for i, p in enumerate(proposals):\n                    if not any(np.allclose(p, q, atol=1e-12) for q in unique):\n                        unique.append(p)\n                        uniq_meta.append(meta[i] if i < len(meta) else (None, None))\n\n                # rank proposals:\n                # prefer surrogate predictions (lower first), then those closer to center\n                preds_idx = [i for i, mm in enumerate(uniq_meta) if mm[0] == 'quad' and not np.isnan(mm[1])]\n                rest_idx = [i for i in range(len(unique)) if i not in preds_idx]\n                preds_idx.sort(key=lambda i: uniq_meta[i][1])\n                rest_idx.sort(key=lambda i: np.linalg.norm(unique[i] - center))\n                ranked = preds_idx + rest_idx\n\n                # evaluate until one accepted or out of work_allow\n                accepted = False\n                for ridx in ranked:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    xprop = unique[ridx]\n                    out = safe_eval(xprop)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    fprop, xprop = out\n\n                    # acceptance criteria: if improves center or accepted by annealed prob\n                    fcenter = center_f[idx] if not np.isnan(center_f[idx]) else np.inf\n                    # if global best improved, prefer global improvement\n                    accept = False\n                    if fprop < fcenter - 1e-12:\n                        accept = True\n                    else:\n                        # annealed acceptance, temperature depends on center\n                        delta = fprop - fcenter\n                        # if center_f unknown (nan), use global best as baseline to compute delta\n                        if np.isnan(fcenter):\n                            delta = fprop - f_best\n                        # scaled probability\n                        prob = np.exp(-max(0.0, delta) / (temp + 1e-12))\n                        if self.rng.rand() < prob:\n                            accept = True\n\n                    if accept:\n                        accepted = True\n                        improved_any = True\n                        # update velocity and center\n                        new_vel = self.momentum * vel + 0.9 * (xprop - center)\n                        velocity[idx] = new_vel\n                        centers[idx] = xprop.copy()\n                        center_f[idx] = float(fprop)\n                        # success: expand trust and increase lr\n                        trust_radius[idx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                        lr[idx] = np.minimum(np.array(lr_vec) * self.lr_inc, self.trust_max_frac * rng_range)\n                        stagn[idx] = 0\n                        # reduce local temperature slightly\n                        temps[idx] = max(temps[idx] * self.temp_decay, 1e-6)\n                        break\n                    else:\n                        # rejected: keep record; continue\n                        pass\n\n                # if no proposal accepted, attempt a short local adaptive line informed by lr & gradient proxy\n                improved_local = False\n                if (not accepted) and work_allow > 0 and local_model is not None:\n                    _, b_loc, _, pcs, _ = local_model\n                    if np.linalg.norm(b_loc) > 1e-12:\n                        dir0 = -b_loc / (np.linalg.norm(b_loc) + 1e-12)\n                    else:\n                        dir0 = pcs[:, 0] if pcs.shape[1] > 0 else self.rng.randn(n)\n                        dir0 = dir0 / (np.linalg.norm(dir0) + 1e-12)\n                    avg_lr = np.mean(np.maximum(lr_vec, 1e-12))\n                    steps = [0.4, 0.9, 2.0]\n                    for s in steps:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        step = s * avg_lr * (0.8 + 0.6 * self.rng.rand())\n                        x_try = np.clip(center + dir0 * step, lb, ub)\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < (center_f[idx] if not np.isnan(center_f[idx]) else np.inf) - 1e-12:\n                            improved_any = True\n                            improved_local = True\n                            centers[idx] = xtry.copy()\n                            center_f[idx] = float(ftry)\n                            velocity[idx] = self.momentum * vel + 0.7 * (xtry - center)\n                            trust_radius[idx] = np.minimum(tr * 1.35, self.trust_max_frac * rng_range)\n                            lr[idx] = np.minimum(np.array(lr_vec) * self.lr_inc, self.trust_max_frac * rng_range)\n                            stagn[idx] = 0\n                            temps[idx] = max(temps[idx] * self.temp_decay, 1e-6)\n                            break\n\n                # if still no improvement: shrink trust and lr, increase temperature slightly\n                if (not accepted) and (not improved_local):\n                    stagn[idx] += 1\n                    trust_radius[idx] = np.maximum(tr * self.failure_shrink, self.trust_min_frac * rng_range)\n                    lr[idx] = np.maximum(np.array(lr_vec) * self.lr_dec, self.lr_min_frac * rng_range)\n                    velocity[idx] = self.momentum * vel\n                    temps[idx] = temps[idx] / self.temp_decay  # temporarily increase exploration\n\n                # replace stagnated center from archive or random restart\n                if stagn[idx] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        # pick a point from archive that is far from current centers\n                        X_arr = np.asarray(X)\n                        # compute distance to nearest center\n                        dists_to_cent = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        topk = max(1, min(60, len(X_arr)))\n                        cand_idx = np.argsort(dists_to_cent)[-topk:]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[idx] = X_arr[pick].copy()\n                        center_f[idx] = float(F[pick])\n                    else:\n                        centers[idx] = np.array(self.rng.uniform(lb, ub))\n                        center_f[idx] = np.nan\n                    trust_radius[idx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                    lr[idx] = np.maximum(self.lr_init_frac * rng_range, 1e-12)\n                    velocity[idx] = np.zeros(n)\n                    stagn[idx] = 0\n                    temps[idx] = self.temp_init\n\n            # archive pruning to bound memory\n            max_archive = max(self.max_archive_base, 20 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:300]\n                rest_idx = idx_sorted[300:]\n                if len(rest_idx) > 0:\n                    step = max(1, len(rest_idx) // (max_archive - 300))\n                    keep_rest = rest_idx[::step]\n                else:\n                    keep_rest = np.array([], dtype=int)\n                keep_idx = np.concatenate([keep_best, keep_rest])[:max_archive]\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasional center refresh from top archive\n            if len(X) > 0 and (iter_count % 7 == 0):\n                best_centers = get_top_centers_with_f(min(len(centers), max(1, len(X) // 10)))\n                for i in range(min(len(best_centers), len(centers))):\n                    centers[i] = best_centers[i][0].copy()\n                    center_f[i] = float(best_centers[i][1])\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 288, in __call__, the following error occurred:\nIndexError: index 1080 is out of bounds for axis 0 with size 1079\nOn line: X_nei = X_arr[idx_nei]; F_nei = F_arr[idx_nei]", "error": "In the code, line 288, in __call__, the following error occurred:\nIndexError: index 1080 is out of bounds for axis 0 with size 1079\nOn line: X_nei = X_arr[idx_nei]; F_nei = F_arr[idx_nei]", "parent_ids": "7f1408a6-3d25-4e48-bd31-d2dacd374f35", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9aae7931-84c6-40e6-8b35-178c77d5f7ed", "fitness": 0.3703250529908733, "name": "HESAS", "description": "HESAS is a hybrid ensemble surrogate-adaptive search that maintains multiple per-center states (anisotropic per-dimension trust radii, per-center learning rates and velocities) and iteratively proposes candidates around a set of top “centers” drawn from an archive. It fits cheap weighted local surrogates (a linear term + separable per-dimension quadratic plus a low-rank directional curvature estimated via weighted SVD and small-rank regression with ridge regularization) and uses conservative per-dimension minimizers from those models. A rich mix of move generators (momentum/mirror toward the global best, recombination between centers, surrogate-guided line probes, multi-scale directional probes, coordinate tweaks and occasional tempered Cauchy jumps) are de-duplicated, ranked (surrogate predictions first, then short moves) and evaluated with adaptive trust/learning-rate expansion on success and shrinking + velocity damping on failure, while stagnated centers are replaced by distant archive points. Budget-awareness and robustness are enforced by strict safe_eval counting, archive pruning/refresh, modest ensemble/curvature ranks and low mutation probabilities (e.g. cauchy_prob=0.1, recomb_prob=0.12) to bias toward careful local exploitation with occasional exploration.", "code": "import numpy as np\n\nclass HESAS:\n    \"\"\"\n    Hybrid Ensemble Surrogate-Adaptive Search (HESAS)\n\n    Ensemble-driven continuous optimizer combining:\n    - anisotropic per-dimension trust radii and per-center learning rates\n    - cheap weighted separable quadratic + low-rank directional surrogate fits\n    - conservative surrogate minimizers, momentum/mirror steps, recombination,\n      multi-scale directional probes, coordinate tweaks, and tempered Lévy/Cauchy escapes\n    - strict safe_eval budget enforcement, archive pruning and stagnation-based center replacement\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 ensemble_size=None, init_ratio=0.12, max_eval_per_iter=48):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed if seed is not None else 0\n        self.rng = np.random.RandomState(self.seed)\n\n        # ensemble sizing\n        if ensemble_size is None:\n            self.ensemble_size = max(3, min(7, self.dim // 2 + 1))\n        else:\n            self.ensemble_size = int(ensemble_size)\n\n        # initialization sampling\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(8, 2 * self.dim)\n        self.max_init = min(600, int(0.4 * self.budget))\n\n        # neighbor/modeling\n        self.nei_mult = 6           # neighbors = nei_mult * dim\n        self.ridge = 1e-6\n\n        # low-rank curvature\n        self.curv_rank = min(3, self.dim)\n\n        # trust & learning-rate adaptation\n        self.trust_init_frac = 0.5\n        self.trust_min_frac = 1e-6\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.6\n        self.failure_shrink = 0.65\n\n        self.lr_init_frac = 0.2\n        self.lr_inc = 1.25\n        self.lr_dec = 0.6\n        self.lr_min_frac = 1e-8\n\n        # momentum / mirror\n        self.momentum_decay = 0.75\n        self.mirror_strength = 0.6\n\n        # escapes & recombination\n        self.cauchy_prob = 0.10\n        self.cauchy_scale_frac = 0.6\n        self.cauchy_clip = 8.0\n        self.recomb_prob = 0.12\n\n        # per-loop controls\n        self.max_eval_per_iter = int(max_eval_per_iter)\n\n        # archive & center management\n        self.max_archive_base = max(1500, 40 * self.dim)\n        self.center_replace_patience = 10\n        self.max_centers = max(2, min(12, self.ensemble_size * 2))\n\n    def __call__(self, func):\n        n = int(self.dim)\n\n        # bounds handling (functions in Many Affine BBOB use [-5,5])\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        budget = int(self.budget)\n        evals = 0\n\n        # archive\n        X = []\n        F = []\n\n        f_best = np.inf\n        x_best = None\n\n        # initial space-filling-ish samples\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper to pick distinct top centers\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if all(np.linalg.norm(x - c) > 1e-10 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.max_centers, len(X)))\n        if len(centers) == 0:\n            centers = [np.array(self.rng.uniform(lb, ub))]\n\n        # per-center state\n        trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]  # per-dim arrays\n        lr = [np.maximum(self.lr_init_frac * rng_range, 1e-12) for _ in centers]\n        velocity = [np.zeros(n) for _ in centers]\n        stagn = [0 for _ in centers]\n\n        # safe_eval ensures we never exceed total budget and clips to bounds, updates archive and best\n        def safe_eval(x):\n            nonlocal evals, budget, X, F, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-12:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # fit local model: weighted linear + separable (per-dim) curvature + low-rank directional curvature\n        def fit_local_model(center, X_nei, F_nei, rank):\n            dx = X_nei - center  # m x n\n            m = dx.shape[0]\n            if m < 3:\n                return None\n            dists = np.linalg.norm(dx, axis=1) + 1e-12\n            w = 1.0 / dists\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n\n            # linear fit\n            A_lin = np.hstack([np.ones((m, 1)), dx])\n            try:\n                Ar = (W * A_lin)\n                br = (W * F_nei)\n                ridge = self.ridge * np.eye(A_lin.shape[1])\n                params_lin, *_ = np.linalg.lstsq(Ar.T @ Ar + ridge, Ar.T @ br, rcond=None)\n                a = float(params_lin[0])\n                b = params_lin[1:].flatten()\n            except Exception:\n                return None\n\n            # separable quadratic terms (per-dim) using dx^2 columns\n            A_sep = np.hstack([np.ones((m, 1)), dx, 0.5 * (dx ** 2)])\n            try:\n                Ar = (W * A_sep)\n                ridge2 = (1e-6) * np.eye(A_sep.shape[1])\n                params_q, *_ = np.linalg.lstsq(Ar.T @ Ar + ridge2, Ar.T @ (W * F_nei), rcond=None)\n                params_q = params_q.flatten()\n                h_diag = params_q[1 + n:1 + 2 * n]\n                h_diag = np.maximum(h_diag, 1e-8)\n            except Exception:\n                h_diag = np.ones(n) * 1e-6\n\n            # low-rank directional curvature via weighted SVD of dx\n            try:\n                Dx = W * dx\n                U, S, Vt = np.linalg.svd(Dx, full_matrices=False)\n                pcs = Vt.T[:, :max(1, min(rank, Vt.shape[0]))]  # n x r\n            except Exception:\n                pcs = np.eye(n, 1)\n\n            # estimate directional curvature on residuals\n            resid = F_nei - (a + dx.dot(b))\n            h_dir = np.zeros(pcs.shape[1])\n            for j in range(pcs.shape[1]):\n                v = pcs[:, j]\n                p = dx.dot(v)\n                P = np.vstack([p, 0.5 * (p ** 2)]).T  # m x 2\n                try:\n                    Pr = (W * P)\n                    c, *_ = np.linalg.lstsq(Pr.T @ Pr + 1e-9 * np.eye(2), Pr.T @ (W.flatten() * resid), rcond=None)\n                    h_dir[j] = float(c[1]) if c.shape[0] > 1 else 0.0\n                except Exception:\n                    h_dir[j] = 0.0\n\n            # distribute directional curvatures to per-dim approx\n            h_per_dim = np.zeros(n)\n            for j in range(pcs.shape[1]):\n                v = pcs[:, j]\n                h_per_dim += np.abs(h_dir[j]) * (v ** 2)\n            h_per_dim = np.maximum(h_per_dim, 1e-8)\n\n            return a, b, h_diag, h_per_dim, pcs, h_dir\n\n        iter_count = 0\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            iter_count += 1\n\n            # refresh centers periodically to include recent bests\n            if (iter_count % 6) == 0 and len(X) > 0:\n                topk = min(self.max_centers, max(1, len(X) // 12))\n                new_centers = get_top_centers(topk)\n                merged = []\n                for c in (centers + new_centers):\n                    if not any(np.allclose(c, m, atol=1e-12) for m in merged):\n                        merged.append(np.array(c))\n                    if len(merged) >= self.max_centers:\n                        break\n                centers = merged[:self.max_centers]\n                # ensure state arrays match length\n                if len(trust_radius) != len(centers):\n                    trust_radius = [np.maximum(self.trust_init_frac * rng_range, 1e-9) for _ in centers]\n                    lr = [np.maximum(self.lr_init_frac * rng_range, 1e-12) for _ in centers]\n                    velocity = [np.zeros(n) for _ in centers]\n                    stagn = [0 for _ in centers]\n\n            improved_any = False\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            for idx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[idx])\n                tr = np.array(trust_radius[idx])\n                lr_vec = np.array(lr[idx])\n                vel = np.array(velocity[idx])\n\n                # gather neighbors\n                X_arr = np.asarray(X) if len(X) > 0 else np.empty((0, n))\n                F_arr = np.asarray(F) if len(F) > 0 else np.empty((0,))\n                enough_data = len(X) >= max(3 * n, self.nei_mult * n)\n\n                local_model = None\n                if enough_data:\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m = min(len(X), max(3 * n, self.nei_mult * n))\n                    idx_nei = idx_sorted[:m]\n                    X_nei = X_arr[idx_nei]\n                    F_nei = F_arr[idx_nei]\n                    local_model = fit_local_model(center, X_nei, F_nei, self.curv_rank)\n\n                proposals = []\n                surrogate_preds = []\n\n                # 1) conservative separable quadratic minimizer from model (per-dim)\n                if local_model is not None:\n                    a_loc, b_loc, h_diag_sep, h_per_dim, pcs, h_dir = local_model\n                    delta_sep = - b_loc / (h_diag_sep + 1e-20)\n                    delta_sep = np.clip(delta_sep, -tr, tr)\n                    x_sep = np.clip(center + delta_sep, lb, ub)\n                    proposals.append(x_sep)\n                    # conservative predicted value using per-dim curvature\n                    pred_sep = a_loc + delta_sep.dot(b_loc) + 0.5 * np.sum(h_per_dim * (delta_sep ** 2))\n                    surrogate_preds.append(float(pred_sep))\n                else:\n                    surrogate_preds.append(np.nan)\n\n                # 2) momentum / mirror step toward global best + small jitter\n                if x_best is not None:\n                    mirror = center + self.mirror_strength * (center - x_best)\n                else:\n                    mirror = center.copy()\n                noise = self.rng.randn(n) * (0.35 * tr)\n                x_m = np.clip(0.6 * mirror + 0.4 * center + 0.9 * vel + noise, lb, ub)\n                proposals.append(x_m); surrogate_preds.append(np.nan)\n\n                # 3) recombination with another center\n                if (len(centers) > 1) and (self.rng.rand() < self.recomb_prob):\n                    # pick partner far from this center to encourage exploration\n                    dists_cent = np.array([np.linalg.norm(center - np.array(c)) for c in centers])\n                    prob = dists_cent + 1e-9\n                    prob[idx] = 0.0\n                    if prob.sum() > 0:\n                        prob = prob / prob.sum()\n                        j = int(self.rng.choice(len(centers), p=prob))\n                    else:\n                        j = self.rng.randint(len(centers))\n                    partner = np.array(centers[j])\n                    alpha = self.rng.rand()\n                    child = np.clip(alpha * center + (1 - alpha) * partner + self.rng.randn(n) * (0.2 * tr), lb, ub)\n                    proposals.append(child); surrogate_preds.append(np.nan)\n\n                # 4) surrogate-guided line probes along negative gradient or top pc\n                if local_model is not None:\n                    _, b_loc, _, _, pcs, _ = local_model\n                    g = b_loc\n                    if np.linalg.norm(g) > 1e-12:\n                        g_dir = -g / (np.linalg.norm(g) + 1e-12)\n                    else:\n                        g_dir = pcs[:, 0] if pcs.shape[1] > 0 else self.rng.randn(n)\n                        g_dir = g_dir / (np.linalg.norm(g_dir) + 1e-12)\n                    base = np.mean(tr)\n                    for s in (0.3, 0.7, 1.1):\n                        step = s * base\n                        proposals.append(np.clip(center + g_dir * step, lb, ub)); surrogate_preds.append(np.nan)\n                        proposals.append(np.clip(center - g_dir * step, lb, ub)); surrogate_preds.append(np.nan)\n                else:\n                    # fallback small random probes\n                    for _ in range(2):\n                        z = np.clip(center + self.rng.randn(n) * (0.6 * tr), lb, ub)\n                        proposals.append(z); surrogate_preds.append(np.nan)\n\n                # 5) multi-scale directional probes (short list to conserve budget)\n                num_dirs = min(6, 4 + n // 3)\n                for _ in range(min(3, num_dirs)):\n                    v = self.rng.randn(n)\n                    v = v / (np.linalg.norm(v) + 1e-12)\n                    for s in (0.4, 1.0):\n                        x1 = np.clip(center + v * (s * np.linalg.norm(tr) / np.sqrt(float(n) + 1e-12)), lb, ub)\n                        x2 = np.clip(center - v * (s * np.linalg.norm(tr) / np.sqrt(float(n) + 1e-12)), lb, ub)\n                        proposals.append(x1); surrogate_preds.append(np.nan)\n                        proposals.append(x2); surrogate_preds.append(np.nan)\n\n                # 6) coordinate tweaks (a handful)\n                coord_candidates = []\n                coord_step = np.maximum(tr, 1e-12)\n                coords = np.arange(n)\n                max_coords = min(n, max(3, int(max(1, work_allow // 6))))\n                if max_coords < n:\n                    pick_coords = self.rng.choice(coords, size=max_coords, replace=False)\n                else:\n                    pick_coords = coords\n                for i in pick_coords:\n                    for sign in (+1.0, -1.0):\n                        x_ct = center.copy()\n                        x_ct[i] = x_ct[i] + sign * coord_step[i]\n                        coord_candidates.append(np.clip(x_ct, lb, ub))\n                proposals.extend(coord_candidates)\n                surrogate_preds.extend([np.nan] * len(coord_candidates))\n\n                # 7) occasional tempered Cauchy jump\n                if self.rng.rand() < self.cauchy_prob:\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -self.cauchy_clip, self.cauchy_clip)\n                    scale = self.cauchy_scale_frac * np.maximum(rng_range, 1e-9)\n                    x_jump = np.clip(center + jump * scale, lb, ub)\n                    proposals.append(x_jump); surrogate_preds.append(np.nan)\n\n                # deduplicate proposals\n                unique = []\n                uniq_preds = []\n                for i, p in enumerate(proposals):\n                    if not any(np.allclose(p, q, atol=1e-12) for q in unique):\n                        unique.append(np.array(p))\n                        uniq_preds.append(surrogate_preds[i] if i < len(surrogate_preds) else np.nan)\n\n                # ranking: proposals with finite surrogate preds first (lower predicted better),\n                # then short moves (smaller norm) next\n                finite_idx = [i for i, s in enumerate(uniq_preds) if not (s is None or np.isnan(s))]\n                nonfinite_idx = [i for i in range(len(unique)) if i not in finite_idx]\n                finite_idx.sort(key=lambda i: uniq_preds[i])\n                nonfinite_idx.sort(key=lambda i: np.linalg.norm(unique[i] - center))\n                ranked = finite_idx + nonfinite_idx\n\n                # evaluate proposals until success or work_allow exhausted\n                improved_local = False\n                for ridx in ranked:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    xprop = unique[ridx]\n                    # skip if identical to last archived to reduce redundant evals\n                    if len(X) > 0 and np.allclose(xprop, X[-1], atol=1e-12):\n                        continue\n                    out = safe_eval(xprop)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    fprop, xprop = out\n                    if fprop < f_best - 1e-12:\n                        # success: update center and state\n                        improved_any = True\n                        improved_local = True\n                        # velocity update (momentum)\n                        new_vel = self.momentum_decay * vel + 0.9 * (xprop - center)\n                        velocity[idx] = new_vel\n                        centers[idx] = xprop.copy()\n                        # expand trust and learning rates\n                        trust_radius[idx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                        lr[idx] = np.minimum(np.array(lr_vec) * self.lr_inc, self.trust_max_frac * rng_range)\n                        stagn[idx] = 0\n                        break\n\n                # if not improved, attempt short line probes along predicted descent direction (if model available)\n                if (not improved_local) and (work_allow > 0) and (local_model is not None):\n                    _, b_loc, _, _, pcs, _ = local_model\n                    if np.linalg.norm(b_loc) > 1e-12:\n                        dir0 = -b_loc / (np.linalg.norm(b_loc) + 1e-12)\n                    else:\n                        dir0 = pcs[:, 0] if pcs.shape[1] > 0 else self.rng.randn(n)\n                        dir0 = dir0 / (np.linalg.norm(dir0) + 1e-12)\n                    avg_lr = np.mean(np.maximum(lr_vec, 1e-12))\n                    steps = [0.6, 1.0, 1.8]\n                    for s in steps:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        step = s * avg_lr\n                        x_try = np.clip(center + dir0 * step, lb, ub)\n                        if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        ftry, xtry = out\n                        if ftry < f_best - 1e-12:\n                            improved_any = True\n                            improved_local = True\n                            centers[idx] = xtry.copy()\n                            velocity[idx] = self.momentum_decay * vel + 0.8 * (xtry - center)\n                            trust_radius[idx] = np.minimum(tr * 1.25, self.trust_max_frac * rng_range)\n                            lr[idx] = np.minimum(np.array(lr_vec) * self.lr_inc, self.trust_max_frac * rng_range)\n                            stagn[idx] = 0\n                            break\n\n                # if still no improvement, shrink trust and learning rates and damp velocity\n                if not improved_local:\n                    stagn[idx] += 1\n                    trust_radius[idx] = np.maximum(tr * self.failure_shrink, self.trust_min_frac * rng_range)\n                    lr[idx] = np.maximum(np.array(lr_vec) * self.lr_dec, self.lr_min_frac * rng_range)\n                    velocity[idx] = self.momentum_decay * vel\n\n                # replace stagnated center with a distant but decent archive point\n                if stagn[idx] >= self.center_replace_patience:\n                    if len(X) > 0:\n                        X_arr = np.asarray(X)\n                        # distance to nearest center\n                        d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                        topk = max(1, min(40, len(X_arr)))\n                        cand_idx = np.argsort(d_to_centers)[-topk:]\n                        pick = int(self.rng.choice(cand_idx))\n                        centers[idx] = X_arr[pick].copy()\n                        trust_radius[idx] = np.maximum(self.trust_init_frac * rng_range, 1e-9)\n                        lr[idx] = np.maximum(self.lr_init_frac * rng_range, 1e-12)\n                        velocity[idx] = np.zeros(n)\n                        stagn[idx] = 0\n\n            # archive pruning\n            max_archive = max(self.max_archive_base, 20 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                if len(rest_idx) > 0:\n                    step = max(1, len(rest_idx) // (max_archive - 200))\n                    keep_rest = rest_idx[::step]\n                else:\n                    keep_rest = np.array([], dtype=int)\n                keep_idx = np.concatenate([keep_best, keep_rest])[:max_archive]\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # occasional refresh of centers from archive bests\n            if len(X) > 0 and (iter_count % 5 == 0):\n                best_centers = get_top_centers(min(len(centers), max(1, len(X) // 12)))\n                for i in range(min(len(best_centers), len(centers))):\n                    centers[i] = best_centers[i].copy()\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HESAS scored 0.370 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "7f1408a6-3d25-4e48-bd31-d2dacd374f35", "operator": null, "metadata": {"aucs": [0.11936857289697178, 0.20539789114383689, 0.42200748274926914, 0.8776901583047834, 0.39874954822627895, 0.48385269162730193, 0.24420213940878455, 0.37651062540791425, 0.39565442622349645, 0.17981699392009542]}, "task_prompt": ""}
{"id": "9649c605-0872-4d65-87d9-6d31ea4ae126", "fitness": "-inf", "name": "HSME", "description": "HSME is a hybrid search that combines per-coordinate diagonal scaling (D) with a learned low-rank subspace U (SVD of recent successful steps) to sample anisotropic, low-dimensional directions while using mirrored Gaussian sampling for variance reduction. It maintains a short LRU memory of recent successful unit directions (dir_mem) and mixes samples toward that memory with probability mem_mix to bias search, while also allowing occasional DE-style archive differences (p_de = 0.20) and heavy-tailed Cauchy jumps (p_cauchy = 0.12) for diversification. Step-size control is dual: CMA-style path-length adaptation (ps, cs, damps, chi_n) for smooth sigma updates and multiplicative success/failure scaling (success_alpha=1.15, failure_alpha=0.85) to react quickly to improvements, and diagonal D is updated as an exponential moving second moment of selected steps. Budget-aware local improvements (cheap 1‑D golden-section line-search along evolution path and micro-polishing probes), an archive of evaluated points, and stagnation handling (inflate sigma and nudge mean toward archive/random) provide exploration/exploitation trade-offs; sensible defaults include k ≈ ceil(sqrt(n)), sigma₀ = 0.2·domain, and a small population lam tuned to dimension.", "code": "import numpy as np\nfrom collections import deque\n\nclass HSME:\n    \"\"\"\n    HSME (Hybrid Subspace-Memory Evolution)\n    - Combines per-coordinate (diagonal) adaptation and a low-rank subspace (SVD of recent successes)\n    - Biases sampling with an LRU memory of recent successful unit directions (MiSCH-style)\n    - Uses mirrored sampling, archive DE-style perturbations, occasional Cauchy jumps,\n      path-length sigma control + light multiplicative adaptation, and cheap budget-aware 1-D refinement.\n    One-line: diagonal + learned low-rank sampling guided by short directional memory and evolution-path line-search.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 subspace_k=None, memory_size=12, lambda0=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.memory_size = int(memory_size)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n        # population size\n        if lambda0 is None:\n            self.lambda0 = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        else:\n            self.lambda0 = int(lambda0)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (assume func provides bounds; fallback -5,5)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0; ub = 5.0\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        domain = np.maximum(1e-12, np.mean(ub - lb))\n\n        # basic strategy params\n        lam = max(1, int(self.lambda0))\n        mu = max(1, lam // 2)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1).astype(float))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length control constants\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # multiplicative sigma adapt (MiSCH-like)\n        success_alpha = 1.15\n        failure_alpha = 0.85\n        mem_mix = 0.6  # probability to bias an offspring in memory-driven combination\n\n        # dynamic state\n        m = self.rng.uniform(lb, ub)  # initial mean\n        sigma = 0.2 * domain\n        sigma_min = 1e-12\n        sigma_max = 1e2 * domain\n\n        # diagonal scales (std per-coordinate)\n        D = np.ones(n)\n        # low-rank subspace U (n x k) initial\n        rand_mat = self.rng.standard_normal(size=(n, self.k))\n        try:\n            U_init, _ = np.linalg.qr(rand_mat)\n            U = U_init[:, :self.k]\n        except Exception:\n            U = np.zeros((n, self.k))\n\n        # path and evolution accumulator\n        ps = np.zeros(n)\n        evo_path = np.zeros(n)\n        evo_decay = 0.85\n\n        # LRU memory of unit directions\n        dir_mem = deque(maxlen=self.memory_size)\n\n        # success buffer for subspace updating\n        success_buffer = []\n        buffer_max = max(10 * self.k, 20)\n        subspace_update_every = max(1, int(5))\n\n        # archive for DE-style differences\n        archive_X = []\n        archive_F = []\n\n        # other controls\n        p_cauchy = 0.12\n        cauchy_scale = 1.0\n        p_de = 0.20\n        F_de = 0.7\n        mirrored = True\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n        last_improv_eval = 0\n        stagnation_limit = max(10, int(5 + np.log(1 + n)))\n        no_improve_gens = 0\n        gen_count = 0\n\n        # safe_eval\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best, last_improv_eval\n            x = clip(x)\n            if evals >= budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n                last_improv_eval = evals\n            return float(f), x.copy()\n\n        # cheap 1-D bracket + golden-section along direction d from x0 (budget-aware, few evals)\n        def cheap_line_search(x0, f0, d, init_step=None, max_evals=8):\n            nonlocal evals\n            if init_step is None:\n                init_step = sigma\n            dn = np.linalg.norm(d)\n            if dn == 0 or evals >= budget:\n                return None, None\n            d = d / dn\n            remaining = max(0, budget - evals)\n            if remaining <= 0:\n                return None, None\n            # evaluate +s and -s\n            s = init_step\n            fa = f0\n            xb = clip(x0 + s * d)\n            fb, _ = safe_eval(xb)\n            if fb is None:\n                return None, None\n            remaining -= 1\n            if fb >= fa:\n                # try -s\n                xb2 = clip(x0 - s * d)\n                fb2, _ = safe_eval(xb2)\n                if fb2 is None:\n                    return None, None\n                remaining -= 1\n                if fb2 >= fa:\n                    return None, None\n                # use negative side\n                b = -s; fb = fb2\n            else:\n                b = s\n            # expand a bit with limited expansions\n            expand = 1.5\n            exp_count = 0\n            max_exp = 3\n            while remaining > 0 and exp_count < max_exp:\n                nb = b * expand\n                xn = clip(x0 + nb * d)\n                fn, _ = safe_eval(xn)\n                if fn is None:\n                    return None, None\n                remaining -= 1\n                if fn < fb:\n                    b = nb; fb = fn; exp_count += 1\n                    continue\n                break\n            # golden section on [0, b]\n            a = 0.0\n            b0 = b\n            gr = (np.sqrt(5) - 1) / 2\n            c = b0 - gr * (b0 - a)\n            d_alpha = a + gr * (b0 - a)\n            xc = clip(x0 + c * d); fd, _ = safe_eval(xc)\n            if fd is None:\n                return None, None\n            remaining -= 1\n            xd = clip(x0 + d_alpha * d); fc, _ = safe_eval(xd)\n            if fc is None:\n                return None, None\n            remaining -= 1\n            bestf = fa; bestx = x0.copy()\n            for val, xval in ((fd, xc), (fc, xd)):\n                if val < bestf:\n                    bestf = val; bestx = xval.copy()\n            it = 0\n            max_iters = max_evals\n            while remaining > 0 and it < max_iters and abs(b0 - a) > 1e-12:\n                it += 1\n                if fd < fc:\n                    b0 = d_alpha\n                    d_alpha = c\n                    fc = fd\n                    d_alpha = a + gr * (b0 - a)\n                    xd = clip(x0 + d_alpha * d)\n                    fd, _ = safe_eval(xd)\n                    if fd is None:\n                        break\n                    remaining -= 1\n                    if fd < bestf:\n                        bestf = fd; bestx = xd.copy()\n                else:\n                    a = c\n                    c = d_alpha\n                    fd = fc\n                    c = b0 - gr * (b0 - a)\n                    xc = clip(x0 + c * d)\n                    fc, _ = safe_eval(xc)\n                    if fc is None:\n                        break\n                    remaining -= 1\n                    if fc < bestf:\n                        bestf = fc; bestx = xc.copy()\n            if bestf < f0:\n                return bestf, bestx\n            return None, None\n\n        # initial evaluation of mean\n        if evals < budget:\n            fm, xm = safe_eval(m)\n            if fm is None:\n                return float(f_best), np.array(x_best, dtype=float)\n            # ensure m shares x_best if better\n            if x_best is not None:\n                m = x_best.copy()\n\n        # main loop: generations until budget exhausted\n        while evals < budget:\n            gen_count += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            if current_lambda <= 0:\n                break\n            half = (current_lambda + 1) // 2\n            arz_half = self.rng.standard_normal(size=(half, n))\n            arz = np.vstack([arz_half, -arz_half])[:current_lambda]\n\n            arx = np.zeros((current_lambda, n))\n            ary = np.zeros((current_lambda, n))\n\n            # Build an orthonormal basis from dir_mem (MiSCH-like) to bias samples\n            if len(dir_mem) > 0:\n                try:\n                    M = np.column_stack(list(dir_mem))\n                    Qm, _ = np.linalg.qr(M)\n                except Exception:\n                    Qm = None\n            else:\n                Qm = None\n\n            # sampling: diagonal + low-rank + memory-driven mixture + occasional cauchy/de\n            for k_idx in range(current_lambda):\n                z = arz[k_idx].copy()\n                # low-rank contribution\n                if self.k > 0 and U.size != 0:\n                    z_low = self.rng.standard_normal(self.k)\n                    low = U @ z_low\n                    beta = 0.7 * np.mean(D)\n                    y = (D * z) + beta * low\n                else:\n                    y = D * z\n\n                # bias toward memory subspace on some offspring (MiSCH mixing)\n                if Qm is not None and Qm.shape[1] > 0 and self.rng.random() < mem_mix:\n                    coeffs = self.rng.standard_normal(Qm.shape[1])\n                    mem_comp = Qm @ coeffs\n                    mem_comp = mem_comp / (np.linalg.norm(mem_comp) + 1e-20)\n                    iso = y / (np.linalg.norm(y) + 1e-20)\n                    mix = 0.65\n                    y = mix * mem_comp + (1.0 - mix) * iso\n                    # scale back by average D magnitude\n                    y = y * np.mean(D)\n\n                # occasional Cauchy heavy-tailed jump\n                if self.rng.random() < p_cauchy:\n                    r = self.rng.standard_cauchy() * cauchy_scale\n                    nz = np.linalg.norm(y) + 1e-20\n                    y = r * (y / nz) * np.mean(D)\n\n                # mirrored: odd indices are negated (we already constructed arz mirrored, but keep safety)\n                if mirrored and (k_idx % 2 == 1):\n                    y = -y\n\n                x = m + sigma * y\n\n                # archive DE mutation\n                if (self.rng.random() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                x = clip(x)\n                arx[k_idx] = x\n                ary[k_idx] = y\n\n            # evaluate offspring sequentially (budget-aware)\n            arfit = np.full(current_lambda, np.inf)\n            for k_idx in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[k_idx]\n                f, x_clipped = safe_eval(x)\n                if f is None:\n                    break\n                arfit[k_idx] = f\n                archive_X.append(x_clipped.copy()); archive_F.append(f)\n                # keep archive size modest\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n\n            # selection and recombination\n            valid_mask = np.isfinite(arfit)\n            if np.sum(valid_mask) == 0:\n                break\n            idx = np.argsort(arfit)\n            sel = idx[:min(mu, np.sum(valid_mask).astype(int))]\n            x_sel = arx[sel]\n            y_sel = ary[sel]\n            m_old = m.copy()\n            # recombine m\n            if len(sel) < len(weights):\n                w_sub = weights[:len(sel)]\n                w_sub = w_sub / np.sum(w_sub)\n                m = np.sum(w_sub[:, None] * x_sel, axis=0)\n                y_w = np.sum(w_sub[:, None] * y_sel, axis=0)\n            else:\n                m = np.sum(weights[:, None] * x_sel, axis=0)\n                y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # update evolution path (for occasional line-search) and ps for sigma\n            evo_step = m - m_old\n            evo_path = evo_decay * evo_path + (1.0 - evo_decay) * evo_step\n\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n\n            # path-length sigma adaptation\n            sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n            # multiplicative adjustment based on whether generation improved global best\n            gen_best = np.min(arfit)\n            if gen_best < f_best - 1e-15:\n                sigma = min(sigma * success_alpha, sigma_max)\n                # store successful displacement direction\n                disp = m - m_old\n                dn = np.linalg.norm(disp)\n                if dn > 1e-16:\n                    vec = disp / dn\n                    dir_mem.appendleft(vec.copy())\n                no_improve_gens = 0\n            else:\n                sigma = max(sigma * failure_alpha, sigma_min)\n                no_improve_gens += 1\n\n            # clamp sigma\n            sigma = np.clip(sigma, sigma_min, sigma_max)\n\n            # update diagonal scales D (exponential moving second moment of selected y)\n            y2 = np.sum((y_sel ** 2) * (weights[:len(y_sel)][:, None]), axis=0) if len(y_sel) > 0 else np.zeros(n)\n            c_d = 0.25\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n            # avoid degenerate tiny D\n            D = np.maximum(D, 1e-12)\n\n            # buffer successes for subspace learning\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank U occasionally using SVD on success_buffer\n            if (len(success_buffer) >= self.k) and (gen_count % subspace_update_every == 0):\n                try:\n                    Y = np.vstack(success_buffer).T  # n x m\n                    Y = Y - np.mean(Y, axis=1, keepdims=True)\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        if U.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U.shape[1]))\n                            U = np.hstack([U, pad])\n                except Exception:\n                    # keep previous U\n                    pass\n\n            # occasional cheap line-search along evo_path if it has some magnitude\n            if np.linalg.norm(evo_path) > 1e-12 and self.rng.random() < 0.3 and (budget - evals) >= 3:\n                f_m, _ = safe_eval(m)  # evaluate current mean (budget-aware)\n                if f_m is None:\n                    break\n                res = cheap_line_search(m, f_m, evo_path, init_step=sigma, max_evals=min(10, budget - evals))\n                if res is not None:\n                    f_ls, x_ls = res\n                    if f_ls is not None and f_ls < f_m - 1e-15:\n                        # accept line-search improvement\n                        m = x_ls.copy()\n                        # store direction into memory\n                        disp = x_ls - m\n                        if np.linalg.norm(disp) > 1e-16:\n                            dir_mem.appendleft((disp / (np.linalg.norm(disp))).copy())\n                        sigma = min(sigma * success_alpha, sigma_max)\n                        no_improve_gens = 0\n\n            # micro-polish on improvement\n            if (evals - last_improv_eval) <= 2:\n                # a few small gaussian probes around new mean\n                local_probes = min(6, max(1, int(np.ceil(np.log(1 + n)))))\n                for _ in range(local_probes):\n                    if evals >= budget:\n                        break\n                    dloc = self.rng.standard_normal(n)\n                    dloc = dloc / (np.linalg.norm(dloc) + 1e-20)\n                    a = self.rng.uniform(-0.5 * sigma, 0.5 * sigma)\n                    f_try, x_try = safe_eval(clip(m + a * dloc))\n                    if f_try is None:\n                        break\n                    if f_try < f_best - 1e-15:\n                        # record direction\n                        disp = x_try - m\n                        dn = np.linalg.norm(disp)\n                        if dn > 1e-16:\n                            dir_mem.appendleft((disp / dn).copy())\n                        m = x_try.copy()\n                        sigma = min(sigma * success_alpha, sigma_max)\n\n            # stagnation handling: if no improvement for many generations, diversify\n            if no_improve_gens >= stagnation_limit:\n                no_improve_gens = 0\n                # inflate sigma and nudge mean toward an archive point or random point\n                sigma = min(sigma * 2.0, sigma_max)\n                if len(archive_X) > 0 and self.rng.random() < 0.8:\n                    pick = self.rng.integers(len(archive_X))\n                    m = clip(0.7 * m + 0.3 * archive_X[pick])\n                else:\n                    m = clip(self.rng.uniform(lb, ub))\n                # clear some memory to avoid biasing into the same trap\n                keep = max(0, len(dir_mem) // 2)\n                dir_mem = deque(list(dir_mem)[:keep], maxlen=self.memory_size)\n                # clear success buffer for fresh subspace learning\n                success_buffer = []\n\n            # safety: if best is very small, stop early\n            if f_best <= 1e-14 or evals >= budget:\n                break\n\n        # final fallback if nothing found\n        if x_best is None:\n            x_best = np.clip(self.rng.uniform(lb, ub), lb, ub)\n            f_best = float(func(x_best)) if evals < budget else float(np.inf)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 87, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_mem = deque(maxlen=self.memory_size)", "error": "In the code, line 87, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_mem = deque(maxlen=self.memory_size)", "parent_ids": "7f1408a6-3d25-4e48-bd31-d2dacd374f35", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "03ee2b30-d20c-4a6e-8607-dc18a67a23ea", "fitness": 0.1511451856908027, "name": "MASE", "description": "MASE maintains a small ensemble of anchors (size ~ceil(sqrt(dim)) up to 6) each with a center m, trust radius r (init 0.6·span), an EMA covariance C (cov_lr=0.18) and per-dimension scales s, a rolling buffer of recent steps for learning, and a fixed random subspace projector for robust low‑dimensional surrogates. Proposals come from a diverse operator set — local anisotropic Gaussian, ellipse sampling from C, heavy‑tailed Lévy-like jumps (levy_scale=1.2), linear surrogate descent in a random subspace (requires buf_min≈6), and anchor crossover — with operator selection adapted by a soft bandit (op_logw, bandit_alpha=0.18). Anchors are allocated evaluations by a softmax on anchor log-weights, adapt their radii (shrink 0.85 on success, expand 1.08 on failure), update C via step outer-product EMA, accept improvements greedily or occasionally uphill via a decaying simulated‑annealing temperature, and perform occasional surrogate multi-probes and stagnation perturbations/restarts to maintain exploration. The algorithm enforces box bounds by clipping, periodically prunes or reinitializes worst anchors (often near the global best), and uses moderate buffer sizes and exploration hyperparameters to be robust across affine transforms and the Many Affine BBOB noiseless suite.", "code": "import numpy as np\n\nclass MASE:\n    \"\"\"\n    Multiscale Adaptive Subspace Ensemble (MASE)\n\n    Main ideas:\n    - Maintain a small ensemble of anchors (centers) each with its own trust radius r_k,\n      covariance estimate C_k (used for anisotropic sampling), per-dimension scales s_k,\n      and a small success buffer for learning low-dimensional surrogate directions.\n    - Proposals are generated by multiple operators: local Gaussian, anisotropic ellipse (from C_k),\n      heavy-tailed Levy-like jumps, surrogate-guided directional probes (linear surrogate in a random\n      subspace), and anchor crossover. Operator selection is driven by a soft bandit that adapts\n      operator preferences based on normalized reward.\n    - Anchors are sampled by another soft allocation mechanism to concentrate budget on promising regions.\n    - Covariance and radius are adapted from recent steps; surrogates are fit in small random subspaces\n      (sketching) to produce robust descent directions under affine transforms.\n    - Mild restarts/perturbations for stagnation. Works with bounds (the BBOB -5..5 default).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, n_anchors=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # ensemble size: scale with dim but small\n        if n_anchors is None:\n            self.n_anchors = min(6, max(2, int(np.ceil(np.sqrt(self.dim)))))\n        else:\n            self.n_anchors = int(n_anchors)\n        # operator set\n        self.operators = ['local', 'ellipse', 'levy', 'surrogate', 'crossover']\n        # hyperparameters\n        self.init_radius = 0.6  # relative to bounds width (will be scaled to actual)\n        self.min_r = 1e-8\n        self.max_r_factor = 6.0\n        self.radius_shrink = 0.85\n        self.radius_expand = 1.08\n        self.cov_lr = 0.18  # learning rate for covariance EMA\n        self.scale_clip = (1e-6, 1e3)\n        self.bandit_alpha = 0.18\n        self.surrogate_min = 6\n        self.k_sub = min(4, max(1, self.dim // 8 + 1))\n        self.buf_max = max(40, 6 * int(np.ceil(np.sqrt(self.dim))))\n        self.levy_scale = 1.2\n        self.crossover_prob = 0.12\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (the Many Affine BBOB uses [-5,5], but read from func)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        span = ub - lb\n        global_scale = np.mean(span)  # problem scale\n        r0 = self.init_radius * global_scale\n\n        # Anchor structure\n        anchors = []\n        for k in range(self.n_anchors):\n            m = self.rng.uniform(lb, ub)\n            anchor = {\n                'm': m.copy(),\n                'f': np.inf,\n                'r': r0 * (1.0 + 0.5 * (self.rng.random() - 0.5)),  # slightly different radii\n                'C': np.eye(n) * (0.3 * (global_scale ** 2)),  # covariance estimate (scale^2)\n                's': np.ones(n),\n                'buf_X': [],  # store dx = x - m\n                'buf_df': [],  # improvements\n                'P': None,   # random subspace projector (n x k_sub)\n                'last_improv_eval': 0,\n                'logw': 0.0,  # bandit score per anchor\n            }\n            # fixed subspace sketch for this anchor (for surrogate fitting)\n            ksub = min(self.k_sub, n)\n            # Gaussian random embedding (orthonormalized)\n            Q, _ = np.linalg.qr(self.rng.standard_normal((n, ksub)))\n            anchor['P'] = Q[:, :ksub]\n            anchors.append(anchor)\n\n        # operator bandit log-weights (global)\n        op_logw = np.zeros(len(self.operators))\n        op_count = np.zeros(len(self.operators), dtype=int)\n\n        evals = 0\n        # Evaluate initial anchors (one eval each if budget allows)\n        for anchor in anchors:\n            if evals >= budget:\n                break\n            anchor['m'] = np.clip(anchor['m'], lb, ub)\n            f = func(anchor['m'])\n            anchor['f'] = float(f)\n            evals += 1\n\n        # initialize global best\n        f_opt = np.inf\n        x_opt = None\n        for anchor in anchors:\n            if anchor['f'] < f_opt:\n                f_opt = anchor['f']; x_opt = anchor['m'].copy()\n\n        # helpers\n        def softmax(logw, temp=0.6):\n            w = np.exp((logw - np.max(logw)) / max(1e-12, temp))\n            return w / (np.sum(w) + 1e-20)\n\n        def sample_levy_scalar():\n            # Cauchy-ish tail limited\n            u = self.rng.random()\n            return np.tan(np.pi * (u - 0.5)) * 0.5\n\n        def fit_linear_surrogate(P, buf_X, buf_df, reg=1e-4):\n            # Fit df ≈ b^T z where z = P^T dx, returns b in full space (approx gradient in original coords)\n            if len(buf_X) < 3:\n                return None\n            Z = np.vstack([ (P.T @ x) for x in buf_X ])  # m x k_sub\n            y = np.array(buf_df)\n            # ridge solve for b_sub: (Z^T Z + reg I) b_sub = Z^T y\n            ksub = Z.shape[1]\n            ATA = Z.T @ Z\n            try:\n                b_sub = np.linalg.solve(ATA + reg * np.eye(ksub), Z.T @ y)\n            except np.linalg.LinAlgError:\n                b_sub = np.linalg.pinv(ATA + reg * np.eye(ksub)) @ (Z.T @ y)\n            # map back to full space\n            b_full = P @ b_sub\n            return b_full\n\n        gen = 0\n        # main loop\n        while evals < budget:\n            gen += 1\n            # choose an anchor to allocate next evaluation(s) to via softmax on anchor log-weights\n            anchor_logw = np.array([a['logw'] for a in anchors])\n            anchor_probs = softmax(anchor_logw, temp=0.8)\n            k_idx = self.rng.choice(len(anchors), p=anchor_probs)\n            anchor = anchors[k_idx]\n\n            # choose operator\n            op_probs = softmax(op_logw, temp=0.6)\n            op_idx = self.rng.choice(len(self.operators), p=op_probs)\n            op = self.operators[op_idx]\n\n            # create one candidate (we keep proposals one-by-one to track evals precisely)\n            m = anchor['m']\n            r = anchor['r']\n            C = anchor['C']\n            s = anchor['s']\n\n            x = m.copy()\n\n            if op == 'local':\n                # anisotropic element-wise normal scaled by s and r\n                z = self.rng.standard_normal(n) * s\n                x = m + (r * z)\n\n            elif op == 'ellipse':\n                # sample from multivariate normal with covariance proportional to C\n                # use chol if possible\n                cov = C + 1e-12 * np.eye(n)\n                try:\n                    L = np.linalg.cholesky(cov)\n                    z = L @ self.rng.standard_normal(n)\n                except np.linalg.LinAlgError:\n                    # fallback to diagonal\n                    z = np.sqrt(np.maximum(np.diag(cov), 1e-12)) * self.rng.standard_normal(n)\n                x = m + (1.5 * r) * z / (np.linalg.norm(z) + 1e-20)\n\n            elif op == 'levy':\n                # heavy tail jump: direction from anisotropic gaussian times Levy scalar\n                z = self.rng.standard_normal(n) * s\n                lev = sample_levy_scalar() * self.levy_scale\n                x = m + r * (1.5 + abs(lev)) * z\n\n            elif op == 'surrogate':\n                # fit linear surrogate in anchor's subspace and step along -grad estimate\n                if len(anchor['buf_X']) >= self.surrogate_min:\n                    g = fit_linear_surrogate(anchor['P'], anchor['buf_X'], anchor['buf_df'])\n                    if g is not None:\n                        ng = np.linalg.norm(g) + 1e-20\n                        step = - (r * 0.8) * (g / ng) * (0.5 + 0.5 * self.rng.random())\n                        x = m + step\n                    else:\n                        # fallback\n                        z = self.rng.standard_normal(n) * s\n                        x = m + r * z\n                else:\n                    # fallback gaussian\n                    z = self.rng.standard_normal(n) * s\n                    x = m + r * z\n\n            elif op == 'crossover':\n                # combine with another anchor and add small perturbation\n                if self.rng.random() < self.crossover_prob and len(anchors) > 1:\n                    other_idx = self.rng.choice([i for i in range(len(anchors)) if i != k_idx])\n                    other = anchors[other_idx]\n                    alpha = self.rng.beta(1.5, 1.5)\n                    x = alpha * m + (1 - alpha) * other['m']\n                    # small local perturbation\n                    x = x + 0.5 * r * self.rng.standard_normal(n) * s\n                else:\n                    # fallback local\n                    z = self.rng.standard_normal(n) * s\n                    x = m + r * z\n\n            # clip to bounds\n            x = np.clip(x, lb, ub)\n\n            # Evaluate candidate (single eval)\n            if evals >= budget:\n                break\n            f_x = func(x)\n            evals += 1\n\n            # store step and improvement relative to anchor center\n            dx = x - m\n            improvement = max(0.0, anchor['f'] - f_x)  # positive if candidate better than anchor center\n\n            # update anchor buffers\n            if len(anchor['buf_X']) >= self.buf_max:\n                anchor['buf_X'].pop(0); anchor['buf_df'].pop(0)\n            anchor['buf_X'].append(dx.copy())\n            anchor['buf_df'].append(improvement)\n\n            # update covariance via EMA on normalized step outer product\n            if np.linalg.norm(dx) > 1e-12:\n                xnorm = dx / (np.linalg.norm(dx) + 1e-20)\n                S = np.outer(xnorm, xnorm) * (np.linalg.norm(dx) ** 2)\n                anchor['C'] = (1 - self.cov_lr) * anchor['C'] + self.cov_lr * (S + 1e-8 * np.eye(n))\n\n            # update per-dimension scales (sqrt diag)\n            diag = np.maximum(np.diag(anchor['C']), 1e-12)\n            s_new = np.sqrt(diag) / (np.mean(np.sqrt(diag)) + 1e-12)\n            s_new = np.clip(s_new, *self.scale_clip)\n            anchor['s'] = s_new\n\n            # acceptance: if candidate better than anchor center, accept; else accept occasionally (simulated annealing style)\n            accepted_anchor_move = False\n            if f_x < anchor['f']:\n                anchor['m'] = x.copy()\n                anchor['f'] = float(f_x)\n                accepted_anchor_move = True\n                anchor['last_improv_eval'] = evals\n            else:\n                # small uphill acceptance decreasing with generation\n                delta = f_x - anchor['f']\n                T = max(1e-4, 0.2 * np.exp(-0.001 * gen))\n                if self.rng.random() < np.exp(-delta / (T + 1e-12)):\n                    # biased partial move toward x\n                    anchor['m'] = 0.9 * anchor['m'] + 0.1 * x\n                    accepted_anchor_move = True\n\n            # update anchor radius based on local success\n            if improvement > 1e-12:\n                anchor['r'] = max(self.min_r, anchor['r'] * self.radius_shrink)\n            else:\n                anchor['r'] = min(self.max_r_factor * global_scale, anchor['r'] * self.radius_expand)\n\n            # Update global best\n            if f_x < f_opt:\n                f_opt = float(f_x)\n                x_opt = x.copy()\n\n            # update operator bandit using reward normalized by step magnitude (to prefer efficient operators)\n            step_norm = np.linalg.norm(dx) + 1e-20\n            reward = (max(0.0, (anchor['f'] + improvement) - f_x)) / (1.0 + step_norm)\n            # Here anchor['f'] might have changed if accepted; use the positive improvement measured before anchor update\n            # update op_logw\n            # we treat small rewards; map to log space target\n            target = np.log1p(reward + 1e-12)\n            op_logw[op_idx] = (1 - self.bandit_alpha) * op_logw[op_idx] + self.bandit_alpha * target\n            op_count[op_idx] += 1\n\n            # update anchor allocation weight (so better anchors get more budget)\n            # use exponential moving average on logw\n            if improvement > 1e-12:\n                anchor['logw'] = 0.9 * anchor['logw'] + 0.1 * np.log1p(improvement)\n            else:\n                # small decay\n                anchor['logw'] *= 0.995\n\n            # occasional surrogate-driven multi-probe: evaluate a small batch around surrogate descent if promising\n            if (len(anchor['buf_X']) >= self.surrogate_min) and (self.rng.random() < 0.08) and (evals < budget):\n                # compute surrogate gradient\n                g = fit_linear_surrogate(anchor['P'], anchor['buf_X'], anchor['buf_df'])\n                if g is not None:\n                    ng = np.linalg.norm(g) + 1e-20\n                    base_dir = - g / ng\n                    # sample a small sequence along this direction\n                    alphas = [0.25, -0.25, 0.5]\n                    for a in alphas:\n                        if evals >= budget:\n                            break\n                        probe = np.clip(anchor['m'] + anchor['r'] * a * base_dir, lb, ub)\n                        fp = func(probe)\n                        evals += 1\n                        dxp = probe - anchor['m']\n                        improv_p = max(0.0, anchor['f'] - fp)\n                        if len(anchor['buf_X']) >= self.buf_max:\n                            anchor['buf_X'].pop(0); anchor['buf_df'].pop(0)\n                        anchor['buf_X'].append(dxp.copy()); anchor['buf_df'].append(improv_p)\n                        # update covariance lightly\n                        if np.linalg.norm(dxp) > 1e-12:\n                            xn = dxp / (np.linalg.norm(dxp) + 1e-20)\n                            anchor['C'] = (1 - 0.06) * anchor['C'] + 0.06 * (np.outer(xn, xn) * (np.linalg.norm(dxp) ** 2) + 1e-8 * np.eye(n))\n                        if fp < anchor['f']:\n                            anchor['m'] = probe.copy(); anchor['f'] = float(fp); anchor['last_improv_eval'] = evals\n                        if fp < f_opt:\n                            f_opt = float(fp); x_opt = probe.copy()\n\n            # stagnation handling per-anchor: if many evals since last improvement, perturb and inflate r\n            if (evals - anchor['last_improv_eval']) > max(40, 8 * n):\n                anchor['r'] = min(self.max_r_factor * global_scale, anchor['r'] * 2.0)\n                perturb = 0.08 * (global_scale) * self.rng.standard_normal(n)\n                anchor['m'] = np.clip(anchor['m'] + perturb, lb, ub)\n                anchor['C'] += 0.02 * np.eye(n)\n                anchor['last_improv_eval'] = evals\n\n            # global maintenance: prune or diversify worst anchors occasionally\n            if gen % max(50, 6 * n) == 0:\n                # find worst anchor and reinitialize near global best with perturbation\n                worst_idx = np.argmax([a['f'] for a in anchors])\n                if f_opt < np.inf and self.rng.random() < 0.8:\n                    anchors[worst_idx]['m'] = np.clip(x_opt + 0.6 * global_scale * self.rng.standard_normal(n), lb, ub)\n                    anchors[worst_idx]['r'] = 0.8 * global_scale\n                    anchors[worst_idx]['C'] = np.eye(n) * (0.4 * (global_scale ** 2))\n                    anchors[worst_idx]['buf_X'] = []; anchors[worst_idx]['buf_df'] = []\n                    anchors[worst_idx]['logw'] = 0.0\n                else:\n                    # random restart\n                    anchors[worst_idx]['m'] = self.rng.uniform(lb, ub)\n                    anchors[worst_idx]['r'] = r0\n                    anchors[worst_idx]['C'] = np.eye(n) * (0.3 * (global_scale ** 2))\n                    anchors[worst_idx]['buf_X'] = []; anchors[worst_idx]['buf_df'] = []\n                    anchors[worst_idx]['logw'] = 0.0\n\n            # safety: break if budget exhausted\n            if evals >= budget:\n                break\n\n        # ensure we return numpy array x_opt\n        if x_opt is None:\n            # fallback: evaluate a random point if nothing found (shouldn't happen)\n            x_opt = np.clip(self.rng.uniform(lb, ub), lb, ub)\n            f_opt = float(func(x_opt))\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm MASE scored 0.151 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "19996f2e-d5b3-4a5f-87ca-8ba1bc709672", "operator": null, "metadata": {"aucs": [0.08130752359020355, 0.13263070660348897, 0.207272301626102, 0.15004079352548683, 0.14322849721926978, 0.1799130719608918, 0.18276284192169656, 0.1602781949915446, 0.14880253482503658, 0.1252153906443063]}, "task_prompt": ""}
{"id": "99bd1eea-4135-432b-8f68-f5bcbc85fb1b", "fitness": "-inf", "name": "AESE", "description": "AESE maintains a central solution m with a multiplicative trust radius r, per-dimension adaptive scales s and a learned rotation/subspace R learned from a success buffer and PCA, while keeping a large archive for diversity. Candidate generation is an ensemble of operators (gaussian, heavy‑tailed Cauchy, DE-style difference, linear-model guided descent, and block shuffle) with mirror sampling and DE/shuffle probabilities; operator choice is governed by a softmax bandit with optimistic bonuses and fast learning (bandit_alpha=0.25, bandit_temp≈0.9). Adaptation mechanics include multiplicative trust-radius updates driven by observed success rate (r_eta=0.7 toward r_target_psucc=0.2), per-dimension scale updates via exponential moving second moments (c_s=0.25), PCA blending into R (gamma≈0.35), a ridge-regularized linear descent model, rank‑exponential recombination weights, and periodic 1‑D probes along the top PCA direction. Robustness features include conservative initial radius, clipping to bounds, budget-aware evaluations and annealed center acceptance, stagnation detection with radius inflation and mild restart, larger success buffer and archive trimming to stabilize learning.", "code": "import numpy as np\n\nclass AESE:\n    \"\"\"\n    Adaptive Ensemble Subspace Explorer (AESE)\n\n    Main ideas:\n    - Maintain center m, multiplicative trust radius r, per-dimension scales s, and a small learned subspace R.\n    - Use a compact archive and a success buffer for learning directions and a linear descent model.\n    - Candidate generation mixes Gaussian, Cauchy/Levy, DE-difference, linear-descent, and block-shuffle operators.\n    - Operator selection via softmax bandit; bandit updates use a larger learning rate and optimistic bonus.\n    - Trust radius adapts smoothly via multiplicative exponential rule (r <- r * exp(eta*(psucc - target))).\n    - Per-dimension scales updated by exponential moving second-moment.\n    - Subspace (R) updated by PCA on recent successes and blended into previous R more strongly.\n    - Occasional 1D line probes along top PCA direction. Mild restarts on stagnation.\n    - Designed for bounded [-5,5]^n search spaces but uses func.bounds to be general.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, pop_factor=2.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n\n        # population size: small, grows slowly with dim (different from original)\n        self.lambda_ = max(6, int(4 + 2.0 * np.sqrt(max(1, self.dim)) * pop_factor))\n        # block partitioning for block-shuffle operator (different heuristics)\n        max_block = max(1, int(np.ceil(np.sqrt(self.dim))))\n        self.blocks = []\n        sizes = [self.dim // max_block] * max_block\n        for i in range(self.dim % max_block):\n            sizes[i] += 1\n        idx = 0\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (support scalar or array)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center uniform in bounds\n        m = self.rng.uniform(lb, ub)\n        # initial trust radius (different constant than original)\n        r = 0.12 * np.mean(ub - lb)  # more conservative start\n        # per-dim scales\n        s = np.ones(n)\n\n        # initial identity subspace (rotation)\n        R = np.eye(n)\n\n        # memory structures\n        archive_X = []   # store evaluated points\n        archive_F = []\n        success_buf_X = []  # store dx = x - m (global coords)\n        success_buf_df = []  # positive improvements (f_center - f_x)\n\n        # operators and bandit state (different temperature & learning rate)\n        operators = ['gauss', 'cauchy', 'de', 'lin', 'shuffle']\n        k_ops = len(operators)\n        op_logw = np.zeros(k_ops)  # log-weights (neutral)\n        op_count = np.zeros(k_ops, dtype=int)\n\n        # parameters (changed from provided algorithm)\n        p_mirror = True\n        p_de = 0.25              # prob to apply DE when archive available\n        F_de = 0.8               # DE scaling\n        p_shuffle = 0.18         # shuffle probability\n        c_s = 0.25               # per-dim scale update momentum (higher than original)\n        lin_reg_lambda = 1e-3    # ridge reg for linear model (stronger)\n        buf_max = max(60, 10 * int(np.ceil(np.sqrt(n))))  # success buffer size (larger)\n        k_sub = min(4, n)        # number of PCA directions to keep (a bit larger)\n        line_search_every = max(10, int(3 * (n / 10 + 1)))  # more frequent\n        bandit_temp = 0.9         # softer softmax (different)\n        bandit_alpha = 0.25       # faster adaptation of log-weights\n        # trust-radius multiplicative adaptation parameters (different rule)\n        r_target_psucc = 0.20\n        r_eta = 0.7\n        # stagnation thresholds and behaviors\n        stagnation_thresh = max(80, 8 * n)\n        stagnation_r_inflate = 2.5\n\n        # bookkeeping\n        evals = 0\n        gen = 0\n\n        # evaluate initial center\n        m = np.clip(m, lb, ub)\n        f_center = func(m)\n        evals += 1\n        archive_X.append(m.copy()); archive_F.append(f_center)\n        f_opt = float(f_center); x_opt = m.copy()\n        last_improv = evals\n\n        # helper: softmax probabilities from log-weights\n        def op_probs(logw, temp=1.0):\n            mx = np.max(logw)\n            ex = np.exp((logw - mx) / max(1e-12, temp))\n            return ex / (np.sum(ex) + 1e-20)\n\n        # helper: Cauchy-like heavy tail sampled direction (different scale)\n        def sample_cauchy_dir(z):\n            # scalar Cauchy sample limited by tanh to avoid extreme leaps\n            u = self.rng.random()\n            scalar = np.tan(np.pi * (u - 0.5)) * 0.8\n            normz = np.linalg.norm(z) + 1e-20\n            return (z / normz) * scalar\n\n        # helper: fit linear model df ≈ g^T dx  (ridge regression)\n        def fit_linear_model(buf_X, buf_df):\n            if len(buf_X) < 3:\n                return None\n            A = np.vstack(buf_X)  # rows dx (global coords)\n            y = np.array(buf_df)\n            # Solve (A^T A + λI) g = A^T y\n            ATA = A.T @ A\n            reg = lin_reg_lambda * np.eye(n)\n            try:\n                g = np.linalg.solve(ATA + reg, A.T @ y)\n            except np.linalg.LinAlgError:\n                g = np.linalg.pinv(ATA + reg) @ (A.T @ y)\n            return g\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            lam = max(1, lam)\n\n            # snapshot baseline center to compute rewards consistently this generation\n            baseline_center_val = f_center\n\n            # operator probabilities via softmax\n            probs = op_probs(op_logw, temp=bandit_temp)\n\n            Xcand = np.zeros((lam, n))\n            Fcand = np.full(lam, np.inf)\n            cand_op_idx = np.zeros(lam, dtype=int)\n            cand_step_local = np.zeros((lam, n))  # dx = x - m stored (global coords)\n\n            # pre-generate gaussian normals (to reuse in Cauchy direction base)\n            Z = self.rng.standard_normal((lam, n))\n\n            for i in range(lam):\n                # choose operator\n                op_idx = self.rng.choice(k_ops, p=probs)\n                cand_op_idx[i] = op_idx\n                op = operators[op_idx]\n\n                z = Z[i].copy()\n                if p_mirror and (i % 2 == 1):\n                    z = -z\n\n                # local scaled noise (in rotated coords)\n                y_local = s * z\n                y_global = R.dot(y_local)\n\n                # default candidate\n                # slight shrink factor to gaussian base to be conservative\n                x = m + r * 0.9 * y_global\n\n                if op == 'gauss':\n                    # standard gaussian candidate (already set)\n                    pass\n\n                elif op == 'cauchy':\n                    # heavy-tailed jump guided by direction z but with larger multiplier\n                    cdir = sample_cauchy_dir(z)\n                    yg = R.dot(s * cdir)\n                    x = m + r * 2.8 * yg  # larger multiplier than gaussian\n\n                elif op == 'de':\n                    if len(archive_X) >= 3 and self.rng.random() < p_de:\n                        # pick two archive indices distinct from current center if possible\n                        idxs = self.rng.choice(len(archive_X), size=2, replace=False)\n                        de_mut = F_de * (archive_X[idxs[0]] - archive_X[idxs[1]])\n                        x = x + de_mut  # additive DE-style perturbation\n\n                elif op == 'lin':\n                    # guided descent using local linear model if available\n                    if len(success_buf_X) >= 4:\n                        g = fit_linear_model(success_buf_X, success_buf_df)\n                        if g is not None:\n                            ng = np.linalg.norm(g) + 1e-20\n                            # move along -g scaled by r and a random step fraction\n                            frac = 0.3 + 0.7 * self.rng.random()\n                            step = - (r * (g / ng)) * frac\n                            x = m + step\n                    # else fallback to gaussian\n\n                elif op == 'shuffle':\n                    # random small block shuffle to explore combinatorial interactions\n                    if len(self.blocks) > 0 and self.rng.random() < p_shuffle:\n                        # pick a random block size each time (up to sqrt(n))\n                        block_idx = self.rng.integers(0, len(self.blocks))\n                        b = self.blocks[block_idx]\n                        if len(b) > 1:\n                            perm = self.rng.permutation(len(b))\n                            x_block = x[b].copy()\n                            x[b] = x_block[perm]\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                Xcand[i] = x\n                cand_step_local[i] = (x - m)\n\n            # Evaluate candidates carefully within budget\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fcand[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy(); last_improv = evals\n\n            # selection: top mu (half) candidates\n            mu = max(1, lam // 2)\n            idx_sorted = np.argsort(Fcand)\n            sel_idx = idx_sorted[:mu]\n            X_sel = Xcand[sel_idx]\n            F_sel = Fcand[sel_idx]\n\n            # recombination: exponential rank-based weights (different equation)\n            ranks = np.arange(1, mu + 1)\n            # sharper emphasis on best\n            weights = np.exp(- (ranks - 1) / max(0.5, mu * 0.6))\n            weights = weights / np.sum(weights)\n            m_new = np.sum(weights[:, None] * X_sel, axis=0)\n            m_new = np.clip(m_new, lb, ub)\n\n            # center acceptance: evaluate m_new if budget allows, with different annealing schedule\n            accepted_center_move = False\n            if evals < budget:\n                fm_new = func(m_new)\n                evals += 1\n                archive_X.append(m_new.copy()); archive_F.append(fm_new)\n                if fm_new < f_center:\n                    accepted_center_move = True\n                    f_center = fm_new\n                    m = m_new.copy()\n                else:\n                    delta = fm_new - f_center\n                    T = max(0.01, 0.25 * np.exp(-0.002 * gen))\n                    if self.rng.random() < np.exp(-delta / (T + 1e-12)):\n                        accepted_center_move = True\n                        f_center = fm_new\n                        m = m_new.copy()\n                    else:\n                        # small biased move toward m_new occasionally\n                        if self.rng.random() < 0.04:\n                            m = 0.88 * m + 0.12 * m_new\n            else:\n                # no budget left for center evaluation: use mean of selected as proxy\n                mean_sel = np.mean(F_sel)\n                if mean_sel < f_center:\n                    accepted_center_move = True\n                    m = m_new.copy()\n\n            # Update trust radius multiplicatively based on success fraction (different equation)\n            improvements = np.sum(Fcand < f_center - 1e-12)\n            psucc = improvements / max(1.0, lam)\n            # multiplicative smooth adaptation: r <- r * exp(eta*(psucc - target))\n            r *= float(np.exp(r_eta * (psucc - r_target_psucc)))\n            # clamp r\n            r = float(np.clip(r, 1e-8, 5.0 * np.mean(ub - lb)))\n\n            # Update per-dimension scales s via exponential moving second moment in rotated coords\n            if r > 0:\n                # recompute selected local coordinates relative to possibly-updated m\n                Ylocal_sel = (R.T @ ((X_sel - m).T)).T / (r + 1e-20)\n                y2 = np.sum(weights[:, None] * (Ylocal_sel ** 2), axis=0)\n                s2 = (1.0 - c_s) * (s ** 2) + c_s * (y2 + 1e-20)\n                s = np.sqrt(s2)\n                s = np.clip(s, 1e-6, 1e3)\n\n            # Update success buffer: store best half of the selected candidates (different choice)\n            h = max(1, mu // 2)\n            for j in range(h):\n                idx_j = sel_idx[j]\n                dx = (Xcand[idx_j] - m)  # relative to possibly-updated m\n                df = max(0.0, (baseline_center_val - Fcand[idx_j]))  # improvement vs baseline\n                if len(success_buf_X) >= buf_max:\n                    success_buf_X.pop(0); success_buf_df.pop(0)\n                success_buf_X.append(dx.copy())\n                success_buf_df.append(df)\n\n            # PCA-based subspace update blended more strongly\n            if len(success_buf_X) >= min(6, n):\n                M = np.vstack(success_buf_X)\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                try:\n                    C = (Mc.T @ Mc) / max(1, Mc.shape[0] - 1)\n                    eigvals, eigvecs = np.linalg.eigh(C)\n                    order = np.argsort(eigvals)[::-1]\n                    top = eigvecs[:, order[:k_sub]]\n                    # blend top into R with stronger mixing\n                    gamma = 0.35\n                    # orthonormalize top\n                    Qt, _ = np.linalg.qr(top)\n                    # ensure R columns orthonormal\n                    Qr, _ = np.linalg.qr(R)\n                    R = (1 - gamma) * Qr + gamma * Qt[:, :n]\n                    Qr2, _ = np.linalg.qr(R)\n                    R = Qr2[:, :n]\n                except np.linalg.LinAlgError:\n                    # mild perturbation fallback\n                    R = R + 0.02 * self.rng.standard_normal(R.shape)\n                    Qr, _ = np.linalg.qr(R)\n                    R = Qr[:, :n]\n\n            # Bandit rewards and log-weight updates (different reward scaling and optimism)\n            op_rewards = np.zeros(k_ops)\n            op_seen = np.zeros(k_ops)\n            baseline = baseline_center_val\n            for i in range(lam):\n                opi = cand_op_idx[i]\n                fi = Fcand[i]\n                if not np.isfinite(fi):\n                    continue\n                improv = max(0.0, baseline - fi)\n                step_norm = np.linalg.norm(cand_step_local[i]) + 1e-20\n                # different reward scaling: sqrt scaling to favor moderate steps\n                reward = improv / np.sqrt(1.0 + step_norm)\n                op_rewards[opi] += reward\n                op_seen[opi] += 1\n            for oi in range(k_ops):\n                if op_seen[oi] > 0:\n                    avg_reward = op_rewards[oi] / op_seen[oi]\n                    # optimistic bonus for rarely-used operators\n                    bonus = 1.0 / np.sqrt(1.0 + op_count[oi])\n                    target = np.log1p(avg_reward + 1e-12) + 0.1 * bonus\n                    op_logw[oi] = (1 - bandit_alpha) * op_logw[oi] + bandit_alpha * target\n                    op_count[oi] += int(op_seen[oi])\n\n            # Periodic line probes along top PCA direction (different set of alphas)\n            gen += 1\n            if (gen % line_search_every == 0) and (evals < budget):\n                top_d = R[:, 0]\n                alphas = [0.0, 0.15, -0.15, 0.35, -0.35, 0.7]\n                for a in alphas:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + r * a * top_d, lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = float(fp); x_opt = probe.copy(); last_improv = evals\n                    if fp < f_center:\n                        f_center = fp; m = probe.copy()\n\n            # stagnation detection and mild restart/perturbation (different policy)\n            if (evals - last_improv) > stagnation_thresh:\n                # inflate radius considerably and perturb center around best found\n                r *= stagnation_r_inflate\n                # reposition center near best found with moderate gaussian noise\n                m = np.clip(x_opt + 0.08 * r * self.rng.standard_normal(n), lb, ub)\n                # perturb scales and rotation modestly\n                s = s * (1.0 + 0.18 * (self.rng.random(n) - 0.5))\n                R = R + 0.05 * self.rng.standard_normal(R.shape)\n                Qr, _ = np.linalg.qr(R)\n                R = Qr[:, :n]\n                last_improv = evals\n\n            # trim archive\n            if len(archive_X) > 4000:\n                archive_X = archive_X[-4000:]\n                archive_F = archive_F[-4000:]\n\n            # ensure center within bounds\n            m = np.clip(m, lb, ub)\n\n            # safety break\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 313, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (10,10) (10,4) \nOn line: R = (1 - gamma) * Qr + gamma * Qt[:, :n]", "error": "In the code, line 313, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (10,10) (10,4) \nOn line: R = (1 - gamma) * Qr + gamma * Qt[:, :n]", "parent_ids": "19996f2e-d5b3-4a5f-87ca-8ba1bc709672", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "0a50e8f0-154e-4372-86c9-5fd01986fb15", "fitness": 0.2594335799442909, "name": "MGTS", "description": "The algorithm is a trust-region style population search around a maintained center c with an adaptive trust radius rho (initialized at 0.35 of the range), per-dimension scales sigma and a learned orthonormal basis P, backed by an archive of evaluated points and a buffer of recent successful steps for manifold/PCA learning (success buffer size ~max(60, 8*ceil(sqrt(n)))). It uses a small set of stochastic operators (gauss, cauchy, de, model, subspace, coord) with Thompson-sampling-like operator selection whose rewards are tracked online via Welford statistics to balance exploration and exploitation, and generates mirrored batched candidates (population default pop=8) including DE-style differences, heavy-tailed cauchy jumps, coordinate blocks, model-derived descent, and subspace sampling. Local guidance comes from a ridge-fitted local linear model (least-squares gradient) and periodic PCA on recent successful steps to define low-dimensional search manifolds, while candidate recombination uses soft fitness-weighted averaging and simulated-annealing acceptance; occasional finite-difference probes along top principal directions refine the center. Adaptation and robustness are provided by success-rate-driven rho shrink/expand, per-dimension sigma updates from selected-step variances, gentle restarts by re-centering to good archive points when stagnating, budget-respecting evaluations, and archive trimming for memory control.", "code": "import numpy as np\n\nclass MGTS:\n    \"\"\"\n    Manifold-Guided Thompson Search (MGTS)\n\n    Key ideas:\n    - Maintain a center `c`, trust radius `rho`, and per-dimension scales `sigma`.\n    - Keep an archive of evaluated points and a buffer of recent successful steps.\n    - Use several stochastic operators to propose candidates. Operator selection is done\n      by a Thompson-sampling-like draw from Gaussian posteriors on operator rewards.\n    - Fit a local linear model (ridge) from nearby archive points to get a descent direction.\n    - Learn a low-dimensional manifold (PCA) from recent successful steps and sample inside that manifold.\n    - Update trust radius and scales adaptively from success statistics. Mild restarts when stagnating.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, pop=8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.pop = max(4, int(pop))\n        # operator list\n        self.operators = ['gauss', 'cauchy', 'de', 'model', 'subspace', 'coord']\n        # Thompson-like posterior stats (Welford) for operator rewards\n        k = len(self.operators)\n        self.op_n = np.zeros(k, dtype=int)\n        self.op_mu = np.zeros(k, dtype=float)\n        self.op_M2 = np.zeros(k, dtype=float)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n        rng = self.rng\n\n        # bounds: many-BBOB uses [-5,5], but allow func.bounds if present\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initial center and evaluation\n        c = rng.uniform(lb, ub)\n        rho = 0.35 * np.mean(ub - lb)  # trust radius\n        sigma = np.ones(n)             # per-dim scale modifiers\n        # principal directions (identity initially)\n        P = np.eye(n)\n\n        # archives and buffers\n        archive_X = [c.copy()]\n        archive_F = []\n        f_c = func(c); evals = 1\n        archive_F.append(f_c)\n        f_best = float(f_c); x_best = c.copy()\n        success_steps = []   # store dx = x - center for PCA and modelling\n        max_success_buf = max(60, 8 * int(np.ceil(np.sqrt(n))))\n\n        # other params\n        pop = self.pop\n        k_sub = min(3, n)\n        gen = 0\n        last_improv = evals\n        stagn_thresh = max(40, 8 * n)\n\n        # helper: sample from posterior for operator (Thompson-like)\n        def sample_op_index():\n            # for each op, sample Normal(op_mu, var_est / max(1,op_n) + eps)\n            kops = len(self.operators)\n            samples = np.empty(kops)\n            for i in range(kops):\n                if self.op_n[i] <= 1:\n                    # encourage exploration\n                    samples[i] = rng.normal(0.0, 1.0)\n                else:\n                    var = (self.op_M2[i] / max(1, self.op_n[i] - 1)) if (self.op_n[i] > 1) else 1.0\n                    sigma_post = np.sqrt(var / max(1, self.op_n[i]) + 1e-6)\n                    samples[i] = rng.normal(self.op_mu[i], sigma_post)\n            # with small probability pick a random op\n            if rng.random() < 0.08:\n                return rng.integers(0, kops)\n            return int(np.argmax(samples))\n\n        # helper: Welford update\n        def update_op_stats(idx, reward):\n            self.op_n[idx] += 1\n            n_i = self.op_n[idx]\n            delta = reward - self.op_mu[idx]\n            self.op_mu[idx] += delta / n_i\n            delta2 = reward - self.op_mu[idx]\n            self.op_M2[idx] += delta * delta2\n\n        # helper: local linear model fit (ridge) using k nearest archive points\n        def local_linear_direction(center, archive_X, archive_F, k_neigh=30, lam=1e-4):\n            if len(archive_X) < 3:\n                return None\n            X = np.vstack(archive_X)\n            F = np.array(archive_F)\n            # distances to center\n            dists = np.linalg.norm(X - center, axis=1)\n            idx = np.argsort(dists)[:min(k_neigh, len(X))]\n            Xn = X[idx]\n            Fn = F[idx]\n            # avoid tiny neighborhood: require at least two distinct points\n            if Xn.shape[0] < 2:\n                return None\n            # We fit a linear model: F ≈ a + g^T (x - center). Solve for g by least squares on centered coordinates\n            A = (Xn - center)\n            y = (Fn - np.mean(Fn))\n            ATA = A.T @ A\n            reg = lam * np.eye(n)\n            try:\n                g = np.linalg.solve(ATA + reg, A.T @ y)\n            except np.linalg.LinAlgError:\n                g = np.linalg.pinv(ATA + reg) @ (A.T @ y)\n            # we want descent direction; if gradient small return None\n            if np.linalg.norm(g) < 1e-12:\n                return None\n            return -g  # move opposite gradient\n\n        # main loop: propose batches until budget used\n        while evals < budget:\n            remaining = budget - evals\n            batch = min(pop, remaining)\n            gen += 1\n\n            # snapshot center and f_c for reward baseline\n            baseline_center = c.copy()\n            baseline_f = f_c\n\n            Xcand = np.zeros((batch, n))\n            Fcand = np.full(batch, np.inf)\n            cand_op_idx = np.zeros(batch, dtype=int)\n            cand_step = np.zeros((batch, n))  # x - center (baseline_center)\n            # Pre-generate gaussian noises for paired mirroring\n            Z = rng.standard_normal((batch, n))\n            for i in range(batch):\n                op_i = sample_op_index()\n                cand_op_idx[i] = op_i\n                op = self.operators[op_i]\n                z = Z[i].copy()\n                # mirrored pairing: alternate signs to reduce estimator variance\n                if i % 2 == 1:\n                    z = -z\n\n                # default proposal: center + rho * (P @ (sigma * z))\n                local = sigma * z\n                x_proposed = baseline_center + rho * (P @ local)\n\n                if op == 'gauss':\n                    # small gaussian perturbation (already encoded)\n                    pass\n\n                elif op == 'cauchy':\n                    # heavy-tailed step along local direction\n                    cauchy_scalar = np.tan(np.pi * (rng.random() - 0.5)) * 0.7\n                    x_proposed = baseline_center + rho * (P @ (sigma * z)) * (1.0 + 1.5 * cauchy_scalar)\n\n                elif op == 'de':\n                    # differential mutation from archive pairs (if available)\n                    if len(archive_X) >= 2 and rng.random() < 0.9:\n                        # pick two distinct archive indices\n                        i1, i2 = rng.choice(len(archive_X), size=2, replace=False)\n                        de_vec = 0.6 * (archive_X[i1] - archive_X[i2])\n                        # mix with local\n                        mix = 0.5 + 0.5 * rng.random()\n                        x_proposed = baseline_center + mix * (rho * (P @ (sigma * z))) + (1.0 - mix) * de_vec\n\n                elif op == 'model':\n                    # model-based descent from local linear model\n                    gdir = local_linear_direction(baseline_center, archive_X, archive_F, k_neigh=28, lam=1e-4)\n                    if gdir is not None:\n                        # step length adaptive: prefer lengths proportional to rho but modulated by estimated improvement scale\n                        ng = np.linalg.norm(gdir) + 1e-20\n                        step_len = rho * (0.2 + 0.8 * rng.random())\n                        step = (gdir / ng) * (-step_len)  # gdir is -grad, so -gdir is ascent; we already returned -grad, so negative yields descent\n                        x_proposed = baseline_center + step\n                    else:\n                        # fallback to gaussian\n                        pass\n\n                elif op == 'subspace':\n                    # sample inside learned subspace spanned by top k_sub principal directions\n                    if len(success_steps) >= max(6, k_sub):\n                        # create small gaussian in subspace\n                        coeffs = rng.normal(0, 0.8, size=k_sub)\n                        subvec = np.zeros(n)\n                        for j in range(k_sub):\n                            subvec += coeffs[j] * P[:, j]\n                        # small orthogonal jitter\n                        ort = rng.normal(0, 0.1, size=n)\n                        x_proposed = baseline_center + rho * (0.8 * subvec + 0.2 * ort * sigma)\n                    else:\n                        # fallback\n                        pass\n\n                elif op == 'coord':\n                    # perturb a random block or coordinate\n                    if n > 1 and rng.random() < 0.6:\n                        # choose a block size sqrt(n)\n                        bsize = max(1, int(np.ceil(np.sqrt(n))))\n                        start = rng.integers(0, n)\n                        idxs = [(start + j) % n for j in range(bsize)]\n                        x_proposed = baseline_center.copy()\n                        x_proposed[idxs] += rho * (sigma[idxs] * rng.normal(0, 1, size=len(idxs)))\n                    else:\n                        # single coordinate\n                        j = rng.integers(0, n)\n                        x_proposed = baseline_center.copy()\n                        x_proposed[j] += rho * sigma[j] * rng.normal(0, 1)\n\n                # clip to bounds\n                x_proposed = np.clip(x_proposed, lb, ub)\n                Xcand[i, :] = x_proposed\n                cand_step[i, :] = x_proposed - baseline_center\n\n            # Evaluate candidates sequentially until budget\n            for i in range(batch):\n                if evals >= budget:\n                    break\n                x_i = Xcand[i]\n                f_i = func(x_i)\n                evals += 1\n                Fcand[i] = f_i\n                archive_X.append(x_i.copy()); archive_F.append(f_i)\n                # update global best\n                if f_i < f_best:\n                    f_best = float(f_i); x_best = x_i.copy(); last_improv = evals\n\n            # compute rewards for operators based on baseline_center snapshot\n            # reward = positive improvement relative to baseline_f scaled by step size penalty\n            # (we don't change baseline_f until center acceptance)\n            op_rewards_sum = np.zeros(len(self.operators))\n            op_seen = np.zeros(len(self.operators), dtype=int)\n            for i in range(batch):\n                opi = cand_op_idx[i]\n                if np.isfinite(Fcand[i]):\n                    improv = max(0.0, baseline_f - Fcand[i])\n                    step_norm = np.linalg.norm(cand_step[i]) + 1e-12\n                    reward = improv / (1.0 + 0.5 * step_norm / (rho + 1e-12))\n                else:\n                    reward = 0.0\n                op_rewards_sum[opi] += reward\n                op_seen[opi] += 1\n\n            # update operator posteriors (Welford)\n            for oi in range(len(self.operators)):\n                if op_seen[oi] > 0:\n                    avg_r = op_rewards_sum[oi] / float(op_seen[oi])\n                    # scale reward to moderate range and update\n                    update_op_stats(oi, avg_r)\n\n            # selection and recombination: pick top half (mu) by Fcand\n            valid_idx = np.where(np.isfinite(Fcand))[0]\n            if valid_idx.size == 0:\n                # no valid candidates in this batch, continue\n                # small random jitter to center\n                c = np.clip(c + 0.02 * rho * rng.normal(size=n), lb, ub)\n                continue\n            mu = max(1, len(valid_idx) // 2)\n            idx_sorted = np.argsort(Fcand)\n            sel_idx = idx_sorted[:mu]\n            X_sel = Xcand[sel_idx]\n            F_sel = Fcand[sel_idx]\n\n            # weighted recombination: soft-weights from fitness differences\n            # lower F -> higher weight; use ranking-based tempering\n            ranks = np.arange(1, mu + 1)\n            w = np.exp(-0.5 * (ranks - 1))  # geometric preference for top ranks\n            w = w / (np.sum(w) + 1e-20)\n            c_new = np.sum(w[:, None] * X_sel, axis=0)\n            c_new = np.clip(c_new, lb, ub)\n\n            # Evaluate c_new if we have budget; otherwise use surrogate decision\n            accepted = False\n            if evals < budget:\n                f_c_new = func(c_new)\n                evals += 1\n                archive_X.append(c_new.copy()); archive_F.append(f_c_new)\n                if f_c_new < f_c:\n                    accepted = True\n                    c = c_new.copy(); f_c = f_c_new\n                else:\n                    # simulated annealing style acceptance with temperature decaying with generation\n                    T = max(1e-3, 0.4 * np.exp(-0.002 * gen))\n                    delta = f_c_new - f_c\n                    if rng.random() < np.exp(-delta / (T + 1e-12)):\n                        accepted = True\n                        c = c_new.copy(); f_c = f_c_new\n                    else:\n                        # occasionally take a partial biased step toward c_new\n                        if rng.random() < 0.04:\n                            c = np.clip(0.9 * c + 0.1 * c_new, lb, ub)\n            else:\n                # no budget left for center evaluation: accept if mean of selected is better than current\n                if np.mean(F_sel) < f_c:\n                    c = c_new.copy()\n                    accepted = True\n\n            # success statistic: count strict improvements among candidates relative to previous center\n            n_improv = int(np.sum(Fcand < baseline_f - 1e-12))\n            ps = n_improv / max(1, batch)\n\n            # adapt trust radius\n            if ps > 0.22:\n                rho = rho * (0.92 - 0.02 * rng.random())  # shrink a bit\n            elif ps < 0.06:\n                rho = rho * (1.12 + 0.03 * rng.random())  # expand to explore\n            # clamp rho to problem scale\n            rho = float(np.clip(rho, 1e-8, 4.0 * np.mean(ub - lb)))\n\n            # update per-dimension sigmas using selected steps projected into P basis\n            try:\n                if rho > 0:\n                    Y_sel_local = (P.T @ ((X_sel - c).T)).T / (rho + 1e-20)\n                    y2 = np.sum(w[:, None] * (Y_sel_local ** 2), axis=0)\n                    cs = 0.14\n                    sigma2 = (1.0 - cs) * (sigma ** 2) + cs * (y2 + 1e-20)\n                    sigma = np.sqrt(np.clip(sigma2, 1e-8, 1e6))\n            except Exception:\n                pass\n\n            # update success buffer with best fraction of selected (store dx relative to new center)\n            h = max(1, mu // 2)\n            for j in range(h):\n                idx_j = sel_idx[j]\n                dx = (Xcand[idx_j] - c)  # relative to possibly updated center\n                df = max(0.0, f_c - Fcand[idx_j])\n                if len(success_steps) >= max_success_buf:\n                    success_steps.pop(0)\n                success_steps.append(dx.copy())\n\n            # rebuild local manifold (P) via PCA on success_steps occasionally\n            if len(success_steps) >= max(6, k_sub) and (gen % 3 == 0):\n                M = np.vstack(success_steps)\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                # use economy SVD\n                try:\n                    U, Svals, Vt = np.linalg.svd(Mc, full_matrices=False)\n                    # top-k_sub directions\n                    top = Vt[:k_sub].T  # n x k_sub\n                    # gently blend into P's first k_sub columns by orthonormalized rotation\n                    A = P[:, :k_sub]\n                    Msmall = top.T @ A\n                    U_m, _, Vt_m = np.linalg.svd(Msmall, full_matrices=False)\n                    Qsmall = U_m @ Vt_m\n                    P[:, :k_sub] = top @ Qsmall\n                    # orthonormalize full P\n                    Qr, _ = np.linalg.qr(P)\n                    P = Qr[:, :n]\n                except np.linalg.LinAlgError:\n                    # small random perturbation fallback\n                    P = P + 0.02 * rng.standard_normal(P.shape)\n                    Qr, _ = np.linalg.qr(P)\n                    P = Qr[:, :n]\n\n            # occasionally perform a cheap directional finite-difference gradient probe along top P[:,0]\n            if (gen % max(10, 5 + n//10) == 0) and (evals + 2 < budget):\n                d = P[:, 0]\n                step = 0.25 * rho\n                p1 = np.clip(c + step * d, lb, ub)\n                p2 = np.clip(c - step * d, lb, ub)\n                f1 = func(p1); evals += 1\n                archive_X.append(p1.copy()); archive_F.append(f1)\n                if f1 < f_best:\n                    f_best = float(f1); x_best = p1.copy(); last_improv = evals\n                f2 = func(p2); evals += 1\n                archive_X.append(p2.copy()); archive_F.append(f2)\n                if f2 < f_best:\n                    f_best = float(f2); x_best = p2.copy(); last_improv = evals\n                # if one is better than current center, adopt it\n                if f1 < f_c or f2 < f_c:\n                    if f1 < f2:\n                        c = p1.copy(); f_c = f1\n                    else:\n                        c = p2.copy(); f_c = f2\n\n            # stagnation detection: if many evals without improvement, gentle restart with cultural memory\n            if (evals - last_improv) > stagn_thresh:\n                # pick a promising archived point (top quantile) and re-center with jitter\n                if len(archive_X) > 5:\n                    idxs = np.argsort(archive_F)\n                    topk = max(2, min(20, len(idxs)//10))\n                    pick = idxs[rng.integers(0, topk)]\n                    c = np.clip(archive_X[pick] + 0.08 * rho * rng.normal(size=n), lb, ub)\n                    # enlarge radius to escape basin\n                    rho *= 1.8\n                    # slightly randomize P and sigma\n                    P = P + 0.06 * rng.standard_normal(P.shape)\n                    Qr, _ = np.linalg.qr(P)\n                    P = Qr[:, :n]\n                    sigma = sigma * (1.0 + 0.25 * (rng.random(n) - 0.5))\n                    last_improv = evals\n                    # evaluate new center if budget allows\n                    if evals < budget:\n                        f_c = func(c); evals += 1\n                        archive_X.append(c.copy()); archive_F.append(f_c)\n                        if f_c < f_best:\n                            f_best = float(f_c); x_best = c.copy(); last_improv = evals\n\n            # keep archive trimmed\n            if len(archive_X) > 8000:\n                archive_X = archive_X[-8000:]\n                archive_F = archive_F[-8000:]\n\n            # ensure center within bounds\n            c = np.clip(c, lb, ub)\n\n            # safety: break if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm MGTS scored 0.259 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "19996f2e-d5b3-4a5f-87ca-8ba1bc709672", "operator": null, "metadata": {"aucs": [0.12005280052191591, 0.21548934769753503, 0.30774919869228734, 0.3630622677993739, 0.2353046561550285, 0.3544393914916155, 0.2488627656764698, 0.3405276968164114, 0.23151206992458562, 0.17733560466768594]}, "task_prompt": ""}
{"id": "3e29d308-29b0-4a2b-9d16-8e6068e82e1b", "fitness": 0.547926065376162, "name": "ASES", "description": "The algorithm maintains a single search center m with a trust radius r, adaptive per-dimension scales s, and a learned low-rank PCA subspace R to produce anisotropic, rotated steps and to guide periodic 1‑D probes along leading directions. Each generation it samples a compact population (lambda = max(6, 4+2*ceil(sqrt(dim))) ) from a mixture of operators — Gaussian, heavy‑tailed Student‑t jumps, DE-style differences from an archive, a linear surrogate descent fitted on a success buffer, and small block permutations — with operator selection governed by a softmax bandit (temperature ~0.4, log‑weight learning rate 0.25) that is updated using average reward per operator. Promising candidates are recombined by a robust blend (0.7 weighted mean + 0.3 median) into a trial center accepted by a simulated-annealing-like rule; r is adapted from the observed success rate (psucc>0.3 shrink, psucc<0.1 expand) and per-dimension scales s are updated via second-moment smoothing (c_s=0.25). The algorithm keeps a small archive and success buffer (trimmed), fits a ridge linear surrogate from recent successes, gently updates the PCA subspace via eigen-decomposition with mixing alpha_sub=0.2, performs periodic line searches along the top eigenvector, and applies larger perturbations (inflate r, randomize s and R, reshuffle blocks) on stagnation to reintroduce exploration.", "code": "import numpy as np\n\nclass ASES:\n    \"\"\"\n    Adaptive Subspace Ensemble Search (ASES)\n\n    Short description:\n      - Maintain a single center m, trust radius r, per-dimension scales s and a light PCA subspace R.\n      - Keep a small archive and success buffer. Generate a small population each generation using\n        a mixture of operators chosen by a soft bandit. Recombine promising candidates into a\n        candidate center, accept with an annealing-like rule, adapt r and s from success statistics,\n        update a learned subspace, and occasionally perform a 1D probe along top direction.\n      - This implementation intentionally uses alternative parameter choices and update equations\n        compared to the original BSAS: different initial r, different population sizing, different\n        operator probabilities, different bandit temperature and learning rate, different trust-radius\n        adaptation factors, a distinct surrogate regularization, and different line-search cadence.\n\n    Main tunable parameters (exposed or hard-coded here):\n      - budget, dim, seed\n      - lambda_: population size computed from dim (different formula than BSAS)\n      - init_r_factor: initial radius factor times mean(range) (here 0.25 vs 0.18)\n      - bandit_temp: temperature for softmax (here 0.4 vs ~0.6)\n      - bandit_alpha: learning rate for log-weights (here 0.25 vs 0.15)\n      - p_de, F_de: DE probability and scale (here 0.3 and 0.8)\n      - levy_scale / t_df: heavy-tailed parameters (here Student-t like)\n      - c_s: second-moment smoothing for per-dim scales (here 0.25 vs 0.15)\n      - line_search_every: cadence (here scaled differently)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # population size: somewhat larger for higher dims but still compact\n        self.lambda_ = max(6, int(4 + 2 * np.ceil(np.sqrt(self.dim))))\n        # blocks for small block-permute operator (coarser than BSAS)\n        n_blocks = max(1, int(np.ceil(self.dim ** 0.6)))\n        sizes = [self.dim // n_blocks] * n_blocks\n        for i in range(self.dim % n_blocks):\n            sizes[i] += 1\n        self.blocks = []\n        idx = 0\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (Most BBOB functions use [-5,5], but respect func.bounds if present)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center uniform in bounds\n        m = self.rng.uniform(lb, ub)\n        # initial trust radius: different from BSAS (more exploratory)\n        init_r_factor = 0.25\n        r = float(init_r_factor * np.mean(ub - lb))\n        # per-dimension relative scales\n        s = np.ones(n)\n\n        # learned subspace R (identity initially)\n        R = np.eye(n)\n\n        # small archive and success buffer\n        archive_X = []\n        archive_F = []\n        success_buf_X = []   # store dx = x - m (global coords)\n        success_buf_df = []  # improvement f_center - f_x\n\n        # operator portfolio and bandit log-weights\n        operators = ['gauss', 'tjump', 'de', 'surrogate', 'blockswap']\n        k_ops = len(operators)\n        op_logw = np.zeros(k_ops)   # neutral start\n        op_count = np.zeros(k_ops, dtype=int)\n\n        # operator / algorithmic hyperparameters (different choices)\n        p_mirror = False  # no mirroring\n        p_de = 0.3\n        F_de = 0.8\n        p_blockswap = 0.18\n        t_df = 2.0       # Student-t degrees of freedom style\n        t_scale = 0.9\n        lin_reg_lambda = 1e-3\n        buf_max = max(50, 10 * int(np.ceil(np.sqrt(n))))\n        k_sub = min(4, n)  # learn up to 4 directions\n        line_search_every = max(15, int(8 * (n / 12 + 1)))\n        gen = 0\n\n        evals = 0\n        # evaluate initial center\n        m = np.clip(m, lb, ub)\n        f_center = func(m)\n        evals += 1\n        archive_X.append(m.copy()); archive_F.append(f_center)\n        f_opt = float(f_center); x_opt = m.copy()\n        last_improv = evals\n\n        # helpers\n        def softmax_from_logw(logw, temp=1.0):\n            z = (logw - np.max(logw)) / max(1e-12, temp)\n            ex = np.exp(z)\n            return ex / (np.sum(ex) + 1e-20)\n\n        def sample_t_direction(base):\n            # Student-t-like scalar times unit direction in base direction\n            # draw scalar from scaled Student-t approx using normal / sqrt(chi2/df)\n            g = self.rng.standard_normal()\n            chi2 = np.sum(self.rng.standard_normal(int(max(1, np.ceil(t_df))))**2)\n            denom = np.sqrt(max(1e-12, chi2 / t_df))\n            scalar = (g / denom) * t_scale\n            normb = np.linalg.norm(base) + 1e-20\n            return (base / normb) * scalar\n\n        def fit_linear_surrogate(buf_X, buf_df):\n            # Fit ridge regression: df ≈ g^T dx  => solve for g\n            if len(buf_X) < 3:\n                return None\n            A = np.vstack(buf_X)  # m x n rows are dx\n            y = np.array(buf_df)\n            ATA = A.T @ A\n            reg = lin_reg_lambda * np.eye(n)\n            try:\n                g = np.linalg.solve(ATA + reg, A.T @ y)\n            except np.linalg.LinAlgError:\n                g = np.linalg.pinv(ATA + reg) @ (A.T @ y)\n            return g\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            probs = softmax_from_logw(op_logw, temp=0.4)  # different temperature\n            Xcand = np.zeros((lam, n))\n            Fcand = np.full(lam, np.inf)\n            cand_op_idx = np.zeros(lam, dtype=int)\n            cand_step = np.zeros((lam, n))  # store dx = x - m\n\n            # pre-generate normals\n            Z = self.rng.standard_normal((lam, n))\n            for i in range(lam):\n                op_idx = self.rng.choice(k_ops, p=probs)\n                cand_op_idx[i] = op_idx\n                op = operators[op_idx]\n                z = Z[i].copy()\n\n                # local scaled vector (relative coords)\n                y_local = s * z\n                y_global = R.dot(y_local)\n                x = m + r * y_global\n\n                # operator specifics\n                if op == 'gauss':\n                    # small gaussian step (already in x)\n                    pass\n\n                elif op == 'tjump':\n                    # heavy-tailed jump in direction z\n                    tj = sample_t_direction(z)\n                    yg = R.dot(s * tj)\n                    x = m + r * 2.5 * yg  # slightly bigger multiplier than gauss\n\n                elif op == 'de':\n                    if len(archive_X) >= 2 and self.rng.random() < p_de:\n                        i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                        de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                        x = x + de_mut\n\n                elif op == 'surrogate':\n                    # guided surrogate descent from fit on success buffer\n                    if len(success_buf_X) >= 4:\n                        g = fit_linear_surrogate(success_buf_X, success_buf_df)\n                        if g is not None:\n                            ng = np.linalg.norm(g) + 1e-20\n                            # step magnitude depends on current radius and randomness\n                            step = - (r * (g / ng)) * (0.3 + 0.7 * self.rng.random())\n                            x = m + step\n                    else:\n                        # fallback to gaussian-like\n                        pass\n\n                elif op == 'blockswap':\n                    # small permutation inside a block\n                    if (len(self.blocks) > 0) and (self.rng.random() < p_blockswap):\n                        bidx = int(self.rng.integers(0, len(self.blocks)))\n                        b = self.blocks[bidx]\n                        if len(b) > 1:\n                            perm = self.rng.permutation(len(b))\n                            x_block = x[b].copy()\n                            x[b] = x_block[perm]\n\n                x = np.clip(x, lb, ub)\n                Xcand[i] = x\n                cand_step[i] = (x - m)\n\n            # Evaluate candidates, ensure budget not exceeded\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fcand[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy(); last_improv = evals\n\n            # Selection: choose top mu candidates\n            mu = max(1, lam // 2)\n            idx_sorted = np.argsort(Fcand)\n            sel_idx = idx_sorted[:mu]\n            X_sel = Xcand[sel_idx]\n            F_sel = Fcand[sel_idx]\n\n            # recombination: blend weighted mean and median to be robust\n            ranks = np.arange(1, mu + 1)\n            weights = (mu + 1 - ranks) / np.sum(mu + 1 - ranks)  # linear rank weights\n            weighted_mean = np.sum(weights[:, None] * X_sel, axis=0)\n            median = np.median(X_sel, axis=0)\n            m_new = 0.7 * weighted_mean + 0.3 * median\n            m_new = np.clip(m_new, lb, ub)\n\n            # center acceptance: accept if better or with annealed uphill tolerance\n            accepted_center_move = False\n            fm_new = None\n            if evals < budget:\n                fm_new = func(m_new)\n                evals += 1\n                archive_X.append(m_new.copy()); archive_F.append(fm_new)\n                if fm_new < f_center:\n                    accepted_center_move = True\n                    f_center = fm_new\n                    m = m_new.copy()\n                else:\n                    delta = fm_new - f_center\n                    # different annealing schedule than BSAS\n                    tau = 0.05 * (1.0 + 0.5 * gen / 1000.0)\n                    if self.rng.random() < np.exp(-delta / (tau + 1e-12)):\n                        accepted_center_move = True\n                        f_center = fm_new\n                        m = m_new.copy()\n                    else:\n                        # soft interpolation toward m_new occasionally\n                        if self.rng.random() < 0.05:\n                            m = 0.85 * m + 0.15 * m_new\n            else:\n                # if no budget to evaluate, do conservative adoption based on mean of selected\n                mean_sel = np.mean(F_sel)\n                if mean_sel < f_center:\n                    m = m_new.copy()\n                    accepted_center_move = True\n\n            # Compute number of improving candidates relative to the pre-update center (safe baseline)\n            # Use the value before m may have been updated this generation; baseline = f_center_snapshot\n            # We use the snapshot before possible center update: approximate with min of previous f_center and f_opt observed earlier\n            # For simplicity, count improvements relative to current f_center (conservative)\n            improvements = np.sum(Fcand < f_center - 1e-12)\n            psucc = improvements / max(1, lam)\n            # adapt radius with different multipliers\n            if psucc > 0.30:\n                r *= 0.85\n            elif psucc < 0.10:\n                r *= 1.12\n            # clamp r to reasonable range\n            r = float(np.clip(r, 1e-8, 6.0 * np.mean(ub - lb)))\n\n            # Update per-dimension scales s using second moments in rotated-local coords with different smoothing\n            if r > 0:\n                Ylocal_sel = (R.T @ ((X_sel - m).T)).T / (r + 1e-20)\n                y2 = np.sum(weights[:, None] * (Ylocal_sel ** 2), axis=0)\n                c_s = 0.25\n                s2 = (1.0 - c_s) * (s ** 2) + c_s * (y2 + 1e-20)\n                s = np.sqrt(s2)\n                s = np.clip(s, 1e-6, 1e3)\n\n            # Update success buffer: store top half of selected (relative to the possibly updated center)\n            h = max(1, mu // 2)\n            for j in range(h):\n                idx_j = sel_idx[j]\n                dx = (Xcand[idx_j] - m)\n                df = max(0.0, (f_center - Fcand[idx_j]))\n                if len(success_buf_X) >= buf_max:\n                    success_buf_X.pop(0); success_buf_df.pop(0)\n                success_buf_X.append(dx.copy())\n                success_buf_df.append(df)\n\n            # Update learned subspace R via PCA on success buffer (different gentle mixing)\n            if len(success_buf_X) >= min(8, n):\n                M = np.vstack(success_buf_X)\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                try:\n                    C = (Mc.T @ Mc) / max(1, Mc.shape[0] - 1)\n                    eigvals, eigvecs = np.linalg.eigh(C)\n                    order = np.argsort(eigvals)[::-1]\n                    top = eigvecs[:, order[:k_sub]]\n                    # move R gently toward top: convex blend of current subspace and new top\n                    alpha_sub = 0.2\n                    A = R[:, :k_sub]\n                    # align via SVD\n                    Msmall = top.T @ A\n                    try:\n                        U, _, Vt = np.linalg.svd(Msmall)\n                        Q = U @ Vt\n                        newblock = top @ Q\n                        R[:, :k_sub] = (1.0 - alpha_sub) * A + alpha_sub * newblock\n                        # orthonormalize\n                        Qr, _ = np.linalg.qr(R)\n                        R = Qr[:, :n]\n                    except np.linalg.LinAlgError:\n                        R = R + 0.02 * self.rng.standard_normal(R.shape)\n                        Qr, _ = np.linalg.qr(R)\n                        R = Qr[:, :n]\n                except np.linalg.LinAlgError:\n                    pass\n\n            # Update operator bandit using average reward per operator (different alpha and baseline)\n            op_rewards = np.zeros(k_ops)\n            op_seen = np.zeros(k_ops)\n            # baseline for improvements: we use f_center snapshot before center update approximate by min(f_center, f_opt)\n            baseline = f_center\n            for i in range(lam):\n                opi = cand_op_idx[i]\n                improv = max(0.0, baseline - Fcand[i]) if np.isfinite(Fcand[i]) else 0.0\n                step_norm = np.linalg.norm(cand_step[i]) + 1e-20\n                reward = improv / (1.0 + 0.5 * step_norm)  # slightly different scaling\n                op_rewards[opi] += reward\n                op_seen[opi] += 1\n            bandit_alpha = 0.25\n            for oi in range(k_ops):\n                if op_seen[oi] > 0:\n                    avg_reward = op_rewards[oi] / op_seen[oi]\n                    target = np.log1p(avg_reward + 1e-12)\n                    op_logw[oi] = (1.0 - bandit_alpha) * op_logw[oi] + bandit_alpha * target\n                    op_count[oi] += int(op_seen[oi])\n\n            # periodic 1D line probes along leading eigenvector (different set of alphas than BSAS)\n            gen += 1\n            if (gen % line_search_every == 0) and (evals < budget):\n                d = R[:, 0]\n                alphas = [0.0, 0.2, -0.2, 0.4, -0.4, 0.8, -0.8]\n                for a in alphas:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + r * a * d, lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = float(fp); x_opt = probe.copy(); last_improv = evals\n                    if fp < f_center:\n                        f_center = fp; m = probe.copy()\n\n            # stagnation detection: different thresholds and perturbation strengths\n            if (evals - last_improv) > max(60, 8 * n):\n                r *= 1.6\n                s = s * (1.0 + 0.3 * (self.rng.random(n) - 0.5))\n                R = R + 0.04 * self.rng.standard_normal(R.shape)\n                Qr, _ = np.linalg.qr(R)\n                R = Qr[:, :n]\n                # small reshuffle of blocks\n                if len(self.blocks) > 1 and self.rng.random() < 0.5:\n                    b1, b2 = self.rng.choice(len(self.blocks), size=2, replace=False)\n                    if len(self.blocks[b1]) and len(self.blocks[b2]):\n                        i1 = self.rng.choice(self.blocks[b1])\n                        i2 = self.rng.choice(self.blocks[b2])\n                        try:\n                            self.blocks[b1].remove(i1); self.blocks[b1].append(i2)\n                            self.blocks[b2].remove(i2); self.blocks[b2].append(i1)\n                        except ValueError:\n                            pass\n                last_improv = evals\n\n            # trim archive to avoid memory blowup\n            if len(archive_X) > 4000:\n                archive_X = archive_X[-4000:]\n                archive_F = archive_F[-4000:]\n\n            # ensure center in bounds\n            m = np.clip(m, lb, ub)\n\n            # break if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ASES scored 0.548 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "19996f2e-d5b3-4a5f-87ca-8ba1bc709672", "operator": null, "metadata": {"aucs": [0.16528860628660913, 0.31959711366922183, 0.5293037498850505, 0.9456696181312516, 0.6685837837030801, 0.78828790377434, 0.304619657144637, 0.5081163482439607, 0.6234758328337828, 0.6263180400896857]}, "task_prompt": ""}
{"id": "aff48501-3f11-4549-8c98-008da082e0d4", "fitness": "-inf", "name": "HASCADE", "description": "HASCADE is a hybrid strategy combining CMA-like global step-size control and weighted recombination with a diagonal+low-rank sampling backbone: it maintains per-coordinate scales D and a low-rank orthonormal basis U (k ≈ ceil(sqrt(dim))) to generate candidates (with mirrored sampling), uses a small population λ ≈ 4+3·log(dim) and μ≈λ/2, and initializes sigma to 0.25·range. It mixes five operators (diag Gaussian, low-rank Gaussian, archive-based DE-style mutation, heavy-tailed Cauchy jumps, and a simple linear-direction exploit) and adapts their selection via a softmax “operator bandit” (op_logw) that is updated from heuristic rewards computed from candidate improvements. Adaptation mechanisms include a CMA-like path ps with cs/damps/chi_n for sigma control, exponential-moving updates of D from selected squared steps, periodic SVD on a success_buffer to rotate and scale U/s_sub, and an archive used both for DE mutations and credit assignment. Practical refinements include budget-aware batch evaluations and occasional center re-evaluation, periodic subspace-guided 1D line-search, stagnation reheating (perturb D/U and inflate sigma), mirrored sampling and safety clamps to keep all scales bounded.", "code": "import numpy as np\n\nclass HASCADE:\n    \"\"\"\n    Hybrid Adaptive Subspace CMA-DE Bandit Search (HASCADE)\n\n    One-line: Combines CMA-like sigma control and recombination with an efficient\n    diagonal+low-rank sampling backbone, archive/DE mutations, heavy-tailed jumps,\n    and an operator bandit to adaptively mix exploration operators; includes\n    periodic subspace SVD and line-search for fast local refinement.\n\n    Expected objective improvement: aims to beat previous best (~0.264) on many noiseless BBOB problems.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population size heuristic (small to moderate)\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n        # subspace rank\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # recombination weights (log-weights)\n        mu = min(self.mu, self.lambda_)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length control constants (CMA-like)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize center and state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.25 * np.mean(ub - lb)  # initial global step-size\n        # diagonal per-coordinate std (D) initialized to 1\n        D = np.ones(n)\n        # low-rank orthonormal basis U (n x k)\n        rand_mat = np.random.randn(n, self.k)\n        try:\n            U, _ = np.linalg.qr(rand_mat)\n            U = U[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = np.eye(n)[:, :self.k]\n\n        # low-rank scale for subspace coordinates\n        s_sub = np.ones(self.k)\n\n        # evolution path for sigma (approximate invsqrt preconditioner via D)\n        ps = np.zeros(n)\n\n        # buffers and archive\n        success_buffer = []    # store recent successful y = (x-m)/sigma steps (in original coords)\n        buffer_max = max(10 * self.k, 40)\n        archive_X = []\n        archive_F = []\n\n        # operator bandit between operators: 0=diag,1=lowrank,2=de,3=cauchy,4=linear\n        op_logw = np.zeros(5)\n        op_counts = np.zeros(5, dtype=int)\n\n        # other hyperparams\n        p_de_base = 0.20\n        F_de = 0.7\n        p_cauchy = 0.12\n        mirrored = True\n        line_search_every = max(20, int(5 * (n / 10 + 1)))\n        gen = 0\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of center\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_opt = float(fm)\n            x_opt = xm.copy()\n            f_center = float(fm)\n\n        # helper: softmax operator probabilities\n        def op_probs(logw, temp=0.6):\n            w = np.exp((logw - np.max(logw)) / max(1e-12, temp))\n            return w / (np.sum(w) + 1e-20)\n\n        # helper: sample a combined diag+lowrank step (returns y such that x = m + sigma*y)\n        def sample_combined(D, U, s_sub):\n            # diag part\n            z = np.random.randn(n)\n            y_diag = D * z\n            # low-rank part\n            if U.shape[1] > 0:\n                z_sub = np.random.randn(U.shape[1])\n                y_low = (U @ (s_sub * z_sub))\n            else:\n                y_low = 0.0\n            # combine with blend factor\n            alpha = 0.6  # weight for diag vs low-rank\n            y = (1 - alpha) * y_diag + alpha * y_low\n            return y\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            arx = np.zeros((lam, n))\n            arfit = np.full(lam, np.inf)\n            ar_y = np.zeros((lam, n))  # store y = (x-m)/sigma\n            # operator selection probabilities\n            probs = op_probs(op_logw, temp=0.6)\n            # pre-generate gaussian normals for efficiency\n            Z = np.random.randn(lam, n)\n            for i in range(lam):\n                op = np.random.choice(5, p=probs)  # choose operator index\n                # mirrored sampling\n                z = Z[i].copy()\n                if mirrored and (i % 2 == 1):\n                    z = -z\n                y = np.zeros(n)\n                # operator behaviors\n                if op == 0:\n                    # diag-dominant Gaussian\n                    y = (D * z)\n                elif op == 1:\n                    # low-rank dominated: sample subspace coords and small diag noise\n                    if self.k > 0:\n                        z_sub = np.random.randn(self.k)\n                        y = 0.9 * (U @ (s_sub * z_sub)) + 0.1 * (D * z)\n                    else:\n                        y = D * z\n                elif op == 2:\n                    # DE-style mutation around mean using archive if available\n                    y = D * z\n                    if (len(archive_X) >= 2) and (np.random.rand() < p_de_base):\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                        # translate de_mut to y-space (divide by sigma)\n                        y = y + de_mut / (sigma + 1e-20)\n                elif op == 3:\n                    # heavy-tailed Cauchy jump in a random direction (but scaled by current diag)\n                    r = np.random.standard_cauchy() * 0.8\n                    nz = np.linalg.norm(z) + 1e-20\n                    dir_unit = z / nz\n                    y = dir_unit * (np.mean(D) * r)\n                elif op == 4:\n                    # linear model guided small step if success buffer available\n                    y = D * z * 0.5\n                    # attempt a simple directional descent from linear approx\n                    if len(success_buffer) >= min(6, n):\n                        # compute approximate average step direction among successes\n                        S = np.vstack(success_buffer[-min(20, len(success_buffer)):])  # m x n\n                        # average direction weighted by magnitude improvement implicit\n                        avg_dir = np.mean(S, axis=0)\n                        nd = np.linalg.norm(avg_dir) + 1e-20\n                        y = 0.5 * y + 0.5 * (avg_dir / nd) * np.mean(D) * 0.8\n\n                # occasionally add small random remix from low-rank for exploration\n                if (np.random.rand() < 0.08) and (self.k > 0):\n                    z_sub = np.random.randn(self.k)\n                    y += 0.3 * (U @ (s_sub * z_sub))\n\n                # store y, generate x candidate\n                x = m + sigma * y\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n                arx[i] = x\n                ar_y[i] = y\n\n            # Evaluate candidates until budget exhausted\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = arx[i]\n                fi = func(xi)\n                evals += 1\n                arfit[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                # limit archive size\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy()\n                # (no immediate early break here; we still complete generation bookkeeping)\n\n            # Selection and recombination\n            valid_idx = np.argsort(arfit)\n            sel_idx = valid_idx[:mu]\n            X_sel = arx[sel_idx]\n            Y_sel = ar_y[sel_idx]  # weighted steps in y-space\n            # recombine mean m using log-weights\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * X_sel, axis=0)\n            m = np.clip(m, lb, ub)\n\n            # update ps (path length) using approximate invsqrt via D (diagonal)\n            # compute weighted mean y_w\n            y_w = np.sum(weights[:, None] * Y_sel, axis=0)\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n            # sigma adaptation\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e3 * np.mean(ub - lb))\n\n            # Update diagonal scales D by exponential moving average of selected squared y's\n            c_d = 0.2\n            y2 = np.sum(weights[:, None] * (Y_sel ** 2), axis=0)\n            D2 = (1 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n            D = np.clip(D, 1e-6, 1e3)\n\n            # Append weighted successful steps (in original coords y) to success buffer\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # Update low-rank basis U and its scales via SVD on success buffer\n            if (len(success_buffer) >= self.k) and (evals % max(1, int(self.mu)) == 0):\n                Ymat = np.vstack(success_buffer).T  # n x m\n                Yc = Ymat - np.mean(Ymat, axis=1, keepdims=True)\n                try:\n                    U_s, svals, Vt = np.linalg.svd(Yc, full_matrices=False)\n                    k_take = min(self.k, U_s.shape[1])\n                    if k_take > 0:\n                        U_new = U_s[:, :k_take]\n                        # gently rotate existing U toward U_new (blend)\n                        blend = 0.6\n                        # pad/truncate if shapes differ\n                        if U_new.shape[1] < self.k:\n                            # pad with random orthonormal\n                            pad = np.random.randn(n, self.k - U_new.shape[1])\n                            Qu, _ = np.linalg.qr(pad)\n                            U_new = np.hstack((U_new, Qu[:, : (self.k - U_new.shape[1])]))\n                        U = (1 - blend) * U + blend * U_new[:, :self.k]\n                        # orthonormalize U\n                        try:\n                            Q, _ = np.linalg.qr(U)\n                            U = Q[:, :self.k]\n                        except np.linalg.LinAlgError:\n                            pass\n                        # update subspace scales s_sub by svals\n                        s_sub = 0.5 * s_sub + 0.5 * (np.sqrt(np.maximum(svals[:self.k], 1e-12)))\n                except np.linalg.LinAlgError:\n                    pass\n\n            # Operator bandit reward update: compute per-operator aggregate reward for this generation\n            baseline = f_center if 'f_center' in locals() else f_opt\n            op_rewards = np.zeros_like(op_logw)\n            op_seen = np.zeros_like(op_logw)\n            # We didn't store per-candidate operator index explicitly here; approximate by attributing\n            # reward to each operator proportional to similarity of its generated y to selected Y_sel\n            # (cheap surrogate): for each candidate i, compute improv = baseline - fi\n            for i in range(lam):\n                fi = arfit[i]\n                if not np.isfinite(fi):\n                    continue\n                improv = max(0.0, baseline - fi)\n                # attribute to an operator by similarity heuristics: compute variance explained by projection on U\n                y = ar_y[i]\n                # projection energy in low-rank subspace\n                if self.k > 0:\n                    proj = U.T @ y\n                    e_low = np.sum(proj ** 2)\n                else:\n                    e_low = 0.0\n                e_diag = np.sum((y - (U @ (U.T @ y))) ** 2) if self.k > 0 else np.sum(y ** 2)\n                # heuristic attribution\n                # if e_low dominates -> reward to lowrank operator; if e_diag dominates -> diag; large de-like diff -> de; heavy tails -> cauchy; linear -> linear\n                total = e_low + e_diag + 1e-20\n                p_low = e_low / total\n                p_diag = e_diag / total\n                # small heuristics: if candidate is far from mean relative to sigma -> likely cauchy or de\n                dist = np.linalg.norm((arx[i] - m)) / (sigma + 1e-20)\n                # attribute fractions\n                op_rewards[1] += improv * p_low * (1 - min(1.0, dist / 5.0))\n                op_rewards[0] += improv * p_diag * (1 - min(1.0, dist / 5.0))\n                # de credit if candidate had large difference from archive (approx by distance to nearest archive)\n                if len(archive_X) >= 2:\n                    dists = np.linalg.norm(np.array(archive_X) - arx[i], axis=1)\n                    near = np.min(dists) if len(dists) > 0 else 1e9\n                    if near > 1e-3 and dist > 1.0:\n                        op_rewards[2] += 0.5 * improv\n                # cauchy credit if dist quite large\n                if dist > 2.5:\n                    op_rewards[3] += 0.6 * improv\n                # linear credit if y aligns with average success direction\n                if len(success_buffer) >= 3:\n                    avg = np.mean(np.vstack(success_buffer), axis=0)\n                    if np.dot(avg, y) > 0:\n                        op_rewards[4] += 0.3 * improv\n\n            # update op_logw with small learning rate\n            alpha = 0.12\n            for oi in range(len(op_logw)):\n                if op_rewards[oi] > 0:\n                    target = np.log1p(op_rewards[oi])\n                    op_logw[oi] = (1 - alpha) * op_logw[oi] + alpha * target\n\n            # Update f_center if new center evaluated earlier (we have f_center in locals or update with current m)\n            # Safer: evaluate current center if budget permits occasionally to maintain accurate baseline\n            if evals < budget and (gen % max(1, int(self.mu)) == 0):\n                xm = np.clip(m, lb, ub)\n                fm = func(xm)\n                evals += 1\n                archive_X.append(xm.copy()); archive_F.append(fm)\n                if fm < f_opt:\n                    f_opt = float(fm); x_opt = xm.copy()\n                f_center = float(fm)\n\n            # periodic subspace-guided 1D line-search along top U direction\n            gen += 1\n            if (gen % line_search_every == 0) and (evals < budget) and (self.k > 0):\n                d = U[:, 0]\n                alphas = [0.0, 0.5, -0.5, 0.25, -0.25]\n                for a in alphas:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + sigma * a * d * np.mean(D), lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = float(fp); x_opt = probe.copy()\n                        # nudge m toward better probe\n                        m = 0.8 * m + 0.2 * probe\n                        m = np.clip(m, lb, ub)\n                # after line-search, lightly reduce sigma if improvement found, else increase slightly\n                if x_opt is not None and np.linalg.norm(x_opt - m_old) < 1e-8:\n                    sigma *= 1.06\n                else:\n                    sigma *= 0.92\n                sigma = np.clip(sigma, 1e-12, 1e3 * np.mean(ub - lb))\n\n            # stagnation: if no improvement for long, reheating and mild randomization\n            # define improvement threshold by last recorded f_opt change\n            if hasattr(self, 'last_seen_opt'):\n                last_seen = getattr(self, 'last_seen_opt')\n            else:\n                last_seen = f_opt\n                self.last_seen_opt = f_opt\n            # track last improvement epoch via f_opt snapshot\n            if f_opt < getattr(self, 'last_seen_opt', np.inf) - 1e-12:\n                self.last_seen_opt = f_opt\n                stagn_counter = 0\n            else:\n                stagn_counter = getattr(self, 'stagn_counter', 0) + 1\n                self.stagn_counter = stagn_counter\n            if stagn_counter > max(20, 5 * n):\n                # perturb D, U and increase sigma\n                sigma *= 1.8\n                D = D * (1.0 + 0.2 * (np.random.randn(n) * 0.1))\n                if self.k > 0:\n                    U = U + 0.08 * np.random.randn(*U.shape)\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n                # clear success buffer to allow new directions\n                success_buffer = []\n                self.stagn_counter = 0\n\n            # safety clamps\n            sigma = max(sigma, 1e-12)\n            D = np.clip(D, 1e-6, 1e3)\n            if evals >= budget:\n                break\n\n        # final return best found\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "19996f2e-d5b3-4a5f-87ca-8ba1bc709672", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e63875a7-ba52-4ac3-ac49-4d1d69ec3e07", "fitness": "-inf", "name": "MiSADES", "description": "MiSADES mixes ensemble directional probes drawn either isotropically or from a trust-weighted PCA subspace built from recent successful step directions (memory_size, mem_mix) so search focuses on promising subspaces while still exploring (probes_scale ≈ 3·k, occasional large steps p_large_step). It uses budget-aware local refinements: immediate mirrored sampling with cheap parabolic 3‑point line fits, occasional golden‑section line search along an evolution path (evo_path, evo_decay) and small jitter polishing to exploit detected descent directions. Step-size (sigma) is multiplicatively adapted (success_alpha > 1, failure_alpha < 1) and memory entries carry decaying “trust” weights that bias future subspace sampling and are pruned/boosted to balance intensification vs diversification. Robustness mechanisms include Cauchy heavy‑tailed jumps, controlled restarts around the global best, strict bounds clipping, and a safe_eval wrapper that enforces the exact evaluation budget.", "code": "import numpy as np\nfrom collections import deque\n\nclass MiSADES:\n    \"\"\"\n    Memory-informed Subspace Adaptive Directional Ensemble Search (MiSADES)\n\n    One-line: Ensemble directional probes sampled from a trust-weighted PCA subspace mixed\n    with isotropic directions, immediate parabolic refinements on successes, multiplicative\n    sigma adaptation, evolution-path guided occasional golden-section line-search, and budget-safe restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=16, init_step_factor=0.5,\n                 mem_mix=0.65, success_alpha=1.10, failure_alpha=0.86):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self._init_step_factor = float(init_step_factor)\n        self.mem_mix = float(mem_mix)\n        self.success_alpha = float(success_alpha)\n        self.failure_alpha = float(failure_alpha)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds handling\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        lb = lb.reshape(n,)\n        ub = ub.reshape(n,)\n\n        # helpers\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # safe evaluator that respects budget and updates best\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initialize current point uniformly and evaluate\n        x_cur = np.random.uniform(lb, ub)\n        res = safe_eval(x_cur)\n        if res[0] is None:\n            return float(f_best), np.array(x_best, dtype=float)\n        f_cur, x_cur = res\n\n        # step-size initialization (absolute)\n        domain_size = np.mean(ub - lb)\n        sigma = max(self._init_step_factor * domain_size, 1e-12)\n        sigma_min = 1e-12\n        sigma_max = 20.0 * domain_size + 1e-12\n\n        # memory of recent successful (unit) steps and trust weights\n        mem_steps = deque(maxlen=self.memory_size)  # store vectors (n,)\n        mem_trust = deque(maxlen=self.memory_size)\n\n        # evolution path accumulator\n        evo_path = np.zeros(n)\n        evo_decay = 0.86\n\n        # stagnation / restart controls\n        no_improve_iters = 0\n        stagnation_limit = max(12, int(10 + 2 * np.log1p(n)))\n        restarts = 0\n        max_restarts = 6\n\n        # parameters for probes and behavior\n        min_k = 1\n        base_k = max(2, int(min(np.ceil(np.sqrt(n)), n)))\n        probes_scale = 3  # probes ~ 3*k\n        p_large_step = 0.14  # occasional larger alpha\n        p_line_search = 0.28  # probability to do golden-section along evo_path\n        p_cauchy = 0.06  # occasional heavy-tailed global jump\n\n        # small parabolic 3-point line fit (uses up to 3 evals)\n        def parabolic_line_min(x0, f0, d, delta=None):\n            nonlocal evals\n            if delta is None:\n                delta = max(0.8 * sigma, 1e-12)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                return None, None\n            d = d / (nd + 1e-20)\n            if evals >= self.budget:\n                return None, None\n            # +delta\n            xp = clip(x0 + delta * d)\n            res_p = safe_eval(xp)\n            if res_p[0] is None:\n                return None, None\n            fp, xp = res_p\n            if evals >= self.budget:\n                # only xp available\n                if fp < f0 - 1e-12:\n                    return fp, xp\n                return None, None\n            # -delta\n            xm = clip(x0 - delta * d)\n            res_m = safe_eval(xm)\n            if res_m[0] is None:\n                return None, None\n            fm, xm = res_m\n            # quadratic fit\n            denom = 2.0 * (delta ** 2)\n            A = (fp + fm - 2.0 * f0) / (denom + 1e-20)\n            B = (fp - fm) / (2.0 * delta)\n            if A <= 0 or np.isclose(A, 0.0):\n                # choose best of sampled\n                bestf = f0; bestx = x0.copy()\n                if fp < bestf - 1e-12:\n                    bestf, bestx = fp, xp.copy()\n                if fm < bestf - 1e-12:\n                    bestf, bestx = fm, xm.copy()\n                if bestf < f0 - 1e-12:\n                    return bestf, bestx\n                return None, None\n            alpha_star = -B / (2.0 * A)\n            if abs(alpha_star) > 4.0 * delta:\n                bestf = f0; bestx = x0.copy()\n                if fp < bestf - 1e-12:\n                    bestf, bestx = fp, xp.copy()\n                if fm < bestf - 1e-12:\n                    bestf, bestx = fm, xm.copy()\n                if bestf < f0 - 1e-12:\n                    return bestf, bestx\n                return None, None\n            if evals >= self.budget:\n                return None, None\n            xs = clip(x0 + alpha_star * d)\n            res_s = safe_eval(xs)\n            if res_s[0] is None:\n                return None, None\n            fs, xs = res_s\n            if fs < f0 - 1e-12:\n                return fs, xs\n            # fallback to best sampled\n            bestf = f0; bestx = x0.copy()\n            if fp < bestf - 1e-12:\n                bestf, bestx = fp, xp.copy()\n            if fm < bestf - 1e-12:\n                bestf, bestx = fm, xm.copy()\n            if bestf < f0 - 1e-12:\n                return bestf, bestx\n            return None, None\n\n        # budget-aware golden-section line search along direction d\n        def golden_line_search(x0, f0, d, init_step=None, max_evals=12):\n            nonlocal evals\n            if init_step is None:\n                init_step = sigma\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            remain = max(0, self.budget - evals)\n            if remain <= 0:\n                return None, None\n            # try +init and -init\n            a = 0.0\n            fa = f0\n            b = init_step\n            xb = clip(x0 + b * d)\n            resb = safe_eval(xb)\n            if resb[0] is None:\n                return None, None\n            fb, xb = resb\n            remain = max(0, self.budget - evals)\n            if fb >= fa:\n                b = -init_step\n                xb = clip(x0 + b * d)\n                resb2 = safe_eval(xb)\n                if resb2[0] is None:\n                    return None, None\n                fb, xb = resb2\n                remain = max(0, self.budget - evals)\n                if fb >= fa:\n                    return None, None\n            # expand\n            expand = 1.5\n            max_exp = 4\n            exp_count = 0\n            while remain > 0 and exp_count < max_exp:\n                new_b = b * expand\n                xn = clip(x0 + new_b * d)\n                resn = safe_eval(xn)\n                if resn[0] is None:\n                    return None, None\n                fn, xn = resn\n                remain = max(0, self.budget - evals)\n                if fn < fb:\n                    b = new_b\n                    fb = fn\n                    xb = xn.copy()\n                    exp_count += 1\n                    continue\n                break\n            if remain <= 0:\n                if fb < fa:\n                    return fb, xb\n                return None, None\n            # golden section on [0,b]\n            gr = (np.sqrt(5) - 1.0) / 2.0\n            a0 = a\n            b0 = b\n            c = b0 - gr * (b0 - a0)\n            d_alpha = a0 + gr * (b0 - a0)\n            xc = clip(x0 + c * d)\n            res_c = safe_eval(xc)\n            if res_c[0] is None:\n                return None, None\n            fc, xc = res_c\n            xd = clip(x0 + d_alpha * d)\n            res_d = safe_eval(xd)\n            if res_d[0] is None:\n                return None, None\n            fd, xd = res_d\n            bestf = fa\n            bestx = x0.copy()\n            for val, xval in ((fc, xc), (fd, xd), (fb, xb)):\n                if val < bestf:\n                    bestf = val; bestx = xval.copy()\n            it = 0\n            max_iters = max_evals\n            remain = max(0, self.budget - evals)\n            while remain > 0 and it < max_iters and abs(b0 - a0) > 1e-12:\n                it += 1\n                if fc < fd:\n                    b0 = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    d_alpha = a0 + gr * (b0 - a0)\n                    xd = clip(x0 + d_alpha * d)\n                    res_d = safe_eval(xd)\n                    if res_d[0] is None:\n                        break\n                    fd, xd = res_d\n                    if fd < bestf:\n                        bestf, bestx = fd, xd.copy()\n                else:\n                    a0 = c\n                    c = d_alpha\n                    fc = fd\n                    c = b0 - gr * (b0 - a0)\n                    xc = clip(x0 + c * d)\n                    res_c = safe_eval(xc)\n                    if res_c[0] is None:\n                        break\n                    fc, xc = res_c\n                    if fc < bestf:\n                        bestf, bestx = fc, xc.copy()\n                remain = max(0, self.budget - evals)\n            if bestf < f0:\n                return bestf, bestx\n            return None, None\n\n        # basis builder from mem_steps (returns n x k orthonormal matrix and variances)\n        def build_basis(k):\n            # if insufficient memory -> random orthonormal\n            if len(mem_steps) >= 2:\n                M = np.vstack(list(mem_steps))  # m x n (steps as rows)\n                # center rows\n                M_cent = M - np.mean(M, axis=0, keepdims=True)\n                try:\n                    U, S, Vt = np.linalg.svd(M_cent, full_matrices=False)\n                    pcs = Vt.T  # n x r\n                    r = pcs.shape[1]\n                    take = min(k, r)\n                    if take <= 0:\n                        R = np.random.randn(n, k)\n                        Q, _ = np.linalg.qr(R)\n                        return Q[:, :k], np.ones(k)\n                    basis_cols = pcs[:, :take]\n                    if take < k:\n                        R = np.random.randn(n, k - take)\n                        stacked = np.column_stack((basis_cols, R))\n                        Q, _ = np.linalg.qr(stacked)\n                        var_est = np.concatenate([S[:take]**2, np.ones(k - take) * 1e-6])\n                        var_est = var_est + 1e-12\n                        return Q[:, :k], var_est / np.mean(var_est)\n                    else:\n                        Q, _ = np.linalg.qr(basis_cols)\n                        var_est = S[:take]**2 + 1e-12\n                        return Q[:, :k], var_est / np.mean(var_est)\n                except Exception:\n                    R = np.random.randn(n, k)\n                    Q, _ = np.linalg.qr(R)\n                    return Q[:, :k], np.ones(k)\n            else:\n                R = np.random.randn(n, k)\n                Q, _ = np.linalg.qr(R)\n                return Q[:, :k], np.ones(k)\n\n        # main loop: rounds of subspace sampling\n        while evals < self.budget:\n            # adapt subspace dimension slightly\n            k = int(min(n, max(min_k, base_k + int(np.random.randn() * 0.3))))\n            basis, var_coef = build_basis(k)  # n x k, variances length k\n            # bias variances by memory trust average\n            avg_trust = (np.mean(mem_trust) if len(mem_trust) > 0 else 1.0)\n            var_coef = np.maximum(var_coef, 1e-8) * (1.0 + 0.5 * avg_trust)\n            probes = max(4, probes_scale * k)\n            improved_round = False\n\n            candidate_list = []\n\n            # mirrored sampling: produce pairs if even probes\n            half = probes // 2\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # decide whether to sample in learned subspace\n                if len(mem_steps) >= 1 and (np.random.rand() < self.mem_mix):\n                    coeffs = np.random.randn(k) * np.sqrt(var_coef)\n                    d = basis.dot(coeffs)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                else:\n                    d = np.random.randn(n)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                # sample alpha\n                if np.random.rand() < p_large_step:\n                    alpha = np.random.uniform(-2.0 * sigma, 2.0 * sigma)\n                else:\n                    alpha = np.random.uniform(-sigma, sigma)\n                # candidate\n                x_try = clip(x_cur + alpha * d)\n                res_try = safe_eval(x_try)\n                if res_try[0] is None:\n                    break\n                f_try, x_try = res_try\n                candidate_list.append((f_try, x_try.copy(), d.copy(), alpha))\n                # immediate acceptance if improved over current point\n                if f_try < f_cur - 1e-12:\n                    # parabolic refinement along d\n                    delta = max(abs(alpha), 0.9 * sigma)\n                    res_par = parabolic_line_min(x_cur, f_cur, d, delta=delta)\n                    if res_par[0] is not None and res_par[0] < f_try - 1e-12:\n                        f_try, x_try = res_par\n                    # accept\n                    prev = x_cur.copy()\n                    x_cur = x_try.copy()\n                    f_cur = float(f_try)\n                    # update memory\n                    disp = x_cur - prev\n                    dn = np.linalg.norm(disp)\n                    if dn > 1e-16:\n                        mem_steps.appendleft((disp / dn).copy())\n                        mem_trust.appendleft(1.0)\n                    # decay older trusts a bit\n                    mem_trust = deque([t * 0.98 for t in mem_trust], maxlen=self.memory_size)\n                    # boost sigma moderately\n                    sigma = min(sigma * self.success_alpha, sigma_max)\n                    # update evo_path\n                    evo_step = (x_cur - prev) / (sigma + 1e-20)\n                    evo_path = evo_decay * evo_path + (1.0 - evo_decay) * evo_step\n                    improved_round = True\n                    no_improve_iters = 0\n                    # quick small polish jitter occasionally\n                    if evals < self.budget and np.random.rand() < 0.35:\n                        jitter = np.random.randn(n)\n                        jitter = jitter / (np.linalg.norm(jitter) + 1e-20)\n                        jitter *= 0.15 * sigma\n                        res_j = safe_eval(clip(x_cur + jitter))\n                        if res_j[0] is not None:\n                            fj, xj = res_j\n                            if fj < f_cur - 1e-12:\n                                prev2 = x_cur.copy()\n                                x_cur = xj.copy()\n                                f_cur = float(fj)\n                                disp2 = x_cur - prev2\n                                dn2 = np.linalg.norm(disp2)\n                                if dn2 > 1e-16:\n                                    mem_steps.appendleft((disp2 / dn2).copy())\n                                    mem_trust.appendleft(1.0)\n                                sigma = min(sigma * self.success_alpha, sigma_max)\n                    # continue generating more probes (we don't break)\n                else:\n                    # small chance to attempt cheap parabolic exploitation even if not improved\n                    if np.random.rand() < 0.035 and (self.budget - evals) >= 3:\n                        res_par = parabolic_line_min(x_cur, f_cur, d, delta=0.8 * sigma)\n                        if res_par[0] is not None and res_par[0] < f_cur - 1e-12:\n                            prev = x_cur.copy()\n                            x_cur = res_par[1].copy()\n                            f_cur = float(res_par[0])\n                            disp = x_cur - prev\n                            dn = np.linalg.norm(disp)\n                            if dn > 1e-16:\n                                mem_steps.appendleft((disp / dn).copy())\n                                mem_trust.appendleft(1.0)\n                            sigma = min(sigma * self.success_alpha, sigma_max)\n                            evo_step = (x_cur - prev) / (sigma + 1e-20)\n                            evo_path = evo_decay * evo_path + (1.0 - evo_decay) * evo_step\n                            improved_round = True\n                            no_improve_iters = 0\n\n            # after probes: if no immediate improvement maybe try candidate-based refinements\n            if (not improved_round) and candidate_list:\n                candidate_list.sort(key=lambda t: t[0])\n                topk = min(3, len(candidate_list))\n                for i in range(topk):\n                    f_try, x_try, d_try, alpha_try = candidate_list[i]\n                    if evals >= self.budget:\n                        break\n                    # attempt parabolic along d_try centered at x_cur\n                    if (self.budget - evals) >= 3:\n                        res_par = parabolic_line_min(x_cur, f_cur, d_try, delta=max(sigma, abs(alpha_try)))\n                        if res_par[0] is not None and res_par[0] < f_cur - 1e-12:\n                            prev = x_cur.copy()\n                            x_cur = res_par[1].copy()\n                            f_cur = float(res_par[0])\n                            dn = np.linalg.norm(x_cur - prev)\n                            if dn > 1e-16:\n                                mem_steps.appendleft(((x_cur - prev) / dn).copy())\n                                mem_trust.appendleft(1.0)\n                            sigma = min(sigma * self.success_alpha, sigma_max)\n                            evo_path = evo_decay * evo_path + (1.0 - evo_decay) * ((x_cur - prev) / (sigma + 1e-20))\n                            improved_round = True\n                            no_improve_iters = 0\n                            break\n\n            # occasional golden-section along evolution path if it has some magnitude\n            if (np.linalg.norm(evo_path) > 1e-12) and (np.random.rand() < p_line_search) and (self.budget - evals >= 4):\n                res_mean_eval = safe_eval(x_cur)  # may re-evaluate current (harmless if same)\n                if res_mean_eval[0] is None:\n                    break\n                f0 = res_mean_eval[0]\n                res_ls = golden_line_search(x_cur, f0, evo_path, init_step=sigma, max_evals=min(10, self.budget - evals))\n                if res_ls[0] is not None and res_ls[0] < f_cur - 1e-12:\n                    prev = x_cur.copy()\n                    f_ls, x_ls = res_ls\n                    x_cur = x_ls.copy()\n                    f_cur = float(f_ls)\n                    dn = np.linalg.norm(x_cur - prev)\n                    if dn > 1e-16:\n                        mem_steps.appendleft(((x_cur - prev) / dn).copy())\n                        mem_trust.appendleft(1.0)\n                    sigma = min(sigma * self.success_alpha, sigma_max)\n                    evo_path = evo_decay * evo_path + (1.0 - evo_decay) * ((x_cur - prev) / (sigma + 1e-20))\n                    improved_round = True\n                    no_improve_iters = 0\n\n            # occasional heavy-tailed jump to escape basins\n            if (not improved_round) and (self.budget - evals > 0) and (np.random.rand() < p_cauchy):\n                scale = 0.5 * domain_size\n                jump = np.random.standard_cauchy(size=n)\n                jump = np.clip(jump, -10, 10)\n                xj = clip(x_cur + jump * scale)\n                resj = safe_eval(xj)\n                if resj[0] is not None:\n                    fj, xj = resj\n                    if fj < f_cur - 1e-12:\n                        prev = x_cur.copy()\n                        x_cur = xj.copy()\n                        f_cur = float(fj)\n                        dn = np.linalg.norm(x_cur - prev)\n                        if dn > 1e-16:\n                            mem_steps.appendleft(((x_cur - prev) / dn).copy())\n                            mem_trust.appendleft(1.0)\n                        sigma = min(sigma * self.success_alpha, sigma_max)\n                        evo_path = evo_decay * evo_path + (1.0 - evo_decay) * ((x_cur - prev) / (sigma + 1e-20))\n                        improved_round = True\n                        no_improve_iters = 0\n\n            # adapt sigma and memory trust on failure/success\n            if not improved_round:\n                no_improve_iters += 1\n                sigma = max(sigma * self.failure_alpha, sigma_min)\n                # decay trusts\n                mem_trust = deque([t * 0.95 for t in mem_trust], maxlen=self.memory_size)\n            else:\n                # grow a bit in successful round and reinforce top trusts\n                sigma = min(sigma * 1.06, sigma_max)\n                mem_trust = deque([min(t * 1.06, 20.0) for t in mem_trust], maxlen=self.memory_size)\n\n            # prune tiny trust entries\n            if len(mem_trust) > 0:\n                keep = [i for i, t in enumerate(mem_trust) if t > 1e-4]\n                if len(keep) != len(mem_trust):\n                    mem_steps = deque([mem_steps[i] for i in keep], maxlen=self.memory_size)\n                    mem_trust = deque([mem_trust[i] for i in keep], maxlen=self.memory_size)\n\n            # stagnation handling: restart around best or diversify memory\n            if no_improve_iters >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final tiny polish until budget exhausted\n                    while evals < self.budget:\n                        d = np.random.randn(n)\n                        d /= (np.linalg.norm(d) + 1e-20)\n                        alpha = np.random.uniform(-0.2 * sigma, 0.2 * sigma)\n                        res_t = safe_eval(clip(x_cur + alpha * d))\n                        if res_t[0] is None:\n                            break\n                        ft, xt = res_t\n                        if ft < f_cur - 1e-12:\n                            x_cur = xt.copy(); f_cur = float(ft)\n                            # update memory modestly\n                            disp = x_cur - x_best if x_best is not None else x_cur - xt\n                            dn = np.linalg.norm(disp)\n                            if dn > 1e-16:\n                                mem_steps.appendleft((disp / dn).copy())\n                                mem_trust.appendleft(1.0)\n                        sigma = max(sigma * 0.97, sigma_min)\n                    break\n                else:\n                    # restart around global best if available, else random\n                    if x_best is not None and np.isfinite(f_best):\n                        radius = min(1.0, 0.5 * (1.0 + restarts / 2.0)) * domain_size\n                        perturb = np.random.randn(n) * radius\n                        x_new = clip(x_best + perturb)\n                        res_new = safe_eval(x_new)\n                        if res_new[0] is None:\n                            break\n                        f_new, x_new = res_new\n                        x_cur = x_new.copy(); f_cur = float(f_new)\n                    else:\n                        x_new = np.random.uniform(lb, ub)\n                        res_new = safe_eval(x_new)\n                        if res_new[0] is None:\n                            break\n                        f_new, x_new = res_new\n                        x_cur = x_new.copy(); f_cur = float(f_new)\n                    # enlarge sigma to escape basins\n                    sigma = min(2.0 * sigma + 0.2 * domain_size, sigma_max)\n                    # slightly decay memory trust to encourage new directions\n                    mem_trust = deque([t * 0.25 for t in mem_trust], maxlen=self.memory_size)\n                    no_improve_iters = 0\n\n            # early exit if insanely good\n            if f_best <= 1e-12:\n                break\n\n        # final best fallback\n        if x_best is None:\n            x_best = x_cur.copy()\n            f_best = float(f_cur)\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 77, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem_steps = deque(maxlen=self.memory_size)  # store vectors (n,)", "error": "In the code, line 77, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem_steps = deque(maxlen=self.memory_size)  # store vectors (n,)", "parent_ids": "19996f2e-d5b3-4a5f-87ca-8ba1bc709672", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "cce2f89f-b4b3-48dc-85a6-41c23d32bf67", "fitness": 0.1689311928307075, "name": "AMSS_E", "description": "AMSS‑E combines per-coordinate adaptive scales (EMA on squared successful steps) with a low‑rank correlated search subspace learned by periodic SVD on a rolling success buffer (rank r ≈ √dim, buffer_max, svd_every) and uses log‑based recombination with a small population (λ, μ). Sampling is a mixture: dominant diagonal Gaussian steps (alpha≈0.85) plus a low‑rank projection, occasional heavy‑tailed Levy jumps (p_levy=0.09, levy_scale=1.0), DE‑style archive differences (p_de=0.16, F_de=0.7) and opposition probes, with mirrored paired sampling and boundary clipping for robustness. Step‑size σ is adapted by a CMA‑style path cumulation (ps, cs, damps, chi_n) combined with a short‑term success‑rate controller toward a 0.2 target and clamped to reasonable ranges, while a per‑dimension trust vector (bounded ~0.4–1.0) damps sensitive or boundary dimensions. Practical safeguards include an evaluation archive, FIFO cap, gentle stagnation restarts/opposition nudges with σ inflation, and conservative low‑rank updates to keep the search stable.", "code": "import numpy as np\n\nclass AMSS_E:\n    \"\"\"\n    AMSS-E: Adaptive Multi-Scale Subspace Evolution with Levy restarts.\n\n    Key ideas:\n    - Per-coordinate scales (diagonal) learned by EMA on successful steps.\n    - Low-rank subspace V,S learned by incremental SVD on a rolling success buffer\n      to inject correlated, informed search directions.\n    - Mixture sampling: Gaussian in scaled-coordinates, low-rank correlated steps,\n      occasional heavy-tailed Levy (Cauchy) jumps and DE-style archive differences.\n    - Mirrored sampling and log-weighted recombination. Sigma adapts by a\n      cumulative-path statistic and by a short-term success-rate controller.\n    - Light stagnation handling: gentle restarts and opposition-based probes.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_r=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        # population size heuristic (slightly larger for exploration)\n        self.lambda_ = max(6, int(np.round(4 + 3.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n        # subspace rank\n        if subspace_r is None:\n            self.r = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.r = min(max(1, int(subspace_r)), self.dim)\n\n        # algorithm knobs (tunable)\n        self.p_levy = 0.09        # heavy-tailed jump prob\n        self.levy_scale = 1.0     # scale for heavy-tailed jumps\n        self.p_de = 0.16          # probability of DE-like difference injection\n        self.F_de = 0.7           # DE scaling factor\n        self.mirrored = True\n        # PCA / buffer sizes\n        self.buffer_max = max(30, 8 * self.r)\n        self.svd_every = max(5, min(20, int(np.ceil(self.dim / 2.0))))\n        # stagnation detection\n        self.stag_threshold = max(50, 8 * self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (Many Affine BBOB uses [-5,5] but keep generic)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        span = ub - lb\n        avg_range = float(np.mean(span))\n\n        # initialize mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n\n        # initial step-size and per-dim scales\n        sigma = 0.20 * avg_range\n        scales = np.ones(n)  # per-coordinate multiplicative scale (like D)\n        # small trust-like shrink factors near boundaries (start neutral)\n        trust = np.ones(n)\n\n        # low-rank subspace initialization (orthonormal V, singular scales S)\n        rand_mat = np.random.randn(n, self.r)\n        try:\n            Q, _ = np.linalg.qr(rand_mat)\n            V = Q[:, :self.r]\n        except np.linalg.LinAlgError:\n            V = np.zeros((n, self.r))\n        S = np.ones(self.r)\n\n        # CMA-like path cumulation for sigma adaptation (but simplified)\n        # weights (log-based) for recombination\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        cc = (4.0 + mu_eff / n) / (n + 4.0 + 2.0 * mu_eff / n)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        ps = np.zeros(n)   # cumulative path for sigma\n        pc = np.zeros(n)   # covariance path (not building full cov here)\n\n        # buffers and archive\n        success_buffer = []          # recent successful y steps (in search space)\n        archive_X = []               # archive of evaluated x\n        archive_F = []\n        archive_cap = 10000\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(float(fm))\n            f_opt = float(fm); x_opt = xm.copy()\n            last_improvement_eval = evals\n\n        # helper: sample a Levy (Cauchy) step on the unit sphere\n        def levy_direction(n):\n            # use standard Cauchy scaled onto random direction\n            v = np.random.randn(n)\n            v /= (np.linalg.norm(v) + 1e-20)\n            c = np.random.standard_cauchy()\n            return c * v\n\n        # main optimization loop\n        gen_count = 0\n        success_win_buffer = []  # boolean: whether generation improved\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # mirrored requires even lambda\n            if self.mirrored and (current_lambda % 2 == 1) and current_lambda > 1:\n                current_lambda -= 1\n            if current_lambda <= 0:\n                break\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))  # normalized step vectors (y)\n            arfit = np.full(current_lambda, np.inf)\n\n            # sampling phase: build current_lambda candidates\n            for k_idx in range(current_lambda):\n                # draw Gaussian components\n                z = np.random.randn(n)\n                # low-rank contribution\n                if self.r > 0:\n                    z_low = np.random.randn(self.r)\n                    low = V @ (S * z_low)\n                else:\n                    low = 0.0\n\n                # mixture coefficients vary adaptively: prefer diag for stability\n                alpha = 0.85  # diag dominance\n                beta = 0.8 * (1.0 - alpha)  # low-rank multiplier\n                y_diag = scales * z\n                y = alpha * y_diag + beta * low\n\n                # trust vector: shrink steps near boundaries or previously identified sensitive dims\n                # trust in [0.4,1.0], gradually adjusted below via trust update\n                y = y * trust\n\n                # occasional Levy (large jump)\n                if np.random.rand() < self.p_levy:\n                    y = (np.mean(scales) * self.levy_scale) * levy_direction(n)\n\n                # mirrored sampling: use previous y's negation\n                if self.mirrored and (k_idx % 2 == 1):\n                    y = -prev_y\n\n                x = m + sigma * y\n\n                # DE-like archive difference injection\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # opposition sampling occasionally (diversification)\n                if (np.random.rand() < 0.02) and (len(archive_X) > 0):\n                    # push x toward the opposite of the best in archive relative to center\n                    center = 0.5 * (lb + ub)\n                    best_idx = np.argmin(archive_F)\n                    opp = 2.0 * center - archive_X[best_idx]\n                    x = 0.6 * x + 0.4 * opp\n\n                # clip inside bounds\n                x = np.clip(x, lb, ub)\n\n                arx[k_idx] = x\n                arz[k_idx] = y\n                prev_y = y.copy()\n\n            # evaluation phase (evaluate until budget or done)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = float(f)\n                archive_X.append(x.copy())\n                archive_F.append(float(f))\n                if len(archive_X) > archive_cap:\n                    # simple FIFO cap\n                    del archive_X[0]; del archive_F[0]\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection & recombination\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            idx_sorted = valid_idx[np.argsort(arfit[valid_idx])]\n            sel_count = min(mu, idx_sorted.size)\n            sel_idx = idx_sorted[:sel_count]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            # recombination weights (truncate to sel_count)\n            w = weights[:sel_count].copy()\n            w = w / np.sum(w)\n\n            # new mean and weighted y in search-space (relative steps)\n            m_old = m.copy()\n            m = np.sum(w[:, None] * x_sel, axis=0)\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # compute approximate inverse sqrt \"invsqrt\" using diag+lowrank Sherman-Morrison idea:\n            # approximate covariance ~ diag(scales^2) + V diag(S^2) V^T\n            # for path update we'll use invdiag plus small projection correction:\n            invdiag = 1.0 / (scales + 1e-20)\n            # construct quick invsqrt application: invsqrt @ v ≈ invdiag * v - V * ( ((S^2) * (V^T (invdiag * v))) / (1 + S^2_sum ) )\n            # implement a simple stable version: prefer diag-only but subtract small correlated correction\n            invsqrt = np.diag(invdiag)\n            # apply invsqrt to y_w for ps update\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrt @ y_w)\n            norm_ps = np.linalg.norm(ps)\n\n            # use a conservative hsig like CMA to bias pc update\n            hsig = 1.0 if (norm_ps / np.sqrt(1.0 - (1.0 - cs) ** (2.0 * max(1, evals / max(1, lam)))) / chi_n) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # sigma adaptation: combine cumulation and short-term success-rate controller\n            # cumulation-based multiplicative update:\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            # short-term success-rate control (1/5 like): compute fraction of offspring improving parent\n            # improvement criteria: better than parent's last known f (m_old evaluated?), approximate by archive_best\n            parent_f = f_opt if (len(archive_F) == 0) else min(archive_F[-min(len(archive_F), 50):])\n            success_count = np.sum(np.array(arfit) < parent_f)\n            success_rate = float(success_count) / max(1, current_lambda)\n            # adjust sigma modestly toward target success rate (0.2)\n            target_succ = 0.2\n            sigma *= np.exp(0.6 * (success_rate - target_succ))\n\n            # clamp sigma to reasonable ranges\n            sigma = np.clip(sigma, 1e-12, 1e3 * avg_range + 1e-12)\n\n            # update diagonal scales via EMA of weighted squared steps\n            c_d = 0.24\n            y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n            scales2 = (1.0 - c_d) * (scales ** 2) + c_d * (y2 + 1e-20)\n            scales = np.sqrt(scales2)\n            # prevent extremely small or large per-dim scales\n            scales = np.clip(scales, 1e-8, 1e8)\n\n            # trust update: shrink trust for dims that had repeatedly large relative step sizes\n            # maintain a simple moving counter per-dim: if y magnitude >> scales then reduce trust\n            rel_mag = np.abs(y_w) / (scales + 1e-20)\n            trust = 0.95 * trust + 0.05 * np.clip(1.0 / (1.0 + rel_mag), 0.4, 1.0)\n\n            # push weighted successful step into buffer (shift-mean) to learn subspace\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace via SVD on buffer periodically\n            gen_count += 1\n            if (len(success_buffer) >= max(3, self.r)) and (gen_count % self.svd_every == 0):\n                Y = np.vstack(success_buffer).T  # n x m\n                # center rows\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    # economical SVD\n                    U_new, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.r, U_new.shape[1])\n                    if k_take > 0:\n                        V = U_new[:, :k_take]\n                        # normalize singular values to per-sample scale\n                        S = (svals[:k_take] / (np.sqrt(max(1, Y.shape[1])) + 1e-12)).copy()\n                        # if rank smaller pad S/V\n                        if V.shape[1] < self.r:\n                            pad = np.zeros((n, self.r - V.shape[1]))\n                            V = np.hstack([V, pad])\n                            S = np.concatenate([S, np.ones(self.r - len(S))])\n                except np.linalg.LinAlgError:\n                    # skip update if SVD fails\n                    pass\n\n            # update success indicator for stagnation handling: whether generation produced a better point than previous best\n            gen_improved = np.any(arfit < f_opt)\n            success_win_buffer.append(1 if gen_improved else 0)\n            if len(success_win_buffer) > 20:\n                success_win_buffer.pop(0)\n            # stagnation check: if no improvement for long time -> diversification\n            if (evals - last_improvement_eval) > self.stag_threshold:\n                # gentle restart: nudge mean toward a random archived good point or random location\n                if len(archive_X) > 0 and (np.random.rand() < 0.8):\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                else:\n                    # opposition-based randomization around best\n                    if x_opt is not None:\n                        center = 0.5 * (lb + ub)\n                        opp = 2.0 * center - x_opt\n                        m = 0.6 * m + 0.4 * opp\n                    else:\n                        m = np.random.uniform(lb, ub)\n                # inflate sigma modestly and reset some internal buffers\n                sigma *= 2.0\n                scales = 0.8 * scales + 0.2 * (1.0 + 0.05 * np.random.randn(n))\n                ps *= 0.5\n# Description: Adaptive Multi-Scale Subspace Evolution with Levy restarts (AMSS-E) — combines per-coordinate EMA scales and trust radii, an online low-rank principal subspace learned from successful steps, mixed Gaussian/Levy/DE perturbations, mirrored sampling, and a two-component sigma controller (path-cumulation + short-term success-rate).\n\n# Code: ", "configspace": "", "generation": 0, "feedback": "The algorithm AMSS_E scored 0.169 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e157d1cc-26d6-4b5f-b0e2-deda091f662a", "operator": null, "metadata": {"aucs": [0.0895403316522676, 0.16744649978436354, 0.2281697609813994, 0.1811738135201899, 0.11649805884813624, 0.22946454776948577, 0.2177186598092018, 0.1839222198539483, 0.14769691928310114, 0.12768111680498162]}, "task_prompt": ""}
{"id": "0ec60cb5-02c1-4979-9105-5c8bc2dd5ffa", "fitness": 0.4564022442177816, "name": "AOSE", "description": "The algorithm is a hybrid CMA‑like explorer that combines a per‑coordinate diagonal scaling D with a learned low‑rank orthonormal subspace U (with singular scales S) and uses a moderate population size (lambda scaled with sqrt(n)) and exponential rank weights to recombine mu parents. Sampling mixes an alpha_diag‑weighted diagonal Gaussian and a low‑rank projection (alpha_diag=0.75) with mirrored pairing, occasional heavy‑tailed Cauchy jumps, DE‑style archive difference injections, and rare orthogonal projection jumps to encourage diverse moves. Adaptation uses evolution paths ps and pc with modified cs/cc/damps and a conservative hsig test to update sigma (nonlinear exponent) and updates D with a weak EMA (c_d=0.15); an approximate invsqrt operator falls back to diagonal but is replaced by a periodic eigendecomposition of a cov_approx when available. The algorithm continuously learns U,S from a limited success_buffer via SVD, keeps an archive for DE and restarts nudges, and triggers stagnation recovery (inflate sigma, convexly nudge the mean to an archive point, randomize D and shrink paths) to re‑diversify the search.", "code": "import numpy as np\n\nclass AOSE:\n    \"\"\"\n    Adaptive Orthogonal Subspace Explorer (AOSE)\n\n    One-line: Blend of diagonal scaling + learned orthogonal low-rank subspace,\n    with altered sampling mixes, alternative EMA / step-size equations, and\n    varied exploration hooks (Cauchy, DE differences, orthogonal jumps).\n    Designed for bounded continuous optimization (e.g., Many Affine BBOB).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population settings (different heuristic than original)\n        # slightly larger base pop for small dims, scales with sqrt(n) but different factor\n        self.lambda_ = max(6, int(6 + np.floor(2.0 * np.sqrt(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace dimension: different scaling (n^0.6) by default to favor larger low-rank\n        if subspace_k is None:\n            self.k = max(0, int(np.ceil(self.dim ** 0.6)))\n            self.k = min(self.k, self.dim)\n        else:\n            self.k = min(max(0, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (BBOB style -5..5 typically)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        span = ub - lb\n        avg_range = float(np.mean(span))\n\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n\n        # alternative weighting: exponential rank weights (different equation)\n        ranks = np.arange(1, mu + 1)\n        decay_scale = max(0.6, 0.5 + 0.5 * (mu / max(1, n)))  # different scaling\n        weights = np.exp(- (ranks - 1) / (decay_scale * mu))\n        weights /= np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length / adaptation constants (modified)\n        cs = 0.28 * (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        cc = 0.27 * (4.0 + mu_eff / n) / (n + 4.0 + 2.0 * mu_eff / n)\n        damps = 1.0 + 1.5 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 0.5) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.15 * avg_range  # different initial step-size (smaller)\n        # diagonal scale approximation (per-coordinate std estimates)\n        D = np.ones(n)\n\n        # low-rank subspace U and singular scales S (orthonormal columns)\n        if self.k > 0:\n            rand_mat = np.random.randn(n, self.k)\n            # use QR for orthonormal U\n            try:\n                Q, _ = np.linalg.qr(rand_mat)\n                U = Q[:, :self.k].copy()\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n            S = np.ones(self.k) * 0.8  # initial subspace scales (different)\n        else:\n            U = np.zeros((n, 0))\n            S = np.zeros(0)\n\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n\n        # buffers and archive\n        success_buffer = []\n        buffer_max = max(8 * max(1, self.k), 20)  # changed buffer sizing\n        archive_X = []\n        archive_F = []\n\n        # exploration knobs (modified probabilities/scales)\n        p_cauchy = 0.15       # slightly increased cauchy prob\n        cauchy_scale = 0.7    # smaller scale for heavy tails\n        p_de = 0.25           # more frequent DE-style mutations\n        F_de = 0.6            # DE factor changed\n        p_ortho = 0.08        # occasional orthogonal projection jumps\n        mirrored = True\n\n        # reconditioning / eigdecomp frequency (different)\n        eig_every = max(40, 8 * n)\n        eigen_counter = 0\n        B_full = np.eye(n)\n        D_full = np.ones(n)\n        invsqrtC_full = np.eye(n)\n        have_full = False\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n\n        # initial evaluation at mean if budget permits\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_opt = float(fm)\n            x_opt = xm.copy()\n            last_improvement_eval = evals\n\n        prev_y = np.zeros(n)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # mirrored pairing enforcement\n            if mirrored and (current_lambda % 2 == 1) and current_lambda > 1:\n                current_lambda -= 1\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n\n            # dynamic mixing coefficients (slightly different formula)\n            alpha_diag = 0.75  # reduced diag dominance vs original's 0.9\n            low_scale_mult = 0.95  # low-rank amplitude multiplier\n\n            for k_idx in range(current_lambda):\n                # independent standard normal for diag part\n                z = np.random.randn(n)\n\n                # low-rank projection\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U @ (S * z_low)\n                else:\n                    low = np.zeros(n)\n\n                # mix diag and low-rank. Use median of D for scaling of low-rank, different mixing\n                medD = np.median(D)\n                y = alpha_diag * (D * z) + low_scale_mult * medD * low\n\n                # occasional orthogonal exploration: push component orthogonal to U (if k>0)\n                if (self.k > 0) and (np.random.rand() < p_ortho):\n                    # project z onto orthogonal complement approx: use z - U(U^T z)\n                    proj = U @ (U.T @ z)\n                    ortho = z - proj\n                    if np.linalg.norm(ortho) > 1e-12:\n                        ortho = ortho / np.linalg.norm(ortho)\n                        magnitude = 0.9 * np.mean(D)\n                        y += magnitude * ortho * np.random.randn()\n\n                # heavy-tailed jump occasionally (different scaling)\n                if np.random.rand() < p_cauchy:\n                    r = np.random.standard_cauchy() * cauchy_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * np.median(D)\n\n                # mirrored sampling: second of pair is negative previous\n                if mirrored and (k_idx % 2 == 1):\n                    y = -prev_y\n\n                x = m + sigma * y\n\n                # DE-style archive difference injection\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                x = np.clip(x, lb, ub)\n                arx[k_idx] = x\n                arz[k_idx] = y\n                prev_y = y.copy()\n\n            # evaluate candidates (respect budget)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                # cap archive size to avoid memory bloat\n                if len(archive_X) > 6000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection: choose best mu (or fewer if limited)\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            idx_sorted = valid_idx[np.argsort(arfit[valid_idx])]\n            sel_count = min(mu, idx_sorted.size)\n            sel_idx = idx_sorted[:sel_count]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            # adjust weights if fewer selected than mu\n            w = weights[:sel_count].copy()\n            w = w / np.sum(w)\n\n            # recombine mean (weighted)\n            m_old = m.copy()\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # weighted step in y-space\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # choose invsqrt approx: prefer full if available else diag-based (different fallback)\n            if have_full:\n                invsqrt = invsqrtC_full\n            else:\n                invsqrt = np.diag(1.0 / (D + 1e-20))\n\n            # evolution path update (modified prefactor)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrt @ y_w)\n            norm_ps = np.linalg.norm(ps)\n\n            # alternative hsig criterion (more conservative)\n            denom = np.sqrt(1.0 - (1.0 - cs) ** (2.0 * max(1, evals / max(1, lam)))) + 1e-20\n            hsig = 1.0 if (norm_ps / denom / chi_n) < (1.3 + 1.8 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # step-size adaptation (different damping / exponent)\n            sigma *= np.exp((cs / damps) * ( (norm_ps / chi_n) ** 0.9 - 1.0 ))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e3 * avg_range + 1e-12)\n\n            # update diagonal D with a different EMA constant (weaker smoothing)\n            c_d = 0.15\n            y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n\n            # push weighted successful step into buffer (for subspace learning)\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace occasionally using SVD on centered buffer (different frequency/normalization)\n            svd_every = max(1, min(12, n // 2))\n            if (self.k > 0) and (len(success_buffer) >= self.k) and (evals % svd_every == 0):\n                Y = np.vstack(success_buffer).T  # n x m\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    U_new, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        # normalize singular values by sqrt(count) with a different bias\n                        S = (svals[:k_take] / (np.sqrt(max(1, Y.shape[1])) + 0.5)).copy()\n                        # if less columns than self.k, pad to keep shapes stable\n                        if U.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U.shape[1]))\n                            U = np.hstack([U, pad])\n                            S = np.concatenate([S, np.ones(self.k - len(S)) * 0.5])\n                    else:\n                        # fallback: small random orthonormal directions\n                        rand_mat = np.random.randn(n, self.k)\n                        try:\n                            Q, _ = np.linalg.qr(rand_mat)\n                            U = Q[:, :self.k]\n                        except np.linalg.LinAlgError:\n                            pass\n                except np.linalg.LinAlgError:\n                    pass\n\n            # periodic approximate covariance eigendecomposition (different threshold)\n            eigen_counter += current_lambda\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                try:\n                    cov_approx = np.diag(D ** 2)\n                    if self.k > 0:\n                        cov_approx = cov_approx + (U @ np.diag(S ** 2) @ U.T)\n                    cov_approx = 0.5 * (cov_approx + cov_approx.T)\n                    Dvals, B = np.linalg.eigh(cov_approx)\n                    Dvals = np.maximum(Dvals, 1e-20)\n                    D_full = np.sqrt(Dvals)\n                    B_full = B\n                    invsqrtC_full = (B_full * (1.0 / D_full)) @ B_full.T\n                    have_full = True\n                except np.linalg.LinAlgError:\n                    have_full = False\n                    B_full = np.eye(n); D_full = np.ones(n); invsqrtC_full = np.eye(n)\n\n            # numerical protection for D\n            if np.any(np.isnan(D)) or (np.any(D <= 0)):\n                D = np.ones(n) * max(1e-6, np.median(np.abs(D) + 1e-6))\n\n            # stagnation detection and mild diversification (different thresholds/behavior)\n            stagnation_threshold = max(60, 12 * n)\n            if (evals - last_improvement_eval) > stagnation_threshold:\n                sigma *= 1.6\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    # use a different convex combination to nudge mean\n                    m = 0.55 * m + 0.45 * archive_X[pick]\n                # slightly randomize diagonal scales and shrink paths\n                D = 0.85 * D + 0.15 * (1.0 + 0.2 * np.random.randn(n))\n                ps *= 0.4\n                pc *= 0.4\n                success_buffer = []\n                last_improvement_eval = evals\n\n            # keep mean inside bounds\n            m = np.clip(m, lb, ub)\n\n            # guard - end if budget reached\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AOSE scored 0.456 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e157d1cc-26d6-4b5f-b0e2-deda091f662a", "operator": null, "metadata": {"aucs": [0.2102934937268327, 0.1698506175631116, 0.43802095662864615, 0.9870595580255154, 0.5407379511525177, 0.7054947339236561, 0.25638435719730257, 0.5565878487617739, 0.284010340500526, 0.41558258469793297]}, "task_prompt": ""}
{"id": "da12c32e-0612-44a5-8560-95085a0e382d", "fitness": 0.3289241757187239, "name": "AdaptiveEnsembleSubspaceSearch", "description": "The algorithm combines cheap per-coordinate adaptive scaling (diagonal D with EMA c_d=0.18 and initialization sigma=0.25*avg_range) with a learned low-rank correlated subspace U (k ≈ ceil(sqrt(n)), Oja updates with eta_oja≈0.08 and small orthogonality regularization) so it can exploit both axis-aligned and correlated directions. It runs an ensemble of generators — diag-Gaussian, subspace-Gaussian (scaled by S), DE-style difference jumps (p_de≈0.20, F_de≈0.65), and occasional linear-surrogate gradient proposals — allocated by a softmax of recent reward signals, uses mirrored sampling and rare Cauchy heavy tails (p_cauchy≈0.08) to balance exploration/exploitation, and keeps an archive for DE and surrogate fitting. Mean recombination uses log-rank-style weights on the top half of candidates, sigma is adapted by a success-ratio rule (grow/shrink by ~1.08/0.92) plus a path-length signal ps (smoothing cs≈0.2), and D, U, S are updated from the weighted selected steps; stagnation triggers diversification (inflate sigma, randomize D, nudge mean). Practical safeguards include bounds clipping, a limited archive, surrogate_every and surrogate_min thresholds, mirrored-even population handling, caps on sigma and S, and small constants to avoid numerical issues.", "code": "import numpy as np\n\nclass AdaptiveEnsembleSubspaceSearch:\n    \"\"\"\n    Adaptive Ensemble Subspace Search (AESS)\n\n    Key ideas:\n    - Maintain per-coordinate scales D (cheap diagonal preconditioning).\n    - Maintain a low-rank subspace U (n x k) and scale vector S via online Oja updates\n      (cheap incremental PCA) to capture correlated productive directions.\n    - Use an ensemble of candidate generators (diag-Gaussian, subspace-Gaussian,\n      DE-difference jumps, and occasional linear-model informed step), with a softmax\n      allocation that is updated by recent rewards (improvement magnitudes).\n    - Use mirrored sampling, occasional Cauchy heavy tails for global jumps,\n      and simple step-size control based on success ratio + a path-length signal.\n    - Keep an archive of evaluated points to support DE moves and a small linear\n      regression surrogate used occasionally for gradient-like proposals.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, k_subspace=None, pop_factor=1.5):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # subspace dimension: default ~ ceil(sqrt(n))\n        if k_subspace is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(k_subspace)), self.dim)\n        # base population size scales mildly with dimension\n        self.lambda_base = max(4, int(np.round(pop_factor * (4 + np.sqrt(self.dim)))))\n        # internal RNG\n        self.rng = np.random.RandomState(self.seed)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds from func or default to [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0\n            ub = 5.0\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        span = ub - lb\n        avg_range = float(np.mean(span))\n\n        # initial mean and scales\n        m = self.rng.uniform(lb, ub)               # current mean\n        sigma = 0.25 * avg_range                   # global step-size\n        D = np.ones(n)                             # per-coordinate scale (std proxy)\n\n        # Oja's rule subspace U and scale S\n        U = np.zeros((n, self.k))\n        # initialize with random orthonormal basis if k>0\n        if self.k > 0:\n            R = self.rng.randn(n, self.k)\n            Q, _ = np.linalg.qr(R)\n            U[:, :Q.shape[1]] = Q[:, :self.k]\n        S = np.ones(self.k) * 0.5  # initial subspace amplitude\n\n        # strategy set: 0 diag, 1 subspace, 2 DE-diff, 3 linear-model\n        n_strat = 4\n        # softmax log-weights for strategies (higher -> more sampling)\n        strat_scores = np.zeros(n_strat)\n        strat_counts = np.ones(n_strat) * 1e-3  # to avoid div by zero\n\n        # DE parameters\n        p_de = 0.20\n        F_de = 0.65\n\n        # cauchy heavy tail\n        p_cauchy = 0.08\n        cauchy_scale = 1.0\n\n        # mirrored sampling\n        mirrored = True\n\n        # infrastructure\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        archive_X = []\n        archive_F = []\n        archive_cap = max(2000, 20 * n)\n\n        # buffers for step-size success\n        recent_success = []\n        success_window = max(20, 5 * n)\n\n        # Oja learning rate\n        eta_oja = 0.08  # modest\n        # small regularization for orthonormality\n        ortho_reg = 1e-3\n\n        # linear surrogate frequency and minimum data required\n        surrogate_every = max(50, 5 * n)\n        surrogate_min = 3 * n\n\n        # path signal (for additional sigma modulation)\n        ps = np.zeros(n)\n        cs = 0.2  # small smoothing for path\n\n        # initial evaluation\n        if evals < budget:\n            x0 = np.clip(m, lb, ub)\n            f0 = func(x0)\n            evals += 1\n            archive_X.append(x0.copy()); archive_F.append(float(f0))\n            f_opt = float(f0); x_opt = x0.copy()\n\n        iteration = 0\n        prev_y = np.zeros(n)\n\n        # main loop\n        while evals < budget:\n            iteration += 1\n            remaining = budget - evals\n            lam = min(self.lambda_base, remaining)\n            # ensure even for mirrored\n            if mirrored and (lam % 2 == 1) and lam > 1:\n                lam -= 1\n            if lam <= 0:\n                break\n\n            # derive strategy probabilities via softmax of strat_scores\n            probs = np.exp(strat_scores - np.max(strat_scores))\n            probs /= probs.sum()\n            # generate candidates\n            Xcand = np.zeros((lam, n))\n            Ysteps = np.zeros((lam, n))  # y steps in normalized coordinates (x = m + sigma*y)\n            strat_used = np.zeros(lam, dtype=int)\n            for i in range(lam):\n                # choose strategy\n                s = self.rng.choice(n_strat, p=probs)\n                strat_used[i] = s\n                # mirrored handling: odd indices mirror previous y\n                if mirrored and (i % 2 == 1):\n                    y = -prev_y.copy()\n                else:\n                    # baseline random draws\n                    if s == 0:  # diag-Gaussian\n                        z = self.rng.randn(n)\n                        alpha = 0.9\n                        y = alpha * (D * z) + 0.08 * np.mean(D) * self.rng.randn(n)\n                    elif s == 1 and self.k > 0:  # subspace-Gaussian (along learned U)\n                        z_k = self.rng.randn(self.k)\n                        y_sub = U @ (S * z_k)\n                        # add small diag noise so purely subspace doesn't blindside\n                        y = 0.92 * y_sub + 0.12 * (D * self.rng.randn(n))\n                    elif s == 2 and len(archive_X) >= 2 and (self.rng.rand() < p_de):\n                        # DE-difference mutation around mean: draw difference from archive\n                        i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                        diff = np.array(archive_X[i1]) - np.array(archive_X[i2])\n                        # scale to represent y-space relative to sigma; add noise\n                        y = (F_de * diff) / max(1e-12, sigma) + 0.12 * (D * self.rng.randn(n))\n                    elif s == 3 and len(archive_X) >= surrogate_min and (iteration % surrogate_every == 0):\n                        # linear surrogate: fit least squares to recent archive to get rough gradient\n                        # use a small random subset to limit cost\n                        idxs = self.rng.choice(len(archive_X), size=min(len(archive_X), 5 * n), replace=False)\n                        XA = np.vstack([archive_X[i] for i in idxs])\n                        yA = np.array([archive_F[i] for i in idxs])\n                        # center\n                        XA_c = XA - XA.mean(axis=0)\n                        yA_c = yA - yA.mean()\n                        # solve ridge regression for g approx: minimize ||XA_c @ g - yA_c||\n                        try:\n                            ridge = 1e-6\n                            # solve normal equations\n                            A = XA_c.T @ XA_c + ridge * np.eye(n)\n                            g = np.linalg.solve(A, XA_c.T @ yA_c)\n                            # propose negative gradient step (in y-space)\n                            step_len = 0.6 * sigma / (np.linalg.norm(g) + 1e-12)\n                            y = -step_len * g\n                        except np.linalg.LinAlgError:\n                            y = D * self.rng.randn(n)\n                    else:\n                        # fallback to diag\n                        y = D * self.rng.randn(n)\n\n                    # occasional heavy tail\n                    if self.rng.rand() < p_cauchy:\n                        r = self.rng.standard_cauchy() * cauchy_scale\n                        z = self.rng.randn(n)\n                        znorm = np.linalg.norm(z) + 1e-12\n                        y = r * (z / znorm) * np.mean(D)\n\n                prev_y = y.copy()\n                x = m + sigma * y\n\n                # boundary clip\n                x = np.clip(x, lb, ub)\n                Xcand[i] = x\n                Ysteps[i] = y\n\n            # Evaluate candidates sequentially (stop if budget exhausted)\n            Fcand = np.full(lam, np.inf)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = Xcand[i]\n                f = func(x)\n                evals += 1\n                Fcand[i] = float(f)\n                # archive\n                archive_X.append(x.copy()); archive_F.append(float(f))\n                if len(archive_X) > archive_cap:\n                    # pop oldest\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f); x_opt = x.copy()\n                    # record as success\n                    recent_success.append(1)\n                else:\n                    recent_success.append(0)\n                # maintain recent success window\n                if len(recent_success) > success_window:\n                    recent_success.pop(0)\n\n            # select top performers (mu selection)\n            valid_idx = np.where(np.isfinite(Fcand))[0]\n            if valid_idx.size == 0:\n                break\n            idx_sorted = valid_idx[np.argsort(Fcand[valid_idx])]\n            mu = max(1, int(np.ceil(0.5 * len(idx_sorted))))  # choose half as recombination set (heuristic)\n            sel_idx = idx_sorted[:mu]\n            x_sel = Xcand[sel_idx]\n            y_sel = Ysteps[sel_idx]\n            f_sel = Fcand[sel_idx]\n\n            # recombine mean: use log-rank-like weights (favor better)\n            ranks = np.arange(1, mu + 1)\n            w = np.log(mu + 0.5) - np.log(ranks)\n            w = w / np.sum(w)\n            m_old = m.copy()\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # combined y in y-space\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # update path for sigma modulation\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu) * y_w\n            norm_ps = np.linalg.norm(ps)\n\n            # success-based reward signals for strategies: reward = positive improvement relative to m_old baseline\n            # baseline: f(m_old) is approximated by best-known f_opt at start of iteration; we also check candidates improvement\n            # reward for each strategy is average positive improvement among its selected candidates\n            strat_rewards = np.zeros(n_strat)\n            strat_reward_counts = np.zeros(n_strat)\n            for j in range(len(sel_idx)):\n                idx = sel_idx[j]\n                strat = strat_used[idx]\n                # improvement relative to m_old projected value (approx): we don't have f(m_old) freshly, compare to current best\n                # but we can approximate improvement as max(0, f_best_before - f_candidate)\n                # use archive last best before evaluating this batch as baseline: f_opt prior to this batch is approximated by existing f_opt\n                improv = max(0.0, (f_opt - f_sel[j]))\n                # an alternative signal: if candidate improved global best, give larger reward\n                strat_rewards[strat] += improv\n                strat_reward_counts[strat] += 1.0\n\n            # normalize and update strat_scores (exponential recency)\n            for s in range(n_strat):\n                if strat_reward_counts[s] > 0:\n                    avg_r = strat_rewards[s] / strat_reward_counts[s]\n                    # increase log-score proportionally to avg reward (small learning rate)\n                    strat_scores[s] = 0.85 * strat_scores[s] + 0.15 * avg_r\n                    strat_counts[s] += strat_reward_counts[s]\n                else:\n                    # decay unused strategies mildly\n                    strat_scores[s] *= 0.995\n\n            # step-size control: combine success ratio and ps signal\n            succ_ratio = (np.sum(recent_success) + 1e-12) / max(1, len(recent_success))\n            # increase sigma if succ_ratio > 0.25, decrease if less; also modulate by ps norm compared to dimension\n            if succ_ratio > 0.25:\n                sigma *= 1.08\n            elif succ_ratio < 0.15:\n                sigma *= 0.92\n            # additional gentle adjustment from ps norm\n            sigma *= np.exp(0.03 * (norm_ps / (np.sqrt(n) + 1e-12) - 1.0))\n            # safety clamps\n            sigma = np.clip(sigma, 1e-12, 1e3 * avg_range + 1e-12)\n\n            # update diagonal D via EMA of squared y of selected set (like a cheap covariance diag)\n            c_d = 0.18\n            y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n\n            # Oja update for subspace U based on weighted y_w (centered)\n            # We treat y_w as sample vector; update k components with Oja's rule and re-orthonormalize\n            if self.k > 0:\n                v = y_w.copy()\n                # small normalization\n                v = v / (np.linalg.norm(v) + 1e-12)\n                for j in range(self.k):\n                    uj = U[:, j]\n                    # Oja update: uj <- uj + eta * ( (uj^T v) * v - (uj*(uj^T v)^2) )  (variant to keep norms)\n                    proj = np.dot(uj, v)\n                    uj += eta_oja * (proj * v - (proj ** 2) * uj) - ortho_reg * uj\n                    # normalize\n                    norm_uj = np.linalg.norm(uj) + 1e-12\n                    U[:, j] = uj / norm_uj\n                # re-orthonormalize U via QR to reduce drift\n                try:\n                    Q, R = np.linalg.qr(U)\n                    U[:, :Q.shape[1]] = Q[:, :self.k]\n                except Exception:\n                    # fallback: normalize columns\n                    for j in range(self.k):\n                        U[:, j] /= (np.linalg.norm(U[:, j]) + 1e-12)\n                # update S as magnitudes of projection of recent top steps onto components\n                # small heuristic: S_j <- 0.9*S_j + 0.1*|U_j^T y_w|\n                proj_mag = np.abs(U.T @ y_w)\n                S = 0.9 * S + 0.1 * proj_mag\n                # clamp S\n                S = np.clip(S, 1e-6, 10.0)\n\n            # stagnation detection: if no improvement for long time, diversify\n            stagnation_thresh = max(100, 20 * n)\n            if evals - np.argmin(archive_F) > stagnation_thresh if len(archive_F) > 0 else False:\n                # diversify: inflate sigma and randomize some D\n                sigma *= 1.7\n                D = 0.7 * D + 0.3 * (1.0 + 0.2 * self.rng.randn(n))\n                # nudge mean to a random good archive entry\n                if len(archive_X) > 0:\n                    pick = self.rng.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n\n            # guard D sanity\n            if np.any(np.isnan(D)) or np.any(D <= 0):\n                D = np.ones(n) * max(1e-6, np.median(D[np.isfinite(D)] if np.any(np.isfinite(D)) else np.array([1.0])))\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # end looping if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveEnsembleSubspaceSearch scored 0.329 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e157d1cc-26d6-4b5f-b0e2-deda091f662a", "operator": null, "metadata": {"aucs": [0.12772983569009466, 0.1674929190245048, 0.42034257580083667, 0.38808766016879936, 0.2511731185208851, 0.602246996506381, 0.2519512429953986, 0.2234286349757203, 0.6880017200621081, 0.168787053442511]}, "task_prompt": ""}
{"id": "e203d67b-755d-436d-aba3-70542162dc0d", "fitness": 0.2082043411047098, "name": "DSL_DE", "description": "DSL-DE is a hybrid heuristic that combines cheap diagonal preconditioning (per-coordinate scales D, initialized conservatively and updated via a MAD-like smoothing) with a rolling low-rank subspace (U,S from SVD of a success buffer) to capture correlated moves, while occasionally reconditioning a full covariance approximation by eigendecomposition. Sampling mixes a scaled diagonal Gaussian component and the low-rank projection, uses mirrored pairs for variance reduction, and injects heavy-tailed Pareto/Levy jumps (p_levy≈0.08) and DE-style archive differences (p_de≈0.20, F_de≈0.6) to promote exploration. Population is modest (lambda grows slowly with dim, mu≈lambda/3) with power-law selection weights and convex recombination of parents; step-size sigma is adapted by a success-rate rule (target ~0.25) rather than a CMA-ES path-only law. Practical robustness features include an archive, bounded sigma, occasional evaluation of the mean, SVD cadence and buffer limits, and stagnation detection that inflates sigma, perturbs mean/D/paths, and partially restarts the low-rank subspace.", "code": "import numpy as np\n\nclass DSL_DE:\n    \"\"\"\n    DSL-DE: Diagonal-preconditioned Subspace + Levy-Differential Evolution hybrid.\n    - Diagonal per-coordinate scaling (cheap)\n    - Rolling-buffer SVD builds a low-rank subspace for correlated moves\n    - Mirrored sampling, Levy/Pareto heavy-tailed jumps, and DE-difference archive injections\n    - Success-rate based sigma adaptation (different from CMA-ES step-size law)\n    - Stagnation detection with modest diversification\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population and selection heuristics (different choices from the provided algorithm)\n        self.lambda_ = max(6, int(6 + np.floor(2.0 * np.log(self.dim))))   # a bit smaller growth with dim\n        self.mu = max(1, int(np.ceil(self.lambda_ / 3.0)))                  # more conservative parent set\n\n        # subspace dimension: slightly smaller than sqrt(n) by default\n        if subspace_k is None:\n            self.k = max(1, min(self.dim, int(np.maximum(1, np.round(np.sqrt(self.dim) * 0.8)))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (Many Affine BBOB uses [-5,5], but read from func to be general)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        span = ub - lb\n        avg_range = float(np.mean(span))\n\n        # population params\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n\n        # different weight scheme (power law weights, not log-weights)\n        raw_w = np.arange(1.0, mu + 1.0) ** (-1.2)\n        weights = raw_w / np.sum(raw_w)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path constants (alternative to original)\n        cs = (mu_eff + 3.0) / (n + mu_eff + 6.0)\n        cc = 4.0 / (n + 4.0)\n        damps = 1.0 + 0.3 * cs + 0.2 * (mu_eff / n)\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.18 * avg_range  # slightly different initial scale\n        # diagonal per-coordinate scaling (start conservative)\n        D = np.maximum(1e-8, np.ones(n) * (0.35 * avg_range))\n\n        # low-rank subspace initialization (orthonormal)\n        if self.k >= 1:\n            rand = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand)\n                U = Q[:, :self.k].copy()\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n        S = np.ones(self.k) * 0.5\n\n        # evolution paths\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n\n        # buffers and archive\n        success_buffer = []             # store successful weighted y steps (in y-space)\n        buffer_max = max(5 * self.k, 20)   # different from original (smaller multiple)\n        archive_X = []\n        archive_F = []\n\n        # exploration knobs (changed)\n        p_levy = 0.08            # chance of heavy-tailed jump\n        pareto_alpha = 1.5       # Pareto tail approx (1.5 => heavy tail)\n        levy_scale = 1.0\n        p_de = 0.20              # DE-style injection probability\n        F_de = 0.6               # DE difference scale\n        mirrored = True\n\n        # reconditioning schedule (different frequency)\n        eig_every = max(40, 8 * n)\n        eigen_counter = 0\n        have_full = False\n        B_full = np.eye(n)\n        D_full_vals = np.ones(n)\n        invsqrtC_full = np.eye(n)\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n\n        # track mean's evaluated value when evaluated (not every generation)\n        f_mean = None\n\n        # initial evaluate mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_opt = float(fm)\n            x_opt = xm.copy()\n            last_improvement_eval = evals\n            f_mean = float(fm)\n\n        prev_y = np.zeros(n)\n\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # ensure mirrored pairing\n            if mirrored and (current_lambda % 2 == 1) and current_lambda > 1:\n                current_lambda -= 1\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # sampling generation\n            for k_idx in range(current_lambda):\n                # diag component\n                z = np.random.randn(n)\n                # low-rank component\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U @ (S * z_low)\n                else:\n                    low = 0.0\n\n                alpha_diag = 0.85\n                beta_low = 0.8 * np.mean(D)\n                y = alpha_diag * (D * z) + beta_low * low\n\n                # occasional Pareto/Levy-like heavy tail injection (approx via Pareto)\n                if np.random.rand() < p_levy:\n                    # symmetric Pareto: magnitude ~ (1 + pareto(alpha)), random sign\n                    r = (np.random.pareto(pareto_alpha) + 1.0) * levy_scale\n                    signs = np.sign(np.random.randn(n))\n                    # scale by diag magnitudes to keep units\n                    y = signs * r * (np.mean(D) * 0.5)\n\n                # mirrored sampling\n                if mirrored and (k_idx % 2 == 1):\n                    y = -prev_y\n\n                x = m + sigma * y\n\n                # DE-style archive difference injection\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                x = np.clip(x, lb, ub)\n                arx[k_idx] = x\n                arz[k_idx] = y\n                prev_y = y.copy()\n\n            # evaluate offspring\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(float(f))\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection and recombination\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            idx_sorted = valid_idx[np.argsort(arfit[valid_idx])]\n            sel_count = min(mu, idx_sorted.size)\n            sel_idx = idx_sorted[:sel_count]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            # normalized weights for current selection (may be fewer than mu)\n            w = weights[:sel_count].copy()\n            if w.sum() <= 0:\n                w = np.ones(sel_count) / sel_count\n            else:\n                w = w / np.sum(w)\n\n            # recombine mean (different blending: convex recombination)\n            m_old = m.copy()\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space (relative to old mean in units of y)\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # choose invsqrtC: prefer full if available else diag-based\n            if have_full:\n                invsqrt = invsqrtC_full\n            else:\n                invsqrt = np.diag(1.0 / (D + 1e-20))\n\n            # update evolution paths (similar spirit but different constants)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrt @ y_w)\n            norm_ps = np.linalg.norm(ps)\n            # hsig: slightly different thresholding\n            hsig = 1.0 if (norm_ps / np.sqrt(1.0 - (1.0 - cs) ** (2.0 * max(1, evals / max(1, lam)))) / chi_n) < (1.5 + 1.0 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # success-rate based sigma adaptation (novel compared to CMA-like only)\n            # success measured as fraction of offspring improving over current best mean if known else global best\n            base_f = f_mean if (f_mean is not None) else f_opt\n            success_rate = np.mean(arfit[valid_idx] < base_f) if valid_idx.size > 0 else 0.0\n            p_target = 0.25\n            adapt_strength = 0.3 / (1.0 + n / 20.0)\n            sigma *= np.exp(adapt_strength * (success_rate - p_target))\n            # safety bounds on sigma\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e4 * avg_range + 1e-12)\n\n            # update diagonal D via a MAD-like smoothing (different from EMA squared)\n            y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n            # use sqrt of mean square with smoothing\n            newD = np.sqrt(y2 + 1e-20)\n            D = 0.92 * D + 0.08 * (newD + 1e-12)\n            # prevent degenerate D\n            D = np.maximum(D, 1e-8)\n\n            # push weighted successful step into buffer (weighted by sigma to keep scale)\n            success_buffer.append((y_w * 1.0).copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace via SVD on buffer periodically (different cadence)\n            if (len(success_buffer) >= max(2, self.k)) and (evals % max(1, min(8, n // 2 + 1)) == 0):\n                Y = np.vstack(success_buffer).T  # shape n x m\n                # center and normalize by simple scaling\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    # use SVD but take top-k; normalize singulars differently\n                    U_new, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take].copy()\n                        # scale singular values to represent per-step typical magnitude (divide by sqrt(count) but with different exponent)\n                        S = (svals[:k_take] / (np.sqrt(max(1, Y.shape[1])) + 1e-12)) ** 0.9\n                        # pad if needed\n                        if U.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U.shape[1]))\n                            U = np.hstack([U, pad])\n                            S = np.concatenate([S, np.ones(self.k - len(S)) * 0.3])\n                except np.linalg.LinAlgError:\n                    # ignore SVD failures; keep old subspace\n                    pass\n\n            # occasionally recondition full approx covariance (different frequency and combination)\n            eigen_counter += current_lambda\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                try:\n                    cov_approx = np.diag((D ** 2) + 1e-20)\n                    if self.k > 0:\n                        cov_approx += (U @ np.diag(S ** 2) @ U.T)\n                    cov_approx = 0.5 * (cov_approx + cov_approx.T)\n                    Dvals, B = np.linalg.eigh(cov_approx)\n                    Dvals = np.maximum(Dvals, 1e-20)\n                    D_full_vals = np.sqrt(Dvals)\n                    B_full = B\n                    invsqrtC_full = (B_full * (1.0 / D_full_vals)) @ B_full.T\n                    have_full = True\n                except np.linalg.LinAlgError:\n                    have_full = False\n                    B_full = np.eye(n)\n                    D_full_vals = np.ones(n)\n                    invsqrtC_full = np.eye(n)\n\n            # small protection for numerical issues\n            if np.any(np.isnan(D)) or np.any(D <= 0):\n                D = np.ones(n) * max(1e-6, np.median(np.abs(D[np.isfinite(D)]) if np.any(np.isfinite(D)) else 1.0))\n\n            # small update to f_mean: occasionally evaluate or approximate mean's fitness if budget allows and beneficial\n            # We evaluate the new mean only if we have spare budget and a good chance to improve (heuristic)\n            if (evals < budget) and (np.random.rand() < 0.04):\n                xm = np.clip(m, lb, ub)\n                fm = func(xm)\n                evals += 1\n                archive_X.append(xm.copy()); archive_F.append(float(fm))\n                if fm < f_opt:\n                    f_opt = float(fm); x_opt = xm.copy(); last_improvement_eval = evals\n                f_mean = float(fm)\n\n            # stagnation detection and modest restart (different thresholds)\n            stagnation_threshold = 80 + 5 * n\n            if (evals - last_improvement_eval) > stagnation_threshold:\n                # inflate sigma and push mean towards a random archive point\n                sigma *= 1.6\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                # slightly randomize D and damp paths\n                D = 0.85 * D + 0.15 * (0.5 * avg_range * (1.0 + 0.2 * np.random.randn(n)))\n                ps *= 0.5\n                pc *= 0.5\n                success_buffer = []\n                last_improvement_eval = evals\n                # also perturb low-rank subspace a bit\n                if self.k > 0:\n                    perturb = 0.05 * np.random.randn(*U.shape)\n                    try:\n                        Q, _ = np.linalg.qr(U + perturb)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # ensure mean is in bounds\n            m = np.clip(m, lb, ub)\n\n            # final budget guard\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm DSL_DE scored 0.208 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e157d1cc-26d6-4b5f-b0e2-deda091f662a", "operator": null, "metadata": {"aucs": [0.07309855313808689, 0.15669339954457828, 0.33865070922850127, 0.2606416844488476, 0.10352084988871835, 0.3519636490902768, 0.14931739439725922, 0.31384858021108786, 0.23560312991578947, 0.09870546118395274]}, "task_prompt": ""}
{"id": "fcc9f8e1-9ae1-454a-a381-1f9c3563a3db", "fitness": 0.18214707006575978, "name": "ADiagSubMix", "description": "The algorithm mixes a cheap per-coordinate diagonal Gaussian (D) with a learned low-rank correlated subspace (U,S) — k is set to ~ceil(sqrt(n)) and population size λ≈4+3log(n) — and adapts the mixture weight α from the subspace vs diagonal explained variance while starting with a conservative sigma = 0.18 * mean(range). It uses CMA-style machinery (rank-based log-weights, evolution paths ps/pc, sigma control with damps and χn) and updates D by an EMA of selected squared steps, updates S by projecting selected steps onto U, and refreshes U via SVD on a success buffer with periodic full eigen-decomposition to build a stable invsqrt for more accurate path updates. To increase exploration and robustness it employs mirrored sampling to reduce variance, occasional heavy-tailed Cauchy jumps, archive-based differential-evolution (DE) perturbations, and an archive of past candidates; sampling and evaluation are budget-aware and mirrored pairs are enforced when possible. Stagnation triggers diversification (sigma boost, mean mixing with archive member, perturb D, reduce subspace influence), parameters use momentum/smoothing (α_mom=0.9, small update rates), and bounds are enforced by clipping throughout.", "code": "import numpy as np\n\nclass ADiagSubMix:\n    \"\"\"\n    Adaptive Diagonal+Subspace Mixture CMA (ADiagSubMix)\n    One-line: Mix a cheap per-coordinate diagonal Gaussian with a learned low-rank correlated subspace,\n    adapt the mixture weight from explained variance, use CMA-style path-length sigma control with mirrored\n    sampling, archive-DE and occasional Cauchy jumps, and periodic eigen-reconditioning for stable updates.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size heuristic similar to CMA-ES\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        span = ub - lb\n        avg_range = float(np.mean(span))\n\n        # strategy params\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length / adaptation constants\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        cc = (4.0 + mu_eff / n) / (n + 4.0 + 2.0 * mu_eff / n)\n        c1 = 2.0 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1.0 - c1, 2.0 * (mu_eff - 2.0 + 1.0 / mu_eff) / ((n + 2.0) ** 2 + mu_eff))\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        m = np.random.uniform(lb, ub)                 # mean\n        sigma = 0.18 * avg_range                      # initial step-size slightly conservative\n        D = np.ones(n)                                # diagonal std approx\n        # initialize low-rank U and singular scales S\n        if self.k >= 1:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand_mat)\n                U = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n        S = np.ones(self.k)\n\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n\n        # success buffer for SVD\n        success_buffer = []\n        buffer_max = max(10 * self.k, 20)\n\n        # archive for DE mutations\n        archive_X = []\n        archive_F = []\n        archive_cap = 5000\n\n        # adaptive mixing weight for subspace vs diagonal\n        alpha = 0.5\n        alpha_mom = 0.9  # momentum for smoothing alpha updates\n\n        # exploration knobs\n        p_cauchy = 0.12\n        cauchy_scale = 1.0\n        p_de = 0.18\n        F_de = 0.8\n        mirrored = True\n\n        # reconditioning / full invsqrt\n        eig_every = max(50, 10 * n)\n        eigen_counter = 0\n        have_full = False\n        invsqrtC_full = np.eye(n)\n        B_full = np.eye(n)\n        D_full = np.ones(n)\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n\n        # initial eval of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(float(fm))\n            f_opt = float(fm); x_opt = xm.copy()\n            last_improvement_eval = evals\n\n        # helper: safe normalize vector\n        def safe_norm(v):\n            nv = np.linalg.norm(v)\n            return nv if nv > 1e-20 else 1e-20\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # ensure mirrored pairing\n            if mirrored and (current_lambda % 2 == 1) and current_lambda > 1:\n                current_lambda -= 1\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n\n            # adapt mixing alpha from explained variance of subspace vs diag energy\n            # use S^2 total energy vs D^2 total energy\n            sub_energy = np.sum(S ** 2) if self.k > 0 else 0.0\n            diag_energy = np.sum(D ** 2)\n            explained_ratio = sub_energy / (sub_energy + diag_energy + 1e-20)\n            target_alpha = float(np.clip(0.08 + 0.9 * explained_ratio, 0.05, 0.95))\n            alpha = alpha_mom * alpha + (1.0 - alpha_mom) * target_alpha\n\n            # sample generation (mirrored pairs reduce variance)\n            k_idx = 0\n            while k_idx < current_lambda:\n                # diagonal Gaussian\n                z = np.random.randn(n)\n                y_diag = D * z\n                # low-rank sample\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    y_low = U @ (S * z_low)   # correlated direction\n                else:\n                    y_low = np.zeros(n)\n\n                # combine according to alpha; scale low-rank by mean(D) to keep units compatible\n                beta = 0.9 * np.mean(D)\n                y = (1.0 - alpha) * y_diag + alpha * (beta * y_low)\n\n                # occasional heavy-tailed jump\n                if np.random.rand() < p_cauchy:\n                    r = np.random.standard_cauchy() * cauchy_scale\n                    nz = safe_norm(z)\n                    y = r * (z / nz) * np.mean(D)\n\n                # mirrored: second of pair will be negation of first\n                x = m + sigma * y\n\n                # DE-like diff mutation occasionally\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    x = x + F_de * (archive_X[i1] - archive_X[i2])\n\n                # clip and store\n                x = np.clip(x, lb, ub)\n                arx[k_idx] = x\n                arz[k_idx] = y\n                k_idx += 1\n\n                # mirrored partner if room\n                if mirrored and k_idx < current_lambda:\n                    y2 = -y\n                    x2 = m + sigma * y2\n                    if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        x2 = x2 + F_de * (archive_X[i1] - archive_X[i2])\n                    x2 = np.clip(x2, lb, ub)\n                    arx[k_idx] = x2\n                    arz[k_idx] = y2\n                    k_idx += 1\n\n            # evaluate candidates sequentially (budget-aware)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = float(f)\n                archive_X.append(x.copy()); archive_F.append(float(f))\n                if len(archive_X) > archive_cap:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f); x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection & recombination\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            idx_sorted = valid_idx[np.argsort(arfit[valid_idx])]\n            sel_count = min(mu, idx_sorted.size)\n            sel_idx = idx_sorted[:sel_count]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            # normalized weights (in case sel_count < mu)\n            w = weights[:sel_count].copy()\n            w = w / np.sum(w)\n\n            # update mean\n            m_old = m.copy()\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # compute approximate inverse sqrt C: prefer full if available\n            if have_full:\n                invsqrt = invsqrtC_full\n            else:\n                invsqrt = np.diag(1.0 / (D + 1e-20))\n\n            # update sigma path\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrt @ y_w)\n            norm_ps = np.linalg.norm(ps)\n\n            # hsig for covariance path (conservative)\n            hsig = 1.0 if (norm_ps / np.sqrt(1.0 - (1.0 - cs) ** (2.0 * max(1, evals / max(1, lam)))) / chi_n) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # sigma update\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e3 * avg_range + 1e-12)\n\n            # update diagonal D via EMA of selected y^2\n            c_d = 0.24\n            y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n\n            # update low-rank S by projecting selected y's onto U\n            if self.k > 0 and sel_count > 0:\n                try:\n                    Yproj = (U.T @ y_sel.T)  # k x sel_count\n                    proj_mom = np.sum((w[None, :] * (Yproj ** 2)), axis=1)\n                    beta_s = 0.18\n                    S2 = (1.0 - beta_s) * (S ** 2) + beta_s * (proj_mom + 1e-20)\n                    S = np.sqrt(S2)\n                except Exception:\n                    pass\n\n            # push weighted successful step into buffer\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace U via SVD on buffer periodically\n            if (len(success_buffer) >= self.k) and (evals % max(1, min(10, n)) == 0):\n                Y = np.vstack(success_buffer).T  # n x m\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    U_new, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        # normalize singulars to approximate std scales\n                        S = (svals[:k_take] / (np.sqrt(max(1, Y.shape[1])) + 1e-12)).copy()\n                        # pad if necessary\n                        if U.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U.shape[1]))\n                            U = np.hstack([U, pad])\n                            S = np.concatenate([S, np.ones(self.k - len(S))])\n                except np.linalg.LinAlgError:\n                    pass\n\n            # occasional reconditioning: compute approximate full covariance and eigendecompose\n            eigen_counter += current_lambda\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                try:\n                    cov_approx = np.diag(D ** 2)\n                    if self.k > 0:\n                        cov_approx = cov_approx + (U @ np.diag(S ** 2) @ U.T)\n                    cov_approx = 0.5 * (cov_approx + cov_approx.T)\n                    Dvals, B = np.linalg.eigh(cov_approx)\n                    Dvals = np.maximum(Dvals, 1e-20)\n                    D_full = np.sqrt(Dvals)\n                    B_full = B\n                    invsqrtC_full = (B_full * (1.0 / D_full)) @ B_full.T\n                    have_full = True\n                except np.linalg.LinAlgError:\n                    have_full = False\n                    invsqrtC_full = np.eye(n)\n\n            # stagnation detection: if no improvement for long, diversify\n            stagn_thr = max(50, 8 * n)\n            if (evals - last_improvement_eval) > stagn_thr:\n                sigma *= 1.8\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                # perturb D and reduce ps/pc\n                D = 0.9 * D + 0.1 * (1.0 + 0.05 * np.random.randn(n))\n                ps *= 0.5; pc *= 0.5\n                success_buffer = []\n                last_improvement_eval = evals\n                # slightly reduce subspace influence temporarily\n                alpha = max(0.05, alpha * 0.7)\n\n            # keep mean in bounds\n            m = np.clip(m, lb, ub)\n\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ADiagSubMix scored 0.182 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e157d1cc-26d6-4b5f-b0e2-deda091f662a", "operator": null, "metadata": {"aucs": [0.06086792470430247, 0.14958859881238185, 0.23905160381442114, 0.13854778399003054, 0.2838138047178541, 0.3144505103014391, 0.15459963286020462, 0.1989635353733964, 0.1810254257837185, 0.10056188029984914]}, "task_prompt": ""}
{"id": "24c9b088-c2ad-427f-ad86-bb58a82b9994", "fitness": "-inf", "name": "LARDE_RS", "description": "LARDE_RS is a hybrid search that combines CMA-style covariance adaptation and cumulation (ps, pc, C, periodic eigendecomposition and invsqrtC) with population sampling scaled by an anisotropic BD mapping and mirrored sampling, where population size lambda grows mildly with log(dim) (pop_scale) and per-coordinate RMS scaling biases active coordinates. Diversification is provided by archive-driven DE-difference perturbations (p_de), occasional Mantegna Levy long jumps (p_levy, levy_alpha), and a small LRU directional memory of successful unit steps that can be replayed as jumps to escape local traps. Exploitation features include budget-aware, short geometric line searches along the mean-improvement direction, archive-conditioned random-subspace quadratic surrogate minimizers (small subspace fraction) for cheap local proposals, and sigma adaptation combining success-rate feedback with ps-based cumulation. Practical safeguards and budget-awareness are pervasive: safe_eval enforces bounds and evaluation counting, archive pruning limits memory growth, tiny numerical floors (sigma, D, reg) stabilize matrices, and conservative default probabilities and memory sizes (e.g., p_de~0.22, p_levy~0.10, memory_size=8) balance exploration vs exploitation.", "code": "import numpy as np\n\nclass LARDE_RS:\n    \"\"\"\n    LARDE_RS: Levy-Adaptive Rank-Differential Evolution with Random-Subspace probes\n    One-line: Hybrid CMA/DE population loop augmented by mirrored sampling modified by per-coordinate RMS scaling,\n    an LRU memory of successful directions, occasional DE-differences and Cauchy jumps, random-subspace surrogate\n    minimizers and short budget-aware line searches for inexpensive local exploitation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 pop_scale=5, p_de=0.22, p_levy=0.10, levy_alpha=1.5,\n                 memory_size=8, subspace_k_frac=None, max_eval_per_subspace=6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.pop_scale = pop_scale\n        self.p_de = float(p_de)\n        self.p_levy = float(p_levy)\n        self.levy_alpha = float(levy_alpha)\n        self.memory_size = int(memory_size)\n        self.subspace_k_frac = subspace_k_frac if subspace_k_frac is not None else min(0.33, max(0.05, 0.25))\n        self.max_eval_per_subspace = int(max_eval_per_subspace)\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def _levy_mantegna(self, n, alpha=1.5, scale=1.0):\n        # Mantegna's symmetric Levy stable step\n        sigma_u = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                   (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n        u = self.rng.normal(0, sigma_u, size=n)\n        v = self.rng.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1.0 / alpha))\n        return scale * step\n\n    def __call__(self, func):\n        n = int(self.dim)\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        range_mean = np.mean(domain_range)\n        range_norm = np.linalg.norm(domain_range)\n\n        # population settings\n        lam = max(4, int(self.pop_scale + np.floor(5.0 * np.log(max(1, n)))))\n        mu = max(1, lam // 2)\n\n        # recombination weights (descending log-weights)\n        weights = np.log(np.arange(1, mu + 1) + 0.5)[::-1]\n        weights = weights / (np.sum(weights) + 1e-20)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # adaptation constants (moderate cumulation, conservative rank-one)\n        cc = 2.0 / (n + 2.0)\n        cs = 0.4 * (mu_eff / (n + mu_eff + 1.0))\n        c1 = 1.0 / max(1.0, (n + 3.0) ** 2)\n        cmu = 0.6 * (1.0 - c1) * min(1.0, mu_eff / (2.0 * n))\n        damps = 1.0 + 0.3 * cs + 0.2 * np.sqrt(max(0.0, mu_eff / n))\n\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # dynamic state\n        m = self.rng.uniform(lb, ub)\n        sigma = 0.25 * range_mean\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n\n        eig_every = max(1, int(8 * n))\n        eigen_eval_counter = 0\n\n        # archive and tracking\n        archive_X = []\n        archive_F = []\n        f_opt = np.inf\n        x_opt = None\n\n        evals = 0\n\n        # directional LRU memory and per-coordinate RMS\n        dir_memory = []\n        coord_var = np.ones(n) * (1e-3 * (range_mean + 1.0))\n        coord_alpha = 0.14\n        coord_eps = 1e-12\n\n        # helper safe eval wrapper\n        def safe_eval(x):\n            nonlocal evals, f_opt, x_opt\n            if evals >= budget:\n                return None\n            x_clipped = np.clip(np.asarray(x, dtype=float), lb, ub)\n            try:\n                f = float(func(x_clipped))\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_X.append(x_clipped.copy())\n            archive_F.append(f)\n            if f < f_opt - 1e-12:\n                f_opt = f\n                x_opt = x_clipped.copy()\n            return f, x_clipped\n\n        # initial evaluation at mean\n        out = safe_eval(m)\n        if out is None:\n            return float(np.inf), np.zeros(n)\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # mirrored sampling for symmetry if possible\n            if current_lambda % 2 == 0:\n                half = current_lambda // 2\n                arz_half = self.rng.standard_normal(size=(half, n))\n                arz = np.vstack([arz_half, -arz_half])\n            else:\n                arz = self.rng.standard_normal(size=(current_lambda, n))\n\n            # apply B * (D * z) mapping to create anisotropic sample\n            BD = B * D[np.newaxis, :]\n            ary = arz @ BD.T  # shape (current_lambda, n)\n\n            # apply per-coordinate RMS scaling to samples to bias active coordinates\n            coord_scale = np.sqrt(coord_var + coord_eps)\n            ary = ary * coord_scale[np.newaxis, :]\n\n            # normalize per-row to keep intended scale variability moderate\n            norms = np.linalg.norm(ary, axis=1)\n            norms[norms == 0] = 1.0\n            ary = ary / norms[:, None]\n\n            # produce candidate population\n            arx = m + sigma * ary\n\n            # apply occasional DE-differences and Levy jumps and memory jumps\n            for k in range(current_lambda):\n                # DE-like archive difference\n                if (len(archive_X) >= 2) and (self.rng.random() < self.p_de):\n                    i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                    de_vec = 0.6 * (archive_X[i1] - archive_X[i2])\n                    arx[k] = arx[k] + 0.45 * de_vec  # modest perturbation\n\n                # occasional Levy long jump\n                if self.rng.random() < self.p_levy:\n                    lv = self._levy_mantegna(n, alpha=self.levy_alpha, scale=0.6 * sigma)\n                    arx[k] = arx[k] + 0.5 * lv\n\n                # occasional directional memory jump\n                if dir_memory and (self.rng.random() < 0.12):\n                    u = dir_memory[self.rng.integers(len(dir_memory))]\n                    jump = np.tan(np.pi * (self.rng.random() - 0.5))\n                    arx[k] = arx[k] + 0.6 * sigma * 0.6 * jump * u\n\n                # clip\n                arx[k] = np.clip(arx[k], lb, ub)\n\n            # Evaluate offspring (budget-aware)\n            arfit = np.full(current_lambda, np.inf)\n            for k in range(current_lambda):\n                if evals >= budget:\n                    break\n                out = safe_eval(arx[k])\n                if out is None:\n                    break\n                f_k, xk = out\n                arfit[k] = f_k\n\n            # if no offspring evaluated, break\n            if np.sum(np.isfinite(arfit)) == 0:\n                break\n\n            # Select top mu individuals\n            sel_mu = min(mu, int(np.sum(np.isfinite(arfit))))\n            idx_sorted = np.argsort(arfit)\n            sel_idx = idx_sorted[:sel_mu]\n            x_sel = arx[sel_idx]\n            y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n\n            # store old mean\n            m_old = m.copy()\n\n            # recombine to new mean\n            # if fewer than mu, renormalize weights\n            if sel_mu < mu:\n                w_sub = weights[:sel_mu]\n                w_sub = w_sub / (np.sum(w_sub) + 1e-20)\n                m = np.sum(w_sub[:, None] * x_sel, axis=0)\n                y_w = np.sum(w_sub[:, None] * y_sel, axis=0)\n            else:\n                m = np.sum(weights[:, None] * x_sel, axis=0)\n                y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # short budget-aware line search along improvement direction if meaningful improvement occurred\n            # choose direction from m_old -> best selected\n            if (f_opt < np.inf) and (np.linalg.norm(m - m_old) > 1e-12) and (remaining > 2):\n                dir_vec = m - m_old\n                dn = np.linalg.norm(dir_vec)\n                if dn > 0:\n                    d_unit = dir_vec / dn\n                    # tiny bracketed backtracking line search: try scale factors in geometric series\n                    scales = [0.8, 0.5, 0.25, 0.125]\n                    for s in scales[:self.max_eval_per_subspace]:\n                        if evals >= budget:\n                            break\n                        cand = np.clip(m_old + s * dn * d_unit, lb, ub)\n                        out = safe_eval(cand)\n                        if out is None:\n                            break\n                        fcand, xcand = out\n                        if fcand < f_opt - 1e-12:\n                            # accept and update mean and memory\n                            # push directional memory\n                            unit = (xcand - m_old)\n                            nrm = np.linalg.norm(unit)\n                            if nrm > 0:\n                                unit = unit / nrm\n                                dir_memory.insert(0, unit.copy())\n                                if len(dir_memory) > self.memory_size:\n                                    dir_memory.pop()\n                            # update coord var\n                            step_rel = (xcand - m_old)\n                            coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (step_rel ** 2 + 1e-12)\n                            # set m to xcand for further adaptation\n                            m = xcand.copy()\n                            break\n\n            # update cumulation paths and covariance\n            y_w = np.asarray(y_w, dtype=float)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrtC @ y_w)\n            norm_ps = np.linalg.norm(ps)\n            hsig = 1.0 if (norm_ps / (np.sqrt(1.0 - (1.0 - cs) ** (2.0 * max(1, evals / max(1, current_lambda)))) + 1e-20) / chi_n) < (1.5 + 1.0 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # rank-one and rank-mu updates\n            rank_one = np.outer(pc, pc)\n            rank_mu = np.zeros((n, n))\n            # weights used for rank_mu correspond to selected set\n            if sel_mu < mu:\n                w_for = w_sub\n            else:\n                w_for = weights\n            for i in range(sel_mu):\n                yi = y_sel[i][:, None]\n                rank_mu += w_for[i] * (yi @ yi.T)\n            C = (1.0 - c1 - cmu) * C + c1 * (rank_one + (1.0 - hsig) * cc * (2.0 - cc) * C) + cmu * rank_mu\n\n            # sigma adaptation using ps and a lightweight success-rate factor\n            # success_rate: fraction of offspring better than previous mean estimate (approx using archive)\n            # fallback: compare to median of offspring\n            if 'f_opt' in locals() and np.isfinite(f_opt):\n                f_parent_est = np.median(archive_F[-min(len(archive_F), max(2, current_lambda)):]) if len(archive_F) > 0 else np.median(arfit[np.isfinite(arfit)])\n            else:\n                f_parent_est = np.median(arfit[np.isfinite(arfit)]) if np.any(np.isfinite(arfit)) else np.inf\n            s_rate = np.mean(arfit[np.isfinite(arfit)] < f_parent_est) if np.any(np.isfinite(arfit)) else 0.0\n            target_s = 0.2\n            adapt_strength = 0.6 / (1.0 + 0.05 * n)\n            sigma = max(1e-12, sigma * np.exp(adapt_strength * (s_rate - target_s) / (damps + 1e-20)))\n            # additionally apply cumulation-based update\n            sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n\n            # update per-coordinate RMS from the successful steps (use best selected vs old mean)\n            if len(sel_idx) > 0:\n                best_idx = sel_idx[0]\n                best_step = (arx[best_idx] - m_old)\n                coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (best_step ** 2 + 1e-12)\n                # push memory\n                dn = np.linalg.norm(best_step)\n                if dn > 1e-12:\n                    unit = best_step / dn\n                    dir_memory.insert(0, unit.copy())\n                    if len(dir_memory) > self.memory_size:\n                        dir_memory.pop()\n\n            # random-subspace surrogate probe occasionally (cheap small-k quadratic along subspace)\n            if (len(archive_X) >= 4 * max(1, int(np.ceil(self.subspace_k_frac * n))) + 6) and (self.rng.random() < 0.18) and (remaining > 1):\n                k = max(1, int(min(8, np.ceil(self.subspace_k_frac * n))))\n                # build random orthonormal subspace Q of size (n,k)\n                R = self.rng.standard_normal(size=(n, k))\n                Q, _ = np.linalg.qr(R, mode='reduced')\n                Q = Q[:, :k]\n                # project nearest neighbors to m\n                X_arr = np.asarray(archive_X)\n                F_arr = np.asarray(archive_F)\n                dists = np.linalg.norm(X_arr - m, axis=1)\n                idx = np.argsort(dists)[:min(len(X_arr), max(4*k + 6, 10))]\n                Z = (X_arr[idx] - m) @ Q  # (m_count, k)\n                y = F_arr[idx]\n                # build simple ridge-linear+diag quad model in subspace\n                try:\n                    # design: [1, z, 0.5*z^2]\n                    mcount = Z.shape[0]\n                    Phi = np.ones((mcount, 1 + k + k))\n                    Phi[:, 1:1 + k] = Z\n                    Phi[:, 1 + k:1 + 2 * k] = 0.5 * (Z ** 2)\n                    reg = 1e-6\n                    theta, *_ = np.linalg.lstsq(Phi.T @ Phi + reg * np.eye(Phi.shape[1]), Phi.T @ y, rcond=None)\n                    theta = theta.flatten()\n                    g = theta[1:1 + k]\n                    hdiag = theta[1 + k:1 + 2 * k]\n                    # ensure positive curvature\n                    hdiag = np.maximum(hdiag, 1e-8)\n                    # subspace minimizer z* = -g / hdiag\n                    z_star = -g / (hdiag + 1e-20)\n                    step_vec = Q @ z_star\n                    step_norm = np.linalg.norm(step_vec)\n                    if step_norm > 1e-12:\n                        scale = min(1.0, (0.9 * sigma * np.sqrt(n)) / (step_norm + 1e-16))\n                    else:\n                        scale = 1.0\n                    x_prop = np.clip(m + scale * step_vec, lb, ub)\n                    if evals < budget:\n                        out = safe_eval(x_prop)\n                        if out is not None:\n                            fp, xp = out\n                            # if improved, accept as new mean and update memory/coord var\n                            if fp < f_opt - 1e-12:\n                                step_rel = xp - m\n                                coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (step_rel ** 2 + 1e-12)\n                                dn = np.linalg.norm(step_rel)\n                                if dn > 1e-12:\n                                    unit = step_rel / dn\n                                    dir_memory.insert(0, unit.copy())\n                                    if len(dir_memory) > self.memory_size:\n                                        dir_memory.pop()\n                                m = xp.copy()\n                except Exception:\n                    pass\n\n            # eigen decomposition periodically to update B, D, invsqrtC\n            eigen_eval_counter += current_lambda\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                # enforce symmetry\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # archive pruning\n            max_archive = max(2000, 50 * n)\n            if len(archive_X) > max_archive:\n                F_arr = np.asarray(archive_F)\n                idx_sorted = np.argsort(F_arr)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                keep_rest = rest[::max(1, len(rest) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # tiny safeguard on sigma\n            sigma = max(sigma, 1e-12)\n\n            # early exit if near-perfect\n            if f_opt <= 1e-12:\n                break\n\n        # final return\n        if x_opt is None:\n            # fallback: return best in archive if any\n            if len(archive_X) > 0:\n                return float(archive_F[np.argmin(archive_F)]), np.array(archive_X[np.argmin(archive_F)], dtype=float)\n            else:\n                return float(np.inf), np.zeros(n)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "e157d1cc-26d6-4b5f-b0e2-deda091f662a", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "7831fee0-05e0-4401-8750-27ecd3f6609c", "fitness": 0.30614864065052033, "name": "ASCS", "description": "The algorithm probes randomized low-dimensional subspaces (k ≈ ceil(sqrt(n))) formed by QR orthonormalization and generates mirrored, heavy-tailed (Student-t) samples scaled by a projected coordinate-importance weight, with occasional DE-like differences and Cauchy jumps for escape. It collects subspace coordinates and values to fit cheap diagonal quadratic models (least-squares over z_i^2 and z_i) and proposes clipped subspace minimizers as candidate steps. Adaptive control comes from a trust radius (init ~0.25·mean_range) with grow/shrink factors (1.18/0.80), momentum, an LRU directional memory of successful unit steps, and finite-difference curvature estimates along successful directions to update a diagonal Hessian (diag_H) and coordinate importance via exponential blending. Budget-aware design (safe_eval enforcing self.budget), archive pruning, model_every / mem_size / cauchy_prob / de_prob / stagnation_limit knobs, and stagnation-driven large Cauchy jumps or local restarts provide practical safeguards and robustness.", "code": "import numpy as np\n\nclass ASCS:\n    \"\"\"\n    Adaptive Subspace Curvature Search (ASCS)\n\n    Key ideas:\n    - Probe randomized low-dimensional subspaces (k ~ ceil(sqrt(n))) built via QR.\n    - Fit cheap diagonal quadratic models in the subspace (a_i z_i^2/2 + b_i z_i + c)\n      from mirrored / heavy-tailed samples and propose subspace minimizers.\n    - Estimate directional curvature (finite-difference) along successful directions\n      to adapt a diagonal Hessian estimate in the full space, guiding coordinate scaling.\n    - Keep a small LRU of successful unit directions and an archive of evaluated points.\n    - Use momentum, occasional DE-like differences, and Cauchy jumps for escapes.\n    - Budget-aware: always check remaining eval budget and never exceed it.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 mem_size=10, model_every=12, subspace_factor=1.0,\n                 cauchy_prob=0.10, de_prob=0.10, stagnation_limit=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.mem_size = int(mem_size)\n        self.model_every = int(model_every)\n        self.subspace_factor = float(subspace_factor)\n        self.cauchy_prob = float(cauchy_prob)\n        self.de_prob = float(de_prob)\n        self.rng = np.random.default_rng(seed)\n        self.stagnation_limit = stagnation_limit if stagnation_limit is not None else max(40, int(5 * np.sqrt(dim)))\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # get bounds (scalar or vector)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        mean_range = float(np.mean(domain_range))\n\n        # safe eval wrapper that enforces budget\n        evals = 0\n        X_archive = []\n        F_archive = []\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            x_clip = np.clip(x, lb, ub)\n            try:\n                f = float(func(x_clip))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x_clip.copy())\n            F_archive.append(f)\n            return f, x_clip\n\n        # initialization\n        x_cur = self.rng.uniform(lb, ub)\n        out = safe_eval(x_cur)\n        if out[0] is None:\n            return float(np.inf), np.zeros(n)\n        f_cur, x_cur = out\n        f_best = float(f_cur)\n        x_best = x_cur.copy()\n\n        # parameters\n        trust = 0.25 * mean_range * self.subspace_factor\n        min_trust = 1e-9 * max(1.0, mean_range)\n        grow = 1.18\n        shrink = 0.80\n        tol = 1e-12\n\n        # momentum and directional memory\n        momentum = np.zeros(n)\n        beta_m = 0.6\n        dir_memory = []  # LRU unit vectors\n\n        # diagonal curvature (H diagonal estimate) and coordinate weights\n        diag_H = np.ones(n) * (1e-3 / (mean_range + 1e-12))\n        coord_importance = np.ones(n)\n        imp_alpha = 0.12\n\n        iter_since_improve = 0\n        iteration = 0\n\n        # helper: build orthonormal subspace basis (k columns)\n        def build_subspace(k, use_memory=0):\n            # use up to use_memory recent directions as first columns, then random\n            cols = []\n            for i in range(min(use_memory, len(dir_memory))):\n                cols.append(dir_memory[i].copy())\n            need = k - len(cols)\n            if need > 0:\n                R = self.rng.normal(size=(n, need))\n                if cols:\n                    R = np.column_stack([np.column_stack(cols), R])\n                Q, _ = np.linalg.qr(R, mode='reduced')\n                B = Q[:, :k]\n            else:\n                # if memory fills k, orthonormalize them\n                R = np.column_stack(cols[:k])\n                Q, _ = np.linalg.qr(R, mode='reduced')\n                B = Q[:, :k]\n            return B\n\n        # cheap diagonal quadratic fit in subspace: f ≈ 0.5 * sum(a_i z_i^2) + b^T z + c\n        # returns proposed z* (length k) or None\n        def subspace_diagonal_model_fit(Zs, Fs):\n            # Zs: m x k array of subspace coordinates, Fs: m array of function values\n            m, k = Zs.shape\n            # design matrix: columns for z_i^2 and z_i and intercept\n            A = np.hstack([0.5 * (Zs ** 2), Zs, np.ones((m, 1))])\n            reg = 1e-8\n            try:\n                sol, *_ = np.linalg.lstsq(A, Fs, rcond=None)\n                sol = sol.flatten()\n                a = sol[:k]  # quadratic diag coefficients\n                b = sol[k:2 * k]\n                # prefer directions where a is positive (convex)\n                a_reg = a.copy()\n                # regularize small or non-positive curvatures\n                for i in range(k):\n                    if a_reg[i] < 1e-8:\n                        a_reg[i] = 1e-8\n                z_star = - b / (a_reg + 1e-20)\n                return z_star\n            except Exception:\n                return None\n\n        # finite-difference curvature along d (unit) centered at x, uses small delta fraction of trust\n        def curvature_estimate(x, d, delta_factor=0.8):\n            # requires 2 extra evaluations\n            nonlocal evals\n            if evals + 2 > self.budget:\n                return None\n            delta = max(min(trust * delta_factor, 0.5 * mean_range), 1e-12)\n            out1 = safe_eval(np.clip(x + delta * d, lb, ub))\n            if out1[0] is None:\n                return None\n            f1, _ = out1\n            out2 = safe_eval(np.clip(x - delta * d, lb, ub))\n            if out2[0] is None:\n                return None\n            f2, _ = out2\n            # second directional derivative approx: (f(x+δ)-2f(x)+f(x-δ)) / δ^2\n            # use central f0 from archive last value if available (we assume x is current)\n            f0 = f_cur\n            kappa = (f1 - 2.0 * f0 + f2) / (delta ** 2 + 1e-20)\n            return kappa\n\n        # main optimization loop\n        while evals < self.budget:\n            iteration += 1\n            # adapt subspace dimension; use sqrt heuristic but at least 1\n            k = max(1, int(np.ceil(np.sqrt(n))))\n            probes = max(4, 3 * k)  # more probes per iteration\n\n            # decide how many memory columns to reuse (at most half)\n            use_mem = min(len(dir_memory), k // 2)\n\n            B = build_subspace(k, use_memory=use_mem)\n\n            # sampling strategy: mirrored + heavy-tailed Student-t (nu=3) to allow escapes\n            half = (probes + 1) // 2\n            Z_samples = []\n            for _ in range(half):\n                # sample heavy-tailed in subspace coords\n                z = self.rng.standard_t(df=3, size=k)\n                # scale by coordinate importance projected to subspace (diag importance projected)\n                proj_weights = np.sqrt(np.maximum(0.0, (B.T @ coord_importance).flatten()))\n                z = z * (proj_weights + 1e-12)\n                Z_samples.append(z)\n            # mirror\n            Z_samples = Z_samples + [(-z) for z in Z_samples]\n            Z_samples = Z_samples[:probes]\n\n            improved_in_round = False\n            Z_model_pts = []\n            F_model_pts = []\n\n            for z in Z_samples:\n                if evals >= self.budget:\n                    break\n                # scale by trust radius\n                step_scale = trust\n                x_try = x_cur + (B @ (z * step_scale)) + 0.5 * momentum\n                # occasionally add DE-like difference\n                if len(X_archive) >= 3 and self.rng.random() < self.de_prob:\n                    i1, i2 = self.rng.choice(len(X_archive), size=2, replace=False)\n                    de = 0.5 * (X_archive[i1] - X_archive[i2])\n                    x_try = x_try + 0.4 * de\n                # occasional Cauchy jump for escape\n                if self.rng.random() < self.cauchy_prob and dir_memory:\n                    u = dir_memory[self.rng.integers(len(dir_memory))]\n                    jump = np.tan(np.pi * (self.rng.random() - 0.5))\n                    x_try = x_try + jump * 0.8 * trust * u\n                x_try = np.clip(x_try, lb, ub)\n                out = safe_eval(x_try)\n                if out[0] is None:\n                    break\n                f_try, x_try = out\n\n                # store model point (subspace coordinate) for fitting if within moderate range\n                # compute z_effective: B^T (x_try - x_cur) / trust\n                z_eff = (B.T @ (x_try - x_cur)) / (step_scale + 1e-20)\n                Z_model_pts.append(z_eff)\n                F_model_pts.append(f_try)\n\n                # success criteria\n                if f_try < f_cur - tol:\n                    prev_x = x_cur.copy()\n                    x_cur = x_try.copy()\n                    f_cur = f_try\n                    improved_in_round = True\n                    iter_since_improve = 0\n                    # update best\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n                    # update momentum\n                    momentum = beta_m * momentum + (1 - beta_m) * (x_cur - prev_x)\n                    # push unit direction into memory\n                    d_succ = x_cur - prev_x\n                    dn = np.linalg.norm(d_succ)\n                    if dn > 0:\n                        du = d_succ / dn\n                        dir_memory.insert(0, du.copy())\n                        if len(dir_memory) > self.mem_size:\n                            dir_memory.pop()\n                    # update coordinate importance via the normalized squared step map\n                    rel = (x_cur - prev_x) ** 2\n                    rel = rel / (np.linalg.norm(rel) + 1e-20)\n                    coord_importance = (1 - imp_alpha) * coord_importance + imp_alpha * rel\n                    # estimate curvature along the successful direction if enough budget\n                    if evals + 2 <= self.budget:\n                        d_unit = d_succ / (dn + 1e-20)\n                        kappa = curvature_estimate(prev_x, d_unit)\n                        if kappa is not None and np.isfinite(kappa):\n                            # incorporate curvature into diag_H on coordinates active in d_unit\n                            # update rule: H_new_j += gamma * kappa * u_j^2\n                            gamma = 0.12\n                            diag_H = diag_H * (1 - gamma) + gamma * np.maximum(1e-8, kappa) * (d_unit ** 2 + 1e-12)\n                    # increase trust gently\n                    trust = min(trust * grow, 5.0 * mean_range)\n                # otherwise no immediate accept, but collect for modeling\n            # end probes loop\n\n            # modeling step: if we have enough model points, fit diagonal quadratic in subspace and propose minimizer\n            if len(Z_model_pts) >= max(6, k + 2) and (iteration % self.model_every == 0) and (evals < self.budget):\n                Zs = np.array(Z_model_pts)\n                Fs = np.array(F_model_pts)\n                z_star = subspace_diagonal_model_fit(Zs, Fs)\n                if z_star is not None and np.all(np.isfinite(z_star)):\n                    # limit the proposed step magnitude\n                    max_z = np.maximum(1.0, np.linalg.norm(z_star))\n                    z_star = z_star / (max_z + 1e-12) * min(3.0, np.linalg.norm(z_star))\n                    x_prop = x_cur + B @ (z_star * trust)\n                    x_prop = np.clip(x_prop, lb, ub)\n                    outp = safe_eval(x_prop)\n                    if outp[0] is not None:\n                        f_prop, x_prop = outp\n                        if f_prop < f_cur - tol:\n                            prev_x = x_cur.copy()\n                            x_cur = x_prop.copy()\n                            f_cur = f_prop\n                            improved_in_round = True\n                            iter_since_improve = 0\n                            # update momentum and dir memory\n                            d_succ = x_cur - prev_x\n                            dn = np.linalg.norm(d_succ)\n                            if dn > 0:\n                                du = d_succ / dn\n                                dir_memory.insert(0, du.copy())\n                                if len(dir_memory) > self.mem_size:\n                                    dir_memory.pop()\n                            momentum = beta_m * momentum + (1 - beta_m) * (x_cur - prev_x)\n                            # update coord_importance\n                            rel = (x_cur - prev_x) ** 2\n                            rel = rel / (np.linalg.norm(rel) + 1e-20)\n                            coord_importance = (1 - imp_alpha) * coord_importance + imp_alpha * rel\n                            # try small curvature update using extra two FEs if available\n                            if evals + 2 <= self.budget:\n                                d_unit = d_succ / (dn + 1e-20)\n                                kappa = curvature_estimate(prev_x, d_unit)\n                                if kappa is not None and np.isfinite(kappa):\n                                    gamma = 0.12\n                                    diag_H = diag_H * (1 - gamma) + gamma * np.maximum(1e-8, kappa) * (d_unit ** 2 + 1e-12)\n\n            # adapt trust if no improvement\n            if not improved_in_round:\n                iter_since_improve += 1\n                trust = max(trust * shrink, min_trust)\n            else:\n                # slightly reward aggressive trust if repeat improvements\n                trust = min(trust * grow, 5.0 * mean_range)\n\n            # occasional aggressive escape when stagnating\n            if iter_since_improve > self.stagnation_limit and (evals < self.budget):\n                # large Cauchy jump around best\n                jump_vec = np.tan(np.pi * (self.rng.random(n) - 0.5))\n                x_jump = np.clip(x_best + 1.8 * jump_vec * trust, lb, ub)\n                outj = safe_eval(x_jump)\n                if outj[0] is not None:\n                    fj, x_jump = outj\n                    if fj < f_best:\n                        f_best = fj\n                        x_best = x_jump.copy()\n                        x_cur = x_jump.copy()\n                        f_cur = fj\n                        iter_since_improve = 0\n                        momentum = np.zeros(n)\n                # tiny local restart around best if still stagnating heavily\n                if iter_since_improve > 2 * self.stagnation_limit and (evals < self.budget):\n                    radius = max(0.2 * mean_range, trust)\n                    x_restart = np.clip(x_best + self.rng.normal(scale=radius, size=n), lb, ub)\n                    out_r = safe_eval(x_restart)\n                    if out_r[0] is not None:\n                        fr, xr = out_r\n                        if fr < f_best:\n                            f_best = fr\n                            x_best = xr.copy()\n                            x_cur = xr.copy()\n                            f_cur = fr\n                            iter_since_improve = 0\n                            momentum = np.zeros(n)\n\n            # prune archive growth occasionally\n            max_archive = max(2000, 50 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:400]\n                rest = idx_sorted[400:]\n                if len(rest) > 0:\n                    keep_rest = rest[::max(1, len(rest) // (max_archive - 400))]\n                    keep_idx = np.concatenate([keep_best, keep_rest])\n                else:\n                    keep_idx = keep_best\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # tiny safeguard for trust\n            if trust < min_trust:\n                trust = min_trust\n\n            # early exit if nearly optimal\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ASCS scored 0.306 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "83618236-2a28-4ed9-8668-80c937a24407", "operator": null, "metadata": {"aucs": [0.11311547545753142, 0.1404309219223513, 0.3874471560065239, 0.915036209941281, 0.22531186820608073, 0.38779318806119867, 0.21272309193438055, 0.29407592807155847, 0.23695716861575744, 0.1485953982885394]}, "task_prompt": ""}
{"id": "f20f8787-9c6b-4797-a36e-6f98f2d0077e", "fitness": 0.11755701315628068, "name": "ACCS", "description": "The algorithm uses a UCB-style multi-armed bandit to select probe directions from a pool of arms (coordinate axes, a few random orthonormal directions, and a growing memory of successful move-directions), forcing exploration of untried arms and periodically replacing poor random arms. Per-coordinate RMS scaling adapts direction magnitudes to historically productive coordinates while a global step size (initialized at 0.25·mean_range, bounded by min/max_step and adjusted by succ_grow/succ_shrink) controls move magnitudes. Local exploitation is enhanced by mirrored evaluations and a cheap parabolic 1D refinement when a probe fails to improve, while occasional heavy-tailed (Cauchy/Levy-like) jumps enable basin escape. A lightweight diagonal quadratic model is fit periodically from the best archived points to propose conservative model-driven moves, and the code is budget-aware (safe_eval, archive pruning, limited extra evaluations) with tunable parameters (memory_size, levy_prob, levy_scale, model_every, ucb_c) to balance exploration/exploitation.", "code": "import numpy as np\n\nclass ACCS:\n    \"\"\"\n    Adaptive Coordinate-Subspace Bandit Search (ACCS)\n\n    One-line: UCB-driven selection of coordinate and random-subspace probe directions,\n    scaled by per-coordinate RMS statistics, with mirrored/parabolic 1D refinement,\n    occasional Cauchy jumps for basin escape, and periodic diagonal quadratic model\n    proposals. Budget-aware, robust to boundaries and noisy evaluations.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=10, levy_prob=0.15, levy_scale=0.85,\n                 model_every=20, ucb_c=1.8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        # algorithmic params (tunable)\n        self.memory_size = int(memory_size)\n        self.levy_prob = float(levy_prob)\n        self.levy_scale = float(levy_scale)\n        self.model_every = int(model_every)\n        self.ucb_c = float(ucb_c)\n\n    def __call__(self, func):\n        n = self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        mean_range = float(np.mean(domain_range))\n        # initial point\n        x_cur = self.rng.uniform(lb, ub)\n        evals = 0\n\n        # archive\n        X_archive = []\n        F_archive = []\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            x_c = np.clip(x, lb, ub)\n            try:\n                f = float(func(x_c))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x_c.copy())\n            F_archive.append(f)\n            return f, x_c\n\n        # initial evaluation\n        out = safe_eval(x_cur)\n        if out[0] is None:\n            return float(np.inf), np.zeros(n)\n        f_cur, x_cur = out\n        f_best = float(f_cur)\n        x_best = x_cur.copy()\n\n        # step sizes and safety\n        step = 0.25 * mean_range               # initial step (different from original)\n        min_step = 1e-8 * max(1.0, mean_range)\n        max_step = 4.0 * mean_range\n        succ_grow = 1.25\n        succ_shrink = 0.72\n\n        # per-coordinate RMS-like scaling (starts modest)\n        coord_rms = np.ones(n) * 1e-2\n        rms_alpha = 0.16\n        eps = 1e-12\n\n        # build initial arms: coordinate axes + random directions\n        def make_random_unit(m):\n            R = self.rng.standard_normal(size=(n, m))\n            Q, _ = np.linalg.qr(R, mode='reduced')\n            return [Q[:, i].copy() / (np.linalg.norm(Q[:, i]) + 1e-20) for i in range(Q.shape[1])]\n\n        arms = []\n        # coordinate axes\n        for i in range(n):\n            v = np.zeros(n)\n            v[i] = 1.0\n            arms.append(v)\n        # additional random arms\n        rand_count = max(2, int(np.ceil(np.sqrt(n))))\n        arms += make_random_unit(rand_count)\n\n        # arm statistics for UCB\n        pulls = np.zeros(len(arms), dtype=int)\n        rewards = np.zeros(len(arms), dtype=float)  # cumulative reward\n        avg_reward = np.zeros(len(arms), dtype=float)\n\n        total_pulls = 0\n        iter_count = 0\n\n        # small parabolic 1D fit using three points: -a,0,+a returns best alpha (relative)\n        def parabolic_refine(x0, f0, v_unit, a, allowed_evals=2):\n            \"\"\"\n            Evaluate at +/- a (if not available) and fit parabola through ( -a, f- ), (0,f0), (+a, f+ )\n            Return (f_new, x_new) if better, else (None, None).\n            This function performs up to allowed_evals evaluations (it will skip already-known points if present).\n            \"\"\"\n            nonlocal evals\n            if evals >= self.budget or allowed_evals <= 0:\n                return None, None\n            # mirrored points\n            x_plus = np.clip(x0 + a * v_unit, lb, ub)\n            out_p = safe_eval(x_plus)\n            if out_p[0] is None:\n                return None, None\n            f_plus, x_plus = out_p\n            if evals >= self.budget or allowed_evals - 1 <= 0:\n                # cannot continue, return improvement if any\n                if f_plus < f0 - 1e-12:\n                    return f_plus, x_plus\n                return None, None\n\n            x_minus = np.clip(x0 - a * v_unit, lb, ub)\n            out_m = safe_eval(x_minus)\n            if out_m[0] is None:\n                return None, None\n            f_minus, x_minus = out_m\n\n            # fit parabola f(t) = A t^2 + B t + C using t = -a,0,+a\n            # Solve for A,B:\n            # f(+a) = A a^2 + B a + C\n            # f(0)  = C\n            # f(-a) = A a^2 - B a + C\n            C = f0\n            A = ((f_plus + f_minus) / 2.0 - C) / (a * a + 1e-20)\n            B = (f_plus - f_minus) / (2.0 * a + 1e-20)\n            if abs(A) < 1e-18:\n                # essentially linear -> pick best among sampled\n                if f_plus < f0 - 1e-12:\n                    return f_plus, x_plus\n                if f_minus < f0 - 1e-12:\n                    return f_minus, x_minus\n                return None, None\n            t_star = -B / (2.0 * A)\n            # limit t_star to reasonable range\n            if abs(t_star) > 2.0 * a:\n                return None, None\n            x_star = np.clip(x0 + t_star * v_unit, lb, ub)\n            if evals >= self.budget:\n                return None, None\n            out_s = safe_eval(x_star)\n            if out_s[0] is None:\n                return None, None\n            f_s, x_s = out_s\n            if f_s < f0 - 1e-12:\n                return f_s, x_s\n            # else try if direct +- gave improvement\n            if f_plus < f0 - 1e-12:\n                return f_plus, x_plus\n            if f_minus < f0 - 1e-12:\n                return f_minus, x_minus\n            return None, None\n\n        # periodic diagonal quadratic model (lighter than original, uses top n+3 points)\n        def try_diag_model():\n            nonlocal evals, f_best, x_best, x_cur, f_cur, step\n            if len(X_archive) < (n + 3):\n                return False\n            k = min(len(X_archive), n + 10)\n            idx = np.argsort(F_archive)[:k]\n            Xm = np.array([X_archive[i] for i in idx])\n            Fm = np.array([F_archive[i] for i in idx])\n            # design: x_j^2 and x_j and intercept\n            A = np.hstack([Xm ** 2, Xm, np.ones((Xm.shape[0], 1))])\n            reg = 1e-6\n            try:\n                sol, *_ = np.linalg.lstsq(A.T @ A + reg * np.eye(A.shape[1]), A.T @ Fm, rcond=None)\n                sol = sol.flatten()\n                a = sol[:n]\n                b = sol[n:2 * n]\n                # safe diagonal minimizer\n                a_safe = a.copy()\n                mask = np.abs(a_safe) < 1e-8\n                a_safe[mask] = np.sign(a_safe[mask]) * 1e-8 + 1e-8\n                x_star = -0.5 * b / (a_safe + 1e-20)\n                # propose a conservative move towards diag-minimizer\n                x_prop = np.clip(x_cur + 0.5 * (x_star - x_cur), lb, ub)\n                if evals < self.budget:\n                    out = safe_eval(x_prop)\n                    if out[0] is None:\n                        return False\n                    f_prop, x_prop = out\n                    if f_prop < f_best:\n                        f_best = f_prop\n                        x_best = x_prop.copy()\n                        x_cur = x_prop.copy()\n                        f_cur = f_prop\n                        step = min(step * succ_grow, max_step)\n                        return True\n            except Exception:\n                return False\n            return False\n\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n\n            # refresh arms occasionally: replace worst-performing random arm by new random direction\n            if iter_count % max(10, n // 2) == 0:\n                # ensure arms length consistent with pulls arrays\n                if len(arms) != len(pulls):\n                    pulls = np.resize(pulls, len(arms))\n                    rewards = np.resize(rewards, len(arms))\n                    avg_reward = np.resize(avg_reward, len(arms))\n                # replace one of the random ones (not coordinates) if available\n                if len(arms) > n:\n                    # identify indices for random arms (after first n)\n                    cand = list(range(n, len(arms)))\n                    idx_replace = self.rng.choice(cand)\n                    new_dirs = make_random_unit(1)\n                    arms[idx_replace] = new_dirs[0]\n                    pulls[idx_replace] = 0\n                    rewards[idx_replace] = 0.0\n                    avg_reward[idx_replace] = 0.0\n\n            # choose arm by UCB\n            total_pulls = np.sum(pulls) + 1\n            # ensure arrays large enough\n            m = len(arms)\n            ucb_scores = np.zeros(m)\n            for i in range(m):\n                if pulls[i] == 0:\n                    ucb_scores[i] = 1e9 + self.rng.random()  # force exploration\n                else:\n                    ucb_scores[i] = avg_reward[i] + self.ucb_c * np.sqrt(np.log(total_pulls) / pulls[i])\n            arm_idx = int(np.argmax(ucb_scores))\n            v = arms[arm_idx].copy()\n            # apply per-coordinate RMS scaling to direction\n            v = v * np.sqrt(coord_rms + eps)\n            v = v / (np.linalg.norm(v) + 1e-20)  # renormalize\n\n            # sample a magnitude (mirrored) from uniform in [-1,1] scaled by step\n            alpha = self.rng.uniform(-1.0, 1.0) * step\n            x_try = np.clip(x_cur + alpha * v, lb, ub)\n            out = safe_eval(x_try)\n            if out[0] is None:\n                break\n            f_try, x_try = out\n\n            # bandit bookkeeping pre-update\n            pulls[arm_idx] += 1\n            total_pulls += 1\n\n            improved = False\n            reward = 0.0\n\n            if f_try < f_cur - 1e-12:\n                # accept\n                prev_x = x_cur.copy()\n                x_cur = x_try.copy()\n                f_cur = f_try\n                improved = True\n                reward = max(0.0, f_cur - f_try) + 0.0  # though f_cur == f_try here, reward approx 0; use best improvement measure below\n            else:\n                # if not improved check short parabolic refinement with small probability or if neighboring mirrored evaluation\n                if self.rng.random() < 0.45 and (self.budget - evals) >= 2:\n                    # try parabolic refine centered at x_cur with amplitude = |alpha|\n                    a = max(abs(alpha), 0.25 * step)\n                    pr = parabolic_refine(x_cur, f_cur, v, a, allowed_evals=min(2, self.budget - evals))\n                    if pr[0] is not None:\n                        f_new, x_new = pr\n                        if f_new < f_cur - 1e-12:\n                            prev_x = x_cur.copy()\n                            x_cur = x_new.copy()\n                            f_cur = f_new\n                            improved = True\n                            reward = max(0.0, f_cur - f_new)\n\n            # if improvement update best, arm rewards, coord rms and step\n            if improved:\n                if f_cur < f_best:\n                    f_best = f_cur\n                    x_best = x_cur.copy()\n                # compute actual improvement as positive number (previous best locally)\n                # reward: reduction in objective relative to previous value stored in 'prev_x'\n                try:\n                    delta = float(F_archive[-1]) - f_cur if len(F_archive) > 0 else 0.0\n                except Exception:\n                    delta = 0.0\n                reward = max(reward, delta, 1e-12)\n                rewards[arm_idx] += reward\n                avg_reward[arm_idx] = rewards[arm_idx] / pulls[arm_idx]\n                # update per-coordinate RMS based on step taken\n                step_vector = (x_cur - prev_x)\n                denom = (np.linalg.norm(step_vector) + 1e-20)\n                inst = (step_vector / denom) ** 2\n                coord_rms = (1 - rms_alpha) * coord_rms + rms_alpha * (inst + 1e-12)\n                # grow step gently\n                step = min(step * succ_grow, max_step)\n                # store direction in memory (replace last)\n                new_unit = (x_cur - prev_x)\n                dn = np.linalg.norm(new_unit)\n                if dn > 1e-18:\n                    new_unit = new_unit / dn\n                    # replace or append as additional arm (bounded)\n                    if len(arms) < n + self.memory_size:\n                        arms.append(new_unit.copy())\n                        pulls = np.pad(pulls, (0,1), 'constant', constant_values=0)\n                        rewards = np.pad(rewards, (0,1), 'constant', constant_values=0.0)\n                        avg_reward = np.pad(avg_reward, (0,1), 'constant', constant_values=0.0)\n                    else:\n                        # replace worst arm among the extra ones (not coordinates)\n                        extra_start = n\n                        if extra_start < len(arms):\n                            extra_scores = avg_reward[extra_start:]\n                            worst = extra_start + int(np.argmin(extra_scores))\n                            arms[worst] = new_unit.copy()\n                            pulls[worst] = 0\n                            rewards[worst] = 0.0\n                            avg_reward[worst] = 0.0\n            else:\n                # failure: shrink step\n                step = max(step * succ_shrink, min_step)\n                # small negative reward to discourage bad arms slightly\n                rewards[arm_idx] *= 0.995\n                avg_reward[arm_idx] = rewards[arm_idx] / max(1, pulls[arm_idx])\n\n            # occasional Levy/Cauchy jump to escape basins\n            if (self.rng.random() < self.levy_prob) and (self.budget - evals > 0):\n                jump = np.tan(np.pi * (self.rng.random() - 0.5))\n                dir_rand = self.rng.standard_normal(n)\n                dir_rand /= (np.linalg.norm(dir_rand) + 1e-20)\n                mag = self.levy_scale * step * jump\n                x_jump = np.clip(x_best + mag * dir_rand, lb, ub)\n                out_j = safe_eval(x_jump)\n                if out_j[0] is None:\n                    break\n                f_j, x_j = out_j\n                if f_j < f_best:\n                    f_best = f_j\n                    x_best = x_j.copy()\n                    # accept to current as escape\n                    x_cur = x_best.copy()\n                    f_cur = f_best\n                    step = min(step * succ_grow, max_step)\n\n            # periodic diagonal model attempt\n            if (iter_count % self.model_every == 0) and (self.budget - evals > 0):\n                try_diag_model()\n\n            # prune archive occasionally (budget aware)\n            if len(X_archive) > max(2000, 50 * n):\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                keep_rest = rest[::max(1, len(rest) // (max(0, 1000)))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # tiny safeguard\n            if step < min_step:\n                step = min_step\n\n            # early exit if excellent\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ACCS scored 0.118 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "83618236-2a28-4ed9-8668-80c937a24407", "operator": null, "metadata": {"aucs": [0.016563734466303548, 0.15118182328266228, 0.1683100901220369, 0.09021610492124732, 0.08919897482108408, 0.13786951087549038, 0.16516613032866856, 0.13933430574493333, 0.13511601447901123, 0.08261344252136926]}, "task_prompt": ""}
{"id": "ca1d0d75-ffb2-4c11-a5aa-bc0deb432b6c", "fitness": 0.24543858258823179, "name": "LRALeTR", "description": "The algorithm builds a compact, adaptive low-rank subspace from recent successful step directions (dir_memory, dir_weights, mem_size) and uses that basis to bias anisotropic sampling and cheap subspace models instead of full-space covariance estimation. Exploration mixes controlled subspace Gaussian radial probes (with archive-difference nudges and coefficient scaling by dir_weights) and occasional heavy‑tailed Levy-like full-space jumps (levy_prob ≈ 0.14) while the trust-region radius is initialized proportional to domain range and adapted per batch by a success ratio (success_target=0.2, succ_inc=1.18, fail_dec=0.72). Exploitation is enabled by budget-aware micro backtracking line-searches and periodic low-dimensional quadratic fits in the learned subspace to propose minimizers from archive points, with strict improvement acceptance and memory updates. Practical budget-awareness features include archive pruning, occasional small re‑initializations around the best point to escape stagnation, and conservative numerical regularization in model fitting.", "code": "import numpy as np\n\nclass LRALeTR:\n    \"\"\"\n    Low-Rank Adaptive Levy Trust-Region Search (LRALeTR)\n\n    One-line: Build a low-rank subspace from recent successful directions,\n    sample inside that adaptive subspace, occasionally perform heavy-tailed\n    jumps and archive-differential nudges, adapt a trust-region radius from\n    success statistics, and inject cheap subspace-quadratic proposals and\n    micro line-searches for efficient budget-aware global-local search.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 mem_size=12, model_every=20, levy_prob=0.14, init_radius=1.6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.mem_size = int(mem_size)\n        self.model_every = int(model_every)\n        self.levy_prob = float(levy_prob)\n        self.init_radius = float(init_radius)\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # take bounds from func if available; default to [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0\n            ub = 5.0\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        # initial state\n        x_cur = self.rng.uniform(lb, ub)\n        evals = 0\n\n        # archive\n        X_archive = []\n        F_archive = []\n\n        # safe eval wrapper (respects budget)\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            x_clipped = np.clip(x, lb, ub)\n            try:\n                f = float(func(x_clipped))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x_clipped.copy())\n            F_archive.append(f)\n            return f, x_clipped\n\n        # initial evaluation\n        out = safe_eval(x_cur)\n        if out[0] is None:\n            return float(np.inf), np.zeros(n)\n        f_cur, x_cur = out\n        f_best = float(f_cur)\n        x_best = x_cur.copy()\n\n        # memory of recent successful (signed) steps (low-rank building blocks)\n        dir_memory = []   # stores vectors (u) with lengths\n        dir_weights = []  # recency/importance weights\n\n        # trust-region radius (scale in full space)\n        radius = float(self.init_radius) * np.mean(domain_range)\n        min_radius = 1e-9 * max(1.0, np.mean(domain_range))\n        max_radius = 4.0 * np.mean(domain_range)\n\n        # parameters for adaptation\n        batch_probes = 18\n        succ_inc = 1.18\n        fail_dec = 0.72\n        success_target = 0.2   # desired success ratio per batch\n\n        iter_count = 0\n\n        # tiny backtracking line-search (few evals)\n        def backtrack_line_search(x0, f0, d, init_alpha, max_evals=6):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            alpha = init_alpha\n            c = 0.6  # sufficient decrease factor\n            rho = 0.5\n            tries = 0\n            best_f = f0\n            best_x = x0.copy()\n            while tries < max_evals and evals < self.budget:\n                x_try = np.clip(x0 + alpha * d, lb, ub)\n                out = safe_eval(x_try)\n                if out[0] is None:\n                    break\n                f_try, x_try = out\n                tries += 1\n                if f_try < best_f - 1e-12:\n                    best_f = f_try\n                    best_x = x_try.copy()\n                    # try to go further\n                    alpha = alpha / rho\n                else:\n                    # reduce step\n                    alpha = alpha * rho\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # build low-rank basis from dir_memory\n        def build_basis(k):\n            # if memory insufficient, return random orthonormal basis in full space\n            if len(dir_memory) == 0:\n                R = self.rng.standard_normal(size=(n, k))\n                Q, _ = np.linalg.qr(R, mode='reduced')\n                return Q[:, :k]\n            # stack weighted memory columns (n x m)\n            M = np.column_stack(dir_memory)  # shape n x m (m <= mem_size)\n            # center columns by their weighted mean to avoid dominance\n            weights = np.array(dir_weights).reshape(1, -1)\n            if weights.sum() <= 0:\n                W = np.ones(M.shape[1])\n            else:\n                W = weights.flatten() / (weights.sum() + 1e-20)\n            Mw = M * W\n            # compute SVD on the small side by computing Mw @ Mw.T (n x n might be large),\n            # but since m small (<= mem_size), do economy SVD of M (n x m)\n            try:\n                U, s, Vt = np.linalg.svd(Mw, full_matrices=False)\n                k_eff = min(k, U.shape[1])\n                basis = U[:, :k_eff]\n                if basis.shape[1] < k:\n                    # complete with random orthonormal complement\n                    R = self.rng.standard_normal(size=(n, k - basis.shape[1]))\n                    Q2, _ = np.linalg.qr(R - basis @ (basis.T @ R))\n                    basis = np.column_stack((basis, Q2[:, :(k - basis.shape[1])]))\n                return basis[:, :k]\n            except Exception:\n                R = self.rng.standard_normal(size=(n, k))\n                Q, _ = np.linalg.qr(R, mode='reduced')\n                return Q[:, :k]\n\n        # periodic subspace quadratic proposal (fit quadratic in small subspace of size k)\n        def subspace_quadratic_proposal(basis, x_center, topk=30):\n            # project archive points to coordinates in the basis around x_center\n            if len(X_archive) < (basis.shape[1] + 5):\n                return False\n            k = basis.shape[1]\n            # select best topk points\n            ksel = min(len(X_archive), topk)\n            idx = np.argsort(F_archive)[:ksel]\n            Xm = np.array([X_archive[i] for i in idx])\n            # coefficients in subspace: c = basis.T @ (x - x_center)\n            C = (Xm - x_center) @ basis  # shape (ksel, k)\n            Fm = np.array([F_archive[i] for i in idx])\n            # design matrix with quadratic diagonal terms and linear terms (no cross terms, cheap)\n            # columns: c1^2,...,ck^2, c1,...,ck, const\n            A = np.hstack([C ** 2, C, np.ones((C.shape[0], 1))])\n            reg = 1e-6\n            try:\n                # solve ridge: (A^T A + reg I) sol = A^T F\n                ATA = A.T @ A\n                rhs = A.T @ Fm\n                sol = np.linalg.solve(ATA + reg * np.eye(ATA.shape[0]), rhs)\n                a = sol[:k]     # quadratic diag\n                b = sol[k:2 * k]  # linear\n                # safe minimizer in subspace approx: -b / (2a) with stabilization\n                a_safe = a.copy()\n                a_safe[np.abs(a_safe) < 1e-8] = 1e-8 * np.sign(a_safe[np.abs(a_safe) < 1e-8]) + 1e-8\n                c_star = -0.5 * b / (a_safe + 1e-20)\n                # trust-scale the proposal\n                c_star = 0.6 * c_star\n                x_prop = np.clip(x_center + basis @ c_star, lb, ub)\n                out = safe_eval(x_prop)\n                if out[0] is None:\n                    return False\n                f_prop, x_prop = out\n                nonlocal_flag = False\n                if f_prop < f_best - 1e-12:\n                    # accept and also set as current\n                    return x_prop, f_prop\n                return False\n            except Exception:\n                return False\n\n        # main loop: operate in batches to adapt radius from success ratio\n        while evals < self.budget:\n            iter_count += 1\n            # dynamic subspace size k: use log2 scaling to be different from sqrt\n            k = max(1, min(n, int(np.ceil(np.log2(n + 1)) + 1)))\n            probes = max(8, 3 * k)\n\n            # build basis\n            basis = build_basis(k)\n\n            successes = 0\n            attempts = 0\n\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                attempts += 1\n\n                # choose exploration mode\n                if self.rng.random() < self.levy_prob:\n                    # heavy-tailed Cauchy/Levy-like jump, full-space\n                    # use scaled Cauchy (tan(pi*(u-0.5))) seeded\n                    u = self.rng.random(n)\n                    levy = np.tan(np.pi * (u - 0.5))\n                    # scale and orient by some dominant memory direction if available\n                    if dir_memory:\n                        udir = dir_memory[0]\n                        jump = (self.levy_prob * radius) * levy * (0.4 + 0.6 * np.abs(udir))\n                    else:\n                        jump = 0.6 * radius * levy\n                    x_try = np.clip(x_cur + jump, lb, ub)\n                else:\n                    # subspace Gaussian sampling and radial scaling\n                    coeffs = self.rng.normal(size=k)\n                    # impose anisotropy by scaling coefficients ~ (1 + s_i) where s_i from singulars ~ dir_weights\n                    if len(dir_weights) >= k:\n                        svals = np.array(dir_weights[:k])\n                        svals = (svals - svals.min()) if svals.max() != svals.min() else (svals * 0.0 + 0.5)\n                        coeffs = coeffs * (1.0 + 0.8 * (svals / (svals.max() + 1e-20)))\n                    # create direction in full space\n                    d = basis @ coeffs\n                    dnrm = np.linalg.norm(d)\n                    if dnrm == 0:\n                        continue\n                    d = d / dnrm\n                    # radial distance sampled from folded normal scaled by radius\n                    r = np.abs(self.rng.normal(loc=0.6 * radius, scale=0.3 * radius))\n                    # small archive-difference nudge occasionally\n                    x_try = x_cur + r * d\n                    if (len(X_archive) >= 3) and (self.rng.random() < 0.13):\n                        i1, i2 = self.rng.choice(len(X_archive), size=2, replace=False)\n                        de = 0.35 * (X_archive[i1] - X_archive[i2])\n                        x_try = x_try + 0.5 * de\n                    x_try = np.clip(x_try, lb, ub)\n\n                # evaluate\n                out = safe_eval(x_try)\n                if out[0] is None:\n                    break\n                f_try, x_try = out\n\n                # accept criterion (strict improvement)\n                if f_try < f_cur - 1e-12:\n                    # accept\n                    prev_x = x_cur.copy()\n                    x_cur = x_try.copy()\n                    f_cur = f_try\n                    successes += 1\n                    # update best\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n                    # compute step vector and store in memory\n                    step_vec = x_cur - prev_x\n                    step_norm = np.linalg.norm(step_vec)\n                    if step_norm > 1e-16:\n                        u = step_vec / step_norm\n                        # insert LRU\n                        dir_memory.insert(0, u.copy())\n                        # weight proportional to step size and recency\n                        dir_weights.insert(0, step_norm + 1e-12)\n                        # trim memory\n                        if len(dir_memory) > self.mem_size:\n                            dir_memory.pop()\n                            dir_weights.pop()\n                    # tiny backtracking line-search along accepted direction to exploit\n                    if (self.budget - evals) >= 2:\n                        ls_out = backtrack_line_search(x_cur, f_cur, step_vec, init_alpha=radius * 0.8, max_evals=4)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                x_cur = x_ls.copy()\n                                f_cur = f_ls\n                                if f_ls < f_best:\n                                    f_best = f_ls\n                                    x_best = x_ls.copy()\n                else:\n                    # maybe short local line-search from current along direction tried with small prob\n                    if (self.rng.random() < 0.05) and (self.budget - evals >= 3):\n                        # if the candidate direction was computed, try small ls\n                        if 'd' in locals():\n                            ls_out = backtrack_line_search(x_cur, f_cur, d if 'd' in locals() else (x_try - x_cur), init_alpha=0.5 * radius, max_evals=3)\n                            if ls_out[0] is not None:\n                                f_ls, x_ls = ls_out\n                                if f_ls < f_cur - 1e-12:\n                                    prev_x = x_cur.copy()\n                                    x_cur = x_ls.copy()\n                                    f_cur = f_ls\n                                    if f_ls < f_best:\n                                        f_best = f_ls\n                                        x_best = x_ls.copy()\n                                    step_vec = x_cur - prev_x\n                                    step_norm = np.linalg.norm(step_vec)\n                                    if step_norm > 1e-16:\n                                        u = step_vec / step_norm\n                                        dir_memory.insert(0, u.copy())\n                                        dir_weights.insert(0, step_norm + 1e-12)\n                                        if len(dir_memory) > self.mem_size:\n                                            dir_memory.pop()\n                                            dir_weights.pop()\n                                        successes += 1\n\n            # adjust radius by success ratio\n            succ_ratio = successes / max(1.0, attempts)\n            if succ_ratio > success_target:\n                radius = min(radius * succ_inc, max_radius)\n            else:\n                radius = max(radius * fail_dec, min_radius)\n\n            # probabilistic subspace quadratic proposal every model_every iterations\n            if (iter_count % self.model_every == 0) and (self.budget - evals > 0):\n                # small attempt: if enough archive data, try quadratic in a subspace\n                try_k = max(1, min(k, 6))\n                small_basis = build_basis(try_k)\n                res = subspace_quadratic_proposal(small_basis, x_cur, topk= min(40, max(10, len(X_archive))))\n                if res and res is not False:\n                    x_prop, f_prop = res\n                    if f_prop < f_cur - 1e-12:\n                        prev = x_cur.copy()\n                        x_cur = x_prop.copy()\n                        f_cur = f_prop\n                        if f_prop < f_best:\n                            f_best = f_prop\n                            x_best = x_prop.copy()\n                        # record direction\n                        step_vec = x_cur - prev\n                        sn = np.linalg.norm(step_vec)\n                        if sn > 1e-16:\n                            dir_memory.insert(0, (step_vec / sn).copy())\n                            dir_weights.insert(0, sn + 1e-12)\n                            if len(dir_memory) > self.mem_size:\n                                dir_memory.pop(); dir_weights.pop()\n\n            # occasional small random re-initialization around best to escape stagnation\n            if (self.rng.random() < 0.02) and (self.budget - evals > 0):\n                perturb = 0.6 * radius * self.rng.normal(size=n)\n                x_nudge = np.clip(x_best + perturb, lb, ub)\n                out = safe_eval(x_nudge)\n                if out[0] is None:\n                    break\n                fn, xn = out\n                if fn < f_best:\n                    f_best = fn\n                    x_best = xn.copy()\n                    # accept as current to exploit\n                    x_cur = x_best.copy()\n                    f_cur = f_best\n\n            # prune archive if extremely large (budget-minded)\n            max_archive = max(2000, 60 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:400]\n                rest = idx_sorted[400:]\n                keep_rest = rest[::max(1, len(rest) // (max_archive - 400))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # tiny safeguard on radius\n            if radius < min_radius:\n                radius = min_radius\n            if radius > max_radius:\n                radius = max_radius\n\n            # early stopping if very small objective\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm LRALeTR scored 0.245 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "83618236-2a28-4ed9-8668-80c937a24407", "operator": null, "metadata": {"aucs": [0.11360178106304997, 0.14744938485227221, 0.31042020889140565, 0.8619663224542176, 0.19362782233340503, 0.20688410193022666, 0.19612045762108876, 0.1450644659430913, 0.16495957787934368, 0.1142917029142172]}, "task_prompt": ""}
{"id": "599d38c9-7696-4354-862c-1fe195a89045", "fitness": 0.1702620736142178, "name": "ASE_TP", "description": "ASE-TP is an adaptive subspace-evolution heuristic that builds small k-dimensional orthonormal search subspaces (QR on Gaussian/random + reuse of up to half LRU memory directions) and probes each subspace with mirrored Gaussian coefficient samples (probes = max(6, 3*k)) to get budget-efficient, directional exploration. Step control is conservative-to-aggressive: a relatively small initial step (0.25·avg_range) with aggressive grow/shrink (succ_grow=1.20, succ_shrink=0.70, min_step floor) and budget-aware focused line-search refinements after successes or occasional failures. Diversification/escape mechanisms include tempered symmetric Pareto long jumps (pareto_alpha≈1.6) both along stored memory directions and as global nudges, plus occasional mild archive-difference (DE-style) perturbations to inject population-like diversity. Adaptation and modeling: a diagonal quadratic surrogate (ridge ~1e-4, trust 0.5, invoked periodically) proposes conservative moves, while per-coordinate inverse-RMS scaling (smoothing alpha≈0.12) biases steps toward low-variance coordinates; archive pruning and strict budget-aware safe evaluations keep memory and function-eval usage under control.", "code": "import numpy as np\n\nclass ASE_TP:\n    \"\"\"\n    Adaptive Subspace Evolution with Tempered-Pareto Jumps (ASE-TP)\n\n    Main ideas (differences vs. the provided MDARSS):\n    - slightly smaller initial step and more aggressive growth (grow/shrink factors changed),\n      different min_step scaling.\n    - subspace dimension computed with a different heuristic (floor(sqrt(n)/1.4)).\n    - per-coordinate scaling uses inverse-RMS style (emphasize low-variance coords),\n      and a different smoothing alpha.\n    - LRU memory default larger (12) and occasionally used for tempered Pareto directional jumps.\n    - mirrored sampling retained but with more probes per subspace (3*k, minimum 6).\n    - tempered Pareto heavy-tail jump (symmetric Pareto) instead of plain Cauchy to diversify escapes.\n    - simpler, budget-conscious focused line-search (few samples + local refine) with distinct scheduling.\n    - diagonal quadratic model still used but with a slightly larger ridge and trust-scaling.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=12, model_every=12, mem_jump_prob=0.22, pareto_alpha=1.6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.model_every = int(model_every)\n        self.mem_jump_prob = float(mem_jump_prob)\n        self.pareto_alpha = float(pareto_alpha)\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        avg_range = float(np.mean(domain_range))\n\n        # initial point\n        x_cur = self.rng.uniform(lb, ub)\n        evals = 0\n\n        # archive\n        X_archive = []\n        F_archive = []\n\n        # safe eval wrapper\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            x_clipped = np.clip(x, lb, ub)\n            try:\n                f = float(func(x_clipped))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x_clipped.copy())\n            F_archive.append(f)\n            return f, x_clipped\n\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            return float(np.inf), np.zeros(n)\n        f_best = float(f_cur)\n        x_best = x_cur.copy()\n\n        # step settings (different from MDARSS)\n        step = 0.25 * avg_range\n        min_step = 1e-8 * max(1.0, avg_range)\n        succ_grow = 1.20\n        succ_shrink = 0.70\n        max_step = 6.0 * avg_range\n\n        # directional memory (LRU)\n        dir_memory = []\n\n        # per-coordinate \"inverse-RMS\" emphasis on small-variation coords\n        coord_var = np.ones(n) * 1e-2\n        coord_alpha = 0.12  # different smoothing\n\n        iter_count = 0\n\n        # small focused line search: sample a tiny set of offsets then refine around best\n        def focused_line_search(x0, f0, d, init_step, max_evals=6):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            remaining = min(max_evals, self.budget - evals)\n            if remaining <= 0:\n                return None, None\n            # coarse probing at multiples\n            multiples = [0.0, 0.5, -0.5, 1.0, -1.0]\n            # trim to budget\n            best_f = f0\n            best_x = x0.copy()\n            used = 0\n            for m in multiples:\n                if used >= remaining:\n                    break\n                t = m * init_step\n                xt = np.clip(x0 + t * d, lb, ub)\n                out = safe_eval(xt)\n                if out[0] is None:\n                    break\n                used += 1\n                ft, xt = out\n                if ft < best_f:\n                    best_f, best_x = ft, xt.copy()\n            # local refine: if improvement found, sample two nearby points\n            if best_f < f0 and (used + 2) <= remaining:\n                # small perturbations around best_t\n                for rel in [0.3, -0.3]:\n                    if used >= remaining:\n                        break\n                    # approximate direction vector from x0 to best_x\n                    t_dir = best_x - x0\n                    if np.linalg.norm(t_dir) == 0:\n                        t_vec = d\n                    else:\n                        t_vec = t_dir / (np.linalg.norm(t_dir) + 1e-20)\n                    xt = np.clip(best_x + rel * init_step * t_vec, lb, ub)\n                    out = safe_eval(xt)\n                    if out[0] is None:\n                        break\n                    used += 1\n                    ft, xt = out\n                    if ft < best_f:\n                        best_f, best_x = ft, xt.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # diagonal quadratic model (slightly larger ridge and trust-scaling)\n        def diagonal_model_proposal():\n            nonlocal evals, f_best, x_best, x_cur, f_cur\n            if len(X_archive) < (n + 3):\n                return False\n            k = min(len(X_archive), 3 * n)\n            idx = np.argsort(F_archive)[:k]\n            Xm = np.array([X_archive[i] for i in idx])\n            Fm = np.array([F_archive[i] for i in idx])\n            # design: x_j^2, x_j, intercept\n            A = np.hstack([Xm ** 2, Xm, np.ones((Xm.shape[0], 1))])\n            reg = 1e-4\n            try:\n                ATA = A.T @ A + reg * np.eye(A.shape[1])\n                sol = np.linalg.solve(ATA, A.T @ Fm)\n                a = sol[:n]\n                b = sol[n:2 * n]\n                # regularize a to avoid division by zero and ensure not wildly negative\n                a_safe = a.copy()\n                small = np.abs(a_safe) < 1e-6\n                a_safe[small] = np.sign(a_safe[small]) * 1e-6 + 1e-6\n                x_star = -0.5 * (b / (a_safe + 1e-20))\n                # trust-scale proposal: move fraction towards x_star from current\n                trust = 0.5  # more conservative than MDARSS's 0.6\n                x_prop = np.clip(x_cur + trust * (x_star - x_cur), lb, ub)\n                if evals < self.budget:\n                    out = safe_eval(x_prop)\n                    if out[0] is None:\n                        return False\n                    f_prop, x_prop = out\n                    if f_prop < f_best:\n                        f_best = f_prop\n                        x_best = x_prop.copy()\n                        # accept sometimes as current to exploit model\n                        # only accept if sufficiently better than current\n                        if f_prop < f_cur - 1e-9:\n                            x_cur = x_prop.copy()\n                            f_cur = f_prop\n                        return True\n            except Exception:\n                return False\n            return False\n\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n            # subspace dimension heuristic (different)\n            k = max(1, int(np.floor(max(1.0, np.sqrt(n) / 1.4))))\n            probes = max(6, 3 * k)\n\n            # build basis: reuse half from memory if available\n            use_mem = min(len(dir_memory), k // 2)\n            basis_cols = []\n            if use_mem > 0:\n                for i in range(use_mem):\n                    basis_cols.append(dir_memory[i].copy())\n            needed = k - len(basis_cols)\n            if needed > 0:\n                R = self.rng.standard_normal(size=(n, needed))\n                if basis_cols:\n                    R = np.column_stack((np.column_stack(basis_cols), R))\n                # QR and take first k orthonormal vectors\n                Q, _ = np.linalg.qr(R, mode='reduced')\n                basis = Q[:, :k]\n            else:\n                Q, _ = np.linalg.qr(np.column_stack(basis_cols), mode='reduced')\n                basis = Q[:, :k]\n\n            # mirrored coefficients\n            coeffs_list = []\n            half = (probes + 1) // 2\n            for _ in range(half):\n                coeffs_list.append(self.rng.standard_normal(k))\n            coeffs_list = coeffs_list + [(-c) for c in coeffs_list]\n            coeffs_list = coeffs_list[:probes]\n\n            improved_in_round = False\n            successes = 0\n\n            for coeffs in coeffs_list:\n                if evals >= self.budget:\n                    break\n                # map to full space\n                d = basis @ coeffs\n                dnrm = np.linalg.norm(d)\n                if dnrm == 0:\n                    continue\n                d = d / dnrm\n\n                # per-coordinate inverse-RMS scaling (emphasize low var coords)\n                coord_scale = 1.0 / (np.sqrt(coord_var) + 1e-12)\n                # normalize scale to avoid extreme stretching\n                coord_scale = coord_scale / np.mean(coord_scale)\n                d = d * coord_scale\n                if np.linalg.norm(d) == 0:\n                    continue\n                d = d / (np.linalg.norm(d) + 1e-20)\n\n                alpha = self.rng.uniform(-step, step)\n                x_try = x_cur + alpha * d\n\n                # occasional archive-difference (DE style) but milder coefficients\n                if (len(X_archive) >= 3) and (self.rng.random() < 0.10):\n                    i1, i2 = self.rng.choice(len(X_archive), size=2, replace=False)\n                    de = 0.45 * (X_archive[i1] - X_archive[i2])\n                    x_try = x_try + 0.4 * de\n\n                # tempered Pareto long jump using memory directions\n                if dir_memory and (self.rng.random() < self.mem_jump_prob):\n                    u = dir_memory[self.rng.integers(len(dir_memory))]\n                    # symmetric Pareto: sign * pareto\n                    p = self.rng.pareto(self.pareto_alpha)\n                    sign = -1.0 if self.rng.random() < 0.5 else 1.0\n                    # temper by logistic-ish damping to avoid astronomic jumps\n                    damp = 1.0 / (1.0 + 0.2 * p)\n                    jump = sign * p * damp\n                    x_try = x_try + (0.8 * jump * step) * u\n\n                x_try = np.clip(x_try, lb, ub)\n                out = safe_eval(x_try)\n                if out[0] is None:\n                    break\n                f_try, x_try = out\n\n                if f_try < f_cur - 1e-12:\n                    # accept\n                    prev_x = x_cur.copy()\n                    x_cur = x_try.copy()\n                    f_cur = f_try\n                    successes += 1\n                    improved_in_round = True\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n                    # store direction\n                    dir_succ = x_cur - prev_x\n                    dn = np.linalg.norm(dir_succ)\n                    if dn > 0:\n                        dir_unit = dir_succ / dn\n                        dir_memory.insert(0, dir_unit.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                    # update coord_var with relative step (different normalization)\n                    y = (x_cur - prev_x) / (avg_range + 1e-20)\n                    coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (y ** 2 + 1e-12)\n                    # focused line-search along successful direction (budget-aware)\n                    remaining_budget = self.budget - evals\n                    if remaining_budget >= 2:\n                        ls_max = min(8, remaining_budget)\n                        ls_out = focused_line_search(x_cur, f_cur, dir_unit if dn > 0 else d,\n                                                     init_step=abs(alpha) if abs(alpha) > 1e-12 else step,\n                                                     max_evals=ls_max)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                x_cur = x_ls.copy()\n                                f_cur = f_ls\n                                if f_ls < f_best:\n                                    f_best = f_ls\n                                    x_best = x_ls.copy()\n                    # grow step on success (more aggressive)\n                    step = min(step * succ_grow, max_step)\n                else:\n                    # occasional attempt: short focused line-search from current along d\n                    if (self.rng.random() < 0.06) and (self.budget - evals >= 3):\n                        ls_max = min(5, self.budget - evals)\n                        ls_out = focused_line_search(x_cur, f_cur, d, init_step=step, max_evals=ls_max)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                prev_x = x_cur.copy()\n                                x_cur = x_ls.copy()\n                                f_cur = f_ls\n                                if f_ls < f_best:\n                                    f_best = f_ls\n                                    x_best = x_ls.copy()\n                                dir_succ = x_cur - prev_x\n                                dn = np.linalg.norm(dir_succ)\n                                if dn > 0:\n                                    dir_unit = dir_succ / dn\n                                    dir_memory.insert(0, dir_unit.copy())\n                                    if len(dir_memory) > self.memory_size:\n                                        dir_memory.pop()\n                                # update coord_var in normalized sense\n                                y = (x_cur - prev_x) / (np.linalg.norm(x_cur - prev_x) + 1e-20)\n                                coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (y ** 2 + 1e-12)\n                                successes += 1\n                                improved_in_round = True\n\n            # after probing round\n            if not improved_in_round:\n                step = max(step * succ_shrink, min_step)\n\n            # prune archive if too large (keeps diversity)\n            max_archive = max(1500, 40 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:150]\n                rest = idx_sorted[150:]\n                keep_rest = rest[::max(1, len(rest) // (max_archive - 150))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # periodic diagonal model attempt\n            if (iter_count % self.model_every == 0) and (self.budget - evals > 0):\n                diagonal_model_proposal()\n\n            # occasional tempered Pareto nudge around best if stagnating\n            if (self.rng.random() < 0.018) and (self.budget - evals > 0):\n                p = self.rng.pareto(self.pareto_alpha, size=n)\n                signs = self.rng.choice([-1.0, 1.0], size=n)\n                damp = 1.0 / (1.0 + 0.15 * p)\n                jump = signs * p * damp\n                x_nudge = np.clip(x_best + 0.6 * step * jump, lb, ub)\n                out = safe_eval(x_nudge)\n                if out[0] is None:\n                    break\n                f_nudge, x_nudge = out\n                if f_nudge < f_best:\n                    f_best = f_nudge\n                    x_best = x_nudge.copy()\n                    x_cur = x_best.copy()\n                    f_cur = f_best\n\n            # safeguard step floor\n            if step < min_step:\n                step = min_step\n\n            # exit criterion for very good solutions\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ASE_TP scored 0.170 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "83618236-2a28-4ed9-8668-80c937a24407", "operator": null, "metadata": {"aucs": [0.06518443777342808, 0.15861311194776528, 0.3283751415271823, 0.18142167697933442, 0.1924346458163072, 0.1213799950757074, 0.16522836327446, 0.179762734856745, 0.2122964680118179, 0.09792416087943012]}, "task_prompt": ""}
{"id": "b31beca1-a1d8-4104-b08b-75f2a2a637ec", "fitness": 0.13808308573132017, "name": "HASMS", "description": "1) HASMS is a hybrid adaptive-subspace optimizer that mixes randomized low-dimensional search (basis built from recent successful directions and PCA of the archive) with occasional low-rank quadratic subspace models and budget-aware 1D line searches for local exploitation.  \n2) Sampling uses mirrored Gaussian coefficients, per-coordinate RMS scaling (coord_var, coord_alpha) and mirrored/uniform step draws (init_step_scale -> step) with multiplicative step adaptation on success/failure (succ_grow, succ_shrink, min_step, domain_mean) to control search radius.  \n3) Global exploration mechanisms include DE-like archive perturbations, Cauchy-style heavy-tailed jumps from memory (mem_jump_prob) and stagnation-triggered Cauchy nudges, plus archive pruning to bound memory and keep useful samples.  \n4) Robustness is enforced by a small direction memory (memory_size), periodic subspace model proposals (model_every) with constrained quadratic minimizers, stagnation counting and controlled restarts (stagnation_limit, max_restarts) and strict budget-aware evaluation via safe_eval.", "code": "import numpy as np\n\nclass HASMS:\n    \"\"\"\n    Hybrid Adaptive Subspace & Model Search (HASMS)\n    One-line: adaptive random-subspace search with PCA-augmented memory, mirrored sampling,\n    per-coordinate RMS scaling, budget-aware 1D line searches and occasional low-rank\n    subspace model proposals + Cauchy/DE nudges for robust global-local optimization.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=12, model_every=20, mem_jump_prob=0.12,\n                 init_step_scale=0.4):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.memory_size = int(memory_size)\n        self.model_every = int(model_every)\n        self.mem_jump_prob = float(mem_jump_prob)\n        self.init_step_scale = float(init_step_scale)\n\n    def __call__(self, func):\n        n = self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # budget & archive\n        evals = 0\n        X_archive = []\n        F_archive = []\n\n        # safe eval\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            x_c = np.clip(x, lb, ub)\n            try:\n                f = float(func(x_c))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x_c.copy())\n            F_archive.append(f)\n            return f, x_c\n\n        # initial point (random)\n        x_cur = self.rng.uniform(lb, ub)\n        out = safe_eval(x_cur)\n        if out[0] is None:\n            return float(np.inf), np.zeros(n)\n        f_cur, x_cur = out\n        f_best = float(f_cur)\n        x_best = x_cur.copy()\n\n        # parameters\n        domain_mean = np.mean(ub - lb)\n        step = max(1e-12, self.init_step_scale * domain_mean)\n        min_step = 1e-9 * max(1.0, domain_mean)\n        succ_grow = 1.14\n        succ_shrink = 0.86\n\n        # memory and statistics\n        dir_memory = []  # list of unit directions (most recent first)\n        coord_var = np.ones(n) * 1e-3\n        coord_alpha = 0.16\n        stagnation = 0\n        stagnation_limit = max(12, int(8 + np.log(1 + n)))\n        restarts = 0\n        max_restarts = 6\n\n        iter_count = 0\n\n        # short golden-section line search (budget-aware)\n        def short_line_search(x0, f0, d, init_step, max_evals=10):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                return None, None\n            d = d / dn\n            remaining = self.budget - evals\n            remaining = min(remaining, max_evals)\n            if remaining <= 0:\n                return None, None\n\n            # try +init_step and -init_step to find improving direction\n            a = 0.0\n            fa = f0\n            b = init_step\n            xb = np.clip(x0 + b * d, lb, ub)\n            out = safe_eval(xb)\n            if out[0] is None:\n                return None, None\n            fb, xb = out\n            remaining -= 1\n            if fb >= fa:\n                b = -init_step\n                xb = np.clip(x0 + b * d, lb, ub)\n                out = safe_eval(xb)\n                if out[0] is None:\n                    return None, None\n                fb, xb = out\n                remaining -= 1\n                if fb >= fa:\n                    return None, None\n            # golden on [0,b]\n            gr = (np.sqrt(5) - 1) / 2\n            left, right = a, b\n            c = right - gr * (right - left)\n            dd = left + gr * (right - left)\n            xc = np.clip(x0 + c * d, lb, ub)\n            out = safe_eval(xc)\n            if out[0] is None:\n                return None, None\n            fc, xc = out\n            remaining -= 1\n            xd = np.clip(x0 + dd * d, lb, ub)\n            out = safe_eval(xd)\n            if out[0] is None:\n                return None, None\n            fd, xd = out\n            remaining -= 1\n            best_f = fa; best_x = x0.copy()\n            for val, pt in ((fc, xc), (fd, xd)):\n                if val < best_f:\n                    best_f = val; best_x = pt.copy()\n            it = 0\n            while remaining > 0 and abs(right - left) > 1e-12:\n                it += 1\n                if fc < fd:\n                    right = dd\n                    dd = c\n                    fd = fc\n                    c = right - gr * (right - left)\n                    xc = np.clip(x0 + c * d, lb, ub)\n                    out = safe_eval(xc)\n                    if out[0] is None:\n                        break\n                    fc, xc = out; remaining -= 1\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    left = c\n                    c = dd\n                    fc = fd\n                    dd = left + gr * (right - left)\n                    xd = np.clip(x0 + dd * d, lb, ub)\n                    out = safe_eval(xd)\n                    if out[0] is None:\n                        break\n                    fd, xd = out; remaining -= 1\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # subspace quadratic model fit (low-rank): fit quadratic in subspace coords using recent samples\n        def try_subspace_model(basis, recent_pts= min(50, max(10, 5 * n))):\n            nonlocal evals, f_best, x_best, x_cur, f_cur\n            if len(X_archive) < (n // 2 + 5):\n                return False\n            # select recent points (or best ones) to build model\n            k = basis.shape[1]\n            # project archive into subspace coordinates relative to x_cur\n            X = np.array(X_archive)\n            F = np.array(F_archive)\n            # take up to recent_pts points (prefer best ones)\n            idx = np.argsort(F)[:min(len(F), recent_pts)]\n            Xm = X[idx] - x_cur  # relative\n            Z = Xm @ basis  # coordinates in subspace (m x k)\n            if Z.shape[0] < (k + 3):\n                return False\n            y = F[idx]\n            # design matrix for diagonal-in-subspace quadratic: z_i^2 (per dim), z_i, intercept\n            A = np.hstack([Z ** 2, Z, np.ones((Z.shape[0], 1))])\n            reg = 1e-6\n            try:\n                sol, *_ = np.linalg.lstsq(A.T @ A + reg * np.eye(A.shape[1]), A.T @ y, rcond=None)\n                sol = sol.flatten()\n                a = sol[:k]\n                bvec = sol[k:2 * k]\n                # avoid tiny a\n                a_safe = a.copy()\n                small = np.abs(a_safe) < 1e-8\n                a_safe[small] = 1e-8 * np.sign(a_safe[small] + 1e-12) + 1e-8\n                z_star = -0.5 * bvec / (a_safe + 1e-20)\n                # limit z_star magnitude to reasonable fraction of step*k\n                limit = 2.0 * step\n                z_star = np.clip(z_star, -limit, limit)\n                x_prop = np.clip(x_cur + basis @ z_star, lb, ub)\n                out = safe_eval(x_prop)\n                if out[0] is None:\n                    return False\n                f_prop, x_prop = out\n                if f_prop < f_best:\n                    f_best = f_prop; x_best = x_prop.copy()\n                if f_prop < f_cur - 1e-12:\n                    x_cur = x_prop.copy(); f_cur = f_prop\n                    return True\n            except Exception:\n                pass\n            return False\n\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n            # adaptive subspace dimension: base sqrt(n) but shrink if stagnating, grow if frequent successes\n            base_k = max(1, int(np.ceil(np.sqrt(n))))\n            if stagnation > stagnation_limit:\n                k = max(1, base_k // 2)\n            else:\n                k = min(n, base_k + (1 if len(dir_memory) > base_k else 0))\n            probes = max(4, 2 * k)\n\n            # build basis: use PCA of recent successful directions plus memory\n            # collect directions for PCA\n            mem_mat = np.array(dir_memory) if dir_memory else np.empty((0, n))\n            # also use recent archive differences to get covariance info\n            if len(X_archive) >= 4:\n                recent = np.array(X_archive[-min(len(X_archive), 100):])\n                diffs = recent - np.mean(recent, axis=0, keepdims=True)\n                # compute small covariance via SVD on diffs' transpose if possible\n                try:\n                    U, S, Vt = np.linalg.svd(diffs, full_matrices=False)\n                    pca_dirs = Vt[:min(k, Vt.shape[0])]\n                    pca_dirs = pca_dirs.copy()\n                except Exception:\n                    pca_dirs = np.empty((0, n))\n            else:\n                pca_dirs = np.empty((0, n))\n\n            # assemble initial columns preferring memory and PCA\n            cols = []\n            # use up to half from memory\n            use_mem = min(len(dir_memory), k // 2)\n            for i in range(use_mem):\n                cols.append(dir_memory[i].copy())\n            # append PCA directions up to k\n            for i in range(min(pca_dirs.shape[0], k - len(cols))):\n                cols.append(pca_dirs[i].copy())\n            # fill remaining with random normals\n            needed = k - len(cols)\n            if needed > 0:\n                R = self.rng.standard_normal((n, needed))\n                if cols:\n                    R = np.column_stack((np.column_stack(cols), R))\n                    Q, _ = np.linalg.qr(R)\n                    basis = Q[:, :k]\n                else:\n                    Q, _ = np.linalg.qr(R)\n                    basis = Q[:, :k]\n            else:\n                Q, _ = np.linalg.qr(np.column_stack(cols))\n                basis = Q[:, :k]\n\n            # mirrored sampling coeffs\n            half = (probes + 1) // 2\n            coeffs_list = [self.rng.standard_normal(k) for _ in range(half)]\n            coeffs_list = coeffs_list + [(-c) for c in coeffs_list]\n            coeffs_list = coeffs_list[:probes]\n\n            improved_round = False\n            successes = 0\n\n            for coeffs in coeffs_list:\n                if evals >= self.budget:\n                    break\n                d = basis @ coeffs\n                dn = np.linalg.norm(d)\n                if dn == 0:\n                    continue\n                # per-coordinate RMS scaling\n                coord_scale = np.sqrt(coord_var + 1e-12)\n                d = d * coord_scale\n                d = d / (np.linalg.norm(d) + 1e-20)\n                alpha = self.rng.uniform(-step, step)\n                x_try = x_cur + alpha * d\n\n                # occasional DE-like archive perturb\n                if len(X_archive) >= 3 and self.rng.random() < 0.10:\n                    i1, i2 = self.rng.choice(len(X_archive), size=2, replace=False)\n                    de = 0.5 * (X_archive[i1] - X_archive[i2])\n                    x_try = x_try + 0.5 * de\n\n                # occasional Cauchy jump using memory\n                if dir_memory and (self.rng.random() < self.mem_jump_prob):\n                    u = dir_memory[self.rng.integers(len(dir_memory))]\n                    jump = np.tan(np.pi * (self.rng.random() - 0.5))\n                    x_try = x_try + (0.7 * jump * step) * u\n\n                x_try = np.clip(x_try, lb, ub)\n                out = safe_eval(x_try)\n                if out[0] is None:\n                    break\n                f_try, x_try = out\n\n                if f_try < f_cur - 1e-12:\n                    prev_x = x_cur.copy()\n                    x_cur = x_try.copy(); f_cur = f_try\n                    successes += 1\n                    improved_round = True\n                    stagnation = 0\n                    # update best\n                    if f_try < f_best:\n                        f_best = f_try; x_best = x_try.copy()\n                    # update memory with unit direction\n                    d_succ = x_cur - prev_x\n                    dn2 = np.linalg.norm(d_succ)\n                    if dn2 > 0:\n                        u = d_succ / dn2\n                        dir_memory.insert(0, u.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n                        # update coord_var from normalized step components\n                        y = (d_succ / (abs(alpha) + 1e-20))\n                        coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (y ** 2 + 1e-12)\n\n                    # short line search along successful direction\n                    if self.budget - evals >= 2:\n                        ls_max = min(10, self.budget - evals)\n                        ls_out = short_line_search(x_cur, f_cur, u if dn2 > 0 else d, init_step=abs(alpha) if abs(alpha) > 1e-12 else step, max_evals=ls_max)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                x_cur = x_ls.copy(); f_cur = f_ls\n                                if f_ls < f_best:\n                                    f_best = f_ls; x_best = x_ls.copy()\n\n                    # gentle step growth\n                    step = min(step * succ_grow, 5.0 * domain_mean)\n                else:\n                    # occasional focused local line-search from current point\n                    if (self.rng.random() < 0.04) and (self.budget - evals) >= 3:\n                        ls_max = min(6, self.budget - evals)\n                        ls_out = short_line_search(x_cur, f_cur, d, init_step=step, max_evals=ls_max)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                prev_x = x_cur.copy()\n                                x_cur = x_ls.copy(); f_cur = f_ls\n                                if f_ls < f_best:\n                                    f_best = f_ls; x_best = x_ls.copy()\n                                d_succ = x_cur - prev_x\n                                dn2 = np.linalg.norm(d_succ)\n                                if dn2 > 0:\n                                    u = d_succ / dn2\n                                    dir_memory.insert(0, u.copy())\n                                    if len(dir_memory) > self.memory_size:\n                                        dir_memory.pop()\n                                    y = (d_succ / (np.linalg.norm(d_succ) + 1e-20))\n                                    coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (y ** 2 + 1e-12)\n                                successes += 1\n                                improved_round = True\n\n            # end probes\n            if improved_round:\n                step = min(step * (1.0 + 0.02 * successes), 5.0 * domain_mean)\n                stagnation = 0\n            else:\n                stagnation += 1\n                step = max(step * succ_shrink, min_step)\n\n            # occasional subspace model proposal\n            if (iter_count % self.model_every == 0) and (self.budget - evals > 0):\n                # try model in the last basis (build on basis above)\n                try_subspace_model(basis)\n\n            # small random Cauchy nudges around best if stagnating\n            if (stagnation > stagnation_limit // 2) and (self.rng.random() < 0.25) and (self.budget - evals > 0):\n                jump = np.tan(np.pi * (self.rng.random(n) - 0.5))\n                x_nudge = np.clip(x_best + 0.8 * step * jump, lb, ub)\n                out = safe_eval(x_nudge)\n                if out[0] is None:\n                    break\n                f_nudge, x_nudge = out\n                if f_nudge < f_best:\n                    f_best = f_nudge; x_best = x_nudge.copy()\n                    x_cur = x_best.copy(); f_cur = f_best\n                    stagnation = 0\n\n            # prune archive occasionally\n            max_archive = max(1000, 50 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep = idx_sorted[:min(200, len(idx_sorted))]\n                rest = idx_sorted[200:]\n                if len(rest) > 0:\n                    step_ = max(1, len(rest) // (max_archive - 200))\n                    keep_rest = rest[::step_]\n                    keep_idx = np.concatenate([keep, keep_rest])\n                else:\n                    keep_idx = keep\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # restart when strongly stagnating\n            if stagnation >= 3 * stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final polish with tiny step\n                    step = max(step * 0.25, min_step)\n                else:\n                    # diversify around best\n                    perturb = self.rng.normal(scale=0.6 * domain_mean, size=n)\n                    x_cur = np.clip(x_best + perturb, lb, ub)\n                    out = safe_eval(x_cur)\n                    if out[0] is None:\n                        break\n                    f_cur, x_cur = out\n                    step = min(1.0 * domain_mean, step * 1.8)\n                    dir_memory = []\n                    coord_var = np.ones(n) * 1e-3\n                    stagnation = 0\n\n            if step < min_step:\n                step = min_step\n\n            # early exit if extremely small\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HASMS scored 0.138 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "83618236-2a28-4ed9-8668-80c937a24407", "operator": null, "metadata": {"aucs": [0.059393780005523755, 0.14569519255752383, 0.1811798259340337, 0.21602570985485636, 0.0863722624266583, 0.10173453026381718, 0.1691137502551301, 0.16907937920967486, 0.1309746699604376, 0.12126175684554619]}, "task_prompt": ""}
{"id": "e3e316ce-f120-4746-bcb5-55577e399c25", "fitness": 0.3444932802751578, "name": "EnsembleBanditSubspaceSurrogate", "description": "The algorithm runs an ensemble of centers (ensemble_size ~ dim/2) where each center explores a low-rank orthonormal subspace D of dimension p ≈ ceil(sqrt(dim)); each column of D is treated as a bandit “arm” with per-arm counts/rewards and adaptive step sizes (initial steps ≈ 0.35·domain_mean) and a per-center trust vector to scale proposals. It uses a cheap local surrogate: a weighted separable quadratic fitted on nearest neighbors (requires ≳2n+1 or model_neighbor_multiplier·n samples) to propose a multi-dimensional step (-b/h clipped by trust), and follows up successful directional probes with 1‑D parabolic interpolation using three sampled alphas for fine refinement. Diversification mechanisms include an archive with pruning (max_archive = max(2000,50·n)), recent_moves memory to bias new directions and warm restarts, periodic rotation/replacement of poorly performing arms, occasional heavy‑tailed Cauchy global jumps (prob 0.10, scale_frac 0.45) and occasional Gaussian polishing. The loop is budget‑aware (safe_eval and per‑iteration work_allow limited by max_eval_per_iter), uses UCB (ucb_beta=1) to balance exploration/exploitation, re‑orthonormalizes directions periodically, and adapts trust/step by multiplicative expansion (success_expand=1.5) or shrinkage (failure_shrink=0.7).", "code": "import numpy as np\n\nclass EnsembleBanditSubspaceSurrogate:\n    \"\"\"\n    EnsembleBanditSubspaceSurrogate (EBSS)\n\n    One-line: Maintain an ensemble of centers; each center operates in a small orthonormal\n    subspace whose directions are treated as UCB bandit arms with adaptive step-sizes;\n    combine cheap separable-quadratic surrogate proposals, bandit-guided directional probes,\n    parabolic 1-D refinement, recent-move driven rotation of poor arms, and occasional\n    global/Cauchy jumps and soft restarts.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 ensemble_size=None, subspace_size=None, max_eval_per_iter=60):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n        self.ensemble_size = ensemble_size if ensemble_size is not None else max(2, min(8, self.dim // 2 + 1))\n        # subspace dimension, default similar scaling to sqrt(n) but at least 2\n        self.p = subspace_size if subspace_size is not None else max(2, int(np.ceil(np.sqrt(self.dim))))\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        # bandit UCB parameter\n        self.ucb_beta = 1.0\n        # recent moves memory for rotation / warm restarts\n        self.recent_moves_size = 12\n        # surrogate neighborhood multiplier\n        self.model_neighbor_multiplier = 8\n        # trust adaptation\n        self.success_expand = 1.5\n        self.failure_shrink = 0.7\n        # heavy-tailed jump prob\n        self.cauchy_prob = 0.10\n        self.cauchy_scale_frac = 0.45\n\n    def __call__(self, func):\n        n = self.dim\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        domain_mean = np.mean(ub - lb)\n        min_step_global = 1e-8 * max(1.0, domain_mean)\n\n        evals = 0\n        budget = int(self.budget)\n\n        # safe eval wrapper\n        f_best = np.inf\n        x_best = None\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            if evals >= budget:\n                return None, None\n            x = clip(np.asarray(x, dtype=float))\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x\n\n        # simple archive for surrogate fitting and diversification\n        X_archive = []\n        F_archive = []\n\n        # initialization: small space filling sample\n        init_budget = max(4, int(min(0.12 * budget, 2 * n + 10)))\n        for _ in range(init_budget):\n            if evals >= budget:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            out = safe_eval(x0)\n            if out[0] is None:\n                break\n            X_archive.append(out[1].copy()); F_archive.append(float(out[0]))\n\n        # initialize ensemble centers from archive (best distinct points) or random\n        def get_top_centers(k):\n            if len(X_archive) == 0:\n                return [self.rng.uniform(lb, ub) for _ in range(k)]\n            idx = np.argsort(F_archive)\n            centers = []\n            for i in idx:\n                x = np.array(X_archive[i])\n                if all(np.linalg.norm(x - c) > 1e-8 for c in centers):\n                    centers.append(x.copy())\n                if len(centers) >= k:\n                    break\n            while len(centers) < k:\n                centers.append(self.rng.uniform(lb, ub))\n            return centers\n\n        centers = get_top_centers(self.ensemble_size)\n\n        # per-center structures\n        centers_data = []\n        for c in centers:\n            # low-rank orthonormal directions (n x p)\n            p_local = min(self.p, n)\n            R = self.rng.randn(n, p_local)\n            Q, _ = np.linalg.qr(R)\n            D = Q[:, :p_local].copy()\n            counts = np.zeros(p_local, dtype=int)\n            rewards = np.zeros(p_local, dtype=float)\n            steps = np.full(p_local, 0.35 * domain_mean)\n            trust = np.maximum(0.5 * (ub - lb), 1e-9)\n            centers_data.append({\n                'center': c.copy(),\n                'D': D,\n                'counts': counts,\n                'rewards': rewards,\n                'steps': steps,\n                'trust': trust,\n                'stagn': 0\n            })\n\n        recent_moves = []\n\n        # helper: UCB selector\n        def select_ucb(counts, rewards):\n            total = counts.sum()\n            if total == 0:\n                untried = np.where(counts == 0)[0]\n                if len(untried) > 0:\n                    return int(self.rng.choice(untried))\n                return int(self.rng.randint(0, len(counts)))\n            avg = np.zeros_like(rewards)\n            nz = counts > 0\n            avg[nz] = rewards[nz] / counts[nz]\n            bonus = np.sqrt(np.log(1 + total) / (1 + counts))\n            scores = avg + self.ucb_beta * bonus\n            return int(np.argmax(scores))\n\n        # parabolic interpolation helper (3 points)\n        def parabolic_vertex(alpha_vals, f_vals, x0, d):\n            if len(alpha_vals) != 3 or len(f_vals) != 3:\n                return None, None, None\n            a1, a2, a3 = alpha_vals\n            f1, f2, f3 = f_vals\n            denom = (a1 - a2) * (a1 - a3) * (a2 - a3)\n            if abs(denom) < 1e-16:\n                return None, None, None\n            A = ((f1 * (a2 - a3) + f2 * (a3 - a1) + f3 * (a1 - a2))) / denom\n            B = ((f1 * (a3 ** 2 - a2 ** 2) + f2 * (a1 ** 2 - a3 ** 2) + f3 * (a2 ** 2 - a1 ** 2))) / denom\n            if abs(A) < 1e-16:\n                return None, None, None\n            alpha_star = -B / (2 * A)\n            low = min(alpha_vals) - 2.0 * max(1.0, abs(min(alpha_vals)))\n            high = max(alpha_vals) + 2.0 * max(1.0, abs(max(alpha_vals)))\n            alpha_star = float(np.clip(alpha_star, low, high))\n            x_star = clip(x0 + alpha_star * d)\n            out = safe_eval(x_star)\n            if out[0] is None:\n                return None, None, None\n            return float(out[0]), out[1], float(alpha_star)\n\n        # helper: fit separable quadratic around point using nearest neighbors\n        def fit_separable_quad(center, Xs, Fs):\n            if Xs.shape[0] < (2 * n + 1):\n                return None\n            dx = Xs - center\n            m = dx.shape[0]\n            M = np.ones((m, 1 + 2 * n))\n            M[:, 1:1 + n] = dx\n            M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n            y = Fs\n            dists = np.linalg.norm(dx, axis=1)\n            w = 1.0 / (dists + 1e-12)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            b = W * y\n            ridge = 1e-6 * np.eye(M.shape[1])\n            try:\n                params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ b, rcond=None)\n                params = params.flatten()\n                a = params[0]\n                b_lin = params[1:1 + n]\n                h_diag = params[1 + n:1 + 2 * n]\n                h_diag[h_diag < 1e-8] = 1e-8\n                return a, b_lin, h_diag\n            except Exception:\n                return None\n\n        # main loop: iterate until budget exhausted\n        iter_count = 0\n        rotate_every = max(20, self.p * 5)\n        while evals < budget:\n            iter_count += 1\n            work_allow = min(self.max_eval_per_iter, budget - evals)\n            improved_any = False\n\n            # refresh centers ordering\n            order = list(range(len(centers_data)))\n            self.rng.shuffle(order)\n\n            # possibly refresh centers from archive periodically\n            if (iter_count % 6) == 0 and len(X_archive) >= 1:\n                topk = min(len(centers_data), max(1, len(X_archive) // 10))\n                idx_sorted = np.argsort(F_archive) if len(F_archive) > 0 else np.arange(len(X_archive))\n                new_centers = []\n                for idx in idx_sorted[:topk]:\n                    new_centers.append(X_archive[idx].copy())\n                # merge preserving existing but preferring new good ones\n                for i in range(min(len(new_centers), len(centers_data))):\n                    centers_data[i]['center'] = new_centers[i].copy()\n                    centers_data[i]['stagn'] = 0\n\n            for cidx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                data = centers_data[cidx]\n                center = data['center']\n                D = data['D']\n                counts = data['counts']\n                rewards = data['rewards']\n                steps = data['steps']\n                trust = data['trust']\n\n                # Try surrogate proposal first if enough archive\n                improved_local = False\n                if len(X_archive) >= max(2 * n + 1, self.model_neighbor_multiplier * n) and work_allow > 0:\n                    X_arr = np.asarray(X_archive)\n                    F_arr = np.asarray(F_archive)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sort = np.argsort(dists)[:min(len(X_arr), max(2 * n + 1, self.model_neighbor_multiplier * n))]\n                    X_nei = X_arr[idx_sort]; F_nei = F_arr[idx_sort]\n                    quad = fit_separable_quad(center, X_nei, F_nei)\n                    if quad is not None and work_allow > 0:\n                        a_q, b_q, h_q = quad\n                        delta_q = -b_q / (h_q + 1e-20)\n                        delta_q = np.clip(delta_q, -trust, trust)\n                        x_q = clip(center + delta_q)\n                        out = safe_eval(x_q)\n                        work_allow -= 1\n                        if out[0] is not None:\n                            X_archive.append(out[1].copy()); F_archive.append(float(out[0]))\n                            if out[0] < f_best - 1e-12:\n                                improved_local = True\n                                data['center'] = out[1].copy()\n                                data['trust'] = np.minimum(trust * self.success_expand, 2.0 * (ub - lb))\n                                data['stagn'] = 0\n                                improved_any = True\n                # If surrogate didn't improve, do bandit-guided directional probing inside subspace\n                probes = max(2, min(4 + D.shape[1], work_allow))\n                for _ in range(probes):\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    arm = select_ucb(counts, rewards)\n                    d = D[:, arm].copy()\n                    dn = np.linalg.norm(d)\n                    if dn == 0:\n                        v = self.rng.randn(n)\n                        v /= (np.linalg.norm(v) + 1e-20)\n                        D[:, arm] = v\n                        d = D[:, arm].copy()\n                    # sample alpha: gaussian around current step for this arm\n                    alpha = self.rng.normal(loc=0.0, scale=steps[arm])\n                    if abs(alpha) < 1e-16:\n                        alpha = steps[arm] * (0.2 + 0.6 * self.rng.rand())\n                    x_try = clip(center + alpha * d)\n                    out = safe_eval(x_try)\n                    work_allow -= 1\n                    if out[0] is None:\n                        break\n                    X_archive.append(out[1].copy()); F_archive.append(float(out[0]))\n                    counts[arm] += 1\n                    reward = max(0.0, (f_best if f_best < np.inf else out[0]) - out[0])\n                    # But use improvement over center fitness if available in archive\n                    # find center fitness from archive or compute\n                    # approximate f_center from nearest in archive or evaluate if needed\n                    # For simplicity compare to overall current best known center value:\n                    reward = max(0.0, (f_best - out[0]) )\n                    rewards[arm] += reward\n\n                    if out[0] < f_best - 1e-12:\n                        # global improvement\n                        improved_any = True\n                        data['center'] = out[1].copy()\n                        data['stagn'] = 0\n                        # store recent move\n                        # compute move vector from prior center to new center\n                        move_vec = out[1] - center\n                        mvn = np.linalg.norm(move_vec)\n                        if mvn > 0:\n                            unit_move = move_vec / mvn\n                            recent_moves.insert(0, unit_move.copy())\n                            if len(recent_moves) > self.recent_moves_size:\n                                recent_moves.pop()\n                            # soft update direction toward move\n                            gamma = 0.30\n                            D[:, arm] = (1 - gamma) * D[:, arm] + gamma * unit_move\n                            D[:, arm] /= (np.linalg.norm(D[:, arm]) + 1e-20)\n                        # increase step\n                        steps[arm] = min(steps[arm] * 1.18, 5.0 * domain_mean)\n                        # attempt parabolic refinement if budget allows\n                        if work_allow >= 2 and evals < budget:\n                            # pick three alphas near current alpha\n                            a_center = alpha\n                            avals = [a_center * 0.5, 0.0, a_center * 1.5]\n                            fvals = []\n                            alphas = []\n                            for a in avals:\n                                x_a = clip(center + a * d)\n                                out2 = safe_eval(x_a)\n                                work_allow -= 1\n                                if out2[0] is None:\n                                    break\n                                X_archive.append(out2[1].copy()); F_archive.append(float(out2[0]))\n                                fvals.append(float(out2[0])); alphas.append(a)\n                            if len(fvals) == 3:\n                                f_par, x_par, a_par = parabolic_vertex(alphas, fvals, center, d)\n                                if f_par is not None and f_par < f_best - 1e-12:\n                                    # accept parabolic optimum\n                                    data['center'] = x_par.copy()\n                                    improved_any = True\n                                    # update direction modestly toward parabolic move\n                                    mv = x_par - center\n                                    mn = np.linalg.norm(mv)\n                                    if mn > 0:\n                                        unit_mv = mv / mn\n                                        D[:, arm] = (1 - 0.25) * D[:, arm] + 0.25 * unit_mv\n                                        D[:, arm] /= (np.linalg.norm(D[:, arm]) + 1e-20)\n                                    steps[arm] = min(steps[arm] * 1.12, 5.0 * domain_mean)\n                    else:\n                        # no improvement: shrink\n                        steps[arm] = max(steps[arm] * 0.80, min_step_global)\n                        data['stagn'] += 1\n\n                    # small opposition sampling occasionally\n                    if self.rng.rand() < 0.12 and work_allow > 0 and evals < budget:\n                        alpha_op = -alpha * (0.4 + 0.6 * self.rng.rand())\n                        x_op = clip(center + alpha_op * d)\n                        outop = safe_eval(x_op)\n                        work_allow -= 1\n                        if outop[0] is None:\n                            break\n                        X_archive.append(outop[1].copy()); F_archive.append(float(outop[0]))\n                        counts[arm] += 1\n                        rewards[arm] += max(0.0, (f_best - outop[0]))\n                        if outop[0] < f_best - 1e-12:\n                            data['center'] = outop[1].copy()\n                            improved_any = True\n                            data['stagn'] = 0\n\n                # periodic rotate/replace poor arms for this center\n                if iter_count % rotate_every == 0:\n                    score = np.full_like(rewards, -1e9)\n                    nz = counts > 0\n                    score[nz] = rewards[nz] / counts[nz]\n                    worst = np.argsort(score)[:max(1, len(score) // 4)]\n                    for wi in worst:\n                        rnd = self.rng.randn(n)\n                        if len(recent_moves) > 0 and self.rng.rand() < 0.6:\n                            candidate = 0.5 * rnd + 0.5 * recent_moves[self.rng.randint(len(recent_moves))]\n                        else:\n                            candidate = rnd\n                        candidate /= (np.linalg.norm(candidate) + 1e-20)\n                        D[:, wi] = candidate\n                        counts[wi] = 0\n                        rewards[wi] = 0.0\n                        steps[wi] = 0.35 * domain_mean\n\n                # occasional heavy-tailed jump around this center\n                if work_allow > 0 and self.rng.rand() < self.cauchy_prob and evals < budget:\n                    scale = self.cauchy_scale_frac * np.maximum(ub - lb, 1e-9)\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -10, 10)\n                    x_jump = clip(center + jump * scale)\n                    outj = safe_eval(x_jump)\n                    work_allow -= 1\n                    if outj[0] is None:\n                        break\n                    X_archive.append(outj[1].copy()); F_archive.append(float(outj[0]))\n                    if outj[0] < f_best - 1e-12:\n                        data['center'] = outj[1].copy()\n                        data['trust'] = np.minimum(trust * 2.0, 2.0 * (ub - lb))\n                        data['stagn'] = 0\n                        improved_any = True\n                    else:\n                        data['trust'] = np.maximum(trust * self.failure_shrink, 1e-9)\n\n            # after centers loop: if no improvement globally, shrink trusts mildly\n            if not improved_any:\n                for d in centers_data:\n                    d['trust'] = np.maximum(d['trust'] * self.failure_shrink, 1e-9)\n            else:\n                # decrease stagn counters slightly\n                for d in centers_data:\n                    if d['stagn'] > 0:\n                        d['stagn'] = max(0, d['stagn'] - 1)\n\n            # replace chronically stagnated centers by diverse archive points\n            for i, d in enumerate(centers_data):\n                if d['stagn'] >= max(8, self.recent_moves_size):\n                    if len(X_archive) > 0:\n                        X_arr = np.asarray(X_archive)\n                        # pick a reasonably good but diverse point\n                        quality_idx = np.argsort(F_archive)\n                        pick_idx = quality_idx[self.rng.randint(min(20, len(quality_idx)))]\n                        d['center'] = X_arr[pick_idx].copy()\n                        d['stagn'] = 0\n                        # reinit some directions biased by recent moves\n                        p_local = d['D'].shape[1]\n                        for j in range(min(p_local, len(recent_moves))):\n                            d['D'][:, j] = recent_moves[j].copy()\n                        # re-orthonormalize\n                        Q, _ = np.linalg.qr(d['D'])\n                        d['D'] = Q[:, :p_local].copy()\n                        d['counts'] = np.zeros_like(d['counts'])\n                        d['rewards'] = np.zeros_like(d['rewards'])\n                        d['steps'] = np.full_like(d['steps'], 0.35 * domain_mean)\n\n            # archive pruning to keep memory manageable\n            max_archive = max(2000, 50 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # occasional small full-space Gaussian polishing around best so far\n            if evals < budget and self.rng.rand() < 0.05:\n                sigma = 0.08 * domain_mean\n                xg = clip((x_best if x_best is not None else self.rng.uniform(lb, ub)) + self.rng.randn(n) * sigma)\n                outg = safe_eval(xg)\n                if outg[0] is not None:\n                    X_archive.append(outg[1].copy()); F_archive.append(float(outg[0]))\n\n            # re-orthonormalize directions occasionally for numerical stability\n            if iter_count % 7 == 0:\n                for d in centers_data:\n                    Q, _ = np.linalg.qr(d['D'])\n                    d['D'] = Q[:, :d['D'].shape[1]].copy()\n\n            # quick exit if extremely good\n            if f_best <= 1e-12:\n                break\n\n        if x_best is None:\n            # fallback: pick any center's current center\n            x_best = centers_data[0]['center'].copy()\n            f_best = float(func(clip(x_best))) if evals < budget else float(f_best)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm EnsembleBanditSubspaceSurrogate scored 0.344 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "83618236-2a28-4ed9-8668-80c937a24407", "operator": null, "metadata": {"aucs": [0.1563362449972835, 0.1642503649691358, 0.4348008300033589, 0.7266189664721425, 0.43262183416812816, 0.5032001585753567, 0.2076626892645833, 0.37195113225204823, 0.2847750815211124, 0.16271550052842843]}, "task_prompt": ""}
{"id": "b3e06860-5cc0-4032-ad9e-f13219475391", "fitness": "-inf", "name": "HYPERION", "description": "HYPERION maintains an adaptive low-rank search subspace (orthonormal basis B with per-vector strengths and an incremental PCA-like replacement) and mixes proposals from that rotated Gaussian subspace with heavy‑tailed per‑coordinate Laplace perturbations, controlled by an adaptive mixing logit that initially favors the subspace (mix ~0.6) and is nudged by recent success and novelty. It tracks per‑coordinate variability with a slow RMS-like estimator V (beta_v=0.92) and transforms it with a negative exponent (V_exponent=-0.48) to produce diagonal scales, while a global sigma (initialized ≈0.3·domain) and a temperature T are multiplicatively adapted (cool on success, warm on failure) to control exploration amplitude. The algorithm uses a small population sized ~pop_factor·log(dim), mirrored sampling for variance reduction, archive-driven DE‑style recombination biased toward novel/distant archive members (p_recombine=0.3, novelty_weight=0.25), and momentum/inertia (momentum_decay=0.85) to encourage persistent directions. Robustness features include occasional Lévy‑style heavy‑tailed restart trials triggered by stagnation over a window (stagnation_window = max(40,4·dim)), basis re‑orthonormalization, and clipping to bounds so it is suited for Many‑Affine BBOB noiseless continuous tasks.", "code": "import numpy as np\n\nclass HYPERION:\n    \"\"\"\n    HYPERION optimizer\n\n    Main novel ideas:\n    - Maintain a small learned rotation basis B (incremental PCA-like) that captures\n      recently successful directions; proposals combine a rotated low-rank Gaussian\n      component and a per-coordinate Laplace (heavy-tailed) component.\n    - Per-coordinate variance tracker V (RMS-like) yields diagonal scales via a\n      tunable exponent. This is updated from squared steps of successful offspring.\n    - A mixing logit controls interpolation between low-rank rotated proposals and\n      diagonal proposals; the logit is adapted additively based on recent success\n      and a novelty signal (diversity vs archive).\n    - A \"temperature\" T controls exploration amplitude and is annealed on success,\n      warmed on failure; sigma multiplies T for absolute scale.\n    - Archive-driven recombination: novel \"diversity-biased\" parent selection.\n    - Occasional Lévy-like restarts guided by archive stagnation detection.\n    - Mirrored sampling supported for variance reduction.\n    - Designed for bounded search (often [-5,5]) and Many Affine BBOB-style noiseless functions.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 k_subspace=None, pop_factor=3.0, mirror=True):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population heuristics: scale with log(dim)\n        lam = max(4, int(np.round(pop_factor * np.log(max(2, self.dim)) + 2)))\n        self.lambda_ = lam\n        self.mu = max(1, lam // 2)\n\n        # learned low-rank subspace dimension\n        if k_subspace is None:\n            self.k = min(self.dim, max(1, int(np.ceil(np.log2(max(2, self.dim)) + 1))))\n        else:\n            self.k = min(self.dim, max(1, int(k_subspace)))\n\n        # global multipliers and adaptation rates\n        self.init_sigma = 0.3 * 10.0  # starting absolute scale (domain-scaled idea)\n        self.sigma = None\n\n        # per-coordinate variance tracker (RMS-like)\n        self.beta_v = 0.92\n        self.V_eps = 1e-8\n        self.V_exponent = -0.48  # transform exponent for diagonal scaling\n\n        # mixing logit (controls rotation vs diag)\n        self.mix_logit = np.log(0.6 / (1.0 - 0.6))  # prefer subspace initially\n        self.mix_lr = 0.10\n        self.mix_target = 0.28  # desired short-term success rate\n\n        # temperature controller for annealing exploration\n        self.T = 1.0\n        self.T_decay_on_success = 0.92\n        self.T_warm_on_fail = 1.08\n\n        # archive & recombination\n        self.archive = []\n        self.archive_f = []\n        self.archive_limit = 1500\n        self.p_recombine = 0.30\n\n        # mirrored sampling\n        self.mirror = bool(mirror)\n\n        # Lévy restart triggers\n        self.stagnation_window = max(40, 4 * self.dim)\n        self.levy_trials = 8\n\n        # momentum in parameter space (inertia)\n        self.momentum = np.zeros(self.dim)\n        self.momentum_decay = 0.85\n\n        # novelty bias weight for recombination (0..1)\n        self.novelty_weight = 0.25\n\n        # miscellaneous\n        self.seed = seed\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (fall back to [-5,5])\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initialize mean randomly in bounds\n        m = np.random.uniform(lb, ub)\n        if self.sigma is None:\n            # scale initial sigma to domain range\n            domain_scale = np.mean(ub - lb)\n            self.sigma = max(1e-12, self.init_sigma * (domain_scale / 10.0))\n\n        sigma = float(self.sigma)\n\n        # initialize low-rank basis B: orthonormal columns (n x k)\n        k = self.k\n        rand = np.random.randn(n, k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            B = Q[:, :k]\n        except np.linalg.LinAlgError:\n            B = rand / (np.linalg.norm(rand, axis=0, keepdims=True) + 1e-12)\n\n        # singular strength for each basis vector (importance)\n        strengths = np.ones(k) * 1e-3\n\n        # per-coordinate variance tracker V\n        V = np.ones(n) * 1e-3\n\n        # mixing helper\n        def logit_to_mix(logit):\n            s = 1.0 / (1.0 + np.exp(-logit))\n            return float(np.clip(s, 1e-5, 1.0 - 1e-5))\n\n        mix = logit_to_mix(self.mix_logit)\n\n        # archive lists\n        archive_X = []\n        archive_F = []\n\n        # recombination weights (linear decreasing)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        ranks = np.arange(1, mu + 1)\n        weights = (mu + 1.0 - ranks)\n        weights = np.maximum(weights, 1e-12)\n        weights = weights / np.sum(weights)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate current mean once if budget allows\n        if evals < budget:\n            x0 = np.clip(m, lb, ub)\n            f0 = func(x0)\n            evals += 1\n            archive_X.append(x0.copy()); archive_F.append(f0)\n            f_best = f0; x_best = x0.copy()\n        else:\n            f0 = np.inf\n\n        recent_success = []  # binary history window\n        success_window = max(6, mu)\n\n        # main loop: generate populations until budget exhausted\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # prepare radial samples: mix Laplace (heavy-tailed) and Gaussian for per-coordinate variability\n            for i in range(current_lambda):\n                # independent radial magnitudes:\n                # diag part: scaled Laplace per coordinate (heavy tails)\n                b_scale = (V + self.V_eps) ** self.V_exponent\n                lap = np.random.laplace(loc=0.0, scale=1.0, size=n)  # unit laplace\n                z_diag = lap * b_scale\n\n                # low-rank part: sample k-dim gaussian and project\n                z_sub = np.random.randn(k)\n                # scale directions by strengths so that strong directions contribute more\n                z_sub_weighted = z_sub * (1.0 + 5.0 * (strengths / (np.max(strengths) + 1e-12)))\n                proj = B.dot(z_sub_weighted)  # back to n-dim\n\n                # construct final direction y as non-linear blend\n                mix = logit_to_mix(self.mix_logit)\n                # non-linear gating: use softsign to bound influence and create asymmetry\n                gated_diag = np.tanh(0.9 * z_diag)\n                gated_sub = np.tanh(1.2 * proj)\n\n                y = (1.0 - mix) * gated_diag + mix * gated_sub\n\n                # apply momentum bias (directional inertia), scaled relative to sigma and temperature\n                # momentum encourages persistence along previously promising directions\n                y = y + 0.25 * (self.momentum / (np.linalg.norm(self.momentum) + 1e-12))\n\n                # occasional mirrored sampling (variance reduction)\n                if self.mirror and (i % 2 == 1):\n                    y = -y\n\n                # combine into candidate\n                x = m + sigma * self.T * y\n\n                # small archive-driven perturbation / DE-inspired combination\n                if (np.random.rand() < self.p_recombine) and (len(archive_X) >= 3):\n                    # pick two archive members biased by novelty: prefer distant ones\n                    idxs = np.random.choice(len(archive_X), size=3, replace=False)\n                    A = np.array(archive_X)[idxs]\n                    # compute pairwise distances to mean and use novelty bias\n                    dists = np.linalg.norm(A - m, axis=1)\n                    probs = (1.0 - self.novelty_weight) * (1.0 / (1.0 + dists)) + self.novelty_weight * (dists / (np.sum(dists) + 1e-12))\n                    probs = probs / np.sum(probs)\n                    i1, i2 = np.random.choice(3, size=2, replace=False, p=probs)\n                    donor = A[i1]\n                    diff = A[i2] - m\n                    # add a scaled differential perturbation (inspired by DE/rand-1)\n                    x = x + 0.6 * diff\n\n                # clip\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates (respecting exact budget)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n\n                # update archive\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    # purge oldest entries\n                    archive_X.pop(0); archive_F.pop(0)\n\n                # global best tracking\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # selection\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            if sel.size == 0:\n                # no evaluated children (shouldn't happen but guard)\n                continue\n\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n            f_sel = arfit[sel]\n\n            # recombine to new mean using linear weights (weighted mean)\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # compute weighted step in y-space (used to update basis & variance)\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # determine success: if best selected improved mean-evaluation or global best\n            prev_ref = np.min(archive_F) if archive_F else f_best\n            sel_best_f = np.min(f_sel) if f_sel.size > 0 else np.inf\n            success_flag = 1 if sel_best_f < prev_ref - 1e-12 else 0\n            recent_success.append(success_flag)\n            if len(recent_success) > success_window:\n                recent_success.pop(0)\n            success_rate = float(np.mean(recent_success)) if recent_success else 0.0\n\n            # update mixing logit additively: push towards more subspace if success above target\n            delta_mix = self.mix_lr * (success_rate - self.mix_target)\n            # add small novelty incentive: if selected solutions are far from archive mean, encourage subspace\n            if len(archive_X) > 0:\n                sel_center = np.mean(x_sel, axis=0)\n                arch_mean = np.mean(np.array(archive_X), axis=0)\n                novelty = np.linalg.norm(sel_center - arch_mean) / (np.linalg.norm(arch_mean) + 1e-12)\n                # positive novelty increases subspace mixing slightly\n                delta_mix += 0.03 * (novelty - 0.5)\n            self.mix_logit += float(delta_mix)\n\n            # update momentum: retain inertia but blend with new drift\n            self.momentum = self.momentum_decay * self.momentum + (1.0 - self.momentum_decay) * (sigma * self.T * y_w)\n\n            # update per-coordinate variance V using weighted squared y_sel\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            V = self.beta_v * V + (1.0 - self.beta_v) * (y2 + self.V_eps)\n\n            # update strengths of basis components based on projection of y_w onto each basis vector\n            # stronger alignment increases corresponding strength\n            for j in range(k):\n                u = B[:, j]\n                proj_coeff = abs(np.dot(y_w, u))\n                strengths[j] = 0.98 * strengths[j] + 0.02 * proj_coeff\n\n            # incremental PCA-style update: inject residual of y_w not explained by current B\n            # compute projection of y_w onto basis\n            proj = B.dot(B.T.dot(y_w))\n            residual = y_w - proj\n            res_norm = np.linalg.norm(residual)\n            if res_norm > 1e-8:\n                # candidate direction to add/replace a basis vector, normalized\n                cand = residual / (res_norm + 1e-12)\n                # pick weakest basis index to replace (smallest strength)\n                weakest = int(np.argmin(strengths))\n                # replace and renormalize basis via simple Gram-Schmidt\n                B[:, weakest] = cand\n                # re-orthonormalize occasionally (small cost)\n                if np.random.rand() < 0.25:\n                    try:\n                        Q, _ = np.linalg.qr(B)\n                        B = Q[:, :k]\n                    except np.linalg.LinAlgError:\n                        # fallback: normalize columns\n                        B = B / (np.linalg.norm(B, axis=0, keepdims=True) + 1e-12)\n\n            # adapt sigma and temperature T\n            # new formula: multiplicative factor from logistic-shaped gain on success_rate\n            gain = (2.0 / (1.0 + np.exp(-12.0 * (success_rate - 0.25)))) - 1.0  # in (-1,1)\n            sigma *= np.exp(0.6 * gain / np.sqrt(1.0 + n / 16.0))\n            # clip sigma to reasonable domain-proportional bounds\n            domain_scale = np.mean(ub - lb)\n            sigma = float(np.clip(sigma, 1e-12, 30.0 * domain_scale))\n\n            # adjust temperature T (cool down when successful, warm when failing)\n            if success_flag:\n                self.T *= self.T_decay_on_success\n                # shrink sigma slightly more on big improvements\n                if (sel_best_f < f_best - 1e-6):\n                    sigma *= 0.98\n            else:\n                # warm up to encourage escape\n                self.T *= self.T_warm_on_fail\n                # but bound T to avoid runaway\n                self.T = float(np.clip(self.T, 0.2, 5.0))\n\n            # stagnation detection using archive fitness trend: trigger Lévy-like bursts\n            if (len(archive_F) >= self.stagnation_window) and (evals % self.stagnation_window == 0):\n                last_vals = np.array(archive_F[-self.stagnation_window:])\n                # if the last window did not yield an improvement on the global best, consider stagnation\n                if np.min(last_vals) >= f_best - 1e-12:\n                    old_sigma = sigma\n                    # perform several Lévy-like trials biased along basis directions and coordinate axes\n                    for t in range(min(self.levy_trials, budget - evals)):\n                        # direction: choose either a basis vector or a coordinate axis\n                        if np.random.rand() < 0.6:\n                            # pick one of the basis vectors weighted by strengths\n                            probs = strengths / (np.sum(strengths) + 1e-12)\n                            j = np.random.choice(k, p=probs)\n                            direction = B[:, j]\n                        else:\n                            e = np.zeros(n); idx_coord = np.random.randint(0, n); e[idx_coord] = 1.0\n                            direction = e\n                        # heavy-tailed radial from Cauchy\n                        r = np.random.standard_cauchy() * 2.0\n                        y = r * direction * (np.median((V + self.V_eps) ** self.V_exponent))\n                        x_try = np.clip(m + old_sigma * self.T * y, lb, ub)\n                        f_try = func(x_try)\n                        evals += 1\n                        archive_X.append(x_try.copy()); archive_F.append(f_try)\n                        if len(archive_X) > self.archive_limit:\n                            archive_X.pop(0); archive_F.pop(0)\n                        if f_try < f_best:\n                            f_best = f_try\n                            x_best = x_try.copy()\n                            # adopt new mean and reset inertia/temperature partially\n                            m = x_try.copy()\n                            self.momentum = np.zeros(n)\n                            self.T = 1.0\n                            recent_success = []\n                            break\n                    # restore sigma (we used a temporary sigma for trials)\n                    sigma = old_sigma\n\n            # update stored sigma\n            self.sigma = sigma\n\n            # ensure mean within bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 256, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (4,1) (3,10) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "error": "In the code, line 256, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (4,1) (3,10) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "parent_ids": "7001612a-f98b-47b3-b88d-77166bdf3b68", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3664dc4b-f9a9-481d-95ff-9763a2b105e1", "fitness": "-inf", "name": "ASSMS", "description": "The algorithm mixes an adaptive low-rank subspace search with coordinate-wise adaptive scaling and a small injected momentum: proposals are formed as a convex mix (gamma) of a learned subspace component (V, updated incrementally by a power-style rule) and a diagonal, per-coordinate scaled component derived from biased 1st/2nd moment accumulators (M,S) mapped through a negative exponent (scale_exp) to produce D. Sampling and recombination include mirrored Gaussian draws for variance reduction, occasional Student‑t heavy‑tailed bursts for global escape, rare DE-style current-to-pbest/1 recombination using an archive with binomial crossover, and exponential (softmax-like) ranking weights to form the new mean. Step-size (sigma) is adapted multiplicatively from a smooth success-rate signal (with its own normalization and clipping), while gamma is updated multiplicatively toward a target success rate and a small momentum term is maintained and injected into proposals. Practical heuristics bias behavior toward small-projection search (lambda ~ 6+2√d, k ≈ ceil(√d/3)), with tuned hyperparameters (beta1=0.80, beta2=0.92, momentum_decay=0.88, p_de=0.20, p_student=0.07, archive_limit=1500) and a stagnation detector that triggers temporary large-sigma Student‑t bursts to escape local traps.", "code": "import numpy as np\n\nclass ASSMS:\n    \"\"\"\n    ASSMS: Adaptive Subspace & Scaled-Momentum Search\n\n    Key design points (novel choices vs the provided LAROS):\n    - Coordinate-wise adaptive scaling via biased 1st/2nd moment accumulators (beta1/beta2).\n    - Small subspace V learned by an incremental power-style update (different from Oja exact form).\n    - Mixing factor gamma stored and updated multiplicatively (not logit/additive).\n    - Softmax-like exponential ranking weights for recombination (different weighting).\n    - Sigma (global step) adapted multiplicatively using a smooth success-based rule,\n      but with a different normalization and gain than LAROS.\n    - Archive-based DE/current-to-pbest/1-style recombination used rarely (different formula/F).\n    - Mirrored sampling for variance reduction and Student-t bursts for escapes (nu=3).\n    - Stagnation detection triggers a batch heavy-tailed directional search with different scale.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, init_sigma=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        # population size heuristic different from LAROS\n        self.lambda_ = max(6, int(6 + 2.0 * np.sqrt(max(1, self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n        # small subspace: somewhat between log(dim) and sqrt(dim)/3\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(max(1, np.sqrt(max(1, self.dim))) / 3.0)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n        # default sigma\n        self.init_sigma = 0.2 if init_sigma is None else float(init_sigma)\n        # accumulators params (distinct)\n        self.beta1 = 0.80\n        self.beta2 = 0.92\n        self.eps = 1e-8\n        # exponent for mapping second moment to scale (different)\n        self.scale_exp = -0.33\n        # mixing gamma multiplicative adaptation\n        self.gamma = 0.4\n        self.gamma_lr = 0.18  # multiplicative update gain (different)\n        self.gamma_target = 0.28\n        # archive and DE parameters\n        self.archive_limit = 1500\n        self.p_de = 0.20\n        self.F_de = 0.6  # different\n        # mirrored sampling toggle\n        self.mirror = True\n        # Student-t heavy tail\n        self.p_student = 0.07\n        self.student_df = 3.0\n        self.student_scale = 1.4\n        # momentum\n        self.momentum_decay = 0.88\n        # sigma adaptation\n        self.sigma_gain = 0.6\n        # success window\n        self.success_window = max(6, int(self.lambda_ // 2))\n        # stagnation detection\n        self.stagnation_window = max(30, 4 * self.dim)\n        self.levy_trials = 10  # tries during a burst\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds handling (defaults to [-5,5] if func has no bounds)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initialize mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = float(max(1e-12, self.init_sigma))\n        # first and second moment accumulators\n        M = np.zeros(n)\n        S = np.ones(n) * 1e-3  # small initial second moment\n        # initial subspace V via QR on gaussian matrix\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            V = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            V = rand / (np.linalg.norm(rand, axis=0, keepdims=True) + 1e-12)\n        # momentum vector\n        mom = np.zeros(n)\n        # exponential ranking weights (softmax-like): compute once for mu\n        ranks = np.arange(1, self.mu + 1)\n        raw_w = -ranks  # decreasing with rank\n        weights = np.exp(raw_w - np.max(raw_w))\n        weights = weights / np.sum(weights)\n        # effective mu for statistics\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = fm; x_best = xm.copy()\n            fm_ref = fm\n        else:\n            fm_ref = np.inf\n\n        recent_successes = []\n\n        lam = self.lambda_\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # base gaussian draws\n            base_z = np.random.randn(current_lambda, n)\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # project to subspace and complement\n                z_sub = V @ (V.T @ z)\n                z_comp = z - z_sub\n\n                # compute per-coordinate scale from S\n                D = (S + self.eps) ** self.scale_exp\n                sub_scale = float(np.mean(D))  # scale subspace proposals by mean(D)\n\n                # compute candidate proposal: diagonal scaled comp + subspace scaled\n                y_diag = D * z_comp\n                y_sub = sub_scale * z_sub\n\n                # mixing controlled by gamma multiplicative factor (keep in [1e-4,1-1e-4])\n                gamma = float(np.clip(self.gamma, 1e-4, 1.0 - 1e-4))\n\n                # nonlinear mapping: use softsign to moderate extremes\n                def softsign(x): return x / (1.0 + np.abs(x))\n                y = (1.0 - gamma) * softsign(y_diag) + gamma * (0.85 * y_sub)\n\n                # occasional student-t burst (heavy tails)\n                if np.random.rand() < self.p_student:\n                    t_r = np.random.standard_t(self.student_df, size=1)[0] * self.student_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = t_r * (z / nz) * sub_scale\n\n                # mirrored sampling for variance reduction\n                if self.mirror and (i % 2 == 1):\n                    y = -y\n\n                x = m + sigma * y + 0.3 * mom  # small momentum injection\n\n                # DE-style current-to-pbest/1 recombination with archive\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                    # choose x_pbest from top 10% of archive or entire archive if small\n                    sorted_idx = np.argsort(archive_F)\n                    top_k = max(1, int(0.1 * len(sorted_idx)))\n                    top_idx = sorted_idx[:top_k]\n                    pbest = archive_X[np.random.choice(top_idx)]\n                    # choose two other distinct archive members\n                    a, b = np.random.choice(len(archive_X), size=2, replace=False)\n                    donor = pbest + self.F_de * (archive_X[a] - archive_X[b])\n                    # binomial crossover (50%)\n                    mask = (np.random.rand(n) < 0.5)\n                    x[mask] = donor[mask]\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates (respect budget)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # selection\n            idx = np.argsort(arfit)\n            sel = idx[:self.mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            # recombine mean using exponential weights\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # success measure: improvement of best selected vs reference fm_ref\n            sel_best_f = np.min(arfit[sel]) if sel.size > 0 else np.inf\n            success_flag = 1 if sel_best_f < fm_ref - 1e-12 else 0\n            recent_successes.append(success_flag)\n            if len(recent_successes) > self.success_window:\n                recent_successes.pop(0)\n            success_rate = float(np.mean(recent_successes)) if recent_successes else 0.0\n\n            # multiplicative gamma update (different scheme)\n            # push gamma up if success_rate > target, down otherwise, but clip modestly\n            self.gamma *= float(np.exp(self.gamma_lr * (success_rate - self.gamma_target)))\n            self.gamma = float(np.clip(self.gamma, 1e-4, 1.0 - 1e-4))\n\n            # update momentum\n            mom = self.momentum_decay * mom + (1.0 - self.momentum_decay) * (sigma * y_w)\n\n            # biased moment updates for per-coordinate scaling (Adam-like but used differently)\n            M = self.beta1 * M + (1.0 - self.beta1) * y_w\n            S = self.beta2 * S + (1.0 - self.beta2) * (y_w ** 2 + self.eps)\n\n            # compute D again (used later for subspace scale)\n            D = (S + self.eps) ** self.scale_exp\n\n            # sigma adaptation multiplicative (different normalization / target)\n            # target success for this adaptation is 0.22 (distinct)\n            adj = self.sigma_gain * (success_rate - 0.22) / max(0.08, np.sqrt(1.0 + n / 6.0))\n            sigma *= np.exp(adj)\n            # clip to domain-relative bounds\n            domain_scale = np.mean(ub - lb)\n            sigma = float(np.clip(sigma, 1e-12, 40.0 * domain_scale))\n\n            # incremental subspace update (power-style): use y_w as driving direction\n            signal = y_w.copy()\n            sig_norm = np.linalg.norm(signal) + 1e-12\n            if sig_norm > 1e-12:\n                u = signal / sig_norm\n                # project out current V and update by rank-1 incremental step\n                for j in range(V.shape[1]):\n                    vj = V[:, j]\n                    # push vj slightly toward u minus its projection (different scaling)\n                    proj = np.dot(vj, u)\n                    vj = vj + 0.18 * (u - proj * vj)\n                    V[:, j] = vj / (np.linalg.norm(vj) + 1e-12)\n                # occasional orthonormalize (rare)\n                if np.random.rand() < 0.12:\n                    try:\n                        Q, _ = np.linalg.qr(V)\n                        V = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # stagnation detection and heavy-tailed bursts\n            if (len(archive_F) >= self.stagnation_window) and (evals % self.stagnation_window == 0):\n                last_vals = archive_F[-self.stagnation_window:]\n                # if no significant improvement during window, attempt a burst\n                if len(last_vals) == self.stagnation_window and np.min(last_vals) >= f_best - 1e-12:\n                    old_sigma = sigma\n                    sigma *= 3.0  # temporary increase\n                    # try several student-t directional probes, adopt if improved\n                    for _ in range(min(self.levy_trials, budget - evals)):\n                        z = np.random.standard_t(self.student_df, size=n)\n                        z = z / (np.linalg.norm(z) + 1e-12)\n                        r = np.random.randn() * 1.5 + np.random.standard_t(self.student_df)\n                        y = r * z * float(np.mean(D))\n                        x_try = np.clip(m + sigma * y, lb, ub)\n                        f_try = func(x_try)\n                        evals += 1\n                        archive_X.append(x_try.copy()); archive_F.append(f_try)\n                        if len(archive_X) > self.archive_limit:\n                            archive_X.pop(0); archive_F.pop(0)\n                        if f_try < f_best:\n                            f_best = f_try\n                            x_best = x_try.copy()\n                            # adopt new mean and reset momentum and successes\n                            m = x_try.copy()\n                            mom = np.zeros(n)\n                            recent_successes = []\n                            fm_ref = f_try\n                    sigma = old_sigma\n\n            # update fm_ref conservatively\n            fm_ref = min(f_best, fm_ref)\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 213, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (4,1) (3,2) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "error": "In the code, line 213, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (4,1) (3,2) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "parent_ids": "7001612a-f98b-47b3-b88d-77166bdf3b68", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "618c8f55-9f17-4b9d-84fc-50f37f4ab727", "fitness": 0.49600168764418395, "name": "ARSS", "description": "ARSS mixes per-coordinate diagonal scaling (S updated as an EMA of absolute successful step components) with a learned low-rank rotated subspace B (Oja-like updates and occasional re-orthonormalization) and a logit-controlled stochastic blend between the two to shape proposals. Sampling uses mirrored draws, momentum, DE-style archive recombination (p_de, F_de) and rank-exponential weighted recombination (lambda/mu) to form a new mean; occasional Cauchy/Heavy-tail perturbations and Lévy bursts provide global exploration. Online adaptation includes multiplicative sigma control driven by short-term success rate (sigma_lr, success_window, mix_target), directional curvature probes (paired evaluations to update dir_curv) to scale subspace coordinates, and per-coordinate S for anisotropic steps. Robustness features: clipped bounds, archive trimming, stagnation detection with soft restarts/randomized basis, and conservative parameter choices (k ~ O(log dim), init_sigma ~0.18·range, p_levy≈0.08) to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ARSS:\n    \"\"\"\n    ARSS: Adaptive Rotated Subspace Search\n\n    Main novel ingredients:\n    - Per-coordinate adaptive scale S (EMA of absolute successful step components) to shape diagonal proposals.\n    - An online orthonormal basis B (k-dim) learned via an Oja-like rule from accepted step directions.\n    - Mixture proposals combining diagonal-scaled orthogonal components and a rotated low-rank subspace component.\n    - Directional curvature probes: occasional paired evaluations along basis vectors to estimate local curvature and adapt per-direction scales.\n    - DE-style archive recombination and mirrored sampling for variance reduction.\n    - Temperature-like mixing weight (stored as logit) adaptively shifted toward a target success frequency.\n    - Heavy-tailed Lévy/Cauchy bursts and restarts when stagnation is detected.\n    - All samples clipped to provided bounds (default [-5,5]).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, init_sigma=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # Population and recombination sizes\n        self.lambda_ = max(4, int(np.round(4 + 3.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # Subspace dimension (default small relative to dim)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(1.8 * np.log(max(2, self.dim + 1)))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # initial sigma relative to domain (domain is [-5,5] typically)\n        if init_sigma is None:\n            self.init_sigma = 0.18 * (5.0 - (-5.0))\n        else:\n            self.init_sigma = float(init_sigma)\n\n        # Oja-like learning rate for basis\n        self.oja_eta = 0.12\n\n        # momentum for carrying promising directions\n        self.momentum = 0.7\n\n        # EMA for per-coordinate absolute steps\n        self.S_beta = 0.92\n        self.S_eps = 1e-9\n\n        # Archive for DE-like recombination\n        self.archive_limit = 1500\n        self.p_de = 0.22\n        self.F_de = 0.6\n\n        # Mirroring\n        self.mirror = True\n\n        # Logit-based mixing weight between diagonal and subspace proposals (stored as logit)\n        init_mix = 0.5\n        self.mix_logit = np.log(init_mix / (1.0 - init_mix))\n        self.mix_lr = 0.10\n        self.mix_target = 0.28  # desired short-term success rate\n\n        # sigma adaptation\n        self.sigma_lr = 0.75\n        self.success_window = max(8, self.lambda_)\n        self.stagnation_window = max(60, 5 * self.dim)\n\n        # burst parameters\n        self.p_levy = 0.08\n        self.levy_scale = 1.6\n        self.cauchy_scale = 1.0\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds handling\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initialize mean in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = max(1e-12, self.init_sigma)\n\n        # per-coordinate scale S (EMA of |accepted step|)\n        S = np.ones(n) * 0.5\n\n        # initialize small orthonormal basis B (n x k)\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            B = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            B = rand / (np.linalg.norm(rand, axis=0, keepdims=True) + 1e-12)\n\n        # curvature estimate per basis direction (positive definite scale)\n        dir_curv = np.ones(self.k) * 1.0\n\n        # momentum vector\n        p = np.zeros(n)\n\n        # function evaluation bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            f_best = fm\n            x_best = xm.copy()\n            archive_X = [xm.copy()]\n            archive_F = [fm]\n            fm_mean = fm\n        else:\n            archive_X = []\n            archive_F = []\n            fm_mean = np.inf\n\n        # precompute recombination log-linear weights (exponential-rank)\n        mu = min(self.mu, self.lambda_)\n        ranks = np.arange(1, mu + 1)\n        # exponential decay on ranks\n        weights = np.exp(-0.5 * (ranks - 1) / max(1.0, mu / 5.0))\n        weights /= np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # recent successes for mix and sigma adaptation\n        recent_succ = []\n\n        # helper: logit to mixing weight in (0,1)\n        def logit_to_mix(lg):\n            s = 1.0 / (1.0 + np.exp(-lg))\n            return float(np.clip(s, 1e-4, 1.0 - 1e-4))\n\n        # main optimization loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n\n            # generate half the samples and mirror them if requested for variance reduction\n            half = (lam + 1) // 2 if self.mirror else lam\n            base_z = np.random.randn(half, n)\n\n            arx = np.zeros((lam, n))\n            arz = np.zeros((lam, n))\n            arfit = np.full(lam, np.inf)\n\n            # current mixing weight\n            mix = logit_to_mix(self.mix_logit)\n\n            # subspace scale factor\n            median_S = float(np.median(S))\n\n            idx = 0\n            for j in range(half):\n                z = base_z[j].copy()\n                # project onto learned subspace and orthogonal complement\n                low = B @ (B.T @ z)\n                high = z - low\n\n                # scale orthogonal component per-coordinate\n                scaled_high = S * high\n\n                # scale subspace component based on directional curvature estimate:\n                # project low onto basis coordinates to weigh directions\n                coords = B.T @ low  # k-dim\n                # scale each coordinate by 1/sqrt(curvature) to prefer low curvature directions\n                coord_scale = 1.0 / (np.sqrt(dir_curv) + 1e-9)\n                scaled_low = B @ (coords * coord_scale)  # back to n\n\n                # stochastic blend: use squared-root blending to preserve overall variance\n                y = np.sqrt(1.0 - mix) * scaled_high + np.sqrt(mix) * (0.9 * scaled_low)\n\n                # occasionally apply Cauchy heavy tail to a sample\n                if np.random.rand() < 0.06:\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    # preserve direction of y, rescale to median\n                    ynorm = np.linalg.norm(y) + 1e-12\n                    y = r * (y / ynorm) * median_S\n\n                # mirror pair\n                for sign in (1.0, -1.0) if self.mirror else (1.0,):\n                    if idx >= lam:\n                        break\n                    y_signed = sign * y\n                    # incorporate momentum gently into the draw (not multiplied by sigma again)\n                    x = m + sigma * y_signed + 0.3 * p\n                    # occasional DE recombination with archive\n                    if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                        i1, i2, i3 = np.random.choice(len(archive_X), size=3, replace=False)\n                        de_mut = self.F_de * (archive_X[i1] - archive_X[i2]) + 0.2 * (archive_X[i3] - m)\n                        mask = (np.random.rand(n) < 0.5)\n                        x[mask] += de_mut[mask]\n                    x = np.clip(x, lb, ub)\n                    arx[idx, :] = x\n                    arz[idx, :] = y_signed\n                    idx += 1\n\n            # Evaluate candidates until budget or all evaluated\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # selection and recombination\n            idx_sorted = np.argsort(arfit)\n            sel = idx_sorted[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            # recombine mean and weighted step\n            m_old = m.copy()\n            # recombination weights might be longer than mu, so use only mu length subset of weights\n            w = weights[:len(sel)]\n            w = w / np.sum(w)\n            m = np.sum(w[:, None] * x_sel, axis=0)\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # success measure: did best of selected improve over last mean evaluation (fm_mean)\n            sel_best_f = np.min(arfit[sel]) if sel.size > 0 else np.inf\n            success = 1 if sel_best_f < fm_mean - 1e-12 else 0\n            recent_succ.append(success)\n            if len(recent_succ) > self.success_window:\n                recent_succ.pop(0)\n            succ_rate = float(np.mean(recent_succ)) if recent_succ else 0.0\n\n            # update mixing logit additively towards target success\n            delta_mix = self.mix_lr * (succ_rate - self.mix_target)\n            self.mix_logit += float(delta_mix)\n            mix = logit_to_mix(self.mix_logit)\n\n            # update momentum vector p (exponential blending)\n            p = 0.85 * p + 0.15 * (sigma * y_w)\n\n            # update per-coordinate scale S using EMA of absolute weighted selected steps\n            abs_y = np.sum(w[:, None] * np.abs(y_sel), axis=0)\n            S = self.S_beta * S + (1.0 - self.S_beta) * (abs_y + self.S_eps)\n\n            # RMS-like second moment to help sigma adaptation (small memory)\n            # adapt sigma multiplicatively with moderated gain and normalization by dimension\n            adj = self.sigma_lr * (succ_rate - 0.25) / max(0.08, np.sqrt(1.0 + n / 12.0))\n            sigma *= np.exp(adj)\n            # bound sigma relative to domain scale\n            domain_scale = np.mean(ub - lb)\n            sigma = float(np.clip(sigma, 1e-12, 40.0 * domain_scale))\n\n            # Update basis B via Oja using the accepted weighted directions projected to subspace coordinates\n            # compute aggregated signal in subspace coordinates\n            if np.linalg.norm(y_w) > 1e-12:\n                # project weighted step into subspace coords (k-dim)\n                signal_coords = B.T @ y_w\n                # normalize\n                norm_sc = np.linalg.norm(signal_coords) + 1e-12\n                ucoords = signal_coords / norm_sc\n                # Oja step for each basis vector (in coordinate space)\n                for j in range(self.k):\n                    uvec = B[:, j]\n                    proj = np.dot(uvec, B @ ucoords)\n                    # Oja-like update in ambient space using subspace signal back-projected\n                    step = self.oja_eta * proj * ((B @ ucoords) - proj * uvec)\n                    uvec = uvec + step\n                    # renormalize\n                    uvec = uvec / (np.linalg.norm(uvec) + 1e-12)\n                    B[:, j] = uvec\n                # occasional re-orthonormalize for numerical stability\n                if np.random.rand() < 0.12:\n                    try:\n                        Q, _ = np.linalg.qr(B)\n                        B = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # directional curvature probe: occasionally probe along each basis vector with paired evals\n            if (np.random.rand() < 0.08) and (evals + self.k * 2 <= budget):\n                # small probing scale relative to sigma and direction scale\n                probe_scale = 0.5 * sigma\n                for j in range(self.k):\n                    vdir = B[:, j]\n                    # two-sided probe\n                    x_plus = np.clip(m + probe_scale * vdir, lb, ub)\n                    x_minus = np.clip(m - probe_scale * vdir, lb, ub)\n                    f_plus = func(x_plus); evals += 1\n                    f_minus = func(x_minus); evals += 1\n                    archive_X.append(x_plus.copy()); archive_F.append(f_plus)\n                    archive_X.append(x_minus.copy()); archive_F.append(f_minus)\n                    if len(archive_X) > self.archive_limit:\n                        # trim to limit\n                        while len(archive_X) > self.archive_limit:\n                            archive_X.pop(0); archive_F.pop(0)\n                    # approximate curvature by second difference: (f+ + f- - 2*f0)/(scale^2)\n                    f0 = fm_mean\n                    cur = max(1e-6, (f_plus + f_minus - 2.0 * f0) / (probe_scale ** 2 + 1e-12))\n                    # update dir_curv as EMA towards observed curvature (clip positive)\n                    dir_curv[j] = 0.85 * dir_curv[j] + 0.15 * max(cur, 1e-6)\n                    # update best if found\n                    if f_plus < f_best:\n                        f_best = f_plus; x_best = x_plus.copy()\n                    if f_minus < f_best:\n                        f_best = f_minus; x_best = x_minus.copy()\n\n            # occasional Lévy burst when random or when stagnation detected\n            stagnated = (len(archive_F) >= self.stagnation_window) and (np.min(archive_F[-self.stagnation_window:]) >= f_best - 1e-12)\n            if (np.random.rand() < self.p_levy) or stagnated:\n                # attempt several heavy-tailed trials but respect budget\n                tries = min(6, budget - evals)\n                # temporarily inflate sigma for burst\n                old_sigma = sigma\n                sigma *= 2.2\n                for _ in range(tries):\n                    z = np.random.randn(n)\n                    r = np.random.standard_cauchy() * self.levy_scale\n                    yn = z / (np.linalg.norm(z) + 1e-12)\n                    y = r * yn * median_S\n                    x_try = np.clip(m + sigma * y, lb, ub)\n                    f_try = func(x_try)\n                    evals += 1\n                    archive_X.append(x_try.copy()); archive_F.append(f_try)\n                    if f_try < f_best:\n                        f_best = f_try\n                        x_best = x_try.copy()\n                        # adopt new mean, clear momentum and successes to encourage local exploitation\n                        m = x_try.copy()\n                        p = np.zeros(n)\n                        recent_succ = []\n                        fm_mean = f_try\n                sigma = old_sigma\n\n            # update fm_mean conservatively (best-known function at the mean or best selected)\n            # Evaluate current mean occasionally (but never exceed budget): we only update fm_mean when\n            # we have an evaluated point that corresponds to the mean, or use selected best as proxy.\n            # Here we set fm_mean to min(previous fm_mean, best selected)\n            fm_mean = min(fm_mean, sel_best_f, f_best)\n\n            # If stagnated very long time - soft restart by randomizing some basis vectors and slightly reset sigma\n            if stagnated and (np.random.rand() < 0.5):\n                # randomize half of basis vectors\n                for j in range(max(1, self.k // 2)):\n                    newvec = np.random.randn(n)\n                    newvec /= (np.linalg.norm(newvec) + 1e-12)\n                    B[:, j] = newvec\n                try:\n                    Q, _ = np.linalg.qr(B)\n                    B = Q[:, :self.k]\n                except np.linalg.LinAlgError:\n                    pass\n                sigma = max(0.5 * sigma, 0.5 * self.init_sigma)\n                recent_succ = []\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ARSS scored 0.496 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "7001612a-f98b-47b3-b88d-77166bdf3b68", "operator": null, "metadata": {"aucs": [0.31746307649896877, 0.16096154647503336, 0.53811793257007, 0.946278218189719, 0.6388460172318652, 0.6651448349341442, 0.3328545438117837, 0.5107657581803926, 0.6443411063532072, 0.20524384219665592]}, "task_prompt": ""}
{"id": "e7c8b601-7ad0-498e-b8b7-d714cc8ed3a0", "fitness": 0.17787713899951849, "name": "SPECTRE", "description": "SPECTRE mixes an Adam-like per-coordinate scaler (beta1=0.9, beta2=0.95, D_exponent=-0.5) with a learned low-dimensional spectral subspace (Oja incremental PCA, oja_eta=0.08, k≈ceil(log2(dim)+1)) and an adaptive operator bandit that probabilistically blends diagonal Gaussian, spectral Gaussian, Cauchy heavy-tail, and archive-based DE proposals. Population sizing is conservative and dimension-aware (lambda = 4+3·log(dim), mu = lambda//2) with a relatively large initial sigma (init_sigma ≈ 1.2) that is multiplicatively adapted by a CSA-like rule toward a target success rate (alpha_target/sigma targets ≈0.2) while alpha (diagonal vs subspace mix) is also multiplicatively adjusted. The algorithm uses mirrored sampling, linear recombination weights, a small momentum term for inertia, and an archive (limit 1500) to drive DE-style mutations (p_de≈0.22, F_de≈0.65); operator choices are updated via softmaxed success rates (eta≈1.2). To escape stagnation it triggers Lévy-like heavy-tail bursts, always clips to provided bounds (default [-5,5]) and strictly respects the given evaluation budget.", "code": "import numpy as np\n\nclass SPECTRE:\n    \"\"\"\n    SPECTRE: Spectral Probabilistic Ensemble Search\n\n    One-line idea:\n      Combine an adaptive per-coordinate second-moment scaler (Adam-like),\n      a learned small spectral subspace (incremental Oja/PCA), and an adaptive\n      operator-choice bandit to mix Gaussian-diagonal, spectral, DE and Cauchy\n      proposals; adapt mixing (alpha) multiplicatively and sigma via a CSA-like\n      multiplicative rule. Includes mirrored sampling, archive-based DE, and\n      occasional Lévy bursts on stagnation.\n\n    Main parameters (defaults chosen to be different from the provided LAROS code\n    and to use different update equations):\n      - lambda_ : population size heuristic (4 + 3*log(dim))\n      - mu : number recombined (lambda_//2)\n      - k : spectral/subspace dimension (ceil(log2(dim)+1))\n      - beta1, beta2 : first/second moment decay for adaptive diagonal scaling (Adam-like)\n      - v_eps : small epsilon for second-moment stability\n      - D_exponent : -0.5 (inverse-sqrt style scaling unlike -0.4 in the reference)\n      - oja_eta : small learning rate for incremental subspace updates (0.08)\n      - alpha : diagonal/subspace mixing probability (stored directly, multiplicative updates)\n      - alpha_lr : multiplicative learning rate for alpha (0.45)\n      - alpha_target : target short-term success rate (0.20)\n      - sigma_lr : multiplicative sigma gain (0.6) applied CSA-like\n      - p_cauchy : chance to use Cauchy heavy tail (0.12)\n      - p_de : chance to attempt DE recombination (0.22), F_de default 0.65\n      - mirror : mirrored sampling enabled\n      - archive_limit : archive size for DE (1500)\n      - operator_bandit_eta : responsiveness for operator probability softmax (1.2)\n\n    The code respects the budget of evaluations and assumes bounds [-5,5] if func.bounds not provided.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, k=None, init_sigma=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # population sizing\n        self.lambda_ = max(4, int(4 + np.round(3.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # spectral subspace dimension\n        if k is None:\n            # choose subspace by log2(dim) to be small for high-dim\n            self.k = max(1, int(np.ceil(np.log2(max(2, self.dim)) + 1.0)))\n        else:\n            self.k = min(max(1, int(k)), self.dim)\n\n        # initial sigma scale (different scaling)\n        if init_sigma is None:\n            self.init_sigma = 0.12 * 10.0\n        else:\n            self.init_sigma = float(init_sigma)\n\n        # adaptive diagonal scaler (Adam-like)\n        self.beta1 = 0.9\n        self.beta2 = 0.95\n        self.v_eps = 1e-8\n        self.D_exponent = -0.5  # inverse-sqrt-ish\n\n        # Oja (incremental PCA) parameters\n        self.oja_eta = 0.08\n\n        # mixing alpha and its multiplicative adaptation\n        self.alpha = 0.40\n        self.alpha_lr = 0.45  # used multiplicatively (alpha *= exp(...))\n        self.alpha_target = 0.20\n\n        # sigma adaptation (CSA-like multiplicative update)\n        self.sigma_lr = 0.6\n\n        # operator probabilities (bandit-adapted)\n        # operators: 0=diag-gauss,1=spectral-gauss,2=cauchy,3=de-mut\n        self.operator_count = 4\n        self.op_success = np.ones(self.operator_count) * 1e-3\n        self.op_trials = np.ones(self.operator_count) * 1e-3\n        self.operator_bandit_eta = 1.2\n\n        # probabilities for special operators (baseline)\n        self.p_cauchy = 0.12\n        self.p_de = 0.22\n        self.F_de = 0.65\n        self.mirror = True\n\n        # archive for DE\n        self.archive_limit = 1500\n\n        # mirrored sampling uses pairs to reduce variance\n        # stagnation detection\n        self.stagnation_window = max(60, 5 * self.dim)\n        self.levy_burst_trials = 8\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds handling (default [-5,5])\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initialize mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = max(1e-12, float(self.init_sigma))\n\n        # adaptive diagonal accumulators (first and second moment)\n        m_acc = np.zeros(n)\n        v_acc = np.ones(n) * 1e-6  # small init for numerical stability\n\n        # spectral subspace initialization via random orthonormal basis\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            U = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = rand / (np.linalg.norm(rand, axis=0, keepdims=True) + 1e-12)\n\n        # momentum-like component (carry inertia)\n        momentum = np.zeros(n)\n\n        # recombination weights (simple linear decreasing weights different from log-linear)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        ranks = np.arange(1, mu + 1)\n        weights = (mu + 1.0 - ranks)  # linear\n        weights = np.maximum(weights, 1e-12)\n        weights /= np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # archive for DE\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = fm; x_best = xm.copy()\n            fm_mean = fm\n        else:\n            fm_mean = np.inf\n\n        recent_successes = []\n\n        # helper: operator probabilities from bandit stats\n        def operator_probs():\n            est = (self.op_success / (self.op_trials + 1e-12))\n            # softmax-like\n            scores = np.exp(self.operator_bandit_eta * (est - np.mean(est)))\n            probs = scores / (np.sum(scores) + 1e-12)\n            return probs\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # sample base normals\n            base_z = np.random.randn(current_lambda, n)\n\n            op_probs = operator_probs()\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # decompose into spectral subspace and orthogonal complement\n                proj_low = U @ (U.T @ z)\n                proj_high = z - proj_low\n\n                # compute diagonal scaler D using v_acc (Adam-like second moment, bias-corrected)\n                # here we use a simple non-biased formulation per-iteration approx\n                D = (v_acc + self.v_eps) ** self.D_exponent  # inverse sqrt\n\n                # pick operator via soft probabilities (bandit)\n                op = np.random.choice(self.operator_count, p=op_probs)\n\n                # base components\n                diag_comp = D * proj_high\n                sub_comp = proj_low  # spectral component (units preserved)\n\n                # combine with current alpha multiplicatively (alpha in [0,1])\n                # different mapping: use sqrt blending to emphasize subspace when alpha large\n                alpha = float(np.clip(self.alpha, 1e-6, 1.0 - 1e-6))\n                y = np.sqrt(1.0 - alpha) * diag_comp + np.sqrt(alpha) * (0.9 * sub_comp)\n\n                # operator-specific tweaks\n                if op == 2 or (np.random.rand() < self.p_cauchy and op != 0):\n                    # Cauchy heavy-tail\n                    r = np.random.standard_cauchy() * 1.5\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * np.median(np.abs(D))\n\n                # DE-style operator: if chosen and archive has entries, perturb with rand/1\n                if op == 3 and len(archive_X) >= 3 and (np.random.rand() < self.p_de):\n                    i1, i2, i3 = np.random.choice(len(archive_X), size=3, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2]) + 0.25 * (archive_X[i3] - m)\n                    # binomial crossover\n                    mask = (np.random.rand(n) < (0.5))\n                    y[mask] += de_mut[mask]\n\n                # mirrored sampling for variance reduction (use sign flip on odd indices)\n                if self.mirror and (i % 2 == 1):\n                    y = -y\n\n                # apply momentum as small additive offset scaled by sigma\n                x = m + sigma * y + 0.4 * momentum\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate offsprings (stop if budget reached)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # selection\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            # recombine mean with linear weights (already set)\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # determine success: improvement over current mean fitness or global best\n            sel_best_f = np.min(arfit[sel]) if sel.size > 0 else np.inf\n            success_flag = 1 if sel_best_f < fm_mean - 1e-12 else 0\n            recent_successes.append(success_flag)\n            if len(recent_successes) > max(5, int(self.mu)):\n                recent_successes.pop(0)\n            success_rate = float(np.mean(recent_successes)) if recent_successes else 0.0\n\n            # update operator bandit statistics: attribute success to chosen operators among selected\n            # for simplicity, increment trials for each operator used in population and successes if selected improved\n            used_ops = []\n            # reconstruct used ops probabilities per individual (approx) by sampling again deterministically using op_probs\n            # here we approximate: sample operators equal to top probable ones for bookkeeping\n            # A simpler approach: credit operator of each selected by re-evaluating probability sampling (deterministic sample)\n            sampled_ops = np.random.choice(self.operator_count, size=current_lambda, p=op_probs)\n            for j in range(current_lambda):\n                opj = int(sampled_ops[j])\n                self.op_trials[opj] += 1.0\n            # credit successful operators among selected individuals (if any selected indices are from j)\n            for j_idx in sel:\n                if j_idx < len(sampled_ops):\n                    opj = int(sampled_ops[j_idx])\n                    self.op_success[opj] += success_flag\n\n            # multiplicative alpha update (different equation): alpha *= exp(alpha_lr * (success_rate - target))\n            self.alpha *= np.exp(self.alpha_lr * (success_rate - self.alpha_target))\n            self.alpha = float(np.clip(self.alpha, 1e-4, 1.0 - 1e-4))\n\n            # update momentum: small momentum towards last weighted step\n            momentum = 0.88 * momentum + 0.12 * (sigma * y_w)\n\n            # update adaptive moments for diagonal scaling based on weighted squared steps (Adam-like)\n            # first moment (not strictly necessary, but included)\n            m_acc = self.beta1 * m_acc + (1.0 - self.beta1) * y_w\n            v_acc = self.beta2 * v_acc + (1.0 - self.beta2) * (y_w ** 2 + self.v_eps)\n\n            # compute D implicitly for next iteration\n            D = (v_acc + self.v_eps) ** self.D_exponent\n\n            # sigma adaptation: CSA-like multiplicative with normalization by sqrt(mu_eff)\n            # target success is 0.2 (alpha_target is 0.2 too)\n            sigma_adj = self.sigma_lr * (success_rate - 0.2) / max(1.0, np.sqrt(mu_eff))\n            sigma *= float(np.exp(sigma_adj))\n            # bound sigma reasonably relative to domain\n            domain_scale = np.mean(ub - lb)\n            sigma = float(np.clip(sigma, 1e-12, 40.0 * domain_scale))\n\n            # Oja incremental update of U using y_w as signal (normalize)\n            sig = y_w.copy()\n            sig_norm = np.linalg.norm(sig) + 1e-12\n            if sig_norm > 1e-12:\n                sig_unit = sig / sig_norm\n                for j in range(U.shape[1]):\n                    u = U[:, j]\n                    proj = np.dot(u, sig_unit)\n                    # multiplicative variant of Oja-like rule\n                    u = u + self.oja_eta * (sig_unit - proj * u) * (proj + 0.05)\n                    U[:, j] = u / (np.linalg.norm(u) + 1e-12)\n                # occasional orthonormalize every few iterations with chance\n                if np.random.rand() < 0.12:\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # stagnation detection: if no improvement for a window, try Lévy-like bursts\n            archive_len = len(archive_F)\n            if archive_len >= self.stagnation_window and (evals % self.stagnation_window == 0):\n                last_vals = archive_F[-self.stagnation_window:]\n                if len(last_vals) == self.stagnation_window and np.min(last_vals) >= f_best - 1e-12:\n                    # try several burst directions\n                    old_sigma = sigma\n                    sigma *= 3.0\n                    for _ in range(min(self.levy_burst_trials, budget - evals)):\n                        z = np.random.randn(n)\n                        r = np.random.standard_cauchy() * 2.0\n                        y = r * (z / (np.linalg.norm(z) + 1e-12)) * np.median(np.abs(D))\n                        x_try = np.clip(m + sigma * y, lb, ub)\n                        f_try = func(x_try)\n                        evals += 1\n                        archive_X.append(x_try.copy()); archive_F.append(f_try)\n                        if f_try < f_best:\n                            f_best = f_try\n                            x_best = x_try.copy()\n                            # adopt escape\n                            m = x_try.copy()\n                            momentum = np.zeros(n)\n                            recent_successes = []\n                            fm_mean = f_try\n                    sigma = old_sigma\n\n            # update fm_mean as conservative reference: min of last mean evaluation or best\n            fm_mean = min(f_best, fm_mean)\n\n            # ensure mean within bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm SPECTRE scored 0.178 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "7001612a-f98b-47b3-b88d-77166bdf3b68", "operator": null, "metadata": {"aucs": [2.0000000000020002e-05, 0.033819129369923706, 0.15125350683879968, 0.8571136949698197, 0.3557920666969352, 0.09517257036011662, 0.06743474515576353, 0.04968631170748483, 0.11354201297639555, 0.05493735191994609]}, "task_prompt": ""}
{"id": "24386d35-eb72-4c42-a70e-b996c476ebb0", "fitness": "-inf", "name": "HSPLASH", "description": "HSPLASH is a hybrid search that mixes per‑coordinate RMS-style scaling (G -> D) with a learned low‑rank sampling subspace U (default k ≈ ceil(sqrt(n))) updated online by Oja and periodically re-aligned by SVD, and samples candidates as a convex mix of diagonal and low‑rank Gaussian components (s_low tracks low‑rank scales). It uses small, budget‑friendly populations (λ ≈ 4 + 3 log(n), μ ≈ λ/2) with mirrored sampling for variance reduction, momentum on the mean, occasional archive‑based DE differences (p_de≈0.2), sporadic Cauchy/Levy jumps (p_cauchy≈0.12 and Levy bursts on stagnation) for robust escapes, and strict clipping to the given bounds. Step‑size σ is adapted multiplicatively by a path‑length style accumulator (ps, cs, damps, χ_n) combined with a short‑term success-rate factor, while success_buffer, archive limits, and conservative parameter choices (e.g. σ_init=0.2·domain_scale, rms_beta=0.92, oja_eta=0.1) provide stability and robustness under a fixed evaluation budget.", "code": "import numpy as np\n\nclass HSPLASH:\n    \"\"\"\n    H-SPLASH: Hybrid Subspace-Path-Length Adaptive Search\n\n    One-line: Mix RMSProp/AdaGrad per-coordinate scaling with an online Oja-learned low-rank subspace,\n    adapt sigma by both path-length and short-term success-rate, use mirrored sampling + momentum, and\n    include archive DE-differences and Cauchy/Levy bursts for robust escapes — tuned for bounded\n    noiseless black-box benchmarks.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population and selection sizes (small, budget-friendly)\n        self.lambda_ = max(6, int(4 + np.floor(3.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank subspace dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))  # sensible default\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # diagonal accumulator hyper-params (RMS-like)\n        self.rms_beta = 0.92\n        self.g_eps = 1e-8\n\n        # Oja learning rate for online subspace\n        self.oja_eta = 0.10\n\n        # path-length style constants (approx by n)\n        # cs/damps tuned similar to CMA-like defaults\n        self.cs = None  # computed per-run from mu_eff and n\n        self.damps = None\n\n        # mixing & escape probabilities\n        self.p_cauchy = 0.12\n        self.cauchy_scale = 1.0\n        self.p_de = 0.20\n        self.F_de = 0.7\n\n        # mirrored sampling toggled by default\n        self.mirror = True\n\n        # momentum\n        self.momentum_decay = 0.88\n\n        # subspace buffer (for periodic SVD)\n        self.buffer_max = max(20, 8 * self.k)\n        self.svd_every_evals = max(50, 10 * self.k)\n\n        # stagnation / Levy burst settings\n        self.stagnation_check = max(60, 6 * self.dim)\n        self.levy_burst_count = 6\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (problem guarantees [-5,5], but respect func.bounds)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # recombination weights (log-based)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length constants\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        self.cs = cs\n        self.damps = damps\n\n        # initialize state\n        m = np.random.uniform(lb, ub)                              # mean\n        domain_scale = np.mean(ub - lb)\n        sigma = 0.2 * domain_scale                                 # initial step-size\n\n        # diagonal RMS accumulator (initialized small)\n        G = np.ones(n) * 1e-3\n        D = (G + self.g_eps) ** (-0.5)                              # per-coordinate scale (std-like)\n\n        # low-rank subspace U (n x k)\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            U = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = rand / (np.linalg.norm(rand, axis=0, keepdims=True) + 1e-12)\n\n        # small diagonal singular values for low-rank sampling (learnable scale)\n        s_low = np.ones(self.k) * 0.8\n\n        # momentum for mean\n        v = np.zeros(n)\n\n        # path evolution vector (in approx invsqrt diag space)\n        ps = np.zeros(n)\n\n        # archives\n        archive_X = []\n        archive_F = []\n        archive_limit = 4000\n\n        # buffers for SVD learning\n        success_buffer = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # counters\n        eigen_counter = 0\n        last_improvement_eval = 0\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            if fm < f_best:\n                f_best = fm; x_best = xm.copy(); last_improvement_eval = evals\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n\n            # produce an even number of candidates if using mirrored sampling when possible\n            current_lambda = min(lam, remaining)\n            if self.mirror and (current_lambda % 2 == 1) and current_lambda > 1:\n                current_lambda -= 1\n            if current_lambda <= 0:\n                break\n\n            half = current_lambda // 2 if self.mirror else current_lambda\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # sample half unique and mirror them (variance reduction)\n            for i in range(half):\n                # diagonal gaussian part\n                z_diag = np.random.randn(n)\n                y_diag = D * z_diag\n\n                # low-rank gaussian part\n                z_low = np.random.randn(self.k)\n                y_low = (U @ (s_low * z_low)) if self.k > 0 else 0.0\n\n                # mix diagonal and low-rank adaptively: weight by relative energy of D vs low-rank\n                alpha = 0.5  # fixed initial mixing weight (could be made adaptive)\n                y = (1.0 - alpha) * y_diag + alpha * y_low\n\n                # occasional Cauchy jump\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    nz = np.linalg.norm(z_diag) + 1e-20\n                    y = r * (z_diag / nz) * np.mean(D)\n\n                # primary candidate\n                x1 = m + sigma * y + 0.5 * v\n                # optional DE-style archive mutation on candidate\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x1 = x1 + de_mut\n\n                x1 = np.clip(x1, lb, ub)\n                arx[2 * i if self.mirror else i] = x1\n                arz[2 * i if self.mirror else i] = y\n\n                if self.mirror:\n                    # mirrored pair\n                    x2 = m - sigma * y + 0.5 * v\n                    if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                        # sometimes apply a milder archive mutation on the mirror\n                        j1, j2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        de_mut2 = 0.5 * self.F_de * (archive_X[j1] - archive_X[j2])\n                        x2 = x2 + de_mut2\n                    x2 = np.clip(x2, lb, ub)\n                    arx[2 * i + 1] = x2\n                    arz[2 * i + 1] = -y  # mirrored y\n\n            # evaluate candidates without exceeding budget\n            for k in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[k]\n                f = func(x)\n                evals += 1\n                arfit[k] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f; x_best = x.copy(); last_improvement_eval = evals\n\n            # selection and recombination\n            idx = np.argsort(arfit)\n            sel_idx = idx[:mu]\n            # If no finite values (shouldn't happen), skip updates\n            if sel_idx.size == 0 or not np.isfinite(arfit[sel_idx]).any():\n                # fallback: small random perturbation to mean\n                m = np.clip(m + 0.1 * sigma * np.random.randn(n), lb, ub)\n                continue\n\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            m_old = m.copy()\n            # recombine new mean\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space (approx, ignoring v)\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # path-length evolution (approx invsqrt diag by 1/D)\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n\n            # sigma adaptation: combine path-length and short-term success-based factor\n            # success_rate: fraction of top-mu that improved relative to best-so-far\n            sel_f = arfit[sel_idx]\n            success_flag_array = (sel_f < f_best + 1e-12)  # conservative; likely 0 often\n            success_rate = float(np.mean(success_flag_array)) if sel_f.size > 0 else 0.0\n            # multiplicative update: path-length term\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            # short-term success multiplier (moderate)\n            sigma *= np.exp(0.6 * (success_rate - 0.2) / max(0.08, np.sqrt(1.0 + n / 8.0)))\n            # bounds for sigma\n            sigma = float(np.clip(sigma, 1e-12, 50.0 * domain_scale))\n\n            # update momentum\n            v = self.momentum_decay * v + 0.9 * sigma * y_w\n\n            # update diagonal RMS accumulator G and D\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            G = self.rms_beta * G + (1.0 - self.rms_beta) * (y2 + self.g_eps)\n            D = (G + self.g_eps) ** (-0.5)\n\n            # push weighted successful step into buffer for SVD learning\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.buffer_max:\n                success_buffer.pop(0)\n\n            # online Oja update for U using normalized y_w as signal\n            signal = y_w.copy()\n            signal_norm = np.linalg.norm(signal) + 1e-12\n            if signal_norm > 1e-12:\n                sunit = signal / signal_norm\n                # update singular scales modestly toward observed projection magnitude\n                proj = U.T @ signal\n                # adapt low-rank singular scales (simple EMA toward observed abs projection)\n                s_low = 0.9 * s_low + 0.1 * (np.abs(proj) + 1e-8)\n                for j in range(U.shape[1]):\n                    u = U[:, j]\n                    p = np.dot(u, sunit)\n                    u = u + self.oja_eta * p * (sunit - p * u)\n                    u_norm = np.linalg.norm(u) + 1e-12\n                    U[:, j] = u / u_norm\n\n            # occasional SVD on the small success buffer to re-align U (cheap because buffer small)\n            eigen_counter += current_lambda\n            if eigen_counter >= self.svd_every_evals:\n                eigen_counter = 0\n                try:\n                    Y = np.vstack(success_buffer).T if len(success_buffer) > 0 else np.zeros((n, 0))\n                    if Y.shape[1] >= min(2, self.k):\n                        U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                        k_take = min(self.k, U_new.shape[1])\n                        if k_take > 0:\n                            U = U_new[:, :k_take]\n                            # pad s_low if needed\n                            if s_low.size < self.k:\n                                s_low = np.pad(s_low, (0, self.k - s_low.size), 'constant', constant_values=1.0)\n                            s_low[:k_take] = 0.8 * s_low[:k_take] + 0.2 * (svals[:k_take] + 1e-8)\n                except np.linalg.LinAlgError:\n                    # ignore and keep previous U\n                    pass\n\n            # stagnation detection: if no improvement for long period, perform Levy-like burst attempts\n            if (evals - last_improvement_eval) > self.stagnation_check and evals < budget:\n                # attempt a few Levy/Cauchy bursts (budget permitting)\n                old_sigma = sigma\n                sigma *= 2.5\n                for _ in range(min(self.levy_burst_count, budget - evals)):\n                    z = np.random.randn(n)\n                    r = np.random.standard_cauchy() * 1.5\n                    y = r * (z / (np.linalg.norm(z) + 1e-12)) * np.mean(D)\n                    x_try = np.clip(m + sigma * y, lb, ub)\n                    f_try = func(x_try)\n                    evals += 1\n                    archive_X.append(x_try.copy()); archive_F.append(f_try)\n                    if len(archive_X) > archive_limit:\n                        archive_X.pop(0); archive_F.pop(0)\n                    if f_try < f_best:\n                        f_best = f_try; x_best = x_try.copy(); last_improvement_eval = evals\n                        # adopt the escaping point as new mean and reset momentum\n                        m = x_try.copy(); v = np.zeros(n)\n                sigma = old_sigma\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # loop continues until budget exhausted\n\n        # final return\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 232, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,1) (2,2) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "error": "In the code, line 232, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,1) (2,2) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "parent_ids": "7001612a-f98b-47b3-b88d-77166bdf3b68", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3fb1021e-8ff8-4257-88e7-6946e2b70d03", "fitness": "-inf", "name": "SLARDE", "description": "SLARDE is a hybrid optimizer combining self-adaptive differential evolution (DE) moves (p_de=0.25, archive-driven differential vectors, log-normal F and Beta CR sampling) with CMA-style path-length step-size control (ps, chi_n, cs/damps) and a cheap per-coordinate diagonal scaling D (initialized to ones, updated via exponential moving second moments with c_d=0.20). It learns a low-rank correlated subspace U (k ≈ ceil(sqrt(dim))) from a rolling success_buffer using SVD and injects low-rank corrections into offspring to capture important directions while keeping full covariance cheap; U is initialized via QR of a random matrix and periodically updated. Mirrored Gaussian sampling (to reduce variance), occasional Lévy long jumps (p_levy=0.12) for global exploration, and population sizing lambda_ ≈ pop_scale + 5·log(dim) are used to balance exploration/exploitation, while recombination uses descending log-weights and mu = lambda_/2 to form the new mean. Practical safeguards include bounds clipping, strict budget accounting, stagnation handling (inflate sigma, nudge mean toward archived good solutions), and limits on sigma/D to keep searches stable.", "code": "import numpy as np\n\nclass SLARDE:\n    \"\"\"\n    SLARDE: Subspace-Learned Adaptive Rank-Differential Evolution\n\n    One-line: Hybrid self-adaptive DE + CMA-style path-length control using a cheap diagonal\n    scaling plus a learned low-rank correlated subspace (U) updated from recent successes.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, pop_scale=5):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population size similar to LARDE but modest\n        self.lambda_ = max(4, int(pop_scale + np.floor(5.0 * np.log(max(1, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n        # subspace dimension (~sqrt(n) by default, can be passed smaller/larger)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def _levy_mantegna(self, n, alpha=1.5, scale=1.0):\n        sigma_u = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                   (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n        u = np.random.normal(0, sigma_u, size=n)\n        v = np.random.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1.0 / alpha))\n        return scale * step\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds from func\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_mean = np.mean(ub - lb)\n\n        # recombination log-weights (descending) and mu_eff\n        weights = np.log(np.arange(1, self.mu + 1) + 0.5)[::-1]\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # adaptation constants (mix of LARDE + ASRES heuristics)\n        cc = 2.0 / (n + 2.0)\n        cs = max(0.1, 0.35 * (mu_eff / (n + mu_eff + 1.0)))\n        damps = 1.0 + 0.3 * cs + 0.2 * np.sqrt(max(0.0, mu_eff / n))\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # dynamic state\n        m = np.random.uniform(lb, ub)                     # mean\n        sigma = 0.25 * domain_mean                         # initial step-size\n        D = np.ones(n)                                    # per-coordinate std approx\n        # low-rank subspace U (n x k) orthonormal\n        rand_mat = np.random.randn(n, self.k)\n        try:\n            U, _ = np.linalg.qr(rand_mat)\n            U = U[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = np.zeros((n, self.k))\n        ps = np.zeros(n)                                  # path for sigma\n        success_buffer = []                               # buffer of recent successful y\n        buffer_max = max(10 * self.k, 40)\n        archive_X = []                                    # archive for DE\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # control probabilities\n        p_levy = 0.12\n        p_de = 0.25\n        mirrored = True\n\n        # DE self-adaptive parameter hyper-choices\n        F_loc_scale = 0.5   # sigma for log-normal F sampling\n        cr_a, cr_b = 2.0, 2.0  # beta parameters for CR\n\n        # update rates for diagonal D\n        c_d = 0.20\n\n        # safe eval wrapper (ensures budget restraint)\n        def clip_to_bounds(x):\n            return np.clip(x, lb, ub)\n\n        def safe_eval(x):\n            nonlocal evals, f_opt, x_opt\n            x = clip_to_bounds(x)\n            if evals >= budget:\n                return None, None\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if f < f_opt:\n                f_opt = float(f)\n                x_opt = x.copy()\n            return f, x\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = clip_to_bounds(m)\n            out = safe_eval(xm)\n            if out[0] is None:\n                # no budget\n                return float(f_opt if x_opt is not None else np.inf), (np.array(x_opt) if x_opt is not None else np.zeros(n))\n        # stagnation trackers\n        last_improvement_eval = evals\n        stagnation_thresh = max(10, int(4 * n / (self.k if self.k > 0 else 1)))\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(self.lambda_, remaining)\n            # mirrored sampling: produce half and mirror if even\n            if mirrored and (current_lambda % 2 == 0):\n                half = current_lambda // 2\n                z_half = np.random.randn(half, n)\n                arz = np.vstack([z_half, -z_half])\n            else:\n                arz = np.random.randn(current_lambda, n)\n\n            # We'll also produce low-rank coefficients for each offspring\n            z_low = np.random.randn(current_lambda, self.k) if self.k > 0 else np.zeros((current_lambda, 0))\n\n            arx = np.zeros((current_lambda, n))\n            arz_combined = np.zeros((current_lambda, n))\n            for i in range(current_lambda):\n                z = arz[i]\n                # low-rank component\n                if self.k > 0:\n                    low = U @ z_low[i]\n                    beta = 0.9 * np.mean(D)  # weight for low-rank component\n                    y = D * z + beta * low\n                else:\n                    y = D * z\n\n                # create candidate before DE/Levy\n                x = m + sigma * y\n\n                # per-offspring self-adaptive DE factor and CR\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    # sample self-adaptive F and CR\n                    Fk = np.exp(np.random.normal(np.log(0.8), F_loc_scale))\n                    CRk = np.clip(np.random.beta(cr_a, cr_b), 0.0, 1.0)\n                    # pick two archive indices to form a differential vector\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_vec = Fk * (archive_X[i1] - archive_X[i2])\n                    # apply binomial-like mask (on continuous coordinates we add only selected dims)\n                    mask = (np.random.rand(n) < CRk)\n                    if not np.any(mask):\n                        mask[np.random.randint(n)] = True\n                    # apply only on masked coordinates\n                    x = x + de_vec * mask\n\n                # occasional Levy long jump (rare)\n                if np.random.rand() < p_levy:\n                    levy = self._levy_mantegna(n, alpha=1.5, scale=0.5 * sigma)\n                    x = x + levy\n\n                # clip and store\n                x = np.clip(x, lb, ub)\n                arx[i] = x\n                arz_combined[i] = y  # unscaled y (so (x - m) / sigma ≈ y)\n\n            # evaluate offspring sequentially\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                out = safe_eval(arx[i])\n                if out[0] is None:\n                    break\n                arfit[i] = out[0]\n\n            # selection & recombination if any finite fitness were obtained\n            finite_idx = np.where(np.isfinite(arfit))[0]\n            if finite_idx.size == 0:\n                break\n\n            idx_sorted = np.argsort(arfit)\n            sel_idx = idx_sorted[:self.mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz_combined[sel_idx]  # approximate normalized steps in y-space\n\n            m_old = m.copy()\n            # weighted recombination in x-space\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # compute weighted mean step y_w\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # path-length control: approximate invsqrtC by dividing by D (ignore low-rank for speed)\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n            # update sigma using path-length\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e2 * np.mean(ub - lb) + 1e-12)\n\n            # update diagonal D with exponential moving second moment of selected y\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n\n            # buffer weighted y_w (scaled to be independent of sigma)\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace U periodically using SVD of buffer\n            if (len(success_buffer) >= max(3, self.k)) and (evals % max(1, int(np.ceil(0.5 * self.k))) == 0):\n                try:\n                    Y = np.vstack(success_buffer).T  # n x m\n                    Y = Y - np.mean(Y, axis=1, keepdims=True)\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        if U.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U.shape[1]))\n                            U = np.hstack([U, pad])\n                except np.linalg.LinAlgError:\n                    pass  # keep old U\n\n            # stagnation handling: if no improvement for long, inflate sigma and nudge mean toward archive best\n            if (evals - last_improvement_eval) > stagnation_thresh * max(1, current_lambda):\n                sigma *= 1.8\n                if len(archive_X) > 0:\n                    # nudge toward a random good archive member (biased to best)\n                    if np.random.rand() < 0.5:\n                        pick = np.argmin(archive_F)\n                        choice = archive_X[pick]\n                    else:\n                        choice = archive_X[np.random.randint(len(archive_X))]\n                    m = 0.6 * m + 0.4 * choice\n                success_buffer = []\n                # small random perturbation of D to explore different scalings\n                D = D * (1.0 + 0.2 * (np.random.rand(n) - 0.5))\n\n            # update last improvement eval tracker\n            if f_opt < np.inf and last_improvement_eval < evals:\n                # if we saw improvement since previous recorded, update last_improvement_eval\n                last_improvement_eval = evals if (evals - last_improvement_eval) == 0 else last_improvement_eval\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # small safeguard to keep D reasonable\n            D = np.clip(D, 1e-8, 1e2)\n\n            # break if budget exhausted (loop condition)\n            if evals >= budget:\n                break\n\n        # return best found\n        if x_opt is None:\n            # fallback to mean\n            xm = np.clip(m, lb, ub)\n            return float(np.inf), xm\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "7001612a-f98b-47b3-b88d-77166bdf3b68", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4bf72b04-03e2-41ba-ba6e-8ccf93662c7e", "fitness": 0.2269251288137765, "name": "ODDS", "description": "The algorithm maintains a central mean m, a global step-size sigma and a per-dimension shape vector A, while learning a compact orthonormal low‑rank basis W online via stabilized Oja updates (rank default ~0.6·sqrt(dim); sigma starts at 0.2·range, A starts at 0.8). Generation sampling is a softmax four‑armed mixture (anisotropic low‑rank + orthogonal residual, coordinate Gaussian, archive-based rotational‑DE differences, and heavy‑tailed Cauchy jumps) with occasional forced random arms (p_random small) and mirrored variance reduction; arms are selected by softmax probabilities and their scores are updated exponentially (arm_lr ≈ 0.18) from observed rewards. Adaptation uses a path‑length style accumulator ps to adapt sigma (chi_n normalization), updates A robustly from medians of selected step magnitudes, and refines W with normalized successful directions plus periodic QR reorthonormalization and decaying Oja learning rate; cheap line probes along the leading Oja vector are attempted with probability p_lineprobe. An archive supports DE-like proposals and reseeding; stagnation triggers sigma inflation, arm-score reset, mean nudging toward archive or random points and mild perturbation of W — all while strictly respecting the evaluation budget and search bounds.", "code": "import numpy as np\n\nclass ODDS:\n    \"\"\"\n    ODDS: Oja-Driven Directional Search\n\n    Main ideas:\n      - Maintain a central mean `m`, a global step `sigma` and elementwise shape `A`.\n      - Learn a low-rank directional basis `W` online with lightweight Oja-style updates\n        from normalized successful steps (no full SVD per generation).\n      - Use a softmax multi-armed selector (four arms) to choose sampling kernels:\n          0) anisotropic low-rank + orthogonal residual,\n          1) coordinate-wise (per-dim) Gaussian,\n          2) rotational DE-like archive differences projected onto basis,\n          3) Lévy/Cauchy heavy-tail global jumps.\n      - Keep an archive of evaluated points for DE-style operations.\n      - Adapt sigma with a path-length style accumulator `ps`, adapt A (per-dim scale)\n        with robust medians. When stagnation occurs, perform sigma inflation and\n        gentle reinitialization of bandit scores and success memory.\n      - Occasionally perform a cheap directional line probe along the leading Oja vector.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 lambda_factor=1.0, rank=None,\n                 subspace_refresh=7,\n                 c_a=0.12, cs=0.3,\n                 oja_eta=0.12, arm_lr=0.18,\n                 p_random=0.03, p_lineprobe=0.12,\n                 seed=None):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: dimensionality\n        lambda_factor: scales population size (lambda = max(6, int(lambda_factor * 2.2*sqrt(dim))))\n        rank: low-rank basis size (if None, set to ~0.6*sqrt(dim))\n        subspace_refresh: how many generations between QR re-orthonormalization of W\n        c_a: per-dimension A smoothing rate\n        cs: smoothing constant for path-length sigma adaptation\n        oja_eta: base learning rate for Oja updates to W\n        arm_lr: soft-update rate for arm scores (exponentially smooth)\n        p_random: probability of a fully random arm selection to avoid lock-in\n        p_lineprobe: probability per generation to attempt a cheap line probe along principal direction\n        seed: RNG seed\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        lam_guess = max(6, int(lambda_factor * max(6, np.floor(2.2 * np.sqrt(self.dim)))))\n        self.lambda_ = lam_guess\n        self.mu = max(1, self.lambda_ // 3)\n\n        if rank is None:\n            self.k = max(1, int(np.ceil(0.6 * np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(rank)), self.dim)\n\n        self.subspace_refresh = max(1, int(subspace_refresh))\n        self.c_a = float(c_a)\n        self.cs = float(cs)\n        self.damps = 1.0 + self.cs + 0.4\n        self.oja_eta = float(oja_eta)\n        self.arm_lr = float(arm_lr)\n        self.p_random = float(p_random)\n        self.p_lineprobe = float(p_lineprobe)\n\n        # DE base factor for rotational-DE\n        self.F_de_base = 0.65\n\n        # stagnation heuristics\n        self.stag_mul = 2.2\n        self.stag_base = max(12, int(2.0 * self.dim / max(1, self.k)))\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds: try to read from func.bounds, else default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initial mean inside bounds\n        m = np.random.uniform(lb, ub)\n        sigma = 0.2 * np.mean(ub - lb)    # global step-size\n        A = np.ones(n) * 0.8              # per-dim shape/scale (like std)\n        ps = np.zeros(n)                  # path accumulator for sigma adaption\n\n        # initialize low-rank orthonormal basis W (n x k)\n        if self.k > 0:\n            R = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(R)\n                W = Q[:, :self.k].copy()\n            except np.linalg.LinAlgError:\n                # fallback\n                W = np.eye(n, self.k)\n        else:\n            W = np.zeros((n, 0))\n\n        # archive (x, f)\n        archive_X = []\n        archive_F = []\n        archive_max = max(500, 40 * self.k)\n\n        # softmax-arm scores (higher -> more likely)\n        n_arms = 4\n        arm_scores = np.zeros(n_arms) + 1e-6  # small positive to avoid NaNs\n\n        # recombination weights\n        ranks = np.arange(self.mu)\n        weights = np.exp(-ranks / max(1.0, (self.mu / 3.0)))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # sigma adaptation constant\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improve_eval = 0\n\n        # evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_opt = fm; x_opt = xm.copy(); last_improve_eval = evals\n\n        generation = 0\n        stagnation_thresh = self.stag_base\n\n        # helper: softmax probabilities with temperature\n        def softmax_probs(scores, temp=0.3):\n            # temp small -> greedy, temp large -> uniform\n            ex = np.exp((scores - np.max(scores)) / (temp + 1e-12))\n            p = ex / np.sum(ex)\n            return p\n\n        # helper: perform small directional line probe (cheap)\n        def line_probe(m_cur, d_dir, step_scale, remaining):\n            # evaluate up to 3 points: -alpha, 0, +alpha along direction (skip 0)\n            # Use alpha = step_scale * sigma\n            n_eval = min(remaining, 2)  # we will evaluate two points: +/- if possible\n            if n_eval <= 0:\n                return None, None, 0\n            alpha = 1.6 * step_scale * sigma\n            points = [m_cur + alpha * d_dir, m_cur - alpha * d_dir]\n            best_f = np.inf; best_x = None; used = 0\n            for i, x in enumerate(points):\n                if used >= n_eval:\n                    break\n                x_cl = np.clip(x, lb, ub)\n                f = func(x_cl)\n                used += 1\n                if f < best_f:\n                    best_f = f; best_x = x_cl.copy()\n            return best_f, best_x, used\n\n        # main loop\n        while evals < budget:\n            generation += 1\n            remaining = budget - evals\n            current_lambda = min(self.lambda_, remaining)\n\n            # occasionally attempt a cheap line probe along the leading Oja vector\n            if (self.k >= 1) and (np.random.rand() < self.p_lineprobe) and (remaining > 2):\n                d0 = W[:, 0]\n                if np.linalg.norm(d0) > 1e-12:\n                    d0 = d0 / (np.linalg.norm(d0) + 1e-12)\n                    lf, lx, used = line_probe(m, d0, np.median(A), remaining)\n                    if used > 0:\n                        evals += used\n                        if lf is not None and lf < f_opt:\n                            f_opt = lf; x_opt = lx.copy(); last_improve_eval = evals\n                        # store probe in archive\n                        if lx is not None:\n                            archive_X.append(lx.copy()); archive_F.append(lf)\n                            if len(archive_X) > archive_max:\n                                archive_X.pop(0); archive_F.pop(0)\n                        # continue to main generation (budget already decreased)\n\n            # prepare candidate storage\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arm_used = np.zeros(current_lambda, dtype=int)\n\n            # precompute temperature adaptive to improvement recency (cooler if improving)\n            temp = 0.25 + 0.75 * np.exp(-0.0008 * (evals - last_improve_eval))\n\n            # sampling loop\n            for i in range(current_lambda):\n                # sample an arm via softmax over arm_scores, sometimes choose random arm\n                if np.random.rand() < self.p_random:\n                    arm = np.random.randint(n_arms)\n                else:\n                    probs = softmax_probs(arm_scores, temp=temp)\n                    arm = np.random.choice(n_arms, p=probs)\n\n                # base gaussian\n                z = np.random.randn(n)\n\n                if arm == 0:\n                    # anisotropic low-rank + orthogonal residual\n                    u = A * z\n                    # project onto low-rank subspace\n                    if self.k > 0:\n                        p = W.T @ u\n                        low = W @ p\n                        ortho = u - low\n                        # combine: emphasize low-rank directions but keep residual\n                        y = 1.0 * low + 0.45 * ortho\n                    else:\n                        y = u\n                elif arm == 1:\n                    # coordinate-wise careful exploitation\n                    y = 0.9 * A * z\n                elif arm == 2:\n                    # rotational DE-like: pick two archive members, form diff, project onto W\n                    if len(archive_X) >= 2:\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        diff = archive_X[i1] - archive_X[i2]\n                        # project diff onto basis and scale with factor F_de_base * randomness\n                        F = self.F_de_base * (0.6 + 0.8 * np.random.rand())\n                        if self.k > 0:\n                            proj = W @ (W.T @ diff)\n                            y = 0.5 * A * z + (F / (sigma + 1e-12)) * proj\n                        else:\n                            y = 0.5 * A * z + (F / (sigma + 1e-12)) * diff\n                    else:\n                        # fallback to coordinate\n                        y = 0.9 * A * z\n                elif arm == 3:\n                    # heavy-tail Cauchy global jump with some anisotropy\n                    r = np.random.standard_cauchy()\n                    # tame extreme tails slightly\n                    r = np.sign(r) * (np.log1p(abs(r)) + 0.15)\n                    rnd_dir = np.random.randn(n)\n                    rnd_dir /= (np.linalg.norm(rnd_dir) + 1e-12)\n                    y = 0.55 * A * z + 1.8 * np.median(A) * r * rnd_dir\n                else:\n                    y = A * z\n\n                # mirrored flip variance reduction\n                if i % 2 == 1:\n                    y = -y\n\n                x = m + sigma * y\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n                arm_used[i] = arm\n\n                # small immediate regularization of arm_scores so older arms don't dominate numeric range\n                arm_scores *= 0.999\n\n            # evaluate candidates sequentially (stop if budget exhausted)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n\n                # archive management\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > archive_max:\n                    archive_X.pop(0); archive_F.pop(0)\n\n                # reward/score assignment (soft update)\n                arm = int(arm_used[i])\n                # improvement over current best yields stronger positive reward\n                if f < f_opt:\n                    reward = (f_opt - f) + 1.0  # encourage large improvements\n                    f_opt = f; x_opt = x.copy(); last_improve_eval = evals\n                else:\n                    # small reward that favors slight improvement vs large degradation\n                    reward = max(1e-6, (f_opt - f) * 1e-4)\n\n                # soft exponential update to arm_scores (increase or decrease)\n                arm_scores[arm] = (1.0 - self.arm_lr) * arm_scores[arm] + self.arm_lr * reward\n\n            # selection and recombination: pick top mu candidates\n            idx = np.argsort(arfit)\n            sel_idx = idx[:self.mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n            f_sel = arfit[sel_idx]\n\n            m_old = m.copy()\n            effective_mu = x_sel.shape[0]\n            if effective_mu >= 1:\n                w = weights[:effective_mu]\n                w = w / np.sum(w)\n                m = np.sum(w[:, None] * x_sel, axis=0)\n                y_w = np.sum(w[:, None] * y_sel, axis=0)\n            else:\n                # unlikely, but take a random small step\n                rnd = np.random.randn(n)\n                y_w = A * rnd\n                m = m + 0.5 * sigma * y_w\n\n            # update Oja low-rank basis using normalized successful directional vectors\n            # consider successes as selected vectors that are no worse than median archive or improved best\n            med_archive = np.median(archive_F[-min(len(archive_F), 30):]) if len(archive_F) > 0 else f_opt\n            # build a small set of candidate normalized vectors to feed Oja\n            oja_updates = []\n            for j, fy in zip(sel_idx, f_sel):\n                yv = arz[j]\n                if np.linalg.norm(yv) < 1e-12:\n                    continue\n                # treat success if improved global best or at least as good as recent median\n                if arfit[j] <= f_opt + 1e-12 or arfit[j] <= med_archive:\n                    u = yv / (np.linalg.norm(yv) + 1e-12)\n                    oja_updates.append(u)\n            # perform Oja update for each u\n            if len(oja_updates) > 0 and self.k > 0:\n                # dynamic eta: scale with sigma and generation to slow learning over time\n                eta = self.oja_eta * (0.9 ** (generation / 50.0))\n                for u in oja_updates:\n                    # per component update (simple stabilized Oja-ish)\n                    for j in range(self.k):\n                        wj = W[:, j]\n                        proj = np.dot(wj, u)\n                        # update rule: wj <- wj + eta * (proj * u - proj**2 * wj)\n                        wj = wj + eta * (proj * u - (proj ** 2) * wj)\n                        # renormalize component\n                        normwj = np.linalg.norm(wj) + 1e-12\n                        W[:, j] = wj / normwj\n                # occasional re-orthonormalize via QR to keep basis stable\n                if (generation % self.subspace_refresh) == 0:\n                    try:\n                        Q, _ = np.linalg.qr(W)\n                        W = Q[:, :self.k].copy()\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # path-length style sigma adaptation (anisotropic-safe)\n            inv_sqrtA = 1.0 / (np.sqrt(A) + 1e-12)\n            ps = (1.0 - self.cs) * ps + np.sqrt(self.cs * (2.0 - self.cs) * mu_eff) * (y_w * inv_sqrtA)\n            norm_ps = np.linalg.norm(ps)\n            sigma *= np.exp((self.cs / self.damps) * (norm_ps / (chi_n + 1e-12) - 1.0))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e3 * np.mean(ub - lb) + 1e-12)\n\n            # update per-dim shape A robustly from selected y's absolute medians\n            if y_sel.shape[0] >= 1:\n                med_abs = np.median(np.abs(y_sel), axis=0)\n                approx_std = 1.4826 * (med_abs + 1e-20)\n                A = (1.0 - self.c_a) * A + self.c_a * (approx_std + 1e-12)\n            else:\n                # tiny jitter\n                A = A * (1.0 + 1e-6 * np.random.randn(n))\n            A = np.clip(A, 1e-12, 1e3)\n\n            # stagnation detection & handling\n            if (evals - last_improve_eval) > stagnation_thresh:\n                # inflate sigma to encourage escapes\n                sigma *= self.stag_mul\n                # reset arm_scores gently to encourage exploration\n                arm_scores = np.zeros_like(arm_scores) + 1e-6\n                # nudge mean to a randomly picked archive point or random point\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                else:\n                    m = np.random.uniform(lb, ub)\n                # slightly increase A to broaden shape\n                A = A * (1.0 + 0.18)\n                # partially clear Oja memory by small random rotation of W\n                if self.k > 0:\n                    perturb = np.random.randn(self.k, self.k) * 0.02\n                    try:\n                        Qp, _ = np.linalg.qr(np.eye(self.k) + perturb)\n                        W = W @ Qp\n                    except np.linalg.LinAlgError:\n                        pass\n                # make stagnation threshold longer next time\n                stagnation_thresh = int(stagnation_thresh * 1.4) + 3\n                last_improve_eval = evals  # avoid repeated triggers immediately\n\n            # keep mean inside bounds\n            m = np.clip(m, lb, ub)\n\n            # safety: if budget exhausted break\n            if evals >= budget:\n                break\n\n        # final evaluation if none found (should not happen)\n        if x_opt is None:\n            xm = np.clip(m, lb, ub)\n            f_final = func(xm)\n            return float(f_final), np.array(xm, dtype=float)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ODDS scored 0.227 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "bb5431ea-d13a-47a4-afb9-8d953d29c548", "operator": null, "metadata": {"aucs": [0.11471352397029111, 0.16300968063257637, 0.3381720022321041, 0.19736388505977387, 0.2433857112857496, 0.26443441136660406, 0.2419463383232655, 0.3037684109004424, 0.24357811370932703, 0.15887921065763067]}, "task_prompt": ""}
{"id": "2f0a6bcd-5018-4ed9-98a7-d0c04612a5af", "fitness": "-inf", "name": "ADPS", "description": "ADPS is a population-based, multi-arm heuristic that samples mirrored candidates from a small adaptive population (lambda ≈ 0.9·1.6√n with mu = lambda/4) and selects the centroid of the best mu as the new mean, using clipping to box bounds. It learns directional structure via an exponentially-weighted low-rank covariance C_dir (cov_beta, refreshed every refresh_subspace gens to form basis B) and adapts per-dimension multiplicative scales D with geometric updates (geom_eta) to capture anisotropy. Exploration is diversified by five arms (axis Gaussian, learned subspace, tempered Cauchy leaps, archive-based DE mutation with adaptive F_de, and block-coordinate bursts) whose selection is governed by a moving-average softmax bandit (arm_value/arm_visits, softmax_temp) with occasional random exploration; mirrored sampling and an archive are used for variance reduction and DE-style proposals. Global controls include a 1/5-like multiplicative sigma controller based on a recent success window, stagnation handling that inflates sigma and perturbs the mean toward archive or random points, and many safeguards (initial sigma = 0.15·range, D=0.6, clipping, bounded archive, and budget-aware evaluation).", "code": "import numpy as np\n\nclass ADPS:\n    \"\"\"\n    ADPS (Adaptive Directional Population Search)\n\n    Main algorithmic differences / main parameters (compared to the ARLSS reference):\n      - lambda_factor: controls population size; default smaller than ARLSS (less aggressive)\n      - k (subspace rank): default 0.5 * sqrt(n) (different constant)\n      - D initial scale: 0.6 (geometric style updates) vs 0.8 in ARLSS\n      - sigma adaptation: simplified 1/5-like success-rate controller (multiplicative adjustments)\n      - directional learner: exponentially-weighted covariance matrix (C_dir) and eigendecomposition\n      - arm selection: softmax sampling (temperature controlled) across 5 arms (adds block-coordinate bursts)\n      - bandit rewards: moving-average rewards (softmax) instead of UCB\n      - archive DE factor F_de adapts from observed DE-arm success rate\n      - success buffer used to seed directional covariance updates with exponential forgetting\n      - mirrored sampling kept but mu recombination uses centroid of top candidates with equal weights\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 lambda_factor=0.9, subspace_rank=None,\n                 refresh_subspace=10, cov_beta=0.12,\n                 geom_eta=0.12, softmax_temp=0.25,\n                 p_random=0.05, stag_multiplier=3.0):\n        \"\"\"\n        budget: number of function evaluations allowed\n        dim: problem dimension\n        seed: RNG seed\n        lambda_factor: multiplier for default population size (smaller than ARLSS default)\n        subspace_rank: rank for low-rank directional basis (if None use heuristic)\n        refresh_subspace: how many generations between recomputing eigenvectors from C_dir\n        cov_beta: learning rate for exponentially-weighted directional covariance (0-1)\n        geom_eta: geometric learning rate for D updates (multiplicative updates)\n        softmax_temp: temperature for softmax bandit (lower -> more greedy)\n        p_random: small probability to ignore bandit and sample fully random\n        stag_multiplier: sigma multiplication factor on stagnation (different from ARLSS 2.5)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size heuristic (different constant than ARLSS)\n        lam_guess = max(6, int(np.ceil(lambda_factor * max(6, 1.6 * np.sqrt(self.dim)))))\n        self.lambda_ = lam_guess\n        # recombination size (we will use equal-weight centroid of top mu)\n        self.mu = max(1, self.lambda_ // 4)\n\n        # subspace rank\n        if subspace_rank is None:\n            self.k = max(1, int(np.ceil(0.5 * np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_rank)), self.dim)\n\n        self.refresh_subspace = int(max(1, refresh_subspace))\n        self.cov_beta = float(cov_beta)\n        self.geom_eta = float(geom_eta)\n        self.softmax_temp = float(softmax_temp)\n        self.p_random = float(p_random)\n        self.stag_multiplier = float(stag_multiplier)\n\n        # arms: 0=axis Gaussian, 1=learned subspace, 2=tempered cauchy leaps,\n        # 3=archive-differential (DE-like), 4=block-coordinate burst\n        self.n_arms = 5\n\n        # archive settings\n        self.archive_max = max(300, 25 * self.k)\n\n        # stagnation threshold baseline (different formula than ARLSS)\n        self.stagnation_base = max(12, int(2.0 * self.dim / max(1, self.k)))\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # read bounds\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # init mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = 0.15 * np.mean(ub - lb)    # different initial sigma\n        D = np.ones(n) * 0.6               # per-dim multiplicative scales (differs from ARLSS)\n        # pathless: we use success-count window for sigma adaptation\n        success_window = []                # store booleans of recent successes\n        success_window_size = max(10, int(2.5 * np.sqrt(n)))\n\n        # directional covariance (exponentially weighted), initially tiny isotropic\n        C_dir = np.eye(n) * 1e-6\n\n        # current learned low-rank basis (n x k)\n        B = np.zeros((n, self.k))\n\n        # archive for DE-like moves\n        archive_X = []\n        archive_F = []\n\n        # softmax bandit stats: moving-average rewards per arm\n        arm_value = np.ones(self.n_arms, dtype=float) * 1e-6\n        arm_visits = np.ones(self.n_arms, dtype=float)  # smoothing denom\n        arm_success_counts = np.zeros(self.n_arms, dtype=float)  # to adapt some arm-specific params\n\n        # adaptive DE mixing factor tracker\n        F_de = 0.6\n        de_success_window = []\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n\n        # evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            if fm < f_opt:\n                f_opt = fm; x_opt = xm.copy(); last_improvement_eval = evals\n\n        generation = 0\n        stagnation_thresh = self.stagnation_base\n\n        # main loop\n        while evals < budget:\n            generation += 1\n            remaining = budget - evals\n            current_lambda = min(self.lambda_, remaining)\n            # ensure even for mirrored sampling we produce an even number when possible\n            if current_lambda > 1 and current_lambda % 2 == 1:\n                current_lambda -= 1\n            if current_lambda <= 0:\n                break\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arm_chosen = np.zeros(current_lambda, dtype=int)\n\n            # compute softmax probabilities\n            vals = arm_value / (arm_visits + 1e-12)\n            # numerical stability\n            logits = vals / (self.softmax_temp + 1e-12)\n            maxl = logits.max()\n            exps = np.exp(logits - maxl)\n            probs = exps / (np.sum(exps) + 1e-12)\n\n            # occasionally reset to encourage exploration\n            if np.random.rand() < 0.02:\n                probs = np.ones(self.n_arms) / self.n_arms\n\n            # generate candidates\n            for i in range(current_lambda):\n                # choose an arm (softmax sample), but small prob pick random global\n                if np.random.rand() < self.p_random:\n                    arm = np.random.randint(self.n_arms)\n                else:\n                    arm = np.random.choice(self.n_arms, p=probs)\n\n                arm_chosen[i] = arm\n                z = np.random.randn(n)\n\n                if arm == 0:\n                    # axis-wise Gaussian exploitation (small)\n                    y = 0.7 * D * z\n                elif arm == 1 and np.any(B):\n                    # learned subspace exploration:\n                    coeff = np.random.randn(self.k)\n                    low = B @ coeff\n                    low_norm = np.linalg.norm(low) + 1e-12\n                    # sample gamma-like length to differ from ARLSS\n                    length = (np.random.gamma(shape=2.0, scale=0.8))\n                    y = 0.4 * D * z + (0.9 * np.median(D) * length) * (low / low_norm)\n                elif arm == 2:\n                    # tempered Cauchy leaps with damping (different scaling)\n                    r = np.random.standard_cauchy(size=1)[0]\n                    r = np.tanh(r) * 3.0  # temper extremes while keeping heavy tails\n                    rnd_dir = np.random.randn(n); rnd_dir /= (np.linalg.norm(rnd_dir) + 1e-12)\n                    y = 0.5 * D * z + (1.5 * np.median(D) * r) * rnd_dir\n                elif arm == 3 and len(archive_X) >= 3:\n                    # adaptive DE-style mutation (archive based): x + F*(x_a - x_b)\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    # adaptive F_de is damped and randomized\n                    F_use = max(0.1, min(1.2, F_de * (0.8 + 0.6 * np.random.rand())))\n                    de_mut = F_use * (archive_X[i1] - archive_X[i2])\n                    # express as y-step relative to sigma\n                    y = 0.5 * D * z + (de_mut / max(1e-12, sigma))\n                else:\n                    # arm 4: block-coordinate burst (pick random block and perturb strongly)\n                    block_size = max(1, int(max(1, 0.12 * n) * (1 + np.random.randint(0, 3))))\n                    idx_block = np.random.choice(n, size=block_size, replace=False)\n                    y = 0.3 * D * z\n                    burst = np.zeros(n)\n                    burst[idx_block] = np.random.randn(block_size) * 3.0 * np.median(D)\n                    y = y + burst\n\n                # mirrored sampling for variance reduction\n                if (i % 2) == 1:\n                    y = -y\n\n                x = m + sigma * y\n                x = np.clip(x, lb, ub)\n                arx[i] = x\n                arz[i] = y\n\n                # update quick bandit visit counts (encourage exploration initially)\n                arm_visits[arm] += 1.0\n\n            # Evaluate candidates sequentially (do not exceed budget)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n\n                # add to archive\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_max:\n                    archive_X.pop(0); archive_F.pop(0)\n\n                # compute reward for the arm that generated this candidate:\n                # reward is improvement over batch mean (encourages above-average candidates)\n                # we use running mean of evaluated in this batch so far\n                batch_mean = np.nanmean(arfit[:i+1])\n                if np.isfinite(batch_mean):\n                    reward = max(0.0, batch_mean - f)\n                else:\n                    reward = 0.0\n\n                arm = int(arm_chosen[i])\n                # moving-average update of arm_value\n                w = 0.12  # short memory for recent observations\n                arm_value[arm] = (1.0 - w) * arm_value[arm] + w * (reward + 1e-8)\n                if reward > 0:\n                    arm_success_counts[arm] += 1.0\n                    # track for adapting F_de\n                    if arm == 3:\n                        de_success_window.append(1)\n                    else:\n                        de_success_window.append(0)\n                else:\n                    de_success_window.append(0)\n\n                # keep small de_success_window\n                if len(de_success_window) > 50:\n                    de_success_window = de_success_window[-50:]\n\n                # update best-so-far\n                if f < f_opt:\n                    f_opt = f; x_opt = x.copy(); last_improvement_eval = evals\n\n            # selection: choose top mu by fitness (lower is better)\n            idx = np.argsort(arfit)\n            sel_idx = idx[:self.mu]\n            # some of the arfit may be inf if budget exhausted; filter\n            sel_idx = sel_idx[np.isfinite(arfit[sel_idx])]\n            x_sel = arx[sel_idx] if sel_idx.size > 0 else np.zeros((0, n))\n            y_sel = arz[sel_idx] if sel_idx.size > 0 else np.zeros((0, n))\n\n            # recombine: centroid (equal weights)\n            if x_sel.shape[0] >= 1:\n                m_old = m.copy()\n                m = np.mean(x_sel, axis=0)\n                # mean step in normalized y-space for updates\n                y_mean = np.mean(y_sel, axis=0)\n            else:\n                # fallback random jitter\n                y_mean = D * np.random.randn(n)\n                m = m + 0.3 * sigma * y_mean\n                m = np.clip(m, lb, ub)\n\n            # Update directional covariance C_dir using normalized y_mean (unit vector)\n            ny = np.linalg.norm(y_mean)\n            if ny > 1e-20:\n                v = y_mean / ny\n                # exponentially-weighted outer product\n                C_dir = (1.0 - self.cov_beta) * C_dir + self.cov_beta * np.outer(v, v)\n\n            # occasionally recompute B via eigendecomposition of C_dir\n            if (generation % self.refresh_subspace) == 0:\n                try:\n                    # C_dir is symmetric; get top-k eigenvectors\n                    vals_eig, vecs = np.linalg.eigh(C_dir)\n                    # pick largest eigenvalues\n                    idxs = np.argsort(vals_eig)[-self.k:]\n                    B = vecs[:, idxs]\n                    # ensure B shape (n,k)\n                    if B.ndim == 1:\n                        B = B[:, None]\n                except np.linalg.LinAlgError:\n                    # ignore and keep previous B\n                    pass\n\n            # per-dim geometric update of D using median abs selected y\n            if y_sel.shape[0] >= 1:\n                med_abs = np.median(np.abs(y_sel), axis=0) + 1e-12\n                # multiplicative (geometric) smoothing: D <- D * exp(eta*(log(med_abs) - log(D)))\n                log_ratio = np.log(med_abs + 1e-12) - np.log(D + 1e-12)\n                D = D * np.exp(self.geom_eta * log_ratio)\n            else:\n                # small jitter\n                D = D * (1.0 + 1e-6 * np.random.randn(n))\n\n            # clip D to reasonable bounds\n            D = np.clip(D, 1e-12, 1e3)\n\n            # sigma adaptation based on 1/5-like rule using recent success window\n            # Define a \"success\" as a selected candidate improving the batch median\n            if y_sel.shape[0] >= 1:\n                selected_fits = np.array([arfit[j] for j in sel_idx])\n                batch_med = np.median(arfit[np.isfinite(arfit)])\n                # success if selected median < batch median\n                succ = float(np.median(selected_fits) < batch_med)\n                success_window.append(succ)\n                if len(success_window) > success_window_size:\n                    success_window = success_window[-success_window_size:]\n                # compute success rate\n                sr = np.mean(success_window) if len(success_window) > 0 else 0.0\n                target = 0.2\n                # multiplicative update; factor depends on deviation from target\n                if sr > target:\n                    sigma *= (1.0 + 0.2 * (sr - target))\n                else:\n                    sigma /= (1.0 + 0.2 * (target - sr))\n                # clamp sigma\n                sigma = max(sigma, 1e-12)\n                sigma = min(sigma, 1e3 * np.mean(ub - lb) + 1e-12)\n\n            # adapt F_de from recent DE-arm success frequency\n            if len(de_success_window) > 5:\n                recent_de_success = np.mean(de_success_window[-min(len(de_success_window), 40):])\n                # increase F_de slightly if DE arm succeeds often, else reduce\n                F_de = 0.5 + 0.8 * recent_de_success\n                F_de = np.clip(F_de, 0.1, 1.2)\n\n            # update arm selection probabilities: (softmax uses arm_value / arm_visits computed earlier)\n            # we apply slight decay to arm_value to adapt to nonstationarity\n            arm_value *= 0.995\n\n            # stagnation detection and handling\n            if (evals - last_improvement_eval) > stagnation_thresh:\n                # inflate sigma strongly but not excessively (different multiplier)\n                sigma *= self.stag_multiplier\n                # randomly perturb mean toward an archive member or random point\n                if len(archive_X) > 0 and np.random.rand() < 0.8:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                else:\n                    m = np.random.uniform(lb, ub)\n                # reset some bandit stats to encourage exploration\n                arm_visits = np.ones(self.n_arms, dtype=float)\n                arm_value = np.ones(self.n_arms, dtype=float) * 1e-6\n                # slightly broaden D\n                D = D * (1.0 + 0.35)\n                # clear some directional memory softly\n                C_dir *= 0.6\n                # increase stagnation threshold a bit\n                stagnation_thresh = int(stagnation_thresh * 1.4) + 3\n\n            # keep mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # safety: do not exceed budget\n            if evals >= budget:\n                break\n\n        # final evaluation if nothing found (shouldn't happen), otherwise return best\n        if x_opt is None:\n            xm = np.clip(m, lb, ub)\n            f_final = func(xm)\n            return float(f_final), np.array(xm, dtype=float)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "error": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "parent_ids": "bb5431ea-d13a-47a4-afb9-8d953d29c548", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "19069010-bcf3-426d-8e2f-521028eb1f40", "fitness": 0.29751265810732797, "name": "RAMP", "description": "RAMP centers search on a population mean m with a global step-size sigma and adaptive per-dimension scales D, augmented by a momentum vector to exploit persistent trends and mirrored pair sampling to reduce variance. It uses a small operator pool (isotropic, axis-wise, low‑rank subspace, Lévy, DE-from-archive, momentum-guided) with stochastic operator selection driven by a reward EMA and count-based noise (plus a small p_random) to balance exploration/exploitation. Successful normalized steps are accumulated in an exponential-decay success_cov and periodically eigendecomposed to learn a low-rank subspace (k ≈ 0.5√dim) for biased low-rank exploration, while sigma is adapted multiplicatively from batch success rate (σ_eta, target_success) and D is updated from median absolute deviations of selected y’s. An archive supports DE-like mutations and stagnation is handled by sigma inflation, operator reinitialization, nudging m toward archive points, and gentle restarts to recover from local traps.", "code": "import numpy as np\n\nclass RAMP:\n    \"\"\"\n    RAMP - Rotating Adaptive Momentum Population\n\n    Main ideas:\n      - Maintain a population-centric mean `m`, global step `sigma` and per-dimension scales `D`.\n      - Keep an archive for DE-like variation and a small exponential-decay covariance matrix\n        of successful normalized steps to learn a low-rank subspace (via eigendecomposition).\n      - Use a small operator pool (isotropic Gaussian, axis-wise, low-rank, Lévy, DE, momentum)\n        and adapt operator preferences with a stochastic, reward-ema guided selector.\n      - Update `sigma` by comparing batch success rate to a target (smoothed 1/5th-style rule).\n      - Maintain a momentum vector that biases search directions (helps exploit consistent trends).\n      - Handle stagnation with mild restarts, sigma inflation, and operator re-initialization.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 lambda_factor=1.0, subspace_k=None,\n                 subspace_refresh=8, cov_gamma=0.12,\n                 c_d=0.10, sigma_eta=1.8, target_success=0.2,\n                 p_random=0.03):\n        \"\"\"\n        budget: number of function evaluations allowed\n        dim: problem dimension\n        seed: RNG seed\n        lambda_factor: scales population size (lambda = max(6, int(lambda_factor * 2.2*sqrt(dim))))\n        subspace_k: rank of learned subspace (defaults to ~0.5*sqrt(dim))\n        subspace_refresh: generations between eigendecomps of success covariance\n        cov_gamma: decay rate for exponential covariance accumulation (0..1)\n        c_d: per-dimension scale learning rate (0..1)\n        sigma_eta: learning rate used when adapting sigma from success_rate\n        target_success: desired fraction of improving offspring (for sigma adaptation)\n        p_random: small probability to pick a random operator (encourage exploration)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size heuristics\n        lam_guess = max(6, int(max(6, np.floor(2.2 * np.sqrt(self.dim))) * lambda_factor))\n        self.lambda_ = lam_guess\n        self.mu = max(1, self.lambda_ // 3)\n\n        # subspace rank\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(0.5 * np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n        self.subspace_refresh = max(1, int(subspace_refresh))\n\n        # adaptation params\n        self.cov_gamma = float(cov_gamma)\n        self.c_d = float(c_d)\n        self.sigma_eta = float(sigma_eta)\n        self.target_success = float(target_success)\n        self.p_random = float(p_random)\n\n        # operator pool (six operators)\n        # indices: 0=isotropic, 1=axis-wise, 2=low-rank-subspace, 3=levy, 4=DE-archive, 5=momentum-guided\n        self.n_ops = 6\n        self.F_de_base = 0.7\n\n        # stagnation controls\n        self.stag_sigma_mult = 2.2\n        self.stagnation_base = max(12, int(2.5 * self.dim / max(1, self.k)))\n\n        # internal file-level RNG can be seeded\n        self._rng = np.random.RandomState(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # read bounds (Many Affine BBOB: assumed -5..5 if not present)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initialize mean randomly in bounds\n        m = self._rng.uniform(lb, ub)\n        sigma = 0.18 * np.mean(ub - lb)   # initial global step (slightly different constant from ARLSS)\n        D = np.ones(n) * 0.9              # per-dim scale (y-space scales)\n        momentum = np.zeros(n)            # momentum vector (y-space)\n        op_counts = np.ones(self.n_ops, dtype=float)\n        op_reward_ema = np.zeros(self.n_ops, dtype=float) + 1e-8  # small positive baseline\n\n        # learned-subspace via exponential covariance of normalized successes\n        success_cov = np.zeros((n, n), dtype=float)\n        B = np.zeros((n, self.k), dtype=float)  # eigenvectors\n        eigvals = np.zeros(self.k, dtype=float)\n\n        # archive for DE-like mutations: store (x,f)\n        archive_X = []\n        archive_F = []\n        archive_max = max(500, 40 * self.k)\n\n        # recombination weights similar to mu-best but simpler here\n        ranks = np.arange(self.mu)\n        weights = np.exp(-ranks / max(1.0, (self.mu / 3.0)))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement_eval = 0\n        generation = 0\n        stagnation_thresh = self.stagnation_base\n\n        # Evaluate initial mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_opt = float(fm); x_opt = xm.copy(); last_improvement_eval = evals\n\n        # helper: operator selection (stochastic with noise proportional to 1/sqrt(count))\n        def select_operator():\n            if self._rng.rand() < self.p_random:\n                return int(self._rng.randint(self.n_ops))\n            noise = self._rng.normal(scale=1.0, size=self.n_ops) * (1.0 / (np.sqrt(op_counts + 1e-12)))\n            scores = op_reward_ema + noise\n            # break ties randomly\n            bests = np.flatnonzero(np.isclose(scores, scores.max()))\n            return int(self._rng.choice(bests))\n\n        # main loop\n        while evals < budget:\n            generation += 1\n            remaining = budget - evals\n            current_lambda = min(self.lambda_, remaining)\n            # ensure even for mirrored pairs: if odd, last one not mirrored\n            mirrored = True\n            if current_lambda % 2 == 1:\n                mirrored = False\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))  # y = (x - m) / sigma\n            ar_op = np.zeros(current_lambda, dtype=int)\n\n            # pre-generate some randomness\n            medD = np.median(D)\n            for i in range(current_lambda):\n                op = select_operator()\n                ar_op[i] = op\n                op_counts[op] += 1.0  # immediate increment\n\n                # base gaussian vector\n                z = self._rng.randn(n)\n\n                if op == 0:\n                    # isotropic gaussian scaled by sigma and average D\n                    y = z * (0.9 * D * (0.6 + 0.8 * self._rng.rand()))\n                elif op == 1:\n                    # axis-wise: amplify along selected coordinates, damp others\n                    mask = (self._rng.rand(n) < 0.35).astype(float)  # 35% axes amplified\n                    axis_amp = 1.2 * mask + 0.6 * (1.0 - mask)\n                    y = z * D * axis_amp\n                elif op == 2 and np.linalg.norm(B) > 0:\n                    # low-rank exploration within learned subspace + small axis noise\n                    coeff = self._rng.randn(self.k) * (np.sqrt(eigvals + 1e-12) + 1e-6)\n                    low = B @ coeff\n                    # mix with axis noise\n                    y = 0.35 * D * z + 1.05 * medD * low / (np.linalg.norm(low) + 1e-12)\n                elif op == 3:\n                    # Lévy-like heavy-tailed random jump (Cauchy) on a random direction\n                    r = self._rng.standard_cauchy()\n                    rnd_dir = self._rng.randn(n)\n                    rnd_dir /= (np.linalg.norm(rnd_dir) + 1e-12)\n                    y = 0.5 * D * z + (1.5 * medD * r) * rnd_dir\n                elif op == 4 and len(archive_X) >= 2:\n                    # DE-like from archive but normalized to y-space (difference divided by sigma)\n                    i1, i2 = self._rng.choice(len(archive_X), size=2, replace=False)\n                    F_de = self.F_de_base * (0.6 + 0.8 * self._rng.rand())\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2]) / (sigma + 1e-12)\n                    y = 0.2 * D * z + de_mut\n                elif op == 5:\n                    # momentum-guided perturbation: follow trend with exploration\n                    y = 0.6 * momentum + 0.6 * D * z * (0.5 + self._rng.rand())\n                else:\n                    # fallback to isotropic\n                    y = z * D\n\n                # mirrored pairing to reduce sampling variance\n                if mirrored and (i % 2 == 1):\n                    y = -y\n\n                x = m + sigma * y\n                x = np.clip(x, lb, ub)\n                arx[i] = x\n                arz[i] = y\n\n            # Evaluate candidates sequentially (stop if budget exhausted)\n            arfit = np.full(current_lambda, np.inf)\n            improved_flags = np.zeros(current_lambda, dtype=bool)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = float(f)\n\n                # update archive\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > archive_max:\n                    archive_X.pop(0); archive_F.pop(0)\n\n                # reward assignment: improvement over current best or over mean m evaluation proxy\n                if f < f_opt:\n                    impro = max(0.0, f_opt - f)\n                    f_opt = float(f); x_opt = x.copy(); last_improvement_eval = evals\n                    improved_flags[i] = True\n                    # give larger reward to operator that created it\n                    op = ar_op[i]\n                    # exponential moving average on rewards (additive)\n                    op_reward_ema[op] = 0.9 * op_reward_ema[op] + 0.1 * (impro + 1.0)\n                else:\n                    # small negative/zero improvement reward (encourage exploration about near-best)\n                    op = ar_op[i]\n                    # base tiny reward depending on closeness to best\n                    closeness = max(0.0, (f_opt - f) * 1e-4)\n                    op_reward_ema[op] = 0.95 * op_reward_ema[op] + 0.05 * (closeness + 1e-6)\n\n            # Selection: pick top-mu candidates\n            idx = np.argsort(arfit)\n            sel_idx = idx[:self.mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n            f_sel = arfit[sel_idx]\n\n            # combine selected to update mean using recombination weights\n            effective_mu = x_sel.shape[0]\n            if effective_mu >= 1:\n                w = weights[:effective_mu]\n                w = w / np.sum(w)\n                m_old = m.copy()\n                m = np.sum(w[:, None] * x_sel, axis=0)\n                # weighted y in y-space\n                y_w = np.sum(w[:, None] * y_sel, axis=0)\n            else:\n                # fallback random step\n                y_w = D * self._rng.randn(n)\n                m_old = m.copy()\n                m = m + 0.5 * sigma * y_w\n\n            # update momentum (smooth trend in y-space)\n            beta_m = 0.6\n            momentum = beta_m * momentum + (1.0 - beta_m) * y_w\n\n            # compute generation success rate (improvements in this batch)\n            success_count = np.sum(improved_flags)\n            success_rate = float(success_count) / max(1.0, float(current_lambda))\n\n            # adapt sigma by comparing success_rate to target (multiplicative)\n            adapt_factor = np.exp(self.sigma_eta * (success_rate - self.target_success) / max(1.0, np.sqrt(n)))\n            sigma *= adapt_factor\n            # keep sigma within safe bounds relative to search space\n            sigma = np.clip(sigma, 1e-12, 2.0 * np.mean(ub - lb) * 1e2)\n\n            # per-dimension scale update (robustly from selected y's)\n            if y_sel.shape[0] >= 1:\n                med_abs = np.median(np.abs(y_sel), axis=0)\n                approx_std = 1.4826 * (med_abs + 1e-20)\n                D = (1.0 - self.c_d) * D + self.c_d * (approx_std + 1e-12)\n            else:\n                D = D * (1.0 + 1e-6 * (self._rng.randn(n)))\n\n            D = np.clip(D, 1e-12, 1e3)\n\n            # update success covariance (exponential-decay) with normalized successful directions (y normalized)\n            # use all selected vectors that improved or the best half of selected as pseudo-successes\n            # pick those selected with fitness better than median of selected\n            try:\n                sel_med = np.median(f_sel)\n            except Exception:\n                sel_med = np.inf\n            success_vectors = []\n            for j, ys in enumerate(y_sel):\n                if (f_sel[j] <= sel_med) or improved_flags[sel_idx[j]]:\n                    # normalize by norm\n                    nv = ys.copy()\n                    norm_nv = np.linalg.norm(nv) + 1e-12\n                    nv = nv / norm_nv\n                    success_vectors.append(nv)\n            if len(success_vectors) > 0:\n                avg_outer = np.zeros((n, n), dtype=float)\n                for v in success_vectors:\n                    avg_outer += np.outer(v, v)\n                avg_outer /= float(len(success_vectors))\n                success_cov = (1.0 - self.cov_gamma) * success_cov + self.cov_gamma * avg_outer\n\n            # recompute low-rank eigenvectors occasionally\n            if (generation % self.subspace_refresh) == 0:\n                # ensure symmetry\n                sym = 0.5 * (success_cov + success_cov.T)\n                # small regularizer for numerical stability\n                sym += np.eye(n) * 1e-14\n                try:\n                    # compute top-k eigenpairs via full eig (n is moderate in many benchmarking tasks)\n                    vals, vecs = np.linalg.eigh(sym)\n                    # sort descending\n                    order = np.argsort(vals)[::-1]\n                    vals = vals[order]\n                    vecs = vecs[:, order]\n                    # keep top self.k (if non-zero eigenvalues)\n                    take_k = min(self.k, vecs.shape[1])\n                    if take_k > 0:\n                        B[:, :take_k] = vecs[:, :take_k]\n                        eigvals[:take_k] = np.maximum(vals[:take_k], 0.0)\n                    else:\n                        B = np.zeros((n, self.k))\n                        eigvals[:] = 0.0\n                except np.linalg.LinAlgError:\n                    # ignore and keep old B\n                    pass\n\n            # stagnation handling\n            if (evals - last_improvement_eval) > stagnation_thresh:\n                sigma *= self.stag_sigma_mult\n                # encourage exploration by resetting op_rewards mildly\n                op_counts = np.ones(self.n_ops, dtype=float)\n                op_reward_ema = np.zeros(self.n_ops, dtype=float) + 1e-6\n                # nudge mean towards a randomly selected archive element (or random point)\n                if len(archive_X) > 0:\n                    pick = self._rng.randint(len(archive_X))\n                    m = 0.4 * m + 0.6 * archive_X[pick]\n                else:\n                    m = self._rng.uniform(lb, ub)\n                # clear momentum and enlarge D slightly\n                momentum = np.zeros(n)\n                D = D * (1.0 + 0.22)\n                # relax stagnation threshold to avoid oscillation\n                stagnation_thresh = int(stagnation_thresh * 1.4) + 3\n\n            # enforce m within bounds\n            m = np.clip(m, lb, ub)\n\n            # safety check stop\n            if evals >= budget:\n                break\n\n        # final answer - if never improved, return current mean (if budget still allows evaluate)\n        if x_opt is None:\n            xm = np.clip(m, lb, ub)\n            if evals < budget:\n                f_final = func(xm)\n                return float(f_final), np.array(xm, dtype=float)\n            else:\n                # we cannot call func; return mean without evaluation (best-known f_opt may be inf)\n                return float(np.inf), np.array(xm, dtype=float)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm RAMP scored 0.298 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "bb5431ea-d13a-47a4-afb9-8d953d29c548", "operator": null, "metadata": {"aucs": [0.16187809833397115, 0.16512579792840743, 0.6216932611730001, 0.22066773541115037, 0.23662405079214255, 0.7233870302901169, 0.21941015095068817, 0.25002287864447514, 0.23349701149343238, 0.14282056605589533]}, "task_prompt": ""}
{"id": "955e38ed-70f5-4ddf-8aa9-0db726109f71", "fitness": "-inf", "name": "HySCaDE", "description": "HySCaDE centers search on a mean vector m with a global step-size sigma (initialized small at 0.1*range and strongly inflatable on stagnation) and per-dimension scales S that are updated via median-absolute smoothing, while recombination uses a weighted average of the top mu candidates. Candidate generation is controlled by a 4-arm softmax bandit (temperature decays from temp0=0.9 and p_learn≈0.6 biases learning) that selects between axis-wise Gaussians, learned low-rank subspace moves (PCA from a success buffer sized ~4*k), heavy-tailed Student-t jumps, and archive-based DE-style mutations (archive sized large: max(300,30*k)). Variance reduction is achieved with mirrored sampling, PCA basis B is recomputed periodically (subspace_refresh), and sigma adapts with a smoothed success-rate rule targeting ~20% success; S is constrained and smoothed to avoid degeneration. Robustness measures include a sizable archive for DE proposals, rank heuristics k≈0.4*sqrt(dim), small baseline rewards to keep bandit exploration alive, and stagnation maneuvers that inflate sigma, nudge the mean toward archive solutions or random restarts, and softly reset bandit skew.", "code": "import numpy as np\n\nclass HySCaDE:\n    \"\"\"\n    HySCaDE — Hybrid Subspace-Cauchy Differential Evolution\n\n    Key ideas:\n      - Maintain a central mean `m`, a global step `sigma`, and per-dimension scales `S`.\n      - Use a 4-arm softmax-bandit (not UCB) to select generation modes:\n         0: axis-wise Gaussian exploitation,\n         1: learned low-rank subspace exploration (PCA of recent successful steps),\n         2: heavy-tailed jumps (Student-t / Cauchy-like),\n         3: archive-based differential mutation (DE-style).\n      - Use mirrored sampling for variance reduction.\n      - Adapt sigma via a smoothed recent-success-rate rule (1/5-like but continuous).\n      - Update per-dim scales S via median absolute selected steps with exponential smoothing.\n      - Recompute low-rank basis periodically from a small success buffer.\n      - On stagnation perform restart maneuvers: inflate sigma, nudge mean, and random injections.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 lambda_factor=1.0, subspace_rank=None,\n                 subspace_refresh=4, c_smooth=0.18, c_scale=0.20,\n                 p_learn=0.6, temp0=0.9):\n        \"\"\"\n        budget: total function evaluations available\n        dim: problem dimension\n        seed: RNG seed\n        lambda_factor: multiplier for population size (lambda)\n        subspace_rank: optional rank for the learned subspace (if None, chosen heuristically)\n        subspace_refresh: how many generations between recomputing PCA basis\n        c_smooth: smoothing constant used in sigma adaptation (controls responsiveness)\n        c_scale: smoothing for per-dim scale S\n        p_learn: fraction of candidates guided by learned mechanisms (vs pure random)\n        temp0: initial softmax temperature for bandit selection\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size heuristics (different from ARLSS)\n        lam_guess = max(8, int(lambda_factor * np.floor(1.8 * np.sqrt(self.dim))))\n        self.lambda_ = lam_guess\n        self.mu = max(1, self.lambda_ // 3)\n\n        # subspace rank (different heuristic)\n        if subspace_rank is None:\n            self.k = max(1, int(0.4 * np.sqrt(self.dim)))\n        else:\n            self.k = min(max(1, int(subspace_rank)), self.dim)\n\n        self.subspace_refresh = max(1, int(subspace_refresh))\n        self.c_smooth = float(c_smooth)\n        self.c_scale = float(c_scale)\n        self.p_learn = float(p_learn)\n        self.temp0 = float(temp0)\n\n        # success buffer size for PCA (different scaling)\n        self.success_buf_size = max(6, 4 * self.k)\n\n        # archive for DE-like moves\n        self.archive_max = max(300, 30 * self.k)\n\n        # stagnation control\n        self.stag_inflation = 3.0\n        self.stag_base = max(8, int(2.0 * self.dim / max(1, self.k)))\n\n        # DE base factor (slightly different)\n        self.F_base = 0.65\n\n        # arms\n        self.n_arms = 4\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # read bounds from func if available; default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initialize mean in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = 0.1 * np.mean(ub - lb)  # different initial sigma (smaller)\n        S = np.ones(n) * 0.5            # per-dim scale initialized differently\n        success_buf = np.zeros((0, n))  # FIFO buffer of normalized successful direction vectors\n\n        # initial PCA basis (random orthonormal if k>0)\n        if self.k > 0:\n            R = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(R)\n                B = Q[:, :self.k].copy()\n            except np.linalg.LinAlgError:\n                B = np.zeros((n, self.k))\n        else:\n            B = np.zeros((n, 0))\n\n        # archive\n        archive_X = []\n        archive_F = []\n\n        # bandit softmax statistics (we store cumulative reward and counts)\n        arm_rewards = np.ones(self.n_arms) * 1e-6  # small positive to avoid zero-division\n        arm_counts = np.ones(self.n_arms) * 1e-6\n\n        # sigma adaptation using recent success-rate window\n        success_window = []\n        success_window_len = max(10, min(50, 5 * self.mu))\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improve_eval = 0\n        generation = 0\n        stagnation_thresh = int(self.stag_base)\n\n        # evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_opt = fm; x_opt = xm.copy(); last_improve_eval = evals\n\n        # main loop\n        while evals < budget:\n            generation += 1\n            remaining = budget - evals\n            current_lambda = min(self.lambda_, remaining)\n\n            # storage\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arm_chosen = np.zeros(current_lambda, dtype=int)\n\n            # compute softmax probabilities for arms\n            avg_quality = arm_rewards / (arm_counts + 1e-12)\n            # temperature decays slowly with progress\n            temp = max(0.15, self.temp0 * (1.0 - (evals / float(max(1, budget))) * 0.9))\n            # stabilize avg_quality (shift)\n            q = avg_quality - np.max(avg_quality)\n            expq = np.exp(q / (temp + 1e-12))\n            probs = expq / (np.sum(expq) + 1e-12)\n\n            # generate candidates (with mirrored sampling)\n            for i in range(current_lambda):\n                # choose arm by sampling softmax probabilities; small chance to pick random arm\n                if np.random.rand() < 0.04:\n                    arm = np.random.randint(self.n_arms)\n                else:\n                    arm = np.random.choice(self.n_arms, p=probs)\n\n                # small chance to ignore bandit and do pure random\n                if np.random.rand() > self.p_learn:\n                    arm = np.random.randint(self.n_arms)\n\n                z = np.random.randn(n)\n\n                if arm == 0:\n                    # axis-wise precise Gaussian\n                    y = 0.7 * (S * z)\n                elif arm == 1 and self.k > 0 and success_buf.shape[0] >= 2:\n                    # learned low-rank subspace exploration + small axis noise\n                    coeff = np.random.randn(self.k)\n                    low = B @ coeff\n                    low_norm = np.linalg.norm(low) + 1e-12\n                    sub_scale = 1.05 * np.median(S) * (0.7 + 0.6 * np.random.rand())\n                    y = 0.25 * (S * z) + sub_scale * (low / low_norm)\n                elif arm == 2:\n                    # heavy-tailed jumps: Student-t with low df for heavy tails\n                    df = 1.7\n                    r = np.random.standard_t(df=size=1)[0]\n                    rnd_dir = np.random.randn(n)\n                    rnd_dir /= (np.linalg.norm(rnd_dir) + 1e-12)\n                    y = 0.45 * (S * z) + 1.7 * np.median(S) * r * rnd_dir\n                elif arm == 3 and len(archive_X) >= 3:\n                    # DE-style archive differential mutation: rand/1-like using three distinct archive members\n                    i1, i2, i3 = np.random.choice(len(archive_X), size=3, replace=False)\n                    x1 = np.asarray(archive_X[i1])\n                    x2 = np.asarray(archive_X[i2])\n                    x3 = np.asarray(archive_X[i3])\n                    F = self.F_base * (0.6 + 0.8 * np.random.rand())\n                    de_candidate = x1 + F * (x2 - x3)\n                    # transform to y-space and add small Gaussian jitter\n                    y = (de_candidate - m) / (sigma + 1e-12) + 0.35 * (S * z)\n                else:\n                    # fallback: axis gaussian\n                    y = S * z * 0.9\n\n                # mirrored variance reduction: mirror every odd index to previous\n                if i % 2 == 1:\n                    y = -arz[i - 1]  # mirror previous normalized step\n                    arm = arm_chosen[i - 1]  # mirror arm choice too\n\n                x = m + sigma * y\n                # clip into bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i, :] = x\n                arz[i, :] = y\n                arm_chosen[i] = int(arm)\n\n                # increment counts for bandit (we charge counts when used)\n                arm_counts[arm] += 1.0\n\n            # Evaluate candidates sequentially (not exceeding budget)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n\n                # add to archive (bounded)\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_max:\n                    archive_X.pop(0); archive_F.pop(0)\n\n                # handle reward for the arm that generated this candidate\n                arm = int(arm_chosen[i])\n                # compute improvement over global best\n                if f < f_opt:\n                    improvement = f_opt - f\n                    # larger improvements give larger reward\n                    reward = improvement + 0.5\n                    f_opt = f; x_opt = x.copy(); last_improve_eval = evals\n                    # mark this candidate as success for the recent window\n                    success_window.append(1)\n                else:\n                    # non-improving but possibly better than current mean: small reward baseline\n                    reward = 0.001 + max(0.0, (np.median(archive_F[-min(len(archive_F), 10):]) - f)) * 1e-3\n                    success_window.append(0)\n\n                arm_rewards[arm] += reward\n\n                # keep success window bounded\n                if len(success_window) > success_window_len:\n                    success_window.pop(0)\n\n            # selection: choose best mu for recombination\n            idx = np.argsort(arfit)\n            sel_idx = idx[:self.mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n            f_sel = arfit[sel_idx]\n\n            # recombine: weighted average biased to top-ranked (different weight formula)\n            eff_mu = x_sel.shape[0]\n            if eff_mu >= 1:\n                ranks = np.arange(eff_mu)\n                w = np.exp(-ranks / max(1.0, (eff_mu / 4.0)))\n                w = w / (np.sum(w) + 1e-12)\n                m_old = m.copy()\n                m = np.sum(w[:, None] * x_sel, axis=0)\n                y_w = np.sum(w[:, None] * y_sel, axis=0)\n            else:\n                y_w = S * np.random.randn(n)\n                m = m + 0.6 * sigma * y_w\n\n            # update success buffer: push normalized y_w if selection had at least one success or median of selected is good\n            norm_yw = np.linalg.norm(y_w) + 1e-20\n            v = y_w / norm_yw\n            # criterion: if any selected is better than median of recent archive or any selected improved global best\n            if (np.any(f_sel <= f_opt + 1e-12)) or (np.median(f_sel) < np.median(archive_F[-min(len(archive_F), 20):]) if len(archive_F) > 0 else False):\n                vec = v.copy()[None, :]\n                if success_buf.shape[0] < self.success_buf_size:\n                    success_buf = np.vstack([success_buf, vec])\n                else:\n                    success_buf = np.vstack([success_buf[1:], vec])\n\n            # occasionally recompute PCA basis B from success_buf (different frequency)\n            if (generation % self.subspace_refresh) == 0 and success_buf.shape[0] >= max(2, self.k):\n                try:\n                    Sb = success_buf - np.mean(success_buf, axis=0)\n                    # SVD to get top-k right singular vectors\n                    _, _, Vt = np.linalg.svd(Sb, full_matrices=False)\n                    V = Vt.T\n                    if V.shape[1] >= self.k:\n                        B = V[:, :self.k].copy()\n                except np.linalg.LinAlgError:\n                    pass\n\n            # sigma adaptation via smoothed success rate (1/5-like soft rule)\n            if len(success_window) > 0:\n                suc_rate = float(np.sum(success_window)) / float(len(success_window))\n            else:\n                suc_rate = 0.0\n            # move sigma towards target success 0.2, responsiveness controlled by c_smooth\n            target = 0.20\n            adapt_factor = np.exp((suc_rate - target) * self.c_smooth)\n            sigma *= adapt_factor\n            # clamp sigma to reasonable range\n            sigma = np.clip(sigma, 1e-12, 2.0 * np.mean(ub - lb) * 10.0)\n\n            # per-dim scale update (median absolute of selected y's)\n            if y_sel.shape[0] >= 1:\n                med_abs = np.median(np.abs(y_sel), axis=0)\n                approx_std = 1.4826 * (med_abs + 1e-12)\n                S = (1.0 - self.c_scale) * S + self.c_scale * (approx_std + 1e-12)\n            else:\n                S = S * (1.0 + 1e-6 * np.random.randn(n))\n\n            # prevent degenerate S\n            S = np.clip(S, 1e-12, 1e3)\n\n            # stagnation detection\n            if (evals - last_improve_eval) > stagnation_thresh:\n                # inflate sigma strongly\n                sigma *= self.stag_inflation\n                # reset bandit softly: reduce skew by shrinking rewards toward mean\n                mean_reward = np.mean(arm_rewards)\n                arm_rewards = 0.5 * (arm_rewards - mean_reward) + mean_reward\n                # nudge mean toward a random archive point or random restart\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * np.asarray(archive_X[pick])\n                else:\n                    m = np.random.uniform(lb, ub)\n                # clear some success buffer entries\n                success_buf = success_buf[max(0, success_buf.shape[0] - 2):, :]\n                # slightly enlarge scales\n                S = S * (1.0 + 0.35)\n                # widen stagnation threshold for next period\n                stagnation_thresh = int(stagnation_thresh * 1.4) + 3\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # safety stop\n            if evals >= budget:\n                break\n\n        # final return\n        if x_opt is None:\n            if evals < budget:\n                xm = np.clip(m, lb, ub)\n                f_final = func(xm)\n                return float(f_final), np.array(xm, dtype=float)\n            else:\n                # no stored best (unlikely) — fall back to mean without eval\n                return float(np.inf), np.clip(m, lb, ub)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 143, in evaluate, the following error occurred:\nSyntaxError: invalid syntax\nOn line: arx = np.zeros((current_lambda, n))", "error": "In the code, line 143, in evaluate, the following error occurred:\nSyntaxError: invalid syntax\nOn line: arx = np.zeros((current_lambda, n))", "parent_ids": "bb5431ea-d13a-47a4-afb9-8d953d29c548", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "94f68057-18ed-4bae-8928-9303286d6074", "fitness": 0.12448709038232667, "name": "HALES", "description": "HALES is a hybrid adaptive search that combines a per-coordinate diagonal scale D (robust, cheap covariance proxy updated by weighted second moments with blend c_d=0.18) with a learned low-rank subspace U (SVD of a FIFO success buffer, rank k ≈ sqrt(n), updated every subspace_update_every steps) and CMA-like recombination (λ, μ, log-weights, path-length ps and sigma adaptation using cs/damps/chi_n) to get both global and anisotropic local steps. Candidate generation uses mirrored sampling and an ensemble of four “arms” (axis-wise scaled Gaussian, low-rank subspace sampling, archive-based DE differential mutation with factor F_de, and occasional Cauchy heavy-tailed jumps) whose selection is directed by a simple UCB bandit (arm_counts/arm_rewards with tiny exploration) to balance exploitation/exploration. Practical mechanisms include a bounded archive for DE candidates, success_buf_max = max(10*k,30) to control SVD data, initial sigma ~0.18·range, clipping to user bounds, and small nudges/fallbacks when no good selections occur. Stagnation detection multiplies sigma, resets the bandit, slightly perturbs the mean (toward archive or random), decays the success buffer, and inflates D to reintroduce exploration.", "code": "import numpy as np\n\nclass HALES:\n    \"\"\"\n    HALES: Hybrid Adaptive Low-rank Ensemble Search\n\n    Key ideas combined:\n      - Diagonal per-coordinate scaling D + low-rank subspace U learned from successes\n      - Mirrored sampling, CMA-like path-length sigma adaptation, and weighted recombination\n      - UCB bandit picks among arms: axis-exploit, subspace-explore, DE-diff, Cauchy-jump\n      - Bounded archive for DE, success buffer for SVD, stagnation detection and nudges\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population/default sizes (CMA-like baseline)\n        self.lambda_ = max(4, int(4 + np.floor(3.0 * np.log(max(1, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace rank (default ~ sqrt(n))\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(max(1, self.dim)))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # probabilities / factors\n        self.p_de = 0.20\n        self.F_de = 0.75\n        self.p_cauchy = 0.10\n\n        # adaptation rates\n        self.c_d = 0.18         # diag update blending\n        self.subspace_update_every = max(5, int(2 + self.k))\n        self.success_buf_max = max(10 * self.k, 30)\n\n        # stagnation controls\n        self.stag_sigma_mult = 2.0\n        self.stag_patience = max(8, int(3 * self.dim / max(1, self.k)))\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds handling (uniform -5..5 fallback)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # recombination weights (log-weights)\n        mu = min(self.mu, self.lambda_)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length sigma constants (CMA-like)\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.18 * np.mean(ub - lb)\n        D = np.ones(n)  # per-dim scales (~std)\n        ps = np.zeros(n)  # path vector\n\n        # low-rank subspace U init (orthonormal)\n        if self.k >= 1:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                U, _ = np.linalg.qr(rand_mat)\n                U = U[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        # success buffer for subspace learning (rows are normalized successful y)\n        success_buf = np.zeros((0, n))\n\n        # bounded archive\n        archive_X = []\n        archive_F = []\n        archive_max = max(1000, 50 * self.k)\n\n        # bandit UCB over arms: 0=axis,1=subspace,2=DE,3=Cauchy\n        n_arms = 4\n        arm_counts = np.ones(n_arms, dtype=float)\n        arm_rewards = np.full(n_arms, 1e-6, dtype=float)\n\n        # book-keeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improvement = 0\n        generation = 0\n\n        # evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_opt = float(fm); x_opt = xm.copy(); last_improvement = evals\n\n        # main loop (budget-aware)\n        while evals < budget:\n            generation += 1\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            # mirrored sampling: produce half and mirror\n            half = (lam + 1) // 2\n            arx = np.zeros((lam, n))\n            arz = np.zeros((lam, n))  # y = (x - m) / sigma\n            arms_used = np.zeros(lam, dtype=int)\n\n            # compute UCB scores\n            total_pulls = np.sum(arm_counts)\n            avg_rewards = arm_rewards / (arm_counts + 1e-12)\n            ucb = avg_rewards + np.sqrt(2.0 * np.log(max(1.0, total_pulls)) / (arm_counts + 1e-12))\n\n            for i in range(half):\n                # pick an arm (with small random exploration)\n                if np.random.rand() < 0.03:\n                    arm = np.random.randint(n_arms)\n                else:\n                    candidates = np.flatnonzero(np.isclose(ucb, ucb.max()))\n                    arm = int(np.random.choice(candidates))\n                arm_counts[arm] += 1.0\n\n                # base random direction\n                z = np.random.randn(n)\n\n                # create y according to arm\n                if arm == 0:\n                    # axis-wise exploitation\n                    y = 0.9 * (D * z)\n                elif arm == 1 and self.k > 0:\n                    # low-rank exploration: sample low-rank coefficients and mix with diag\n                    z_low = np.random.randn(self.k)\n                    low = U @ z_low\n                    beta = 0.9 * np.median(D) * (0.7 + 0.4 * np.random.rand())\n                    y = 0.5 * (D * z) + beta * (low / (np.linalg.norm(low) + 1e-12))\n                elif arm == 2 and len(archive_X) >= 2:\n                    # DE-style differential mutation around mean\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de = self.F_de * (archive_X[i1] - archive_X[i2])\n                    # add small gaussian noise in diag space\n                    y = 0.6 * (D * z) + (de / max(1e-12, sigma))\n                else:\n                    # Cauchy heavy-tailed jump\n                    if np.random.rand() < self.p_cauchy:\n                        r = np.random.standard_cauchy()\n                        nd = np.linalg.norm(z) + 1e-20\n                        y = (np.sign(r) * (np.log1p(abs(r)) + 0.4)) * (z / nd) * np.median(D)\n                    else:\n                        y = D * z\n\n                # mirrored pair\n                idx = i\n                j = i + half if i + half < lam else None\n\n                # candidate 1\n                x1 = m + sigma * y\n                x1 = np.clip(x1, lb, ub)\n                arx[idx] = x1\n                arz[idx] = y\n                arms_used[idx] = arm\n\n                # mirrored candidate if space allows\n                if j is not None:\n                    arx[j] = np.clip(m - sigma * y, lb, ub)\n                    arz[j] = -y\n                    arms_used[j] = arm\n\n                # recompute ucb quickly\n                total_pulls = np.sum(arm_counts)\n                avg_rewards = arm_rewards / (arm_counts + 1e-12)\n                ucb = avg_rewards + np.sqrt(2.0 * np.log(max(1.0, total_pulls)) / (arm_counts + 1e-12))\n\n            # Evaluate candidates sequentially until budget exhausted\n            arfit = np.full(lam, np.inf)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = float(f)\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > archive_max:\n                    archive_X.pop(0); archive_F.pop(0)\n\n                # assign reward to arm which produced that candidate\n                arm = int(arms_used[i])\n                # reward: improvement amount (positive if improved)\n                if f < f_opt:\n                    improvement = float(f_opt - f)\n                    f_opt = float(f); x_opt = x.copy(); last_improvement = evals\n                    arm_rewards[arm] += improvement + 1e-8\n                else:\n                    # small reward to keep arms from starving\n                    arm_rewards[arm] += 1e-8\n\n            # Selection & recombination\n            idxs = np.argsort(arfit)\n            sel_idx = idxs[:mu]\n            valid_sel = sel_idx[~np.isinf(arfit[sel_idx])]\n            if valid_sel.size > 0:\n                x_sel = arx[valid_sel]\n                y_sel = arz[valid_sel]\n                eff_mu = valid_sel.size\n                w = weights[:eff_mu]\n                w = w / np.sum(w)\n                m_old = m.copy()\n                m = np.sum(w[:, None] * x_sel, axis=0)\n                y_w = np.sum(w[:, None] * y_sel, axis=0)\n            else:\n                # fallback small random nudge\n                y_w = D * np.random.randn(n)\n                m_old = m.copy()\n                m = np.clip(m + 0.2 * sigma * y_w, lb, ub)\n\n            # update ps and sigma (approx invsqrtC by 1/sqrt(D))\n            invsqrtD = 1.0 / (np.sqrt(D) + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invsqrtD)\n            norm_ps = np.linalg.norm(ps)\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = np.clip(sigma, 1e-12, 1e3 * np.mean(ub - lb) + 1e-12)\n\n            # update diagonal scales D using weighted second-moment of selected y (robustified)\n            if valid_sel.size > 0:\n                y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n                # convert to std-like and blend\n                D_new = np.sqrt(y2 + 1e-20)\n                D = (1.0 - self.c_d) * D + self.c_d * (D_new + 1e-12)\n            else:\n                D = D * (1.0 + 1e-6 * np.random.randn(n))\n\n            # bound D\n            D = np.clip(D, 1e-12, 1e3)\n\n            # update success buffer: push normalized y_w if selection produced improvement or top selected close to best\n            if valid_sel.size > 0:\n                # consider success if any selected is near best or median of selected better than recent archive median\n                selected_f = arfit[valid_sel]\n                recent_med = np.median(archive_F[-min(len(archive_F), 50):]) if len(archive_F) > 0 else np.inf\n                if (np.any(selected_f < f_opt + 1e-12)) or (np.median(selected_f) < recent_med):\n                    v = y_w.copy()\n                    nrm = np.linalg.norm(v) + 1e-20\n                    vec = (v / nrm)[None, :]\n                    # append FIFO\n                    if success_buf.shape[0] < self.success_buf_max:\n                        success_buf = np.vstack([success_buf, vec])\n                    else:\n                        success_buf = np.vstack([success_buf[1:], vec])\n\n            # periodic subspace update via SVD on success buffer\n            if (generation % self.subspace_update_every) == 0 and success_buf.shape[0] >= max(2, self.k):\n                try:\n                    Sb = success_buf - np.mean(success_buf, axis=0, keepdims=True)\n                    # compute small SVD (rows x n) -> get principal right singular vectors\n                    U_s, svals, Vt = np.linalg.svd(Sb, full_matrices=False)\n                    V = Vt.T\n                    if V.shape[1] >= self.k:\n                        U = V[:, :self.k].copy()\n                except np.linalg.LinAlgError:\n                    pass\n\n            # stagnation detection & handling\n            if (evals - last_improvement) > self.stag_patience:\n                sigma *= self.stag_sigma_mult\n                # reset bandit to encourage exploration\n                arm_counts = np.ones(n_arms, dtype=float)\n                arm_rewards = np.full(n_arms, 1e-6, dtype=float)\n                # nudge mean toward a random archive point or random location\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                else:\n                    m = np.random.uniform(lb, ub)\n                # decay success buffer a bit\n                if success_buf.shape[0] > 2:\n                    success_buf = success_buf[max(0, success_buf.shape[0] - 2):, :]\n                # increase D modestly\n                D = D * 1.2\n                last_improvement = evals  # avoid immediate repeated triggers\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # loop continues until budget exhausted\n\n        # final return best found\n        if x_opt is None:\n            xm = np.clip(m, lb, ub)\n            f_final = func(xm)\n            return float(f_final), np.array(xm, dtype=float)\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HALES scored 0.124 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "bb5431ea-d13a-47a4-afb9-8d953d29c548", "operator": null, "metadata": {"aucs": [0.06882383641304046, 0.10336631917004935, 0.19139175765434402, 0.1158078748598369, 0.1513135104048099, 0.12203727277508547, 0.1524020619951557, 0.13544505583163524, 0.10335835870907151, 0.10092485601023826]}, "task_prompt": ""}
{"id": "c3a2a970-fa64-4a82-87a7-761a84bb1bec", "fitness": 0.1642400285573015, "name": "EPAC", "description": "The algorithm maintains a small ensemble of adaptive \"centers\" (ensemble_size scaled with dim, initialized from a budget-aware space‑filling seed and an archived best set) and always evaluates through a budget‑aware safe_eval that clips to bounds and counts/archives function calls, with archive pruning (max_archive = max(2000,50*n)) to limit memory. Each center has per‑dim trust radii and a scalar step that adaptively expand/shrink (success_expand=1.5, failure_shrink=0.68; trust_init_frac=0.45, trust_min_frac tiny, trust_max_frac up to 2.0) plus a short memory of recent successful direction unit vectors (mem_size=12) used to build a PCA‑biased subspace basis for variance‑weighted Gaussian probes (probes_per_center=10, max_eval_per_iter=60) to favor previously productive directions. Local exploitation is accelerated by cheap model‑based moves: a separable quadratic fit over nearby archived neighbors proposes a trust‑clipped step when enough data exist, and a parabolic_line_min routine performs up to 3‑point line refinements along promising directions to polish moves. Global diversification is preserved via occasional heavy‑tailed Cauchy jumps (scaled by 0.45*rng_range), random jitter polish, and center replacement after stagnation (center_replace_patience=8), balancing exploration/exploitation within the given evaluation budget.", "code": "import numpy as np\n\nclass EPAC:\n    \"\"\"\n    Ensemble PCA-Probed Adaptive Centers (EPAC)\n\n    One-line: Maintain multiple adaptive centers, each with a short memory of successful directions to build PCA-biased subspaces\n    for variance-weighted Gaussian probes; accept improvements immediately with cheap parabolic line refinements and occasional\n    surrogate minimizers, adapting per-center trust radii and step sizes, with budget-aware safe evaluation and archive pruning.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        # initialization sizing\n        self.init_ratio = 0.12\n        self.min_init = max(10, 2 * self.dim)\n        self.max_init = min(400, int(0.4 * self.budget))\n\n        # ensemble & modeling\n        self.ensemble_size = max(2, min(8, self.dim // 2 + 1))\n        self.model_neighbor_multiplier = 8\n        self.ridge = 1e-6\n\n        # per-center memory (for PCA and trust)\n        self.mem_size = 12\n        self.min_step = 1e-8\n\n        # trust and step dynamics\n        self.trust_init_frac = 0.45\n        self.trust_min_frac = 1e-6\n        self.trust_max_frac = 2.0\n        self.success_expand = 1.5\n        self.failure_shrink = 0.68\n\n        # probing parameters\n        self.probes_per_center = 10\n        self.max_eval_per_iter = 60\n\n        # center management\n        self.center_replace_patience = 8\n\n        # random\n        self.rng = np.random.RandomState(self.seed)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        lb = lb.reshape(n,)\n        ub = ub.reshape(n,)\n\n        rng_range = ub - lb\n        range_norm = np.linalg.norm(rng_range)\n        evals = 0\n        budget = int(self.budget)\n\n        # archives\n        X = []\n        F = []\n\n        f_best = np.inf\n        x_best = None\n\n        # initial sampling: small space-filling random + jitter\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x0 = self.rng.uniform(lb, ub)\n            f0 = func(x0)\n            evals += 1\n            X.append(x0.copy()); F.append(float(f0))\n            if f0 < f_best:\n                f_best = float(f0); x_best = x0.copy()\n            if evals >= budget:\n                return float(f_best), np.array(x_best, dtype=float)\n\n        # helper safe eval wrapper\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            if evals >= budget:\n                return None\n            x_c = np.clip(np.asarray(x, dtype=float), lb, ub)\n            try:\n                f = func(x_c)\n            except Exception:\n                f = np.inf\n            evals += 1\n            X.append(x_c.copy()); F.append(float(f))\n            if f < f_best - 1e-15:\n                f_best = float(f); x_best = x_c.copy()\n            return float(f), x_c\n\n        # initialize centers: pick best distinct points from archive\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idxs = np.argsort(F)\n            centers = []\n            for idx in idxs:\n                cand = np.array(X[idx])\n                if all(np.linalg.norm(cand - c) > 1e-9 for c in centers):\n                    centers.append(cand)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.ensemble_size, max(1, len(X))))\n        if len(centers) < self.ensemble_size:\n            # fill remaining with random points\n            for _ in range(self.ensemble_size - len(centers)):\n                centers.append(self.rng.uniform(lb, ub))\n        # per-center metadata\n        center_trust = [np.maximum(self.trust_init_frac * rng_range, 1e-12) for _ in centers]  # per-dim radii\n        center_step = [max(0.15 * np.linalg.norm(t), 1e-12) for t in center_trust]  # scalar base step\n        center_mem = [list() for _ in centers]  # list of direction unit vectors\n        center_mem_trust = [[1.0 for _ in range(0)] for _ in centers]  # empty initially\n        center_stag = [0 for _ in centers]\n\n        # parabolic line refinement (uses up to 3 fn evals)\n        def parabolic_line_min(x0, f0, d, delta, allow_center_clip=True):\n            nonlocal evals\n            if evals >= budget:\n                return None\n            d = np.asarray(d, dtype=float)\n            nd = np.linalg.norm(d)\n            if nd == 0:\n                return None\n            d = d / (nd + 1e-20)\n            # evaluate at +delta and -delta (if budget allows)\n            remaining = budget - evals\n            if remaining <= 0:\n                return None\n            x_p = np.clip(x0 + delta * d, lb, ub)\n            outp = safe_eval(x_p)\n            if outp is None:\n                return None\n            f_p, x_p = outp\n            remaining = budget - evals\n            if remaining <= 0:\n                if f_p < f0 - 1e-12:\n                    return f_p, x_p\n                return None\n            x_m = np.clip(x0 - delta * d, lb, ub)\n            outm = safe_eval(x_m)\n            if outm is None:\n                return None\n            f_m, x_m = outm\n            # quadratic fit\n            denom = 2.0 * (delta ** 2)\n            A = (f_p + f_m - 2.0 * f0) / (denom + 1e-24)\n            B = (f_p - f_m) / (2.0 * delta)\n            if A <= 0 or np.isclose(A, 0.0):\n                # return best of samples if improved\n                best_f = f0; best_x = x0\n                if f_p < best_f - 1e-12:\n                    best_f = f_p; best_x = x_p.copy()\n                if f_m < best_f - 1e-12:\n                    best_f = f_m; best_x = x_m.copy()\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None\n            alpha_star = -B / (2.0 * A)\n            # accept only if within reasonable bracket\n            if abs(alpha_star) > 4.0 * delta:\n                # fallback\n                best_f = f0; best_x = x0\n                if f_p < best_f - 1e-12:\n                    best_f = f_p; best_x = x_p.copy()\n                if f_m < best_f - 1e-12:\n                    best_f = f_m; best_x = x_m.copy()\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None\n            # eval at alpha_star\n            remaining = budget - evals\n            if remaining <= 0:\n                return None\n            x_s = np.clip(x0 + alpha_star * d, lb, ub)\n            outs = safe_eval(x_s)\n            if outs is None:\n                return None\n            f_s, x_s = outs\n            if f_s < f0 - 1e-12:\n                return f_s, x_s\n            # otherwise return best of sampled if improved\n            best_f = f0; best_x = x0\n            if f_p < best_f - 1e-12:\n                best_f = f_p; best_x = x_p.copy()\n            if f_m < best_f - 1e-12:\n                best_f = f_m; best_x = x_m.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None\n\n        # helper: build PCA-biased subspace basis from mem steps\n        def build_basis(mem_steps, k):\n            if len(mem_steps) >= 2:\n                M = np.vstack(mem_steps)\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                try:\n                    U, S, Vt = np.linalg.svd(Mc, full_matrices=False)\n                    pcs = Vt.T\n                    take = min(k, pcs.shape[1])\n                    basis_cols = pcs[:, :take]\n                    if take < k:\n                        R = self.rng.randn(n, k - take)\n                        stacked = np.column_stack((basis_cols, R))\n                        Q, _ = np.linalg.qr(stacked)\n                        return Q[:, :k]\n                    else:\n                        Q, _ = np.linalg.qr(basis_cols)\n                        return Q[:, :k]\n                except Exception:\n                    R = self.rng.randn(n, k)\n                    Q, _ = np.linalg.qr(R)\n                    return Q[:, :k]\n            else:\n                R = self.rng.randn(n, k)\n                Q, _ = np.linalg.qr(R)\n                return Q[:, :k]\n\n        # fit cheap separable quadratic around center (returns delta to minimizer)\n        def fit_separable_quad_and_propose(center, neighbors_X, neighbors_F, tr):\n            dx = neighbors_X - center\n            m = dx.shape[0]\n            if m < 3:\n                return None\n            M = np.ones((m, 1 + 2 * n))\n            M[:, 1:1 + n] = dx\n            M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n            y = neighbors_F\n            # weights by distance\n            dists = np.linalg.norm(dx, axis=1)\n            w = 1.0 / (dists + 1e-12)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            bvec = W * y\n            try:\n                lhs = A.T @ A + self.ridge * np.eye(M.shape[1])\n                rhs = A.T @ bvec\n                params = np.linalg.solve(lhs, rhs).flatten()\n                b_lin = params[1:1 + n]\n                h_diag = params[1 + n:1 + 2 * n]\n                h_reg = np.copy(h_diag)\n                h_reg[h_reg < 1e-8] = 1e-8\n                delta = -b_lin / (h_reg + 1e-20)\n                # clip by per-dim trust tr\n                delta = np.clip(delta, -tr, tr)\n                x_prop = np.clip(center + delta, lb, ub)\n                return x_prop\n            except Exception:\n                return None\n\n        # main loop: iterate until budget exhausted\n        iter_since_refresh = 0\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            improved_global = False\n\n            # occasionally refresh centers from archive (take best distinct)\n            if iter_since_refresh % 6 == 0 and len(X) > 0:\n                topk = min(len(centers), max(1, len(X)//10))\n                new_centers = get_top_centers(topk)\n                # merge keeping uniqueness and preserving count\n                merged = []\n                for c in centers + new_centers:\n                    tup = tuple(np.round(c, 12))\n                    if tup not in [tuple(np.round(m,12)) for m in merged]:\n                        merged.append(c)\n                    if len(merged) >= len(centers):\n                        break\n                # ensure right length\n                while len(merged) < len(centers):\n                    merged.append(self.rng.uniform(lb, ub))\n                centers = merged[:len(centers)]\n                # ensure metadata lengths\n                center_trust = [np.maximum(self.trust_init_frac * rng_range, 1e-12) for _ in centers]\n                center_step = [max(0.15 * np.linalg.norm(t), 1e-12) for t in center_trust]\n                center_mem = [list() for _ in centers]\n                center_mem_trust = [[1.0 for _ in range(0)] for _ in centers]\n                center_stag = [0 for _ in centers]\n\n            iter_since_refresh += 1\n\n            # process centers in random order\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n\n            for idx in order:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                center = np.array(centers[idx], dtype=float)\n                tr = np.array(center_trust[idx], dtype=float)\n                step = center_step[idx]\n                mem_steps = center_mem[idx]\n                mem_trust = center_mem_trust[idx] if idx < len(center_mem_trust) else []\n                improved_this_center = False\n                # gather neighbors if available\n                if len(X) >= max(3 * n, self.model_neighbor_multiplier * n):\n                    X_arr = np.asarray(X)\n                    F_arr = np.asarray(F)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    sorted_idx = np.argsort(dists)[:min(len(X_arr), max(2*n+1, self.model_neighbor_multiplier * n))]\n                    X_nei = X_arr[sorted_idx]\n                    F_nei = F_arr[sorted_idx]\n                    # surrogate proposal\n                    prop = fit_separable_quad_and_propose(center, X_nei, F_nei, tr)\n                    if prop is not None and work_allow > 0 and evals < budget:\n                        if len(X) == 0 or not np.allclose(prop, X[-1], atol=1e-12):\n                            out = safe_eval(prop)\n                            work_allow -= 1\n                            if out is not None:\n                                f_prop, x_prop = out\n                                if f_prop < f_best - 1e-12:\n                                    # accept\n                                    centers[idx] = x_prop.copy()\n                                    center_stag[idx] = 0\n                                    center_trust[idx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                                    center_step[idx] = min(center_step[idx] * 1.15, 5.0 * range_norm)\n                                    # update mem\n                                    dv = x_prop - center\n                                    ndv = np.linalg.norm(dv)\n                                    if ndv > 0:\n                                        dir_unit = dv / ndv\n                                        mem_steps.insert(0, dir_unit)\n                                        mem_trust.insert(0, 1.0)\n                                        if len(mem_steps) > self.mem_size:\n                                            mem_steps.pop(); mem_trust.pop()\n                                    improved_this_center = True\n                                    improved_global = True\n                                else:\n                                    # shrink trust\n                                    center_trust[idx] = np.maximum(tr * self.failure_shrink, self.trust_min_frac * rng_range)\n                                    center_step[idx] = max(center_step[idx] * 0.85, self.min_step)\n                # directional ensemble probes in PCA-biased subspace\n                if not improved_this_center and work_allow > 0 and evals < budget:\n                    k = min(n, max(1, int(np.clip(2 + int(np.log1p(n)), 1, n))))\n                    basis = build_basis(mem_steps, k)\n                    # compute variances of coefficients\n                    if len(mem_steps) >= 2:\n                        M = np.vstack(mem_steps)\n                        proj = M @ basis\n                        var_coef = np.var(proj, axis=0) + 1e-12\n                        avg_tr = np.mean(mem_trust) if mem_trust else 1.0\n                        var_coef = var_coef * (1.0 + 0.5 * avg_tr)\n                    else:\n                        var_coef = np.ones(basis.shape[1], dtype=float)\n                    probes = min(self.probes_per_center, max(4, 3 * basis.shape[1]))\n                    candidate_list = []\n                    for p in range(probes):\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        coeffs = self.rng.randn(basis.shape[1]) * np.sqrt(var_coef)\n                        d = basis @ coeffs\n                        nd = np.linalg.norm(d)\n                        if nd == 0:\n                            continue\n                        d = d / (nd + 1e-20)\n                        # alpha sampling biased toward current step magnitude\n                        if self.rng.rand() < 0.15:\n                            alpha = self.rng.uniform(-2.0 * step, 2.0 * step)\n                        else:\n                            alpha = self.rng.uniform(-step, step)\n                        x_try = np.clip(center + alpha * d, lb, ub)\n                        if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                            continue\n                        out = safe_eval(x_try)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_try, x_t = out\n                        candidate_list.append((f_try, x_t.copy(), d.copy(), alpha))\n                        if f_try < f_best - 1e-12:\n                            # parabolic refine anchored at center along direction d\n                            delta = max(abs(alpha), 0.9 * step)\n                            res = parabolic_line_min(center, np.inf if x_best is None else float(np.min(F)), d, delta)\n                            # Note: parabolic expects f0 at center. Use current best at center if available in archive near center\n                            # compute f0 as nearest archived point to center if possible\n                            f0_center = None\n                            if len(X) > 0:\n                                dists_local = np.linalg.norm(np.asarray(X) - center, axis=1)\n                                nearest = int(np.argmin(dists_local))\n                                f0_center = float(F[nearest])\n                                # perform parabolic with correct f0\n                                res = parabolic_line_min(center, f0_center, d, delta)\n                            if res is not None:\n                                f_line, x_line = res\n                                if f_line is not None and f_line < f_try - 1e-12:\n                                    f_try = f_line; x_t = x_line.copy()\n                            # accept move\n                            centers[idx] = x_t.copy()\n                            center_stag[idx] = 0\n                            center_trust[idx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                            center_step[idx] = min(center_step[idx] * 1.12, 5.0 * range_norm)\n                            # update memory\n                            dv = x_t - center\n                            ndv = np.linalg.norm(dv)\n                            if ndv > 0:\n                                dir_unit = dv / ndv\n                                mem_steps.insert(0, dir_unit)\n                                mem_trust.insert(0, 1.0)\n                                if len(mem_steps) > self.mem_size:\n                                    mem_steps.pop(); mem_trust.pop()\n                            improved_this_center = True\n                            improved_global = True\n                            break\n                        else:\n                            # small chance to attempt parabolic if budget available\n                            if self.rng.rand() < 0.035 and (budget - evals) >= 3:\n                                # use center nearest archived f0 if available\n                                f0_center = None\n                                if len(X) > 0:\n                                    dists_local = np.linalg.norm(np.asarray(X) - center, axis=1)\n                                    nearest = int(np.argmin(dists_local))\n                                    f0_center = float(F[nearest])\n                                else:\n                                    f0_center = f_best\n                                res = parabolic_line_min(center, f0_center, d, delta=step * 0.6)\n                                if res is not None:\n                                    f_line, x_line = res\n                                    if f_line is not None and f_line < f_best - 1e-12:\n                                        centers[idx] = x_line.copy()\n                                        center_stag[idx] = 0\n                                        center_trust[idx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                                        center_step[idx] = min(center_step[idx] * 1.08, 5.0 * range_norm)\n                                        dv = x_line - center\n                                        ndv = np.linalg.norm(dv)\n                                        if ndv > 0:\n                                            mem_steps.insert(0, dv / ndv)\n                                            mem_trust.insert(0, 1.0)\n                                            if len(mem_steps) > self.mem_size:\n                                                mem_steps.pop(); mem_trust.pop()\n                                        improved_this_center = True\n                                        improved_global = True\n                                        break\n                    # end probes\n\n                    # if no immediate improvement, try using top candidate list to parabolically polish from center\n                    if (not improved_this_center) and candidate_list and work_allow > 0 and (budget - evals) >= 3:\n                        candidate_list.sort(key=lambda z: z[0])\n                        topk = min(3, len(candidate_list))\n                        for j in range(topk):\n                            f_cand, x_cand, d_cand, alpha_cand = candidate_list[j]\n                            # use nearest archived f0 at center if available\n                            f0_center = f_best\n                            if len(X) > 0:\n                                dists_local = np.linalg.norm(np.asarray(X) - center, axis=1)\n                                nearest = int(np.argmin(dists_local))\n                                f0_center = float(F[nearest])\n                            res = parabolic_line_min(center, f0_center, d_cand, delta=max(step, abs(alpha_cand)))\n                            if res is not None:\n                                f_line, x_line = res\n                                if f_line is not None and f_line < f_best - 1e-12:\n                                    centers[idx] = x_line.copy()\n                                    center_stag[idx] = 0\n                                    center_trust[idx] = np.minimum(tr * self.success_expand, self.trust_max_frac * rng_range)\n                                    center_step[idx] = min(center_step[idx] * 1.09, 5.0 * range_norm)\n                                    dv = x_line - center\n                                    ndv = np.linalg.norm(dv)\n                                    if ndv > 0:\n                                        mem_steps.insert(0, dv / ndv)\n                                        mem_trust.insert(0, 1.0)\n                                        if len(mem_steps) > self.mem_size:\n                                            mem_steps.pop(); mem_trust.pop()\n                                    improved_this_center = True\n                                    improved_global = True\n                                    break\n\n                # small local jitter polishing occasionally\n                if improved_this_center and evals < budget and self.rng.rand() < 0.35 and work_allow > 0:\n                    jitter = self.rng.randn(n)\n                    jitter = jitter / (np.linalg.norm(jitter) + 1e-20)\n                    jitter *= 0.2 * center_step[idx]\n                    out = safe_eval(np.clip(centers[idx] + jitter, lb, ub))\n                    work_allow -= 1\n                    if out is not None:\n                        fj, xj = out\n                        if fj < f_best - 1e-12:\n                            centers[idx] = xj.copy()\n                            # update memory\n                            dv = xj - center\n                            ndv = np.linalg.norm(dv)\n                            if ndv > 0:\n                                mem_steps.insert(0, dv / ndv)\n                                mem_trust.insert(0, 1.0)\n                                if len(mem_steps) > self.mem_size:\n                                    mem_steps.pop(); mem_trust.pop()\n\n                # update trust & memory decay based on whether improved\n                if improved_this_center:\n                    # boost memory trusts slightly\n                    mem_trust = [min(t * 1.06, 10.0) for t in mem_trust]\n                    # reduce stagnation\n                    center_stag[idx] = 0\n                else:\n                    # shrink step and trust on failure\n                    center_trust[idx] = np.maximum(center_trust[idx] * self.failure_shrink, self.trust_min_frac * rng_range)\n                    center_step[idx] = max(center_step[idx] * 0.86, self.min_step)\n                    center_stag[idx] += 1\n                    # decay memory trust\n                    mem_trust = [t * 0.94 for t in mem_trust]\n\n                # prune low-trust memory entries\n                if mem_trust:\n                    keep_idx = [i for i, t in enumerate(mem_trust) if t > 1e-3]\n                    if len(keep_idx) != len(mem_trust):\n                        mem_steps = [mem_steps[i] for i in keep_idx]\n                        mem_trust = [mem_trust[i] for i in keep_idx]\n\n                # write back mem and trust\n                center_mem[idx] = mem_steps\n                if idx < len(center_mem_trust):\n                    center_mem_trust[idx] = mem_trust\n                else:\n                    center_mem_trust.append(mem_trust)\n\n                # if center stagnated, replace it with a distant-ish archive sample\n                if center_stag[idx] >= self.center_replace_patience and len(X) > 0:\n                    X_arr = np.asarray(X)\n                    # compute distance to current centers\n                    d_to_centers = np.min([np.linalg.norm(X_arr - c, axis=1) for c in centers], axis=0)\n                    # choose among top distant candidates but from reasonably good ones\n                    cand_idx = np.argsort(d_to_centers)[-max(1, min(30, len(X_arr))):]\n                    pick = int(self.rng.choice(cand_idx))\n                    centers[idx] = X_arr[pick].copy()\n                    center_trust[idx] = np.maximum(self.trust_init_frac * rng_range, 1e-12)\n                    center_step[idx] = max(0.15 * np.linalg.norm(center_trust[idx]), 1e-12)\n                    center_mem[idx] = []\n                    center_mem_trust[idx] = []\n                    center_stag[idx] = 0\n\n                # small global diversification occasional jump from this center\n                if (not improved_this_center) and work_allow > 0 and self.rng.rand() < 0.06 and evals < budget:\n                    # Cauchy-style heavy tail scaled jump\n                    jump = self.rng.standard_cauchy(size=n)\n                    jump = np.clip(jump, -8, 8)\n                    scale = 0.45 * rng_range\n                    x_jump = np.clip(center + jump * scale, lb, ub)\n                    out = safe_eval(x_jump)\n                    work_allow -= 1\n                    if out is not None:\n                        fj, xj = out\n                        if fj < f_best - 1e-12:\n                            centers[idx] = xj.copy()\n                            center_trust[idx] = np.minimum(center_trust[idx] * 2.0, self.trust_max_frac * rng_range)\n                            center_step[idx] = min(center_step[idx] * 1.3, 5.0 * range_norm)\n                            center_mem[idx] = []\n                            center_mem_trust[idx] = []\n                            center_stag[idx] = 0\n                            improved_global = True\n                        else:\n                            center_trust[idx] = np.maximum(center_trust[idx] * 0.8, self.trust_min_frac * rng_range)\n\n            # end centers loop\n\n            # archive pruning\n            max_archive = max(2000, 50 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                if len(rest) > 0:\n                    keep_rest = rest[::max(1, len(rest) // (max_archive - 200))]\n                    keep_idx = np.concatenate([keep_best, keep_rest])\n                else:\n                    keep_idx = keep_best\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # optional early exit if extremely good\n            if f_best <= 1e-12:\n                break\n\n        # final best\n        if x_best is None and len(X) > 0:\n            x_best = np.array(X[np.argmin(F)])\n            f_best = float(min(F))\n        elif x_best is None:\n            # fallback random\n            x_best = self.rng.uniform(lb, ub)\n            f_best = float(func(x_best))\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm EPAC scored 0.164 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "bb5431ea-d13a-47a4-afb9-8d953d29c548", "operator": null, "metadata": {"aucs": [0.10891852806747182, 0.16342883171904743, 0.2229498599187184, 0.15769938804021633, 0.16252994847058677, 0.17534906457391752, 0.1895796564193325, 0.1691190492043475, 0.1499424439646624, 0.1428835151947141]}, "task_prompt": ""}
{"id": "ee69a0f5-da4f-4073-b8bf-8abcdb0c6680", "fitness": "-inf", "name": "EvoTrustPCA", "description": "EvoTrustPCA is an ensemble trust-region method that samples candidate steps in a learned low-rank PCA subspace (U) plus random orthonormal directions, using a trust radius r and per-dimension robust scaling D to respect anisotropy; the subspace size k defaults to about sqrt(n) and U is orthonormalized (QR/SVD). It uses a small set of relative scales (scales = [0.15,0.4,1.0,2.2]) with multiplicative (exponential) weight updates to perform multi-scale search and mirrored sampling for variance reduction, while D is estimated robustly from the archive via MAD and initially set to 0.08 of variable range. A lightweight linear surrogate (f(x) ≈ f(m) + g^T(x−m)) fitted on recent archive points predicts improvements and drives trust-region adaptation and soft center updates (acceptance by predicted vs actual improvement rho). The algorithm preserves an evaluated archive, updates PCA from recent successful steps, enforces bounds and the evaluation budget, and includes stagnation handling via occasional heavy‑tailed (Lévy/Cauchy) restarts and scale resets to promote global exploration.", "code": "import numpy as np\n\nclass EvoTrustPCA:\n    \"\"\"\n    EvoTrustPCA: Ensemble Trust-Region + PCA-driven low-rank sampling + linear surrogate.\n    Key ideas:\n     - Maintain a trust radius r and sample candidate steps y = r * scale * d, where d are directions\n       drawn from a learned low-rank PCA basis (plus some random directions).\n     - Maintain multiple relative scales (multi-scale search) and adapt their sampling probabilities\n       using a simple reward-weighting (multiplicative weights / bandit-like).\n     - Fit a lightweight linear surrogate f(x) ≈ f(m) + g^T (x-m) from recent archive and use its\n       predicted improvements to update the trust radius (like trust-region methods).\n     - Use mirrored sampling for variance reduction, per-coordinate robust scaling via MAD,\n       occasional heavy-tailed restarts if stagnation detected, and adaptive PCA updates from\n       recent successful steps.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, sub_k=None, init_radius=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.seed = seed\n\n        # low-rank subspace size (default ~ sqrt(n))\n        if sub_k is None:\n            self.k = min(self.dim, max(1, int(np.ceil(np.sqrt(self.dim)))))\n        else:\n            self.k = min(self.dim, max(1, int(sub_k)))\n\n        # default multi-scale multipliers (relative to trust radius r)\n        self.scales = np.array([0.15, 0.4, 1.0, 2.2])\n        # initial uniform weights for the scales (will be adapted multiplicatively)\n        self.scale_weights = np.ones(len(self.scales), dtype=float)\n\n        # per-dimension robust smoothing parameter (for MAD)\n        self.c_mad = 0.18\n\n        # how many recent points to keep for surrogate / PCA\n        self.archive_max = 5000\n        self.surrogate_window = max(8, 5 * self.k)\n        self.pca_update_every = 6\n        self.pca_buffer_max = max(4 * self.k, 20)\n\n        # trust radius bounds (relative to coordinate range; set later when bounds known)\n        self.init_radius = init_radius  # if None set inside __call__\n\n        # stagnation handling\n        self.stagnation_threshold = max(30, 8 * self.dim)\n\n        # miscellaneous\n        self.mirrored = True\n        self.levy_restart_prob = 0.06\n        self.levy_scale = 0.6  # multiplier of variable range for heavy-tailed jump\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds provided by func.bounds or default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        rng = np.random\n\n        # initial mean uniformly in bounds\n        m = rng.uniform(lb, ub)\n\n        # initialize trust radius relative to mean range\n        var_range = np.maximum(ub - lb, 1e-12)\n        mean_range = np.mean(var_range)\n        if self.init_radius is None:\n            r = 0.3 * mean_range\n        else:\n            r = float(self.init_radius)\n\n        # per-dimension robust scale D (start uniform)\n        D = np.maximum(1e-9, 0.08 * var_range)  # initial per-dim step-size \"pre-scaler\"\n\n        # low-rank basis U (n x k), start random orthonormal\n        if self.k >= 1:\n            R = rng.randn(n, self.k)\n            try:\n                U, _ = np.linalg.qr(R)\n                U = U[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.eye(n)[:, :self.k]\n        else:\n            U = np.zeros((n, 0))\n\n        # archive of evaluated points and values\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = float(\"inf\")\n        x_opt = None\n\n        # track last improvement\n        last_improvement_eval = 0\n\n        # initial evaluation at m\n        xm = np.clip(m, lb, ub)\n        f = func(xm)\n        evals += 1\n        archive_X.append(xm.copy())\n        archive_F.append(float(f))\n        f_opt = float(f)\n        x_opt = xm.copy()\n        last_improvement_eval = evals\n\n        gen = 0\n        success_buffer = []  # store successful steps y for PCA\n        surrogate_reg = 1e-6\n\n        # helper: compute robust per-dim MAD-based D estimate from recent archive entries\n        def update_D_from_archive(window=20):\n            nonlocal D\n            if len(archive_X) < 3:\n                return\n            m_recent = np.asarray(archive_X[-window:])\n            # compute y = x - median (per-dim)\n            med = np.median(m_recent, axis=0)\n            diffs = np.abs(m_recent - med)\n            mad = np.median(diffs, axis=0)\n            approx_std = 1.4826 * (mad + 1e-12)\n            D = (1.0 - self.c_mad) * D + self.c_mad * (approx_std + 1e-12)\n\n        # helper: fit linear surrogate g in original space using last surrogate_window points:\n        # f(x) ≈ f(m) + g^T (x - m)\n        def fit_linear_surrogate(center):\n            if len(archive_X) < 4:\n                return np.zeros(n)\n            # take recent points centered at 'center'\n            use_count = min(len(archive_X), self.surrogate_window)\n            Xarr = np.asarray(archive_X[-use_count:])\n            Farr = np.asarray(archive_F[-use_count:])\n            Y = (Xarr - center).T  # shape (n, m)\n            f_rel = (Farr - np.median(Farr)).astype(float)  # center f-values to stabilize\n            # compute g = (Y Y^T + reg I)^{-1} Y f_rel\n            YYt = Y @ Y.T\n            reg = surrogate_reg * np.mean(np.diag(YYt) + 1e-12)\n            try:\n                A = YYt + reg * np.eye(n)\n                g = np.linalg.solve(A, Y @ f_rel)\n                return g\n            except np.linalg.LinAlgError:\n                # fallback small ridge via pseudo-inverse\n                try:\n                    g = np.linalg.pinv(YYt + reg * np.eye(n)) @ (Y @ f_rel)\n                    return g\n                except Exception:\n                    return np.zeros(n)\n\n        # main loop: generate batches of candidates and evaluate until budget consumed\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n\n            # update per-dim D periodically\n            if (gen % 3) == 0:\n                update_D_from_archive(window=40)\n\n            # update PCA basis occasionally from success_buffer\n            if (len(success_buffer) >= max(2, self.k)) and (gen % self.pca_update_every == 0):\n                Y = np.vstack(success_buffer).T  # n x m\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        # pad if needed\n                        if U.shape[1] < self.k:\n                            pad = rng.randn(n, self.k - U.shape[1])\n                            try:\n                                Qp, _ = np.linalg.qr(pad)\n                                U = np.hstack([U, Qp[:, : self.k - U.shape[1]]])\n                            except Exception:\n                                pass\n                except Exception:\n                    pass\n                # keep buffer limited\n                if len(success_buffer) > self.pca_buffer_max:\n                    success_buffer = success_buffer[-self.pca_buffer_max:]\n\n            # determine how many candidates to propose this iteration\n            # produce a small batch, at least 4 and at most remaining\n            batch_target = min( max(4, int(2 + 2 * len(self.scales) + self.k)), remaining )\n\n            # build candidate list\n            candidates = []\n            candidate_info = []  # store (scale_idx, dir_vec, y_vec) for use after eval\n            # prepare direction pool: use U columns and some random orthonormals\n            dirs = []\n            if self.k > 0 and U.shape[1] >= 1:\n                for j in range(U.shape[1]):\n                    dirs.append(U[:, j].copy())\n            # add some random orthonormal directions to reach up to batch_target/2\n            num_random_dirs = max(1, batch_target // 3)\n            R = rng.randn(n, num_random_dirs)\n            try:\n                Q, _ = np.linalg.qr(R)\n                for j in range(Q.shape[1]):\n                    dirs.append(Q[:, j].copy())\n            except Exception:\n                for j in range(R.shape[1]):\n                    v = R[:, j]\n                    norm = np.linalg.norm(v) + 1e-20\n                    dirs.append(v / norm)\n\n            # normalize directions with D-weighting (so they act in scaled space)\n            normed_dirs = []\n            for d in dirs:\n                d_scaled = d / (np.sqrt(np.mean((D * d) ** 2)) + 1e-12)\n                nd = d_scaled / (np.linalg.norm(d_scaled) + 1e-20)\n                normed_dirs.append(nd)\n\n            # sample scales according to weights (softmax-like)\n            w = self.scale_weights + 1e-12\n            p_scales = w / np.sum(w)\n\n            # populate candidate list by iterating scales and directions (mirrored)\n            di = 0\n            si = 0\n            while (len(candidates) < batch_target) and (len(normed_dirs) > 0):\n                scale_idx = np.random.choice(len(self.scales), p=p_scales)\n                # pick a direction cycling through normed_dirs\n                dir_vec = normed_dirs[di % len(normed_dirs)]\n                di += 1\n                s = float(self.scales[scale_idx])\n                # per-dimension scaling via D incorporated into final y as (D * dir) normalized\n                y = (D * dir_vec)\n                if np.linalg.norm(y) < 1e-20:\n                    continue\n                y = y / np.linalg.norm(y)\n                y = s * y  # unit-length scaled in \"D-space\"\n                # mirrored pair\n                for sign in (1.0, -1.0) if self.mirrored else (1.0,):\n                    y_c = sign * y\n                    x_c = m + r * y_c\n                    x_c = np.clip(x_c, lb, ub)\n                    # avoid duplicates\n                    candidates.append(x_c)\n                    candidate_info.append((scale_idx, dir_vec.copy(), y_c.copy()))\n                    if len(candidates) >= batch_target:\n                        break\n\n            # ensure unique candidates and limit to remaining budget\n            # evaluate sequentially until budget or candidate list exhausted\n            # fit surrogate once per batch at center m\n            g_sur = fit_linear_surrogate(m)\n            f_m = np.interp(0, [0,1], [0,1])  # dummy to ensure variable exists\n            # get f at current m from archive (exact)\n            f_m = archive_F[-1] if np.allclose(archive_X[-1], m) else np.min(archive_F + [archive_F[-1]])  # safe fallback\n            # Evaluate candidates\n            for idx_c, x in enumerate(candidates):\n                if evals >= budget:\n                    break\n                x = np.asarray(x)\n                # skip evaluating if x equals last evaluated (rare)\n                if np.allclose(x, archive_X[-1]):\n                    # still count as not evaluated and skip\n                    continue\n                f_x = func(x)\n                evals += 1\n                archive_X.append(x.copy())\n                archive_F.append(float(f_x))\n                # maintain archive size\n                if len(archive_X) > self.archive_max:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                # record improvement\n                if f_x < f_opt - 1e-15:\n                    f_opt = float(f_x)\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n                # evaluate predicted improvement per candidate (for scale adaptation)\n                scale_idx, dir_vec, y_c = candidate_info[idx_c]\n                # compute predicted improvement by surrogate: predicted = -g^T (r*y_c)\n                pred = - float(np.dot(g_sur, r * y_c))\n                # actual reward\n                actual_reward = max(0.0, f_m - f_x)\n                # update multiplicative weight for this scale by an exponential factor\n                # reward normalized by (abs(f_m) + 1) to maintain scale invariant updates\n                denom = (abs(f_m) + 1.0)\n                eta = 0.8 / max(1.0, len(self.scales))  # learning rate modest\n                self.scale_weights[scale_idx] *= np.exp(eta * (actual_reward / denom))\n                # store successful step for PCA if it was an improvement over m\n                if f_x < f_m:\n                    # store the step in \"y-space\"\n                    success_buffer.append((x - m) / max(r, 1e-12))\n                # adaptive trust-region update based on predicted vs actual improvement (rho)\n                # only meaningful if pred > small threshold\n                pred_pos = max(1e-12, pred)\n                rho = (f_m - f_x) / pred_pos if pred_pos > 0 else 0.0\n                # update rules inspired by trust-region\n                if f_x < f_m:\n                    # accept move towards x (soft update)\n                    alpha = 0.9 if rho > 0.5 else 0.6\n                    # set m closer to x (not full replacement) to keep stability\n                    m = (1.0 - alpha) * m + alpha * x\n                else:\n                    # small exploratory move away from m possibly; keep m unchanged\n                    pass\n                # adjust trust radius\n                if rho > 0.75:\n                    r = min(5.0 * mean_range, r * 1.5)\n                elif rho < 0.25:\n                    r = max(1e-6, r * 0.6)\n                # occasionally perform small random local perturbation to avoid local noise\n                if np.random.rand() < 0.02:\n                    m = np.clip(m + 0.02 * r * rng.randn(n), lb, ub)\n                # update f_m to the value at current m if we moved; else keep\n                # approximate f at m by nearest archive point if equal to x; otherwise keep last known best at m\n                # we'll simply set f_m = value at the last point equal to m if present, else min archive\n                # (This is used only for normalizing updates and simple heuristics.)\n                # update f_m to minimum of archive within a small tolerance near m\n                distances = np.linalg.norm(np.asarray(archive_X) - m, axis=1)\n                close_idx = np.argmin(distances)\n                f_m = archive_F[close_idx]\n                # break early if budget consumed\n                if evals >= budget:\n                    break\n\n            # normalize scale weights to avoid numerical overflow & keep minimal probability\n            self.scale_weights = np.maximum(self.scale_weights, 1e-8)\n            self.scale_weights = self.scale_weights / np.sum(self.scale_weights)\n\n            # limit success_buffer size and keep as y-steps (n-d vectors)\n            if len(success_buffer) > self.pca_buffer_max:\n                success_buffer = success_buffer[-self.pca_buffer_max:]\n            # convert success_buffer elements to array form for PCA later\n            try:\n                success_buffer = [np.asarray(s).reshape(-1) for s in success_buffer]\n            except Exception:\n                pass\n\n            # stagnation detection: if no improvement recently, escalate exploration\n            if (evals - last_improvement_eval) > self.stagnation_threshold:\n                # heavy-tailed jump: Lévy/Cauchy step scaled by variable range\n                c = np.random.standard_cauchy(size=n)\n                c = c / (np.linalg.norm(c) + 1e-20)\n                jump = self.levy_scale * mean_range * c\n                new_m = m + jump\n                new_m = np.clip(new_m, lb, ub)\n                # evaluate the jump point if budget allows\n                if evals < budget:\n                    f_j = func(new_m)\n                    evals += 1\n                    archive_X.append(new_m.copy())\n                    archive_F.append(float(f_j))\n                    if f_j < f_opt:\n                        f_opt = float(f_j)\n                        x_opt = new_m.copy()\n                        last_improvement_eval = evals\n                    # set new center if improved, else nudge center towards some archived good point\n                    if f_j < f_m:\n                        m = new_m.copy()\n                        r = min(5.0 * mean_range, r * 1.8)\n                    else:\n                        # nudge toward global best in archive\n                        best_idx = int(np.argmin(archive_F))\n                        m = 0.5 * m + 0.5 * np.asarray(archive_X[best_idx])\n                        r = min(2.0 * mean_range, r * 1.5)\n                # clear PCA buffer to relearn directions\n                success_buffer = []\n                # slightly reset scale weights to promote exploration\n                self.scale_weights = np.ones_like(self.scale_weights)\n\n            # small decay of exploration if we're improving recently\n            if (evals - last_improvement_eval) < max(3, self.stagnation_threshold // 6):\n                # encourage successful scales slightly\n                self.scale_weights = np.maximum(self.scale_weights * 0.995, 1e-8)\n                self.scale_weights = self.scale_weights / np.sum(self.scale_weights)\n\n            # ensure center m in bounds\n            m = np.clip(m, lb, ub)\n\n            # break if budget used\n            if evals >= budget:\n                break\n\n        # final return\n        if x_opt is None:\n            x_opt = np.clip(m, lb, ub)\n            f_opt = float(func(x_opt)) if evals < budget else float(np.min(archive_F))\n        return float(f_opt), np.asarray(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "792cad44-1b70-4f9e-be4a-99117c215645", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ab9465ee-fdad-4d57-a706-b8e49e93d768", "fitness": "-inf", "name": "NovaHopper", "description": "NovaHopper is a hybrid layered-subspace heuristic that combines per-coordinate scaling, a learned low-dimensional subspace, differential-evolution-like archive differences and occasional heavy‑tailed Student-t jumps, with a mirrored sampling scheme and a modest population sized by a log-informed heuristic (lambda, mu) and linear descending recombination weights. It initializes with a relatively large global step-size (sigma0 = 0.25·range), conservative per-coordinate scales (D=0.8), and a subspace dimension k ≈ 0.6·√n, and uses modified path-length constants (cs, damps) computed from mu_eff to drive sigma adaptation. Candidate generation mixes coordinate-wise and subspace components (random beta), applies mirrored pairs to reduce variance, sometimes replaces or perturbs samples with DE-style archive differences (probability p_de, adaptive F_de) and occasionally uses Student-t jumps (p_student, df=3) for heavy tails. Adaptation updates include per-coordinate scale smoothing via IQR-to-std (c_scale=0.12), subspace learning through SVD on a rolling success buffer, sigma/path-length updates via ps, and explicit stagnation responses that inflate sigma, increase exploration probabilities, nudge the mean toward top archive points, reset buffers and clip sigma to a safe upper bound.", "code": "import numpy as np\n\nclass NovaHopper:\n    \"\"\"\n    NovaHopper: Hybrid layered-subspace heuristic for continuous (BBOB-like) problems.\n\n    Main design differences vs HALC-LM (parameters / equations changed):\n    - Population sizing and recombination: slightly different lambda heuristic and linear (descending) weights.\n    - Global step-size initialization sigma0 = 0.25*range (instead of 0.18*range).\n    - Per-coordinate scaling uses IQR-to-std conversion (robust) with smoothing c_scale = 0.12 (instead of MAD with c_d=0.18).\n    - Subspace dimension k defaults to ~0.6*sqrt(n) and buffer/subspace-update scheduling changed.\n    - Heavy tails: Student-t (df=3) jumps instead of Cauchy; different jump scaling.\n    - DE mixing: initial F_de=0.6 and p_de0=0.18; F_de adapts slowly on stagnation.\n    - Sigma/path-length adaptation uses a slightly different cs/damps choice and a damping multiplier.\n    - Stagnation response: stronger, includes mild shrink/expand cycles, nudges to a top-5 archive sample, resets small buffer.\n    - Uses mirrored sampling with pairwise generation (orthogonalization avoided for speed).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # --- Primary algorithm parameters (user-facing choices) ---\n        # Population sizing (different from HALC-LM)\n        self.lambda_ = max(8, int(5 + np.floor(2.5 * np.log(max(1, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # Subspace dimension: slightly smaller than sqrt(n) by default (~0.6*sqrt)\n        if subspace_k is None:\n            self.k = min(self.dim, max(1, int(np.ceil(0.6 * np.sqrt(self.dim)))))\n        else:\n            self.k = min(self.dim, max(1, int(subspace_k)))\n\n        # Exploration & mixing params (different defaults)\n        self.p_de0 = 0.18\n        self.F_de = 0.6\n        self.p_student0 = 0.08   # Student-t heavy-tail prob\n        self.student_df = 3      # degrees of freedom for Student-t\n        self.mirrored = True\n\n        # Robust per-coordinate scaling smoothing (IQR-based)\n        self.c_scale = 0.12\n\n        # Buffers and update frequencies\n        self.buffer_max = max(12, 10 * self.k)\n        self.subspace_update_every = max(2, 3)\n        self.max_archive = 2000\n\n        # Limits\n        self.sigma_clip_upper_factor = 1e2\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (use func.bounds if provided)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # linear descending recombination weights (different from log-weights)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = (mu + 1.0) - np.arange(1, mu + 1)  # descending  mu..1\n        weights = np.maximum(weights, 0.0)\n        if weights.sum() <= 0:\n            weights = np.ones(mu)\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length control constants (modified)\n        cs = 0.20 * (mu_eff / (n + mu_eff))  # different scaling\n        damps = 1.0 + cs + 0.25\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        # Start mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = 0.25 * np.mean(ub - lb)  # larger initial step-size\n        D = np.ones(n) * 0.8  # per-coordinate scale initial guess (slightly conservative)\n        ps = np.zeros(n)\n\n        # initial small random orthonormal subspace (n x k)\n        if self.k >= 1:\n            R = np.random.randn(n, self.k)\n            try:\n                Ufull, _ = np.linalg.qr(R)\n                U = Ufull[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        success_buffer = []\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # dynamic exploration probabilities and DE factor\n        p_de = float(self.p_de0)\n        p_student = float(self.p_student0)\n        F_de = float(self.F_de)\n\n        # stagnation tracking and thresholds\n        last_improvement_eval = 0\n        stagnation_threshold = max(30, int(6 * n))\n\n        # evaluate initial mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            f = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(f)\n            f_opt = float(f)\n            x_opt = xm.copy()\n            last_improvement_eval = evals\n\n        gen = 0\n        # main loop\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            current_lambda = int(min(lam, remaining))\n\n            # Prepare storage\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            argen_was_de = np.zeros(current_lambda, dtype=bool)  # hint whether DE used (for simple adaptation)\n\n            # Generate half unique normals for mirrored sampling to reduce variance\n            half = current_lambda // 2 if self.mirrored else current_lambda\n            base_z = np.random.randn(half, n)\n\n            idx_out = 0\n            # produce pairs (z, -z) if mirrored\n            for j in range(half):\n                z = base_z[j].copy()\n\n                # construct y mixing per-coordinate and subspace\n                if self.k > 0:\n                    # subspace contribution scaled by mean(D)\n                    z_low = np.random.randn(self.k)\n                    low = U @ z_low\n                    beta = np.random.uniform(0.2, 0.7)  # mixing coefficient different range\n                    y = (1.0 - beta) * (D * z) + beta * (np.mean(D) * low)\n                else:\n                    y = D * z\n\n                # possibly apply Student-t heavy tail jump\n                if np.random.rand() < p_student:\n                    r = np.random.standard_t(self.student_df) * 1.3\n                    dirn = z / (np.linalg.norm(z) + 1e-20)\n                    y = r * dirn * (np.median(D) + 1e-10)\n\n                x = m + sigma * y\n\n                # DE-style archive difference mutation occasionally\n                used_de = False\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n                    used_de = True\n\n                x = np.clip(x, lb, ub)\n                arx[idx_out] = x\n                arz[idx_out] = y\n                argen_was_de[idx_out] = used_de\n                idx_out += 1\n\n                # mirrored partner\n                if self.mirrored and idx_out < current_lambda:\n                    y2 = -y\n                    x2 = m + sigma * y2\n                    # allow DE mutation on mirrored as well (independent draw)\n                    used_de2 = False\n                    if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        de_mut2 = F_de * (archive_X[i1] - archive_X[i2])\n                        x2 = x2 + de_mut2\n                        used_de2 = True\n                    x2 = np.clip(x2, lb, ub)\n                    arx[idx_out] = x2\n                    arz[idx_out] = y2\n                    argen_was_de[idx_out] = used_de2\n                    idx_out += 1\n\n            # If not mirrored and odd, we may have left space: handle separately (rare)\n            while idx_out < current_lambda:\n                z = np.random.randn(n)\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U @ z_low\n                    beta = np.random.uniform(0.2, 0.7)\n                    y = (1.0 - beta) * (D * z) + beta * (np.mean(D) * low)\n                else:\n                    y = D * z\n                if np.random.rand() < p_student:\n                    r = np.random.standard_t(self.student_df) * 1.3\n                    dirn = z / (np.linalg.norm(z) + 1e-20)\n                    y = r * dirn * (np.median(D) + 1e-10)\n                x = m + sigma * y\n                used_de = False\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n                    used_de = True\n                x = np.clip(x, lb, ub)\n                arx[idx_out] = x\n                arz[idx_out] = y\n                argen_was_de[idx_out] = used_de\n                idx_out += 1\n\n            # evaluate candidates (respect budget)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection & recombination\n            idx = np.argsort(arfit)\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            # new mean and weighted y\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # path and sigma update (slightly different damping)\n            invD = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invD) * 0.95\n            norm_ps = np.linalg.norm(ps)\n\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, self.sigma_clip_upper_factor * np.mean(ub - lb) + 1e-12)\n\n            # update per-coordinate scales D using IQR robust estimate (different eq)\n            if y_sel.size > 0:\n                q75 = np.percentile(y_sel, 75, axis=0)\n                q25 = np.percentile(y_sel, 25, axis=0)\n                iqr = (q75 - q25)\n                approx_std = (iqr / 1.349)  # convert IQR to approx std\n                approx_std = np.maximum(approx_std, 1e-12)\n                D = (1.0 - self.c_scale) * D + self.c_scale * (approx_std)\n\n            # success buffer (weighted)\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.buffer_max:\n                success_buffer.pop(0)\n\n            # Subspace update: use economy SVD on buffer but less frequently\n            if (len(success_buffer) >= max(2, self.k)) and ((gen % self.subspace_update_every) == 0):\n                Y = np.vstack(success_buffer).T  # n x m\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        if U.shape[1] < self.k:\n                            pad = np.random.randn(n, self.k - U.shape[1])\n                            try:\n                                Qp, _ = np.linalg.qr(pad)\n                                U = np.hstack([U, Qp[:, : self.k - U.shape[1]]])\n                            except np.linalg.LinAlgError:\n                                pass\n                except np.linalg.LinAlgError:\n                    pass\n\n            # simple adaptation of F_de: if many selected individuals were generated with DE, slightly increase F_de\n            # (we infer DE-usage by checking the pool, not exact causality)\n            if current_lambda > 0:\n                mean_de_usage = np.mean(argen_was_de)\n                if mean_de_usage > 0.25:\n                    F_de = min(1.0, F_de * 1.02)\n                else:\n                    F_de = max(0.2, F_de * 0.995)\n\n            # stagnation handling: escalate exploration if stuck\n            if (evals - last_improvement_eval) > stagnation_threshold:\n                # expand sigma and increase mixing probabilities\n                sigma *= 2.0\n                p_de = min(0.7, p_de * 1.5)\n                p_student = min(0.45, p_student * 1.5)\n                F_de = min(1.0, F_de * 1.15)\n                # nudge mean toward a random top-5 archived point (promotes jumps)\n                if len(archive_X) >= 5:\n                    topk = np.argsort(archive_F)[:min(5, len(archive_F))]\n                    idx_pick = np.random.choice(topk)\n                    m = 0.5 * m + 0.5 * archive_X[int(idx_pick)]\n                elif len(archive_X) > 0:\n                    idx_pick = int(np.argmin(archive_F))\n                    m = 0.6 * m + 0.4 * archive_X[idx_pick]\n                # slightly inflate per-coordinate scales to encourage exploration\n                D = D * 1.3\n                # reset success buffer to relearn subspace anew\n                success_buffer = []\n                last_improvement_eval = evals\n\n            # gentle recovery of exploration prob when we recently improved\n            if (evals - last_improvement_eval) < max(3, stagnation_threshold // 6):\n                p_de = max(self.p_de0, p_de * 0.96)\n                p_student = max(self.p_student0, p_student * 0.96)\n                F_de = max(self.F_de, F_de * 0.997)\n\n            # keep mean within bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 252, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (4,1) (3,2) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "error": "In the code, line 252, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (4,1) (3,2) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "parent_ids": "792cad44-1b70-4f9e-be4a-99117c215645", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3527f1ef-5efa-4e11-a737-59b34445d980", "fitness": 0.16186540827763893, "name": "ASGMOS", "description": "ASGMOS mixes several complementary design ideas: it generates candidates by combining per-coordinate robust scales (S updated via MAD smoothing) with an adaptively learned low-rank subspace U (k ≈ ceil(sqrt(n))) and selects among multiple mutation operators via a soft bandit, while using mirrored sampling and a trust-radius to keep steps controlled. The operator palette includes isotropic Gaussian, sparse Laplace (coordinate-sparse heavy changes), heavy-tailed Cauchy moves, archive-difference DE steps (F_de=0.9), and a cheap ridge linear surrogate fit in the subspace to propose descent directions; symmetric operators are antithetically mirrored to reduce variance. Adaptation is driven by a CMA-like sigma update using a path ps and log-weights recombination (λ ≈ 4+3 log n, μ ≈ λ/2), per-coordinate scale S blending with new MAD estimates, subspace learning from a success buffer via SVD, and an archive of evaluated points for diversity and DE-style mutations. Operator probabilities are learned with an exponential/softmax of op_scores updated by reward (bandit_lr≈0.12), and stagnation triggers inflation of sigma/trust_radius, mean nudges toward good archived elites, U perturbation and buffer reset; bounds, clipping, and safety clamps (sigma/trust limits, step capping) enforce robust, bounded search.", "code": "import numpy as np\n\nclass ASGMOS:\n    \"\"\"\n    ASGMOS: Adaptive Subspace Guided Mixed-Operator Search\n\n    One-line: Combine per-coordinate robust scaling, an adaptively learned low-rank subspace,\n    and a soft bandit over multiple mutation operators (Gaussian, sparse Laplace,\n    Cauchy/Levy, archive-DE differences, and a cheap linear surrogate in subspace)\n    with mirrored sampling and trust-region control for noiseless continuous optimization.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population sizing (CMA-like heuristic)\n        self.lambda_ = max(6, int(4 + np.floor(3.0 * np.log(max(1, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace dimension default: moderate low-rank ~ sqrt(n)\n        if subspace_k is None:\n            self.k = min(self.dim, max(1, int(np.ceil(np.sqrt(self.dim)))))\n        else:\n            self.k = min(self.dim, max(1, int(subspace_k)))\n\n        # robust smoothing for per-coordinate scale (MAD)\n        self.c_scale = 0.16\n\n        # success buffer and archive sizes\n        self.buffer_max = max(30, 10 * self.k)\n        self.archive_max = 2000\n\n        # operator set: 0=Gaussian,1=Sparse-Laplace,2=Cauchy,3=Archive-DE,4=Subspace-LinearModel\n        self.num_ops = 5\n        # initial operator probabilities (uniform-ish but slightly favor Gaussian)\n        init = np.array([0.36, 0.18, 0.12, 0.22, 0.12], dtype=float)\n        self.op_prob0 = init / np.sum(init)\n\n        # DE scale\n        self.F_de = 0.9\n\n        # trust region and sigma settings\n        self.sigma0 = 0.2 * 10.0 / 5.0  # will be reinitialized w.r.t bounds below; placeholder\n        self.trust_radius = None\n\n        # bandit learning rate for operator weights\n        self.bandit_lr = 0.12\n\n        # stagnation thresholds\n        self.stagnation_evals = max(30, 8 * self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # sanity: Many Affine BBOB tests use [-5,5] but accept func.bounds\n        domain_range = ub - lb\n        avg_range = float(np.mean(domain_range))\n        # sensible defaults\n        sigma = max(1e-12, 0.18 * avg_range)\n        self.sigma0 = sigma\n        self.trust_radius = max(0.5 * avg_range, 0.1)\n\n        # recombination weights (log-based)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path constants (CMA-inspired, but used loosely)\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # state\n        m = np.random.uniform(lb, ub)  # initial mean\n        S = np.ones(n)                  # per-coordinate robust scale (std-like)\n        ps = np.zeros(n)                # path for sigma\n        U = np.zeros((n, self.k))       # learned subspace (columns orthonormal)\n        if self.k >= 1:\n            R = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(R)\n                U[:, :Q.shape[1]] = Q[:, :self.k]\n            except Exception:\n                # leave zeros region handled\n                pass\n\n        # archive and buffer\n        archive_X = []\n        archive_F = []\n        success_buffer = []\n\n        # operator bandit\n        op_scores = np.ones(self.num_ops) * 1.0\n        op_probs = self.op_prob0.copy()\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_impr_eval = 0\n\n        # initial evaluation at mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            f = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(float(f))\n            f_opt = float(f)\n            x_opt = xm.copy()\n            last_impr_eval = evals\n\n        gen = 0\n\n        # helper: fit cheap linear surrogate in subspace U using recent archive\n        def fit_linear_subspace(archX, archF, Umat, max_pts=60, reg=1e-6):\n            # returns vector in full space that points to predicted downhill direction (approx gradient)\n            if Umat is None or Umat.shape[1] == 0:\n                return None\n            mA = len(archX)\n            if mA < min(6, Umat.shape[1] + 2):\n                return None\n            # take last up to max_pts\n            X = np.vstack(archX[-max_pts:])  # m x n\n            y = np.array(archF[-max_pts:])   # m\n            Xc = X - X.mean(axis=0, keepdims=True)\n            # project to subspace\n            Z = Xc.dot(Umat)  # m x k\n            # add bias column\n            Zb = np.hstack([Z, np.ones((Z.shape[0], 1))])\n            # target: predict f value (we want direction of negative gradient of f)\n            # solve ridge: w = (Zb^T Zb + reg I)^{-1} Zb^T y\n            try:\n                A = Zb.T.dot(Zb) + reg * np.eye(Zb.shape[1])\n                rhs = Zb.T.dot(y)\n                w = np.linalg.solve(A, rhs)\n                wk = w[:-1]  # coefficients in subspace\n                # estimated gradient in full space approx = U * wk\n                grad_est = Umat.dot(wk)\n                # sign: we want direction of decrease -> negative gradient\n                return -grad_est\n            except Exception:\n                return None\n\n        # main loop\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # produce base normals for mirrored sampling if we will mirror\n            base_z = np.random.randn(current_lambda, n)\n            cand_X = np.zeros((current_lambda, n))\n            cand_y = np.zeros((current_lambda, n))\n            cand_ops = np.full(current_lambda, -1, dtype=int)\n\n            # update operator probabilities (softmax of scores)\n            # use temperature to avoid extreme probabilities\n            temp = 0.8\n            raw = np.array(op_scores, dtype=float)\n            # ensure positive\n            raw = np.maximum(raw, 1e-8)\n            ex = np.exp((raw - raw.max()) / temp)\n            op_probs = ex / np.sum(ex)\n\n            # optional cheap model (fit once per generation)\n            model_dir = fit_linear_subspace(archive_X, archive_F, U, max_pts=80)\n\n            for i in range(current_lambda):\n                # choose operator\n                op = np.random.choice(self.num_ops, p=op_probs)\n                cand_ops[i] = op\n\n                # mirrored: for even i we sample, for odd we mirror previous except for operators which are non-symmetric (model/DE)\n                mirror = (i % 2 == 1)\n                z = base_z[i].copy()\n\n                # base stochastic mixing: subspace component and coordinate scaling\n                if self.k > 0 and U.shape[1] > 0:\n                    z_low = np.random.randn(U.shape[1])\n                    low = U.dot(z_low)\n                    alpha = np.random.uniform(0.35, 0.85)\n                    base_y = alpha * (S * z) + (1.0 - alpha) * (np.mean(S) * low)\n                else:\n                    base_y = S * z\n\n                y = np.zeros(n)\n\n                if op == 0:\n                    # Gaussian-like exploratory step (isotropic mix)\n                    y = base_y * np.random.uniform(0.85, 1.15)\n\n                elif op == 1:\n                    # Sparse Laplace for coordinate-wise heavy sparse changes\n                    # choose random subset of coordinates to modify (sparsity)\n                    sparsity = max(1, int(np.ceil(0.12 * n)))\n                    idxs = np.random.choice(n, size=sparsity, replace=False)\n                    # Laplace samples center 0 scale S[idxs]\n                    b = S[idxs] * np.random.uniform(0.6, 1.4, size=idxs.shape)\n                    lap = np.random.laplace(loc=0.0, scale=b)\n                    y[idxs] = lap\n                    # add small noise elsewhere\n                    other = np.setdiff1d(np.arange(n), idxs)\n                    if len(other) > 0:\n                        y[other] = 0.02 * base_y[other]\n\n                elif op == 2:\n                    # Cauchy/Levy heavy-tailed directional move along normalized base_y\n                    dirn = base_y / (np.linalg.norm(base_y) + 1e-20)\n                    scale = (np.median(S) + 1e-10)\n                    r = np.random.standard_cauchy() * np.random.uniform(0.6, 1.2)\n                    y = r * dirn * scale\n                    # cap extremely large values from Cauchy\n                    y = np.clip(y, -50.0 * scale, 50.0 * scale)\n\n                elif op == 3:\n                    # Archive-DE difference mutation (if archive has enough)\n                    if len(archive_X) >= 2:\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                        y = base_y + de_mut / (np.linalg.norm(de_mut) + 1e-12) * (np.mean(S) * 0.9)\n                    else:\n                        y = base_y\n\n                elif op == 4:\n                    # Subspace linear model proposed descent (if available)\n                    if model_dir is not None:\n                        # take step in direction of model_dir scaled adaptively\n                        # use projected magnitude in subspace\n                        dnorm = np.linalg.norm(model_dir)\n                        if dnorm < 1e-12:\n                            y = 0.5 * base_y\n                        else:\n                            # scale relative to current sigma and trust radius\n                            gamma = np.random.uniform(0.6, 1.8)\n                            y = gamma * (model_dir / (dnorm + 1e-20)) * (np.mean(S) * np.clip(sigma / (self.sigma0 + 1e-20), 0.25, 4.0))\n                            # add some exploration\n                            y = 0.6 * y + 0.4 * (0.12 * base_y)\n                    else:\n                        y = base_y\n\n                # mirrored antithetic variance reduction for symmetric operators\n                if mirror and op in (0, 1, 2):\n                    y = -y\n\n                # enforce trust radius (euclidean)\n                y_norm = np.linalg.norm(y)\n                max_step = max(1e-12, self.trust_radius / (sigma + 1e-20))\n                if y_norm > max_step:\n                    y = y / y_norm * max_step\n\n                # candidate x\n                x = m + sigma * y\n\n                # Occasionally augment with tiny random coordinate jitter to break ties\n                if np.random.rand() < 0.02:\n                    jitter = 1e-3 * np.random.randn(n) * S\n                    x = x + jitter\n\n                # DE-like repair: small probability, nudge towards archive best\n                if (np.random.rand() < 0.03) and len(archive_X) > 0:\n                    best_idx = int(np.argmin(archive_F))\n                    x = 0.5 * x + 0.5 * archive_X[best_idx]\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                cand_X[i] = x\n                cand_y[i] = y\n\n            # evaluate candidates up to remaining budget\n            cand_f = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = cand_X[i]\n                f = func(x)\n                evals += 1\n                cand_f[i] = float(f)\n\n                # archive\n                archive_X.append(x.copy())\n                archive_F.append(float(f))\n                if len(archive_X) > self.archive_max:\n                    # pop oldest\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n\n                if f < f_opt - 1e-15:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n                    last_impr_eval = evals\n\n            # selection + recombination\n            idx = np.argsort(cand_f)\n            sel_idx = idx[:mu]\n            x_sel = cand_X[sel_idx]\n            y_sel = cand_y[sel_idx]\n\n            m_old = m.copy()\n            # recombine weighted mean\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # path update (diag-inv heuristic using S)\n            invS = 1.0 / (S + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invS)\n            norm_ps = np.linalg.norm(ps)\n\n            # sigma adaptation (CMA-like but damped)\n            adapt_factor = np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma *= adapt_factor\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e3 * avg_range + 1e-12)\n\n            # update per-coordinate scales S via robust MAD on selected y's\n            if y_sel.size > 0:\n                med = np.median(y_sel, axis=0)\n                mad = np.median(np.abs(y_sel - med), axis=0)\n                approx_std = 1.4826 * (mad + 1e-12)\n                S = (1.0 - self.c_scale) * S + self.c_scale * (approx_std + 1e-12)\n                # prevent scales becoming too small\n                S = np.maximum(S, 1e-12)\n\n            # store weighted success step for low-rank learning\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace U from success buffer via SVD every few generations\n            if len(success_buffer) >= max(3, self.k) and (gen % 3 == 0):\n                Y = np.vstack(success_buffer).T  # n x m\n                Yc = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    U_new, svals, Vt = np.linalg.svd(Yc, full_matrices=False)\n                    take = min(self.k, U_new.shape[1])\n                    if take > 0:\n                        U[:, :take] = U_new[:, :take]\n                        if take < self.k:\n                            # pad remaining columns with small noise orthonormalized\n                            pad = np.random.randn(n, self.k - take)\n                            Qp, _ = np.linalg.qr(pad)\n                            U[:, take:self.k] = Qp[:, :self.k - take]\n                    # else leave U unchanged\n                except Exception:\n                    pass\n\n            # update operator bandit scores: reward proportional to improvement by operator-produced candidates\n            # For each operator, compute best improvement among its candidates\n            base_best = f_opt\n            # but we prefer relative improvement compared to m_old's evaluation if available in archive; fallback to current f_opt\n            # compute per-op reward\n            for op_idx in range(self.num_ops):\n                mask = (cand_ops == op_idx)\n                if not np.any(mask):\n                    # small decay\n                    op_scores[op_idx] = (1.0 - 0.02) * op_scores[op_idx]\n                    continue\n                fvals = cand_f[mask]\n                # best candidate of this op\n                if np.all(np.isinf(fvals)):\n                    # none evaluated, small decay\n                    op_scores[op_idx] *= 0.985\n                    continue\n                best_f = np.nanmin(fvals)\n                # reward: positive when we find improvement relative to m_old's immediate neighborhood\n                # estimate baseline as median archive best over recent history\n                baseline = min(base_best, np.median(archive_F[-min(len(archive_F), 20):]))\n                reward = max(0.0, baseline - best_f)\n                # scale reward by step magnitude to favor informative moves\n                # normalize by (1 + |baseline|)\n                norm_reward = reward / (1.0 + abs(baseline))\n                # update score with learning rate\n                op_scores[op_idx] = (1.0 - self.bandit_lr) * op_scores[op_idx] + self.bandit_lr * (norm_reward + 1e-8)\n\n            # stagnation handling: if no improvement for a while, oscillatory restart and increase exploration\n            if (evals - last_impr_eval) > self.stagnation_evals:\n                # increase sigma, expand trust radius, nudge mean towards a random elite and reset small parts\n                sigma *= 1.9\n                self.trust_radius *= 1.3\n                # nudge mean to a random good archived point (1st quartile)\n                if len(archive_X) > 0:\n                    q = np.percentile(archive_F, 30)\n                    candidates = [i for i, fv in enumerate(archive_F) if fv <= q]\n                    if len(candidates) > 0:\n                        pick = np.random.choice(candidates)\n                        m = 0.5 * m + 0.5 * archive_X[pick]\n                # reinitialize some columns of U and inflate S to escape basin\n                U += 0.03 * np.random.randn(*U.shape)\n                S *= 1.25\n                # clear success buffer to relearn subspace\n                success_buffer = []\n                last_impr_eval = evals\n\n            # gentle decay of exploration after improvement\n            if (evals - last_impr_eval) < max(5, int(0.2 * self.stagnation_evals)):\n                # shrink trust radius a bit and reduce sigma gently\n                self.trust_radius = max(0.25 * avg_range, 0.98 * self.trust_radius)\n                sigma = max(self.sigma0 * 0.05, 0.995 * sigma)\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # safety clamp: never let sigma overflow\n            sigma = np.clip(sigma, 1e-12, 1e2 * avg_range + 1e-12)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ASGMOS scored 0.162 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "792cad44-1b70-4f9e-be4a-99117c215645", "operator": null, "metadata": {"aucs": [0.050658240821706446, 0.1553895422935636, 0.3761633361349731, 0.13939714067141762, 0.13203012171140016, 0.12278645956649592, 0.1613148716151186, 0.24241067380026338, 0.12946988468022724, 0.10903381148122349]}, "task_prompt": ""}
{"id": "b4f3cb6f-b25f-4260-87eb-7f1249c0e3b8", "fitness": 0.22014279102165454, "name": "ARDS", "description": "The ARDS design is a hybrid strategy that mixes population-based selection (lambda/mu sizing with linear-decreasing recombination weights) with differential-evolution-style archive differences and occasional Student-t heavy-tailed perturbations to balance exploration and exploitation. It maintains per-coordinate robust scaling D (initialized conservatively as ~0.5*mean(range)/sqrt(n) and updated via MAD smoothing with c_d=0.08) and a global step-size sigma (initialized as sigma0_fraction≈0.08 of the range) that is adapted by a simple one‑fifth‑inspired success-rate controller (adapt_up=1.12, adapt_down=0.88) using a rolling recent_successes window. A learned low-rank subspace U (k ≈ ceil(sqrt(n)/1.6)) is updated from a bounded success_buffer via SVD to inject coordinated search directions, while an archive of evaluated points provides DE mutation (F_de=0.9, p_de≈0.35) and diversity; p_student (≈0.15) and p_de are nudged toward baselines when improving. Practical safeguards include hard clipping to bounds, archive/cap memory, stagnation handling that boosts sigma and nudges the mean toward the best archived point, and modest defaults for buffer_max and success_window to control adaptation responsiveness.", "code": "import numpy as np\n\nclass ARDS:\n    \"\"\"\n    ARDS (Adaptive Rotated Differential-Student)\n\n    One-line: Hybrid DE + robust coordinate scaling + learned low-rank directions with Student-t heavy tails\n    and a simple success-rate based sigma adaptation (one-fifth inspired), designed for BBOB-like bounded problems.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 lambda_factor=8, mu_ratio=0.33, subspace_k=None,\n                 sigma0_fraction=0.08, c_d=0.08,\n                 p_de=0.35, F_de=0.9, p_student=0.15,\n                 buffer_max=None, success_window=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population sizing (different formula than HALC-LM)\n        self.lambda_ = max(4, int(lambda_factor + np.ceil(2.0 * np.sqrt(max(1, self.dim)))))\n        self.mu = max(1, int(np.floor(self.lambda_ * mu_ratio)))\n\n        # subspace size default: smaller than sqrt(n) to diversify\n        if subspace_k is None:\n            self.k = min(self.dim, max(1, int(np.ceil(np.sqrt(self.dim) / 1.6))))\n        else:\n            self.k = min(self.dim, max(1, int(subspace_k)))\n\n        # exploration and mutation parameters (different defaults)\n        self.sigma0_fraction = float(sigma0_fraction)  # fraction of (ub-lb) mean\n        self.c_d = float(c_d)                           # MAD smoothing\n        self.p_de0 = float(p_de)\n        self.F_de = float(F_de)\n        self.p_student0 = float(p_student)\n\n        # memory sizes\n        self.buffer_max = buffer_max if buffer_max is not None else max(10, 6 * self.k)\n        self.success_window = success_window if success_window is not None else max(20, 6 * self.mu)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (expected func.bounds.lb/ub)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        size_range = np.mean(ub - lb)\n        # initial mean randomly in bounds\n        m = np.random.uniform(lb, ub)\n\n        # initial global step-size smaller than HALC-LM (different)\n        sigma = max(1e-12, self.sigma0_fraction * size_range)\n\n        # coordinate-wise robust scale D (init to small constant)\n        D = np.ones(n) * (0.5 * (ub - lb).mean() / max(1.0, np.sqrt(n)))\n\n        # learned low-rank subspace U (n x k), start random orthonormal\n        if self.k > 0:\n            R = np.random.randn(n, self.k)\n            try:\n                U, _ = np.linalg.qr(R)\n                U = U[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        # buffers and archive\n        success_buffer = []    # store recent successful steps (y-space)\n        archive_X = []         # archive points for DE differences\n        archive_F = []\n\n        # dynamic probabilities\n        p_de = float(self.p_de0)\n        p_student = float(self.p_student0)\n\n        # tracking best\n        f_best = np.inf\n        x_best = None\n\n        evals = 0\n\n        # initial evaluation of the mean\n        if evals < budget:\n            x0 = np.clip(m, lb, ub)\n            f0 = func(x0); evals += 1\n            archive_X.append(x0.copy()); archive_F.append(float(f0))\n            f_best = float(f0); x_best = x0.copy()\n\n        # success tracking window for one-fifth-like rule\n        recent_successes = []  # booleans of whether candidate improved global best\n        target_success = 0.2   # target success fraction ~ 1/5\n        adapt_up = 1.12        # multiplicative increase when success_rate > target\n        adapt_down = 0.88      # multiplicative decrease otherwise\n        gen = 0\n\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n\n            # sample lam candidate directions\n            candidates = np.zeros((lam, n))\n            candidate_y = np.zeros((lam, n))\n            for i in range(lam):\n                # base normal scaled by D\n                z = np.random.randn(n) * D\n\n                # small randomized mixing into low-rank directions\n                if self.k > 0 and np.random.rand() < 0.75:\n                    # draw low-rank coefficients with smaller variance\n                    z_low = np.random.randn(self.k) * 0.6\n                    low_dir = U @ z_low\n                    mix = np.random.beta(2.0, 4.0)  # skew towards using main coords\n                    y = (1.0 - mix) * low_dir + mix * z\n                else:\n                    y = z\n\n                # occasional Student-t heavy-tailed step (different distribution & tuning)\n                if np.random.rand() < p_student:\n                    # Student-t with moderate df to allow heavier tails than Gaussian but lighter than Cauchy\n                    df = 3.0\n                    t = np.random.standard_t(df, size=1)[0]\n                    # select a direction aligned with z but with student magnitude\n                    dirn = z / (np.linalg.norm(z) + 1e-20)\n                    y = (np.median(D) + 1e-10) * float(t) * dirn\n\n                x = m + sigma * y\n\n                # DE-like archive difference mutation (current-to-archive-best with prob p_de)\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    # pick two archived points and apply scaled difference\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # clip within bounds\n                x = np.clip(x, lb, ub)\n                candidates[i] = x\n                candidate_y[i] = y\n\n            # evaluate candidates until budget or done\n            fitness = np.full(lam, np.inf)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = candidates[i]\n                fi = func(xi); evals += 1\n                fitness[i] = float(fi)\n                archive_X.append(xi.copy()); archive_F.append(float(fi))\n                # maintain archive cap modestly to keep differences useful\n                if len(archive_X) > 2000:\n                    archive_X.pop(0); archive_F.pop(0)\n                # update global best\n                improved = False\n                if fi < f_best:\n                    f_best = float(fi)\n                    x_best = xi.copy()\n                    improved = True\n                recent_successes.append(bool(improved))\n                if len(recent_successes) > self.success_window:\n                    recent_successes.pop(0)\n\n            # selection: choose top mu candidates by fitness\n            valid_idx = np.argsort(fitness)\n            mu = min(self.mu, lam)\n            sel_idx = valid_idx[:mu]\n            x_sel = candidates[sel_idx]\n            y_sel = candidate_y[sel_idx]\n\n            # recombine via linear decreasing weights (different weighting scheme)\n            w = np.arange(mu, 0, -1, dtype=float)\n            w = w / np.sum(w)\n            m_old = m.copy()\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # compute weighted step in y-space for buffer and adaptation\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # update MAD-based per-coordinate scale (different smoothing equation)\n            if y_sel.size > 0:\n                med = np.median(y_sel, axis=0)\n                mad = np.median(np.abs(y_sel - med), axis=0)\n                approx_std = 1.4826 * (mad + 1e-12)\n                # slightly conservative smoothing (different coefficient)\n                D = (1.0 - self.c_d) * D + self.c_d * (approx_std + 1e-12)\n\n            # store successful (weighted) step into buffer for subspace re-learning\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.buffer_max:\n                success_buffer.pop(0)\n\n            # Occasionally update low-rank subspace using SVD on centered buffer (economical)\n            if (len(success_buffer) >= max(2, self.k)) and (gen % 3 == 0):\n                Y = np.vstack(success_buffer).T  # n x m\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        # If U has fewer columns than k, pad with orthonormal random directions\n                        if U.shape[1] < self.k:\n                            pad = np.random.randn(n, self.k - U.shape[1])\n                            try:\n                                Qp, _ = np.linalg.qr(pad)\n                                U = np.hstack([U, Qp[:, : self.k - U.shape[1]]])\n                            except np.linalg.LinAlgError:\n                                pass\n                except np.linalg.LinAlgError:\n                    # keep existing U on failure\n                    pass\n\n            # sigma adaptation: simple one-fifth-inspired success-rate controller (different from CMA path-length)\n            if len(recent_successes) >= 5:\n                success_rate = float(np.sum(recent_successes)) / len(recent_successes)\n                if success_rate > target_success:\n                    sigma *= adapt_up\n                else:\n                    sigma *= adapt_down\n                # gentle restoration of exploration probabilities towards baseline when improving\n                if success_rate > min(0.5, target_success * 2.0):\n                    p_de = max(self.p_de0, p_de * 0.98)\n                    p_student = max(self.p_student0, p_student * 0.98)\n            # ensure sigma stays in reasonable bounds distinct from HALC-LM's hard limits\n            sigma = np.clip(sigma, 1e-12, 2.0 * size_range + 1e-12)\n\n            # stagnation handling: if many generations without improvement, perform a focused restart\n            # define stagnation as no improvement in last success_window evaluations\n            if len(recent_successes) == self.success_window and np.sum(recent_successes) == 0:\n                # widen exploration: boost sigma moderately, increase DE use and student-t chance\n                sigma *= 2.2\n                p_de = min(0.8, p_de * 1.5)\n                p_student = min(0.5, p_student * 1.8)\n                # nudge mean towards best archive member with some randomness\n                if len(archive_X) > 0:\n                    best_idx = int(np.argmin(archive_F))\n                    m = 0.5 * m + 0.5 * archive_X[best_idx] + np.random.randn(n) * (0.05 * size_range)\n                    m = np.clip(m, lb, ub)\n                # clear buffers but retain archive\n                success_buffer = []\n                recent_successes = []\n                # decrease D slightly to encourage exploration across coords\n                D = D * (1.0 + 0.2 * np.random.rand(n))\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ARDS scored 0.220 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "792cad44-1b70-4f9e-be4a-99117c215645", "operator": null, "metadata": {"aucs": [0.1279406823294732, 0.17725272994733, 0.28029291326583694, 0.2886779543685918, 0.2303184213509748, 0.26013902477734674, 0.2277116895377438, 0.23186210468528723, 0.21502307392287578, 0.16220931603108513]}, "task_prompt": ""}
{"id": "39c529c4-2666-4dd5-a571-0af112999c05", "fitness": 0.33165146280389823, "name": "HALE", "description": "HALE is a hybrid strategy that mixes CMA-like global step-size control (ps, cs, damps, chi_n, recombination weights, lambda_/mu sizing) with a learned low-rank search subspace (U of dimension k, default ≈ sqrt(n)) and per-coordinate robust scaling D to handle anisotropy. Candidate generation uses a mixture of diagonal (D * z) and low-rank (U @ z_low) samples with randomized alpha and an adaptive low_rank_beta from a recent success buffer, plus variance-reducing mirrored/antithetic pairs, occasional heavy-tailed Cauchy jumps (p_cauchy0) and archive-based differential mutations (p_de0, F_de) for global escapes. Adaptation is robust: D is updated by an MAD-like estimator with exponential smoothing (c_d), the mean is recombined from the top mu individuals (weights, mu_eff), sigma is adapted via path-length update on an approximate invsqrtC action (diagonal invdiag), and safeguards keep sigma, D and m within bounds. Operational mechanics include an archive with capped size and DE differences, periodic SVD subspace updates from a success buffer, stochastic escalation on stagnation (inflate sigma and exploration probabilities and nudge m toward best archive point), and strict budget/bounds enforcement.", "code": "import numpy as np\n\nclass HALE:\n    \"\"\"\n    HALE - Hybrid Adaptive Low-rank Ensemble\n\n    One-line: Combine robust per-coordinate MAD scaling and a learned low-rank subspace\n    with CMA-like path-length sigma control, mirrored sampling, Cauchy/Lévy jumps and\n    archive DE differences; adapt exploration intensity on stagnation.\n\n    Constructor: HALE(budget=10000, dim=10, seed=None, subspace_k=None)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population sizing (conservative, CMA-like)\n        self.lambda_ = max(6, int(4 + np.floor(3.0 * np.log(max(1, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n        # subspace dimension: prefer moderate low-rank (sqrt(n)) but allow override\n        if subspace_k is None:\n            self.k = min(self.dim, max(1, int(np.ceil(np.sqrt(max(1, self.dim))))))\n        else:\n            self.k = min(self.dim, max(1, int(subspace_k)))\n        # robust smoothing for D (MAD-like)\n        self.c_d = 0.17\n        # exploration probabilities baseline\n        self.p_de0 = 0.22\n        self.p_cauchy0 = 0.10\n        # other params\n        self.mirrored = True\n        self.max_archive = 4000\n        self.subspace_update_every = 5  # generations\n        self.success_buffer_max = max(12, 8 * self.k)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # recombination weights (log based) and mu-eff\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length constants (CMA-like)\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        m = np.random.uniform(lb, ub)                 # mean inside bounds\n        sigma = 0.16 * np.mean(ub - lb)               # conservative start\n        D = np.ones(n)                                # per-coordinate std (robust)\n        ps = np.zeros(n)                              # path for sigma control\n\n        # low-rank subspace U (n x k) orthonormal initial\n        if self.k >= 1:\n            R = np.random.randn(n, self.k)\n            try:\n                U, _ = np.linalg.qr(R)\n                U = U[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        # buffers and archive\n        success_buffer = []\n        archive_X = []\n        archive_F = []\n\n        # dynamic exploration params\n        p_de = float(self.p_de0)\n        p_cauchy = float(self.p_cauchy0)\n        F_de = 0.8\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation at mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(float(fm))\n            f_opt = float(fm)\n            x_opt = xm.copy()\n            last_improvement_eval = evals\n        else:\n            last_improvement_eval = 0\n\n        gen = 0\n        stagnation_base = max(20, int(5 * n))\n\n        # main loop: produce generations until budget exhausted\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # create base normals; use mirrored sampling if possible\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            base_z = np.random.randn(current_lambda, n)\n\n            # adaptive mixing strength for low-rank part based on recent singular values\n            # if no strong modes, reduce low-rank weight\n            low_rank_beta = 0.6\n            # estimate a simple strength from success_buffer singular values if available\n            if len(success_buffer) >= max(2, self.k):\n                Ytmp = np.vstack(success_buffer).T  # n x m\n                try:\n                    s = np.linalg.svd(Ytmp, compute_uv=False)\n                    if s.size > 0:\n                        low_rank_beta = 0.5 + 0.5 * min(1.0, float(s[0]) / (1e-6 + np.mean(s)))\n                        # clamp\n                        low_rank_beta = np.clip(low_rank_beta, 0.2, 0.95)\n                except np.linalg.LinAlgError:\n                    pass\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n                # make a low-rank sample\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U @ z_low  # n-dim\n                    # randomized mix to diversify\n                    alpha = np.random.uniform(0.35, 0.85)\n                    y = alpha * (D * z) + (1.0 - alpha) * (np.mean(D) * low) * low_rank_beta\n                else:\n                    y = D * z\n\n                # mirrored / antithetic sampling to reduce variance\n                if self.mirrored and (i % 2 == 1):\n                    y = -y\n\n                # occasional heavy-tailed jump to escape local minima\n                if np.random.rand() < p_cauchy:\n                    r = np.random.standard_cauchy() * 1.0\n                    dirn = z / (np.linalg.norm(z) + 1e-20)\n                    y = r * dirn * (np.median(D) + 1e-10)\n\n                x = m + sigma * y\n\n                # occasional DE-style archive difference mutation\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # strict clipping\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates (sequentially, never exceeding budget)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = float(f)\n                archive_X.append(x.copy())\n                archive_F.append(float(f))\n                # bound archive size\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0)\n                    archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n                    last_improvement_eval = evals\n\n            # selection and recombination (handle possible inf values if some evals not done)\n            valid_idx = np.arange(current_lambda)\n            # sort by fitness, ignoring any inf (they will be last)\n            idx = np.argsort(arfit)\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n            m_old = m.copy()\n            # recombine to new mean\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # approximate inverse-square-root action on y_w using diagonal approx (1/D)\n            invdiag = 1.0 / (D + 1e-20)\n            invsqrtC_y = y_w * invdiag\n\n            # update path for sigma\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * invsqrtC_y\n            norm_ps = np.linalg.norm(ps)\n            # adapt sigma\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            # safeguard sigma\n            sigma = max(sigma, 1e-12)\n            sigma = min(sigma, 1e2 * np.mean(ub - lb) + 1e-12)\n\n            # Update per-coordinate scales D via a robust MAD-like estimate from selected y's\n            if y_sel.size > 0:\n                med = np.median(y_sel, axis=0)\n                mad = np.median(np.abs(y_sel - med), axis=0)\n                approx_std = 1.4826 * (mad + 1e-12)\n                # combine with previous D using exponential smoothing on std\n                D = (1.0 - self.c_d) * D + self.c_d * (approx_std + 1e-12)\n\n            # store weighted success y_w into success buffer\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.success_buffer_max:\n                success_buffer.pop(0)\n\n            # update low-rank subspace U occasionally via SVD on buffer\n            if (len(success_buffer) >= max(2, self.k)) and ((gen % self.subspace_update_every) == 0):\n                Y = np.vstack(success_buffer).T  # n x m\n                Y = Y - np.mean(Y, axis=1, keepdims=True)\n                try:\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        # pad if necessary\n                        if U.shape[1] < self.k:\n                            pad = np.random.randn(n, self.k - U.shape[1])\n                            try:\n                                Qp, _ = np.linalg.qr(pad)\n                                U = np.hstack([U, Qp[:, : self.k - U.shape[1]]])\n                            except np.linalg.LinAlgError:\n                                pass\n                except np.linalg.LinAlgError:\n                    pass\n\n            # stagnation detection and escalation\n            if (evals - last_improvement_eval) > stagnation_base:\n                # escalate exploration conservatively\n                sigma *= 1.8\n                p_de = min(0.7, p_de * 1.5)\n                p_cauchy = min(0.5, p_cauchy * 1.5)\n                # nudge mean toward a better archived point (best)\n                if len(archive_X) > 0:\n                    best_idx = int(np.argmin(archive_F))\n                    m = 0.6 * m + 0.4 * archive_X[best_idx]\n                # clear success buffer to force new modes\n                success_buffer = []\n                last_improvement_eval = evals  # avoid repeated rapid escalation\n\n            # mild recovery of exploration parameters when improving\n            if (evals - last_improvement_eval) < (stagnation_base // 4):\n                p_de = max(self.p_de0, p_de * 0.95)\n                p_cauchy = max(self.p_cauchy0, p_cauchy * 0.95)\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # small safeguard for D\n            D = np.maximum(D, 1e-8)\n\n            # loop continues until budget exhausted\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HALE scored 0.332 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "792cad44-1b70-4f9e-be4a-99117c215645", "operator": null, "metadata": {"aucs": [0.21679130580859884, 0.16791462519891986, 0.5156537908716572, 0.7025843895758863, 0.2771046841607383, 0.40343973784682696, 0.2398473517346309, 0.3587038203100389, 0.26253176895609587, 0.1719431535755891]}, "task_prompt": ""}
{"id": "e6361dd6-9212-456e-b8fd-35fcbb752de7", "fitness": "-inf", "name": "LevyRotationalHybrid", "description": "The Levy-Rotational Hybrid blends a CMA-style global search (mirrored sampling, lambda/mu recombination with Lehmer-like weights, covariance matrix C updated with rank-one and rank-mu terms and occasional eigendecomposition) with median-success sigma adaptation (target_s=0.2, moderate damps) and a small initial sigma (0.25 of range) to balance exploration/exploitation. Robust global diversification is provided by archive-driven differential-mutations (p_de≈0.28) and Mantegna–Levy heavy-tailed jumps (p_levy≈0.08, levy_beta=1.5, extra jumps with prob 0.12), while a space-filling greedy max–min initialization (init_ratio, pool_factor) and archive pruning (max_archive ~ max(3000,80*n)) keep samples well distributed and memory bounded. Local intensification uses a distance-weighted separable-quadratic surrogate fit around the best (model_neighbor_multiplier * n neighbors), per-dimension trust radii (init ~0.15*range, expand/shrink rules 1.6/0.65) and a principal-eigen correction to capture non-separable curvature, with guarded evaluation via safe_eval and iteration caps (max_eval_per_iter, eig_every).", "code": "import numpy as np\n\nclass LevyRotationalHybrid:\n    \"\"\"\n    Levy-Rotational Hybrid (LRH)\n\n    One-line: Combine rotational CMA/DE-style global sampling and median-success sigma adaptation\n    (ARDE_Enhanced flavor) with AELQS-inspired space-filling initialization, a local distance-weighted\n    separable-quadratic surrogate plus principal-eigen correction, per-dimension trust radii, and\n    archive-driven DE and Mantegna-Levy jumps for robust exploration/exploitation balance.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_ratio=0.12, min_init=None,\n                 p_de=0.28, p_levy=0.08, levy_beta=1.5,\n                 model_neighbor_multiplier=6,\n                 max_eval_per_iter=80, eigen_every_factor=6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.init_ratio = float(init_ratio)\n        self.min_init = int(min_init) if min_init is not None else max(8, 2 * self.dim)\n        self.p_de = float(p_de)\n        self.p_levy = float(p_levy)\n        self.levy_beta = float(levy_beta)\n        self.model_neighbor_multiplier = int(model_neighbor_multiplier)\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        self.eigen_every_factor = int(eigen_every_factor)\n\n    def _levy_mantegna(self, n, alpha=1.5, scale=1.0, rng=None):\n        # Mantegna's algorithm\n        if rng is None:\n            rng = np.random.default_rng()\n        sigma_u = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                   (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n        u = rng.normal(0, sigma_u, size=n)\n        v = rng.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1.0 / alpha) + 1e-20)\n        return scale * step\n\n    def __call__(self, func):\n        rng = np.random.default_rng(self.seed)\n        n = int(self.dim)\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        rng_mean = 0.5 * (ub + lb)\n\n        # dynamic state (CMA-like)\n        lam = max(6, int(6 + 4 * np.log(max(1, n))))\n        mu = max(2, lam // 2)\n        # Lehmer-like weights (similar to ARDE_Enhanced but robust to mu)\n        raw = (np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))) ** 1.15\n        weights = raw / np.sum(raw)\n        mu_eff = (np.sum(weights) ** 2) / np.sum(weights ** 2)\n\n        # adaptation constants (moderate choices)\n        cc = 0.6 / np.sqrt(n + mu_eff)\n        cs = 0.3 / (1.0 + mu_eff / (n + 1.0))\n        c1 = 1.0 / ((n + 2.0) ** 1.5)\n        cmu = min(0.6 * (1 - c1), 0.5 * mu_eff / (mu_eff + 2.0) * (1 - c1))\n        damps = 1.0 + cs + 1.5 * max(0.0, np.sqrt(mu_eff / (n + 1.0)) - 1.0)\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        m = rng.uniform(lb, ub)\n        sigma = 0.25 * np.mean(rng_range)\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eigen_eval_counter = 0\n        eig_every = max(1, int(self.eigen_every_factor * n))\n\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # -- Space-filling initialization (greedy max-min from pool) --\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, int(0.25 * budget)))\n        init_budget = max(1, init_budget)\n        pool_factor = 6\n        pool_size = max(init_budget * pool_factor, init_budget + 20)\n        pool = rng.uniform(lb, ub, size=(int(pool_size), n))\n        chosen = []\n        for i in range(init_budget):\n            if i == 0:\n                idx = rng.integers(0, pool.shape[0])\n                chosen.append(pool[idx])\n            else:\n                cur = np.array(chosen)\n                dists = np.min(np.linalg.norm(pool - cur[:, None, :], axis=2), axis=0)\n                idx = int(np.argmax(dists))\n                chosen.append(pool[idx])\n        for x in chosen:\n            if evals >= budget:\n                break\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            archive_X.append(x_c.copy()); archive_F.append(float(f))\n            if f < f_opt:\n                f_opt = float(f); x_opt = x_c.copy()\n        if evals >= budget:\n            return float(f_opt), np.array(x_opt, dtype=float)\n\n        # trust radii per-dim (for surrogate proposals)\n        trust_radius = np.maximum(0.15 * rng_range, 1e-8)\n        trust_min = 1e-9 * rng_range\n        trust_max = 3.0 * rng_range\n        success_expand = 1.6\n        failure_shrink = 0.65\n\n        # safe eval helper\n        def safe_eval(x):\n            nonlocal evals, f_opt, x_opt\n            if evals >= budget:\n                return None\n            x_c = np.clip(x, lb, ub)\n            f = func(x_c)\n            evals += 1\n            archive_X.append(x_c.copy()); archive_F.append(float(f))\n            if f < f_opt - 1e-12:\n                f_opt = float(f); x_opt = x_c.copy()\n            return float(f), x_c\n\n        # helper: fit distance-weighted separable quadratic around center\n        def fit_separable_quad(center, neigh_X, neigh_F):\n            dx = neigh_X - center\n            mrows = dx.shape[0]\n            M = np.ones((mrows, 1 + 2 * n))\n            M[:, 1:1 + n] = dx\n            M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n            y = neigh_F\n            dists = np.linalg.norm(dx, axis=1)\n            bandwidth = np.median(dists) + 1e-12\n            w = np.exp(-(dists ** 2) / (2.0 * (bandwidth ** 2 + 1e-12)))\n            if np.max(w) > 0:\n                w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            bvec = W * y\n            ridge = 1e-6 * (1.0 + np.mean(np.abs(y)))\n            try:\n                params, *_ = np.linalg.lstsq(A.T @ A + ridge * np.eye(M.shape[1]), A.T @ bvec, rcond=None)\n                params = params.flatten()\n                a = params[0]\n                b_lin = params[1:1 + n]\n                h_diag = params[1 + n:1 + 2 * n]\n                h_reg = np.copy(h_diag)\n                h_reg[h_reg < 1e-8] = 1e-8\n                return a, b_lin, h_reg\n            except Exception:\n                return None\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # mirrored sampling when even\n            if current_lambda % 2 == 0:\n                half = current_lambda // 2\n                arz_half = rng.standard_normal(size=(half, n))\n                arz = np.vstack([arz_half, -arz_half])\n            else:\n                arz = rng.standard_normal(size=(current_lambda, n))\n\n            BD = B * D[np.newaxis, :]\n            ary = arz @ BD.T\n            arx = m + sigma * ary\n\n            # DE archive-driven mutations and Levy jumps\n            F_de = 0.6 + 0.4 * (1.0 - evals / max(1, budget))\n            for k in range(current_lambda):\n                if (rng.random() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = rng.integers(0, len(archive_X), size=2)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    arx[k] = arx[k] + de_mut\n                if rng.random() < self.p_levy:\n                    levy = self._levy_mantegna(n, alpha=self.levy_beta, scale=0.5 * sigma, rng=rng)\n                    arx[k] = arx[k] + levy * 0.5 * sigma\n                arx[k] = np.clip(arx[k], lb, ub)\n\n            # Evaluate offspring\n            arfit = np.full(current_lambda, np.inf)\n            for k in range(current_lambda):\n                if evals >= budget:\n                    break\n                xk = arx[k]\n                f = func(xk)\n                evals += 1\n                arfit[k] = float(f)\n                archive_X.append(xk.copy()); archive_F.append(float(f))\n                if f < f_opt:\n                    f_opt = float(f); x_opt = xk.copy()\n\n            # selection\n            valid = np.isfinite(arfit)\n            if not np.any(valid):\n                break\n            sel_mu = min(mu, int(np.sum(valid)))\n            idx = np.argsort(arfit)[:sel_mu]\n            x_sel = arx[idx]\n            y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n\n            # update mean by weighted recombination\n            if sel_mu < mu:\n                w_sub = weights[:sel_mu]\n                w_sub = w_sub / (np.sum(w_sub) + 1e-20)\n                m_old = m.copy()\n                m = np.sum(w_sub[:, None] * x_sel, axis=0)\n                y_w = np.sum(w_sub[:, None] * y_sel, axis=0)\n            else:\n                m_old = m.copy()\n                m = np.sum(weights[:, None] * x_sel, axis=0)\n                y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # success rate relative to best seen around mean or median fallback\n            if len(archive_F) > 0:\n                recent_parent = archive_F[-1]\n            else:\n                recent_parent = np.inf\n            if np.isfinite(recent_parent):\n                s_rate = np.sum(arfit < recent_parent) / max(1, current_lambda)\n            else:\n                med = np.median(arfit[valid])\n                s_rate = np.sum(arfit < med) / max(1, current_lambda)\n\n            # paths and covariance update\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n            norm_ps = np.linalg.norm(ps)\n            hsig = 1.0 if (norm_ps / (np.sqrt(1.0 - (1.0 - cs) ** (2.0 * max(1, evals / max(1, current_lambda)))) + 1e-20) / chi_n) < (1.5 + 1.0 / (n + 1.0)) else 0.0\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n\n            rank_one = np.outer(pc, pc)\n            rank_mu = np.zeros((n, n))\n            w_for = w_sub if ('w_sub' in locals() and sel_mu < mu) else weights\n            for i in range(sel_mu):\n                yi = y_sel[i][:, None]\n                rank_mu += w_for[i] * (yi @ yi.T)\n            C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n\n            # median-success sigma adaptation (dimension-aware)\n            target_s = 0.2\n            adapt_strength = 0.6 / (1.0 + 0.05 * n)\n            sigma *= np.exp(adapt_strength * (s_rate - target_s) / (damps + 1e-20))\n            sigma = max(sigma, 1e-12)\n\n            # optional evaluate mean if it moved significantly\n            if evals < budget and np.linalg.norm(m - m_old) > 1e-12:\n                xm = np.clip(m, lb, ub)\n                fm = func(xm)\n                evals += 1\n                archive_X.append(xm.copy()); archive_F.append(float(fm))\n                if fm < f_opt:\n                    f_opt = float(fm); x_opt = xm.copy()\n\n            # eigen decomposition occasionally\n            eigen_eval_counter += current_lambda\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(n); B = np.eye(n); D = np.ones(n); invsqrtC = np.eye(n)\n\n            # --- Local surrogate intensification around current best (archive-driven) ---\n            work_allow = min(self.max_eval_per_iter, max(0, budget - evals))\n            improved = False\n            if work_allow > 0 and len(archive_X) >= max(2 * n + 1, self.model_neighbor_multiplier * n):\n                X_arr = np.asarray(archive_X)\n                F_arr = np.asarray(archive_F)\n                # center on best-so-far\n                center = np.asarray(x_opt)\n                dists = np.linalg.norm(X_arr - center, axis=1)\n                idx_sorted = np.argsort(dists)\n                m_neighbors = min(len(X_arr), max(2 * n + 1, self.model_neighbor_multiplier * n))\n                idx_nei = idx_sorted[:m_neighbors]\n                X_nei = X_arr[idx_nei]\n                F_nei = F_arr[idx_nei]\n                quad = fit_separable_quad(center, X_nei, F_nei)\n                if quad is not None:\n                    a_q, b_q, h_q = quad\n                    delta_q = -b_q / (h_q + 1e-20)\n                    delta_q = np.clip(delta_q, -trust_radius, trust_radius)\n                    x_model = np.clip(center + delta_q, lb, ub)\n                    # principal eigen correction\n                    dx = X_nei - center\n                    # weighted covariance\n                    try:\n                        dists_local = np.linalg.norm(dx, axis=1)\n                        w = np.exp(-(dists_local ** 2) / (2.0 * (np.median(dists_local) ** 2 + 1e-12)))\n                        Wdiag = np.sqrt(np.clip(w / (np.max(w) + 1e-12), 0, 1))\n                        Xw = (Wdiag[:, None] * dx).T\n                        cov_local = Xw @ Xw.T + 1e-12 * np.eye(n)\n                        vals, vecs = np.linalg.eigh(cov_local)\n                        principal = vecs[:, -1]\n                        # estimate directional slope\n                        proj = dx @ principal\n                        if np.ptp(proj) > 1e-12:\n                            s = np.sum(w * proj * F_nei) / (np.sum(w * proj * proj) + 1e-12)\n                            step_dir = -np.sign(s) * np.minimum(np.linalg.norm(trust_radius), 0.7 * np.linalg.norm(trust_radius))\n                            x_corr = np.clip(center + principal * step_dir, lb, ub)\n                        else:\n                            x_corr = x_model.copy()\n                    except Exception:\n                        x_corr = x_model.copy()\n                    # evaluate model candidates (model then correction)\n                    for x_cand in (x_model, x_corr):\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        out = safe_eval(x_cand)\n                        work_allow -= 1\n                        if out is None:\n                            break\n                        f_cand, x_cand = out\n                        if f_cand < f_opt - 1e-12:\n                            improved = True\n                            # expand trust radii moderately\n                            trust_radius = np.minimum(trust_radius * success_expand, trust_max)\n                            break\n                    if not improved:\n                        trust_radius = np.maximum(trust_radius * failure_shrink, trust_min)\n\n            # occasional heavy-tailed global jump driven by best\n            if evals < budget and rng.random() < 0.12:\n                t_sample = self._levy_mantegna(n, alpha=1.5, scale=0.6, rng=rng)\n                t_sample = np.clip(t_sample, -15, 15)\n                scale = 0.5 * np.maximum(rng_range, 1e-12)\n                x_jump = np.clip(x_opt + t_sample * scale, lb, ub)\n                out = safe_eval(x_jump)\n                if out is not None:\n                    fj, xj = out\n                    if fj < f_opt - 1e-12:\n                        # successful jump expands trust\n                        trust_radius = np.minimum(trust_radius * 2.2, trust_max)\n                    else:\n                        trust_radius = np.maximum(trust_radius * 0.5, trust_min)\n\n            # prune archive occasionally\n            max_archive = max(3000, 80 * n)\n            if len(archive_X) > max_archive:\n                idx_sorted = np.argsort(archive_F)\n                keep_best = idx_sorted[:400]\n                rest_idx = idx_sorted[400:]\n                stride = max(1, len(rest_idx) // (max_archive - 400))\n                keep_rest = rest_idx[::stride]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # quick termination if extremely good\n            if f_opt <= 1e-12:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "792cad44-1b70-4f9e-be4a-99117c215645", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a287d5bc-8ac4-48ca-b242-0c5973921bbb", "fitness": 0.25728286182908194, "name": "AMSSG", "description": "The algorithm runs a small swarm of local \"anchors\" (pop_size) initialized uniformly and evaluated under a budget-aware safe_eval that enforces bounds and records an archive. Each anchor builds a low-rank sketch basis (k ≈ n^sketch_exp) from a small LRU memory of recent successful unit directions plus random fill, then issues many cheap probes (≈ probes_per_dim * k) with isotropic jitter and momentum to collect (delta_x, delta_f) pairs. It fits a regularized linear model in the sketch coordinates to recover a quasi-gradient, lifts it to full space and performs mirror-like/projected negative-gradient updates with adaptive trust radii (grow/shrink factors, chaotic logistic multiplier), conservative acceptance rules, immediate pivot on probe improvement, and small momentum updates. Global control uses archive pruning, occasional reseeding of the worst anchor near the global best, explicit min/max step bounds and early stopping criteria; numeric choices (mem_size, probes_per_dim, init_step, min_step, sketch_exp) bias the method toward cautious, subspace-based intensification with periodic exploration.", "code": "import numpy as np\n\nclass AMSSG:\n    \"\"\"\n    Adaptive Mirror-Swarm with Sketch Gradients (AMSSG)\n\n    One-line: A population (\"swarm\") of local anchors runs many cheap subspace probes\n    (built from a small memory of successful directions + random fill), fits a low-rank\n    linear model (delta_f ≈ g^T delta_x) in the sketch subspace and uses the recovered\n    quasi-gradient for mirror-like updates with adaptive trust-radii; chaotic step\n    scheduling and archive-based reseeding add exploration.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, pop_size=6, mem_size=16,\n                 sketch_exp=0.65, probes_per_dim=3, init_step=0.6,\n                 min_step=1e-6, max_archive=2000, seed=None):\n        \"\"\"\n        budget: total function evaluations allowed\n        dim: problem dimension\n        pop_size: number of anchors / micro-clusters\n        mem_size: memory of recent successful unit directions\n        sketch_exp: exponent to determine sketch dimension ~ n^sketch_exp\n        probes_per_dim: probes multiplier per sketch-dimension\n        init_step: initial step/trust radius (interpreted wrt problem scale)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = int(pop_size)\n        self.mem_size = int(mem_size)\n        self.sketch_exp = float(sketch_exp)\n        self.probes_per_dim = int(probes_per_dim)\n        self.init_step = float(init_step)\n        self.min_step = float(min_step)\n        self.max_archive = int(max_archive)\n        self.seed = seed\n\n    def __call__(self, func):\n        n = self.dim\n\n        # Get bounds if provided by func, otherwise use BBOB default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        domain_mean = float(np.mean(domain_range))\n\n        rng = np.random.default_rng(self.seed)\n\n        # safe evaluation wrapper that respects budget and records archive\n        evals = 0\n        X_archive = []\n        F_archive = []\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            if evals >= self.budget:\n                return None, None\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x.copy())\n            F_archive.append(f)\n            if f < f_best:\n                f_best = f\n                x_best = x.copy()\n            return f, x\n\n        # initialize anchors (pop_size) uniformly in domain (budget-aware)\n        anchors_x = []\n        anchors_f = []\n        anchors_step = []\n        anchors_mom = []\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            x0 = rng.uniform(lb, ub)\n            out = safe_eval(x0)\n            if out[0] is None:\n                break\n            anchors_x.append(out[1].copy())\n            anchors_f.append(out[0])\n            anchors_step.append(self.init_step * domain_mean)\n            anchors_mom.append(np.zeros(n))\n        # if none evaluated, return worst-case\n        if len(anchors_x) == 0:\n            return float(f_best if x_best is not None else np.inf), np.array(x_best if x_best is not None else np.zeros(n))\n\n        # memory of successful unit directions (LRU)\n        dir_memory = []\n\n        # chaotic scheduler (logistic map) to slightly perturb step sizes\n        logistic = 0.6180339887498949  # seed\n        def chaotic_update():\n            nonlocal logistic\n            logistic = 4.0 * logistic * (1.0 - logistic)\n            return 0.8 + 0.4 * logistic  # multiplier in [0.8,1.2]\n\n        # helper to compute sketch dimension\n        def sketch_dim():\n            k = max(1, int(np.clip(int(np.ceil(n ** self.sketch_exp)), 1, n)))\n            return k\n\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            # choose an anchor to work on:\n            # preference to better anchors but include exploration: softmin probabilities\n            fs = np.array(anchors_f, dtype=float)\n            # guard if any inf\n            fs = np.where(np.isfinite(fs), fs, np.max(fs[np.isfinite(fs)]) + 1e6)\n            best_idx = int(np.argmin(fs))\n            # softmin transform\n            temp = 0.7\n            inv = np.exp(- (fs - np.min(fs)) / (np.std(fs) + 1e-9) / (temp + 1e-9))\n            probs = inv / np.sum(inv)\n            idx = int(rng.choice(len(anchors_x), p=probs))\n            anchor_x = anchors_x[idx].copy()\n            anchor_f = anchors_f[idx]\n            anchor_step = anchors_step[idx]\n            anchor_mom = anchors_mom[idx]\n\n            # compute sketch basis from memory + random fill\n            k = sketch_dim()\n            use_mem = min(len(dir_memory), max(0, k // 2))\n            if use_mem > 0:\n                mem_vecs = np.array(dir_memory[:use_mem])  # shape (use_mem, n)\n                R = np.column_stack((mem_vecs.T, rng.standard_normal(size=(n, k - use_mem))))\n            else:\n                R = rng.standard_normal(size=(n, k))\n            # QR -> orthonormal basis\n            try:\n                Q, _ = np.linalg.qr(R, mode='reduced')\n                basis = Q[:, :k]\n            except Exception:\n                R2 = rng.standard_normal(size=(n, k))\n                Q, _ = np.linalg.qr(R2)\n                basis = Q[:, :k]\n\n            # generate many cheap probes in the sketch subspace\n            probes = max(4, self.probes_per_dim * k)\n            deltas_coeffs = []\n            deltas_x = []\n            deltas_f = []\n            improved = False\n            # adaptive local noise scale\n            isotropic_noise_scale = 0.08 * anchor_step\n\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample in basis: normal vector with scale = anchor_step\n                coeffs = rng.normal(scale=anchor_step, size=k)\n                dx = basis @ coeffs\n                # add small isotropic jitter and momentum\n                jitter = rng.normal(scale=isotropic_noise_scale, size=n)\n                cand = anchor_x + 0.6 * anchor_mom + 0.4 * dx + jitter\n                cand = np.clip(cand, lb, ub)\n                out = safe_eval(cand)\n                if out[0] is None:\n                    break\n                f_cand = out[0]\n                # store difference relative to anchor\n                deltas_coeffs.append(coeffs.copy())\n                deltas_x.append(cand - anchor_x)\n                deltas_f.append(f_cand - anchor_f)\n                # if improvement, update memory and anchor immediate mild move\n                if f_cand < anchor_f - 1e-12:\n                    improved = True\n                    # store direction (unit)\n                    dir_succ = cand - anchor_x\n                    dn = np.linalg.norm(dir_succ)\n                    if dn > 0:\n                        unit = dir_succ / dn\n                        dir_memory.insert(0, unit.copy())\n                        if len(dir_memory) > self.mem_size:\n                            dir_memory.pop()\n                    # small immediate pivot: a conservative acceptance to encourage intensification\n                    alpha_local = 0.25\n                    anchor_x =  (1.0 - alpha_local) * anchor_x + alpha_local * cand\n                    anchor_f = min(anchor_f, f_cand)\n                    # increase local radius (trust)\n                    anchor_step = min(anchor_step * 1.18 * chaotic_update(), 5.0 * domain_mean)\n                    anchor_mom = 0.7 * anchor_mom + 0.3 * (cand - anchor_x)\n                    # record back to anchors\n                    anchors_x[idx] = anchor_x.copy()\n                    anchors_f[idx] = anchor_f\n                    anchors_step[idx] = anchor_step\n                    anchors_mom[idx] = anchor_mom.copy()\n\n            # If we collected enough probes, fit a linear model in the sketch subspace:\n            # deltas_f ≈ (basis^T grad)^T coeffs  => solve A g_b = b  for g_b = basis^T grad\n            m = len(deltas_coeffs)\n            if m >= min(3, k):\n                A = np.vstack(deltas_coeffs)  # (m, k)\n                b = np.array(deltas_f, dtype=float)  # (m,)\n                # regularized least squares for stability\n                lam_reg = 1e-6 * (np.linalg.norm(b) + 1e-9)\n                try:\n                    # solve (A^T A + lam I) g_b = A^T b\n                    ATA = A.T @ A\n                    ATb = A.T @ b\n                    g_b = np.linalg.solve(ATA + lam_reg * np.eye(ATA.shape[0]), ATb)\n                except Exception:\n                    g_b, *_ = np.linalg.lstsq(A, b, rcond=None)\n                # lift gradient estimate to full space\n                grad_est = basis @ g_b  # approximate gradient at anchor\n                grad_norm = np.linalg.norm(grad_est) + 1e-12\n\n                # determine mirror-like step size with trust scaling\n                eta = 0.8 * anchor_step / (grad_norm + 1e-12)\n                eta = np.clip(eta, 1e-8, 2.0 * anchor_step)\n\n                # propose a quasi-Newtonish update: projected negative gradient (mirror descent style)\n                cand_move = anchor_x - eta * (grad_est / (np.linalg.norm(grad_est) + 1e-12))\n                cand_move = np.clip(cand_move, lb, ub)\n\n                if evals < self.budget:\n                    out = safe_eval(cand_move)\n                    if out[0] is not None:\n                        f_move = out[0]\n                        # accept if better or apply conservative acceptance rule\n                        if f_move < anchor_f - 1e-12 or rng.random() < 0.15:\n                            # update anchor\n                            anchor_mom = 0.6 * anchor_mom + 0.4 * (cand_move - anchor_x)\n                            anchor_x = 0.85 * anchor_x + 0.15 * cand_move\n                            anchor_f = min(anchor_f, f_move)\n                            # if improved strongly, enlarge trust, else shrink slightly\n                            if f_move < anchors_f[idx] - 1e-8:\n                                anchor_step = min(anchor_step * 1.3 * chaotic_update(), 6.0 * domain_mean)\n                            else:\n                                anchor_step = max(self.min_step * domain_mean, anchor_step * 0.9)\n                            anchors_x[idx] = anchor_x.copy()\n                            anchors_f[idx] = anchor_f\n                            anchors_step[idx] = anchor_step\n                            anchors_mom[idx] = anchor_mom.copy()\n\n            # If no improvement in this anchor round, shrink trust and occasionally reseed worst anchor\n            if not improved:\n                anchors_step[idx] = max(self.min_step * domain_mean, anchors_step[idx] * 0.78)\n                # occasional reseed of the worst anchor when stagnation detected\n                if rng.random() < 0.06 and remaining > max(4, n):\n                    # find worst anchor and reinitialize near global best with jitter\n                    worst = int(np.argmax(np.array(anchors_f, dtype=float)))\n                    jitter_scale = 0.6 * anchors_step[worst]\n                    if x_best is not None:\n                        new_x = np.clip(x_best + rng.normal(scale=jitter_scale, size=n), lb, ub)\n                    else:\n                        new_x = rng.uniform(lb, ub)\n                    if evals < self.budget:\n                        out = safe_eval(new_x)\n                        if out[0] is not None:\n                            anchors_x[worst] = out[1].copy()\n                            anchors_f[worst] = out[0]\n                            anchors_step[worst] = self.init_step * domain_mean\n                            anchors_mom[worst] = np.zeros(n)\n\n            # Archive pruning\n            if len(X_archive) > self.max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                step_keep = max(1, len(rest) // (self.max_archive - 200))\n                keep_rest = rest[::step_keep]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # quick stop if excellent\n            if f_best <= 1e-12:\n                break\n\n        # return global best\n        if x_best is None:\n            # fallback to best anchor\n            best_anchor_idx = int(np.argmin(np.array(anchors_f, dtype=float)))\n            return float(anchors_f[best_anchor_idx]), np.clip(anchors_x[best_anchor_idx], lb, ub)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AMSSG scored 0.257 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "9d9f78ad-978b-4980-9123-24520d7bfe63", "operator": null, "metadata": {"aucs": [0.17788753304587457, 0.16528118649377177, 0.3476369505616479, 0.3537404337086959, 0.22430521638961043, 0.30625493124965475, 0.24783609375602522, 0.3197729205735914, 0.2717908058887608, 0.1583225466231868]}, "task_prompt": ""}
{"id": "f34d0af3-0e8c-4216-9e8e-1cf98c47f6b3", "fitness": "-inf", "name": "MGEDO", "description": "MGEDO builds a low-rank PCA-like subspace from a short deque of recent successful step vectors (mem_size=24) and top archive differences, then performs many cheap Differential-Evolution-style proposals inside that subspace (subspace_frac=0.4 → k≈0.4·n, pop_mult=6 → pop_size≈6·k) to focus search on promising directions while keeping exploration via DE mutation (F with small jitter) and binomial crossover (CR ~0.9). Proposals are mapped back to full space and combined with an EMA momentum vector (momentum_decay=0.85) and Langevin isotropic noise (sigma scaled from sigma_init=0.30·domain_mean) whose scale is adaptively increased/decreased by a sliding-window success statistic (adapt_window=30, target success ≈20%, grow≈1.22, shrink≈0.72) to regulate exploration. To escape subspace stagnation the algorithm occasionally injects orthogonal probes (p_rotate=0.08) computed by QR, stores successful step vectors into dir_memory for subsequent PCA, and keeps a bounded archive with prioritized best points for donor selection. Practical safeguards include initialization from several random starts, strict bound clipping, archive pruning, small adjustments of F/CR on success, and early stop when f_best is tiny.", "code": "import numpy as np\nfrom collections import deque\n\nclass MGEDO:\n    \"\"\"\n    MGEDO: Momentum-Guided Ensemble Differential Evolution with Orthogonal Probing\n\n    One-line: Build a low-rank PCA subspace from recent successful steps and run many\n    cheap DE-style subspace proposals mixed with momentum and Langevin noise;\n    adapt step-size by observed success-rate and occasionally inject orthogonal probes.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 mem_size=24, subspace_frac=0.4, pop_mult=6,\n                 p_rotate=0.08, F_init=0.7, CR_init=0.9, sigma_init=0.30,\n                 momentum_decay=0.85, adapt_window=30):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.mem_size = int(mem_size)\n        self.subspace_frac = float(subspace_frac)\n        self.pop_mult = int(pop_mult)\n        self.p_rotate = float(p_rotate)\n        self.F = float(F_init)\n        self.CR = float(CR_init)\n        self.sigma_init = float(sigma_init)\n        self.momentum_decay = float(momentum_decay)\n        self.adapt_window = int(adapt_window)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        n = self.dim\n\n        # bounds (Many BBOB uses -5..5, but respect func.bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        domain_mean = float(np.mean(domain_range))\n\n        rng = np.random.default_rng(self.seed)\n\n        # state\n        X_archive = []\n        F_archive = []\n        dir_memory = deque(maxlen=self.mem_size)  # store recent successful step vectors (not necessarily unit)\n        momentum = np.zeros(n)\n        sigma = max(1e-12, self.sigma_init * domain_mean)  # isotropic noise scale\n        min_sigma = 1e-9\n        max_sigma = 5.0 * domain_mean\n\n        # control for adaptive sigma using a sliding window of successes\n        recent_attempts = deque(maxlen=self.adapt_window)\n        grow = 1.22\n        shrink = 0.72\n        min_step = 1e-8 * max(1.0, domain_mean)\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # safe eval wrapper\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = np.clip(x, lb, ub)\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x.copy())\n            F_archive.append(f)\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initialize center (mean) by a few random starts to avoid pathological start\n        m = rng.uniform(lb, ub)\n        out0 = safe_eval(m.copy())\n        if out0[0] is None:\n            # budget zero\n            return float(f_best if x_best is not None else np.inf), np.array(x_best if x_best is not None else np.clip(m, lb, ub), dtype=float)\n\n        # small initial randomization of m\n        for _ in range(min(5, max(0, self.budget - evals))):\n            x0 = rng.uniform(lb, ub)\n            out = safe_eval(x0)\n            if out[0] is None:\n                break\n            # set best as initial mean if better\n            if out[0] < f_best:\n                m = x0.copy()\n\n        # helper: build low-rank PCA-ish basis from recent success steps in dir_memory or top archive diffs\n        def build_subspace(k):\n            # collect step vectors; if not enough in memory, derive from top archive entries\n            vecs = []\n            for v in dir_memory:\n                vecs.append(v.copy())\n            if len(vecs) < k:\n                # get differences between top archive points and current mean\n                if len(X_archive) >= 2:\n                    idx_sorted = np.argsort(F_archive)\n                    for i in idx_sorted[:min(len(idx_sorted), self.mem_size)]:\n                        vec = X_archive[i] - m\n                        if np.linalg.norm(vec) > 0:\n                            vecs.append(vec.copy())\n                        if len(vecs) >= k:\n                            break\n            # if still insufficient, fill with random\n            while len(vecs) < k:\n                vecs.append(rng.normal(size=n))\n            # form matrix and perform economy SVD to get orthonormal basis\n            M = np.column_stack(vecs[:k])\n            try:\n                # center columns to remove large mean bias\n                U, S, Vt = np.linalg.svd(M, full_matrices=False)\n                basis = U[:, :k]\n            except Exception:\n                R = rng.normal(size=(n, k))\n                Q, _ = np.linalg.qr(R)\n                basis = Q[:, :k]\n            return basis\n\n        # main loop\n        while evals < self.budget:\n            remaining = self.budget - evals\n            # adapt subspace dimension\n            k = max(1, min(n, int(np.ceil(n * self.subspace_frac))))\n            pop_size = max(6, self.pop_mult * k)\n            basis = build_subspace(k)  # shape (n, k)\n\n            # Prepare DE-style population in subspace: represent m in subspace coords (zero vector)\n            # We'll create pop_size candidates by subspace mutation + momentum + Langevin noise\n            successes_in_iter = 0\n            attempts_in_iter = 0\n\n            for p in range(pop_size):\n                if evals >= self.budget:\n                    break\n                attempts_in_iter += 1\n\n                # choose three distinct archive indices for DE donors (if available), otherwise random normals\n                if len(X_archive) >= 3:\n                    i1, i2, i3 = rng.choice(len(X_archive), size=3, replace=False)\n                    a = X_archive[i1]\n                    b = X_archive[i2]\n                    c = X_archive[i3]\n                    # represent donors relative to mean in subspace coords\n                    ya = basis.T @ (a - m)\n                    yb = basis.T @ (b - m)\n                    yc = basis.T @ (c - m)\n                else:\n                    # fall back to random vectors in subspace\n                    ya = rng.normal(scale=0.3 * domain_mean, size=k)\n                    yb = rng.normal(scale=0.3 * domain_mean, size=k)\n                    yc = rng.normal(scale=0.3 * domain_mean, size=k)\n\n                # mutation (DE/rand/1 in subspace)\n                F = self.F * (0.8 + 0.4 * (rng.random() - 0.5))  # small jitter around F\n                y_mut = ya + F * (yb - yc)\n\n                # crossover with target vector (we use target = 0 vector representing m)\n                # binomial crossover\n                mask = rng.random(size=k) < self.CR\n                if not np.any(mask):\n                    mask[rng.integers(0, k)] = True\n                y_trial = np.where(mask, y_mut, 0.0)\n\n                # map back to full space: trial = m + basis @ y_trial + momentum + Langevin noise\n                trial = m + basis @ y_trial\n                # incorporate momentum as a small shift (directional)\n                if np.linalg.norm(momentum) > 0:\n                    trial = trial + 0.35 * (momentum * domain_mean)\n                # Langevin-style isotropic noise\n                trial = trial + rng.normal(scale=sigma, size=n)\n                trial = np.clip(trial, lb, ub)\n\n                out = safe_eval(trial)\n                if out[0] is None:\n                    break\n                f_trial = out[0]\n\n                # record attempt for adapt logic\n                recent_attempts.append(1 if f_trial < f_best - 1e-12 else 0)\n\n                # success handling\n                if f_trial < f_best - 1e-12:\n                    successes_in_iter += 1\n                    # store step vector relative to mean\n                    step_vec = trial - m\n                    if np.linalg.norm(step_vec) > 0:\n                        dir_memory.appendleft(step_vec.copy())  # LRU via deque: appendleft keeps newest at start\n                    # moderately move mean toward trial\n                    m = 0.75 * m + 0.25 * trial\n                    # update momentum (EMA of normalized step)\n                    s = trial - m\n                    norm_s = np.linalg.norm(s)\n                    if norm_s > 0:\n                        unit_s = s / norm_s\n                        momentum = self.momentum_decay * momentum + (1.0 - self.momentum_decay) * unit_s\n                    # encourage larger sigma when successes happen\n                    sigma = min(max_sigma, sigma * grow)\n                    # adapt F/CR slightly towards more exploration if success came from large mutation\n                    self.F = 0.9 * self.F + 0.1 * (abs(F))\n                    self.CR = 0.9 * self.CR + 0.05 * rng.random()\n                else:\n                    # small negative feedback: slightly reduce exploration if many failures\n                    pass\n\n            # after population probes adapt sigma by empirical success ratio in recent_attempts\n            if len(recent_attempts) >= max(3, int(0.5 * self.adapt_window)):\n                succ_rate = sum(recent_attempts) / len(recent_attempts)\n                target = 0.20  # desire ~20% success rate\n                if succ_rate > target:\n                    sigma = min(max_sigma, sigma * (1.0 + 0.12 * (succ_rate - target)))\n                else:\n                    sigma = max(min_sigma, sigma * (1.0 - 0.18 * (target - succ_rate)))\n\n            # occasional orthogonal rotation probing to avoid subspace stagnation\n            if rng.random() < self.p_rotate and evals < self.budget:\n                probes = min(4, max(1, n // 6))\n                # create random orthonormal directions via QR\n                R = rng.normal(size=(n, probes))\n                try:\n                    Q, _ = np.linalg.qr(R)\n                    ortho_dirs = [Q[:, i] for i in range(probes)]\n                except Exception:\n                    ortho_dirs = [rng.normal(size=n) for _ in range(probes)]\n                    ortho_dirs = [v / (np.linalg.norm(v) + 1e-20) for v in ortho_dirs]\n                for d in ortho_dirs:\n                    if evals >= self.budget:\n                        break\n                    length = rng.uniform(-0.6 * sigma, 0.6 * sigma) + rng.normal(scale=0.5 * sigma)\n                    cand = np.clip(m + length * d, lb, ub)\n                    out = safe_eval(cand)\n                    if out[0] is None:\n                        break\n                    if out[0] < f_best - 1e-12:\n                        # store direction\n                        step_vec = cand - m\n                        if np.linalg.norm(step_vec) > 0:\n                            dir_memory.appendleft(step_vec.copy())\n                        m = 0.80 * m + 0.20 * cand\n                        momentum = self.momentum_decay * momentum + (1.0 - self.momentum_decay) * (step_vec / (np.linalg.norm(step_vec) + 1e-20))\n                        sigma = min(max_sigma, sigma * grow)\n                        recent_attempts.append(1)\n\n            # archive maintenance: keep top region + spaced rest\n            max_archive = max(1500, 100 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                step_keep = max(1, len(rest) // (max_archive - 200))\n                keep_rest = rest[::step_keep]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # small safeguard and normalization\n            sigma = float(max(min_sigma, min(max_sigma, sigma)))\n            if np.linalg.norm(momentum) > 1e6:\n                momentum = momentum / (np.linalg.norm(momentum) + 1e-20)\n\n            # quick stopping if optimum found\n            if f_best <= 1e-12:\n                break\n\n            # ensure we always make progress on mean if we have very poor coverage\n            # if no successes for many iterations, nudge mean to random archive best or random point\n            if len(recent_attempts) == recent_attempts.maxlen and sum(recent_attempts) == 0:\n                if len(X_archive) > 0:\n                    best_idx = int(np.argmin(F_archive))\n                    m = 0.6 * m + 0.4 * X_archive[best_idx]\n                else:\n                    m = rng.uniform(lb, ub)\n                # reset momentum slightly\n                momentum = np.zeros(n)\n                sigma = min(max_sigma, sigma * 1.5)\n                recent_attempts.clear()\n\n        # final return\n        if x_best is None:\n            return float(np.inf), np.clip(m, lb, ub)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 50, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_memory = deque(maxlen=self.mem_size)  # store recent successful step vectors (not necessarily unit)", "error": "In the code, line 50, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_memory = deque(maxlen=self.mem_size)  # store recent successful step vectors (not necessarily unit)", "parent_ids": "9d9f78ad-978b-4980-9123-24520d7bfe63", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "85fb613c-ef17-485f-a4f6-f8905f695087", "fitness": 0.5229953635599902, "name": "SET", "description": "1) The algorithm maintains a small ensemble of momentum-coupled walkers with per-walker trust radii (\"step\") and temperatures, a default pop size scaling with dimension, and a limited LRU archive used for statistics and PCA.  \n2) Proposals are a mixture of spectral-guided Langevin-like moves inside a PCA-derived low-dimensional subspace (spec_frac, eigenvalue-weighted amplitudes), DE-rand/1 differential mixing (F_de, p_de), and orthogonal Gaussian exploration as a fallback (with p_spec controlling spectral frequency).  \n3) Success/failure drives local adaptation: velocities updated with momentum (momentum=0.6), step sizes increase on success and shrink on failure, temperatures slowly cool (and modulate spectral amplitude), and occasional 1D quadratic polishing along top eigenvectors (p_polish) performs budget-aware local refinement.  \n4) Practical safeguards include bound clipping, budget-aware safe_eval and archive trimming (archive_size), PCA computed on a recent subset for efficiency, stagnation detection that re-invigorates worst walkers around the current best, and conservative global cooling to shift from exploration to exploitation.", "code": "import numpy as np\n\nclass SET:\n    \"\"\"\n    SET: Spectral Ensemble Trust\n\n    Main idea (one-line): Maintain a small ensemble of momentum-coupled walkers and\n    exploit an archive-driven PCA (spectral subspace) to perform efficient low-dimensional\n    Langevin-like proposals, occasional DE-style mixes, and 1D quadratic polishing along\n    principal directions; trust radii and \"temperature\" adapt from success/failure history.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 pop=None, archive_size=500, spec_frac=0.35,\n                 init_step_scale=0.5, momentum=0.6, F_de=0.8,\n                 p_de=0.18, p_spec=0.7, p_polish=0.08):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.archive_size = int(archive_size)\n        self.spec_frac = float(spec_frac)  # fraction of dims used in spectral proposals\n        self.init_step_scale = float(init_step_scale)\n        self.momentum = float(momentum)\n        self.F_de = float(F_de)\n        self.p_de = float(p_de)\n        self.p_spec = float(p_spec)\n        self.p_polish = float(p_polish)\n        # ensemble size\n        if pop is None:\n            self.pop = max(4, min(18, 4 + self.dim // 2))\n        else:\n            self.pop = int(pop)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds (BBOB has bounds -5..5, but use func.bounds if present)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        domain_mean = float(np.mean(domain_range))\n\n        rng = np.random.default_rng(self.seed)\n\n        # initialize ensemble positions uniformly, velocities zero, step sizes\n        X = np.vstack([rng.uniform(lb, ub) for _ in range(self.pop)])\n        V = np.zeros((self.pop, n))\n        step = np.full(self.pop, self.init_step_scale * domain_mean)  # trust radii per walker\n        step_min = 1e-8 * max(1.0, domain_mean)\n        step_max = 3.0 * domain_mean\n        # temperatures controlling spectral amplitude\n        temp = np.linspace(1.0, 0.25, self.pop)\n\n        # archive\n        X_archive = []\n        F_archive = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # safe evaluation wrapper\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best, X_archive, F_archive\n            x = np.clip(x, lb, ub)\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            # store into archive (LRU: append)\n            X_archive.append(x.copy())\n            F_archive.append(f)\n            if len(X_archive) > self.archive_size:\n                # drop oldest to keep LRU character\n                X_archive = X_archive[-self.archive_size:]\n                F_archive = F_archive[-self.archive_size:]\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return f, x\n\n        # initial evaluations for ensemble\n        for i in range(self.pop):\n            out = safe_eval(X[i])\n            if out[0] is None:\n                break\n\n        # small utility: compute spectral subspace (PCA) from archive\n        def compute_pca(k_max=None):\n            # returns (U, S) where U shape (n, k) orthonormal, S length k (singular values)\n            if len(X_archive) < 2:\n                return np.zeros((n, 0)), np.zeros(0)\n            Xc = np.asarray(X_archive) - np.mean(X_archive, axis=0)\n            # limit rows for efficiency\n            max_rows = min(len(Xc), 200 + 5 * n)\n            if Xc.shape[0] > max_rows:\n                idx = np.linspace(0, Xc.shape[0] - 1, max_rows).astype(int)\n                Xc = Xc[idx]\n            # compute compact SVD of transposed matrix to get principal directions\n            try:\n                # covariance shortcut if dims small\n                cov = (Xc.T @ Xc) / max(1, Xc.shape[0] - 1)\n                eigvals, eigvecs = np.linalg.eigh(cov)\n                # sort descending\n                order = np.argsort(eigvals)[::-1]\n                eigvals = eigvals[order]\n                eigvecs = eigvecs[:, order]\n                # filter near-zero\n                positive = np.maximum(eigvals, 0.0)\n                k = min(n, int(np.count_nonzero(positive > 1e-16)))\n                if k == 0:\n                    return np.zeros((n, 0)), np.zeros(0)\n                if k_max is None:\n                    k_keep = k\n                else:\n                    k_keep = min(k, k_max)\n                U = eigvecs[:, :k_keep]\n                S = np.sqrt(positive[:k_keep])\n                return U, S\n            except Exception:\n                return np.zeros((n, 0)), np.zeros(0)\n\n        # 1D quadratic polish along direction with 3 point parabolic fit (budget-aware)\n        def polish_1d(x0, f0, d, init_alpha, max_evals=3):\n            # d should be unit direction\n            nonlocal evals\n            if evals >= self.budget or max_evals < 2:\n                return None, None\n            dnorm = np.linalg.norm(d)\n            if dnorm == 0:\n                return None, None\n            d = d / dnorm\n            a1 = init_alpha\n            x1 = np.clip(x0 + a1 * d, lb, ub)\n            out1 = safe_eval(x1)\n            if out1[0] is None:\n                return None, None\n            f1 = out1[0]\n            # second probe opposite direction\n            a2 = -init_alpha * 0.6\n            x2 = np.clip(x0 + a2 * d, lb, ub)\n            out2 = safe_eval(x2)\n            if out2[0] is None:\n                return None, None\n            f2 = out2[0]\n            # fit parabola through (0,f0), (a1,f1), (a2,f2)\n            denom = (a1 * a1 * (a2 - 0) + a2 * a2 * (0 - a1) + 0 * 0 * (a1 - a2))\n            # safer algebraic parabola fit using Lagrange coefficients for vertex alpha* = sum(ai*fi)/sum(...)\n            try:\n                # Using formula for vertex of parabola through (0,f0),(a1,f1),(a2,f2)\n                # compute coefficients via solving 3x3 linear system\n                A = np.array([[0.0**2, 0.0, 1.0],\n                              [a1**2, a1, 1.0],\n                              [a2**2, a2, 1.0]])\n                b = np.array([f0, f1, f2])\n                coeff = np.linalg.lstsq(A, b, rcond=None)[0]  # [A, B, C]\n                Aco = coeff[0]\n                Bco = coeff[1]\n                if abs(Aco) < 1e-18:\n                    return None, None\n                alpha_star = -Bco / (2.0 * Aco)\n                if not np.isfinite(alpha_star):\n                    return None, None\n                x_star = np.clip(x0 + alpha_star * d, lb, ub)\n                out_star = safe_eval(x_star)\n                if out_star[0] is None:\n                    return None, None\n                f_star = out_star[0]\n                if f_star < f0 - 1e-12:\n                    return f_star, x_star\n            except Exception:\n                return None, None\n            return None, None\n\n        # main loop: cycle through ensemble until budget exhausted\n        stagnation = 0\n        iters = 0\n        while evals < self.budget:\n            iters += 1\n            improved_any = False\n\n            # recompute PCA once per iteration\n            k_suggest = max(1, int(np.ceil(self.spec_frac * n)))\n            U, S = compute_pca(k_max=k_suggest)\n            k_spec = U.shape[1]\n\n            # for each walker propose moves\n            indices = np.arange(self.pop)\n            rng.shuffle(indices)\n            for idx in indices:\n                if evals >= self.budget:\n                    break\n                x = X[idx].copy()\n                f_x = None\n                # get current fitness from archive (may be repeated)\n                # we don't have per-walker f stored; check archive for exact match\n                # fallback: evaluate current point once (cheap if already evaluated earlier)\n                out0 = safe_eval(x)\n                if out0[0] is None:\n                    break\n                f_x = out0[0]\n\n                # choose move type\n                r = rng.random()\n                accepted = False\n                # 1) spectral-guided Langevin-like proposal (most often)\n                if (r < self.p_spec) and (k_spec > 0):\n                    # sample coefficients in spectral subspace with anisotropic scaling from S\n                    # amplitude scales with step[idx] and per-eigenvalue S\n                    z = rng.normal(size=k_spec)\n                    # temperature scaling: larger steps for higher temp\n                    amp = step[idx] * temp[idx]\n                    # scale by eigenvalues sqrt (S) to prefer strong directions, regularize\n                    scale = (S[:k_spec] / (np.max(S[:k_spec]) + 1e-12)) * amp\n                    z = z * scale\n                    move_spec = U[:, :k_spec] @ z\n                    # add small orthogonal noise\n                    orth_noise = rng.normal(scale=0.12 * step[idx], size=n)\n                    # subtract projection onto spectral subspace to keep orth component\n                    if k_spec > 0:\n                        orth_noise = orth_noise - U[:, :k_spec] @ (U[:, :k_spec].T @ orth_noise)\n                    x_try = np.clip(x + move_spec + orth_noise, lb, ub)\n                    out = safe_eval(x_try)\n                    if out[0] is None:\n                        break\n                    f_try = out[0]\n                    if f_try < f_x - 1e-12 or rng.random() < 0.045 * temp[idx]:\n                        # accept with small probability even if not better (exploration)\n                        X[idx] = x_try\n                        # update velocity and step\n                        V[idx] = self.momentum * V[idx] + (x_try - x)\n                        step[idx] = min(step_max, step[idx] * (1.12 if f_try < f_x else 0.98))\n                        temp[idx] = max(0.05, temp[idx] * (0.995 if f_try < f_x else 1.005))\n                        improved_any = improved_any or (f_try < f_best - 1e-12)\n                        accepted = True\n\n                # 2) differential-mixing trial (DE-rand/1) with some probability\n                if (not accepted) and (rng.random() < self.p_de) and self.pop >= 4:\n                    idxs = list(range(self.pop))\n                    idxs.remove(idx)\n                    a, b, c = rng.choice(idxs, size=3, replace=False)\n                    trial = X[a] + self.F_de * (X[b] - X[c])\n                    trial = np.clip(trial, lb, ub)\n                    out = safe_eval(trial)\n                    if out[0] is None:\n                        break\n                    if out[0] < f_x - 1e-12:\n                        X[idx] = trial\n                        V[idx] = self.momentum * V[idx] + (trial - x)\n                        step[idx] = min(step_max, step[idx] * 1.15)\n                        temp[idx] = max(0.05, temp[idx] * 0.98)\n                        improved_any = improved_any or (out[0] < f_best - 1e-12)\n                        accepted = True\n                    else:\n                        step[idx] = max(step_min, step[idx] * 0.95)\n\n                # 3) orthogonal exploratory Gaussian step (fallback)\n                if (not accepted):\n                    d = rng.normal(size=n)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                    alpha = rng.normal(scale=0.6 * step[idx])\n                    x_try = np.clip(x + alpha * d, lb, ub)\n                    out = safe_eval(x_try)\n                    if out[0] is None:\n                        break\n                    f_try = out[0]\n                    if f_try < f_x - 1e-12:\n                        X[idx] = x_try\n                        V[idx] = self.momentum * V[idx] + (x_try - x)\n                        step[idx] = min(step_max, step[idx] * 1.10)\n                        temp[idx] = max(0.05, temp[idx] * 0.99)\n                        improved_any = improved_any or (f_try < f_best - 1e-12)\n                    else:\n                        # small shrink on failure\n                        step[idx] = max(step_min, step[idx] * 0.92)\n\n                # occasional 1D polish on best walker along principal direction\n                if (rng.random() < self.p_polish) and (k_spec > 0):\n                    # pick the top eigenvector direction\n                    u1 = U[:, 0]\n                    # use last known f_x (or evaluate)\n                    f_current = None\n                    # evaluate current X[idx] (it may be just updated)\n                    out_cur = safe_eval(X[idx])\n                    if out_cur[0] is None:\n                        break\n                    f_current = out_cur[0]\n                    # polish using at most 3 extra evaluations\n                    max_allowed = min(3, self.budget - evals)\n                    if max_allowed >= 2:\n                        pol = polish_1d(X[idx], f_current, u1, init_alpha=0.9 * step[idx], max_evals=max_allowed)\n                        if pol[0] is not None:\n                            fpol, xpol = pol\n                            # accept and update\n                            X[idx] = xpol\n                            V[idx] = self.momentum * V[idx] + (xpol - x)\n                            step[idx] = min(step_max, step[idx] * 1.25)\n                            temp[idx] = max(0.05, temp[idx] * 0.95)\n                            improved_any = improved_any or (fpol < f_best - 1e-12)\n\n            # after sweep: check global improvement\n            if improved_any:\n                stagnation = 0\n            else:\n                stagnation += 1\n\n            # adapt ensemble: if stagnation high, re-invigorate worst walkers\n            if stagnation > 12:\n                # reinitialize bottom half around best with Gaussian noise scaled by global step\n                if x_best is not None:\n                    worst_idx = np.argsort([F_archive[-1] if len(F_archive) > 0 else np.inf] * self.pop)[:self.pop//2]\n                    for wi in range(self.pop // 2):\n                        i = (wi + 1) % self.pop\n                        X[i] = np.clip(x_best + rng.normal(scale=0.8 * np.mean(step)) , lb, ub)\n                        V[i] = np.zeros(n)\n                        step[i] = max(step_min, np.mean(step) * (0.6 + 0.8 * rng.random()))\n                        temp[i] = 0.9\n                # increase exploration briefly\n                step = np.minimum(step_max, step * 1.6)\n                stagnation = 0\n\n            # keep archive size under absolute cap (already handled in safe_eval)\n            # small global cooling of temperatures to favor exploitation slowly\n            temp = np.maximum(0.05, temp * 0.9998)\n\n            # quick stop if excellent\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            # fallback to best ensemble member evaluated\n            best_idx = None\n            best_f_local = np.inf\n            for i in range(self.pop):\n                out = safe_eval(X[i])\n                if out[0] is None:\n                    break\n                if out[0] < best_f_local:\n                    best_f_local = out[0]\n                    best_idx = i\n            if best_idx is None:\n                return float(np.inf), np.clip(np.zeros(n), lb, ub)\n            return float(best_f_local), np.array(X[best_idx], dtype=float)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm SET scored 0.523 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "9d9f78ad-978b-4980-9123-24520d7bfe63", "operator": null, "metadata": {"aucs": [0.1584152880890931, 0.1718402351635866, 0.6834658557346271, 0.8777163453173317, 0.7593895146614337, 0.7917627405621313, 0.25567044875054057, 0.6152684716212303, 0.7516694534157988, 0.1647552822841285]}, "task_prompt": ""}
{"id": "b02c021c-2b3f-4344-9189-2e830a32e234", "fitness": 0.06774372683214266, "name": "ATSE", "description": "ATSE mixes a PCA-derived rotating subspace (built from a memory of best archive points, memory_size=30 and subspace_frac=0.6 → k≈0.6·n) with many low-cost \"tempered Langevin\" probes (probes_mult=4 → probes ≈ 4·k) that sample coefficients in that subspace and map them to full-space steps, enforcing bounds and using a slow temperature decay (temp_init=1.0, temp_decay=0.997) to control exploration. It maintains an EMA momentum vector (momentum_beta=0.6) to bias probe directions, uses multiplicative sigma adaptation (sigma_init_factor=0.25, sigma_up=1.15, sigma_down=0.85) driven by a 1/5-like success rule and occasional short backtracking line-searches/polishing (p_polish=0.08) to intensify improvements. Occasionally the algorithm runs archive-driven full-space differential-evolution generations (p_de_gen=0.18, F_de=0.7) to introduce larger jumps, and it prunes the archive for scalability while respecting budget-aware safe evaluations so func() is never called beyond the allowed budget. Overall the design balances low-dimensional structured search, momentum-tempered stochastic probing, adaptive step sizing, occasional global jumps, and lightweight local polishing to be robust across many continuous test functions.", "code": "import numpy as np\n\nclass ATSE:\n    \"\"\"\n    ATSE: Adaptive Tempered Subspace Evolution\n\n    Main idea (one-liner):\n      Use PCA of an elite archive to define a rotating subspace and launch many\n      low-cost tempered Langevin-style probes with momentum; occasionally perform\n      archive-driven differential-evolution (DE) generations in full space. Sigma\n      is adapted with a simple success-rule (inspired by 1/5th), and a decaying\n      \"temperature\" controls exploration magnitude.\n\n    Main tunable parameters (different choices/equations vs. the provided LARSC):\n      - memory_size: number of elite points retained (archive/elite)\n      - subspace_frac: fraction of dimensions to keep in PCA subspace (k = ceil(n * subspace_frac))\n      - probes_mult: factor controlling probes per subspace dimension\n      - p_de_gen: probability to run an archive-DE generation round\n      - F_de: differential step size used in archive-DE\n      - temp_init / temp_decay: initial temperature and multiplicative decay per outer iteration\n      - momentum_beta: EMA factor for momentum (higher than LARSC's fixed momentum_scale usage)\n      - sigma_init_factor: initial sigma fraction of domain range (different multiplier)\n      - sigma_up / sigma_down: multiplicative factors on success/failure (different eqs)\n      - p_polish: chance for small full-space local search (polishing)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=30, subspace_frac=0.6, probes_mult=4,\n                 p_de_gen=0.18, F_de=0.7,\n                 temp_init=1.0, temp_decay=0.997,\n                 momentum_beta=0.6,\n                 sigma_init_factor=0.25,\n                 sigma_up=1.15, sigma_down=0.85,\n                 p_polish=0.08):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.subspace_frac = float(subspace_frac)\n        self.probes_mult = int(probes_mult)\n        self.p_de_gen = float(p_de_gen)\n        self.F_de = float(F_de)\n        self.temp_init = float(temp_init)\n        self.temp_decay = float(temp_decay)\n        self.momentum_beta = float(momentum_beta)\n        self.sigma_init_factor = float(sigma_init_factor)\n        self.sigma_up = float(sigma_up)\n        self.sigma_down = float(sigma_down)\n        self.p_polish = float(p_polish)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds must be respected; BBOB uses [-5,5] but we read from func\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        domain_mean = float(np.mean(domain_range))\n\n        rng = np.random.default_rng(self.seed)\n\n        # initial mean and sigma (different factor from LARSC)\n        m = rng.uniform(lb, ub)\n        sigma = max(1e-12, self.sigma_init_factor * domain_mean)\n\n        # archive (keeps history and elite subset)\n        X_archive = []\n        F_archive = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # temperature for tempered Langevin style probes\n        T = float(self.temp_init)\n\n        # momentum vector (EMA of successful unit directions)\n        momentum = np.zeros(n)\n\n        # step-size bounders (we use sigma as main scale but keep a 'step' for line-search magnitudes)\n        min_sigma = 1e-10\n        max_sigma = 10.0 * domain_mean\n\n        # safe eval wrapper (budget-aware)\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = np.clip(x, lb, ub)\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x.copy())\n            F_archive.append(f)\n            if f < f_best:\n                f_best = f\n                x_best = x.copy()\n            return f, x\n\n        # initial evaluation\n        out = safe_eval(m.copy())\n        if out[0] is None:\n            return float(f_best if x_best is not None else np.inf), np.array(x_best if x_best is not None else np.zeros(n), dtype=float)\n\n        # helper for small backtracking line-search (budget-aware). Simpler than golden.\n        def short_backtrack(x0, f0, d, init_alpha, max_evals=6):\n            # try decreasing/increasing alphas along direction for a few evals\n            if evals >= self.budget:\n                return None, None\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                return None, None\n            d = d / dn\n            alphas = [init_alpha * (0.5 ** k) for k in range(0, 4)] + [ -init_alpha * (0.5 ** k) for k in range(0, 3)]\n            best_f = f0\n            best_x = x0.copy()\n            tries = 0\n            for a in alphas:\n                if evals >= self.budget or tries >= max_evals:\n                    break\n                tries += 1\n                x_try = np.clip(x0 + a * d, lb, ub)\n                out = safe_eval(x_try)\n                if out[0] is None:\n                    break\n                if out[0] < best_f - 1e-12:\n                    best_f = out[0]; best_x = x_try.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # main loop: outer rounds until budget exhausted\n        outer_iter = 0\n        while evals < self.budget:\n            outer_iter += 1\n            remaining = self.budget - evals\n\n            # determine PCA subspace dimension (different eq: fraction of n)\n            k = max(1, int(np.ceil(n * self.subspace_frac)))\n            probes = max(6, self.probes_mult * k)\n\n            # Build PCA basis from elite archive if possible\n            basis = None\n            try:\n                # take up to memory_size best points\n                if len(X_archive) >= 2:\n                    idx_sorted = np.argsort(F_archive)\n                    take = min(len(idx_sorted), max(4, self.memory_size))\n                    elite_idx = idx_sorted[:take]\n                    X_elite = np.vstack([X_archive[i] for i in elite_idx])\n                    # center\n                    Xc = X_elite - np.mean(X_elite, axis=0)\n                    # compute covariance with small regularization\n                    cov = (Xc.T @ Xc) / max(1, Xc.shape[0] - 1)\n                    cov += np.eye(n) * 1e-8\n                    eigvals, eigvecs = np.linalg.eigh(cov)\n                    # take top-k eigenvectors\n                    order = np.argsort(eigvals)[::-1]\n                    topk = eigvecs[:, order[:k]]\n                    # orthonormalize (in case)\n                    Q, _ = np.linalg.qr(topk)\n                    basis = Q[:, :k]\n                else:\n                    basis = None\n            except Exception:\n                basis = None\n\n            # fallback to random orthonormal basis\n            if basis is None:\n                R = rng.standard_normal(size=(n, k))\n                Q, _ = np.linalg.qr(R)\n                basis = Q[:, :k]\n\n            # Decide to run an archive-driven DE generation (full-space occasional)\n            run_de = (rng.random() < self.p_de_gen) and (len(X_archive) >= 6) and (remaining >= 4)\n            if run_de:\n                # produce lam candidates (we choose a moderate lambda different from LARSC formula)\n                lam = min(12, remaining)\n                candidates = []\n                for _ in range(lam):\n                    # classic DE/rand/1: pick 3 distinct archive members\n                    i1, i2, i3 = rng.choice(len(X_archive), size=3, replace=False)\n                    a = X_archive[i1].copy()\n                    b = X_archive[i2].copy()\n                    c = X_archive[i3].copy()\n                    mutant = a + self.F_de * (b - c)\n                    # small gaussian perturbation scaled by sigma and temperature\n                    mutant += rng.normal(scale=sigma * np.sqrt(max(1e-12, T)), size=n)\n                    mutant = np.clip(mutant, lb, ub)\n                    candidates.append(mutant)\n                # evaluate\n                for x_cand in candidates:\n                    if evals >= self.budget:\n                        break\n                    out = safe_eval(x_cand)\n                    if out[0] is None:\n                        break\n                # after generation, cool temperature a bit\n                T *= self.temp_decay\n                # continue to next outer round\n                continue\n\n            # Otherwise run many tempered Langevin probes within PCA subspace\n            improved_round = False\n            success_count = 0\n            for p in range(probes):\n                if evals >= self.budget:\n                    break\n                # sample coefficients in subspace: normal with scale = sigma * sqrt(T)\n                coeffs = rng.normal(scale=sigma * np.sqrt(max(1e-12, T)), size=k)\n                d_sub = basis @ coeffs  # full-dim candidate direction\n                norm_d = np.linalg.norm(d_sub)\n                if norm_d == 0:\n                    continue\n                d_unit = d_sub / (norm_d + 1e-20)\n                # combine with momentum (EMA style)\n                cand_dir = (1.0 - self.momentum_beta) * d_unit + self.momentum_beta * momentum\n                nd = np.linalg.norm(cand_dir)\n                if nd == 0:\n                    cand_dir = d_unit\n                else:\n                    cand_dir = cand_dir / nd\n                # Langevin-style random length: mixture of gaussian + uniform\n                length = rng.normal(loc=0.0, scale=sigma * np.sqrt(T)) + rng.uniform(-0.5 * sigma, 0.5 * sigma)\n                x_prop = np.clip(m + length * cand_dir, lb, ub)\n                out = safe_eval(x_prop)\n                if out[0] is None:\n                    break\n                f_prop = out[0]\n                if f_prop < f_best - 1e-12:\n                    # success: update momentum, archive already updated in safe_eval\n                    success_count += 1\n                    improved_round = True\n                    # update momentum as EMA of unit vector from mean to success\n                    dir_succ = x_prop - m\n                    dn = np.linalg.norm(dir_succ)\n                    if dn > 0:\n                        unit = dir_succ / dn\n                        momentum = self.momentum_beta * momentum + (1.0 - self.momentum_beta) * unit\n                        # store momentum-normalized in small elite if desired by keeping archive already\n                    # moderately move mean towards success (different weighting from LARSC)\n                    m = 0.9 * m + 0.1 * x_prop\n                    # intensify via short backtrack line-search if budget allows\n                    remaining_budget = self.budget - evals\n                    if remaining_budget >= 2:\n                        ls_budget = min(6, remaining_budget)\n                        ls_out = short_backtrack(x_prop, f_prop, unit if dn > 0 else cand_dir, init_alpha=abs(length) + sigma, max_evals=ls_budget)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_prop - 1e-12:\n                                f_prop = f_ls\n                                x_prop = x_ls\n                                # adjust mean again slightly\n                                m = 0.92 * m + 0.08 * x_prop\n                    # adapt sigma upward (different equation than LARSC: multiplicative)\n                    sigma = min(max_sigma, sigma * self.sigma_up)\n                else:\n                    # occasionally try a polish from current mean along cand_dir\n                    if rng.random() < self.p_polish and (self.budget - evals) >= 2:\n                        ls_out = short_backtrack(m, f_best if x_best is not None else np.inf, cand_dir, init_alpha=sigma, max_evals=min(4, self.budget - evals))\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_best - 1e-12:\n                                # treat as success\n                                dir_succ = x_ls - m\n                                dn = np.linalg.norm(dir_succ)\n                                if dn > 0:\n                                    unit = dir_succ / dn\n                                    momentum = self.momentum_beta * momentum + (1.0 - self.momentum_beta) * unit\n                                m = 0.9 * m + 0.1 * x_ls\n                                sigma = min(max_sigma, sigma * self.sigma_up)\n                                improved_round = True\n                                success_count += 1\n                            else:\n                                # small penalty for wasted probes\n                                sigma = max(min_sigma, sigma * self.sigma_down)\n\n            # end probes loop\n\n            # adapt sigma by success ratio in this outer round (1/5-like rule, different eq)\n            if probes > 0:\n                succ_ratio = success_count / float(probes)\n                # target success 0.2 (1/5); increase if above, decrease if below\n                if succ_ratio > 0.2:\n                    sigma = min(max_sigma, sigma * self.sigma_up)\n                elif succ_ratio < 0.05:\n                    sigma = max(min_sigma, sigma * self.sigma_down)\n\n            # decay temperature slowly\n            T = max(1e-6, T * self.temp_decay)\n\n            # small random full-space draws as polishing if improved this round\n            if improved_round:\n                n_polish = min(3 + n // 8, 6)\n                for _ in range(n_polish):\n                    if evals >= self.budget:\n                        break\n                    d = rng.normal(size=n)\n                    d = d / (np.linalg.norm(d) + 1e-20)\n                    alpha = rng.uniform(-0.5 * sigma, 0.5 * sigma)\n                    outp = safe_eval(np.clip(m + alpha * d, lb, ub))\n                    if outp[0] is None:\n                        break\n                    if outp[0] < f_best - 1e-12:\n                        # update momentum and mean lightly\n                        dir_succ = outp[1] - m\n                        dn = np.linalg.norm(dir_succ)\n                        if dn > 0:\n                            momentum = self.momentum_beta * momentum + (1.0 - self.momentum_beta) * (dir_succ / dn)\n                        m = 0.95 * m + 0.05 * outp[1]\n\n            else:\n                # if completely unsuccessful, shrink sigma somewhat\n                sigma = max(min_sigma, sigma * self.sigma_down)\n\n            # archive pruning: keep recent + best\n            max_archive = max(2000, 50 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                step_keep = max(1, len(rest) // (max_archive - 200))\n                keep_rest = rest[::step_keep]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # quick stop if excellent\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            return float(np.inf), np.clip(m, lb, ub)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ATSE scored 0.068 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "9d9f78ad-978b-4980-9123-24520d7bfe63", "operator": null, "metadata": {"aucs": [0.07058451468659344, 0.050313810294559946, 0.1426360184658344, 0.05575628064470495, 0.05203936451066382, 0.10124625148284494, 0.009854699461089633, 0.10767985171447547, 0.009003426512754098, 0.078323050547906]}, "task_prompt": ""}
{"id": "359c806e-2f66-4730-93e9-30327364f0b6", "fitness": 0.16719914994596133, "name": "HASCE", "description": "HASCE is a hybrid optimizer combining a CMA-ES style global backbone (population-based mirrored sampling, evolution paths ps/pc, covariance C updates and periodic eigen-decomposition) with a learned low-rank orthonormal subspace U (k ≈ ceil(subspace_mult * sqrt(n))) built from recent successful step directions (success_buffer via SVD) to drive many inexpensive Langevin-style probes. The inner loop runs many cheap subspace + diagonal-noise probes (probes ∝ k) with momentum from recent successes, lightweight per-coordinate scales Sd (EMA of squared step magnitudes), occasional DE-style archive differences and heavy-tailed Cauchy jumps, and budget-aware short line-search intensifications along promising directions. Occasionally a full CMA generation (probability p_cma_round) is executed using mirrored sampling and eigen factors B/D_eig, while covariance/eigen updates are kept infrequent (eig_every ≈ 12·n) for efficiency and stability. Stagnation is handled by adaptive sigma inflation and nudging the mean toward archived points, with parameters (p_cauchy, p_de, mem_size_factor, etc.) chosen to balance exploration, exploitation, and budget-aware intensification.", "code": "import numpy as np\n\nclass HASCE:\n    \"\"\"\n    HASCE: Hybrid Adaptive Subspace-Covariance Explorer\n    One-line: A CMA-style global backbone with a learned low-rank subspace used for many\n    inexpensive Langevin-style probes, combined with mirrored sampling, Cauchy/DE perturbations,\n    periodic full CMA rounds, and short line-search intensifications.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 p_cma_round=0.18, p_cauchy=0.10, p_de=0.18,\n                 subspace_mult=1.2, mem_size_factor=6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.p_cma_round = float(p_cma_round)\n        self.p_cauchy = float(p_cauchy)\n        self.p_de = float(p_de)\n        # subspace sizing factor (k ~ ceil(subspace_mult*sqrt(n)) clipped)\n        self.subspace_mult = float(subspace_mult)\n        # memory size for success buffer relative to k\n        self.mem_size_factor = int(mem_size_factor)\n        # default RNG\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        budget = int(self.budget)\n\n        # bounds: functions in the suite use [-5,5] but read from func\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # domain statistics\n        domain_range = ub - lb\n        domain_mean = float(np.mean(domain_range))\n\n        # CMA-like population sizes & weights\n        lam = max(4, int(4 + np.floor(3 * np.log(max(1, n)))))\n        mu = max(1, lam // 2)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # CMA parameters\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        c1 = 2.0 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1.0 - c1, 2.0 * (mu_eff - 2.0 + 1.0 / mu_eff) / ((n + 2.0) ** 2 + mu_eff))\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # Dynamic state\n        m = self.rng.uniform(lb, ub)                     # center\n        sigma = max(1e-12, 0.28 * domain_mean)            # global step-size\n        C = np.eye(n)\n        ps = np.zeros(n); pc = np.zeros(n)\n        B = np.eye(n); D_eig = np.ones(n); invsqrtC = np.eye(n)\n        eig_every = max(1, int(12 * n))\n        eigen_counter = 0\n\n        # diagonal per-coordinate scales (fast approximate anisotropy)\n        Sd = np.ones(n)\n\n        # low-rank subspace state\n        k = max(1, int(min(n, np.ceil(self.subspace_mult * np.sqrt(max(1, n))))))\n        success_buffer = []             # store recent successful y-steps (shape n)\n        mem_size = max(10, self.mem_size_factor * k)\n        U = np.zeros((n, k))            # will be orthonormal basis when available\n\n        # archive for DE style difference mutations\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # safe eval wrapper that enforces budget and bookkeeping\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            if evals >= budget:\n                return None, None\n            x = np.clip(np.asarray(x, dtype=float), lb, ub)\n            try:\n                f = float(func(x))\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            if f < f_best:\n                f_best = f\n                x_best = x.copy()\n            # keep archive bounded moderately\n            if len(archive_X) > max(2000, 100 * n):\n                # drop oldest\n                del archive_X[0]; del archive_F[0]\n            return f, x\n\n        # initial eval of m\n        out = safe_eval(m.copy())\n        if out[0] is None:\n            return float(f_best if x_best is not None else np.inf), np.array(x_best if x_best is not None else np.zeros(n), dtype=float)\n\n        # helper: update low-rank U from success_buffer\n        def update_subspace():\n            nonlocal U, k\n            if len(success_buffer) < 2:\n                # keep previous or random\n                if np.linalg.norm(U) == 0:\n                    Rmat = self.rng.standard_normal((n, k))\n                    Q, _ = np.linalg.qr(Rmat)\n                    U = Q[:, :k]\n                return\n            # build matrix (n x m)\n            Y = np.vstack(success_buffer).T  # n x m\n            Y = Y - np.mean(Y, axis=1, keepdims=True)\n            try:\n                U_s, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                take = min(k, U_s.shape[1])\n                if take > 0:\n                    U = U_s[:, :take]\n                    if U.shape[1] < k:\n                        pad = self.rng.standard_normal((n, k - U.shape[1]))\n                        Qp, _ = np.linalg.qr(pad)\n                        U = np.hstack([U, Qp[:, :k - U.shape[1]]])\n                else:\n                    Rmat = self.rng.standard_normal((n, k))\n                    Q, _ = np.linalg.qr(Rmat)\n                    U = Q[:, :k]\n            except Exception:\n                Rmat = self.rng.standard_normal((n, k))\n                Q, _ = np.linalg.qr(Rmat)\n                U = Q[:, :k]\n\n        # helper: short budget-aware line-search along direction d from x0\n        def short_line_search(x0, f0, d, init_step, max_evals=6):\n            nonlocal evals, f_best, x_best\n            if evals >= budget:\n                return None, None\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            remaining = budget - evals\n            if remaining <= 0:\n                return None, None\n            s = abs(init_step) + 1e-12\n            # probe +s, -s\n            xa = np.clip(x0 + s * d, lb, ub)\n            out = safe_eval(xa)\n            if out[0] is None:\n                return None, None\n            fa = out[0]\n            if fa < f0:\n                a, b = 0.0, s\n                fa_val, fb_val = f0, fa\n            else:\n                xb = np.clip(x0 - s * d, lb, ub)\n                out2 = safe_eval(xb)\n                if out2[0] is None:\n                    return None, None\n                fb = out2[0]\n                if fb < f0:\n                    a, b = -s, 0.0\n                    fa_val, fb_val = fb, f0\n                else:\n                    return None, None\n            # golden-section-like limited steps\n            gr = (np.sqrt(5) - 1.0) / 2.0\n            c = b - gr * (b - a)\n            d_alpha = a + gr * (b - a)\n            xc = np.clip(x0 + c * d, lb, ub); outc = safe_eval(xc)\n            if outc[0] is None:\n                return None, None\n            fc = outc[0]\n            xd = np.clip(x0 + d_alpha * d, lb, ub); outd = safe_eval(xd)\n            if outd[0] is None:\n                return None, None\n            fd = outd[0]\n            best_f = f0; best_x = x0.copy()\n            for val, px in ((fa if 'fa' in locals() else None, xa if 'xa' in locals() else None),\n                            (fb if 'fb' in locals() else None, xb if 'xb' in locals() else None),\n                            (fc, xc), (fd, xd)):\n                if val is not None and val < best_f:\n                    best_f = val; best_x = px.copy()\n            iters = 0\n            while evals < budget and iters < max_evals:\n                iters += 1\n                if fc < fd:\n                    b = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    c = b - gr * (b - a)\n                    xc = np.clip(x0 + c * d, lb, ub)\n                    outc = safe_eval(xc)\n                    if outc[0] is None:\n                        break\n                    fc = outc[0]\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    a = c\n                    c = d_alpha\n                    fc = fd\n                    d_alpha = a + gr * (b - a)\n                    xd = np.clip(x0 + d_alpha * d, lb, ub)\n                    outd = safe_eval(xd)\n                    if outd[0] is None:\n                        break\n                    fd = outd[0]\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # main loop: use budget-aware alternation between many subspace probes and occasional full CMA rounds\n        stagnation_counter = 0\n        last_improve_eval = evals\n\n        while evals < budget:\n            remaining = budget - evals\n            # adapt subspace dimension\n            k = max(1, int(min(n, np.ceil(self.subspace_mult * np.sqrt(max(1, n))))))\n            probes = max(6, 6 * k)  # many cheap probes per round\n            # occasionally run a full CMA generation (multi-offspring)\n            run_cma = (self.rng.random() < self.p_cma_round) and (remaining >= lam)\n            if run_cma:\n                # perform a mirrored CMA generation using eigen factors B/D_eig\n                half = (lam + 1) // 2\n                arz = self.rng.standard_normal((half, n))\n                arz_full = np.vstack([arz, -arz])[:lam]\n                # sample via approximate sqrt(C) using B and D_eig\n                ary = arz_full * D_eig[np.newaxis, :]\n                ary = ary @ B.T\n                arx = m[np.newaxis, :] + sigma * ary\n                # occasional DE/perturbations and Cauchy\n                for i in range(lam):\n                    if self.rng.random() < self.p_de and len(archive_X) >= 2:\n                        i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                        arx[i] = arx[i] + 0.8 * (archive_X[i1] - archive_X[i2])\n                    if self.rng.random() < self.p_cauchy:\n                        # replace with a heavy-tailed jump in a random direction\n                        r = self.rng.standard_cauchy() * 1.0\n                        z = self.rng.standard_normal(n)\n                        z = z / (np.linalg.norm(z) + 1e-20)\n                        arx[i] = m + sigma * np.mean(Sd) * r * z\n                    arx[i] = np.clip(arx[i], lb, ub)\n                # evaluate\n                arfit = np.full(lam, np.inf)\n                for i in range(lam):\n                    if evals >= budget:\n                        break\n                    out = safe_eval(arx[i])\n                    if out[0] is None:\n                        break\n                    arfit[i] = out[0]\n                # selection and update\n                valid = np.isfinite(arfit)\n                if np.any(valid):\n                    idx_valid = np.where(valid)[0]\n                    order = idx_valid[np.argsort(arfit[valid])]\n                    sel_idx = order[:mu]\n                    x_sel = arx[sel_idx]\n                    y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n                    m_old = m.copy()\n                    m = np.sum(weights[:, None] * x_sel, axis=0)\n                    y_w = np.sum(weights[:, None] * y_sel, axis=0)\n                    # update evolution paths and C (CMA-style)\n                    try:\n                        ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrtC @ y_w)\n                    except Exception:\n                        ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * y_w\n                    norm_ps = np.linalg.norm(ps)\n                    denom = np.sqrt(1.0 - (1.0 - cs) ** (2.0 * (1 + evals / max(1, lam))))\n                    hsig = 1.0 if (norm_ps / (denom + 1e-20) / chi_n) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n                    pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n                    rank_one = np.outer(pc, pc)\n                    rank_mu = np.zeros((n, n))\n                    for j in range(min(mu, y_sel.shape[0])):\n                        yj = y_sel[j][:, None]\n                        rank_mu += weights[j] * (yj @ yj.T)\n                    C = (1.0 - c1 - cmu) * C + c1 * (rank_one + (1.0 - hsig) * cc * (2.0 - cc) * C) + cmu * rank_mu\n                    # adapt sigma by ps length\n                    sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n                    sigma = max(sigma, 1e-12)\n                # eigen recompute occasionally\n                eigen_counter += lam\n                if eigen_counter >= eig_every:\n                    eigen_counter = 0\n                    C = np.triu(C) + np.triu(C, 1).T\n                    try:\n                        eigvals, eigvecs = np.linalg.eigh(C)\n                        eigvals = np.maximum(eigvals, 1e-20)\n                        D_eig = np.sqrt(eigvals)\n                        B = eigvecs\n                        invsqrtC = (B * (1.0 / D_eig)) @ B.T\n                    except Exception:\n                        C = np.eye(n); B = np.eye(n); D_eig = np.ones(n); invsqrtC = np.eye(n)\n                # after CMA round continue\n                continue\n\n            # Otherwise run many Langevin-style probes inside learned subspace + diagonal noise\n            # Ensure U is up-to-date\n            update_subspace()\n\n            improved_round = False\n            # momentum from most recent direction\n            momentum = success_buffer[0] if success_buffer else np.zeros(n)\n            momentum = (momentum / (np.linalg.norm(momentum) + 1e-20)) * 0.35\n\n            for p in range(probes):\n                if evals >= budget:\n                    break\n                # generate coefficients in subspace and combine with diagonal noise\n                coeffs = self.rng.normal(scale=1.0, size=k)\n                y_sub = U @ coeffs  # (n,)\n                # mix diagonal scaled noise\n                z_diag = self.rng.normal(size=n) * Sd * 0.6\n                # combine and add momentum & normalize\n                y = 0.65 * y_sub + 0.35 * z_diag + 0.6 * momentum\n                if np.linalg.norm(y) == 0:\n                    continue\n                # occasional Cauchy long jump in subspace\n                if self.rng.random() < self.p_cauchy:\n                    r = self.rng.standard_cauchy() * 1.0\n                    y = r * (y / (np.linalg.norm(y) + 1e-20))\n                # normalize to unit direction and scale by sigma\n                y_unit = y / (np.linalg.norm(y) + 1e-20)\n                length = self.rng.normal(loc=0.0, scale=0.9 * sigma * 0.6) + self.rng.uniform(-0.9 * sigma, 0.9 * sigma)\n                x_prop = np.clip(m + length * y_unit, lb, ub)\n                # occasional DE perturbation\n                if self.rng.random() < self.p_de and len(archive_X) >= 2:\n                    i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                    x_prop = np.clip(x_prop + 0.75 * (archive_X[i1] - archive_X[i2]), lb, ub)\n                out = safe_eval(x_prop)\n                if out[0] is None:\n                    break\n                f_prop = out[0]\n                if f_prop < f_best - 1e-12:\n                    # success: record direction (y_unit), add to success buffer and intensify\n                    dy = (x_prop - m)\n                    dn = np.linalg.norm(dy)\n                    if dn > 0:\n                        dir_unit = dy / dn\n                        success_buffer.insert(0, dir_unit.copy())\n                        if len(success_buffer) > mem_size:\n                            success_buffer.pop()\n                    # short intensification along found direction if budget allows\n                    remaining_budget = budget - evals\n                    if remaining_budget >= 2:\n                        ls_budget = min(6, remaining_budget)\n                        ls_out = short_line_search(x_prop, f_prop, dir_unit if dn > 0 else y_unit, init_step=max(abs(length), sigma), max_evals=ls_budget)\n                        if ls_out[0] is not None and ls_out[0] < f_prop - 1e-12:\n                            f_prop, x_prop = ls_out\n                    # move mean moderately towards x_prop\n                    m = 0.75 * m + 0.25 * x_prop\n                    sigma = max(sigma, 0.18 * domain_mean)\n                    # update Sd (diagonal) using exponential moving second moment from last successful y\n                    y_local = (x_prop - m) / (sigma + 1e-20)\n                    beta_d = 0.20\n                    Sd2 = (1.0 - beta_d) * (Sd ** 2) + beta_d * (y_local ** 2 + 1e-20)\n                    Sd = np.sqrt(Sd2)\n                    # update lightweight CMA paths with this successful step (scaled)\n                    yw = (x_prop - m) / (sigma + 1e-20)\n                    try:\n                        ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrtC @ yw)\n                    except Exception:\n                        ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * yw\n                    norm_ps = np.linalg.norm(ps)\n                    hsig = 1.0 if (norm_ps / (chi_n + 1e-20)) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n                    pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * yw\n                    C = (1.0 - c1 - cmu) * C + c1 * np.outer(pc, pc) + cmu * np.outer(yw, yw)\n                    improved_round = True\n                    last_improve_eval = evals\n                else:\n                    # on failure maybe adjust Sd lightly using this sample's magnitude\n                    y_local = (x_prop - m) / (sigma + 1e-20)\n                    Sd = np.sqrt((0.95 * (Sd ** 2) + 0.05 * (y_local ** 2 + 1e-20)))\n                    # small occasional short line-search from m along y_unit if budget permits\n                    if self.rng.random() < 0.03 and (budget - evals) >= 3:\n                        ls_out = short_line_search(m, f_best if x_best is not None else np.inf, y_unit, init_step=sigma, max_evals=min(4, budget - evals))\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_best - 1e-12:\n                                # accept as improvement\n                                dy = (x_ls - m); dn = np.linalg.norm(dy)\n                                if dn > 0:\n                                    success_buffer.insert(0, (dy / dn).copy())\n                                    if len(success_buffer) > mem_size:\n                                        success_buffer.pop()\n                                m = 0.8 * m + 0.2 * x_ls\n                                sigma = max(sigma, 0.18 * domain_mean)\n                                improved_round = True\n                                last_improve_eval = evals\n\n            # after probes: update subspace occasionally\n            if len(success_buffer) >= 2 and (self.rng.random() < 0.6):\n                update_subspace()\n\n            # adapt sigma depending on success\n            if improved_round:\n                sigma *= 0.98\n            else:\n                sigma *= 1.03\n            sigma = np.clip(sigma, 1e-12, 5.0 * domain_mean + 1e-12)\n\n            # stagnation handling\n            if (evals - last_improve_eval) > max(50, 8 * n):\n                stagnation_counter += 1\n                # shake: inflate sigma, nudge mean toward a good archive point\n                sigma *= 1.5 + 0.2 * stagnation_counter\n                if len(archive_X) > 0:\n                    pick = self.rng.integers(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                # refresh subspace with random orthonormal basis\n                Rmat = self.rng.standard_normal((n, k))\n                Q, _ = np.linalg.qr(Rmat)\n                U = Q[:, :k]\n                success_buffer = []\n                last_improve_eval = evals\n            else:\n                stagnation_counter = max(0, stagnation_counter - 1)\n\n            # periodic eigen recomputation of C\n            eigen_counter += 1\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    eigvals, eigvecs = np.linalg.eigh(C)\n                    eigvals = np.maximum(eigvals, 1e-20)\n                    D_eig = np.sqrt(eigvals)\n                    B = eigvecs\n                    invsqrtC = (B * (1.0 / D_eig)) @ B.T\n                except Exception:\n                    C = np.eye(n); B = np.eye(n); D_eig = np.ones(n); invsqrtC = np.eye(n)\n\n            # quick stop if near optimum\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            return float(np.inf), np.clip(m, lb, ub)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HASCE scored 0.167 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "9d9f78ad-978b-4980-9123-24520d7bfe63", "operator": null, "metadata": {"aucs": [0.08543056402045845, 0.15722981385306944, 0.2235048460575566, 0.16084810358542423, 0.16255161978590127, 0.1911094835099456, 0.211464453189818, 0.178144556626179, 0.16618214138358323, 0.1355259174476775]}, "task_prompt": ""}
{"id": "9782e886-0ed3-4523-970d-eca291bbd3a9", "fitness": "-inf", "name": "CMADMSS", "description": "The design is a hybrid CMA-style search: it keeps CMA-like state (mean m, covariance C with B/D eigendecomposition, evolution paths ps/pc, rank-one and rank-mu updates, and sigma adaptation) while using small population heuristics and mirrored sampling for variance reduction. It augments global covariance search with a compact directional memory (LRU deque) and budget-aware QR-orthonormal subspace probes (subspace dim k ≈ n**subspace_exp, probes ≈ probes_factor*k) using heavy-tailed coefficients to generate diverse long-range directions. Per-coordinate anisotropic scaling (coord_var updated RMS-style) biases sampling, and cheap 3-point parabolic 1‑D line refinements (quick_parabola) plus clipping/archiving enforce bounds and strict budget accounting. Finally, it uses periodic eigendecomposition (eig_every_factor * n), occasional local polishing, and anisotropic restarts guided by coord_var and memory to escape stagnation, with conservative defaults (init_sigma_factor=0.25, mem_size=8, probes_factor=3, max_restarts=6).", "code": "import numpy as np\nfrom collections import deque\n\nclass CMADMSS:\n    \"\"\"\n    CMA-Directional Memory & Subspace Search (CMADMSS)\n    - Combines CMA-like full covariance adaptation (ps/pc, B/D/eig) and mirrored sampling\n      with ADES-inspired directional memory, QR-orthonormal subspace probes, per-coordinate\n      anisotropic scaling and cheap 3-point parabolic 1-D refinements.\n    One-line: budget-aware hybrid: covariance-adaptive global search + memory-guided subspace probes + cheap line refinements.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 mem_size=8, subspace_exp=0.6, probes_factor=3,\n                 init_sigma_factor=0.25, cs_factor=None, cc_factor=None,\n                 eig_every_factor=10, c1=None, cmu=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.mem_size = int(mem_size)\n        self.subspace_exp = float(subspace_exp)\n        self.probes_factor = int(probes_factor)\n        self.init_sigma_factor = float(init_sigma_factor)\n        # optional CMA-like constants, fallback to heuristics if None\n        self.cs_factor = cs_factor\n        self.cc_factor = cc_factor\n        self.c1 = c1\n        self.cmu = cmu\n        self.eig_every_factor = int(eig_every_factor)\n        if seed is not None:\n            self.rng = np.random.default_rng(seed)\n        else:\n            self.rng = np.random.default_rng()\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain = np.maximum(1e-12, np.mean(ub - lb))\n\n        # population size heuristics\n        lam = max(4, int(4 + np.floor(3 * np.log(max(1, n)))))\n        mu = max(1, lam // 2)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # adaptation constants (CMA-like defaults)\n        cs = self.cs_factor if self.cs_factor is not None else (mu_eff + 2) / (n + mu_eff + 5)\n        cc = self.cc_factor if self.cc_factor is not None else 4.0 / (n + 4.0)\n        c1 = self.c1 if self.c1 is not None else 2.0 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = self.cmu if self.cmu is not None else min(1 - c1, 2 * (mu_eff - 2 + 1 / mu_eff) / ((n + 2) ** 2 + mu_eff))\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # dynamic state\n        m = np.clip(self.rng.uniform(lb, ub), lb, ub)\n        sigma = float(self.init_sigma_factor * domain)\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eig_every = max(1, int(self.eig_every_factor * n))\n        eigen_counter = 0\n\n        # per-coordinate RMS-like scaling to bias anisotropy\n        coord_var = np.ones(n) * 1e-6\n        coord_alpha = 0.18\n\n        # directional memory (LRU)\n        mem = deque(maxlen=self.mem_size)\n\n        # archive for diversity\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # safe evaluation that enforces budget and clipping\n        def clip_to_bounds(x):\n            return np.minimum(ub, np.maximum(lb, x))\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(x)\n            if evals >= budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            # store in archive\n            archive_X.append(x.copy())\n            archive_F.append(f)\n            # maintain archive cap\n            if len(archive_X) > 5000:\n                archive_X.pop(0); archive_F.pop(0)\n            return f, x\n\n        # cheap 3-point parabolic 1-D line refinement (budget-aware)\n        def quick_parabola(x0, f0, d, init_step=1.0, max_evals=5):\n            dn = np.linalg.norm(d)\n            if dn == 0 or max_evals < 1:\n                return None, None\n            d = d / (dn + 1e-20)\n            # collect points alpha: f\n            pts = [(0.0, f0, x0.copy())]\n            remaining = max(0, budget - evals)\n            if remaining <= 0:\n                return None, None\n            s = init_step\n            # +s\n            if remaining >= 1:\n                f1, x1 = safe_eval(clip_to_bounds(x0 + s * d))\n                if f1 is None:\n                    return None, None\n                pts.append((s, f1, x1.copy()))\n                remaining = max(0, budget - evals)\n            # -s\n            if remaining >= 1:\n                f2, x2 = safe_eval(clip_to_bounds(x0 - s * d))\n                if f2 is None:\n                    return None, None\n                pts.append((-s, f2, x2.copy()))\n                remaining = max(0, budget - evals)\n            # optional expansion on best side\n            pts_sorted = sorted(pts, key=lambda t: t[1])\n            best_alpha, best_f, best_x = pts_sorted[0]\n            if best_alpha != 0.0 and remaining >= 1 and abs(best_alpha) < 10 * domain:\n                new_alpha = 2.0 * best_alpha\n                f3, x3 = safe_eval(clip_to_bounds(x0 + new_alpha * d))\n                if f3 is None:\n                    return None, None\n                pts.append((new_alpha, f3, x3.copy()))\n                pts_sorted = sorted(pts, key=lambda t: t[1])\n                best_alpha, best_f, best_x = pts_sorted[0]\n            if len(pts) < 3:\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            alphas = np.array([p[0] for p in pts])\n            fs = np.array([p[1] for p in pts])\n            xs = [p[2] for p in pts]\n            # pick three by best value + spread\n            idx_sorted = np.argsort(fs)\n            i0 = idx_sorted[0]\n            others = [i for i in idx_sorted[1:]]\n            if len(others) >= 2:\n                dists = [abs(alphas[i] - alphas[i0]) for i in others]\n                i1 = others[np.argmax(dists)]\n                others2 = [o for o in others if o != i1]\n                if others2:\n                    i2 = others2[np.argmax([abs(alphas[o] - alphas[i0]) for o in others2])]\n                else:\n                    i2 = i1\n            elif len(others) == 1:\n                i1 = others[0]; i2 = others[0]\n            else:\n                i1 = i0; i2 = i0\n            a1, f1, x1 = alphas[i0], fs[i0], xs[i0]\n            a2, f2, x2 = alphas[i1], fs[i1], xs[i1]\n            a3, f3, x3 = alphas[i2], fs[i2], xs[i2]\n            M = np.array([[a1*a1, a1, 1.0],\n                          [a2*a2, a2, 1.0],\n                          [a3*a3, a3, 1.0]])\n            y = np.array([f1, f2, f3])\n            try:\n                A, Bc, Cc = np.linalg.solve(M, y)\n                if A == 0:\n                    return (f1, x1) if f1 < f0 - 1e-12 else (None, None)\n                alpha_opt = -Bc / (2.0 * A)\n            except np.linalg.LinAlgError:\n                return (f1, x1) if f1 < f0 - 1e-12 else (None, None)\n            max_span = max(abs(alphas.max() - alphas.min()), 1e-12)\n            alpha_opt = np.clip(alpha_opt, alphas.min() - 0.5 * max_span, alphas.max() + 0.5 * max_span)\n            if budget - evals >= 1:\n                fopt, xopt = safe_eval(clip_to_bounds(x0 + alpha_opt * d))\n                if fopt is None:\n                    return None, None\n                if fopt < f0 - 1e-12:\n                    return fopt, xopt\n            # fallback to best seen along sampled pts\n            best_seen = min(pts, key=lambda t: t[1])\n            if best_seen[1] < f0 - 1e-12:\n                return best_seen[1], best_seen[2]\n            return None, None\n\n        # initialize by evaluating m\n        if evals < budget:\n            f0, x0 = safe_eval(m.copy())\n            # safe_eval will update f_best and x_best\n\n        # main loop\n        stagnation = 0\n        stagnation_limit = max(12, int(8 + np.log(1 + n) * 6))\n        restarts = 0\n        max_restarts = 6\n\n        while evals < budget:\n            remaining = budget - evals\n            # produce population but don't exceed remaining\n            current_lambda = min(lam, remaining)\n            # mirrored sampling for variance reduction when even\n            if current_lambda % 2 == 0:\n                half = current_lambda // 2\n                arz_half = self.rng.standard_normal(size=(half, n))\n                arz = np.vstack([arz_half, -arz_half])\n            else:\n                arz = self.rng.standard_normal(size=(current_lambda, n))\n\n            # apply approximate sqrt(C) via B * (D * z)\n            # (B * D) is implemented as B * D (column scaling)\n            try:\n                ary = arz @ (B * D).T\n            except Exception:\n                ary = arz  # fallback to isotropic if something wrong\n\n            # apply per-coordinate RMS scaling\n            scaled_ary = ary * np.sqrt(coord_var + 1e-20)\n\n            arx = m + sigma * scaled_ary\n\n            # occasionally supplement with memory-guided subspace probes (budget permitting)\n            # subspace dim k ~ n**subspace_exp\n            k = max(1, int(np.clip(int(np.ceil(n ** self.subspace_exp)), 1, n)))\n            probes = max(4, self.probes_factor * k)\n            # build subspace basis combining memory and random\n            use_mem = min(len(mem), max(0, k // 2))\n            chosen = []\n            if use_mem > 0:\n                # favor recent directions\n                idxs = np.arange(use_mem)\n                for i in idxs:\n                    chosen.append(mem[i])\n            needed = k - len(chosen)\n            if needed > 0:\n                R = self.rng.standard_normal(size=(n, needed))\n                if chosen:\n                    R = np.column_stack((np.column_stack(chosen), R))\n                try:\n                    Q, _ = np.linalg.qr(R)\n                    basis = Q[:, :k]\n                except Exception:\n                    basis = np.eye(n)[:, :k]\n            else:\n                Btmp = np.column_stack(chosen)\n                try:\n                    Q, _ = np.linalg.qr(Btmp)\n                    basis = Q[:, :k]\n                except Exception:\n                    basis = np.eye(n)[:, :k]\n\n            # Evaluate the population arx (respecting budget)\n            arfit = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                fval, xval = safe_eval(arx[i])\n                if fval is None:\n                    break\n                arfit[i] = fval\n\n            # Try subspace probing around current mean m (a few probes only to preserve budget)\n            probes_to_do = min(probes, max(0, budget - evals))\n            for p in range(probes_to_do):\n                if evals >= budget:\n                    break\n                # heavy-tailed coefficients\n                coeffs = self.rng.standard_normal(k) * (1.0 + 0.6 * self.rng.standard_exponential(k))\n                d = basis @ coeffs\n                dn = np.linalg.norm(d)\n                if dn == 0:\n                    continue\n                d = d / dn\n                alpha = self.rng.uniform(-1.0, 1.0) * sigma * 1.2 * max(1.0, coord_var.max() ** 0.5)\n                x_try = clip_to_bounds(m + alpha * d)\n                f_try, x_t = safe_eval(x_try)\n                if f_try is None:\n                    break\n                # if improvement over best, try quick parabola along this direction\n                if f_try < f_best - 1e-12:\n                    # store direction in memory\n                    disp = x_t - m\n                    dn2 = np.linalg.norm(disp)\n                    if dn2 > 0:\n                        dir_unit = disp / dn2\n                        mem.appendleft(dir_unit.copy()) if hasattr(mem, 'appendleft') else mem.append(dir_unit.copy())\n                        # slightly bias coord_var\n                        coord_var = 0.88 * coord_var + 0.12 * (1.0 + np.abs(dir_unit))\n                    # quick parabola from m along dir_unit\n                    f_line, x_line = quick_parabola(m, f_best, dir_unit if dn2 > 0 else d,\n                                                    init_step=alpha if abs(alpha) > 1e-16 else sigma, max_evals=min(5, budget - evals))\n                    if f_line is not None and f_line < f_try - 1e-12:\n                        # quick_parabola updates f_best via safe_eval; we can continue\n                        pass\n\n            # selection & recombination from evaluated offspring\n            valid = np.isfinite(arfit)\n            if np.any(valid):\n                # use evaluated subset\n                idx = np.argsort(arfit)\n                sel_idx = idx[:min(mu, np.sum(valid))]\n                x_sel = np.array([archive_X[-len(arfit)+i] if False else None for i in sel_idx])  # dummy to satisfy shape\n                # It's simpler and safer to recompute x_sel from available arx/evaluated portion\n                evaluated_indices = np.where(valid)[0]\n                arx_eval = arx[evaluated_indices]\n                arfit_eval = arfit[evaluated_indices]\n                idx_eval_sorted = np.argsort(arfit_eval)\n                sel_idx_eval = idx_eval_sorted[:min(mu, len(idx_eval_sorted))]\n                x_sel = arx_eval[sel_idx_eval]\n                y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n                m_old = m.copy()\n                m = np.sum(weights[:len(sel_idx_eval)][:, None] * x_sel, axis=0)\n                y_w = np.sum(weights[:len(sel_idx_eval)][:, None] * y_sel, axis=0)\n                # update evolution paths\n                try:\n                    ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrtC @ y_w)\n                except Exception:\n                    ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * y_w\n                norm_ps = np.linalg.norm(ps)\n                hsig = 1.0 if (norm_ps / (np.sqrt(1.0 - (1.0 - cs) ** (2.0 * (1 + 1))) + 1e-20) / chi_n) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n                pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n                # covariance update\n                rank_one = np.outer(pc, pc)\n                rank_mu = np.zeros((n, n))\n                for i_w in range(min(mu, y_sel.shape[0])):\n                    yi = y_sel[i_w][:, None]\n                    rank_mu += weights[i_w] * (yi @ yi.T)\n                C = (1.0 - c1 - cmu) * C + c1 * (rank_one + (1.0 - hsig) * cc * (2.0 - cc) * C) + cmu * rank_mu\n                # coord_var update (RMS-like)\n                sec_mom = np.sum(weights[:y_sel.shape[0]][:, None] * (y_sel ** 2), axis=0)\n                coord_var = (1.0 - coord_alpha) * coord_var + coord_alpha * (sec_mom + 1e-12)\n                # sigma update via ps\n                sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n                sigma = max(sigma, 1e-12)\n            else:\n                # if no valid offspring (budget likely exhausted)\n                pass\n\n            # occasional local polishing around best found so far\n            if (evals < budget) and (f_best < np.inf) and (self.rng.random() < 0.12):\n                # small gaussian perturbations biased by coord_var\n                for _ in range(min(6, budget - evals)):\n                    noise = self.rng.normal(scale=0.3 * sigma * np.sqrt(coord_var))\n                    f_p, x_p = safe_eval(clip_to_bounds(x_best + noise))\n                    if f_p is None:\n                        break\n                    if f_p < f_best - 1e-12:\n                        # store direction\n                        disp = x_p - x_best\n                        nd = np.linalg.norm(disp)\n                        if nd > 0:\n                            mem.appendleft(disp / nd)\n                            coord_var = 0.9 * coord_var + 0.1 * (1.0 + np.abs(disp / (nd + 1e-20)))\n                        # light acceptance\n                        # continue polishing around new best\n\n            # restart/escape if stagnation\n            # compute a crude stagnation measure: small sigma and few memory entries\n            if sigma < 1e-12 or (len(mem) == 0 and self.rng.random() < 0.02):\n                restarts += 1\n                if restarts > max_restarts:\n                    # final local polish: reduce sigma and try coordinate sweeps\n                    sigma = max(sigma * 0.3, 1e-12)\n                    coords = self.rng.permutation(n)[:min(n, 6)]\n                    for i in coords:\n                        if evals >= budget:\n                            break\n                        d = np.zeros(n); d[i] = 1.0\n                        for sign in (+1, -1):\n                            a = sign * 0.8 * sigma * np.sqrt(coord_var[i])\n                            f_try, x_try = safe_eval(clip_to_bounds(x_best + a * d))\n                            if f_try is None:\n                                break\n                else:\n                    # anisotropic restart around best using coord_var bias\n                    largescale = 0.6 * domain * (1.0 + 0.6 * self.rng.standard_exponential(size=n)) * (np.sqrt(coord_var) / (np.sqrt(coord_var).max() + 1e-20))\n                    jump = self.rng.normal(scale=largescale)\n                    if mem:\n                        mem_dir = mem[0]\n                        jump = 0.6 * jump + 0.4 * (mem_dir * domain * 0.5)\n                    x_new = clip_to_bounds(x_best + jump)\n                    f_new, x_newr = safe_eval(x_new)\n                    if f_new is not None and f_new < f_best - 1e-12:\n                        # push new center towards good area\n                        m = x_newr.copy()\n                        sigma = max(sigma * 1.5, self.init_sigma_factor * domain)\n                        coord_var = 0.9 * coord_var + 0.1 * (1.0 + np.abs(jump))\n                        # keep some memory but decay it\n                        if len(mem) > 0:\n                            for i in range(len(mem)):\n                                mem[i] = 0.85 * mem[i]\n\n            # eigen decomposition periodically\n            eigen_counter += current_lambda\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                # ensure symmetry\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except Exception:\n                    C = np.eye(n); B = np.eye(n); D = np.ones(n); invsqrtC = np.eye(n)\n\n            # quick termination if extremely good\n            if f_best <= 1e-14 or evals >= budget:\n                break\n\n        # ensure x_best is not None\n        if x_best is None:\n            # fallback: random sample if nothing evaluated (shouldn't happen)\n            x_best = np.clip(self.rng.uniform(lb, ub), lb, ub)\n            f_best = float(func(x_best)) if evals < budget else float(f_best)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 80, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem = deque(maxlen=self.mem_size)", "error": "In the code, line 80, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: mem = deque(maxlen=self.mem_size)", "parent_ids": "9d9f78ad-978b-4980-9123-24520d7bfe63", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "27960619-e570-435e-a8e7-cd594206f1cd", "fitness": "-inf", "name": "ASLPS", "description": "ASLPS is a hybrid evolution-like sampler that combines a global step size sigma with per-coordinate RMS scaling (G -> D) and a small learned principal subspace U (k ≈ ceil(sqrt(n))) so samples are mixtures of a diagonal high-frequency component and a low-rank directional component (mixing weight rho), while a short momentum term (decay 0.88, 0.35 injection) and mirrored sampling stabilize and diversify proposals. The algorithm learns U by performing SVD on a displacement buffer (buffer_max = max(30,10*k), refreshed every 20 generations) and adapts rho toward the fraction of variance explained by the top-k singular vectors, plus it occasionally runs a 1-D parabolic line probe along the top singular vector to refine steps. Robust exploration is provided by occasional Mantegna Lévy jumps (p_levy=0.15, alpha=1.5, unit-RMS normalized) and a lightweight DE-style archive difference injection (p_de=0.18, F_de=0.7) with a large archive (limit 4000) used also for mild restarts. Adaptation rules are simple and budget-aware: sigma contracts or expands on success/failure, G accumulates weighted second moments (beta_rms=0.12) to form D, lambda/mu scale with log(n), and a stagnation detector (patience = max(150,8*n)) triggers sigma inflation and nudges toward good archived points.", "code": "import numpy as np\n\nclass ASLPS:\n    \"\"\"\n    ASLPS: Adaptive Subspace Line-Probing Search\n\n    Main ideas / novel pieces compared to typical hybrid schemes:\n    - Maintain global step sigma and per-coordinate RMS scaling (G -> D).\n    - Learn a small principal subspace U from recent successful displacements (SVD on buffer).\n    - Mix diagonal (RMS-scaled) and subspace (low-rank) samples with an adaptive mixing rho\n      driven by the fraction of variance explained by the subspace.\n    - Use Mantegna's Lévy (alpha=1.5) for occasional heavy-tailed jumps (instead of plain Cauchy).\n    - Occasionally perform a short 1-D parabolic line probe along the top principal direction,\n      using three function probes and a quadratic fit to propose a refined step (budget-aware).\n    - Use a lightweight DE-like archive difference injection and mirrored sampling.\n    - Stagnation triggers sigma inflation and targeted perturbation from best archive points.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population & selection sizes (CMA-inspired)\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n        # low-rank dimension default ~ sqrt(n)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # buffers & frequencies\n        self.buffer_max = max(30, 10 * self.k)\n        self.svd_every = 20\n\n        # exploration hyperparameters\n        self.p_levy = 0.15\n        self.p_de = 0.18\n        self.F_de = 0.7\n        self.mirrored = True\n\n        # adaptation hyperparameters\n        self.beta_rms = 0.12    # RMS exponential smoothing\n        self.momentum_decay = 0.88\n        self.lineprobe_every = 10  # attempt line probe every this many SVDs\n        self.lineprobe_scale = 1.6\n\n        # stagnation\n        self.archive_limit = 4000\n        self.stag_patience = max(150, 8 * self.dim)\n\n    def _mantegna_levy_step(self, size, alpha=1.5):\n        \"\"\"\n        Generate a multivariate symmetric step using Mantegna's algorithm for alpha-stable approx.\n        Returns vector of length size with unit typical magnitude.\n        \"\"\"\n        # Mantegna algorithm for Levy-stable increments (alpha in (0,2])\n        # sigma_u and sigma_v per Mantegna\n        beta = 0.0\n        sigma_u = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                   (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1.0 / alpha)\n        u = np.random.normal(0, sigma_u, size)\n        v = np.random.normal(0, 1.0, size)\n        step = u / (np.abs(v) ** (1.0 / alpha) + 1e-20)\n        # normalize to unit RMS\n        step = step / (np.sqrt(np.mean(step ** 2)) + 1e-20)\n        return step\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (BBOB typical -5..5 but use func bounds if given)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_scale = np.mean(ub - lb)\n\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n\n        # initialize mean uniformly in domain\n        m = np.random.uniform(lb, ub)\n        # evaluate mean initially\n        evals = 0\n        fm = func(np.clip(m, lb, ub))\n        evals += 1\n\n        # best-so-far\n        f_best = fm\n        x_best = m.copy()\n\n        # global sigma\n        sigma = 0.25 * domain_scale\n\n        # RMS-like accumulator and diagonal scaling\n        eps = 1e-9\n        G = np.ones(n) * 1e-3\n        D = (G + eps) ** -0.5\n\n        # Random orthonormal initial U\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            U = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = np.zeros((n, self.k))\n\n        # buffers\n        disp_buffer = []  # successful displacements y = (x - m)/sigma compatible\n        archive_X = [m.copy()]\n        archive_F = [fm]\n        momentum = np.zeros(n)\n\n        rho = 0.5  # mixing weight between diag and subspace (adaptive)\n        svd_counter = 0\n        svd_performed = 0\n        no_improve_since = 0\n\n        # main loop\n        gen = 0\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # pre-sample Gaussians\n            zs = np.random.randn(current_lambda, n)\n            zs_low = np.random.randn(current_lambda, self.k) if self.k > 0 else np.zeros((current_lambda, 0))\n\n            for i in range(current_lambda):\n                z = zs[i].copy()\n\n                # compute low-rank projection and complement\n                if self.k > 0:\n                    proj_low = U @ (U.T @ z)\n                    proj_high = z - proj_low\n                else:\n                    proj_low = np.zeros(n)\n                    proj_high = z\n\n                # diagonal scaled high-frequency component\n                y_diag = D * proj_high\n\n                # low-rank reconstruction scaled by mean D\n                if self.k > 0:\n                    y_sub = np.mean(D) * (U @ zs_low[i])\n                else:\n                    y_sub = np.zeros(n)\n\n                # occasionally do a Levy step instead of Gaussian\n                if np.random.rand() < self.p_levy:\n                    levy = self._mantegna_levy_step(n, alpha=1.5)\n                    # scale levy by sigma*mean(D) to keep magnitude consistent\n                    y = levy * np.mean(D)\n                else:\n                    y = (1.0 - rho) * y_diag + rho * y_sub\n\n                # mirrored sampling (ensures pairs of opposite samples)\n                if self.mirrored and (i % 2 == 1):\n                    y = -y\n\n                x = m + sigma * y + 0.35 * momentum\n\n                # DE archive difference injection\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    mask = (np.random.rand(n) < 0.5)\n                    x[mask] += de_mut[mask]\n\n                x = np.clip(x, lb, ub)\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n\n                # archive\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n\n                # update best\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n                    no_improve_since = 0\n                else:\n                    no_improve_since += 1\n\n            # selection\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            # recombine mean (x-space)\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n            # weighted step in y-space (approx)\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # update RMS accumulator via weighted second moment\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            G = (1.0 - self.beta_rms) * G + self.beta_rms * (y2 + eps)\n            D = (G + eps) ** -0.5\n\n            # momentum/inertia\n            momentum = self.momentum_decay * momentum + (1.0 - self.momentum_decay) * (sigma * y_w)\n\n            # update sigma: simple success-based rule (if step improved mean reduce a bit, else increase)\n            # estimate if new mean is better (we may not have evaluated m directly); approximate by looking at selected best.\n            # If best selected candidate improved over current best, tighten; else slightly expand.\n            if arfit[idx[0]] < f_best:\n                sigma = max(sigma * 0.95, 1e-12)\n            else:\n                sigma = min(sigma * 1.02, 1e2 * domain_scale)\n\n            # store displacement (successful direction relative to previous mean)\n            disp = (m - m_old) / (sigma + 1e-20)\n            if np.any(~np.isnan(disp)) and np.linalg.norm(disp) > 1e-12:\n                disp_buffer.append(disp.copy())\n                if len(disp_buffer) > self.buffer_max:\n                    disp_buffer.pop(0)\n\n            # periodic SVD refresh to update U and adapt rho based on variance explained\n            svd_counter += 1\n            if (svd_counter >= self.svd_every) and (len(disp_buffer) >= min(self.k, 2)):\n                svd_counter = 0\n                svd_performed += 1\n                try:\n                    Y = np.vstack(disp_buffer).T  # n x m\n                    Y = Y - np.mean(Y, axis=1, keepdims=True)\n                    # full SVD on modest m is fine (buffer_max sized)\n                    U_new, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                    # obtain top-k\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U_refresh = U_new[:, :k_take]\n                        # mix old/new subspace gently for stability\n                        mix = 0.7\n                        if U.shape[1] != U_refresh.shape[1]:\n                            # pad if needed\n                            pad = np.zeros((n, max(0, self.k - U_refresh.shape[1])))\n                            U_refresh = np.hstack([U_refresh, pad])\n                        U = mix * U_refresh + (1.0 - mix) * U\n                        # re-orthonormalize\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n\n                    # compute explained variance ratio for top-k\n                    total_var = np.sum(svals ** 2) + 1e-20\n                    top_var = np.sum((svals[:self.k]) ** 2) if len(svals) >= self.k else np.sum(svals ** 2)\n                    explained = top_var / total_var\n                    # adapt rho toward explained variance (encourage subspace if it explains variance)\n                    rho = 0.85 * rho + 0.15 * float(np.clip(explained, 0.01, 0.98))\n                    rho = float(np.clip(rho, 0.05, 0.95))\n\n                    # occasionally do a directional line probe along top singular vector\n                    if (svd_performed % self.lineprobe_every == 0) and (remaining - (evals - evals) > 0):\n                        # ensure we have at least 2 extra evals for probe (we need up to 3)\n                        if evals + 2 <= budget:\n                            u1 = U[:, 0]\n                            # scale practical step length a\n                            a = self.lineprobe_scale * sigma * np.mean(D)\n                            # three probe points: m (we already have fm), m - a*u1, m + a*u1\n                            x_minus = np.clip(m - a * u1, lb, ub)\n                            x_plus = np.clip(m + a * u1, lb, ub)\n                            f_minus = func(x_minus); evals += 1\n                            # check budget before next\n                            if evals < budget:\n                                f_plus = func(x_plus); evals += 1\n                            else:\n                                f_plus = np.inf\n                            # quadratic fit if we have three values (f at center fm, f_minus, f_plus)\n                            t = np.array([-a, 0.0, a])\n                            fvals = np.array([f_minus, fm, f_plus])\n                            # fit parabola p(t) = at^2 + bt + c using polyfit\n                            try:\n                                coeff = np.polyfit(t, fvals, 2)\n                                a2, b2, c2 = coeff\n                                if abs(a2) > 1e-12:\n                                    t_star = -b2 / (2.0 * a2)\n                                    # only accept if within plausible region\n                                    if -2 * a <= t_star <= 2 * a:\n                                        x_star = np.clip(m + t_star * u1, lb, ub)\n                                        if evals < budget:\n                                            f_star = func(x_star); evals += 1\n                                            # if better than center fm, accept as new mean\n                                            if f_star < fm:\n                                                m = x_star.copy()\n                                                fm = f_star\n                                                if f_star < f_best:\n                                                    f_best = f_star\n                                                    x_best = x_star.copy()\n                                                    no_improve_since = 0\n                                    # else ignore\n                            except np.linalg.LinAlgError:\n                                pass\n                except np.linalg.LinAlgError:\n                    pass\n\n            # stagnation detection & mild restart / sigma inflation\n            no_improve_since += 0  # already tracked when updating f_best\n            # count since last improvement by checking simple criterion\n            if no_improve_since > self.stag_patience:\n                # inflate sigma and nudge mean toward a good archived point\n                sigma *= 2.5\n                if len(archive_X) > 0:\n                    pick = np.random.randint(0, len(archive_X))\n                    m = 0.5 * m + 0.5 * archive_X[pick]\n                # perturb U slightly\n                rand_mat = np.random.randn(n, self.k) * 0.2\n                try:\n                    Q, _ = np.linalg.qr(rand_mat + U * 0.2)\n                    U = Q[:, :self.k]\n                except np.linalg.LinAlgError:\n                    pass\n                disp_buffer = []\n                no_improve_since = 0\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # small safeguard: always keep best so far in archive\n            if x_best is not None:\n                archive_X.append(x_best.copy()); archive_F.append(f_best)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n\n            # break if budget exhausted (loop condition already guards)\n            # continue to next generation\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "61e0b794-2a70-4ab1-af6b-befa2f61a927", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "19364dd3-0494-48e5-91a7-e5b514143fc5", "fitness": 0.20038253748500018, "name": "HARPS", "description": "HARPS is a hybrid, population-based continuous optimizer that mixes per-coordinate RMSProp-style scaling (G, D) with a small learned low-rank subspace (U) and recombines candidates by weighted selection from a modestly sized, log-scaled population (lambda_); mirrored/antithetic sampling is used to reduce variance and the low-rank dimension k is chosen conservatively as n**0.4 to keep updates cheap. Exploration is diversified via occasional heavy-tailed Cauchy jumps (p_cauchy, cauchy_scale), archive-based DE differences (p_de, F_de) and momentum/inertia on the mean (momentum=0.8), while sigma is multiplicatively adapted by a soft success-rate rule (sigma_target, sigma_adapt_lr) using a rolling success_window. The subspace U is learned incrementally with a small power-iteration step (power_eta) on the weighted selected step y_w, periodically refreshed by an SVD on a success_buffer (buffer_max, svd_every) and alpha (mix between diagonal and subspace) is slowly pushed toward the fraction of energy explained by U. Parameter choices (e.g. sigma start = 0.12*domain_scale, rms_beta=0.5, moderate p_cauchy/p_de, svd_every=30, archive cap 5000) bias the method toward robust, conservative learning of useful directions with occasional aggressive exploration and mild restarts on stagnation.", "code": "import numpy as np\n\nclass HARPS:\n    \"\"\"\n    HARPS: Hybrid Adaptive Rank-Scaled Power-Subspace optimizer\n\n    Main idea (one-liner): Mix RMSProp-style per-coordinate scaling with a small learned\n    low-rank subspace (updated by a power-iteration style update + periodic SVD),\n    adapt the global step-size (sigma) by recent success rate (a soft 1/5th-rule),\n    use mirrored/antithetic sampling, occasional Cauchy leaps, and archive-based DE differences.\n\n    Notes on main parameters (tunable):\n    - lambda_: population size (function of dim)\n    - mu: number of selected parents (lambda_//2)\n    - k: low-rank subspace dimension (here set smaller than sqrt(n) by default)\n    - rms_beta: smoothing for RMSProp-like second moment (default 0.5)\n    - power_eta: learning rate for power-iteration subspace update (default 0.08)\n    - svd_every: how often to refresh U from buffer (default 30)\n    - buffer_max: capacity for success buffer used for SVD\n    - p_cauchy, cauchy_scale: probability and scale for heavy-tailed jumps\n    - p_de, F_de: probability and factor for DE-style archive difference perturbation\n    - sigma_target: desired success rate (default 0.2)\n    - sigma_adapt_lr: learning strength of multiplicative sigma update (default 0.18)\n    - alpha: mixing weight between diag and subspace (starts low, adapted slowly)\n    - momentum: inertia applied to the mean update\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 rng_seed=None,\n                 pop_coef=2.5,\n                 rms_beta=0.5,\n                 power_eta=0.08,\n                 svd_every=30,\n                 buffer_max_coef=8,\n                 p_cauchy=0.08,\n                 cauchy_scale=2.0,\n                 p_de=0.25,\n                 F_de=0.5,\n                 sigma_target=0.2,\n                 sigma_adapt_lr=0.18,\n                 mirrored=True,\n                 momentum=0.8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.RandomState(rng_seed)\n\n        # population size (different formula than original): grows gently with dim, ensure odd for pairing\n        self.lambda_ = max(6, int(np.ceil(pop_coef * (1 + np.log(max(2, self.dim))))))\n        if self.lambda_ % 2 == 0:\n            self.lambda_ += 1\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank dimension: a different scaling (n**0.4) instead of sqrt(n)\n        self.k = max(1, min(self.dim, int(np.maximum(1, np.round(self.dim ** 0.4)))))\n\n        # smoothing / learning rates (distinct from original)\n        self.rms_beta = float(rms_beta)       # for second moment smoothing (RMSProp-like)\n        self.power_eta = float(power_eta)     # for power-iteration style subspace update\n        self.svd_every = int(svd_every)\n        self.buffer_max = max(10, int(buffer_max_coef * self.k))\n\n        # exploration operators\n        self.p_cauchy = float(p_cauchy)\n        self.cauchy_scale = float(cauchy_scale)\n        self.p_de = float(p_de)\n        self.F_de = float(F_de)\n\n        # sigma adaptation by success rate (soft 1/5 rule)\n        self.sigma_target = float(sigma_target)\n        self.sigma_adapt_lr = float(sigma_adapt_lr)\n\n        self.mirrored = bool(mirrored)\n        self.momentum = float(momentum)\n\n        # internal randomness convenience\n        self._rand = self.rng\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds handling (BBOB style implies [-5,5] but follow func.bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_scale = np.mean(ub - lb)\n\n        # strategy params\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n\n        # initialize mean at random in box\n        m = self._rand.uniform(lb, ub)\n        sigma = 0.12 * domain_scale  # different starting sigma from original\n\n        # RMSProp-like second moment accumulator G and per-dim multiplier D\n        G = np.ones(n) * 1e-3\n        eps = 1e-9\n        D = (G + eps) ** -0.5\n\n        # subspace U initialization via random orthonormal basis (n x k)\n        if self.k > 0:\n            R = self._rand.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(R)\n                U = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        # success buffer for subspace SVD\n        success_buffer = []\n\n        # archive for DE style differences\n        archive_X = []\n        archive_F = []\n\n        # success-history window for sigma adaptation (record whether candidate improved best)\n        success_window = []\n\n        # momentum/inertia for mean updates\n        v = np.zeros(n)\n\n        # mixing weight between diag and subspace (start smaller than original)\n        alpha = 0.3\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = float(fm); x_best = xm.copy()\n\n        gen = 0\n\n        # main loop\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # create sample directions (use mirrored pairs if enabled)\n            half = (current_lambda + 1) // 2\n            z_base = self._rand.randn(half, n)\n            z_low_base = self._rand.randn(half, self.k) if self.k > 0 else np.zeros((half, 0))\n\n            # expand to full lambda using mirroring\n            zs = np.zeros((current_lambda, n))\n            zs_low = np.zeros((current_lambda, self.k))\n            for i in range(half):\n                zs[i] = z_base[i]\n                zs_low[i] = z_low_base[i]\n            if self.mirrored:\n                # mirror the first floor(lambda/2) samples\n                for j in range(current_lambda - half):\n                    zs[half + j] = -z_base[j]\n                    if self.k > 0:\n                        zs_low[half + j] = -z_low_base[j]\n            else:\n                # fill remaining with fresh draws\n                for j in range(current_lambda - half):\n                    idx = half + j\n                    zs[idx] = self._rand.randn(n)\n                    if self.k > 0:\n                        zs_low[idx] = self._rand.randn(self.k)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # generate population\n            for i in range(current_lambda):\n                z = zs[i].copy()\n                # decompose to subspace and orthogonal complement\n                if self.k > 0:\n                    proj_low = U @ (U.T @ z)\n                    proj_high = z - proj_low\n                else:\n                    proj_low = np.zeros(n)\n                    proj_high = z\n\n                # apply diag scaling to orthogonal part (use D)\n                y_diag = D * proj_high\n\n                # create low-rank component using zs_low, scale by median of D (different scaling)\n                if self.k > 0:\n                    low = U @ zs_low[i]\n                    y_sub = np.median(D) * low\n                else:\n                    y_sub = np.zeros(n)\n\n                # combine with geometric mixing to bias toward whichever is larger\n                y = (1.0 - alpha) * y_diag + alpha * y_sub\n\n                # occasional heavy-tailed jump (different probability & scale)\n                if self._rand.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * np.median(D)\n\n                x = m + sigma * y + (1.0 - self.momentum) * v  # use weaker inertia in additive form\n\n                # DE-style archive difference mutation with small probability, different F_de\n                if (self._rand.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = self._rand.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    # add partially via uniform mask of dimension-dependent density\n                    mask = (self._rand.rand(n) < min(0.6, 5.0 / n))\n                    x[mask] += de_mut[mask]\n\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # Evaluate candidates with strict budget\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = float(f)\n                archive_X.append(x.copy()); archive_F.append(float(f))\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                # update global best\n                improved = False\n                if f < f_best:\n                    f_best = float(f)\n                    x_best = x.copy()\n                    improved = True\n                success_window.append(1 if improved else 0)\n                if len(success_window) > 60:\n                    # keep a rolling window of last 60 evals\n                    success_window.pop(0)\n\n            # selection and recombination\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            # recombine mean using log-weights style\n            mean_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # update RMSProp-like second moment accumulator with selected y^2\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            G = self.rms_beta * G + (1.0 - self.rms_beta) * (y2 + eps)\n            D = (G + eps) ** -0.5\n\n            # momentum update (inertia)\n            v = self.momentum * v + 0.9 * sigma * y_w\n\n            # sigma adaptation by recent success rate (multiplicative)\n            if len(success_window) > 0:\n                succ_rate = np.mean(success_window)\n            else:\n                succ_rate = self.sigma_target\n            # multiplicative update: increase if success-rate > target, decrease otherwise (soft)\n            sigma *= np.exp(self.sigma_adapt_lr * (succ_rate - self.sigma_target))\n            # clamp sigma relative to domain\n            sigma = float(np.clip(sigma, 1e-12, 5.0 * domain_scale))\n\n            # subspace update via simple power-iteration-like step using the selected y_w (different from Oja)\n            if self.k > 0:\n                signal = y_w.copy()\n                norm_signal = np.linalg.norm(signal)\n                if norm_signal > 1e-12:\n                    signal = signal / (norm_signal + 1e-12)\n                    # apply one-step power update to each column with orthonormalization\n                    for j in range(self.k):\n                        u = U[:, j]\n                        # push towards signal but keep diversity via projection residual\n                        resid = signal - (u @ signal) * u\n                        u_new = u + self.power_eta * resid\n                        U[:, j] = u_new / (np.linalg.norm(u_new) + 1e-12)\n                    # occasional re-orthonormalize to stabilize\n                    if self._rand.rand() < 0.2:\n                        try:\n                            Q, _ = np.linalg.qr(U)\n                            U = Q[:, :self.k]\n                        except np.linalg.LinAlgError:\n                            pass\n\n            # store successful step (y_w) into success buffer for periodic SVD refresh\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.buffer_max:\n                success_buffer.pop(0)\n\n            # periodic SVD refresh to recapture principal directions (less frequent than original)\n            if (gen % self.svd_every == 0) and (len(success_buffer) >= max(3, self.k)):\n                try:\n                    Y = np.vstack(success_buffer).T  # n x m\n                    Y = Y - np.mean(Y, axis=1, keepdims=True)\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U_refresh = U_new[:, :k_take]\n                        # blend new and old more conservatively to avoid abrupt changes\n                        mix = 0.55\n                        if U.shape[1] == U_refresh.shape[1]:\n                            U = mix * U_refresh + (1.0 - mix) * U\n                        else:\n                            # pad / truncate as needed\n                            U_tmp = np.zeros((n, self.k))\n                            U_tmp[:, :U_refresh.shape[1]] = U_refresh\n                            U = mix * U_tmp + (1.0 - mix) * U\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                except np.linalg.LinAlgError:\n                    pass\n\n            # adapt alpha (mix weight) using a soft-saturating step different from original\n            if self.k > 0:\n                total_energy = np.sum(y_sel ** 2) + 1e-20\n                proj_energy = np.sum((U.T @ y_sel.T) ** 2)\n                frac = np.clip(np.sum(proj_energy) / total_energy, 0.0, 1.0)\n                # push alpha toward a sigmoid-transformed frac for smoother behavior\n                target_alpha = 1.0 / (1.0 + np.exp(-8.0 * (frac - 0.2)))\n                alpha += 0.04 * (target_alpha - alpha)\n                alpha = float(np.clip(alpha, 0.02, 0.95))\n\n            # stagnation / mild restart: if no improvement for a long time, perturb around best\n            # detect via last few successes\n            if (len(success_window) >= 60) and (np.sum(success_window[-60:]) == 0):\n                # mild restart: boost sigma and nudge mean to a random archive location near best\n                sigma *= 2.0\n                if x_best is not None and len(archive_X) > 0:\n                    pick = self._rand.randint(len(archive_X))\n                    m = 0.5 * x_best + 0.5 * archive_X[pick] + 0.02 * domain_scale * self._rand.randn(n)\n                # clear buffers to learn new directions\n                success_buffer = []\n                success_window = []\n\n            # keep mean in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HARPS scored 0.200 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "61e0b794-2a70-4ab1-af6b-befa2f61a927", "operator": null, "metadata": {"aucs": [0.10100147521041436, 0.14836051178501264, 0.2474367477871705, 0.27002698000183334, 0.21617039040728747, 0.24049476694734706, 0.18839814975061275, 0.23398769834768074, 0.203333620998554, 0.15461503361408835]}, "task_prompt": ""}
{"id": "3c36a0d7-2646-4302-9cd3-f367777b7c44", "fitness": 0.15955438045989387, "name": "MOSAIC", "description": "MOSAIC combines multiple complementary generators (coordinate-wise scaled Gaussian, learned low-rank samples on U, and structured rotational probes on V) whose contributions are learned by multiplicative/log-weight updates so the algorithm automatically biases toward the most useful search modes. It learns a principal subspace U online via Oja updates and periodic robust SVD sketches (svd_every, buffer_max) while keeping a small structured rotational basis V; population size is scaled modestly with dimension (lambda ~ 4 + 3 log n, k ~ ceil(sqrt(n))) to balance exploration and cost. Per-dimension adaptation uses an RMS accumulator G (rho_G = 0.85) and a multiplicative temperature tau to shape coordinate scales D, combined with momentum (momentum_decay = 0.8), mirrored sampling to reduce variance, occasional Lévy-like heavy tails (p_levy = 0.10) and DE-style archive differences (p_de = 0.22) for long-range moves. Global step-size sigma and a trust-region radius are adjusted by a success-rate proxy (soft 1/5-like control), recombination uses weighted mu-selection, and mild restarts / sigma inflation and large archive capacity (archive_limit = 5000) handle stagnation and maintain diversity.", "code": "import numpy as np\n\nclass MOSAIC:\n    \"\"\"\n    MOSAIC: Multi-scale Orthogonal Subspace Adaptive Interleaved Controller\n\n    Main ideas & novel elements (short):\n    - Multi-generator sampling: coordinate-wise scaled Gaussian, learned low-rank principal subspace,\n      and a structured orthonormal rotational subspace. Generators compete via multiplicative weight updates.\n    - Two complementary subspaces: U (learned via power/Oja & periodic SVD sketch) and V (structured random rotations).\n    - Per-dimension adaptive scale (RMS-style) combined with a multiplicative per-dim \"temperature\" tau.\n    - Trust-region radius R that contracts/expands by observed success rate (noisy 1/5-style proxy).\n    - Occasional Lévy-like (Cauchy) jumps, DE-style archive differences with adaptive F, mirrored sampling,\n      momentum, and budget-aware selection.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size & selection\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n        # subspace sizes\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n        self.k_rot = max(1, min(self.k, self.dim // 2))\n\n        # generator & mutation probabilities\n        self.p_levy = 0.10\n        self.levy_scale = 1.0\n        self.p_de = 0.22\n        self.F_de_init = 0.8\n\n        # dynamics\n        self.mirrored = True\n        self.momentum_decay = 0.80\n        self.oja_eta = 0.08\n        self.svd_every = 30\n        self.buffer_max = max(20, 6 * self.k)\n        self.archive_limit = 5000\n\n        # adaptation speeds\n        self.rho_G = 0.85    # RMS smoothing\n        self.eta_tau = 0.02  # per-dim multiplicative temp updates\n        self.eta_gen = 0.25  # generator multiplicative weight learning rate\n        self.succ_window = 40\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (Many Affine BBOB uses [-5,5] but respect func.bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_scale = np.mean(ub - lb)\n\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n\n        # initialize search state\n        m = np.random.uniform(lb, ub)           # mean\n        sigma = 0.25 * domain_scale             # global scale\n        momentum = np.zeros(n)\n\n        # per-dim RMS accumulator and multiplicative temperature tau\n        G = np.ones(n) * 1e-3\n        eps = 1e-9\n        D = (G + eps) ** -0.5\n        tau = np.ones(n)  # multiplicative per-dim temperature\n\n        # principal subspace U (n x k) via QR, and rotational basis V (n x k_rot)\n        randU = np.random.randn(n, self.k)\n        Q, _ = np.linalg.qr(randU)\n        U = Q[:, :self.k]\n\n        randV = np.random.randn(n, self.k_rot)\n        Qv, _ = np.linalg.qr(randV)\n        V = Qv[:, :self.k_rot]\n\n        # generator log-weights: 0=coord,1=lowrank,2=rotational\n        logw_gen = np.zeros(3)\n\n        # adaptive DE scale\n        F_de = self.F_de_init\n\n        # archive of evaluated points\n        archive_X = []\n        archive_F = []\n\n        success_buffer = []\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # initial evaluation of mean\n        xm = np.clip(m, lb, ub)\n        fm = func(xm)\n        evals += 1\n        archive_X.append(xm.copy()); archive_F.append(fm)\n        f_best = fm; x_best = xm.copy()\n        f_ref = fm  # reference fitness used by s-rate proxy\n\n        gen = 0\n        no_improve_evals = 0\n\n        # trust-region radius (norm of step allowed before clipping)\n        trust_R = 3.0 * sigma\n\n        # small buffer for sketch SVD\n        svd_buffer = []\n\n        # main loop\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # prepare storage\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n            gen_ids = np.zeros(current_lambda, dtype=int)\n            made_by = np.zeros(current_lambda, dtype=float)  # some per-sample info\n\n            # compute generator probabilities (softmax of log-weights)\n            probs = np.exp(logw_gen - np.max(logw_gen))\n            probs = probs / np.sum(probs)\n\n            # mirrored handling: generate pairs (if odd, last unpaired)\n            pairs = []\n            if self.mirrored:\n                half = (current_lambda + 1) // 2\n                # create half samples and mirror them to fill lambda\n                idx = 0\n                for i in range(half):\n                    pairs.append((idx, None))  # idx placeholder\n                    idx += 1\n                # We'll fill arx by generating half and mirroring\n                total_pairs = half\n            else:\n                total_pairs = current_lambda\n\n            # For simplicity generate samples sequentially with mirrored rule:\n            base_needed = (current_lambda + 1) // 2 if self.mirrored else current_lambda\n            base_samples = []\n            base_gen_ids = []\n            base_arz = []\n\n            for b in range(base_needed):\n                # choose generator for this base sample\n                g = np.random.choice(3, p=probs)\n                base_gen_ids.append(g)\n\n                # produce a direction y (unitless)\n                if g == 0:\n                    # coordinate-wise Gaussian scaled by tau and D\n                    z = np.random.randn(n)\n                    y = (D * tau) * z\n                    made_by[b] = 0.0\n                elif g == 1:\n                    # low-rank sample along U\n                    z_k = np.random.randn(self.k)\n                    low = U @ z_k\n                    y = low * np.mean(D) * np.sqrt(np.mean(tau))\n                    made_by[b] = 1.0\n                else:\n                    # rotational structured exploration via V\n                    z_r = np.random.randn(self.k_rot)\n                    rot = V @ z_r\n                    # mix a little coordinate noise so not purely low-rank\n                    y = rot * np.mean(D) + 0.15 * (D * np.random.randn(n))\n                    made_by[b] = 2.0\n\n                # small orthogonal jitter for full-dimensionality\n                y = y + 0.02 * (D * np.random.randn(n))\n\n                # occasional Lévy-like heavy tail using scaled Cauchy mixture\n                if np.random.rand() < self.p_levy:\n                    r = np.random.standard_cauchy() * self.levy_scale\n                    ynorm = np.linalg.norm(y) + 1e-20\n                    y = r * (y / ynorm) * np.mean(D) * 1.2\n\n                base_arz.append(y.copy())\n\n            # fill arx/arz with base samples and mirrored copies if requested\n            idx = 0\n            for b in range(base_needed):\n                y = base_arz[b].copy()\n                x = m + sigma * y + 0.4 * momentum\n                # possibly add DE difference\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    mask = (np.random.rand(n) < 0.5)\n                    x[mask] += de_mut[mask]\n                x = np.clip(x, lb, ub)\n                arx[idx] = x\n                arz[idx] = y\n                gen_ids[idx] = base_gen_ids[b]\n                idx += 1\n                # mirrored copy if space\n                if self.mirrored and idx < current_lambda:\n                    y2 = -y\n                    x2 = m + sigma * y2 + 0.4 * momentum\n                    if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                        mask = (np.random.rand(n) < 0.5)\n                        x2[mask] += de_mut[mask]\n                    x2 = np.clip(x2, lb, ub)\n                    arx[idx] = x2\n                    arz[idx] = y2\n                    gen_ids[idx] = base_gen_ids[b]\n                    idx += 1\n\n            # ensure we didn't exceed current_lambda\n            # Evaluate candidates careful with budget\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                # update best\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n                    no_improve_evals = 0\n                # record success buffer (we store y and f)\n                svd_buffer.append((arz[i].copy(), f))\n                if len(svd_buffer) > self.buffer_max:\n                    svd_buffer.pop(0)\n            # selection & recombination\n            idxs = np.argsort(arfit)\n            sel = idxs[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n            gen_sel = gen_ids[sel]\n\n            m_old = m.copy()\n            # recombine using log-weights (weights computed earlier)\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted step in y\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # success proxy: fraction of offspring improving over recent reference f_ref\n            # f_ref = median of last few archive evaluations\n            lastk = min(len(archive_F), self.succ_window)\n            if lastk >= 3:\n                f_ref = np.median(np.array(archive_F[-lastk:]))\n            else:\n                f_ref = f_best\n            srate = np.sum(arfit[:current_lambda] < f_ref) / float(max(1, current_lambda))\n\n            # adaptive sigma using success-proxy (soft 1/5-rule)\n            target_sr = 0.2\n            kappa = 0.6\n            sigma *= np.exp(kappa * (srate - target_sr) / max(1.0, np.sqrt(n)))\n            # clamp sigma sensibly\n            sigma = float(np.clip(sigma, 1e-12, 10.0 * domain_scale))\n\n            # clip step vector by trust region radius (norm)\n            ynorm = np.linalg.norm(y_w)\n            if ynorm > 1e-12 and ynorm * sigma > trust_R:\n                y_w = y_w * (trust_R / (sigma * ynorm))\n\n            # update RMS G and D from weighted second moments of selected y\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            G = self.rho_G * G + (1.0 - self.rho_G) * (y2 + eps)\n            D = (G + eps) ** -0.5\n\n            # multiplicative per-dim temperature updates based on sign of projection of y_w\n            # if y_w has consistent sign on coordinate, raise tau, else slightly decay\n            proj_sign = np.sign(y_w)\n            tau *= np.exp(self.eta_tau * proj_sign * np.clip(np.abs(y_w) / (np.max(np.abs(y_w)) + 1e-12), -1, 1))\n\n            # momentum update\n            momentum = self.momentum_decay * momentum + 0.9 * sigma * y_w\n\n            # update principal subspace U via Oja using the successful direction y_w (normalized)\n            if self.k > 0:\n                sig = y_w.copy()\n                s_norm = np.linalg.norm(sig)\n                if s_norm > 1e-12:\n                    sig = sig / (s_norm + 1e-12)\n                    for j in range(U.shape[1]):\n                        u = U[:, j]\n                        proj = np.dot(u, sig)\n                        u = u + self.oja_eta * (proj * sig - proj ** 2 * u)\n                        u = u / (np.linalg.norm(u) + 1e-12)\n                        U[:, j] = u\n                    # occasional re-orthonormalize\n                    if np.random.rand() < 0.12:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n\n            # periodic SVD refresh of U from svd_buffer but with robust scaling\n            if (gen % self.svd_every == 0) and (len(svd_buffer) >= min(self.k, 3)):\n                # build matrix of successful y weighted by improvement\n                Ys = []\n                for y_vec, f_val in svd_buffer:\n                    weight = max(0.0, (np.median(archive_F[-min(len(archive_F), 20):]) - f_val))\n                    Ys.append(np.sqrt(1.0 + weight) * y_vec)\n                if len(Ys) >= 2:\n                    Y = np.vstack(Ys).T  # n x m\n                    Y = Y - np.mean(Y, axis=1, keepdims=True)\n                    try:\n                        U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                        k_take = min(self.k, U_new.shape[1])\n                        if k_take > 0:\n                            U_refresh = U_new[:, :k_take]\n                            # mix new with old for stability\n                            alpha_mix = 0.6\n                            if U_refresh.shape[1] < self.k:\n                                pad = np.zeros((n, self.k - U_refresh.shape[1]))\n                                U_refresh = np.hstack([U_refresh, pad])\n                            U = alpha_mix * U_refresh + (1 - alpha_mix) * U\n                            Q, _ = np.linalg.qr(U)\n                            U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # Update generator weights logw_gen using multiplicative rule:\n            # reward generators that contributed to selected winners (rank-weighted)\n            reward = np.zeros_like(logw_gen)\n            # give higher reward to better ranks\n            ranks = np.argsort(arfit[sel])\n            for rank_idx, idx_sel in enumerate(ranks):\n                g = gen_sel[idx_sel]\n                # rank-based reward (higher for top ranks)\n                r = (mu - rank_idx) / float(mu)\n                reward[g] += r\n            # baseline normalization\n            if np.sum(reward) > 0:\n                # apply log-space additive update (multiplicative in probability space)\n                logw_gen += self.eta_gen * (reward / (np.sum(reward) + 1e-12))\n            # tiny random shake to avoid freezing\n            logw_gen += 0.005 * (np.random.rand(*logw_gen.shape) - 0.5)\n\n            # adapt DE scale F_de mildly using simple success heuristic\n            # if many selected had been produced by DE-like mutation (we don't track that exactly), adjust F_de\n            if srate > 0.25:\n                F_de = min(1.0, F_de * 1.02)\n            else:\n                F_de = max(0.4, F_de * 0.99)\n\n            # adapt trust_R\n            if srate > target_sr:\n                trust_R *= 1.03\n            else:\n                trust_R *= 0.97\n            trust_R = float(np.clip(trust_R, 0.2 * domain_scale, 50.0 * domain_scale))\n\n            # store success buffer entries for later\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.buffer_max:\n                success_buffer.pop(0)\n\n            # stagnation detection: if long without improvement, mild restart / jitter and sigma inflation\n            no_improve_evals += 1\n            if no_improve_evals * 1 > max(400, 20 * n) and (evals - (current_lambda)) > 0:\n                # mild restart: perturb mean toward random archive and increase sigma\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.5 * m + 0.5 * archive_X[pick] + 0.1 * np.random.randn(n) * domain_scale\n                sigma *= 2.5\n                trust_R = max(trust_R, 2.0 * sigma)\n                momentum[:] = 0.0\n                # reset generator exploration to encourage diversity\n                logw_gen = np.zeros_like(logw_gen)\n                no_improve_evals = 0\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # update best-tracking reset counters\n            # if we improved global best in this generation, reset stagnation counter\n            if f_best < np.min(archive_F[:-current_lambda]) if len(archive_F) > current_lambda else False:\n                no_improve_evals = 0\n\n            # end generation\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm MOSAIC scored 0.160 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "61e0b794-2a70-4ab1-af6b-befa2f61a927", "operator": null, "metadata": {"aucs": [0.0865684714364493, 0.10375748693157494, 0.22654811097081384, 0.1743730351752577, 0.18947910694640446, 0.17237672019400985, 0.17344615135521002, 0.16696631402764384, 0.176777539857216, 0.12525086770435856]}, "task_prompt": ""}
{"id": "0feb6cc9-ff68-46a4-aced-4ccc00e0e167", "fitness": 0.12387681400638106, "name": "DASS", "description": "The algorithm is a dual-scale adaptive search that combines a global coarse step (sigma_coarse, initialized at 0.15·domain_scale and adapted by a path-length term cs/chi_n plus a success-rate multiplier) with a per-coordinate fine scale D computed by an RMSProp-like accumulator G (beta_g=0.5) to give anisotropic, coordinate-wise steps; momentum v and mirrored sampling are used to stabilize and diversify moves. It maintains a learned low-rank search subspace U (k = ceil(n/3)) trained online via small-step Oja updates (oja_eta=0.02) and periodic SVD refreshes (svd_every=15, buffer_max = max(30,6k)); a multiplicative mixing factor gamma (start 0.4) adaptively blends subspace and diagonal components based on projected energy. Robust exploration is injected via occasional heavy-tailed Student-t jumps (p_student=0.08, df=3) and DE-style archive-difference mutations (p_de=0.35, F_de=0.9) using a bounded archive (archive_limit=5000), while a small population (lambda ≈ 3+2·log(n), mu=lambda//2) with log-recombination weights and path ps implements CMA-like selection and recombination. Additional practical designs include strict budget-aware evaluations and bound clipping, success-windowed sigma control and stagnation detection with mild restarts to inflate sigma and nudge the mean, all tuned to favor faster adaptation and robust global exploration.", "code": "import numpy as np\n\nclass DASS:\n    \"\"\"\n    DASS: Dual-Scale Adaptive Subspace Search\n\n    Key differences / parameter choices (compared to the provided AHLDE example):\n    - lambda ~ 3 + 2*log(n) (smaller pop), mu = lambda//2\n    - subspace k = ceil(n/3) (larger than sqrt(n) for many dims)\n    - two-scale step control: sigma_coarse (global, CMA-like + success-based) and per-coordinate D (RMSProp-like, beta_g=0.5)\n    - multiplicative mixing gamma (starts 0.4) between subspace and diag components, updated multiplicatively\n    - Student-t heavy tails (df=3) with p_student=0.08\n    - DE archive differences with p_de=0.35, F_de=0.9\n    - Oja eta reduced (0.02), SVD refresh more frequent (svd_every=15), buffer_max tuned\n    - sigma update combines a modified path-length term and a success-rate based multiplicative term\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population & selection (different scaling)\n        self.lambda_ = max(4, int(3 + np.floor(2 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank dimension (different from sqrt heuristic)\n        if subspace_k is None:\n            self.k = min(max(1, int(np.ceil(self.dim / 3.0))), self.dim)\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # exploration / adaptation hyperparameters (different values)\n        self.p_student = 0.08\n        self.student_df = 3.0\n        self.p_de = 0.35\n        self.F_de = 0.9\n        self.mirrored = True\n        self.momentum_decay = 0.90\n        self.oja_eta = 0.02\n        self.svd_every = 15\n        self.buffer_max = max(30, 6 * self.k)\n        self.archive_limit = 5000\n\n        # success tracking window for success-rate-based sigma adaptation\n        self.success_window = max(10, self.lambda_)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (the problem statement says [-5,5], but use func.bounds if present)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_scale = np.mean(ub - lb)\n\n        # recombination weights (log-weights similar idea)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # modified cs/damps/chi_n (different equation choices)\n        cs = (mu_eff + 1.0) / (n + mu_eff + 3.0)\n        damps = 1.0 + cs + 0.5\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize mean uniformly in domain\n        m = np.random.uniform(lb, ub)\n        sigma_coarse = 0.15 * domain_scale  # different init scale\n        ps = np.zeros(n)  # path vector\n\n        # RMSProp-like accumulator and derived diagonal scale D (beta_g larger -> faster learning)\n        G = np.ones(n) * 1e-3\n        eps = 1e-8\n        D = (G + eps) ** -0.5\n\n        # low-rank subspace U via QR\n        if self.k > 0:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand_mat)\n                U = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n\n        # buffers and archive\n        success_buffer = []\n        archive_X = []\n        archive_F = []\n\n        # momentum/inertia\n        v = np.zeros(n)\n\n        # multiplicative mixing between subspace and diag (gamma in (0,1), different start and update)\n        gamma = 0.4\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # initial mean evaluation\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = fm; x_best = xm.copy()\n\n        # success tracking\n        recent_successes = []\n\n        gen = 0\n        avg_fit_prev = fm\n\n        # main loop\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # pre-sample noises\n            zs = np.random.randn(current_lambda, n)\n            zs_low = np.random.randn(current_lambda, self.k) if self.k > 0 else np.zeros((current_lambda, 0))\n\n            for i in range(current_lambda):\n                z = zs[i].copy()\n\n                # decompose signal into subspace + orthogonal complement\n                if self.k > 0:\n                    proj_low = U @ (U.T @ z)\n                    proj_high = z - proj_low\n                else:\n                    proj_low = np.zeros(n); proj_high = z\n\n                # diag scaled orthogonal part (per-coordinate fine-scale)\n                y_diag = D * proj_high\n\n                # subspace component scaled by median of D to reduce sensitivity to outliers\n                if self.k > 0:\n                    low = U @ zs_low[i]\n                    scale_sub = np.median(D)\n                    y_sub = scale_sub * low\n                else:\n                    y_sub = np.zeros(n)\n\n                # multiplicative mixing (different from convex mixing)\n                # sample factor p ~ lognormal-like near 1, combine multiplicatively\n                mix_factor = gamma + 0.1 * (np.random.randn() * 0.5)\n                mix_factor = np.clip(mix_factor, 0.02, 0.98)\n                # Combine: elementwise multiplicative blend to allow amplification of coordinates where subspace matters\n                y = (1.0 - mix_factor) * y_diag + mix_factor * y_sub\n\n                # occasional Student-t heavy-tail jump (df small) instead of pure Cauchy\n                if np.random.rand() < self.p_student:\n                    r = np.random.standard_t(self.student_df)\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * np.median(D)\n\n                # mirrored sampling\n                if self.mirrored and (i % 2 == 1):\n                    y = -y\n\n                # combine global coarse sigma and momentum\n                x = m + sigma_coarse * y + 0.3 * v\n\n                # DE-style archive difference mutation (more aggressive)\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    mask = (np.random.rand(n) < 0.5)\n                    x[mask] += de_mut[mask]\n\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # Evaluate candidates with strict budget accounting\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # selection and recombination\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # update path ps with slightly different scaling (uses invdiag)\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n\n            # compute a local success rate relative to previous generation average fitness\n            # success if offspring better than previous gen average (avg_fit_prev)\n            succ = np.sum(arfit[:current_lambda] < avg_fit_prev)\n            success_rate = float(succ) / float(max(1, current_lambda))\n            recent_successes.append(success_rate)\n            if len(recent_successes) > self.success_window:\n                recent_successes.pop(0)\n            avg_success = float(np.mean(recent_successes))\n\n            # sigma_coarse update: combine path-length signal and success-rate multiplicative adjustment\n            # different equation: additive exponent of combined signals\n            sigma_coarse *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0) + 0.35 * (avg_success - 0.2))\n            sigma_coarse = float(np.clip(sigma_coarse, 1e-12, 5.0 * domain_scale))\n\n            # update RMSProp-like G and D (beta_g = 0.5)\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            beta_g = 0.5\n            G = (1.0 - beta_g) * G + beta_g * (y2 + eps)\n            D = (G + eps) ** -0.5\n\n            # momentum/inertia update (different multiplier)\n            v = self.momentum_decay * v + 0.9 * sigma_coarse * y_w\n\n            # online Oja update of U with smaller learning rate\n            if self.k > 0:\n                sig = y_w.copy()\n                nrm = np.linalg.norm(sig)\n                if nrm > 1e-12:\n                    sig = sig / (nrm + 1e-12)\n                    for j in range(U.shape[1]):\n                        u = U[:, j]\n                        proj = np.dot(u, sig)\n                        u = u + self.oja_eta * proj * (sig - proj * u)\n                        u = u / (np.linalg.norm(u) + 1e-12)\n                        U[:, j] = u\n                    # re-orthonormalize occasionally (slightly more often)\n                    if np.random.rand() < 0.25:\n                        try:\n                            Q, _ = np.linalg.qr(U)\n                            U = Q[:, :self.k]\n                        except np.linalg.LinAlgError:\n                            pass\n\n            # store success into buffer\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.buffer_max:\n                success_buffer.pop(0)\n\n            # periodic SVD refresh (more frequent)\n            if (gen % self.svd_every == 0) and (len(success_buffer) >= self.k):\n                try:\n                    Y = np.vstack(success_buffer).T  # n x m\n                    Y = Y - np.mean(Y, axis=1, keepdims=True)\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        # stronger mixing to favor new directions\n                        alpha_mix = 0.8\n                        U_refresh = U_new[:, :k_take]\n                        if U_refresh.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U_refresh.shape[1]))\n                            U_refresh = np.hstack([U_refresh, pad])\n                        U = alpha_mix * U_refresh + (1 - alpha_mix) * U\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                except np.linalg.LinAlgError:\n                    pass\n\n            # adapt multiplicative mixing gamma based on fraction of energy in subspace (multiplicatively)\n            if self.k > 0:\n                total_energy = np.sum(y_sel ** 2)\n                if total_energy > 0:\n                    proj_energy = np.sum((U.T @ y_sel.T) ** 2)\n                    frac = np.sum(proj_energy) / (total_energy + 1e-20)\n                    # multiplicative move toward frac: factor ~1 + 0.12*(frac - 0.5)\n                    factor = 1.0 + 0.12 * (frac.mean() - 0.5)\n                    gamma *= factor\n                    gamma = float(np.clip(gamma, 0.03, 0.97))\n\n            # update avg_fit_prev for next generation (exponential smoothing)\n            avg_fit_prev = 0.8 * avg_fit_prev + 0.2 * np.mean(arfit[np.isfinite(arfit)]) if np.any(np.isfinite(arfit)) else avg_fit_prev\n\n            # stagnation detection: if no improvement over long period, reset some components mildly\n            if len(archive_F) >= 1:\n                # check last improvements\n                worst_window = int(min(len(archive_F), max(200, 10 * n)))\n                recent_best = np.min(archive_F[-worst_window:])\n                if recent_best >= f_best - 1e-12 and (len(archive_F) >= worst_window):\n                    # mild restart: inflate sigma and nudge mean toward a random archive point\n                    sigma_coarse *= 2.0\n                    if len(archive_X) > 0:\n                        pick = np.random.randint(len(archive_X))\n                        m = 0.5 * m + 0.5 * archive_X[pick]\n                    # clear success buffer but keep subspace to avoid losing learned directions completely\n                    success_buffer = []\n                    recent_successes = []\n                    # slightly reduce gamma to enforce more diag exploration\n                    gamma = max(0.05, gamma * 0.85)\n\n            # keep mean in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm DASS scored 0.124 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "61e0b794-2a70-4ab1-af6b-befa2f61a927", "operator": null, "metadata": {"aucs": [0.04627046763523612, 0.10257256706526552, 0.18759590023882333, 0.13112264970985776, 0.10938680968831349, 0.14205926034987848, 0.15235601693317968, 0.12857095748999525, 0.12653239585310205, 0.11230111510015905]}, "task_prompt": ""}
{"id": "d60f5a33-136a-4056-a058-2c520d5a1f69", "fitness": 0.15385848400097163, "name": "EnsembleHybrid", "description": "EnsembleHybrid is an adaptive ensemble optimizer that blends three complementary operators—per-coordinate AdaGrad-like scaling (cheap diagonal adaptation), a learned low‑rank correlated subspace (Oja online updates with periodic SVD refresh), and archive-driven DE-style difference mutations—to capture both independent and correlated search directions. Operator allocation is learned online via softmax log-weights updated from an EMA of per-operator rewards (op_lr ≈ 0.12), while population size (lambda) and selection (mu) scale with dimension and use log‑rank recombination weights (mu_eff) for stable weighted means. Global step-size follows a CMA-like path-length update (ps, cs, damps, chi_n) and momentum/inertia terms (momentum_decay 0.86, inertia_scale 0.38) smooth moves; per-coordinate variability is controlled by an AdaGrad accumulator G → D and occasional heavy‑tailed Cauchy jumps (p_cauchy 0.12) for long-range escapes. Additional practical heuristics include mirrored sampling, archive maintenance for DE differences, stagnation detection that inflates sigma and nudges the mean toward archived solutions, and bounds handling plus budget-aware evaluation.", "code": "import numpy as np\n\nclass EnsembleHybrid:\n    \"\"\"\n    EnsembleHybrid: Adaptive ensemble of Diagonal-AdaGrad, Low-Rank (Oja+SVD) and Archive-DE operators.\n    One-line: Blend cheap per-coordinate adaptation with a learned low-rank correlated subspace and\n    archive-driven difference exploration, allocating generation effort adaptively by recent success.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population & selection\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n        # recombination weights (log-based)\n        weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = weights / np.sum(weights)\n        self.mu_eff = 1.0 / np.sum(self.weights ** 2)\n\n        # low-rank dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # operator mixture: 0=diag,1=lowrank,2=DE\n        # initial soft preferences (log-weights)\n        self.op_logw = np.log(np.array([0.4, 0.4, 0.2], dtype=float) + 1e-12)\n        # learning rate for operator allocation\n        self.op_lr = 0.12\n\n        # AdaGrad-like params\n        self.G_eps = 1e-8\n        self.beta_g = 0.2  # smoothing into G\n\n        # Oja and SVD refresh\n        self.oja_eta = 0.06\n        self.svd_every_gen = 20\n        self.svd_buffer_max = max(20, 8 * self.k)\n\n        # DE params\n        self.p_de_apply = 0.75  # when operator==2 we often use DE style difference\n        self.F_de = 0.75\n\n        # heavy tails\n        self.p_cauchy = 0.12\n        self.cauchy_scale = 1.0\n\n        # momentum\n        self.momentum_decay = 0.86\n        self.inertia_scale = 0.38\n\n        # mirrored sampling\n        self.mirrored = True\n\n        # stagnation behavior\n        self.stagnation_window = max(50, 5 * self.dim)\n        self.stagnation_sigma_boost = 2.5\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds in BBOB: -5..5 usually, but use func.bounds if present\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_scale = float(np.mean(ub - lb))\n\n        # strategy params for sigma path-length (CMA-like)\n        mu_eff = self.mu_eff\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.18 * domain_scale\n        ps = np.zeros(n)\n        # AdaGrad accumulator G -> D\n        G = np.ones(n) * 1e-2\n        D = (G + self.G_eps) ** -0.5\n\n        # low-rank subspace U via QR\n        rand_mat = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand_mat)\n            U = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = np.zeros((n, self.k))\n\n        # success buffer for SVD refresh\n        success_buffer = []\n\n        # archive for DE differences\n        archive_X = []\n        archive_F = []\n\n        # operator moving success memory for softmax allocation\n        op_reward_ema = np.zeros(3, dtype=float)\n\n        # momentum v\n        v = np.zeros(n)\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n        gen = 0\n        last_improve_eval = 0\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = fm; x_best = xm.copy(); last_improve_eval = evals\n\n        # main loop\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            current_lambda = min(self.lambda_, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n            ar_op = np.zeros(current_lambda, dtype=int)\n\n            # compute operator probabilities via softmax of op_logw\n            op_probs = np.exp(self.op_logw - np.max(self.op_logw))\n            op_probs = op_probs / np.sum(op_probs)\n\n            # pre-sample gaussian components\n            base_z = np.random.randn(current_lambda, n)\n            zs_low = np.random.randn(current_lambda, self.k) if self.k > 0 else np.zeros((current_lambda, 0))\n\n            for i in range(current_lambda):\n                # pick operator\n                op = np.random.choice(3, p=op_probs)\n                ar_op[i] = op\n\n                z = base_z[i].copy()\n                y = np.zeros(n)\n\n                # mirrored parity: if mirrored and odd index use negation at end\n                mirror_flip = (self.mirrored and (i % 2 == 1))\n\n                # operator 0: diagonal AdaGrad-scaling move\n                if op == 0:\n                    y = D * z\n                    # modest projection on low-rank to keep some correlation\n                    if self.k > 0 and (np.random.rand() < 0.08):\n                        y += 0.2 * (U @ (U.T @ z))\n\n                # operator 1: low-rank dominated move\n                elif op == 1:\n                    if self.k > 0:\n                        low = U @ zs_low[i]\n                        # combine subspace and diag remainder\n                        y = 0.85 * (np.mean(D) * low) + 0.15 * (D * z)\n                    else:\n                        y = D * z\n\n                # operator 2: archive-DE style difference mutation (plus baseline gaussian)\n                else:\n                    # baseline gaussian\n                    if self.k > 0 and np.random.rand() < 0.5:\n                        low = U @ zs_low[i]\n                        y = 0.5 * (np.mean(D) * low) + 0.5 * (D * z)\n                    else:\n                        y = D * z\n                    # if enough archive, add DE difference\n                    if (np.random.rand() < self.p_de_apply) and (len(archive_X) >= 2):\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                        # add partial DE mutation (stochastic per-dim)\n                        mask = (np.random.rand(n) < 0.5)\n                        y = y + de_mut * mask\n\n                # occasional heavy-tailed Cauchy jump\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * np.mean(D)\n\n                if mirror_flip:\n                    y = -y\n\n                # candidate\n                x = m + sigma * y + self.inertia_scale * v\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # Evaluate candidates (budget-aware)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if f < f_best:\n                    f_best = f; x_best = x.copy(); last_improve_eval = evals\n\n            # selection + recombination\n            idx = np.argsort(arfit)\n            sel = idx[:self.mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n            ops_sel = ar_op[sel]\n\n            m_old = m.copy()\n            # recombine mean\n            m = np.sum(self.weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space (I use the y_sel as approx (x-m_old)/sigma)\n            y_w = np.sum(self.weights[:, None] * y_sel, axis=0)\n\n            # update path ps using invdiag approximation\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n\n            # update sigma (CMA-like)\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = float(np.clip(sigma, 1e-12, 1e2 * domain_scale))\n\n            # update AdaGrad accumulator G from selected y's (weighted second moment)\n            y2 = np.sum(self.weights[:, None] * (y_sel ** 2), axis=0)\n            G = (1.0 - self.beta_g) * G + self.beta_g * (y2 + self.G_eps)\n            D = (G + self.G_eps) ** -0.5\n\n            # update momentum/inertia\n            v = self.momentum_decay * v + 0.95 * sigma * y_w\n\n            # Oja update for U (use normalized y_w as signal)\n            sig_vec = y_w.copy()\n            nrm = np.linalg.norm(sig_vec)\n            if self.k > 0 and nrm > 1e-12:\n                sig_norm = sig_vec / (nrm + 1e-12)\n                for j in range(U.shape[1]):\n                    u = U[:, j]\n                    proj = np.dot(u, sig_norm)\n                    u = u + self.oja_eta * proj * (sig_norm - proj * u)\n                    u = u / (np.linalg.norm(u) + 1e-12)\n                    U[:, j] = u\n                # occasional re-orthonormalize\n                if (gen % 5) == 0:\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # store into success buffer the weighted mean step for later SVD\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.svd_buffer_max:\n                success_buffer.pop(0)\n\n            # periodic SVD refresh to stabilize U\n            if (gen % self.svd_every_gen == 0) and (len(success_buffer) >= max(2, self.k)):\n                try:\n                    Y = np.vstack(success_buffer).T  # n x m\n                    Y = Y - np.mean(Y, axis=1, keepdims=True)\n                    U_new, svals, Vt = np.linalg.svd(Y, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U_refresh = U_new[:, :k_take]\n                        if U_refresh.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U_refresh.shape[1]))\n                            U_refresh = np.hstack([U_refresh, pad])\n                        # mix new and old for stability\n                        alpha_mix = 0.7\n                        U = alpha_mix * U_refresh + (1.0 - alpha_mix) * U\n                        # orthonormalize\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                except np.linalg.LinAlgError:\n                    pass\n\n            # Update operator rewards and log-weights (softmax bandit-like)\n            # reward for an operator: sum of (improvement amount) for selected offspring that belong to that operator\n            # compute baseline: best of generation prior to selection (we approximate baseline by f_best_old)\n            # For safety, compute improvements as positive values only when selected individuals improved over previous global best\n            op_reward = np.zeros(3, dtype=float)\n            # Use selection set fitness relative to previous mean-best (approx) to compute reward\n            for j, sel_idx in enumerate(sel):\n                op_j = ar_op[sel_idx]\n                f_j = arfit[sel_idx]\n                # reward if selected improves global best\n                if f_j < f_best + 1e-20:\n                    # large reward if truly improved\n                    op_reward[op_j] += max(0.0, (f_best - f_j)) + 1e-6\n                else:\n                    # small reward if selected but not better than global best\n                    op_reward[op_j] += 1e-6\n\n            # exponential moving average of rewards\n            op_reward_ema = 0.85 * op_reward_ema + 0.15 * op_reward\n            # update log-weights toward EMA (soft update)\n            # stabilize by converting ema to pseudo-probabilities\n            pseudo = op_reward_ema + 1e-12\n            pseudo = pseudo / np.sum(pseudo)\n            # move logw towards log(pseudo) with op_lr\n            self.op_logw = (1.0 - self.op_lr) * self.op_logw + self.op_lr * np.log(pseudo + 1e-12)\n\n            # adapt mixing alpha heuristics could be implicit via above operator allocation\n\n            # stagnation detection: if no improvement for long, inflate sigma and nudge mean toward archive best\n            if (evals - last_improve_eval) > self.stagnation_window:\n                # inflation and mild restart\n                sigma *= self.stagnation_sigma_boost\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.65 * m + 0.35 * archive_X[pick]\n                # reset some state to encourage fresh learning\n                G = G * 0.6 + 1e-3\n                D = (G + self.G_eps) ** -0.5\n                success_buffer = []\n                last_improve_eval = evals\n\n            # enforce mean bounds\n            m = np.clip(m, lb, ub)\n\n            # ensure archive does not explode\n            if len(archive_X) > 5000:\n                # keep recent 5000\n                archive_X = archive_X[-5000:]\n                archive_F = archive_F[-5000:]\n\n            # loop continues until budget exhausted\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm EnsembleHybrid scored 0.154 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "61e0b794-2a70-4ab1-af6b-befa2f61a927", "operator": null, "metadata": {"aucs": [0.06115632759561729, 0.14849762939838995, 0.23752769474834723, 0.13721847551395705, 0.1200379627319671, 0.18927330162417044, 0.2020504932786532, 0.1348900434557312, 0.19851233777991362, 0.10942057388296922]}, "task_prompt": ""}
{"id": "f55042c2-8118-49b8-a143-8dbec9099275", "fitness": "-inf", "name": "MemoryLevyTrust", "description": "Memory-Lévy Trust Subspace Search combines a cheap separable-quadratic surrogate (weighted ridge least-squares fit of linear + diagonal Hessian terms) to propose trust-region moves with a low-rank LRU directional memory (dir_mem, QR-orthonormalized) and an evolution-path accumulator for guided polishing. Proposals are generated by subspace-biased population probes that mix memory-derived directions and isotropic noise, with step sizes scaled by a multiplicative sigma (sigma_init_frac≈0.38, sigma_grow≈1.18, sigma_shrink≈0.82) and a vectorized trust radius (trust_init≈0.5, success_expand≈1.4, failure_shrink≈0.7) that are adapted on success/failure. Budget-aware refinements include a cheap 1-D parabola/golden-style quick_parabola along promising directions, micro-local probes after improvements, and occasional Lévy/Cauchy jumps (cauchy_prob≈0.12) plus archive pruning and controlled restarts to escape stagnation. Practical design choices emphasize robustness to limited evaluations: initial space-filling sampling (init_samples_ratio≈0.12), safe_eval that enforces bounds and budget, clipped proposals, small ridge regularization, and compact memory (memory_size≈10) to focus search in useful low-dimensional subspaces.", "code": "import numpy as np\nfrom collections import deque\n\nclass MemoryLevyTrust:\n    \"\"\"\n    Memory-Lévy Trust Subspace Search (MLTSS)\n\n    - Surrogate-guided separable-quadratic trust-region proposals (diagonal Hessian + linear terms).\n    - Low-rank LRU memory of successful unit directions (QR orthonormalized) mixed with isotropic probes.\n    - Multiplicative sigma adaptation, evolution-path accumulator to trigger cheap 1-D parabola/golden polishing.\n    - Budget-aware, clipped evaluations, multi-scale directional probes, occasional Cauchy jumps, archive pruning and restarts.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=10, init_samples_ratio=0.12,\n                 trust_init=0.5, success_expand=1.4, failure_shrink=0.7,\n                 sigma_init_frac=0.38, sigma_grow=1.18, sigma_shrink=0.82,\n                 cauchy_prob=0.12, max_eval_per_iter=60):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.init_samples_ratio = float(init_samples_ratio)\n        self.trust_init = float(trust_init)\n        self.success_expand = float(success_expand)\n        self.failure_shrink = float(failure_shrink)\n        self.sigma_init_frac = float(sigma_init_frac)\n        self.sigma_grow = float(sigma_grow)\n        self.sigma_shrink = float(sigma_shrink)\n        self.cauchy_prob = float(cauchy_prob)\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        domain_mean = np.mean(domain_range)\n        # bookkeeping\n        evals = 0\n        budget = int(self.budget)\n        X_archive = []\n        F_archive = []\n\n        # initial sampling budget\n        init_budget = int(np.clip(self.init_samples_ratio * budget, max(1, 2 * n), min(400, int(0.4 * budget))))\n        init_budget = max(1, init_budget)\n\n        # start with a small space-filling initial set\n        x_best = None\n        f_best = np.inf\n        for _ in range(init_budget):\n            if evals >= budget:\n                break\n            x0 = self.rng.uniform(lb, ub)\n            try:\n                f0 = float(func(x0))\n            except Exception:\n                f0 = np.inf\n            evals += 1\n            X_archive.append(x0.copy()); F_archive.append(f0)\n            if f0 < f_best:\n                f_best = f0; x_best = x0.copy()\n\n        # initialize trust region and sigma\n        trust_radius = np.maximum(self.trust_init * domain_range, 1e-9)\n        sigma = max(1e-12, self.sigma_init_frac * domain_mean)\n        sigma_min = 1e-12 * max(1.0, domain_mean)\n        sigma_max = 5.0 * domain_mean\n\n        # directional memory & evolution path\n        dir_mem = deque(maxlen=self.memory_size)\n        evo_path = np.zeros(n)\n        evo_decay = 0.85\n\n        # helper: safe evaluation returns (f, x_clipped) or (None, None) if budget exhausted\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            xc = np.clip(x, lb, ub)\n            if evals >= budget:\n                return None, None\n            try:\n                f = float(func(xc))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(xc.copy()); F_archive.append(f)\n            if f < f_best:\n                f_best = f; x_best = xc.copy()\n            return f, xc\n\n        # build orthonormal basis from dir_mem (columns are unit vectors)\n        def build_basis():\n            if len(dir_mem) == 0:\n                return None\n            M = np.column_stack(list(dir_mem))  # n x m\n            try:\n                Q, _ = np.linalg.qr(M, mode='reduced')\n            except Exception:\n                # fallback normalize columns\n                cols = []\n                for v in M.T:\n                    vn = np.linalg.norm(v)\n                    if vn > 1e-16:\n                        cols.append(v / vn)\n                if len(cols) == 0:\n                    return None\n                Q = np.column_stack(cols)\n            return Q\n\n        # quick 3-point parabola refinement along d (budget-aware)\n        def quick_parabola(x0, f0, d, init_step=None, max_evals=5):\n            if init_step is None:\n                init_step = sigma\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                return None, None\n            d = d / dn\n            remain = budget - evals\n            if remain <= 0:\n                return None, None\n            s = init_step\n            pts = [(0.0, f0, x0.copy())]\n            # +s\n            if remain >= 1:\n                res = safe_eval(np.clip(x0 + s * d, lb, ub))\n                if res[0] is None:\n                    return None, None\n                pts.append((s, res[0], res[1].copy()))\n                remain -= 1\n            # -s\n            if remain >= 1:\n                res = safe_eval(np.clip(x0 - s * d, lb, ub))\n                if res[0] is None:\n                    return None, None\n                pts.append((-s, res[0], res[1].copy()))\n                remain -= 1\n            pts_sorted = sorted(pts, key=lambda t: t[1])\n            best_alpha, best_f, best_x = pts_sorted[0]\n            # if best is not center and we have budget, expand on that side\n            if best_alpha != 0.0 and remain >= 1 and abs(best_alpha) < 10 * domain_mean:\n                new_alpha = 2.0 * best_alpha\n                res = safe_eval(np.clip(x0 + new_alpha * d, lb, ub))\n                if res[0] is None:\n                    return None, None\n                pts.append((new_alpha, res[0], res[1].copy()))\n                remain -= 1\n                pts_sorted = sorted(pts, key=lambda t: t[1])\n                best_alpha, best_f, best_x = pts_sorted[0]\n            if len(pts) < 3:\n                return (best_f, best_x) if best_f < f0 - 1e-12 else (None, None)\n            alphas = np.array([p[0] for p in pts])\n            fs = np.array([p[1] for p in pts])\n            xs = [p[2] for p in pts]\n            idx_sorted = np.argsort(fs)\n            i0 = idx_sorted[0]\n            others = [i for i in idx_sorted[1:]]\n            if len(others) >= 2:\n                dists = [abs(alphas[i] - alphas[i0]) for i in others]\n                i1 = others[int(np.argmax(dists))]\n                others2 = [o for o in others if o != i1]\n                if others2:\n                    dists2 = [abs(alphas[o] - alphas[i0]) for o in others2]\n                    i2 = others2[int(np.argmax(dists2))]\n                else:\n                    i2 = i1\n            elif len(others) == 1:\n                i1 = others[0]; i2 = others[0]\n            else:\n                i1 = i0; i2 = i0\n            a1, f1, x1 = alphas[i0], fs[i0], xs[i0]\n            a2, f2, x2 = alphas[i1], fs[i1], xs[i1]\n            a3, f3, x3 = alphas[i2], fs[i2], xs[i2]\n            M = np.array([[a1*a1, a1, 1.0],\n                          [a2*a2, a2, 1.0],\n                          [a3*a3, a3, 1.0]])\n            y = np.array([f1, f2, f3])\n            try:\n                coeffs = np.linalg.solve(M, y)\n                A, B, Cc = coeffs\n                if A == 0:\n                    return (f1, x1) if f1 < f0 - 1e-12 else (None, None)\n                alpha_opt = -B / (2.0 * A)\n            except np.linalg.LinAlgError:\n                return (f1, x1) if f1 < f0 - 1e-12 else (None, None)\n            span = max(abs(alphas.max() - alphas.min()), 1e-12)\n            alpha_opt = np.clip(alpha_opt, alphas.min() - 0.5*span, alphas.max() + 0.5*span)\n            if (budget - evals) >= 1:\n                res = safe_eval(np.clip(x0 + alpha_opt * d, lb, ub))\n                if res[0] is None:\n                    return None, None\n                f_opt, x_opt = res\n                if f_opt < f0 - 1e-12:\n                    return f_opt, x_opt\n                # else return best seen if better\n                best_seen = min(pts, key=lambda t: t[1])\n                if best_seen[1] < f0 - 1e-12:\n                    return best_seen[1], best_seen[2].copy()\n            else:\n                best_seen = min(pts, key=lambda t: t[1])\n                if best_seen[1] < f0 - 1e-12:\n                    return best_seen[1], best_seen[2].copy()\n            return None, None\n\n        # main loop\n        no_improve = 0\n        stagnation_limit = max(12, int(12 + 2 * np.log(1 + n)))\n        restarts = 0\n        max_restarts = 5\n\n        while evals < budget:\n            remaining = budget - evals\n            if remaining <= 0:\n                break\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            improved_in_iter = False\n\n            # Try surrogate fit when enough points exist\n            neighbors_needed = max(2 * n + 1, 8 * n)\n            if len(X_archive) >= neighbors_needed and x_best is not None and work_allow > 0:\n                X_arr = np.asarray(X_archive)\n                dists = np.linalg.norm(X_arr - x_best, axis=1)\n                idx_sorted = np.argsort(dists)[:min(len(X_arr), neighbors_needed)]\n                X_nei = X_arr[idx_sorted]\n                F_nei = np.array(F_archive)[idx_sorted]\n                dx = X_nei - x_best\n                M = np.ones((dx.shape[0], 1 + 2*n))\n                M[:, 1:1+n] = dx\n                M[:, 1+n:1+2*n] = 0.5 * (dx ** 2)\n                y = F_nei\n                # weight by proximity\n                w = 1.0 / (dists[idx_sorted] + 1e-12)\n                w = w / (np.max(w) + 1e-12)\n                W = np.sqrt(w)[:, None]\n                A = W * M\n                b = W * y\n                ridge = 1e-6 * np.eye(M.shape[1])\n                try:\n                    params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ b, rcond=None)\n                    params = params.flatten()\n                    a0 = params[0]\n                    b_lin = params[1:1+n]\n                    h_diag = params[1+n:1+2*n]\n                    # regularize curvature\n                    h_reg = h_diag.copy()\n                    h_reg[h_reg < 1e-8] = 1e-8\n                    delta_model = -b_lin / (h_reg + 1e-20)\n                    delta_limited = np.clip(delta_model, -trust_radius, trust_radius)\n                    x_model = np.clip(x_best + delta_limited, lb, ub)\n                    if evals < budget and work_allow > 0:\n                        res = safe_eval(x_model)\n                        work_allow -= 1\n                        if res[0] is not None:\n                            f_model, x_model = res\n                            if f_model < f_best - 1e-12:\n                                # accept surrogate\n                                improved_in_iter = True\n                                # update evolution and memory\n                                disp = x_model - x_best\n                                dn = np.linalg.norm(disp)\n                                if dn > 1e-16:\n                                    dir_mem.appendleft(disp / dn)\n                                    evo_path = evo_decay * evo_path + (1.0 - evo_decay) * (disp)\n                                # adapt trust and sigma\n                                trust_radius = np.minimum(trust_radius * self.success_expand, domain_range * 2.0)\n                                sigma = min(sigma * self.sigma_grow, sigma_max)\n                            else:\n                                # surrogate failed -> shrink trust\n                                trust_radius = np.maximum(trust_radius * self.failure_shrink, 1e-9 * domain_range)\n                except Exception:\n                    pass\n\n            if not improved_in_iter and evals < budget and work_allow > 0:\n                # Subspace-biased population probes (mix memory and isotropic)\n                pop_size = min( max(6, 4 + int(np.sqrt(n))), work_allow )\n                basis = build_basis()\n                samples = []\n                for _ in range(pop_size):\n                    if basis is not None and basis.shape[1] > 0 and self.rng.random() < 0.75:\n                        # memory-driven component\n                        coeffs = self.rng.normal(size=basis.shape[1])\n                        mem_comp = basis @ coeffs\n                        mem_comp = mem_comp / (np.linalg.norm(mem_comp) + 1e-20)\n                        iso = self.rng.normal(size=n)\n                        iso = iso / (np.linalg.norm(iso) + 1e-20)\n                        mix = 0.6\n                        d = mix * mem_comp + (1.0 - mix) * iso\n                        d = d / (np.linalg.norm(d) + 1e-20)\n                    else:\n                        d = self.rng.normal(size=n)\n                        d = d / (np.linalg.norm(d) + 1e-20)\n                    # folded-normal radius times sigma scaled by trust magnitude\n                    r = abs(self.rng.normal()) * (np.linalg.norm(trust_radius) / np.sqrt(float(n)) + 1e-20)\n                    x = (x_best if x_best is not None else self.rng.uniform(lb, ub)) + sigma * r * d\n                    x = np.clip(x, lb, ub)\n                    samples.append((x, d, r))\n                # evaluate samples in random order up to work_allow\n                self.rng.shuffle(samples)\n                best_local = (np.inf, None, None, None)\n                for x, d, r in samples:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    res = safe_eval(x)\n                    work_allow -= 1\n                    if res[0] is None:\n                        break\n                    f_x, x_clipped = res\n                    if f_x < best_local[0]:\n                        best_local = (f_x, x_clipped.copy(), d.copy(), r)\n                if best_local[1] is not None and best_local[0] < f_best - 1e-12:\n                    # improvement found\n                    f_new, x_new, d_new, r_new = best_local\n                    # update evo_path and memory\n                    if x_best is None:\n                        prev = x_new.copy()\n                    else:\n                        prev = x_best.copy()\n                    disp = x_new - prev\n                    dn = np.linalg.norm(disp)\n                    if dn > 1e-16:\n                        dir_mem.appendleft(disp / dn)\n                        evo_path = evo_decay * evo_path + (1.0 - evo_decay) * disp\n                    # adapt sigma and trust\n                    sigma = min(sigma * self.sigma_grow, sigma_max)\n                    trust_radius = np.minimum(trust_radius * self.success_expand, domain_range * 2.0)\n                    improved_in_iter = True\n                else:\n                    # no improvement in probes\n                    sigma = max(sigma * self.sigma_shrink, sigma_min)\n                    trust_radius = np.maximum(trust_radius * self.failure_shrink, 1e-9 * domain_range)\n\n            # occasional polishing along evolution path\n            if np.linalg.norm(evo_path) > 1e-12 and (budget - evals) >= 3 and self.rng.random() < 0.35:\n                d_evo = evo_path.copy()\n                f0 = f_best\n                res = quick_parabola(x_best if x_best is not None else self.rng.uniform(lb, ub),\n                                     f0, d_evo, init_step=sigma, max_evals=min(8, budget - evals))\n                if res[0] is not None:\n                    f_ls, x_ls = res\n                    if f_ls < f_best - 1e-12:\n                        # record\n                        if x_best is None:\n                            prev = x_ls.copy()\n                        else:\n                            prev = x_best.copy()\n                        disp = x_ls - prev\n                        dn = np.linalg.norm(disp)\n                        if dn > 1e-16:\n                            dir_mem.appendleft(disp / dn)\n                        sigma = min(sigma * self.sigma_grow, sigma_max)\n                        trust_radius = np.minimum(trust_radius * self.success_expand, domain_range * 2.0)\n                        improved_in_iter = True\n\n            # micro-polishing when we improved\n            if improved_in_iter and (budget - evals) > 0:\n                local_probes = min(6, max(1, int(np.ceil(np.log(1 + n)))))\n                for _ in range(local_probes):\n                    if evals >= budget:\n                        break\n                    dloc = self.rng.normal(size=n)\n                    dloc = dloc / (np.linalg.norm(dloc) + 1e-20)\n                    a = self.rng.uniform(-0.5 * sigma, 0.5 * sigma)\n                    res = safe_eval(np.clip((x_best if x_best is not None else self.rng.uniform(lb, ub)) + a * dloc, lb, ub))\n                    if res[0] is None:\n                        break\n\n            # occasional Cauchy / Lévy jump to escape basins\n            if (budget - evals) > 0 and self.rng.random() < self.cauchy_prob:\n                scale = 0.4 * domain_range\n                jump = self.rng.standard_cauchy(size=n)\n                jump = np.clip(jump, -10, 10)\n                x_jump = np.clip((x_best if x_best is not None else self.rng.uniform(lb, ub)) + jump * scale, lb, ub)\n                res = safe_eval(x_jump)\n                if res[0] is not None:\n                    f_jump, x_jump = res\n                    if f_jump < f_best - 1e-12:\n                        # success\n                        disp = x_jump - (x_best if x_best is not None else x_jump)\n                        dn = np.linalg.norm(disp)\n                        if dn > 1e-16:\n                            dir_mem.appendleft(disp / dn)\n                        sigma = min(sigma * self.sigma_grow, sigma_max)\n                        trust_radius = np.minimum(trust_radius * 2.0, domain_range * 2.0)\n                        improved_in_iter = True\n                    else:\n                        trust_radius = np.maximum(trust_radius * 0.8, 1e-9 * domain_range)\n\n            # stagnation handling\n            if not improved_in_iter:\n                no_improve += 1\n            else:\n                no_improve = 0\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                no_improve = 0\n                if restarts > max_restarts:\n                    # final conservative shrink of sigma\n                    sigma = max(sigma * 0.5, sigma_min)\n                else:\n                    # diverse restart around best\n                    if x_best is not None and np.isfinite(f_best):\n                        perturb = self.rng.normal(scale=0.4 * domain_mean, size=n)\n                        mnew = np.clip(x_best + perturb, lb, ub)\n                        res = safe_eval(mnew)\n                        if res[0] is None:\n                            break\n                        # reset some memory and enlarge sigma a bit\n                        dir_mem = deque(list(dir_mem)[:len(dir_mem)//2], maxlen=self.memory_size)\n                        sigma = min(sigma * 1.8, sigma_max)\n                        trust_radius = np.minimum(trust_radius * 1.2, domain_range * 2.0)\n                    else:\n                        # global re-init\n                        mnew = self.rng.uniform(lb, ub)\n                        res = safe_eval(mnew)\n                        if res[0] is None:\n                            break\n                        dir_mem = deque(maxlen=self.memory_size)\n                        sigma = min(0.8 * domain_mean, sigma * 2.0)\n                        trust_radius = np.maximum(self.trust_init * domain_range, 1e-9)\n\n            # prune archive if too large\n            max_archive = max(2000, 50 * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                keep_rest = rest[::max(1, len(rest) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # safety clamp sigma\n            sigma = np.clip(sigma, sigma_min, sigma_max)\n\n            # early exit if very good\n            if f_best <= 1e-14:\n                break\n\n        # return best found\n        if x_best is None:\n            # fallback random\n            x_best = self.rng.uniform(lb, ub)\n            try:\n                f_best = float(func(x_best))\n            except Exception:\n                f_best = np.inf\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 78, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_mem = deque(maxlen=self.memory_size)", "error": "In the code, line 78, in __call__, the following error occurred:\nNameError: name 'deque' is not defined\nOn line: dir_mem = deque(maxlen=self.memory_size)", "parent_ids": "61e0b794-2a70-4ab1-af6b-befa2f61a927", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9c2d4946-01a9-4379-abd2-e07f6a5381f9", "fitness": 0.2595885129417287, "name": "HDMS", "description": "The HDMS design combines a lightweight low-rank directional memory with adaptive per-coordinate preconditioning and mixture proposals: it incrementally builds a decayed covariance of successful normalized steps (cov_decay=0.95) and extracts top-k directions via power iterations (power_iters=8, k ≈ ceil(sqrt(dim))) to form a direction bank used with probability p_dir≈0.42, complemented by DE-like archive differences (p_de≈0.20), coordinate-wise AdaGrad-scaled Gaussians (acc_sq init=1e-3) and occasional heavy-tailed Cauchy jumps (p_levy≈0.06). Exploration is regulated by a trust radius r tied to the box diagonal (initial r = 0.12·diag) that adapts from a sliding success window (target_success=0.2, success_window ≈ max(12,5+√n)), while momentum (velocity, decay ≈0.85) and weighted recombination (mu from base_lambda ≈ 4+3·log(dim)) guide exploitation. A soft repulsion kernel (repulse_sigma=0.5, repulse_strength=0.08) discourages revisiting crowded archive regions and cheap 3-point directional line-refinements (line_frac=3% of budget) polish the best directions when budget permits. Practical safeguards include clipping to box bounds, an archive of evaluated points, adaptive batch sizes lam, and conservative bookkeeping to respect the evaluation budget.", "code": "import numpy as np\n\nclass HDMS:\n    \"\"\"\n    Hybrid Directional Memetic Search (HDMS)\n\n    Highlights / novel components:\n    - Incremental covariance of successful normalized steps (cheap rank-1 updates) and power-iteration to extract top-k\n      dominant directions (direction bank). Different from SVD-buffer approaches; aims to be memory/lightweight.\n    - Per-coordinate adaptive learning rates (AdaGrad-like) and momentum for directional persistence.\n    - A stochastic mixture of proposal mechanisms:\n        1) direction-bank guided proposals (projected low-rank moves),\n        2) archive-differential (DE-like),\n        3) coordinate-wise preconditioned Gaussian,\n        4) occasional heavy-tailed (Cauchy) jumps.\n    - Soft repulsion from recently visited archive points to encourage exploration in crowded regions.\n    - Small cheap directional line-refinements (3-point bracket) on best candidates when budget allows.\n    - Adaptive trust radius r controlled by a sliding success rate window.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        self.verbose = verbose\n\n        # population heuristics\n        self.base_lambda = max(6, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.base_lambda // 2)\n\n        # direction-bank dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # power-iteration parameters for learning top-k eigenvectors\n        self.power_iters = 8\n        self.cov_decay = 0.95  # decay for incremental covariance\n\n        # repulsion kernel parameters\n        self.repulse_sigma = 0.5  # scale for Gaussian kernel\n        self.repulse_strength = 0.08  # how strongly to repel candidates from archive\n\n        # mixing probabilities (can adapt)\n        self.p_dir = 0.42\n        self.p_levy = 0.06\n        self.p_de = 0.20\n        # rest is diag gaussian\n\n        # directional line-refine budget fraction\n        self.line_frac = 0.03  # at most 3% of budget for line-searches\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # state\n        lam = self.base_lambda\n        mu = max(1, min(self.mu, lam))\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n\n        eps = 1e-12\n\n        # initialize mean uniformly in the box\n        m = np.random.uniform(lb, ub)\n        # trust radius (relative to box diagonal)\n        r = 0.12 * np.linalg.norm(ub - lb)\n        r_min = 1e-8\n        r_max = 2.0 * np.linalg.norm(ub - lb)\n\n        # momentum and per-coordinate adaptive scaler\n        velocity = np.zeros(n)\n        beta_v = 0.9\n        acc_sq = np.ones(n) * 1e-3  # accumulated squared steps (AdaGrad like)\n\n        # incremental covariance for successful normalized steps (n x n)\n        # We store only the running covariance matrix (symmetric)\n        cov = np.zeros((n, n))\n        cov_count = 0.0\n\n        # direction bank (top-k eigenvectors) obtained by power iterations when needed\n        D_vecs = np.zeros((n, 0))\n        D_vals = np.zeros(0)\n\n        # archive and buffers\n        archive_X = []\n        archive_F = []\n        recent_success = []  # binary window\n        success_window = max(12, int(5 + np.sqrt(n)))\n        target_success = 0.2\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # initial evaluation\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = float(fm); x_best = xm.copy()\n\n        # helper: update direction bank via power method on cov\n        def update_direction_bank():\n            nonlocal D_vecs, D_vals\n            if np.allclose(cov, 0.0):\n                D_vecs = np.zeros((n, 0))\n                D_vals = np.zeros(0)\n                return\n            # compute top-k via simple power iteration with deflation\n            ktake = min(self.k, n)\n            Vs = []\n            Lambdas = []\n            C = cov.copy()\n            for ki in range(ktake):\n                # initialize\n                v = np.random.randn(n)\n                # orthogonalize against previous\n                for pv in Vs:\n                    v -= pv * (pv @ v)\n                v = v / (np.linalg.norm(v) + eps)\n                for _ in range(self.power_iters):\n                    v = C @ v\n                    # re-orth against found\n                    for pv in Vs:\n                        v -= pv * (pv @ v)\n                    normv = np.linalg.norm(v) + eps\n                    v = v / normv\n                # Rayleigh quotient\n                lam_est = float(v @ (C @ v))\n                Vs.append(v.copy())\n                Lambdas.append(lam_est)\n                # deflate approximately\n                C = C - lam_est * np.outer(v, v)\n            if len(Vs) > 0:\n                D_vecs = np.column_stack(Vs)\n                D_vals = np.array(Lambdas)\n            else:\n                D_vecs = np.zeros((n, 0))\n                D_vals = np.zeros(0)\n\n        # helper: repulsion vector for x from recent archive entries (use last m)\n        def repulsion(x, recent=20):\n            if len(archive_X) == 0:\n                return 0.0\n            mrec = min(recent, len(archive_X))\n            tail = np.array(archive_X[-mrec:])\n            diffs = x - tail  # mrec x n\n            d2 = np.sum(diffs * diffs, axis=1)\n            # gaussian kernel weights (closer points -> stronger push)\n            w = np.exp(-d2 / (2.0 * (self.repulse_sigma ** 2) + eps))\n            # compute weighted repulsion vector (sum of normalized diffs weighted)\n            norms = np.sqrt(d2) + eps\n            dirs = (diffs.T / norms).T  # mrec x n\n            rep = np.sum((w[:, None] * dirs), axis=0)\n            # scale\n            return self.repulse_strength * rep\n\n        # helper: cheap 3-point directional refine along direction d from x0 within [-alpha, alpha] scaled by r\n        def line_refine(x0, d, max_evals_allowed):\n            # normalize direction\n            if max_evals_allowed <= 0:\n                return x0, np.inf, 0\n            dn = d.copy()\n            nd = np.linalg.norm(dn) + eps\n            dn = dn / nd\n            # bracket points at -s, 0, s where s = min(r * scale, r_max)\n            s = min(r * 1.2, r_max)\n            xs = [np.clip(x0 - s * dn, lb, ub),\n                  np.clip(x0, lb, ub),\n                  np.clip(x0 + s * dn, lb, ub)]\n            vals = []\n            used = 0\n            for xx in xs:\n                if used >= max_evals_allowed:\n                    vals.append(np.inf)\n                else:\n                    vals.append(func(xx))\n                    used += 1\n            # pick best among them, allow small local golden-ish refinement if budget left (one extra eval)\n            best_idx = int(np.argmin(vals))\n            best_x = xs[best_idx].copy()\n            best_f = float(vals[best_idx])\n            # one local midpoint test between best and neighbor if budget allows\n            if used < max_evals_allowed:\n                # choose neighbor towards center\n                if best_idx == 0:\n                    mid = 0.5 * (xs[0] + xs[1])\n                elif best_idx == 2:\n                    mid = 0.5 * (xs[1] + xs[2])\n                else:\n                    # mid towards smaller of endpoints\n                    mid = 0.5 * (xs[0] + xs[2])\n                mid = np.clip(mid, lb, ub)\n                fm = func(mid)\n                used += 1\n                if fm < best_f:\n                    best_f = float(fm)\n                    best_x = mid.copy()\n            return best_x, best_f, used\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            # adapt lam modestly with remaining budget\n            lam = max(2, min(self.base_lambda, int(np.ceil(self.base_lambda * (0.7 + 0.3 * (remaining / max(1, budget)))))))\n            lam = min(lam, remaining)\n            if lam <= 0:\n                break\n            # ensure even number sometimes\n            if lam % 2 == 1 and lam > 1:\n                lam -= 1\n\n            candidates = np.zeros((lam, n))\n            proposals = np.zeros((lam, n))  # steps y in normalized units\n            modes = []\n\n            # adapt mixture mildly based on recent success rate\n            if len(recent_success) >= success_window:\n                sr = np.mean(recent_success)\n            else:\n                sr = np.mean(recent_success) if recent_success else 0.25\n            if sr < target_success * 0.8:\n                # increase exploration\n                p_dir = max(0.2, self.p_dir * 0.7)\n                p_levy = min(0.25, self.p_levy * 1.6)\n            else:\n                p_dir = min(0.7, self.p_dir * 1.1)\n                p_levy = self.p_levy * 0.7\n\n            # occasionally refresh direction bank\n            if (evals % max(10, lam * 2) == 0) and (cov_count > 0):\n                try:\n                    update_direction_bank()\n                except Exception:\n                    pass\n\n            # generate candidates\n            for i in range(lam):\n                rdraw = np.random.rand()\n                if (D_vecs.shape[1] > 0) and (rdraw < p_dir):\n                    # choose 1..k directions and combine\n                    kpick = np.random.randint(1, min(D_vecs.shape[1], max(1, int(np.ceil(np.sqrt(n))))) + 1)\n                    idx = np.random.choice(D_vecs.shape[1], size=kpick, replace=False)\n                    coeffs = np.random.randn(kpick) * (0.8 / np.sqrt(kpick))\n                    dir_part = (D_vecs[:, idx] @ coeffs)\n                    # small orth complement\n                    ortho = 0.5 * np.random.randn(n) / (np.sqrt(acc_sq) + eps)\n                    y = dir_part + ortho\n                    mode = \"dir\"\n                elif rdraw < p_dir + p_levy:\n                    # heavy-tailed Cauchy jump in random direction scaled by r\n                    dirn = np.random.randn(n); dirn /= (np.linalg.norm(dirn) + eps)\n                    c = np.random.standard_cauchy() * 0.9\n                    y = c * dirn\n                    mode = \"levy\"\n                else:\n                    # coordinate-wise scaled gaussian (preconditioned)\n                    y = np.random.randn(n) / (np.sqrt(acc_sq) + eps)\n                    # small bias from velocity\n                    y += 0.5 * velocity\n                    mode = \"diag\"\n\n                # archive-DE mutation occasionally\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                    a, b, c = np.random.choice(len(archive_X), 3, replace=False)\n                    de = np.array(archive_X[a]) - np.array(archive_X[b]) + 0.5 * (np.array(archive_X[c]) - np.array(m))\n                    # map de back to normalized step space by dividing by r\n                    y = y + 0.7 * (de / (r + eps))\n                    mode += \"+de\"\n\n                # apply repulsion in step-space (approx)\n                # repulsion is computed in x-space, convert to step-space by dividing by r\n                x_candidate = np.clip(m + r * y, lb, ub)\n                rep = repulsion(x_candidate)\n                y = y + (rep / (r + eps))\n\n                candidates[i] = np.clip(m + r * y, lb, ub)\n                proposals[i] = y\n                modes.append(mode)\n\n            # evaluate candidates sequentially\n            fitness = np.full(lam, np.inf)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = candidates[i]\n                f = func(x)\n                evals += 1\n                fitness[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if f < f_best:\n                    f_best = float(f); x_best = x.copy()\n            # clamp archive\n            if len(archive_X) > 5000:\n                archive_X = archive_X[-5000:]; archive_F = archive_F[-5000:]\n\n            # selection\n            valid_idx = np.where(np.isfinite(fitness))[0]\n            if valid_idx.size == 0:\n                break\n            idx_sorted = valid_idx[np.argsort(fitness[valid_idx])]\n            sel_count = min(mu, len(idx_sorted))\n            sel_idx = idx_sorted[:sel_count]\n            x_sel = candidates[sel_idx]\n            y_sel = proposals[sel_idx]\n\n            # compute improvements relative to previous best before batch (estimate)\n            if len(archive_F) > lam:\n                prev_best_est = min(archive_F[:-lam])\n            else:\n                prev_best_est = min(archive_F) if len(archive_F) > 0 else f_best\n            improvements = np.sum(fitness[valid_idx] < prev_best_est - 1e-12)\n            recent_success.append(1 if improvements > 0 else 0)\n            if len(recent_success) > success_window:\n                recent_success.pop(0)\n\n            # recombine weighted mean in parameter space\n            if sel_count > 0:\n                w = weights[:sel_count].copy()\n                w = w / np.sum(w)\n                m_old = m.copy()\n                m = np.sum((w[:, None] * x_sel), axis=0)\n                y_w = np.sum((w[:, None] * y_sel), axis=0)  # normalized step\n\n                # update velocity and acc_sq\n                velocity = 0.85 * velocity + 0.15 * (m - m_old)\n                # accumulate squared normalized step magnitudes\n                sq = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n                acc_sq = acc_sq + sq  # AdaGrad-style (non-decay for stability)\n                # update incremental covariance using normalized step y_w\n                cov = self.cov_decay * cov + np.outer(y_w, y_w)\n                cov_count = self.cov_decay * cov_count + 1.0\n\n                # if we had improvement, also add other selected y's into cov to emphasize good directions\n                if improvements > 0:\n                    for yy in y_sel:\n                        cov = self.cov_decay * cov + 0.3 * np.outer(yy, yy)\n                        cov_count = self.cov_decay * cov_count + 0.3\n\n            # adaptive radius based on success rate\n            if len(recent_success) >= success_window:\n                sr = np.mean(recent_success)\n                if sr > target_success * 1.1:\n                    r = min(r * 1.18, r_max)\n                elif sr < target_success * 0.7:\n                    r = max(r * 0.85, r_min)\n\n            # occasionally perform cheap line-refinement on best candidate in the batch\n            # allocate a small fraction of budget to line-searches overall\n            max_line = int(self.line_frac * self.budget)\n            used_line = 0\n            if (remaining > 3) and (max_line > 0):\n                # choose top candidate direction (best - mean)\n                best_local_idx = idx_sorted[0]\n                cand = candidates[best_local_idx]\n                d = cand - m_old\n                # only attempt if direction has magnitude\n                if np.linalg.norm(d) > 1e-12:\n                    # allow up to few evaluations but not exceed remaining or max_line\n                    allowed = min(remaining - 1, max(1, min(5, max_line)))\n                    xr, fr, used = line_refine(m_old, d, allowed)\n                    used_line += used\n                    evals += 0  # line_refine already consumed function evaluations via func directly\n                    # Note: line_refine calls func directly and internally increments used; we must ensure we count those calls.\n                    # But above we underestimated: in line_refine we called func directly, but didn't update outer evals.\n                    # To be correct, we need to track those evals. But to avoid double-counting, we will track evals via a counter\n                    # inside line_refine. For now the line_refine returns used, so update evals accordingly.\n                    # (We already did evals += 0 mistakenly, so fix:)\n                    evals += used\n                    # Update best if improved\n                    if fr < f_best:\n                        f_best = float(fr); x_best = xr.copy()\n                        # also inject the refined point into archive and cov updates\n                        archive_X.append(xr.copy()); archive_F.append(fr)\n                        # map step to normalized and add to cov\n                        yw = (xr - m_old) / (r + eps)\n                        cov = self.cov_decay * cov + np.outer(yw, yw)\n                        cov_count = self.cov_decay * cov_count + 1.0\n                        # and nudge mean to refined point\n                        m = xr.copy()\n                    # keep archive bounded\n                    if len(archive_X) > 5000:\n                        archive_X = archive_X[-5000:]; archive_F = archive_F[-5000:]\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # safety: if we consumed more evals than budget because line_refine used some directly, clamp and break\n            if evals >= budget:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HDMS scored 0.260 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "3d835d35-9b8a-4e75-927a-467ef4c25c4d", "operator": null, "metadata": {"aucs": [0.14235556360487145, 0.1651248614623524, 0.32497817629854575, 0.4057433446296257, 0.2511561766387902, 0.3386550861175668, 0.25743539655990433, 0.27823874389973813, 0.2513549572228535, 0.1808428229830381]}, "task_prompt": ""}
{"id": "6d10ac5a-243a-471b-818a-63e8479b89c6", "fitness": "-inf", "name": "ECHO_SR", "description": "ECHO-SR is an ensemble searcher that mixes four complementary mutation kernels — low-rank subspace sampling (U, S), heavy-tailed Levy/Cauchy jumps, archive-based DE differences, and diagonal preconditioned Gaussian steps with occasional quasi-Newton nudges — selected by fixed probabilities to balance global exploration and local exploitation. It maintains a weighted mean updated by softmax rank-based recombination and a momentum term (mmt) while using an Adam-like per-coordinate second moment (v and inv_scale, b2=0.99) to diagonally precondition proposals. A success buffer of recent weighted steps drives periodic SVD reconditioning to adapt the low-rank subspace U and singular scales S, and a small quasi-Newton memory with ridge regression produces approximate gradients used sparingly to bias proposals. Global control adapts a trust-region radius r by an EMA of success (succ_ema) and modestly varies population size and selection (mu), with design choices (larger base_lambda, log-scaled subspace k, conservative reconditioning cadence, larger Levy/DE scales) tuned to robustly handle many affine transformations under tight evaluation budgets.", "code": "import numpy as np\n\nclass ECHO_SR:\n    \"\"\"\n    ECHO-SR: Ensemble COupled Heuristics with Success-Rate adaptation.\n\n    Main differences / main parameters (compared to ASTRO_DE):\n    - base population size: base_lambda = max(6, 6 + 2*log(dim)) (larger base than ASTRO_DE)\n    - recombination size mu = ceil(lambda / 3) (different fraction)\n    - subspace dimension k = max(1, ceil(2 * log(1 + dim))) (log-scaled, different from sqrt(dim))\n    - momentum smoothing beta_m = 0.7 (less inertia than ASTRO_DE's 0.9)\n    - second-moment smoothing b2 = 0.99 (slower adaptation than ASTRO_DE's 0.9)\n    - trust-region radius r initialized to 0.10 * box_size (smaller), adapted with different multiplicative factors\n    - mixture weights: p_sub=0.35, p_levy=0.12, p_de=0.25 (different mixture)\n    - Levy scale larger (1.2) and DE factor F_de=0.8\n    - reconditioning and subspace update frequencies changed\n    - uses softmax rank-based recombination weights (instead of log-weights)\n    - includes a lightweight quasi-Newton-like approximate gradient computed from recent archive improvements\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # base population size (different heuristic)\n        self.base_lambda = max(6, int(6 + np.floor(2 * np.log(max(1, self.dim)))))\n        # recombination size fraction\n        self.mu_frac = 1.0 / 3.0\n        # subspace dimension (log scaling; different from sqrt)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(2.0 * np.log(1.0 + self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n        # verbosity\n        self.verbose = verbose\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # derived sizes\n        lam = self.base_lambda\n        mu = max(1, int(np.ceil(lam * self.mu_frac)))\n        eps = 1e-12\n\n        # initialize mean randomly inside box\n        m = np.random.uniform(lb, ub)\n        # trust-region radius (smaller initial)\n        r = 0.10 * np.mean(ub - lb)\n        r_min = 1e-9\n        r_max = 2.5 * np.mean(ub - lb)\n\n        # momentum and smoothing (different beta)\n        mmt = np.zeros(n)\n        beta_m = 0.7\n\n        # per-coordinate second moment (Adam-like but slower)\n        v = np.ones(n) * 1e-3\n        b2 = 0.99\n        inv_scale = 1.0 / (np.sqrt(v) + 1e-12)\n\n        # low-rank subspace U and singular magnitudes S\n        if self.k > 0:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand_mat)\n                U = Q[:, :self.k]\n            except Exception:\n                U = np.zeros((n, self.k))\n            S = np.ones(self.k) * 0.3\n        else:\n            U = np.zeros((n, 0))\n            S = np.zeros(0)\n\n        # buffers and archive\n        success_buffer = []\n        buffer_max = max(30, 12 * self.k, 60)  # larger buffer\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # behavior probabilities (different mixture)\n        p_sub = 0.35\n        p_levy = 0.12\n        p_de = 0.25\n        F_de = 0.8\n        levy_scale = 1.2\n\n        # success tracking (exponential moving average)\n        succ_ema = 0.0\n        succ_alpha = 0.15\n        target_succ = 0.25\n\n        # reconditioning / subspace update frequency\n        recond_every = max(20, 4 * n)\n        recond_counter = 0\n\n        # quasi-Newton memory (small)\n        qn_mem_X = []\n        qn_mem_Y = []\n        qn_mem_max = max(20, 5 * n)\n\n        # initial evaluation\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(float(fm))\n            f_opt = float(fm)\n            x_opt = xm.copy()\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            # adapt population modestly (different ramp)\n            lam = max(2, min(self.base_lambda, int(np.ceil(self.base_lambda * (0.9 - 0.4 * (evals / max(1, budget)))))))\n            lam = min(lam, remaining)\n            if lam <= 0:\n                break\n            if lam % 2 == 1 and lam > 1:\n                lam -= 1\n\n            mu = max(1, int(np.ceil(lam * self.mu_frac)))\n\n            candidates = np.zeros((lam, n))\n            proposals = np.zeros((lam, n))  # normalized steps y\n\n            # optionally compute an approximate gradient from recent archive improvements\n            approx_grad = None\n            if len(qn_mem_X) >= 3:\n                # simple linear regression on pairs (X -> F) to estimate gradient direction\n                try:\n                    Xmat = np.vstack(qn_mem_X)  # m x n\n                    fvec = np.array(qn_mem_Y)   # m\n                    # center\n                    Xm = Xmat - np.mean(Xmat, axis=0, keepdims=True)\n                    fm_center = fvec - np.mean(fvec)\n                    # ridge solve for gradient (n dims)\n                    lam_ridge = 1e-6\n                    A = Xm.T @ Xm + lam_ridge * np.eye(n)\n                    b = Xm.T @ fm_center\n                    grad_est = np.linalg.solve(A, b)\n                    # direction of descent\n                    approx_grad = grad_est\n                    # normalize\n                    gnorm = np.linalg.norm(approx_grad)\n                    if gnorm > eps:\n                        approx_grad = approx_grad / gnorm\n                    else:\n                        approx_grad = None\n                except Exception:\n                    approx_grad = None\n\n            for i in range(lam):\n                rrr = np.random.rand()\n                # choose mechanism\n                if (self.k > 0) and (rrr < p_sub):\n                    # subspace sampling: draw k-dim normal, scale by S, plus small orth complement\n                    z = np.random.randn(self.k)\n                    sub_part = (U @ (S * z)) if self.k > 0 else 0.0\n                    comp = 0.5 * (np.random.randn(n) * inv_scale)\n                    y = sub_part + comp\n                    mode = \"sub\"\n                elif rrr < p_sub + p_levy:\n                    # Levy-like (Cauchy) in random direction, scaled and projected slightly onto subspace\n                    dirn = np.random.randn(n)\n                    dirn = dirn / (np.linalg.norm(dirn) + eps)\n                    rlev = np.random.standard_cauchy() * levy_scale\n                    y = rlev * dirn\n                    # small projection onto subspace to bias towards learned modes\n                    if self.k > 0 and np.random.rand() < 0.5:\n                        proj = U @ (U.T @ y)\n                        y = 0.6 * proj + 0.4 * y\n                    mode = \"levy\"\n                elif rrr < p_sub + p_levy + p_de:\n                    # archive-based DE difference with stronger factor\n                    if len(archive_X) >= 2:\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        de_vec = archive_X[i1] - archive_X[i2]\n                        y = np.random.randn(n) * inv_scale + F_de * (de_vec / (np.linalg.norm(de_vec) + eps))\n                    else:\n                        y = np.random.randn(n) * inv_scale\n                    mode = \"de\"\n                else:\n                    # diagonal preconditioned gaussian plus occasional quasi-newton nudge\n                    y = np.random.randn(n) * inv_scale\n                    if (approx_grad is not None) and (np.random.rand() < 0.25):\n                        # move against approximate gradient (small nudge)\n                        y = y - 0.8 * approx_grad\n                    mode = \"diag\"\n\n                # add momentum (different scale)\n                y = y + 0.5 * mmt\n\n                # scale by trust region radius\n                x = m + r * y\n                x = np.clip(x, lb, ub)\n\n                candidates[i] = x\n                proposals[i] = y\n\n            # evaluate candidates sequentially (strict budget)\n            fitness = np.full(lam, np.inf)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = candidates[i]\n                f = func(x)\n                evals += 1\n                fitness[i] = float(f)\n                archive_X.append(x.copy())\n                archive_F.append(float(f))\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f)\n                    x_opt = x.copy()\n\n            # selection\n            valid_idx = np.where(np.isfinite(fitness))[0]\n            if valid_idx.size == 0:\n                break\n            idx_sorted = valid_idx[np.argsort(fitness[valid_idx])]\n            sel_count = min(mu, len(idx_sorted))\n            sel_idx = idx_sorted[:sel_count]\n            x_sel = candidates[sel_idx]\n            y_sel = proposals[sel_idx]\n\n            # compute improvements relative to best before this batch\n            # estimate previous best (exclude last lam if possible)\n            if len(archive_F) > lam:\n                prev_best_est = min(archive_F[:-lam])\n            else:\n                prev_best_est = min(archive_F)\n            improvements = np.sum(fitness[valid_idx] < prev_best_est - 1e-12)\n            is_success = 1 if improvements > 0 else 0\n            # update EMA of success\n            succ_ema = (1 - succ_alpha) * succ_ema + succ_alpha * is_success\n\n            # recombination: softmax on negative fitness to create weights (different from log-weights)\n            if sel_count > 0:\n                sel_f = fitness[sel_idx]\n                # to avoid overflow, shift\n                inv_score = -sel_f\n                inv_score = inv_score - np.max(inv_score)\n                w_unnorm = np.exp(inv_score / (np.std(inv_score) + 1e-8))\n                if np.any(np.isfinite(w_unnorm)) and np.sum(w_unnorm) > 0:\n                    w = w_unnorm / np.sum(w_unnorm)\n                else:\n                    w = np.ones(sel_count) / sel_count\n\n                m_old = m.copy()\n                m = np.sum(w[:, None] * x_sel, axis=0)\n                # weighted step in normalized space\n                y_w = np.sum(w[:, None] * y_sel, axis=0)\n                # momentum update (different formula: incorporate step direction)\n                mmt = beta_m * mmt + (1.0 - beta_m) * (m - m_old)\n\n                # update second moment with bias-correction-like scaling\n                y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n                v = (b2 * v) + (1.0 - b2) * (y2 + eps)\n                # bias-correction (approximate) using effective number of updates\n                inv_scale = 1.0 / (np.sqrt(v) + 1e-12)\n\n                # push normalized successful step into buffer with probability depending on improvement magnitude\n                if improvements > 0:\n                    # push top-weighted y\n                    success_buffer.append(y_w.copy())\n                else:\n                    # small chance to keep diversity\n                    if np.random.rand() < 0.07:\n                        success_buffer.append(y_w.copy())\n                if len(success_buffer) > buffer_max:\n                    success_buffer.pop(0)\n\n                # maintain quasi-Newton memory for gradient estimate\n                qn_mem_X.append(m.copy())\n                # store corresponding best objective at this mean (approx)\n                qn_mem_Y.append(np.min(archive_F[-max(1, sel_count):]) if len(archive_F) > 0 else f_opt)\n                if len(qn_mem_X) > qn_mem_max:\n                    qn_mem_X.pop(0); qn_mem_Y.pop(0)\n\n                # update low-rank subspace occasionally (different cadence)\n                if (len(success_buffer) >= max(3, self.k)) and (recond_counter % max(1, int(np.ceil(n / 6))) == 0):\n                    try:\n                        Y = np.vstack(success_buffer).T  # n x m\n                        Y = Y - np.mean(Y, axis=1, keepdims=True)\n                        U_new, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                        take = min(self.k, U_new.shape[1])\n                        if take > 0:\n                            U = U_new[:, :take]\n                            # scale singulars differently (more conservative)\n                            S = (svals[:take] / (np.sqrt(max(1, Y.shape[1])) + eps)) * 0.8\n                            if U.shape[1] < self.k:\n                                pad_cols = self.k - U.shape[1]\n                                pad = np.zeros((n, pad_cols))\n                                U = np.hstack([U, pad])\n                                S = np.concatenate([S, np.ones(pad_cols) * 1e-3])\n                    except Exception:\n                        pass\n\n            # trust-region adaptation using EMA of success, different multipliers\n            if succ_ema > target_succ * 1.15:\n                r = min(r * 1.15, r_max)\n            elif succ_ema < target_succ * 0.85:\n                r = max(r * 0.88, r_min)\n            # if extremely low success, add random perturbation and reset momentum partially\n            if succ_ema < 0.02:\n                mmt *= 0.25\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n\n            # periodic light reconditioning: recompute inv_scale as diag invsqrt plus small subspace boost\n            recond_counter += lam\n            if recond_counter >= recond_every:\n                recond_counter = 0\n                try:\n                    diag_invsqrt = np.diag(1.0 / (np.sqrt(v) + eps))\n                    approx = diag_invsqrt.copy()\n                    if self.k > 0:\n                        approx += (U @ np.diag(S + eps) @ U.T) * 0.3\n                    # symmetrize and clamp\n                    approx = 0.5 * (approx + approx.T)\n                    # keep only diagonal for numeric stability in this lightweight variant\n                    approx_invsqrt = np.diag(np.diag(approx))\n                    inv_scale = np.sqrt(np.abs(np.diag(approx_invsqrt)))  # update inv_scale vector\n                    inv_scale = 1.0 / (inv_scale + 1e-12)\n                except Exception:\n                    inv_scale = 1.0 / (np.sqrt(v) + 1e-12)\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 174, in __call__, the following error occurred:\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)\nOn line: sub_part = (U @ (S * z)) if self.k > 0 else 0.0", "error": "In the code, line 174, in __call__, the following error occurred:\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)\nOn line: sub_part = (U @ (S * z)) if self.k > 0 else 0.0", "parent_ids": "3d835d35-9b8a-4e75-927a-467ef4c25c4d", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "a62b2496-bec7-4d44-827f-970f9c4ab52e", "fitness": 0.22086936451907052, "name": "OMNI_GE", "description": "The design is a hybrid online-exploration optimizer that builds an incremental linear surrogate from recent normalized steps (least-squares on a buffer of up to buffer_max recent y -> df pairs) to produce a local gradient estimate, which is combined with per-coordinate AdaGrad-like scaling and momentum for guided moves. It learns a low-dimensional exploratory subspace via incremental Oja updates (k ≈ ceil(sqrt(dim)/1.2), oja_lr_base=0.06, periodic orthonormalization) and mixes candidate generation modes—gradient-guided, subspace-directed, diagonal preconditioned Gaussian, archive mirror-reflection, and occasional heavy-tailed Cauchy jumps (p_grad=0.30, p_sub=0.30, p_diag=0.25, p_reflect=0.10, p_cauchy=0.05)—with recombination of top-μ candidates to update the mean. Step-size sigma (initial 0.12·range, bounded, sigma_min/max) is adapted by a success-rate controller toward a target 0.25 (aggressive upscaling on repeated success, conservative shrinking on failures), and auxiliary mechanisms such as occasional 3-point line probes along −g_hat, archive-driven escape moves, and tight budget/bound enforcement ensure robust exploitation/exploration under evaluation limits. Overall the algorithm emphasizes lightweight online learning of gradient and dominant directions, per-coordinate preconditioning, and a probabilistic mixture of local and global proposal strategies to handle varied continuous landscapes.", "code": "import numpy as np\n\nclass OMNI_GE:\n    \"\"\"\n    OMNI-GE: Online Mixed-Strategy Optimization with Gradient Estimation and Oja Subspace Learning.\n\n    Key ideas (distinct from ASTRO_DE):\n    - Maintain an online linear surrogate (incremental least-squares on recent delta_x -> delta_f)\n      to estimate a local gradient g_hat for guided steps.\n    - Maintain per-coordinate AdaGrad-style scaling and momentum on the gradient estimate.\n    - Learn dominant directions with an incremental Oja-style update to form an exploratory subspace B (n x k).\n    - Candidate generation is a mixture of: gradient-guided steps, subspace-directed proposals (via B),\n      diagonal preconditioned gaussian steps, mirror-reflection moves based on archive, and occasional Cauchy jumps.\n    - A small 3-point adaptive line probe is used occasionally along -g_hat to rapidly exploit promising directions.\n    - Step-size (sigma) is adapted by a success-rate controller, but with different rules (aggressive increase on repeated success,\n      conservative shrinking on failures). Momentum and Oja learning rates adapt with sigma.\n    - Strict respect to budget and bounds (-5..5 typically via func.bounds).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        self.verbose = verbose\n\n        # population base (can vary adaptively)\n        self.base_pop = max(6, int(4 + np.floor(2.5 * np.log(self.dim))))\n        # recombination size\n        self.mu = max(1, self.base_pop // 2)\n\n        # subspace rank\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim) / 1.2)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # parameters for controllers\n        self.target_success = 0.25\n        self.sigma_increase = 1.25\n        self.sigma_decrease = 0.80\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # read bounds robustly\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial mean uniformly in box\n        mean = np.random.uniform(lb, ub)\n\n        # initial trust/step-size sigma (absolute)\n        sigma = 0.12 * np.mean(ub - lb)\n        sigma_min = 1e-8\n        sigma_max = 2.0 * np.mean(ub - lb)\n\n        # gradient surrogate buffers: store recent pairs (delta_x normalized by sigma, delta_f)\n        G_buffer_X = []  # list of y vectors (normalized steps)\n        G_buffer_df = [] # list of differences f(x) - f(mean)\n        buffer_max = max(30, 8 * n)\n\n        # per-coordinate scaling (AdaGrad-like) and gradient momentum\n        grad_sq = np.ones(n) * 1e-6\n        beta_grad = 0.9\n        g_mom = np.zeros(n)\n\n        # Oja subspace B (n x k) initialize with random orthonormal columns\n        rand_mat = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand_mat)\n            B = Q[:, :self.k]\n        except Exception:\n            B = np.eye(n, self.k)\n\n        oja_lr_base = 0.06  # base Oja learning rate (will scale with sigma)\n        oja_orth_every = 25\n\n        # archive for mirror moves and DE-like recombination\n        archive_X = []\n        archive_F = []\n        archive_max = 5000\n\n        # population size adaptive\n        pop = self.base_pop\n\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # initial evaluation\n        xm = np.clip(mean, lb, ub)\n        if evals < budget:\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(float(fm))\n            f_best = float(fm); x_best = xm.copy()\n\n        # counters for adaptation\n        recent_successes = []\n        success_window = 20\n        oja_counter = 0\n        epoch = 0\n\n        # mixture probabilities (different from ASTRO_DE)\n        p_grad = 0.30\n        p_sub = 0.30\n        p_diag = 0.25\n        p_reflect = 0.10\n        p_cauchy = 0.05\n\n        # main loop\n        while evals < budget:\n            epoch += 1\n            remaining = budget - evals\n\n            # adapt population modestly\n            pop = max(2, min(self.base_pop, int(self.base_pop * (1.0 - 0.4 * (evals / max(1, budget))))))\n            pop = min(pop, remaining)\n            if pop <= 0:\n                break\n            # produce pop candidates\n            cand_X = np.zeros((pop, n))\n            cand_Y = np.zeros((pop, n))  # normalized step y = (x - mean)/sigma\n            cand_modes = []\n\n            # precompute gradient estimate from surrogate (solve least squares if buffer sufficient)\n            g_hat = None\n            if len(G_buffer_X) >= max(4, n // 4):\n                D = np.vstack(G_buffer_X)  # m x n where rows are y\n                df = np.array(G_buffer_df).reshape(-1)\n                # regularized least squares: minimize ||D g - df||^2 + lambda ||g||^2\n                lam_reg = 1e-6 + 1e-3 * (1.0 / (1 + len(G_buffer_X) / 50.0))\n                try:\n                    # use normal equations with regularization (n x n)\n                    A = D.T @ D + lam_reg * np.eye(n)\n                    b = D.T @ df\n                    g_hat = np.linalg.solve(A, b)\n                    # apply a mild smoothing (clip huge norms)\n                    gnorm = np.linalg.norm(g_hat)\n                    if np.isnan(gnorm) or gnorm < 1e-16 or gnorm > 1e6:\n                        g_hat = None\n                except Exception:\n                    g_hat = None\n\n            # scale factors\n            inv_scale = 1.0 / (np.sqrt(grad_sq) + 1e-12)\n\n            for i in range(pop):\n                r = np.random.rand()\n                y = np.zeros(n)\n\n                if (g_hat is not None) and (r < p_grad):\n                    # gradient-guided move: negative gradient direction with stochastic scaling\n                    step_dir = -g_hat\n                    step_dir = step_dir * inv_scale  # precondition\n                    # add gaussian jitter orthogonal to direction\n                    jitter = np.random.randn(n) * 0.3\n                    # project out component along step_dir to keep exploration\n                    s = np.dot(step_dir, step_dir)\n                    if s > 0:\n                        jitter = jitter - (np.dot(jitter, step_dir) / (s + 1e-12)) * step_dir\n                    alpha = np.random.uniform(0.5, 1.5)\n                    y = alpha * (step_dir / (np.linalg.norm(step_dir) + 1e-12)) + 0.4 * jitter\n                    mode = \"grad\"\n                elif r < p_grad + p_sub:\n                    # subspace-directed exploration via Oja basis B (prefer strong singular directions)\n                    z = np.random.randn(self.k)\n                    # weight components by magnitudes of projection of recent successful steps (approx by norm of B columns)\n                    colnorms = np.linalg.norm(B, axis=0) + 1e-12\n                    z = z * (colnorms / np.mean(colnorms))\n                    y = B @ z + 0.5 * (np.random.randn(n) * inv_scale)\n                    mode = \"sub\"\n                elif r < p_grad + p_sub + p_diag:\n                    # diagonal preconditioned gaussian\n                    y = np.random.randn(n) * inv_scale\n                    mode = \"diag\"\n                elif r < p_grad + p_sub + p_diag + p_reflect and len(archive_X) > 0:\n                    # mirror reflection: pick random archived good point and reflect around mean\n                    idx = np.random.randint(0, len(archive_X))\n                    x_old = archive_X[idx]\n                    y = (2.0 * (x_old - mean) / (sigma + 1e-12)) + 0.2 * (np.random.randn(n) * inv_scale)\n                    mode = \"reflect\"\n                else:\n                    # occasional heavy-tailed jump (Cauchy)\n                    dirn = np.random.randn(n)\n                    dirn = dirn / (np.linalg.norm(dirn) + 1e-12)\n                    c = np.random.standard_cauchy() * 0.6\n                    y = c * dirn\n                    mode = \"cauchy\"\n\n                # add gradient momentum nudging (if g_hat exists)\n                if g_hat is not None:\n                    g_mom = beta_grad * g_mom + (1.0 - beta_grad) * g_hat\n                    y = y - 0.4 * (g_mom * inv_scale)  # bias towards negative momentum scaled per coord\n\n                # form candidate\n                x = mean + sigma * y\n                x = np.clip(x, lb, ub)\n\n                cand_X[i] = x\n                cand_Y[i] = y\n                cand_modes.append(mode)\n\n            # Evaluate candidates sequentially to respect budget\n            fitness = np.full(pop, np.inf)\n            for i in range(pop):\n                if evals >= budget:\n                    break\n                x = cand_X[i]\n                f = float(func(x))\n                evals += 1\n                fitness[i] = f\n                # archive\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > archive_max:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # select best subset and recombine\n            valid_idx = np.where(np.isfinite(fitness))[0]\n            if valid_idx.size == 0:\n                break\n            order = valid_idx[np.argsort(fitness[valid_idx])]\n            sel = order[:min(self.mu, order.size)]\n            x_sel = cand_X[sel]\n            y_sel = cand_Y[sel]\n\n            # recombination: weighted average in parameter space (weights ~ linear rank)\n            ranks = np.arange(1, len(sel) + 1)\n            weights = (len(sel) + 1 - ranks).astype(float)\n            weights = weights / np.sum(weights)\n            mean_old = mean.copy()\n            mean = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # compute weighted mean step in normalized y space for surrogate/momentum updates\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # determine improvements relative to previous best before this batch (approx)\n            # use archived best excluding last pop entries if possible\n            if len(archive_F) > pop:\n                prev_best_est = min(archive_F[:-pop])\n            else:\n                prev_best_est = min(archive_F)\n            improvements = np.sum(fitness[valid_idx] < prev_best_est - 1e-12)\n\n            recent_successes.append(1 if improvements > 0 else 0)\n            if len(recent_successes) > success_window:\n                recent_successes.pop(0)\n\n            # update gradient surrogate buffers: add weighted y_sel and df (f - f(mean_old))\n            # For each selected solution, store its y and df\n            for j_idx, idx in enumerate(sel):\n                yj = y_sel[j_idx]\n                fj = fitness[idx]\n                dfj = fj - float(np.interp(0, [0,1],[0,1]))  # dummy to ensure variable defined\n                # better: compute df relative to mean_old's evaluation if stored in archive; use f_best if necessary\n                # We will approximate df as f_j - f_best (conservative)\n                dfj = fj - f_best\n                G_buffer_X.append(yj.copy())\n                G_buffer_df.append(float(dfj))\n                if len(G_buffer_X) > buffer_max:\n                    G_buffer_X.pop(0); G_buffer_df.pop(0)\n\n            # update per-coordinate grad_sq using squared normalized steps y_w (like Adam second moment)\n            y2 = (y_w ** 2)\n            grad_sq = 0.92 * grad_sq + 0.08 * (y2 + 1e-12)\n\n            # update Oja basis with any successful normalized steps (prefer those that improved)\n            oja_lr = oja_lr_base * (sigma / (sigma + 1e-8))\n            for idx in sel:\n                y_succ = cand_Y[idx]\n                # center\n                v = y_succ.copy()\n                # normalize v\n                v_norm = np.linalg.norm(v) + 1e-12\n                v = v / v_norm\n                # Oja updates for k components\n                for j in range(self.k):\n                    pj = np.dot(B[:, j], v)\n                    B[:, j] += oja_lr * (pj * v - (pj ** 2) * B[:, j])\n                oja_counter += 1\n\n            # occasionally orthonormalize B\n            if oja_counter >= oja_orth_every:\n                oja_counter = 0\n                try:\n                    Q, _ = np.linalg.qr(B)\n                    B = Q[:, :self.k]\n                except Exception:\n                    # fallback to SVD truncated\n                    try:\n                        U_s, _, _ = np.linalg.svd(B, full_matrices=False)\n                        B = U_s[:, :self.k]\n                    except Exception:\n                        pass\n\n            # adaptive sigma update using recent success rate\n            if len(recent_successes) >= success_window:\n                sr = np.mean(recent_successes)\n                if sr > self.target_success * 1.2:\n                    sigma = min(sigma * (1.0 + 0.5 * (sr - self.target_success)), sigma_max)\n                elif sr < self.target_success * 0.8:\n                    sigma = max(sigma * (1.0 - 0.6 * (self.target_success - sr)), sigma_min)\n            else:\n                # mild decay to encourage exploration early\n                sigma = max(sigma * 0.999, sigma_min)\n\n            # occasional focused line probing along -g_hat if available and budget allows\n            if (g_hat is not None) and (evals < budget) and (np.random.rand() < 0.12):\n                # probe three points: t in {0.0, +tau, -tau} along -g_hat direction (tau ~ sigma*scale)\n                d = -g_hat * inv_scale  # preconditioned direction\n                d = d / (np.linalg.norm(d) + 1e-12)\n                tau = sigma * np.random.uniform(0.6, 1.6)\n                probes = [mean,\n                          np.clip(mean + tau * d, lb, ub),\n                          np.clip(mean - tau * d, lb, ub)]\n                for xp in probes[1:]:\n                    if evals >= budget:\n                        break\n                    fp = float(func(xp))\n                    evals += 1\n                    archive_X.append(xp.copy()); archive_F.append(fp)\n                    if fp < f_best:\n                        f_best = fp; x_best = xp.copy()\n                    # if probed point better than mean, accept a small step towards it\n                    if fp < np.interp(0, [0,1],[0,1]) + 1e-12:\n                        # (dummy, safe as fallback)\n                        pass\n                # lightweight adaptation: if any probe improved best, enlarge sigma a bit\n                if len(archive_F) > 0 and min(archive_F[-2:]) < f_best:\n                    sigma = min(sigma * 1.1, sigma_max)\n\n            # occasionally perturb mean by reflecting toward archive best if stagnation detected\n            if len(recent_successes) >= success_window and np.mean(recent_successes) < 0.02 and len(archive_X) > 0:\n                # pick an archive elite and move mean halfway toward it\n                pick = np.argmin(archive_F)\n                elite = archive_X[pick]\n                mean = 0.5 * mean + 0.5 * elite\n                mean = np.clip(mean, lb, ub)\n                # slightly increase sigma to escape\n                sigma = min(sigma * 1.3, sigma_max)\n\n            # ensure mean in bounds\n            mean = np.clip(mean, lb, ub)\n\n            # verbosity\n            if self.verbose and (epoch % 10 == 0):\n                print(f\"evals={evals}/{budget}, best={f_best:.5e}, sigma={sigma:.4e}, pop={pop}\")\n\n            # stop if perfect zero found (optional)\n            if f_best <= 0.0:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm OMNI_GE scored 0.221 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "3d835d35-9b8a-4e75-927a-467ef4c25c4d", "operator": null, "metadata": {"aucs": [0.05290387431726862, 0.148839672502887, 0.2809533736150289, 0.28396638026774157, 0.23728867739541593, 0.39510828128087594, 0.16935998193414814, 0.32145280783782293, 0.182033559776842, 0.1367870362626742]}, "task_prompt": ""}
{"id": "ee36949a-2714-4814-a9ae-ab2877e46249", "fitness": 0.20832576819084184, "name": "ADSE", "description": "ADSE keeps two complementary low-dimensional subspaces—a main one learned from recent successful normalized steps via SVD and an auxiliary one built from archive differences—using a conservative small k (log2/sqrt heuristic) and a slightly larger initial per-coordinate variance so early search is broad while diagonal preconditioning handles coordinate scaling.  \nSampling is a mixture of modalities (main-subspace, auxiliary-subspace, diagonal Gaussian, heavy-tailed Cauchy/Levy jumps and archive-informed DE perturbations) with elevated weights for subspace and DE use (p_main_sub=0.35, p_aux_sub=0.25, p_de=0.22, p_levy=0.10) and a population size computed from a log-dimension heuristic that shrinks over the budget to trade exploration for efficiency.  \nSelection and recombination use small tournaments and rank-based soft weights to form the new mean, while momentum (beta_m=0.7) and a slow second-moment estimator (b2=0.99) produce per-coordinate inverse scales that shape proposals and the diagonal preconditioner.  \nThe algorithm adaptively adjusts a trust-region radius from recent success rates, retains a bounded archive and buffers, periodically reconditions an approximate anisotropic inverse-sqrt covariance (diag(1/v)+low-rank) for anisotropic steps, and uses stochastic nudges/DE mixes to enhance robustness and escape stagnation.", "code": "import numpy as np\n\nclass ADSE:\n    \"\"\"\n    ADSE: Adaptive Dual-Subspace Evolution\n\n    Main differences from ASTRO_DE (intentional parameter/structure changes):\n    - Different initialization and adaptation rates (momentum beta_m=0.7, second-moment b2=0.99).\n    - Two complementary subspaces are maintained: a main one learned from recent successes and an auxiliary one\n      estimated from diverse archive differences. Subspace dimension heuristic differs from sqrt(n).\n    - Mixture probabilities altered: more heavy-tailed jumps (Cauchy) and stronger archive-DE usage.\n    - Population size and reconditioning frequencies chosen differently.\n    - Tournament selection (size 3) instead of simple top-mu by rank.\n    - Reconditioning builds preconditioner more frequently (recond_every = max(20, 4*n)).\n    - Per-coordinate scaling initialized slightly larger and updated with slower smoothing (b2=0.99).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        self.verbose = bool(verbose)\n\n        # population size base (different heuristic from ASTRO_DE)\n        self.base_lambda = max(6, int(6 + np.floor(2.0 * np.log(max(2, self.dim)))))\n        # selection tournament size\n        self.tourn_k = 3\n\n        # subspace dimension: different heuristic than ceil(sqrt(n))\n        if subspace_k is None:\n            # prefer a smaller subspace for higher dims; use log2-ish scaling\n            self.k = max(1, int(np.clip(np.ceil(np.log2(max(2, self.dim)) + 1.0), 1, self.dim)))\n            # ensure k not larger than half sqrt\n            self.k = min(self.k, max(1, int(np.ceil(np.sqrt(self.dim) / 2.0))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # mixture probabilities (different emphasis)\n        self.p_main_sub = 0.35   # main learned subspace\n        self.p_aux_sub = 0.25    # auxiliary subspace (from archive diffs)\n        self.p_levy = 0.10       # heavier use of Levy/Cauchy\n        self.p_de = 0.22         # stronger DE mixing\n        # remainder is diagonal preconditioned Gaussian\n\n        # parameter coefficients (different from ASTRO_DE)\n        self.beta_m = 0.7   # momentum smoothing slower decay (less inertia than 0.9)\n        self.b2 = 0.99      # stronger smoothing for second-moment (slower to change)\n        self.F_de = 0.7     # slightly larger DE factor\n        self.levy_scale = 1.0\n\n        # limits\n        self.archive_max = 4000\n        self.buffer_max = max(30, 8 * self.k, 40)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (BBOB style -5..5) handle scalars\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # internal state\n        lam = self.base_lambda\n        eps = 1e-12\n\n        # initial mean uniformly in the box\n        m = np.random.uniform(lb, ub)\n\n        # trust-region radius: different initial radius (larger)\n        r = 0.25 * np.mean(ub - lb)\n        r_min = 1e-8\n        r_max = 3.0 * np.mean(ub - lb)\n\n        # momentum and second-moment (different defaults)\n        mmt = np.zeros(n)\n        v = np.ones(n) * 5e-2    # slightly larger initial per-coordinate variance\n        inv_scale = 1.0 / (np.sqrt(v) + 1e-12)\n\n        # Main learned subspace U_main (n x k) and S_main (k)\n        if self.k > 0:\n            R = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(R)\n                U_main = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U_main = np.zeros((n, self.k))\n            S_main = np.ones(self.k) * 0.6\n        else:\n            U_main = np.zeros((n, 0)); S_main = np.zeros(0)\n\n        # Auxiliary subspace learned from archive differences (initialized random)\n        if self.k > 0:\n            R2 = np.random.randn(n, self.k)\n            try:\n                Q2, _ = np.linalg.qr(R2)\n                U_aux = Q2[:, :self.k]\n            except np.linalg.LinAlgError:\n                U_aux = np.zeros((n, self.k))\n            S_aux = np.ones(self.k) * 0.4\n        else:\n            U_aux = np.zeros((n, 0)); S_aux = np.zeros(0)\n\n        # buffers\n        success_buffer = []  # store recent successful normalized steps (y)\n        aux_buffer = []      # diversity buffer from archive diffs\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # reconditioning frequency (different, more frequent)\n        recond_every = max(20, 4 * n)\n        recond_counter = 0\n        approx_invsqrt = np.eye(n)\n\n        # Evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(float(fm))\n            f_opt = float(fm); x_opt = xm.copy()\n        else:\n            fm = np.inf\n\n        # success tracking\n        recent_successes = []\n        success_window = 25\n        target_success_rate = 0.2\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n\n            # adapt population size differently\n            lam = max(2, min(self.base_lambda, int(np.ceil(self.base_lambda * (0.5 + 0.5 * (1.0 - (evals / max(1, budget))))))))\n            lam = min(lam, remaining)\n            if lam <= 0:\n                break\n\n            # ensure even for pairing where helpful\n            if lam % 2 == 1 and lam > 1:\n                lam -= 1\n\n            candidates = np.zeros((lam, n))\n            proposals = np.zeros((lam, n))  # normalized steps y (unitless)\n            modes = [None] * lam\n\n            # sample candidates\n            for i in range(lam):\n                rrand = np.random.rand()\n\n                if (self.k > 0) and (rrand < self.p_main_sub):\n                    # main subspace sample\n                    z = np.random.randn(self.k)\n                    sub_part = (U_main @ (S_main * z))\n                    comp_noise = 0.4 * (np.random.randn(n) * inv_scale)\n                    y = sub_part + comp_noise\n                    modes[i] = \"main_sub\"\n                elif (self.k > 0) and (rrand < self.p_main_sub + self.p_aux_sub):\n                    # auxiliary subspace sample (from archive diffs)\n                    z = np.random.randn(self.k)\n                    sub_part = (U_aux @ (S_aux * z))\n                    comp_noise = 0.5 * (np.random.randn(n) * inv_scale)\n                    y = sub_part + comp_noise\n                    modes[i] = \"aux_sub\"\n                elif rrand < self.p_main_sub + self.p_aux_sub + self.p_levy:\n                    # heavy-tailed Cauchy jump on random direction\n                    dirn = np.random.randn(n)\n                    dirn = dirn / (np.linalg.norm(dirn) + eps)\n                    rlev = np.random.standard_cauchy() * self.levy_scale\n                    y = rlev * dirn\n                    modes[i] = \"levy\"\n                else:\n                    # diagonal preconditioned gaussian\n                    y = np.random.randn(n) * inv_scale\n                    modes[i] = \"diag\"\n\n                # momentum bias\n                y = y + 0.5 * mmt  # different momentum scale\n\n                # differential evolution mix from archive sometimes\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_vec = archive_X[i1] - archive_X[i2]\n                    norm_de = np.linalg.norm(de_vec) + eps\n                    y = y + self.F_de * (de_vec / norm_de)\n                    modes[i] += \"+de\"\n\n                # occasionally apply approx preconditioner to get anisotropic step\n                if np.random.rand() < 0.12:\n                    y = approx_invsqrt @ y\n\n                x = m + r * y\n                x = np.clip(x, lb, ub)\n                candidates[i] = x\n                proposals[i] = y\n\n            # evaluate candidates sequentially\n            fitness = np.full(lam, np.inf)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = candidates[i]\n                f = func(x)\n                evals += 1\n                fitness[i] = float(f)\n                archive_X.append(x.copy()); archive_F.append(float(f))\n                if len(archive_X) > self.archive_max:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f); x_opt = x.copy()\n\n                # build aux_buffer from archive differences occasionally\n                if np.random.rand() < 0.02 and len(archive_X) >= 2:\n                    a, b = np.random.choice(len(archive_X), size=2, replace=False)\n                    diff = archive_X[a] - archive_X[b]\n                    if np.linalg.norm(diff) > eps:\n                        aux_buffer.append(diff / (np.linalg.norm(diff) + eps))\n                    if len(aux_buffer) > self.buffer_max:\n                        aux_buffer.pop(0)\n\n            # selection via tournaments (diversity-focused)\n            selected_idx = []\n            indices = np.arange(len(fitness))\n            for _ in range(min(self.tourn_k, len(indices)) * max(1, int(np.ceil(self.base_lambda / 4)))):\n                # simple tournament of size 3\n                if len(indices) == 0:\n                    break\n                sample = np.random.choice(indices, size=min(3, len(indices)), replace=False)\n                winner = sample[np.argmin(fitness[sample])]\n                selected_idx.append(winner)\n            # ensure uniqueness and limit to mu\n            if len(selected_idx) == 0:\n                # fallback: choose the best few\n                sel_count = min(max(1, self.base_lambda // 2), len(indices))\n                sorted_idx = np.argsort(fitness)\n                selected_idx = sorted_idx[:sel_count].tolist()\n            else:\n                selected_idx = list(dict.fromkeys(selected_idx))  # unique preserve order\n                sel_count = min(len(selected_idx), max(1, self.base_lambda // 2))\n                selected_idx = selected_idx[:sel_count]\n\n            sel_idx = np.array(selected_idx, dtype=int)\n            if sel_idx.size == 0:\n                break\n\n            x_sel = candidates[sel_idx]\n            y_sel = proposals[sel_idx]\n\n            # count improvements relative to fm (reference mean fitness before batch)\n            f_ref = fm\n            improvements = int(np.sum(fitness < f_ref - 1e-12))\n\n            # recombine to new mean using simple normalized weights (rank-based softweights)\n            ranks = np.argsort(np.argsort(fitness[sel_idx]))  # 0 best, ...\n            # create decaying weights\n            w = np.exp(-0.5 * ranks)\n            if np.sum(w) == 0:\n                w = np.ones_like(w)\n            w = w / np.sum(w)\n\n            m_old = m.copy()\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # weighted step in normalized space\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # update momentum and second moment (different coefficients)\n            mmt = self.beta_m * mmt + (1.0 - self.beta_m) * (m - m_old)\n\n            y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n            v = (self.b2 * v) + (1.0 - self.b2) * (y2 + eps)\n            inv_scale = 1.0 / (np.sqrt(v) + eps)\n\n            # store successes to buffer\n            if improvements > 0:\n                success_buffer.append(y_w.copy())\n            else:\n                # occasional diversification push\n                if np.random.rand() < 0.07:\n                    success_buffer.append(y_w.copy())\n            if len(success_buffer) > self.buffer_max:\n                success_buffer.pop(0)\n\n            # update main subspace from success_buffer periodically\n            if (len(success_buffer) >= max(3, self.k)) and (recond_counter % max(1, int(np.ceil(n / 3.0))) == 0):\n                try:\n                    Y = np.vstack(success_buffer).T  # n x m\n                    Y = Y - np.mean(Y, axis=1, keepdims=True)\n                    U_new, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                    take = min(self.k, U_new.shape[1])\n                    if take > 0:\n                        U_main = U_new[:, :take]\n                        S_main = (svals[:take] / (np.sqrt(max(1, Y.shape[1])) + eps)).copy()\n                        if U_main.shape[1] < self.k:\n                            pad_cols = self.k - U_main.shape[1]\n                            pad = np.zeros((n, pad_cols))\n                            U_main = np.hstack([U_main, pad])\n                            S_main = np.concatenate([S_main, np.ones(pad_cols) * 1e-3])\n                except Exception:\n                    pass\n\n            # update auxiliary subspace from aux_buffer (archive diffs)\n            if (len(aux_buffer) >= max(3, self.k)) and (np.random.rand() < 0.3):\n                try:\n                    Y2 = np.vstack(aux_buffer).T\n                    Y2 = Y2 - np.mean(Y2, axis=1, keepdims=True)\n                    U2, s2, _ = np.linalg.svd(Y2, full_matrices=False)\n                    take2 = min(self.k, U2.shape[1])\n                    if take2 > 0:\n                        U_aux = U2[:, :take2]\n                        S_aux = (s2[:take2] / (np.sqrt(max(1, Y2.shape[1])) + eps)).copy()\n                        if U_aux.shape[1] < self.k:\n                            pad_cols = self.k - U_aux.shape[1]\n                            pad = np.zeros((n, pad_cols))\n                            U_aux = np.hstack([U_aux, pad])\n                            S_aux = np.concatenate([S_aux, np.ones(pad_cols) * 1e-3])\n                except Exception:\n                    pass\n\n            # adapt trust-region radius based on recent success rate\n            recent_successes.append(1 if improvements > 0 else 0)\n            if len(recent_successes) > success_window:\n                recent_successes.pop(0)\n            if len(recent_successes) >= success_window:\n                succ_rate = np.mean(recent_successes)\n                if succ_rate > target_success_rate * 1.25:\n                    r = min(r * 1.25, r_max)\n                elif succ_rate < target_success_rate * 0.75:\n                    r = max(r * 0.8, r_min)\n                if succ_rate < 0.03:\n                    # shrink momentum for escape\n                    mmt *= 0.25\n                    # small random nudge using archive\n                    if len(archive_X) > 0:\n                        idx = np.random.randint(len(archive_X))\n                        m = 0.3 * m + 0.7 * archive_X[idx]\n\n            # update mean fitness fm if any candidate improved it\n            min_batch = np.min(fitness) if np.any(np.isfinite(fitness)) else fm\n            if min_batch < fm:\n                fm = float(min_batch)\n\n            # periodic reconditioning to update approx_invsqrt (more frequent than ASTRO_DE)\n            recond_counter += lam\n            if recond_counter >= recond_every:\n                recond_counter = 0\n                try:\n                    # construct approx covariance: diag(1/v) + U_main S_main^2 U_main^T + small ridge\n                    diag_part = np.diag(1.0 / (v + eps))\n                    cov_approx = diag_part.copy()\n                    if self.k > 0:\n                        cov_approx += (U_main @ np.diag((S_main ** 2) + eps) @ U_main.T)\n                    cov_approx = 0.5 * (cov_approx + cov_approx.T)\n                    eigvals, eigvecs = np.linalg.eigh(cov_approx)\n                    eigvals = np.maximum(eigvals, 1e-12)\n                    approx_invsqrt = (eigvecs * (1.0 / np.sqrt(eigvals))) @ eigvecs.T\n                except Exception:\n                    approx_invsqrt = np.eye(n)\n\n            m = np.clip(m, lb, ub)\n\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ADSE scored 0.208 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "3d835d35-9b8a-4e75-927a-467ef4c25c4d", "operator": null, "metadata": {"aucs": [0.14356971134635332, 0.16986588924029467, 0.2636382612399143, 0.17570505654987068, 0.20400170634367065, 0.29170881865169995, 0.2532310715026539, 0.21208485560769197, 0.2295772137966221, 0.1398750976296469]}, "task_prompt": ""}
{"id": "31b94a41-a7d2-4ed7-8c6e-86f80be7e623", "fitness": 0.4973351656424825, "name": "FusionARC", "description": "FusionARC is a hybrid CMA-style / trust-region heuristic that combines a CMA-like weighted recombination (population size lam ~ 4+O(log n), mu, and log-weights) with a trust-region radius r (initialized to 0.12 of the box range) that is adapted by path-length ps and a sliding success-rate window. Sampling is a mixture of mechanisms — low-rank Gaussian moves in a learned subspace (k ≈ ceil(sqrt(n))), coordinate-wise preconditioned Gaussian using a diagonal D with Adam-like second-moment scaling (v and inv_scale), occasional heavy-tailed Cauchy jumps, and DE-style difference perturbations — with mirrored sampling and a momentum term to stabilize search. Successful weighted steps are buffered and periodically SVD’d to update a low-rank basis U and singular scales S (used together with the diagonal D to form a cov_approx), and periodic eigen-reconditioning forms an approximate inverse-sqrt (approx_invsqrt) to rotate/precondition path updates. The algorithm keeps a bounded archive, anneals population size with remaining budget, clips proposals to box bounds, and uses simple heuristics (probabilities p_sub/p_de/p_cauchy, buffer sizes, and reconditioning cadence) to balance exploration, exploitation and computational cost.", "code": "import numpy as np\n\nclass FusionARC:\n    \"\"\"\n    FusionARC: Adaptive Rotational-Trust-region CMA hybrid.\n    One-line: CMA-style recombination + trust-region radius, diag preconditioning + learned low-rank subspace,\n    momentum and Adam-like scaling, mirrored sampling, DE-differences and occasional Cauchy jumps.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, verbose=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.verbose = verbose\n        if seed is not None:\n            np.random.seed(seed)\n        # population heuristic (like CMA)\n        self.base_lambda = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.base_lambda // 2)\n        # subspace dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds handling\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # strategy params\n        lam0 = self.base_lambda\n        lam = lam0\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length / constants\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        cc = (4.0 + mu_eff / n) / (n + 4.0 + 2.0 * mu_eff / n)\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n\n        # initialize state\n        m = np.random.uniform(lb, ub)                 # mean (in box)\n        # trust-region radius (acts like sigma but relative to box)\n        r = 0.12 * np.mean(ub - lb)\n        r_min = 1e-9\n        r_max = 2.0 * np.mean(ub - lb)\n        # diagonal preconditioner (std dev per coordinate)\n        D = np.ones(n)\n        # Adam-like second moment for scaling\n        v = np.ones(n) * 1e-3\n        b2 = 0.9\n        inv_scale = 1.0 / (np.sqrt(v) + 1e-12)\n        # momentum\n        mmt = np.zeros(n)\n        beta_m = 0.9\n\n        # low-rank subspace initialization\n        if self.k > 0:\n            rand_mat = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand_mat)\n                U = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n            S = np.ones(self.k) * 0.5\n        else:\n            U = np.zeros((n, 0))\n            S = np.zeros(0)\n\n        # paths for CMA-like update (for possible covariance or success heuristics)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n\n        # buffers & archives\n        success_buffer = []\n        buffer_max = max(20, 10 * self.k, 50)\n        archive_X = []\n        archive_F = []\n\n        # reconditioning state\n        eig_every = max(40, 6 * n)\n        recond_counter = 0\n        approx_invsqrt = np.eye(n)\n        use_approx = False\n\n        # behavior probabilities\n        p_cauchy = 0.08\n        p_de = 0.18\n        F_de = 0.6\n        p_sub = 0.45\n        mirrored = True\n\n        # population & stagnation tracking\n        recent_successes = []\n        success_window = max(8, int(2 * n))\n        target_success_rate = 0.2\n\n        # book-keeping best\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_opt = float(fm); x_opt = xm.copy()\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            # adapt population modestly with remaining budget (more early, fewer later)\n            lam = max(2, min(lam0, int(np.ceil(lam0 * (1.0 - 0.3 * (evals / max(1, budget)))))))\n            lam = min(lam, remaining)\n            if lam <= 0:\n                break\n            if mirrored and (lam % 2 == 1) and lam > 1:\n                lam -= 1\n\n            # prepare containers\n            candidates = np.zeros((lam, n))\n            proposals = np.zeros((lam, n))  # step y such that x = m + r * y\n\n            # generate candidates\n            k_idx = 0\n            while k_idx < lam:\n                # choose mechanism\n                u = np.random.rand()\n                # diagonal gaussian in preconditioned space\n                if (self.k > 0) and (u < p_sub):\n                    # low-rank + small complement\n                    z_low = np.random.randn(self.k)\n                    low = (U @ (S * z_low)) if self.k > 0 else 0.0\n                    diag_noise = np.random.randn(n) * D\n                    y = 0.9 * low + 0.6 * (diag_noise * inv_scale)\n                elif u < p_sub + p_cauchy:\n                    # Cauchy jump\n                    dirn = np.random.randn(n)\n                    dirn = dirn / (np.linalg.norm(dirn) + 1e-20)\n                    rlev = np.random.standard_cauchy()\n                    y = (rlev * dirn) * np.mean(D)\n                else:\n                    # coordinate-scaled gaussian (Adam-preconditioned)\n                    y = np.random.randn(n) * (D * inv_scale)\n\n                # incorporate momentum bias\n                y = y + 0.6 * mmt\n\n                # occasional DE-style mutation from archive\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_vec = archive_X[i1] - archive_X[i2]\n                    # scale DE by typical coordinate scale\n                    if np.linalg.norm(de_vec) > 0:\n                        y = y + F_de * (de_vec / (np.linalg.norm(de_vec) + 1e-12)) * np.mean(D)\n\n                # candidate creation\n                x = m + r * y\n                x = np.clip(x, lb, ub)\n                candidates[k_idx] = x\n                proposals[k_idx] = y\n                k_idx += 1\n\n                # mirrored partner if room\n                if mirrored and k_idx < lam:\n                    y2 = -y\n                    x2 = m + r * y2\n                    # we may also add small random perturbation to mirrored partner\n                    if np.random.rand() < 0.05:\n                        x2 = x2 + 0.02 * r * np.random.randn(n)\n                    x2 = np.clip(x2, lb, ub)\n                    candidates[k_idx] = x2\n                    proposals[k_idx] = y2\n                    k_idx += 1\n\n            # evaluate candidates sequentially (respect budget)\n            fitness = np.full(lam, np.inf)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = candidates[i]\n                f = func(x)\n                evals += 1\n                fitness[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f); x_opt = x.copy()\n\n            # selection among valid evaluations\n            valid_idx = np.where(np.isfinite(fitness))[0]\n            if valid_idx.size == 0:\n                break\n            order = valid_idx[np.argsort(fitness[valid_idx])]\n            sel_count = min(self.mu, len(order))\n            sel_idx = order[:sel_count]\n            x_sel = candidates[sel_idx]\n            y_sel = proposals[sel_idx]  # normalized steps\n\n            # estimate improvement count relative to prior best before this batch\n            if len(archive_F) > lam:\n                prev_best_est = min(archive_F[:-lam])\n            else:\n                prev_best_est = min(archive_F)\n            improvements = np.sum(fitness[valid_idx] < (prev_best_est - 1e-12))\n            recent_successes.append(1 if improvements > 0 else 0)\n            if len(recent_successes) > success_window:\n                recent_successes.pop(0)\n\n            # recombine new mean\n            if sel_count > 0:\n                w = weights[:sel_count].copy()\n                w = w / np.sum(w)\n                m_old = m.copy()\n                m = np.sum(w[:, None] * x_sel, axis=0)\n                y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n                # update momentum and Adam-like second moment\n                mmt = beta_m * mmt + (1.0 - beta_m) * (m - m_old)\n                y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n                v = (b2 * v) + (1.0 - b2) * (y2 + 1e-12)\n                inv_scale = 1.0 / (np.sqrt(v) + 1e-12)\n\n                # update diagonal D (EMA of second moments)\n                c_d = 0.25\n                D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n                D = np.sqrt(np.maximum(D2, 1e-20))\n\n                # CMA-like path update (approx using approx_invsqrt when available)\n                if use_approx:\n                    ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (approx_invsqrt @ y_w)\n                else:\n                    # approximate inv with diag inverse\n                    ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w / (D + 1e-12))\n                norm_ps = np.linalg.norm(ps)\n                # adapt trust-region radius r: via success rate and path-length\n                # prefer to use recent_successes\n                if len(recent_successes) >= success_window:\n                    succ_rate = np.mean(recent_successes)\n                    if succ_rate > target_success_rate * 1.2:\n                        r = min(r * 1.18, r_max)\n                    elif succ_rate < target_success_rate * 0.8:\n                        r = max(r * 0.85, r_min)\n                # small path-length-based tweak\n                r *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n                r = np.clip(r, r_min, r_max)\n\n                # push successful weighted step into buffer for subspace learning (only on improvement)\n                if improvements > 0 or (np.random.rand() < 0.04):\n                    success_buffer.append(y_w.copy())\n                if len(success_buffer) > buffer_max:\n                    success_buffer.pop(0)\n\n                # update low-rank U,S from buffer occasionally\n                recond_counter += lam\n                if (len(success_buffer) >= max(3, self.k)) and (recond_counter % max(1, int(np.ceil(n / 4))) == 0):\n                    try:\n                        Y = np.vstack(success_buffer).T  # n x m\n                        Y = Y - np.mean(Y, axis=1, keepdims=True)\n                        U_new, svals, _ = np.linalg.svd(Y, full_matrices=False)\n                        take = min(self.k, U_new.shape[1])\n                        if take > 0:\n                            U = U_new[:, :take]\n                            S = (svals[:take] / (np.sqrt(max(1, Y.shape[1])) + 1e-12)).copy()\n                            if U.shape[1] < self.k:\n                                pad = np.zeros((n, self.k - U.shape[1])); U = np.hstack([U, pad])\n                                S = np.concatenate([S, np.ones(self.k - len(S)) * 1e-3])\n                    except Exception:\n                        pass\n\n                # periodic approximate reconditioning to compute approx_invsqrt\n                if recond_counter >= eig_every:\n                    recond_counter = 0\n                    try:\n                        cov_approx = np.diag(D ** 2)\n                        if self.k > 0:\n                            cov_approx = cov_approx + (U @ np.diag((S ** 2) + 1e-12) @ U.T)\n                        cov_approx = 0.5 * (cov_approx + cov_approx.T)\n                        Dvals, B = np.linalg.eigh(cov_approx)\n                        Dvals = np.maximum(Dvals, 1e-12)\n                        approx_invsqrt = (B * (1.0 / np.sqrt(Dvals))) @ B.T\n                        use_approx = True\n                    except Exception:\n                        use_approx = False\n                        approx_invsqrt = np.eye(n)\n\n            # if budget exhausted break\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm FusionARC scored 0.497 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "3d835d35-9b8a-4e75-927a-467ef4c25c4d", "operator": null, "metadata": {"aucs": [0.1524347239482241, 0.15166258900587604, 0.5034221237238894, 0.9782625917545484, 0.9707316999717294, 0.9687161039128082, 0.239177342900471, 0.5507224328057198, 0.3109625167224441, 0.1472595316791152]}, "task_prompt": ""}
{"id": "59c79628-ffb8-47bb-bf2c-43bbe95892b5", "fitness": "-inf", "name": "BlockCovLevy", "description": "The algorithm hybridizes several complementary heuristics: blockwise separable quadratic surrogates (block_fraction≈0.35) are fit around the current best to propose constrained Newton-like steps governed by per‑dimension trust radii (trust_init_frac≈0.35, success_expand=1.3, failure_shrink=0.6), while a CMA-style covariance learner (sigma init = 0.2·mean(range), lam0≈10+n/2) builds multivariate proposals from an elite archive and updates B/D via occasional eigen decompositions. Sampling is budget-aware and mixed: mirrored multivariate draws (use_mirroring=True, max_eval_per_iter=80) are combined with archive-based DE-like difference perturbations (de_prob≈0.28) and occasional Mantegna–Lévy jumps (levy_prob≈0.12) to maintain exploration and escape. Step‑size is adapted by a hybrid cumulation (ps/pc) + success‑rate rule (cs/damps/chi_n) and trust radii shrink/expand based on surrogate or sampling successes, while selection uses log‑weighted recombination of best candidates. Practical engineering includes LHS initialization (init_samples_ratio≈0.18, capped), archive pruning, multi‑scale mirrored local probes when stuck, strict budget accounting, and small final polishing near budget end.", "code": "import numpy as np\n\nclass BlockCovLevy:\n    \"\"\"\n    BlockCovLevy: Blockwise Covariance + Levy + DE hybrid\n\n    One-line: Fit small blockwise separable quadratic surrogates for local Newton-like proposals,\n    but rely primarily on a CMA-style covariance-adapted multivariate sampler with mirrored sampling,\n    archive-based self-adaptive DE perturbations, and occasional Mantegna-Levy teleports; step-size\n    is updated by a hybrid cumulation + success-rate rule while per-dimension trust radii bound\n    surrogate steps and multi-scale mirrored probes.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_samples_ratio=0.18, min_init=None, max_init=None,\n                 block_fraction=0.35, max_eval_per_iter=80,\n                 trust_init_frac=0.35, trust_min_frac=1e-8, trust_max_frac=2.5,\n                 success_expand=1.3, failure_shrink=0.6,\n                 levy_prob=0.12, de_prob=0.28, use_mirroring=True,\n                 eig_every_factor=8):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n\n        self.init_samples_ratio = float(init_samples_ratio)\n        self.min_init = int(min_init) if min_init is not None else max(12, 3 * self.dim)\n        self.max_init = int(max_init) if max_init is not None else min(500, int(0.25 * self.budget))\n\n        self.block_fraction = float(block_fraction)\n        self.max_eval_per_iter = int(max_eval_per_iter)\n\n        self.trust_init_frac = float(trust_init_frac)\n        self.trust_min_frac = float(trust_min_frac)\n        self.trust_max_frac = float(trust_max_frac)\n        self.success_expand = float(success_expand)\n        self.failure_shrink = float(failure_shrink)\n\n        self.levy_prob = float(levy_prob)\n        self.de_prob = float(de_prob)\n        self.use_mirroring = bool(use_mirroring)\n\n        self.eig_every_factor = int(eig_every_factor)\n\n        if seed is not None:\n            np.random.seed(seed)\n\n    # helper: Levy via Mantegna (alpha=1.5 used typically)\n    def _levy_mantegna(self, n, alpha=1.5, scale=1.0):\n        # robust implementation similar to No.1\n        sigma_u = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                   (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n        u = np.random.normal(0, sigma_u, size=n)\n        v = np.random.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1.0 / alpha) + 1e-20)\n        return scale * step\n\n    # small LHS-like sampler\n    def _lhs(self, m, n):\n        cut = np.linspace(0.0, 1.0, m + 1)\n        u = np.random.uniform(size=(m, n))\n        a = cut[:m].reshape(m, 1); b = cut[1:m + 1].reshape(m, 1)\n        pts = a + u * (b - a)\n        for j in range(n):\n            np.random.shuffle(pts[:, j])\n        return pts\n\n    def __call__(self, func):\n        n = int(self.dim)\n        budget = int(self.budget)\n\n        # bounds (Many Affine BBOB uses func.bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        lb = lb.reshape(n,); ub = ub.reshape(n,)\n        rng_range = ub - lb\n        domain_mean = np.mean(rng_range) if np.all(np.isfinite(rng_range)) else 10.0\n\n        # archive X,F\n        X = []\n        F = []\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # initialization sampling (LHS-ish)\n        init_budget = int(np.clip(self.init_samples_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        uv = self._lhs(init_budget, n)\n        for i in range(init_budget):\n            if evals >= budget:\n                break\n            x = lb + uv[i] * rng_range\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n\n        # trust radii per-dim\n        trust_radius = np.maximum(self.trust_init_frac * rng_range, 1e-12)\n        trust_min = np.maximum(self.trust_min_frac * rng_range, 1e-12)\n        trust_max = np.maximum(self.trust_max_frac * rng_range, 1e-12)\n\n        # CMA-like state\n        sigma = 0.2 * np.mean(rng_range)  # initial global sigma\n        C = np.diag((trust_radius) ** 2 + 1e-12)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        # weights for recombination\n        lam0 = max(6, min(20 + n, int(10 + n // 2)))\n        lam = lam0\n        mu = max(1, lam // 2)\n        weights = np.log(np.arange(1, mu + 1) + 0.5)[::-1]\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n        # adaptation constants (leaning to No.1 style)\n        cc = 2.0 / (n + 2.0)\n        cs = 0.4 * (mu_eff / (n + mu_eff + 1.0))\n        c1 = 1.0 / max(1.0, (n + 3.0) ** 2)\n        cmu = 0.6 * (1.0 - c1) * min(1.0, mu_eff / (2.0 * n))\n        damps = 1.0 + 0.3 * cs + 0.2 * np.sqrt(max(0.0, mu_eff / n))\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        eig_every = max(1, int(self.eig_every_factor * n))\n        eigen_counter = 0\n\n        # safe_eval wrapper\n        def clip_to_bounds(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip_to_bounds(np.asarray(x, dtype=float))\n            if evals >= budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if f < f_best:\n                f_best = float(f); x_best = x.copy()\n            return float(f), x\n\n        # helper: blockwise separable quadratic fit (like No.5) around x_center\n        def block_quadratic_newton(x_center, block_idx):\n            # require some neighbors\n            if len(X) < 3:\n                return None\n            X_arr = np.asarray(X)\n            F_arr = np.asarray(F)\n            # get neighbors\n            dists = np.linalg.norm(X_arr - x_center, axis=1)\n            k_nei = min(len(X), max(3 * block_idx.size + 1, 6 + 4 * block_idx.size))\n            idxs = np.argsort(dists)[:k_nei]\n            X_nei = X_arr[idxs]; F_nei = F_arr[idxs]\n            dx = X_nei - x_center\n            m_pts = dx.shape[0]\n            p = 1 + block_idx.size + block_idx.size\n            M = np.ones((m_pts, p))\n            M[:, 1:1 + block_idx.size] = dx[:, block_idx]\n            M[:, 1 + block_idx.size:1 + 2 * block_idx.size] = 0.5 * (dx[:, block_idx] ** 2)\n            y = F_nei\n            d_full = np.linalg.norm(X_nei - x_center, axis=1)\n            w = 1.0 / (d_full + 1e-12)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M; b = W * y\n            ridge = 1e-6\n            try:\n                lhs = A.T @ A + ridge * np.eye(p)\n                rhs = A.T @ b\n                params = np.linalg.solve(lhs, rhs).flatten()\n            except Exception:\n                return None\n            b_lin = params[1:1 + block_idx.size]\n            h_diag = params[1 + block_idx.size:1 + 2 * block_idx.size]\n            h_reg = np.array(h_diag, copy=True)\n            h_reg[h_reg < 1e-8] = 1e-8\n            delta = - b_lin / (h_reg + 1e-20)\n            limited = np.clip(delta, -trust_radius[block_idx], trust_radius[block_idx])\n            x_model = x_center.copy()\n            x_model[block_idx] = x_model[block_idx] + limited\n            return x_model\n\n        # main loop\n        # initial mean for covariance sampling: use best so far or center\n        m = x_best.copy() if x_best is not None else 0.5 * (lb + ub)\n\n        while evals < budget:\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n            improved_round = False\n\n            # attempt blockwise surrogate proposals first (one or two blocks)\n            block_size = max(1, int(np.ceil(self.block_fraction * n)))\n            # choose a block (contiguous or random)\n            if block_size >= n:\n                blocks = [np.arange(n)]\n            else:\n                start = np.random.randint(0, n)\n                if start + block_size <= n:\n                    block = np.arange(start, start + block_size)\n                else:\n                    block = np.concatenate((np.arange(start, n), np.arange(0, (start + block_size) % n)))\n                # sometimes scatter block\n                if np.random.rand() < 0.25:\n                    block = np.random.choice(n, size=block_size, replace=False)\n                blocks = [np.array(block, dtype=int)]\n\n            for block_idx in blocks:\n                if work_allow <= 0 or evals >= budget:\n                    break\n                if x_best is None:\n                    x_center = m.copy()\n                else:\n                    x_center = x_best.copy()\n                x_model = block_quadratic_newton(x_center, block_idx)\n                if x_model is None:\n                    continue\n                if work_allow <= 0:\n                    break\n                out = safe_eval(x_model)\n                work_allow -= 1\n                if out is None:\n                    break\n                f_model, x_model = out\n                if f_model < f_best - 1e-12:\n                    improved_round = True\n                    # reward\n                    trust_radius = np.minimum(trust_radius * self.success_expand, trust_max)\n                    sigma = min(sigma * 1.2, 5.0 * np.mean(rng_range))\n                    m = x_model.copy()\n                else:\n                    # shrink trust for these dims\n                    trust_radius[block_idx] = np.maximum(trust_radius[block_idx] * self.failure_shrink, trust_min)\n\n            # If surrogate did not yield, do covariance-guided mirrored sampling + DE + Levy\n            if not improved_round and evals < budget and work_allow > 0:\n                # build empirical covariance from elites if available\n                if len(X) >= 2:\n                    m_cov = min(max(4 * n, 20), len(X))\n                    idx_e = np.argsort(F)[:m_cov]\n                    elites = np.asarray(X)[idx_e]\n                    mu = np.mean(elites, axis=0)\n                    C_emp = np.cov((elites - mu).T) if elites.shape[0] > 1 else np.diag((trust_radius ** 2))\n                    # blend with diagonal trust to keep numeric stability\n                    C = 0.6 * C_emp + 0.4 * np.diag(trust_radius ** 2) + 1e-12 * np.eye(n)\n                else:\n                    mu = m.copy()\n                    C = np.diag(trust_radius ** 2 + 1e-12)\n\n                # eigen-decompose occasionally to update B,D\n                eigen_counter += 1\n                if eigen_counter >= eig_every:\n                    eigen_counter = 0\n                    try:\n                        D2, B = np.linalg.eigh(C)\n                        D2 = np.maximum(D2, 1e-18)\n                        D = np.sqrt(D2)\n                        invsqrtC = (B * (1.0 / D)) @ B.T\n                    except Exception:\n                        B = np.eye(n); D = np.ones(n); invsqrtC = np.eye(n)\n\n                # sample lambda candidates (cap by work_allow)\n                lam = min(lam0, max(4, work_allow))\n                current_lambda = lam\n                # mirrored sampling: generate half z and negatives if even\n                if self.use_mirroring and (current_lambda % 2 == 0):\n                    half = current_lambda // 2\n                    arz_half = np.random.randn(half, n)\n                    arz = np.vstack([arz_half, -arz_half])\n                else:\n                    arz = np.random.randn(current_lambda, n)\n                # map through B and D: candidate steps = B*(D*z)\n                ary = arz @ (B * D).T if (B.shape == (n, n)) else arz\n                arx = np.empty_like(ary)\n                for k in range(current_lambda):\n                    base = mu + sigma * ary[k]\n                    xk = base.copy()\n                    # self-adaptive DE perturbation using archive differences\n                    if (np.random.rand() < self.de_prob) and (len(X) >= 3):\n                        # pick three distinct indices\n                        i1, i2 = np.random.choice(len(X), size=2, replace=False)\n                        Fk = np.exp(np.random.normal(np.log(0.8), 0.5))\n                        de_vec = Fk * (X[i1] - X[i2])\n                        # crossover: mix a subset\n                        cr = np.random.beta(2.0, 2.0)\n                        mask = np.random.rand(n) < cr\n                        if not np.any(mask):\n                            mask[np.random.randint(n)] = True\n                        xk[mask] = xk[mask] + de_vec[mask]\n                    # occasional Levy jump\n                    if np.random.rand() < self.levy_prob:\n                        levy = self._levy_mantegna(n, alpha=1.5, scale=0.6 * sigma)\n                        xk = xk + levy\n                    # clip\n                    arx[k] = clip_to_bounds(xk)\n\n                # evaluate candidates (strict budget)\n                arfit = np.full(current_lambda, np.inf)\n                for k in range(current_lambda):\n                    if evals >= budget or work_allow <= 0:\n                        break\n                    xk = arx[k]\n                    out = safe_eval(xk)\n                    work_allow -= 1\n                    if out is None:\n                        break\n                    fk, xk = out\n                    arfit[k] = fk\n\n                # selection: get best mu and recombine as new mean m\n                finite_idx = np.isfinite(arfit)\n                if np.any(finite_idx):\n                    idx_sort = np.argsort(arfit)\n                    sel = idx_sort[:max(1, min(mu, np.sum(finite_idx)))]\n                    x_sel = arx[sel]\n                    # recombination weights (descending log-weights but normalized)\n                    w = np.log(np.arange(1, len(sel) + 1) + 0.5)[::-1]\n                    w = w / (np.sum(w) + 1e-20)\n                    m_old = m.copy()\n                    m = np.sum(w[:, None] * x_sel, axis=0)\n                    # normalized steps\n                    y_sel = (x_sel - m_old) / (sigma + 1e-20)\n                    y_w = np.sum(w[:, None] * y_sel, axis=0)\n                    # update ps/pc\n                    ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrtC @ y_w)\n                    norm_ps = np.linalg.norm(ps)\n                    denom = np.sqrt(1.0 - (1.0 - cs) ** (2.0 * max(1.0, (evals + 1) / max(1, current_lambda))))\n                    hsig = 1.0 if (norm_ps / (denom + 1e-20) / chi_n) < (1.5 + 1.0 / (n + 1.0)) else 0.0\n                    pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n                    # update covariance rank-one + rank-mu\n                    rank_one = np.outer(pc, pc)\n                    rank_mu = np.zeros((n, n))\n                    for i, yi in enumerate(y_sel):\n                        yi = yi[:, None]\n                        rank_mu += w[i] * (yi @ yi.T)\n                    C = (1.0 - c1 - cmu) * C + c1 * (rank_one + (1.0 - hsig) * cc * (2.0 - cc) * C) + cmu * rank_mu\n                    # success-rate based sigma multiplier: compare selected fitness to previous best\n                    # estimate f_m_est as median of selected\n                    f_sel = arfit[sel]\n                    f_m_est = np.median(f_sel[np.isfinite(f_sel)]) if np.any(np.isfinite(f_sel)) else f_best\n                    success_rate = np.mean(f_sel < (f_best + 1e-12)) if np.any(np.isfinite(f_sel)) else 0.0\n                    # hybrid sigma update: cumulation rule + multiplier\n                    sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n                    if success_rate > 0.2:\n                        sigma *= 1.12\n                    elif success_rate < 0.05:\n                        sigma *= 0.85\n                    sigma = max(sigma, 1e-12)\n                    # if any of selected improved overall best, mark improved_round\n                    if np.any(arfit[sel] < f_best - 1e-12):\n                        improved_round = True\n                        # slightly expand trust radii\n                        trust_radius = np.minimum(trust_radius * self.success_expand, trust_max)\n                        # set new mean to best selected if better\n                        best_idx = sel[np.argmin(arfit[sel])]\n                        m = arx[best_idx].copy()\n                    else:\n                        # shrink trust if no improvement\n                        trust_radius = np.maximum(trust_radius * self.failure_shrink, trust_min)\n\n            # If nothing improved in this outer iteration, perform mirrored multi-scale directional probes (local search)\n            if not improved_round and evals < budget and work_allow > 0:\n                num_directions = int(np.clip(6 + n // 3, 6, 20))\n                base_step = np.exp(np.mean(np.log(np.maximum(trust_radius, 1e-12))))\n                directions = []\n                for _ in range(num_directions):\n                    v = np.random.randn(n); v /= (np.linalg.norm(v) + 1e-20); directions.append(v)\n                if np.random.rand() < 0.4:\n                    for i in range(min(n, 4)):\n                        e = np.zeros(n); e[i] = 1.0; directions.append(e)\n                scales = [0.125, 0.25, 0.5, 1.0, 2.0]\n                np.random.shuffle(directions)\n                for d in directions:\n                    if work_allow <= 0 or evals >= budget:\n                        break\n                    for s in scales:\n                        if work_allow <= 0 or evals >= budget:\n                            break\n                        step = s * base_step\n                        for sign in (+1.0, -1.0):\n                            x_try = (x_best.copy() if x_best is not None else m.copy()) + sign * d * step\n                            x_try = clip_to_bounds(x_try)\n                            if len(X) > 0 and np.allclose(x_try, X[-1], atol=1e-12):\n                                continue\n                            out = safe_eval(x_try)\n                            work_allow -= 1\n                            if out is None:\n                                break\n                            f_try, x_try = out\n                            if f_try < f_best - 1e-12:\n                                improved_round = True\n                                f_best = float(f_try); x_best = x_try.copy()\n                                trust_radius = np.minimum(trust_radius * 1.2, trust_max)\n                                sigma = min(sigma * 1.12, 5.0 * np.mean(rng_range))\n                                break\n                        if improved_round:\n                            break\n                    if improved_round:\n                        break\n\n            # Occasional final random heavy Student-t / Levy jump if stuck\n            if not improved_round and evals < budget and np.random.rand() < 0.06:\n                # use a Mantegna levy jump scaled\n                jump = self._levy_mantegna(n, alpha=1.5, scale=0.8 * np.mean(trust_radius))\n                x_jump = clip_to_bounds((x_best.copy() if x_best is not None else m.copy()) + jump)\n                out = safe_eval(x_jump)\n                if out is not None:\n                    f_jump, x_jump = out\n                    if f_jump < f_best - 1e-12:\n                        improved_round = True\n                        trust_radius = np.minimum(trust_radius * 1.8, trust_max)\n                        sigma = min(sigma * 1.5, 5.0 * np.mean(rng_range))\n\n            # archive pruning to limit memory\n            max_archive = max(1000, 40 * n)\n            if len(X) > max_archive:\n                F_arr = np.asarray(F)\n                idx_sorted = np.argsort(F_arr)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                if len(rest_idx) > 0:\n                    stride = max(1, len(rest_idx) // (max_archive - 200))\n                    keep_rest = rest_idx[::stride]\n                    keep_idx = np.concatenate([keep_best, keep_rest])[:max_archive]\n                else:\n                    keep_idx = keep_best[:max_archive]\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # small final polish if near budget end\n            if (budget - evals) <= 6 and evals < budget:\n                while evals < budget:\n                    d = np.random.randn(n); d /= (np.linalg.norm(d) + 1e-20)\n                    a = np.random.uniform(-0.1 * np.mean(trust_radius), 0.1 * np.mean(trust_radius))\n                    out = safe_eval(clip_to_bounds((x_best.copy() if x_best is not None else m.copy()) + a * d))\n                    if out is None:\n                        break\n\n            # early stop if extraordinarily small\n            if f_best <= 1e-12:\n                break\n\n        if x_best is None:\n            # fallback: return the best in archive if any\n            if len(X) > 0:\n                idx = int(np.argmin(F))\n                x_best = X[idx].copy(); f_best = F[idx]\n            else:\n                # sample one random point if nothing\n                x_best = np.random.uniform(lb, ub)\n                try:\n                    f_best = func(x_best)\n                except Exception:\n                    f_best = np.inf\n\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 249, in __call__, the following error occurred:\nValueError: shape mismatch: value array of shape (2,) could not be broadcast to indexing result of shape (1,)\nOn line: trust_radius[block_idx] = np.maximum(trust_radius[block_idx] * self.failure_shrink, trust_min)", "error": "In the code, line 249, in __call__, the following error occurred:\nValueError: shape mismatch: value array of shape (2,) could not be broadcast to indexing result of shape (1,)\nOn line: trust_radius[block_idx] = np.maximum(trust_radius[block_idx] * self.failure_shrink, trust_min)", "parent_ids": "3d835d35-9b8a-4e75-927a-467ef4c25c4d", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d5d84a18-22a5-4ef0-b556-b9f3848cb179", "fitness": 0.22765483122798463, "name": "ALPS", "description": "ALPS mixes per-coordinate adaptive scaling (EWMA var_est -> s, init sigma = 0.12*range, s init 0.3, var_est init 0.09) with a streaming low‑rank subspace U learned via Oja-style updates to generate correlated perturbations, and a lightweight local linear least-squares model from a recent archive to produce gradient-assisted quasi-Newton probes. Candidate generation is a stochastic mixture of diagonal Gaussian noise (small population lambda ~ max(6, pop_factor*log(dim))), low-rank moves (lowrank_scale=0.8), occasional heavy-tailed Cauchy jumps (p_cauchy=0.08), archive-difference DE-like steps (p_archive_diff=0.12, F_de=0.5) and mirrored proposals (p_mirror=0.18), with robust recombination by coordinate-wise median and a trust-blend toward the center. Adaptation and control use per-dim EWMA (s_update_beta=0.12), subspace learning rate (subspace_eta=0.06), sigma adaptation by success-fraction (target_success=0.2), trust-radius expansion/shrink (trust_expand=1.25, trust_shrink=0.6), periodic gradient line probes, a bounded archive for modeling/DE, and stagnation-triggered reheating and subspace reshaping to escape traps.", "code": "import numpy as np\n\nclass ALPS:\n    \"\"\"\n    Adaptive Low-Rank Projection Search (ALPS)\n\n    Key ideas:\n    - Per-coordinate adaptive scaling (EWMA of selected step variances).\n    - Streaming low-rank subspace (k directions) learned from selected steps via a power/Oja style update.\n    - Lightweight linear local model (least-squares on recent archive near center) to obtain a gradient estimate,\n      used for cheap quasi-Newton-like probes along estimated descent.\n    - Mixture of proposals: diagonal Gaussian (scaled by per-dim s), low-rank subspace perturbations, occasional\n      Cauchy heavy tails, archive-difference (DE-like) perturbations, and mirrored pairs for robustness.\n    - Robust recombination (median + trust-blend), sigma adaptation via success-fraction rule, and stagnation\n      restarts/reshaping of subspace to escape traps.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, k=2, pop_factor=2.0):\n        \"\"\"\n        budget: number of function evaluations allowed\n        dim: problem dimension\n        seed: RNG seed\n        k: low-rank subspace dimensionality (recommend 1..3)\n        pop_factor: controls small population size lam ~ pop_factor * log(dim)\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # small population scaled with log(dim)\n        self.lambda_ = max(6, int(round(pop_factor * max(2.0, np.log(self.dim + 2.0)))))\n\n        # low-rank subspace dimension\n        self.k = max(1, min(k, self.dim))\n\n        # mixture hyper-params (conservative by default)\n        self.p_cauchy = 0.08\n        self.p_archive_diff = 0.12\n        self.p_mirror = 0.18\n        self.p_grad_probe = 0.22      # probability to try gradient-assisted proposal if gradient available\n        self.F_de = 0.5\n        self.target_success = 0.2\n\n        # adaptation speeds\n        self.s_update_beta = 0.12     # EWMA for per-dim variance estimate\n        self.subspace_eta = 0.06      # Oja-like update rate for low-rank\n        self.trust_shrink = 0.6\n        self.trust_expand = 1.25\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (fallback -5..5)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        m = m.astype(float)\n\n        # global step-size and per-dimension scales\n        sigma = 0.12 * np.mean(ub - lb)\n        s = np.ones(n) * (0.3)  # per-dim relative scale (multiplies N(0,1))\n        var_est = np.ones(n) * (0.09)  # EWMA variance proxy (std^2)\n\n        # low-rank subspace U: k orthonormal vectors (k x n)\n        U = np.random.randn(self.k, n)\n        for i in range(self.k):\n            U[i] /= (np.linalg.norm(U[i]) + 1e-20)\n        # small coefficients scaling for low-rank perturbations\n        lowrank_scale = 0.8\n\n        # archive (limited) for DE differences and local linear model\n        archive_X = []\n        archive_F = []\n        max_archive = max(200, 10 * n)\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation\n        xm = np.clip(m, lb, ub)\n        f0 = func(xm)\n        evals += 1\n        archive_X.append(xm.copy()); archive_F.append(f0)\n        f_opt = f0; x_opt = xm.copy()\n        f_center = f0\n        last_improv = evals\n\n        gen = 0\n        stagn_count = 0\n        stagn_limit = max(12, int(8 + n // 3))\n\n        # trust radius (in normalized step units, multiplied by sigma * mean(s))\n        trust_R = 2.0\n\n        # helper: compute local linear gradient estimate around m using nearby archive points\n        def estimate_gradient(center, archive_X_local, archive_F_local, radius_ratio=1.2, min_points=6):\n            if len(archive_X_local) < min_points:\n                return None\n            X = np.array(archive_X_local)\n            F = np.array(archive_F_local)\n            # distances in parameter space scaled by range\n            ranges = (ub - lb)\n            ranges[ranges == 0] = 1.0\n            dists = np.linalg.norm((X - center) / ranges, axis=1)\n            # keep those within radius (in normalized units)\n            med = np.median(dists)\n            r = max(0.01, radius_ratio * (np.mean(dists) + 1e-12))\n            mask = dists <= r\n            if np.sum(mask) < min_points:\n                # fallback: take nearest min_points\n                idx = np.argsort(dists)[:min_points]\n                Xsel = X[idx]\n                Fsel = F[idx]\n            else:\n                Xsel = X[mask]; Fsel = F[mask]\n            # build linear model f ≈ a + g^T (x - center)\n            Y = Xsel - center\n            # regularize small number of points\n            A = Y\n            b = Fsel - np.mean(Fsel)\n            # solve least squares for g (with Tikhonov regularization)\n            reg = 1e-6 * (1.0 + np.var(b))\n            try:\n                # solve for g via normal equations with regularization\n                ATA = A.T.dot(A) + reg * np.eye(n)\n                g = np.linalg.solve(ATA, A.T.dot(b))\n                # if gradient zero-ish return None\n                if np.linalg.norm(g) < 1e-8:\n                    return None\n                return g\n            except np.linalg.LinAlgError:\n                return None\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n\n            # prepare population noise\n            Z = np.random.randn(lam, n)\n\n            X = np.zeros((lam, n))\n            Y_steps = np.zeros((lam, n))  # raw normalized steps y (x = m + sigma*y)\n            Fvals = np.full(lam, np.inf)\n\n            # compute a current gradient estimate sometimes\n            grad = estimate_gradient(m, archive_X, archive_F, radius_ratio=1.0, min_points=8)\n            have_grad = (grad is not None)\n\n            mean_s = np.mean(np.abs(s)) + 1e-12\n\n            for i in range(lam):\n                # base diagonal-Gaussian step\n                z = Z[i]\n                y_diag = s * z\n\n                # low-rank perturbation: sample coefficients and combine with U\n                coeffs = np.random.randn(self.k)\n                y_low = lowrank_scale * (coeffs @ U) * mean_s\n\n                # combine\n                y = y_diag + y_low\n\n                # occasional gradient assisted mini-step (cheap quasi-Newton candidate)\n                if have_grad and (np.random.rand() < self.p_grad_probe):\n                    # build a simple preconditioner using var_est (diag approx to Hessian)\n                    precond = 1.0 / (np.sqrt(var_est) + 1e-8)\n                    # scale gradient direction to unit in preconditioned norm\n                    g = grad\n                    gscaled = precond * g\n                    ng = np.linalg.norm(gscaled) + 1e-20\n                    # choose an adaptive length within trust region\n                    alpha = np.random.choice([0.6, 1.0, 0.35]) * np.random.rand()\n                    yg = -alpha * (gscaled / ng) * mean_s * 1.5\n                    # blend with random noise for robustness\n                    blend = 0.65\n                    y = blend * yg + (1.0 - blend) * y\n\n                # occasional heavy tail Cauchy/Levy jump\n                if np.random.rand() < self.p_cauchy:\n                    c = np.random.standard_cauchy(size=n)\n                    # damp the heavy tails by per-dim scale\n                    y = y + 0.6 * mean_s * (s * c)\n\n                # archive-difference (DE-like)\n                if (np.random.rand() < self.p_archive_diff) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de = self.F_de * (archive_X[i1] - archive_X[i2])\n                    # convert absolute diff to normalized y-space (divide by sigma)\n                    y = y + (de / max(1e-12, sigma))\n\n                # limit step in normalized y-space by trust_R\n                y_norm = np.linalg.norm(y / (np.maximum(s, 1e-8)))\n                if y_norm > trust_R:\n                    y = y * (trust_R / (y_norm + 1e-20))\n\n                x = m + sigma * y\n                x = np.clip(x, lb, ub)\n\n                X[i] = x\n                Y_steps[i] = y\n\n                # mirrored counterpart sometimes: replace with mirror if chosen (we keep budget control by only evaluating one)\n                if (np.random.rand() < self.p_mirror):\n                    # mirrored point across m: x' = m - (x - m) = 2*m - x\n                    xm = np.clip(2.0 * m - x, lb, ub)\n                    # prefer the one closer to center in step-length (we store x and may later evaluate both depending on budget)\n                    # To keep structure simple, we prefer evaluating the one with lower expected magnitude:\n                    if np.linalg.norm(xm - m) < np.linalg.norm(x - m):\n                        X[i] = xm\n                        Y_steps[i] = (xm - m) / max(1e-20, sigma)\n\n            # Evaluate candidates in sequence (budget-limited)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if len(archive_X) > max_archive:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi\n                    x_opt = xi.copy()\n                    last_improv = evals\n\n            # selection: robust top-mu selection\n            mu = max(1, lam // 3)\n            idx = np.argsort(Fvals)[:mu]\n            X_sel = X[idx]\n            Y_sel = Y_steps[idx]\n            F_sel = Fvals[idx]\n\n            # recombination: coordinate-wise median and trust-blend with center\n            m_median = np.median(X_sel, axis=0)\n            # trust-blend step: move somewhat toward m_median but respect trust radius\n            raw_step = m_median - m\n            step_norm = np.linalg.norm(raw_step) / (sigma * mean_s + 1e-20)\n            blend_alpha = 0.5 if step_norm <= trust_R else (0.5 * trust_R / (step_norm + 1e-20))\n            m_proposed = np.clip(m + blend_alpha * raw_step, lb, ub)\n\n            # optionally evaluate proposed center (if budget)\n            accepted_center = False\n            if evals < budget:\n                f_prop = func(m_proposed)\n                evals += 1\n                archive_X.append(m_proposed.copy()); archive_F.append(f_prop)\n                if len(archive_X) > max_archive:\n                    archive_X.pop(0); archive_F.pop(0)\n                delta = f_prop - f_center\n                if delta <= 0:\n                    # accept improvement\n                    m = m_proposed.copy()\n                    f_center = f_prop\n                    accepted_center = True\n                else:\n                    # probabilistic acceptance with gentle temperature-like factor (decreasing with gen)\n                    T = max(1e-3, 0.7 * (0.98 ** gen))\n                    p = np.exp(-delta / T)\n                    if np.random.rand() < p:\n                        m = m_proposed.copy()\n                        f_center = f_prop\n                        accepted_center = True\n                    else:\n                        # small nudges toward proposed center occasionally\n                        if np.random.rand() < 0.07:\n                            m = np.clip(0.92 * m + 0.08 * m_proposed, lb, ub)\n\n            else:\n                # no budget -> adopt clipped median without evaluation, cautious blend\n                m = np.clip(m_proposed, lb, ub)\n\n            # update per-dim variance estimate (EWMA) from selected steps Y_sel\n            if Y_sel.shape[0] >= 1:\n                # convert selected steps to absolute coordinate-space step sizes (multiply by sigma)\n                steps_coords = (sigma * Y_sel)\n                # variance per-dim\n                var_sel = np.var(steps_coords, axis=0)\n                var_est = (1.0 - self.s_update_beta) * var_est + self.s_update_beta * (var_sel + 1e-12)\n                s = np.sqrt(np.maximum(1e-12, var_est)) / (sigma + 1e-12)\n                # clip s\n                s = np.clip(s, 1e-6, 1e3)\n\n            # update low-rank subspace U using Oja-like update with selected steps\n            if Y_sel.shape[0] >= 1:\n                # use top-step mean in coordinate space (not normalized)\n                mean_step = np.mean(Y_sel, axis=0)\n                # form a small k x n update: push existing directions toward mean_step and principal components of Y_sel\n                # compute principal direction of selected steps via simple power iteration\n                cov_vec = np.mean((Y_sel.T * (Y_sel.dot(mean_step))), axis=1)\n                for j in range(self.k):\n                    u = U[j]\n                    # Oja update: u <- u + eta * (cov_vec - (u^T cov_vec) u)\n                    proj = np.dot(u, cov_vec)\n                    u = u + self.subspace_eta * (cov_vec - proj * u)\n                    # re-orthonormalize against previous\n                    if j > 0:\n                        # subtract components along earlier vectors\n                        for prev in range(j):\n                            u = u - np.dot(u, U[prev]) * U[prev]\n                    nrm = np.linalg.norm(u) + 1e-20\n                    U[j] = u / nrm\n\n            # sigma adaptation: count how many candidates improved over current center f_center (strict)\n            if lam > 0:\n                successes = np.sum(Fvals < (f_center - 1e-12))\n                psucc = successes / lam\n                # adjust sigma multiplicatively: if too many successes -> shrink, too few -> grow\n                if psucc > self.target_success:\n                    sigma *= 0.95\n                elif psucc < (self.target_success * 0.5):\n                    sigma *= 1.045\n                else:\n                    sigma *= 0.995\n            # bound sigma\n            sigma = np.clip(sigma, 1e-12, 5.0 * np.mean(ub - lb) + 1e-12)\n\n            # adapt trust radius depending on improvements inside selection\n            if np.min(F_sel) < f_center:\n                # successful local improvement -> perhaps expand trust\n                trust_R = min(6.0, trust_R * self.trust_expand)\n            else:\n                # if no improvement -> shrink trust radius\n                trust_R = max(0.6, trust_R * self.trust_shrink)\n\n            # occasional short line of probes along estimated gradient if available (cheap and limited schedule)\n            if have_grad and (gen % max(6, int(4 + n // 8)) == 0) and (evals < budget):\n                # construct preconditioned direction\n                precond = 1.0 / (np.sqrt(var_est) + 1e-8)\n                g = grad\n                gscaled = precond * g\n                if np.linalg.norm(gscaled) > 1e-12:\n                    gdir = gscaled / (np.linalg.norm(gscaled) + 1e-20)\n                    alphas = [0.9, 0.45, -0.45]\n                    for alpha in alphas:\n                        if evals >= budget:\n                            break\n                        probe = np.clip(m - alpha * sigma * mean_s * gdir, lb, ub)\n                        fp = func(probe)\n                        evals += 1\n                        archive_X.append(probe.copy()); archive_F.append(fp)\n                        if len(archive_X) > max_archive:\n                            archive_X.pop(0); archive_F.pop(0)\n                        if fp < f_opt:\n                            f_opt = fp; x_opt = probe.copy(); last_improv = evals\n\n            # stagnation detection and mild restart/reshaping\n            gen += 1\n            if (evals - last_improv) > stagn_limit:\n                stagn_count += 1\n                # perturb center slightly toward a random archived good point\n                if len(archive_X) > 0:\n                    idx = np.argmin(archive_F[-min(len(archive_F), 50):]) if len(archive_F) > 0 else None\n                    if idx is not None:\n                        candidate = archive_X[-min(len(archive_X), 50) + idx]\n                        # small relocation blend\n                        m = np.clip(0.85 * m + 0.15 * candidate, lb, ub)\n                # reheating sigma and trust radius\n                sigma *= (1.0 + 0.25 * stagn_count)\n                trust_R = max(1.0, trust_R * 1.2)\n                # randomize a bit of the subspace\n                for j in range(self.k):\n                    U[j] = U[j] + 0.08 * np.random.randn(n)\n                    # orthonormalize\n                    for p in range(j):\n                        U[j] = U[j] - np.dot(U[j], U[p]) * U[p]\n                    U[j] /= (np.linalg.norm(U[j]) + 1e-20)\n                last_improv = evals\n\n            # ensure m stays in bounds\n            m = np.clip(m, lb, ub)\n\n            # break on budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ALPS scored 0.228 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "534d32c2-34bb-4886-b9b7-d54d66e1e415", "operator": null, "metadata": {"aucs": [0.1035669063835144, 0.16916778654257458, 0.2637860110760313, 0.4229236254733345, 0.2250116528838889, 0.24639664001358574, 0.23479583052652353, 0.26983657012436657, 0.2020429632571974, 0.1390203259988294]}, "task_prompt": ""}
{"id": "c50e16df-741a-48ce-b6de-f117f594362f", "fitness": 0.3079443781448453, "name": "ABES", "description": "ABES is a hybrid population-based continuous optimizer that scales population size as lambda ≈ max(8, pop_factor * log(dim)) (pop_factor set relatively large) and partitions coordinates into contiguous blocks (~dim^(2/3)) to encourage structured block moves and occasional block permutations. Candidate generation mixes coordinate-wise Gaussian steps with per-coordinate scales (s) estimated by smoothed MAD, an Oja-style learned principal direction (v) for directional bias, occasional heavy-tailed Cauchy jumps (p_cauchy=0.20), DE-like archive differences (p_de=0.25, F_de=0.7) and trimmed-mean recombination for robust center proposals. Control is handled by a multiplicative log-step sigma adaptation targeting a higher success rate (0.25), a Metropolis-like temperature acceptance with reheating and sigma boosts on stagnation, and archive/1D probes to maintain exploration; many probabilities and learning rates (e.g. oja_base=0.25, c_mad=0.25, p_blockswap=0.20) are tuned toward more aggressive exploration and robustness.", "code": "import numpy as np\n\nclass ABES:\n    \"\"\"\n    Adaptive Blockwise Ensemble Search (ABES)\n\n    Main ideas / tunable parameters (visible here):\n    - budget, dim: required\n    - seed: optional RNG seed\n    - pop_factor: controls population size lambda ~ pop_factor * log(dim)\n    - blocks: number of coordinate blocks (None => ~ dim^(2/3))\n    - sigma: global step-size (scalar), adapted with a log-step rule (multiplicative exp)\n    - s: per-coordinate scales (std proxies) estimated robustly via MAD with smoothing\n    - v: single learned principal direction (Oja-like update)\n    - archive: reservoir for DE-like differences (limited size)\n    - mixture: Gaussian coordinate steps + occasional Cauchy heavy tails + DE differences + block permutations\n    - recombination: coordinate-wise trimmed mean (robust)\n    - acceptance: Metropolis-like acceptance with temperature and reheating/stagnation\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, blocks=None, pop_factor=4.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population scaling (larger than original)\n        self.pop_factor = float(pop_factor)\n        self.lambda_ = max(8, int(np.round(self.pop_factor * max(2.0, np.log(self.dim + 2.0)))))\n\n        # blocks default ~ dim^(2/3) to encourage somewhat larger structured moves\n        if blocks is None:\n            self.n_blocks = max(1, int(np.ceil(self.dim ** (2.0 / 3.0))))\n        else:\n            self.n_blocks = max(1, int(min(blocks, self.dim)))\n\n        # create contiguous balanced blocks initially\n        sizes = [self.dim // self.n_blocks] * self.n_blocks\n        for i in range(self.dim % self.n_blocks):\n            sizes[i] += 1\n        idx = 0\n        self.blocks = []\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n        # hyper-parameters (different from original)\n        self.p_cauchy = 0.20       # probability heavy-tailed Cauchy jump (larger)\n        self.p_de = 0.25           # probability DE mutation (larger)\n        self.F_de = 0.7            # DE scaling (larger)\n        self.p_blockswap = 0.20    # block permutation probability (larger)\n        self.alpha_dir = 0.30      # weight for directional bias (smaller)\n        self.target_success = 0.25 # target success probability for sigma adaptation (slightly higher)\n        self.trim_frac = 0.20      # trimmed mean fraction for recombination (robust)\n        self.max_archive = 500     # archive size (moderate)\n        self.c_mad = 0.25          # smoothing for per-coordinate scale update (a bit stronger)\n        # Oja base learning rate (more aggressive initially, decays)\n        self.oja_base = 0.25\n\n    def _trimmed_mean(self, X_sel):\n        # coordinate-wise trimmed mean: remove k elements from both ends per coordinate\n        mu = X_sel.shape[0]\n        if mu <= 2:\n            return np.median(X_sel, axis=0)\n        k = int(np.floor(self.trim_frac * mu))\n        if k == 0:\n            return np.mean(X_sel, axis=0)\n        # sort each column and take mean of central slice\n        Xs = np.sort(X_sel, axis=0)\n        central = Xs[k: max(k + 1, mu - k), :]  # ensure at least one row\n        return np.mean(central, axis=0)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (functions on BBOB typically provide bounds, fallback to -5..5)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initialize state\n        m = np.random.uniform(lb, ub)                         # current center\n        sigma = 0.20 * np.mean(ub - lb)                       # initial global step-size (different)\n        s = np.ones(n) * 1.0                                  # per-coordinate scales\n        v = np.random.randn(n)                                # learned principal direction\n        v /= (np.linalg.norm(v) + 1e-20)\n\n        # temperature for probabilistic acceptance (smaller initial)\n        Temp = 0.5\n\n        # small archive for DE differences\n        archive_X = []\n        archive_F = []\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation at m\n        xm0 = np.clip(m, lb, ub)\n        f0 = func(xm0)\n        evals += 1\n        archive_X.append(xm0.copy()); archive_F.append(f0)\n        f_opt = f0; x_opt = xm0.copy()\n        f_center = f0\n        last_improv = evals\n\n        gen = 0\n        stagn_count = 0\n        stagn_limit = max(10, int(5 * n / max(1, self.n_blocks)))\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(max(4, self.lambda_), remaining)\n\n            # sample independent normals for population\n            Z = np.random.randn(lam, n)\n\n            X = np.zeros((lam, n))\n            Ys = np.zeros((lam, n))   # underlying step vectors (pre-scaling by sigma) for learning\n            Fvals = np.full(lam, np.inf)\n\n            for i in range(lam):\n                z = Z[i].copy()\n                # coordinate-wise scale\n                y_local = s * z\n                # add directional bias along learned direction v\n                dir_scalar = np.random.randn() * self.alpha_dir\n                y = y_local + dir_scalar * v * np.mean(s)\n                # occasionally heavy-tailed jump along v scaled by mean scale (Cauchy)\n                if np.random.rand() < self.p_cauchy:\n                    t = np.random.standard_cauchy()\n                    y = v * t * np.mean(s)\n                # base candidate\n                x = m + sigma * y\n\n                # DE-like archive difference (if archive has enough entries)\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # block permutation: permute coordinates inside a block to propose structured shuffle\n                if (np.random.rand() < self.p_blockswap) and (len(self.blocks) > 0):\n                    bidx = np.random.randint(len(self.blocks))\n                    b = self.blocks[bidx]\n                    if len(b) > 1:\n                        perm = np.random.permutation(len(b))\n                        x_block = x[b].copy()\n                        x[b] = x_block[perm]\n\n                # clip and store\n                x = np.clip(x, lb, ub)\n                X[i] = x\n                Ys[i] = y\n\n            # Evaluate candidates until budget or population finished\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if len(archive_X) > self.max_archive:\n                    # keep the archive recent\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi\n                    x_opt = xi.copy()\n                    last_improv = evals\n\n            # selection: pick top mu\n            mu = max(1, lam // 3)\n            idx = np.argsort(Fvals)\n            sel = idx[:mu]\n            X_sel = X[sel]\n            Y_sel = Ys[sel]    # steps corresponding to selected candidates\n\n            # recombination: coordinate-wise trimmed mean (robust)\n            m_new = self._trimmed_mean(X_sel)\n\n            # acceptance of new center: evaluate if budget allows\n            accept_mean = False\n            if evals < budget:\n                xm_clip = np.clip(m_new, lb, ub)\n                fm_new = func(xm_clip)\n                evals += 1\n                archive_X.append(xm_clip.copy()); archive_F.append(fm_new)\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0); archive_F.pop(0)\n                # accept if improved or by temperature probability\n                delta = fm_new - f_center\n                if delta <= 0:\n                    accept_mean = True\n                    f_center = fm_new\n                    m = xm_clip.copy()\n                else:\n                    p = np.exp(-delta / max(1e-12, Temp))\n                    if np.random.rand() < p:\n                        accept_mean = True\n                        f_center = fm_new\n                        m = xm_clip.copy()\n                    else:\n                        # conservative nudge toward m_new with small probability\n                        if np.random.rand() < 0.08:\n                            m = np.clip(0.90 * m + 0.10 * m_new, lb, ub)\n                # gentle cooling\n                Temp *= 0.995\n                Temp = max(1e-6, Temp)\n            else:\n                # no budget left for center eval: adopt trimmed mean but clipped\n                m = np.clip(m_new, lb, ub)\n\n            # sigma adaptation via log-step update (multiplicative exp rule)\n            if lam > 0:\n                successes = np.sum(Fvals < f_center - 1e-12)\n                psucc = successes / lam\n                # update: sigma <- sigma * exp( factor * (psucc - target) / denom )\n                denom = 1.0 + (np.sqrt(n) / 8.0)\n                factor = 0.9\n                sigma *= np.exp(factor * (psucc - self.target_success) / denom)\n            sigma = np.clip(sigma, 1e-12, 10.0 * np.mean(ub - lb) + 1e-12)\n\n            # per-coordinate scales update using MAD with smoothing\n            if Y_sel.shape[0] >= 1:\n                med = np.median(Y_sel, axis=0)\n                mad = np.median(np.abs(Y_sel - med[None, :]), axis=0)\n                est_std = 1.4826 * (mad + 1e-20)\n                s = np.sqrt((1.0 - self.c_mad) * (s ** 2) + self.c_mad * (est_std ** 2))\n                s = np.clip(s, 1e-8, 1e4)\n\n            # Online Oja-style update for principal direction v using selected steps\n            if Y_sel.shape[0] >= 1:\n                eta = self.oja_base / (1.0 + 0.02 * gen)\n                # covariance-vector approximation:\n                Yv = Y_sel.dot(v)                       # shape mu\n                cov_v = (Y_sel.T.dot(Yv)) / max(1, Y_sel.shape[0])\n                v = v + eta * cov_v\n                v = v / (np.linalg.norm(v) + 1e-20)\n\n            # stagnation handling\n            gen += 1\n            if (evals - last_improv) > stagn_limit:\n                stagn_count += 1\n                # strong reheating: bump temperature and sigma\n                Temp = max(Temp, 1.0)\n                sigma *= 2.0\n                # more aggressive reshuffle of block membership\n                if len(self.blocks) > 1:\n                    for _ in range(min(5, self.n_blocks)):\n                        b1, b2 = np.random.choice(len(self.blocks), size=2, replace=False)\n                        if (len(self.blocks[b1]) > 0) and (len(self.blocks[b2]) > 0):\n                            i1 = np.random.choice(self.blocks[b1])\n                            i2 = np.random.choice(self.blocks[b2])\n                            # swap membership\n                            self.blocks[b1].remove(i1); self.blocks[b1].append(i2)\n                            self.blocks[b2].remove(i2); self.blocks[b2].append(i1)\n                # perturb principal direction more strongly\n                v = v + 0.15 * np.random.randn(n)\n                v /= (np.linalg.norm(v) + 1e-20)\n                last_improv = evals\n\n            # occasional short 1D probe along v with different schedule\n            if (gen % max(6, int(3 + n // 8)) == 0) and (evals < budget):\n                alphas = [0.5, -0.5, 0.25, -0.25]\n                for alpha in alphas:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + sigma * alpha * v * np.mean(s), lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if len(archive_X) > self.max_archive:\n                        archive_X.pop(0); archive_F.pop(0)\n                    if fp < f_opt:\n                        f_opt = fp; x_opt = probe.copy(); last_improv = evals\n\n            # keep center in bounds\n            m = np.clip(m, lb, ub)\n\n            # break on budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ABES scored 0.308 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "534d32c2-34bb-4886-b9b7-d54d66e1e415", "operator": null, "metadata": {"aucs": [0.13351841560376398, 0.20111006399351605, 0.31064447573709664, 0.36130887791026267, 0.27319060040423315, 0.4112622389990913, 0.2913107446540475, 0.3368197736735581, 0.5952489475527452, 0.16502964292013844]}, "task_prompt": ""}
{"id": "638d294f-accf-49e3-8d5a-498dd92ec7c4", "fitness": 0.1919706453077314, "name": "HiSAS", "description": "HiSAS partitions the search space into several overlapping random subspaces (default ~sqrt(dim) subspaces of size ~dim/3) and uses a soft bandit (softmax over decaying sub_scores) to focus sampling on promising subspaces. Within a selected subspace candidates are generated with anisotropic per-dimension Gaussian steps (scales s), augmented by a low-rank directional sketch (leading PCA direction from recent archive differences), occasional Cauchy heavy-tail jumps and DE-style differences, then normalized to a global trust radius TR that contracts on success and expands on failure. The algorithm maintains an archive, recombines top candidates via inverse-rank weights to propose a new center, adapts per-dimension scales by median absolute deviation smoothing, and updates subspace scores multiplicatively with decay to concentrate search. For refinement and robustness it performs periodic diagonal quadratic (Newton-like) fits on top uncertain dimensions, reheats/rebuilds subspaces and does global perturbations on stagnation, always respecting bounds and the evaluation budget.", "code": "import numpy as np\n\nclass HiSAS:\n    \"\"\"\n    Hierarchical Subspace Adaptive Search (HiSAS)\n\n    Key ideas:\n    - Partition the variables into several overlapping subspaces (subspace bandit).\n    - Samples are generated primarily inside one chosen subspace (zero outside),\n      using adaptive per-dimension scales and a low-rank directional sketch\n      (leading PCA direction from recent differences restricted to that subspace).\n    - Maintain a trust radius (TR) that expands/contracts depending on success.\n    - Archive of recent evaluations provides DE-style differences and PCA sketches.\n    - Occasional Cauchy (heavy-tail) jumps and DE-differences increase exploration.\n    - Subspaces have soft-allocated scores (bandit) to focus sampling on promising subspaces.\n    - Periodic local diagonal quadratic fits (on top dimensions) propose short refinements.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 pop=12, n_subspaces=None, subspace_size=None,\n                 p_cauchy=0.08, p_de=0.12, F_de=0.6, max_archive=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # sampling/population per generation\n        self.pop = max(4, int(pop))\n\n        # subspaces: overlapping groups; default ~ sqrt(dim) subspaces\n        if n_subspaces is None:\n            self.n_subspaces = max(2, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.n_subspaces = max(1, int(min(n_subspaces, self.dim)))\n\n        if subspace_size is None:\n            # each subspace covers about 1/3 of dims by default (overlap allowed)\n            self.subspace_size = max(1, int(np.ceil(self.dim / 3)))\n        else:\n            self.subspace_size = max(1, int(min(subspace_size, self.dim)))\n\n        self.p_cauchy = float(p_cauchy)\n        self.p_de = float(p_de)\n        self.F_de = float(F_de)\n\n        self.max_archive = max_archive\n\n    def _make_subspaces(self):\n        # create overlapping random subspaces (list of index lists)\n        S = []\n        all_idx = np.arange(self.dim)\n        for i in range(self.n_subspaces):\n            # biased sampling: ensure some overlap by sampling with replacement and shifting\n            start = np.random.randint(0, self.dim)\n            choices = np.random.choice(all_idx, size=self.subspace_size, replace=False)\n            S.append(sorted(np.unique(choices).tolist()))\n        return S\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (BBOB typical: -5..5), but read if func provides them\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # archive\n        max_archive = self.max_archive if self.max_archive is not None else min(5000, max(200, 50 * n))\n        X_archive = []\n        F_archive = []\n\n        # initialize center uniformly inside bounds\n        m = np.random.uniform(lb, ub)\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation\n        if evals < budget:\n            fm = func(m.copy()); evals += 1\n            f_center = float(fm)\n            X_archive.append(m.copy()); F_archive.append(float(fm))\n            f_opt = float(fm); x_opt = m.copy()\n        else:\n            # no budget\n            return float(f_opt), np.array(x_opt, dtype=float)\n\n        # per-dim scales (start small relative to range)\n        global_range = np.mean(ub - lb)\n        s = np.full(n, 0.2 * global_range)  # typical step scale per-dim (can adapt)\n        # trust radius scalar (controls overall step magnitude)\n        TR = 0.25 * global_range\n\n        # create subspaces and their soft scores\n        subspaces = self._make_subspaces()\n        sub_scores = np.ones(len(subspaces), dtype=float)  # bandit weights\n\n        gen = 0\n        stagn = 0\n        last_improv_eval = evals\n\n        # helper: push to archive\n        def push_archive(x, fx):\n            X_archive.append(x.copy()); F_archive.append(float(fx))\n            if len(X_archive) > max_archive:\n                # drop oldest\n                del X_archive[0]; del F_archive[0]\n\n        # main loop: generate candidates in batches of self.pop until budget exhausted\n        while evals < budget:\n            lam = min(self.pop, budget - evals)\n            candidates = []\n            candidate_steps = []  # store raw steps (y) for adaptation\n            candidate_sub_idx = []\n\n            # precompute subspace probabilities (softmax of scores)\n            probs = np.exp( (sub_scores - np.max(sub_scores)) / max(1e-12, np.std(sub_scores)+1e-6) )\n            probs = probs / np.sum(probs)\n\n            for k in range(lam):\n                # pick a subspace according to softmax probabilities\n                sid = np.random.choice(len(subspaces), p=probs)\n                S = subspaces[sid]\n                candidate_sub_idx.append(sid)\n\n                # build step y (zero outside S)\n                y = np.zeros(n)\n\n                # for dims in S, sample anisotropic Gaussian with per-dim scales\n                # and add a small low-rank directional term estimated from archive\n                if len(X_archive) >= 4:\n                    # build differences relative to current center for points in archive, restricted to S\n                    # take recent up to 60 points\n                    recent = min(len(X_archive), 60)\n                    D = np.array(X_archive[-recent:]) - m[None, :]  # shape recent x n\n                    Dsub = D[:, S]  # recent x |S|\n                    # center Dsub\n                    Dsub_mean = np.mean(Dsub, axis=0)\n                    Dsubc = Dsub - Dsub_mean[None, :]\n                    # PCA sketch (leading direction)\n                    try:\n                        U, W, Vt = np.linalg.svd(Dsubc, full_matrices=False)\n                        # principal direction p in subspace\n                        if Vt.shape[0] > 0:\n                            p = Vt[0, :]\n                        else:\n                            p = np.random.randn(len(S))\n                            p /= (np.linalg.norm(p) + 1e-20)\n                    except Exception:\n                        p = np.random.randn(len(S))\n                        p /= (np.linalg.norm(p) + 1e-20)\n                    # small weight for directional move\n                    dir_amp = 0.6 * (0.7 ** (gen / max(1, 40))) * np.mean(s[S])  # decaying amplitude\n                    dir_term = dir_amp * p * np.random.randn()\n                else:\n                    # random direction if not enough archive\n                    p = np.random.randn(len(S))\n                    p /= (np.linalg.norm(p) + 1e-20)\n                    dir_amp = 0.6 * np.mean(s[S])\n                    dir_term = dir_amp * p * np.random.randn()\n\n                # gaussian local term\n                local = np.random.randn(len(S)) * s[S]\n                # combine: anisotropic local + directional sketch (low-rank) scaled by trust radius\n                yS = local + dir_term\n\n                # occasional Cauchy heavy-tail in this subspace\n                if np.random.rand() < self.p_cauchy:\n                    # one or two coordinates get a Cauchy jump\n                    cauch_mask = np.random.rand(len(S)) < 0.25\n                    if not np.any(cauch_mask):\n                        cauch_mask[np.random.randint(len(S))] = True\n                    yS[cauch_mask] += np.tan(np.pi * (np.random.rand(np.sum(cauch_mask)) - 0.5)) * (np.mean(s[S]) * 1.5)\n\n                # occasionally add DE-like difference restricted to S\n                if (np.random.rand() < self.p_de) and (len(X_archive) >= 2):\n                    i1, i2 = np.random.choice(len(X_archive), size=2, replace=False)\n                    de = self.F_de * (X_archive[i1][S] - X_archive[i2][S])\n                    yS += de\n\n                y[S] = yS\n\n                # final candidate x = m + TR * normalized_step where normalization roughly matches per-dim scales\n                # normalize so that typical L2 <= 1 for scaled step to ensure TR interpretable\n                norm_est = np.linalg.norm(y / (s + 1e-12))\n                if norm_est < 1e-12:\n                    step = np.zeros_like(y)\n                else:\n                    step = (y / (norm_est + 1e-12)) * TR  # scale to trust radius in L2 sense\n\n                x = np.clip(m + step, lb, ub)\n\n                candidates.append(x)\n                candidate_steps.append(step)\n\n            # Evaluate candidates one by one (respect budget)\n            Fvals = np.full(len(candidates), np.inf)\n            for i, x in enumerate(candidates):\n                if evals >= budget:\n                    break\n                fx = func(x.copy()); evals += 1\n                Fvals[i] = float(fx)\n                push_archive(x, fx)\n                if fx < f_opt:\n                    f_opt = float(fx); x_opt = x.copy(); last_improv_eval = evals\n\n            # Selection: choose top mu candidates (robust)\n            valid_idx = np.where(np.isfinite(Fvals))[0]\n            if valid_idx.size == 0:\n                break\n            mu = max(1, len(valid_idx) // 3)\n            idx_sorted = valid_idx[np.argsort(Fvals[valid_idx])]\n            sel_idx = idx_sorted[:mu]\n            sel_x = [candidates[i] for i in sel_idx]\n            sel_steps = [candidate_steps[i] for i in sel_idx]\n            sel_subs = [candidate_sub_idx[i] for i in sel_idx]\n            sel_F = [Fvals[i] for i in sel_idx]\n\n            # robust recombination: weighted average by inverse-rank weights\n            ranks = np.arange(len(sel_F))[::-1] + 1.0\n            weights = 1.0 / (1.0 + np.arange(len(sel_F)))\n            weights = weights / np.sum(weights)\n            m_candidate = np.sum(np.vstack(sel_x) * weights[:, None], axis=0)\n\n            # Evaluate new center candidate only if we have budget left and it's distinct\n            accepted = False\n            if evals < budget:\n                mc = np.clip(m_candidate, lb, ub)\n                fm = func(mc.copy()); evals += 1\n                push_archive(mc, fm)\n                if fm < f_center or (np.random.rand() < 0.02 and fm < f_center + 1e-6):\n                    # accept if improving (or tiny probability)\n                    m = mc.copy(); f_center = float(fm); accepted = True\n                    if fm < f_opt:\n                        f_opt = float(fm); x_opt = mc.copy(); last_improv_eval = evals\n                else:\n                    # soft move toward candidate if not accepted: small interpolation\n                    if np.random.rand() < 0.08:\n                        m = np.clip(0.95 * m + 0.05 * mc, lb, ub)\n\n            else:\n                # if no budget to evaluate center, adopt median-based fallback\n                m = np.median(np.vstack(sel_x), axis=0)\n\n            # Adaptation: per-dim scale s updated using median absolute deviation of selected steps\n            if len(sel_steps) >= 1:\n                Smat = np.vstack(sel_steps)\n                med = np.median(Smat, axis=0)\n                mad = np.median(np.abs(Smat - med[None, :]), axis=0)\n                est_std = 1.4826 * (mad + 1e-12)\n                # blend with previous scales (exponential smoothing)\n                alpha = 0.18\n                s = np.sqrt((1 - alpha) * (s ** 2) + alpha * (est_std ** 2))\n                # clip scales between tiny and full range\n                s = np.clip(s, 1e-8 * global_range, 2.0 * global_range)\n\n            # Update trust radius TR: encourage when several candidates beat center\n            wins = np.sum(np.array(sel_F) < f_center - 1e-12)\n            if wins >= max(1, mu // 2) or accepted:\n                TR = TR * 0.92  # become more precise\n            else:\n                TR = TR * 1.06  # expand to explore more\n            TR = np.clip(TR, 1e-8, 2.0 * global_range)\n\n            # Update subspace bandit scores: reward subspaces that contained improving candidates\n            for i_local, sidx in enumerate(sel_subs):\n                improvement = max(0.0, (f_center - sel_F[i_local]))  # positive if sel better than current center before update\n                # incremental multiplicative update with decay\n                sub_scores[sidx] = 0.9 * sub_scores[sidx] + 0.1 * (1.0 + 5.0 * (improvement / (abs(f_center) + 1e-9)))\n            # small global decay to avoid locking forever\n            sub_scores = sub_scores * 0.997 + 0.001\n\n            # Periodic local diagonal quadratic fit on top-dims (if we have enough archive)\n            gen += 1\n            if (gen % max(6, int(4 + n // 10)) == 0) and len(X_archive) >= (n + 6):\n                # pick top dims with largest s (largest uncertainty)\n                top_k = min(n, max(2, int(max(2, n // 6))))\n                top_dims = np.argsort(s)[-top_k:]\n                # collect recent points and their f differences, form linear system to fit a + b^T dx + 0.5 * dx^T H diag dx\n                recent = min(len(X_archive), 4 * (top_k + 2))\n                Xr = np.array(X_archive[-recent:])\n                Fr = np.array(F_archive[-recent:])\n                # build features: for each sample, dx = x - m restricted to top_dims\n                DX = Xr[:, top_dims] - m[None, top_dims]  # recent x k\n                # design matrix columns: 1, dx_j (k), dx_j^2 (k) -> we fit diagonal Hessian only\n                A = np.ones((DX.shape[0], 1 + top_k + top_k))\n                A[:, 1:1+top_k] = DX\n                A[:, 1+top_k:] = 0.5 * (DX ** 2)\n                # solve least squares for coefficients\n                try:\n                    coeffs, *_ = np.linalg.lstsq(A, Fr, rcond=None)\n                    # extract linear and diag coefficients\n                    b_hat = coeffs[1:1+top_k]\n                    h_diag = coeffs[1+top_k:]\n                    # propose step: approximate Newton step in that subspace: dx = - b / (h + eps)\n                    eps = 1e-6 * global_range\n                    dx_prop = - b_hat / (h_diag + eps)\n                    # limit propose magnitude\n                    max_prop = TR * 0.8\n                    norm_dx = np.linalg.norm(dx_prop)\n                    if norm_dx > max_prop and norm_dx > 0:\n                        dx_prop = dx_prop * (max_prop / norm_dx)\n                    probe = np.clip(m.copy(), lb, ub)\n                    probe[top_dims] = np.clip(probe[top_dims] + dx_prop, lb[top_dims], ub[top_dims])\n                    if evals < budget:\n                        fp = func(probe.copy()); evals += 1\n                        push_archive(probe, fp)\n                        if fp < f_opt:\n                            f_opt = float(fp); x_opt = probe.copy(); last_improv_eval = evals\n                        # if this refines center, accept it\n                        if fp < f_center:\n                            m = probe.copy(); f_center = float(fp)\n                except Exception:\n                    # regression failed -> skip\n                    pass\n\n            # stagnation handling: if no improvement for long, reshuffle subspaces and do a random restart step\n            if (evals - last_improv_eval) > max(40, 4 * n):\n                stagn += 1\n                # randomly rebuild subspaces with some probability\n                if np.random.rand() < 0.7:\n                    subspaces = self._make_subspaces()\n                    sub_scores = np.ones(len(subspaces))\n                # modest reheating\n                TR = min(2.0 * global_range, TR * (1.0 + 0.6 * (1.0 - np.exp(-0.2 * stagn))))\n                # a global random perturbation around the best-known point occasionally\n                if x_opt is not None and evals < budget:\n                    perturb = (np.random.randn(n) * (0.5 * global_range))\n                    probe = np.clip(x_opt + perturb, lb, ub)\n                    fp = func(probe.copy()); evals += 1\n                    push_archive(probe, fp)\n                    if fp < f_opt:\n                        f_opt = float(fp); x_opt = probe.copy(); last_improv_eval = evals\n                last_improv_eval = evals\n\n            # keep center in bounds\n            m = np.clip(m, lb, ub)\n\n            # ensure we never exceed budget (loop guard)\n            if evals >= budget:\n                break\n\n        # final return best observed\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HiSAS scored 0.192 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "534d32c2-34bb-4886-b9b7-d54d66e1e415", "operator": null, "metadata": {"aucs": [0.11804653455125391, 0.18668810921970047, 0.2436346208778457, 0.13068859328409288, 0.21325374537463027, 0.23813192780392145, 0.2455499406507291, 0.1979519984816086, 0.20377962645666792, 0.1419813563768637]}, "task_prompt": ""}
{"id": "abb72e1f-378d-4a5e-bd46-639dbea7a617", "fitness": 0.2892041937027504, "name": "ABDES", "description": "ABDES combines a modest population (lambda ~ 3*sqrt(dim)) with a contiguous block-wise decomposition (fewer-than-sqrt(dim) blocks that can be dynamically reshuffled/merged) to enable both global and focused block moves. It uses per-coordinate adaptive scales s (initialized from coordinate spans and updated via MAD with beta_mad=0.12) plus a learned principal search direction v from a sliding window PCA of recent successful steps, blending PCA into v to bias proposals. The sampler mixes Gaussian local steps, occasional heavy-tailed t‑student jumps (p_tstudent=0.15, df=1), DE-style differential mutations (p_de=0.20), blockwise shuffles/perturbations and directed 1D probes, while selection uses rank-weighted trimmed recombination and a Metropolis-like acceptance test. Global step-size sigma is multiplicatively adapted toward a target success rate (target_success=0.25) and the algorithm applies reheating, archive-driven center mutations and block restructuring on stagnation to escape local optima.", "code": "import numpy as np\n\nclass ABDES:\n    \"\"\"\n    Adaptive Blockwise Directional Ensemble Search (ABDES)\n\n    Overview of novel choices vs. the provided SBPAS:\n    - population size scales ~ 3*sqrt(dim) (different from lam ~ pop_factor*log(dim))\n    - blocks count chosen ~ max(1, ceil(sqrt(dim)/1.5)), with dynamic block resizing on stagnation\n    - per-coordinate scales s updated with a different MAD blending constant (beta)\n    - directional learning via PCA on a sliding window of recent successful steps (instead of Oja-only)\n    - sigma adapted multiplicatively using an exponential rule based on deviation from target success rate\n    - heavier DE usage and slightly higher heavy-tail probing probability\n    - different reheating/shuffle magnitudes and dynamic restarts on stagnation\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 pop_multiplier=3.0,\n                 p_tstudent=0.15, df_t=1.0,\n                 p_de=0.20, F_de=0.7,\n                 p_blockswap=0.07,\n                 alpha_dir=0.4,\n                 target_success=0.25):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population policy: ~ 3 * sqrt(dim) (ensures modest but dimension-aware population)\n        self.lambda_ = max(6, int(np.round(pop_multiplier * np.sqrt(max(1, self.dim)))))\n\n        # blocks: slightly fewer than sqrt(dim) to encourage larger block moves initially\n        self.n_blocks = max(1, int(np.ceil(max(1.0, np.sqrt(self.dim) / 1.5))))\n        # create roughly balanced contiguous blocks\n        sizes = [self.dim // self.n_blocks] * self.n_blocks\n        for i in range(self.dim % self.n_blocks):\n            sizes[i] += 1\n        idx = 0\n        self.blocks = []\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n        # mixture hyperparams (different from SBPAS)\n        self.p_tstudent = float(p_tstudent)   # heavier tails more often\n        self.df_t = float(df_t)               # df=1 -> Cauchy-like heavy tails\n        self.p_de = float(p_de)\n        self.F_de = float(F_de)\n        self.p_blockswap = float(p_blockswap)\n        self.alpha_dir = float(alpha_dir)\n        self.target_success = float(target_success)\n\n        # PCA window for directional learning (different mechanism)\n        self.pca_window = max(8, min(80, 6 * self.dim))  # store up to this many successful steps\n        self.success_steps = []   # recent successful step vectors (y scaled by sigma)\n        # blending rate for v update toward PCA direction\n        self.v_blend = 0.18\n\n        # MAD blending for per-coordinate scales\n        self.beta_mad = 0.12  # different blend term (more weight to new MAD)\n\n        # stagnation heuristics\n        self.stagn_base_limit = max(12, int(4 * self.dim / max(1, self.n_blocks)))\n        self.max_archive = 1000\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n        rng = self.rng\n\n        # bounds (BBOB-like default -5..5)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        span = ub - lb\n        mean_span = float(np.mean(span))\n\n        # initialize center uniformly\n        m = rng.uniform(lb, ub)\n        # initial global sigma (slightly different constant)\n        sigma = 0.16 * mean_span\n        # per-coordinate scales start proportional to coordinate ranges\n        s = 0.12 * span + 1e-12\n\n        # principal search direction (unit)\n        v = rng.randn(n)\n        v /= (np.linalg.norm(v) + 1e-20)\n\n        # archive for DE differences (store recent points and objective values)\n        archive_X = []\n        archive_F = []\n\n        # bookkeeping\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation\n        xm0 = np.clip(m, lb, ub)\n        f0 = func(xm0)\n        evals += 1\n        archive_X.append(xm0.copy()); archive_F.append(f0)\n        if f0 < f_opt:\n            f_opt = f0; x_opt = xm0.copy()\n        f_center = f0\n        last_improv_eval = evals\n\n        gen = 0\n        stagn_count = 0\n        stagn_limit = self.stagn_base_limit\n\n        # PCA update frequency\n        pca_freq = max(3, int(4 + n // 10))\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            # sample normals matrix\n            Z = rng.randn(lam, n)\n\n            X = np.zeros((lam, n))\n            Ys = np.zeros((lam, n))   # underlying y (pre-multiplied by sigma)\n            Fvals = np.full(lam, np.inf)\n\n            for i in range(lam):\n                z = Z[i].copy()\n                # base per-coordinate scaled step\n                y_local = s * z\n                # directional bias along learned v\n                dir_scalar = rng.randn() * self.alpha_dir\n                y = y_local + dir_scalar * v * np.mean(np.abs(s))\n\n                # occasional heavy-tailed jump (Cauchy-like if df=1)\n                if rng.rand() < self.p_tstudent:\n                    t = rng.standard_t(self.df_t)\n                    # use heavier scale to allow escapes occasionally\n                    y = v * t * (np.mean(s) * 1.2)\n\n                # candidate\n                x = m + sigma * y\n\n                # DE-style difference occasionally (from archive)\n                if (rng.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = rng.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # Block-swap / blockwise shuffle\n                if (rng.rand() < self.p_blockswap) and (len(self.blocks) > 0):\n                    bidx = rng.randint(len(self.blocks))\n                    b = self.blocks[bidx]\n                    if len(b) > 1:\n                        perm = rng.permutation(len(b))\n                        xb = x[b].copy()\n                        x[b] = xb[perm]\n\n                # blockwise focused perturbation: pick 1-2 blocks and apply stronger noise\n                if rng.rand() < 0.11 and len(self.blocks) > 0:\n                    nb = rng.randint(1, min(2, len(self.blocks)) + 1)\n                    chosen = rng.choice(len(self.blocks), size=nb, replace=False)\n                    for cb in chosen:\n                        idxs = self.blocks[cb]\n                        # inject extra noise scaled to coordinate scales\n                        extra = (0.8 + rng.rand() * 0.8) * (s[idxs] * rng.randn(len(idxs)))\n                        x[idxs] = x[idxs] + sigma * extra\n\n                # clip and store\n                x = np.clip(x, lb, ub)\n                X[i] = x\n                Ys[i] = y\n\n            # Evaluate candidates (stop if out of budget)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi; x_opt = xi.copy(); last_improv_eval = evals\n\n            # selection: choose top mu (here stronger selection: top half)\n            valid_idx = np.argsort(Fvals)\n            mu = max(1, lam // 2)\n            sel = valid_idx[:mu]\n            X_sel = X[sel]\n            Y_sel = Ys[sel]   # corresponding y steps\n\n            # robust recombination: rank-weighted trimmed mean of selected candidates\n            ranks = np.argsort(np.argsort(Fvals[sel]))\n            # weights: exponential decay by rank\n            w = np.exp(-0.8 * ranks.astype(float))\n            w = w / (np.sum(w) + 1e-20)\n            m_new = np.sum(X_sel * w[:, None], axis=0)\n\n            # acceptance: evaluate median candidate or m_new if budget permits\n            accept_mean = False\n            if evals < budget:\n                xm_clip = np.clip(m_new, lb, ub)\n                fm_new = func(xm_clip)\n                evals += 1\n                archive_X.append(xm_clip.copy()); archive_F.append(fm_new)\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0); archive_F.pop(0)\n                delta = fm_new - f_center\n                # temperature uses sigma (larger sigma => more exploration)\n                temp = max(1e-12, sigma)\n                if delta <= 0:\n                    accept_mean = True\n                    f_center = fm_new\n                    m = xm_clip.copy()\n                else:\n                    p = np.exp(-delta / temp)\n                    if rng.rand() < p:\n                        accept_mean = True\n                        f_center = fm_new\n                        m = xm_clip.copy()\n                    else:\n                        # small conservative step toward m_new with small probability\n                        if rng.rand() < 0.08:\n                            m = np.clip(0.90 * m + 0.10 * m_new, lb, ub)\n            else:\n                # fallback if no evals left to test center\n                m = np.clip(m_new, lb, ub)\n\n            # update per-coordinate scales using MAD with a different blending beta\n            if Y_sel.shape[0] >= 1:\n                med = np.median(Y_sel, axis=0)\n                mad = np.median(np.abs(Y_sel - med[None, :]), axis=0)\n                est_std = 1.4826 * (mad + 1e-20)\n                beta = self.beta_mad\n                s = np.sqrt((1.0 - beta) * (s ** 2) + beta * (est_std ** 2))\n                s = np.clip(s, 1e-8, 1e2)\n\n            # update PCA-based direction v using sliding window of successful steps\n            # consider successes as those selected (Y_sel) whose X_sel improved archive best or improved center\n            # store normalized step vectors (unitized) scaled by their L2\n            for j in range(Y_sel.shape[0]):\n                # store y scaled by sigma to keep scale info\n                yj = Y_sel[j] * 1.0\n                # add if it led to a better-than-center candidate (heuristic)\n                # Here use the corresponding Fvals for selection (safe indexing)\n                # Use jth selected index\n                # We'll add all selected steps but clip magnitude influence\n                normy = np.linalg.norm(yj) + 1e-20\n                ynormed = (yj / normy) * min(4.0, normy)\n                self.success_steps.append(ynormed)\n            # keep window size\n            if len(self.success_steps) > self.pca_window:\n                excess = len(self.success_steps) - self.pca_window\n                if excess > 0:\n                    self.success_steps = self.success_steps[excess:]\n\n            # periodic PCA update\n            if (gen % pca_freq == 0) and (len(self.success_steps) >= 3):\n                M = np.vstack(self.success_steps)\n                # center rows\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                # compute top singular vector robustly (economy SVD)\n                try:\n                    u, svals, vt = np.linalg.svd(Mc, full_matrices=False)\n                    pca_v = vt[0, :]\n                    pca_v = pca_v / (np.linalg.norm(pca_v) + 1e-20)\n                    # blend toward PCA vector\n                    blend = self.v_blend / (1.0 + 0.02 * gen)\n                    v = (1.0 - blend) * v + blend * pca_v\n                    v /= (np.linalg.norm(v) + 1e-20)\n                except Exception:\n                    # fallback: small random perturbation\n                    v = v + 0.03 * rng.randn(n)\n                    v /= (np.linalg.norm(v) + 1e-20)\n\n            # sigma adaptation: exponential multiplicative update based on success fraction\n            if lam > 0:\n                successes = np.sum(Fvals < (f_center - 1e-12))\n                psucc = successes / lam\n                # multiplicative update: sigma <- sigma * exp(k*(psucc - target))\n                k = 0.45  # sensitivity constant different from SBPAS\n                sigma *= np.exp(k * (psucc - self.target_success))\n                # small decay for stability\n                sigma *= 0.9993\n            sigma = np.clip(sigma, 1e-12, 10.0 * mean_span)\n\n            # stagnation and reheating\n            gen += 1\n            if (evals - last_improv_eval) > stagn_limit:\n                stagn_count += 1\n                # strong reheating: enlarge sigma and reshuffle blocks\n                sigma *= (1.8 + 0.35 * stagn_count)\n                # randomly mutate center gently toward best-known or random archive point\n                if len(archive_X) > 0 and rng.rand() < 0.6:\n                    idx = rng.randint(len(archive_X))\n                    m = np.clip(0.6 * m + 0.4 * archive_X[idx], lb, ub)\n                else:\n                    m = np.clip(m + 0.08 * sigma * rng.randn(n), lb, ub)\n                # dynamic block adjustments: randomly merge/split a block\n                if len(self.blocks) > 1 and rng.rand() < 0.7:\n                    # randomly pick two blocks and swap some coordinates\n                    for _ in range(min(4, len(self.blocks))):\n                        b1, b2 = rng.choice(len(self.blocks), size=2, replace=False)\n                        if len(self.blocks[b1]) > 0 and len(self.blocks[b2]) > 0:\n                            i1 = rng.choice(self.blocks[b1])\n                            i2 = rng.choice(self.blocks[b2])\n                            # swap membership\n                            self.blocks[b1].remove(i1); self.blocks[b1].append(i2)\n                            self.blocks[b2].remove(i2); self.blocks[b2].append(i1)\n                # perturb direction more strongly\n                v = v + 0.12 * rng.randn(n)\n                v /= (np.linalg.norm(v) + 1e-20)\n                last_improv_eval = evals\n                # expand stagn_limit a bit to avoid immediate repeated reheats\n                stagn_limit = min(5 * self.stagn_base_limit, stagn_limit + 6)\n\n            # occasional directed 1D probes along v and random block axes\n            if (gen % max(6, int(3 + n // 8)) == 0) and (evals < budget):\n                alphas = [0.9, 0.5, -0.5, -0.9]\n                for alpha in alphas:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + sigma * alpha * v * np.mean(s), lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if len(archive_X) > self.max_archive:\n                        archive_X.pop(0); archive_F.pop(0)\n                    if fp < f_opt:\n                        f_opt = fp; x_opt = probe.copy(); last_improv_eval = evals\n\n            # ensure center bounds\n            m = np.clip(m, lb, ub)\n\n            # break if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ABDES scored 0.289 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "534d32c2-34bb-4886-b9b7-d54d66e1e415", "operator": null, "metadata": {"aucs": [0.11480762188198013, 0.15983277936070128, 0.29749423070354997, 0.9539332176532231, 0.22456598856127252, 0.26286364818994723, 0.24005797359719172, 0.2171867385214813, 0.26451829886565126, 0.1567814396925058]}, "task_prompt": ""}
{"id": "595a468f-de25-42dd-900c-96969989bf23", "fitness": 0.5441444934460077, "name": "HASCE", "description": "HASCE is a hybrid, CMA-like continuous optimizer that combines a small adaptive population (lambda, mu, log-weights, mu_eff) with a cheap covariance approximation split into a per-coordinate diagonal scale D and a learned low-rank subspace U (rank k ≈ √n) so the algorithm gets both coordinate-wise adaptation and targeted subspace search. Global step-size sigma is adapted by a path-length update (ps) using an approximate inv-C (1/D) together with a smoothed success-rate nudging (1/5-style) and damping, while D is updated by an EMA of weighted second moments of selected steps to capture per-coordinate variances. Exploration mixes mirrored Gaussian sampling scaled by D and U, occasional Cauchy jumps and DE-style differences from an archive for heavy-tailed/global moves, and an Oja-like online principal direction v used for cheap 1-D probes (line searches) to accelerate local descent. Robustness comes from budget-aware evaluations, clipping to bounds, an archive of evaluated points, periodic SVD-based updates of U with gentle blending/orthonormalization, and stagnation detection that reheats sigma, nudges the mean toward archived good points, and perturbs U/v to escape traps.", "code": "import numpy as np\n\nclass HASCE:\n    \"\"\"\n    Hybrid Adaptive Subspace-Covariance Evolution (HASCE)\n\n    Key ideas:\n    - Global sigma adapted by path-length (CMA-like) and smoothed success-rate.\n    - Fast per-coordinate diagonal scaling (D) via EMA of second moments.\n    - Low-rank subspace U (k ~ sqrt(n)) learned from recent successful steps via SVD and gently updated.\n    - Mirrored sampling for variance reduction, occasional Cauchy jumps and DE-style archive differences.\n    - Oja-style online principal direction v for quick probes and line-searches.\n    - Stagnation detection with reheating, archive nudges, and light perturbations to subspace.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, pop_factor=3.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population sizing like CMA-ish but small\n        self.lambda_ = max(4, int(np.floor(pop_factor + 3.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n        # subspace rank\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # read bounds (fall back to -5..5)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # strategy parameters\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length constants (CMA-like)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n\n        # initial state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.18 * np.mean(ub - lb)\n        # diagonal scales (std proxies)\n        D = np.ones(n)\n        # low-rank subspace U (n x k) orthonormal\n        rand_mat = np.random.randn(n, self.k)\n        try:\n            U, _ = np.linalg.qr(rand_mat)\n            U = U[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = np.zeros((n, self.k))\n        # principal direction v (unit)\n        v = np.random.randn(n)\n        v /= (np.linalg.norm(v) + 1e-20)\n\n        # evolution paths\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n\n        # buffers and archives\n        success_buffer = []  # for subspace learning (store y_w)\n        buffer_max = max(10 * self.k, 20)\n        archive_X = []\n        archive_F = []\n\n        # hyperparams\n        p_cauchy = 0.12\n        p_de = 0.18\n        F_de = 0.7\n        mirrored = True\n\n        # sigma smoothing with success-rate\n        success_smoothed = 0.0\n        alpha_smooth = 0.2\n        target_success = 0.2\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation\n        xm = np.clip(m, lb, ub)\n        f = func(xm)\n        evals += 1\n        archive_X.append(xm.copy()); archive_F.append(f)\n        f_opt = f; x_opt = xm.copy()\n        f_center = f\n        last_improv_eval = evals\n\n        gen = 0\n        stagn_count = 0\n        stagn_thresh = max(5, int(5 * n / max(1, self.k)))\n\n        # small numerical eps\n        EPS = 1e-12\n\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            lam_eff = current_lambda\n\n            # sample standard normals\n            Z = np.random.randn(current_lambda, n)\n            # prepare containers\n            X = np.empty((current_lambda, n))\n            Ys = np.empty((current_lambda, n))  # stored y=(x-m)/sigma\n            Fvals = np.full(current_lambda, np.inf)\n\n            for i in range(current_lambda):\n                z = Z[i].copy()\n                if mirrored and (i % 2 == 1):\n                    z = -z\n\n                # low-rank component\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U.dot(z_low)\n                    beta = 0.9 * np.mean(D)  # weight for low-rank\n                    y = D * z + beta * low\n                else:\n                    y = D * z\n\n                # occasional heavy-tailed jump (Cauchy) along learned v\n                if np.random.rand() < p_cauchy:\n                    r = np.random.standard_cauchy()\n                    # project z to unit and scale by mean(D)\n                    nz = np.linalg.norm(z) + EPS\n                    y = (z / nz) * r * np.mean(D)\n\n                x = m + sigma * y\n\n                # occasional DE-style archive difference mutation\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                X[i] = x\n                Ys[i] = y\n\n            # Evaluate candidates (budget-aware)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi; x_opt = xi.copy(); last_improv_eval = evals\n\n            # selection & recombination\n            idx = np.argsort(Fvals)\n            sel = idx[:mu]\n            X_sel = X[sel]\n            Y_sel = Ys[sel]\n            # weighted recombination (log-weights)\n            # if mu==1 weights shape must match; ensure safe broadcasting\n            w = weights.copy()\n            if w.shape[0] != len(sel):\n                # adapt weights if mu differs due to small lambda\n                mu_eff_local = max(1, len(sel))\n                w = np.log(mu_eff_local + 0.5) - np.log(np.arange(1, mu_eff_local + 1))\n                w = w / np.sum(w)\n            m_old = m.copy()\n            m = np.sum(w[:, None] * X_sel, axis=0)\n\n            # compute weighted mean step in y-space\n            y_w = np.sum(w[:, None] * Y_sel, axis=0)\n\n            # path-length update (approximate invsqrtC by 1/D)\n            invdiag = 1.0 / (D + EPS)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n            # heuristic hsig based on norm_ps (similar to CMA)\n            hsig = 1.0 if (norm_ps / np.sqrt(1.0 - (1.0 - cs) ** (2.0 * (gen + 1))) / chi_n) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # sigma update: combine path-length and smoothed success-rate (1/5-style)\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            # compute generation success fraction relative to previous center\n            gen_successes = np.sum(Fvals < (f_center - 1e-12))\n            frac = gen_successes / max(1, lam_eff)\n            success_smoothed = (1 - alpha_smooth) * success_smoothed + alpha_smooth * frac\n            # nudging factor from success smoothed towards target_success\n            if success_smoothed > target_success:\n                sigma *= 0.98\n            elif success_smoothed < 0.5 * target_success:\n                sigma *= 1.03\n\n            # clip sigma\n            sigma = np.clip(sigma, 1e-12, 5.0 * np.mean(ub - lb) + 1e-12)\n\n            # update diagonal scales D via EMA of weighted second moments of selected Y\n            y2 = np.sum(w[:, None] * (Y_sel ** 2), axis=0)\n            c_d = 0.22\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + EPS)\n            D = np.sqrt(D2)\n            D = np.clip(D, 1e-8, 1e3)\n\n            # update low-rank subspace U from success buffer periodically\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buffer_max:\n                success_buffer.pop(0)\n            # update every few generations if buffer has enough points\n            if (len(success_buffer) >= max(3, self.k)) and (gen % 3 == 0):\n                Ymat = np.vstack(success_buffer).T  # n x m\n                Ymat = Ymat - np.mean(Ymat, axis=1, keepdims=True)\n                try:\n                    U_new, svals, Vt = np.linalg.svd(Ymat, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        # gently rotate old U towards new top-k subspace\n                        Utop = U_new[:, :k_take]\n                        # simple convex blend to avoid abrupt switches\n                        blend = 0.6\n                        if U.shape[1] != k_take:\n                            # reinitialize if shape mismatch\n                            rand_mat = np.random.randn(n, self.k)\n                            U, _ = np.linalg.qr(rand_mat)\n                        # align via QR of concatenation and re-orthonormalize\n                        M = np.hstack([U[:, :k_take], Utop])\n                        Q, _ = np.linalg.qr(M)\n                        U = (1 - blend) * U + blend * np.hstack([Utop, np.zeros((n, max(0, self.k - k_take)))])\n                        # orthonormalize final U\n                        Qr, _ = np.linalg.qr(U)\n                        U = Qr[:, :self.k]\n                except np.linalg.LinAlgError:\n                    # skip update on SVD failure\n                    pass\n\n            # update principal v via Oja batch from selected Y_sel\n            if Y_sel.shape[0] >= 1:\n                eta = 0.06 / (1.0 + 0.02 * gen)\n                # covariance-vector approximation\n                Yv = Y_sel.dot(v)\n                cov_v = (Y_sel.T.dot(Yv)) / max(1, Y_sel.shape[0])\n                v = v + eta * cov_v\n                v_norm = np.linalg.norm(v) + EPS\n                v = v / v_norm\n\n            # occasionally perform a short 1D probe along v and along first U vector\n            if (gen % max(7, int(3 + n // 8)) == 0) and (evals < budget):\n                probes = [0.6, 0.3, -0.3, -0.6]\n                directions = [v]\n                if self.k > 0:\n                    directions.append(U[:, 0])\n                for d in directions:\n                    for alpha in probes:\n                        if evals >= budget:\n                            break\n                        probe = np.clip(m + sigma * alpha * d * np.mean(D), lb, ub)\n                        fp = func(probe)\n                        evals += 1\n                        archive_X.append(probe.copy()); archive_F.append(fp)\n                        if len(archive_X) > 5000:\n                            archive_X.pop(0); archive_F.pop(0)\n                        if fp < f_opt:\n                            f_opt = fp; x_opt = probe.copy(); last_improv_eval = evals\n                    if evals >= budget:\n                        break\n\n            # stagnation detection: if no improvement for long, reheat and nudge towards archive\n            if (evals - last_improv_eval) > stagn_thresh * max(1, lam):\n                stagn_count += 1\n                sigma *= min(4.0, 1.6 + 0.2 * stagn_count)\n                # nudge mean towards a random archived good point if available\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                # perturb U and v a little\n                if self.k > 0:\n                    U = U + 0.06 * np.random.randn(*U.shape)\n                    Qr, _ = np.linalg.qr(U)\n                    U = Qr[:, :self.k]\n                v = v + 0.08 * np.random.randn(n)\n                v /= (np.linalg.norm(v) + EPS)\n                success_buffer = []\n                last_improv_eval = evals\n\n            # optional small center evaluation update (budget allowing): keep f_center up to date every few gens\n            if (gen % 5 == 0) and (evals < budget):\n                xm_clip = np.clip(m, lb, ub)\n                fm = func(xm_clip)\n                evals += 1\n                archive_X.append(xm_clip.copy()); archive_F.append(fm)\n                if fm < f_opt:\n                    f_opt = fm; x_opt = xm_clip.copy(); last_improv_eval = evals\n                f_center = min(f_center, fm)\n\n            gen += 1\n\n            # ensure mean stays in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HASCE scored 0.544 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "534d32c2-34bb-4886-b9b7-d54d66e1e415", "operator": null, "metadata": {"aucs": [0.1669176699405871, 0.15267495121296193, 0.5989900161108792, 0.2644295512713213, 0.827293868247621, 0.825053702876693, 0.31827098972739676, 0.5891711718285214, 0.9399416485291059, 0.7587013647149897]}, "task_prompt": ""}
{"id": "ed215bb4-b87a-41f0-b503-9d7194117cc3", "fitness": "-inf", "name": "ECDL", "description": "1) The algorithm runs an ensemble of local trust-region centers (ensemble size ~ max(2,min(8,dim//2+1))) initialized from a small space-filling archive (init_ratio=0.12, min_init ~ 10*dim, capped) and maintains per-center state: per-dimension trust radii, local covariance C (with eigen factors B/D), a step cumulation path ps and a sigma scale (initial sigma ≈ 0.25·mean(trust_radius)).  \n2) Candidates are proposed from lightweight surrogates (weighted separable quadratic and linear fits using nearby archive points) and from sampling the local covariance (mirrored normal sampling through B·D), then augmented by self-adaptive DE-style differences (p_de=0.3, adaptive F/CR) and occasional Mantegna–Lévy jumps (p_levy=0.12), all clipped to bounds and trust radii and evaluated under a per-iteration eval cap (max_eval_per_iter).  \n3) Success-driven adaptation promotes candidates to centers, expands trust radii (×~1.4–1.6), performs a rank-one covariance update with weight 0.2, and updates sigma via a CMA-like cumulation rule (parameters cs, damps, chi_n), while failures shrink trust and reduce sigma; eigen factors are recomputed periodically (~10·n steps) for stable sampling.  \n4) An archive of evaluated points is maintained and pruned (max_archive ≈ max(2000,50·n)), used to refresh centers and neighbors for surrogates, and multiple safeguards (ridge regularization in fits, clipping, small-eigenvalue floors) keep updates numerically stable.", "code": "import numpy as np\n\nclass ECDL:\n    \"\"\"\n    ECDL: Ensemble Covariance + DE + Lévy Trust\n    One-line: Ensemble trust-region search where each center keeps a light adaptive covariance\n    with cumulation-based sigma control, proposes surrogate minimizers and covariance-sampled\n    candidates (mirrored), augments proposals with self-adaptive DE diffs and occasional\n    Mantegna–Lévy jumps, and updates per-center covariance by rank-one updates from successful steps.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 ensemble_size=None,\n                 init_ratio=0.12,\n                 min_init_scale=2,\n                 max_init_frac=0.4,\n                 max_eval_per_iter=60,\n                 p_de=0.3,\n                 p_levy=0.12,\n                 mirror=True):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        # ensemble sizing\n        if ensemble_size is None:\n            self.ensemble_size = max(2, min(8, self.dim // 2 + 1))\n        else:\n            self.ensemble_size = int(ensemble_size)\n        # initialization sizing\n        self.init_ratio = float(init_ratio)\n        self.min_init = max(10, min_init_scale * self.dim)\n        self.max_init = min(400, int(max_init_frac * self.budget))\n        self.max_eval_per_iter = int(max_eval_per_iter)\n        # mutation/exploration controls\n        self.p_de = float(p_de)\n        self.p_levy = float(p_levy)\n        self.mirror = bool(mirror)\n\n    # Mantegna Levy generator (alpha in (0,2])\n    def _levy_mantegna(self, n, alpha=1.5, scale=1.0):\n        sigma_u = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                   (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n        u = self.rng.normal(0, sigma_u, size=n)\n        v = self.rng.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1.0 / alpha))\n        return scale * step\n\n    def __call__(self, func):\n        n = int(self.dim)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain = np.maximum(1e-12, ub - lb)\n        mean_range = float(np.mean(domain))\n\n        budget = int(self.budget)\n        evals = 0\n\n        # Archive\n        X = []\n        F = []\n\n        # initial sampling (space-filling-ish)\n        init_budget = int(np.clip(self.init_ratio * budget, self.min_init, self.max_init))\n        init_budget = max(1, init_budget)\n        for _ in range(init_budget):\n            x = self.rng.uniform(lb, ub)\n            f = func(x)\n            evals += 1\n            X.append(x.copy()); F.append(float(f))\n            if evals >= budget:\n                return float(min(F)), np.array(X[int(np.argmin(F))], dtype=float)\n\n        # initialize centers from best archive points\n        def get_top_centers(k):\n            if len(X) == 0:\n                return []\n            idx_sorted = np.argsort(F)\n            centers = []\n            for idx in idx_sorted:\n                x = np.array(X[idx])\n                if all(np.linalg.norm(x - c) > 1e-10 for c in centers):\n                    centers.append(x)\n                if len(centers) >= k:\n                    break\n            return centers\n\n        centers = get_top_centers(min(self.ensemble_size, len(X)))\n        if len(centers) == 0:\n            centers = [np.array(self.rng.uniform(lb, ub))]\n\n        # per-center state: trust radii (vector), covariance (n x n light), sigma (scalar), cumulation paths\n        centers = [np.array(c, dtype=float) for c in centers]\n        k_centers = len(centers)\n        trust_radius = [0.5 * domain.copy() for _ in centers]  # per-dim trust radius\n        # initialize local covariances as small isotropic matrices scaled by trust radius\n        Cs = [np.eye(n) * (max(1e-8, np.mean(trust_radius[i]) ** 2 / max(1.0, n))) for i in range(k_centers)]\n        Bs = [np.eye(n) for _ in centers]\n        Ds = [np.ones(n) for _ in centers]\n        invsqrtC = [np.eye(n) for _ in centers]\n        eig_counters = [0 for _ in centers]\n        eig_every = max(1, int(10 * n))\n        # per-center cumulation step-paths\n        ps_list = [np.zeros(n) for _ in centers]\n        cc = 2.0 / (n + 2.0)\n        cs = 0.4 / max(1.0, np.sqrt(n))  # gentle\n        damps = 1.0 + 0.3 * cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # per-center sigma relative to trust radius\n        sigmas = [0.25 * float(np.mean(trust_radius[i])) for i in range(len(centers))]\n\n        # best so far\n        f_best = np.inf\n        x_best = None\n        # record initial best from archive\n        if len(F) > 0:\n            idx_min = int(np.argmin(F))\n            f_best = float(F[idx_min])\n            x_best = np.array(X[idx_min], dtype=float)\n\n        # safe evaluate wrapper\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x_clipped = np.clip(x, lb, ub)\n            if evals >= budget:\n                return None, None\n            try:\n                f = float(func(x_clipped))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X.append(x_clipped.copy()); F.append(f)\n            if f < f_best - 1e-12 or x_best is None:\n                f_best = float(f); x_best = x_clipped.copy()\n            return f, x_clipped\n\n        # separable quadratic fit (used like No.5)\n        def fit_separable_quad(center, X_arr, F_arr):\n            dx = X_arr - center\n            m = dx.shape[0]\n            if m < 3:\n                return None\n            M = np.ones((m, 1 + 2 * n))\n            M[:, 1:1 + n] = dx\n            M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n            y = F_arr\n            # distance weighting\n            dists = np.linalg.norm(dx, axis=1)\n            w = 1.0 / (dists + 1e-12)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            bvec = W * y\n            try:\n                ridge = 1e-6 * np.eye(M.shape[1])\n                params, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ bvec, rcond=None)\n                params = params.flatten()\n                a = params[0]\n                b_lin = params[1:1 + n]\n                h_diag = params[1 + n:1 + 2 * n]\n                h_diag[h_diag < 1e-8] = 1e-8\n                return a, b_lin, h_diag\n            except Exception:\n                return None\n\n        # main optimization loop\n        it = 0\n        while evals < budget:\n            it += 1\n            remaining = budget - evals\n            work_allow = min(self.max_eval_per_iter, remaining)\n\n            # refresh centers occasionally using archive bests\n            if (it % 6) == 0 and len(X) > 0:\n                new_centers = get_top_centers(min(self.ensemble_size, max(1, len(X)//10)))\n                # merge while keeping uniqueness\n                all_centers = centers + new_centers\n                uniq = []\n                for c in all_centers:\n                    if not any(np.linalg.norm(c - u) < 1e-8 for u in uniq):\n                        uniq.append(c)\n                    if len(uniq) >= self.ensemble_size:\n                        break\n                # reinitialize per-center lists if size changes\n                centers = [u.copy() for u in uniq]\n                k_centers = len(centers)\n                # adjust lists to match centers\n                while len(trust_radius) < k_centers:\n                    trust_radius.append(0.5 * domain.copy())\n                    Cs.append(np.eye(n) * (np.mean(domain) ** 2 / max(1.0, n)))\n                    Bs.append(np.eye(n)); Ds.append(np.ones(n)); invsqrtC.append(np.eye(n))\n                    ps_list.append(np.zeros(n)); sigmas.append(0.25 * float(np.mean(domain))); eig_counters.append(0)\n                if len(trust_radius) > k_centers:\n                    trust_radius = trust_radius[:k_centers]\n                    Cs = Cs[:k_centers]; Bs = Bs[:k_centers]; Ds = Ds[:k_centers]; invsqrtC = invsqrtC[:k_centers]\n                    ps_list = ps_list[:k_centers]; sigmas = sigmas[:k_centers]; eig_counters = eig_counters[:k_centers]\n\n            improved_any = False\n\n            # iterate centers in random order\n            order = list(range(len(centers)))\n            self.rng.shuffle(order)\n            for ci in order:\n                if evals >= budget or work_allow <= 0:\n                    break\n                center = centers[ci]\n                tr = np.maximum(trust_radius[ci], 1e-12)\n                sigma = sigmas[ci]\n                C = Cs[ci]\n                B = Bs[ci]; D = Ds[ci]; invC = invsqrtC[ci]\n                ps = ps_list[ci]\n\n                # gather neighbors\n                if len(X) >= max(2 * n + 1, 8):\n                    X_arr = np.asarray(X)\n                    F_arr = np.asarray(F)\n                    dists = np.linalg.norm(X_arr - center, axis=1)\n                    idx_sorted = np.argsort(dists)\n                    m_nei = min(len(X_arr), max(2 * n + 1, 8 * n))\n                    idx_nei = idx_sorted[:m_nei]\n                    X_nei = X_arr[idx_nei]; F_nei = F_arr[idx_nei]\n                else:\n                    X_nei = None; F_nei = None\n\n                # propose from surrogate if possible\n                proposals = []\n                if X_nei is not None and X_nei.shape[0] >= (n + 3):\n                    quad = fit_separable_quad(center, X_nei, F_nei)\n                    if quad is not None:\n                        a_q, b_q, h_q = quad\n                        delta_q = -b_q / (h_q + 1e-20)\n                        # clamp delta to trust radius per-dim\n                        delta_q = np.clip(delta_q, -tr, tr)\n                        x_q = np.clip(center + delta_q, lb, ub)\n                        proposals.append(x_q)\n                    # linear fallback\n                    try:\n                        A_lin = np.hstack([np.ones((X_nei.shape[0], 1)), X_nei - center])\n                        params_lin, *_ = np.linalg.lstsq(A_lin, F_nei, rcond=None)\n                        b_lin = params_lin[1:].flatten()\n                        step_lin = -0.5 * np.sign(b_lin) * tr\n                        x_lin = np.clip(center + step_lin, lb, ub)\n                        proposals.append(x_lin)\n                    except Exception:\n                        pass\n\n                # evaluate proposals (small number)\n                for xprop in proposals:\n                    if evals >= budget or work_allow <= 0:\n                        break\n                    out = safe_eval(xprop)\n                    work_allow -= 1\n                    if out[0] is None:\n                        break\n                    fprop, xprop = out\n                    if fprop < F[-1] - 1e-12 or fprop < f_best - 1e-12:\n                        # accept as new center location\n                        centers[ci] = xprop.copy()\n                        # update trust and sigma on success\n                        trust_radius[ci] = np.minimum(tr * 1.6, 2.0 * domain)\n                        sigmas[ci] = max(1e-12, sigma * 1.4)\n                        # update covariance rank-one with normalized step\n                        y = (xprop - center) / max(sigma, 1e-12)\n                        y = y.reshape(-1)\n                        C = (1.0 - 0.2) * C + 0.2 * np.outer(y, y)\n                        Cs[ci] = C\n                        # recompute eigen factors\n                        try:\n                            vals, vecs = np.linalg.eigh(C)\n                            vals = np.maximum(vals, 1e-20)\n                            Ds[ci] = np.sqrt(vals)\n                            Bs[ci] = vecs\n                            invsqrtC[ci] = (vecs * (1.0 / Ds[ci])) @ vecs.T\n                        except Exception:\n                            Bs[ci] = np.eye(n); Ds[ci] = np.ones(n); invsqrtC[ci] = np.eye(n)\n                        ps_list[ci] = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs)) * (invsqrtC[ci] @ (y / (np.linalg.norm(y) + 1e-20)))\n                        sigmas[ci] = max(1e-12, sigmas[ci])\n                        improved_any = True\n                        if fprop < f_best:\n                            f_best = fprop; x_best = xprop.copy()\n                    # otherwise continue\n\n                if evals >= budget or work_allow <= 0:\n                    break\n\n                # generate covariance-sampled candidates with mirrored sampling\n                num_candidates = min(8, max(4, int(np.ceil(np.mean(tr) / (1e-6 + sigma)))))  # heuristic\n                num_candidates = min(num_candidates, work_allow)\n                if num_candidates <= 0:\n                    continue\n                # construct standard normals\n                if self.mirror and (num_candidates % 2 == 0):\n                    half = num_candidates // 2\n                    z_half = self.rng.normal(size=(half, n))\n                    arz = np.vstack([z_half, -z_half])\n                else:\n                    arz = self.rng.normal(size=(num_candidates, n))\n                # map through B*D if available\n                try:\n                    BDt = (Bs[ci] * Ds[ci]).T  # (n,n)\n                    ary = arz @ BDt\n                except Exception:\n                    ary = arz\n                arx = center + sigma * ary\n\n                # augment per-candidate with self-adaptive DE and occasional Levy\n                for k in range(arx.shape[0]):\n                    xk = arx[k].copy()\n                    # DE diff mutation using archive\n                    if (self.p_de > 0) and (len(X) >= 2) and (self.rng.random() < self.p_de):\n                        # self-adaptive F and CR\n                        Fk = float(np.exp(self.rng.normal(np.log(0.8), 0.5)))\n                        CRk = float(np.clip(self.rng.beta(2.0, 2.0), 0.0, 1.0))\n                        i1, i2 = self.rng.choice(len(X), size=2, replace=False)\n                        de_vec = Fk * (X[i1] - X[i2])\n                        mask = self.rng.random(n) < CRk\n                        if not np.any(mask):\n                            mask[self.rng.integers(0, n)] = True\n                        xk[mask] = xk[mask] + de_vec[mask]\n                    # occasional Levy jump\n                    if self.rng.random() < self.p_levy:\n                        levy = self._levy_mantegna(n, alpha=1.5, scale=0.5 * sigma)\n                        xk = xk + levy\n                    arx[k] = np.clip(xk, lb, ub)\n\n                # evaluate candidates (respect work_allow)\n                arfit = np.full(arx.shape[0], np.inf)\n                for k in range(arx.shape[0]):\n                    if evals >= budget or work_allow <= 0:\n                        break\n                    out = safe_eval(arx[k])\n                    work_allow -= 1\n                    if out[0] is None:\n                        break\n                    fk, xk = out\n                    arfit[k] = fk\n                    # accept best candidate for this center if it improves center or global best\n                # Choose best candidate\n                if np.any(np.isfinite(arfit)):\n                    ik = int(np.argmin(arfit))\n                    fmin = float(arfit[ik])\n                    xmin = arx[ik].copy()\n                    if fmin < f_best - 1e-12 or fmin < F[-1] - 1e-12:\n                        # promote candidate to center\n                        prev_center = centers[ci].copy()\n                        centers[ci] = xmin.copy()\n                        improved_any = True\n                        # update trust radius and sigma\n                        trust_radius[ci] = np.minimum(tr * 1.4, 2.0 * domain)\n                        # compute y normalized for covariance update\n                        y = (xmin - prev_center) / max(sigma, 1e-12)\n                        # rank-one update\n                        C = (1.0 - 0.2) * C + 0.2 * np.outer(y, y)\n                        Cs[ci] = C\n                        # update ps and sigma via cumulation rule\n                        try:\n                            vals, vecs = np.linalg.eigh(C)\n                            vals = np.maximum(vals, 1e-20)\n                            Ds[ci] = np.sqrt(vals); Bs[ci] = vecs\n                            invsqrtC[ci] = (vecs * (1.0 / Ds[ci])) @ vecs.T\n                        except Exception:\n                            Bs[ci] = np.eye(n); Ds[ci] = np.ones(n); invsqrtC[ci] = np.eye(n)\n                        ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs)) * (invsqrtC[ci] @ (y / (np.linalg.norm(y) + 1e-20)))\n                        # sigma adaptation (cumulation + success multiplier)\n                        norm_ps = np.linalg.norm(ps)\n                        sigmas[ci] = max(1e-12, sigma * np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0)))\n                        # slight success multiplier\n                        sigmas[ci] *= 1.08\n                        ps_list[ci] = ps\n                        Cs[ci] = C\n                        if fmin < f_best:\n                            f_best = fmin; x_best = xmin.copy()\n                    else:\n                        # no candidate improved: shrink trust and maybe reduce sigma\n                        trust_radius[ci] = np.maximum(tr * 0.85, 1e-12 * domain)\n                        sigmas[ci] = max(1e-12, sigma * 0.92)\n\n                # eigen recompute occasionally per center\n                eig_counters[ci] += (arx.shape[0] if 'arx' in locals() else 1)\n                if eig_counters[ci] >= eig_every:\n                    eig_counters[ci] = 0\n                    try:\n                        vals, vecs = np.linalg.eigh(Cs[ci])\n                        vals = np.maximum(vals, 1e-20)\n                        Ds[ci] = np.sqrt(vals); Bs[ci] = vecs\n                        invsqrtC[ci] = (vecs * (1.0 / Ds[ci])) @ vecs.T\n                    except Exception:\n                        Bs[ci] = np.eye(n); Ds[ci] = np.ones(n); invsqrtC[ci] = np.eye(n)\n\n            # end centers loop\n\n            # prune archive to keep memory manageable\n            max_archive = max(2000, 50 * n)\n            if len(X) > max_archive:\n                idx_sorted = np.argsort(F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X = [X[i] for i in keep_idx]\n                F = [F[i] for i in keep_idx]\n\n            # quick termination if very good\n            if f_best <= 1e-14 or evals >= budget:\n                break\n\n        # final return\n        if x_best is None:\n            return float(np.inf), np.zeros(n)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "534d32c2-34bb-4886-b9b7-d54d66e1e415", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "47ac50c6-9e83-4e0b-bc30-14bbb6d03608", "fitness": 0.2153500717824306, "name": "MANTA", "description": "The algorithm learns a compact low-dimensional manifold B from recent successful normalized steps via a weighted SVD (lam_sub stores singular scales) and fits a small quadratic surrogate in those manifold coordinates to predict improvements and steer a trust-region radius sigma. Candidate generation is a budget-aware mixed strategy (lambda small and adaptive) that favors manifold-directed perturbations (p_manifold=0.45) but also uses isotropic (0.25), coordinate-scaled (0.15), global uniform (0.10) probes, occasional DE-style archive differences (p_de=0.16) and rare heavy-tailed Cauchy jumps (p_cauchy=0.07); top candidates are recombined by rank-based soft weights (mu survivors). Curvature-awareness comes from a cheap least-squares gradient estimate and a small L-BFGS memory to propose quasi-Newton corrections, while sigma is adapted by a trust-ratio comparing actual vs surrogate-predicted improvement. Robustness features include strict bound clipping, RMS coordinate scaling, archive and success buffers for manifold/LBFGS learning, stagnation-triggered inflation and nudges toward historical good points, and careful budget-aware probing/acceptance.", "code": "import numpy as np\n\nclass MANTA:\n    \"\"\"\n    MANTA: Manifold-Adaptive Newton-Trust-region Algorithm\n\n    Main ideas:\n    - Learn a low-dimensional manifold (basis B) from recent successful steps (incremental PCA / SVD).\n    - Generate candidates by mixing: local isotropic, manifold-directed, coordinate-scaled, global uniform,\n      and occasional Cauchy/Levy heavy tails; also occasional DE-style archive differences.\n    - Fit a small quadratic surrogate in the learned manifold (ridge regression on linear + diag quadratic terms)\n      to predict improvement and adapt a trust-region radius sigma.\n    - Use a small L-BFGS memory (two-loop) to form a quasi-Newton step in full space when gradient estimates exist.\n    - Maintain archive and a novelty buffer; trigger cautious restarts/inflation on stagnation.\n    - Budget-aware population size and strict bound clipping.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size: modest & budget-aware\n        self.lambda_ = max(6, int(6 + np.floor(3.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 3)\n\n        # subspace target dimension\n        if subspace_k is None:\n            # bias slightly higher than sqrt(dim)/1.5 to give manifold some expressivity\n            self.k = max(1, int(np.clip(np.ceil(self.dim ** 0.5 / 1.5), 1, self.dim)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # L-BFGS memory size\n        self.lbfgs_m = max(3, min(12, 2 * self.k + 2))\n\n        # mixing probabilities (different from ASTRE)\n        self.p_iso = 0.25\n        self.p_manifold = 0.45\n        self.p_coord = 0.15\n        self.p_global = 0.10  # uniform global probe\n        self.p_cauchy = 0.07  # heavy tail prob (applies on top as chance)\n        self.p_de = 0.16\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (support scalar bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center: random within bounds\n        m = np.random.uniform(lb, ub)\n        range_mean = np.mean(ub - lb)\n        sigma = 0.20 * range_mean  # trust-region radius\n        sigma_min = 1e-8\n        sigma_max = 0.5 * np.max(ub - lb) + 1e-12\n\n        # coordinate scales (RMS-like)\n        s2 = np.ones(n)\n        s = np.sqrt(s2)\n\n        # manifold basis B (n x k) initialize orthonormal random\n        if self.k > 0:\n            rand = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand)\n                B = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                B = np.zeros((n, self.k))\n        else:\n            B = np.zeros((n, 0))\n\n        # singular scales along manifold (used as anisotropic scaling)\n        lam_sub = np.ones(self.k)\n\n        # buffers\n        success_buffer = []   # recent successful steps (x - m)/sigma\n        buf_max = max(8 * self.k, 30)\n        archive_X = []\n        archive_F = []\n\n        # L-BFGS memory\n        lbfgs_s = []  # step vectors s = x_{k+1}-x_k\n        lbfgs_y = []  # gradient differences y = g_{k+1}-g_k\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial center\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = float(func(xm))\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = fm; x_best = xm.copy()\n\n        # helper: approximate gradient via weighted linear fit on buffer or archive near m\n        def approx_grad(points, vals):\n            # points: list of x (absolute), vals: list of f\n            if len(points) < 3:\n                return np.zeros(n)\n            X = np.vstack(points)\n            Y = np.array(vals)\n            # shift to current center m to get small-step linearization\n            Xc = X - m\n            # regularized least squares for g: minimize ||Xc g - (Y - mean(Y))||^2 + reg ||g||^2\n            reg = 1e-6 + 1e-3 * (np.mean(np.var(Xc, axis=0)) + 1e-12)\n            try:\n                A = Xc.T @ Xc + reg * np.eye(n)\n                b = Xc.T @ (Y - np.mean(Y))\n                g = np.linalg.solve(A, b)\n                return g\n            except np.linalg.LinAlgError:\n                return np.zeros(n)\n\n        # helper: fit simple quadratic surrogate in manifold coordinates\n        def fit_subspace_quadratic(Ysub, Fsub):\n            # Ysub: m x k (steps in manifold coords), Fsub: m (f values)\n            # We fit f ≈ a + g^T y + 0.5 * sum(h_i * y_i^2)\n            m_samp = Ysub.shape[0]\n            if m_samp < 4:\n                return None  # not enough data\n            # design matrix: [ones, Y, 0.5*Y**2]\n            G = np.hstack([np.ones((m_samp, 1)), Ysub, 0.5 * (Ysub ** 2)])\n            # ridge regression\n            reg = 1e-6 + 1e-3 * np.mean(np.var(Ysub, axis=0))\n            try:\n                coeff, *_ = np.linalg.lstsq(G.T @ G + reg * np.eye(G.shape[1]), G.T @ Fsub, rcond=None)\n                a = coeff[0]\n                g_sub = coeff[1:1 + Ysub.shape[1]]\n                h_sub = coeff[1 + Ysub.shape[1]:]\n                return a, g_sub, h_sub\n            except np.linalg.LinAlgError:\n                return None\n\n        # helper: L-BFGS two-loop for inverse-H times vector (approx)\n        def lbfgs_apply_invH(v):\n            # returns H_k v approximated by L-BFGS two-loop with scaling gamma\n            q = v.copy()\n            m_mem = len(lbfgs_s)\n            if m_mem == 0:\n                return v\n            alpha = np.zeros(m_mem)\n            rho = np.zeros(m_mem)\n            for i in range(m_mem - 1, -1, -1):\n                s = lbfgs_s[i]; y = lbfgs_y[i]\n                rho[i] = 1.0 / (np.dot(y, s) + 1e-12)\n                alpha[i] = rho[i] * np.dot(s, q)\n                q = q - alpha[i] * y\n            # scaling H0 = (s_last^T y_last)/(y_last^T y_last) * I\n            ys = np.dot(lbfgs_y[-1], lbfgs_s[-1]) if m_mem > 0 else 1.0\n            yy = np.dot(lbfgs_y[-1], lbfgs_y[-1]) if m_mem > 0 else 1.0\n            gamma = ys / (yy + 1e-12)\n            r = gamma * q\n            for i in range(m_mem):\n                s = lbfgs_s[i]; y = lbfgs_y[i]\n                beta = rho[i] * np.dot(y, r)\n                r = r + s * (alpha[i] - beta)\n            return r\n\n        # main optimization loop\n        iter_count = 0\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, max(2, remaining))\n            # adaptive temperature for selection (we'll use ranking with a mild temperature)\n            temp = 0.8 + 0.4 * (1.0 - evals / max(1, budget))  # start a bit hotter\n\n            # prepare candidate generation\n            cand_X = np.zeros((lam, n))\n            cand_meta = []  # store (delta, mode)\n            for i in range(lam):\n                r = np.random.rand()\n                # base step vector\n                if r < self.p_iso:\n                    z = np.random.randn(n)\n                    step = (sigma / np.sqrt(n)) * z\n                    mode = 'iso'\n                elif r < self.p_iso + self.p_manifold and self.k > 0:\n                    # sample in manifold coordinates\n                    zsub = np.random.randn(self.k)\n                    sub = B @ ( (np.sqrt(lam_sub + 1e-12)) * zsub )\n                    # combine with small perpendicular noise\n                    perp = np.random.randn(n) * (0.2 * (s / (np.mean(s) + 1e-12)))\n                    step = sigma * (0.8 * sub + 0.2 * perp)\n                    mode = 'manifold'\n                elif r < self.p_iso + self.p_manifold + self.p_coord:\n                    z = np.random.randn(n)\n                    step = sigma * z * (s / (np.mean(s) + 1e-12))\n                    mode = 'coord'\n                else:\n                    # global uniform probe scaled by domain\n                    u = np.random.uniform(lb, ub)\n                    step = u - m\n                    step = 0.6 * step  # modest global nudge\n                    mode = 'global'\n\n                # occasional DE-like archive difference\n                if np.random.rand() < self.p_de and len(archive_X) >= 2:\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    diff = archive_X[i1] - archive_X[i2]\n                    step = step + 0.5 * diff\n                    mode += '+de'\n\n                # heavy-tail perturbation applied on top\n                if np.random.rand() < self.p_cauchy:\n                    c = np.random.standard_cauchy()\n                    factor = np.sign(c) * (np.abs(c) ** 0.6)\n                    nx = np.linalg.norm(step) + 1e-12\n                    step = factor * (step / nx) * (sigma * 3.0)\n                    mode += '+cauchy'\n\n                x = m + step\n                x = np.clip(x, lb, ub)\n                cand_X[i] = x\n                cand_meta.append((step.copy(), mode))\n\n            # evaluate candidates sequentially (respect budget)\n            cand_F = np.full(lam, np.inf)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = cand_X[i]\n                fx = float(func(x))\n                evals += 1\n                cand_F[i] = fx\n                archive_X.append(x.copy()); archive_F.append(fx)\n                if fx < f_best:\n                    f_best = fx; x_best = x.copy()\n\n            # select top individuals by rank and compute soft weights\n            valid_idx = np.where(np.isfinite(cand_F))[0]\n            if valid_idx.size == 0:\n                break\n            sorted_idx = valid_idx[np.argsort(cand_F[valid_idx])]\n            ranks = np.arange(1, len(sorted_idx) + 1)\n            logits = - (ranks - 1) / max(1e-12, (temp * np.sqrt(n)))\n            exps = np.exp(logits - np.max(logits))\n            soft = exps / np.sum(exps)\n            take = min(self.mu, len(sorted_idx))\n            chosen = sorted_idx[:take]\n            weights = soft[:take]\n            if weights.sum() <= 0:\n                weights = np.ones_like(weights) / len(weights)\n            else:\n                weights = weights / np.sum(weights)\n\n            # recombined mean shift (delta_m)\n            chosen_steps = np.array([cand_meta[j][0] for j in chosen])\n            delta_m = np.sum(weights[:, None] * chosen_steps, axis=0)\n            m_candidate = np.clip(m + delta_m, lb, ub)\n\n            # Build small training set for subspace surrogate:\n            # choose recent points projected to manifold coordinates\n            # compute Ysub for points in archive nearest to m (in manifold coords)\n            if self.k > 0 and len(success_buffer) > 0:\n                # use last up to 30 archive points\n                take_train = min(len(archive_X), 30)\n                X_train = np.vstack(archive_X[-take_train:])\n                F_train = np.array(archive_F[-take_train:])\n                # project steps relative to current m into manifold coords\n                Ys = (X_train - m) @ B  # m x k\n                # scale by sigma to get normalized coordinates\n                Ys_norm = Ys / (sigma + 1e-12)\n                # fit surrogate\n                try:\n                    model = fit_subspace_quadratic(Ys_norm, F_train)\n                except Exception:\n                    model = None\n            else:\n                model = None\n\n            # predicted improvement from surrogate (if available) using delta projected to manifold\n            if model is not None:\n                a_sub, g_sub, h_sub = model\n                d_sub = ((m_candidate - m) @ B) / (sigma + 1e-12)  # k-vector\n                # predicted decrease ≈ - g_sub^T d_sub - 0.5 * sum(h_sub * d_sub^2)\n                pred_dec = - np.dot(g_sub, d_sub) - 0.5 * np.dot(h_sub, d_sub * d_sub)\n                predicted_improv = max(0.0, pred_dec)\n            else:\n                predicted_improv = 0.0\n\n            # approximate gradient near m from recent archive points\n            grad_hat = approx_grad(archive_X[-min(len(archive_X), 12):], archive_F[-min(len(archive_F), 12):])\n\n            # use L-BFGS quasi-Newton to propose a curvature-aware correction if gradient available\n            if np.linalg.norm(grad_hat) > 1e-8 and len(lbfgs_s) > 0:\n                qstep = - lbfgs_apply_invH(grad_hat)\n                # scale qstep to trust-region size\n                qnorm = np.linalg.norm(qstep) + 1e-12\n                qstep = qstep * (min(1.0, sigma / qnorm))\n                # combine into candidate mean with small weight\n                m_candidate = np.clip(0.7 * m_candidate + 0.3 * (m + qstep), lb, ub)\n\n            # evaluate candidate mean optionally (budget permitting) as a probe for acceptance\n            probe_f = None\n            if evals < budget:\n                xm_probe = np.clip(m_candidate, lb, ub)\n                probe_f = float(func(xm_probe))\n                evals += 1\n                archive_X.append(xm_probe.copy()); archive_F.append(probe_f)\n                if probe_f < f_best:\n                    f_best = probe_f; x_best = xm_probe.copy()\n\n            # actual improvement (taking latest m's evaluation as baseline)\n            current_mean_f = archive_F[-1] if len(archive_F) > 0 else f_best\n            mean_sel_f = np.mean(cand_F[chosen]) if len(chosen) > 0 else np.inf\n            actual_improv = max(0.0, current_mean_f - mean_sel_f) if np.isfinite(mean_sel_f) else 0.0\n\n            # trust ratio rho = actual / (predicted + eps)\n            eps = 1e-8\n            rho = actual_improv / (predicted_improv + eps) if (predicted_improv + eps) > 0 else 0.0\n\n            # adapt sigma based on rho and observed improvement\n            if predicted_improv > 1e-9 and rho > 0.9:\n                # strong success -> expand\n                sigma = min(sigma * (1.25 + 0.5 * (rho - 0.9)), sigma_max)\n            elif rho > 0.4:\n                sigma = min(sigma * 1.08, sigma_max)\n            elif rho < 0.1:\n                sigma = max(sigma * 0.65, sigma_min)\n            else:\n                sigma = max(sigma * 0.95, sigma_min)\n\n            # accept candidate mean if improves or if rho moderate\n            if probe_f is not None:\n                if probe_f <= current_mean_f or rho > 0.25:\n                    new_m = m_candidate.copy()\n                    accepted = True\n                else:\n                    # partial move towards candidate\n                    alpha = max(0.05, min(0.6, rho + 0.1))\n                    new_m = np.clip((1.0 - alpha) * m + alpha * m_candidate, lb, ub)\n                    accepted = False\n            else:\n                # no probe evaluated: accept if surrogate predicted something or small random acceptance\n                if predicted_improv > 1e-6 or np.random.rand() < 0.1:\n                    new_m = m_candidate.copy()\n                    accepted = True\n                else:\n                    new_m = m.copy()\n                    accepted = False\n\n            # update success buffer and lbfgs memory\n            delta = (new_m - m)\n            if np.linalg.norm(delta) > 1e-12:\n                # store normalized step in buffer for manifold learning\n                step_norm = delta / (sigma + 1e-12)\n                success_buffer.append(step_norm.copy())\n                if len(success_buffer) > buf_max:\n                    success_buffer.pop(0)\n\n                # update s2 / coordinate RMS using delta\n                beta = 0.18\n                y2 = delta ** 2\n                s2 = (1.0 - beta) * s2 + beta * (y2 + 1e-12)\n                s = np.sqrt(s2 + 1e-12)\n\n                # approximate gradient at new point and update L-BFGS memory\n                # estimate grad via small neighborhood points if available\n                grad_new = approx_grad(archive_X[-min(len(archive_X), 12):], archive_F[-min(len(archive_F), 12):])\n                # estimate grad_old using previous m samples if available\n                # For simplicity, use grad_hat (computed earlier) as old grad\n                grad_old = grad_hat\n                s_vec = delta.copy()\n                y_vec = grad_new - grad_old\n                if np.dot(s_vec, y_vec) > 1e-8:\n                    if len(lbfgs_s) >= self.lbfgs_m:\n                        lbfgs_s.pop(0); lbfgs_y.pop(0)\n                    lbfgs_s.append(s_vec.copy()); lbfgs_y.append(y_vec.copy())\n\n            # update manifold basis occasionally using success_buffer (weighted SVD)\n            if len(success_buffer) >= max(3, self.k) and (iter_count % max(1, min(8, n)) == 0):\n                M = np.vstack(success_buffer)  # m x n (steps normalized)\n                mcnt = M.shape[0]\n                weights_buf = np.exp(-np.linspace(0, 1.0, mcnt) * 3.0)\n                weights_buf = weights_buf / np.sum(weights_buf)\n                M_center = M - np.average(M, axis=0, weights=weights_buf)\n                Mw = (M_center.T * np.sqrt(weights_buf)).T\n                try:\n                    U, svals, Vt = np.linalg.svd(Mw, full_matrices=False)\n                    k_take = min(self.k, Vt.shape[0])\n                    if k_take > 0:\n                        B_new = Vt[:k_take].T  # n x k_take\n                        if B_new.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - B_new.shape[1]))\n                            B = np.hstack([B_new, pad])\n                            svals_norm = svals[:B_new.shape[1]] / (np.sqrt(max(1, mcnt)) + 1e-12)\n                            lam_sub = np.concatenate([svals_norm, np.ones(self.k - len(svals_norm))])\n                        else:\n                            B = B_new[:, :self.k]\n                            lam_sub = svals[:self.k] / (np.sqrt(max(1, mcnt)) + 1e-12)\n                except np.linalg.LinAlgError:\n                    pass\n\n            # if accepted, update m\n            m = np.clip(new_m, lb, ub)\n\n            # stagnation handling: if no improvement for long window, inflate sigma and mix mean with archive best\n            window = 60 + 4 * n\n            if len(archive_F) > window:\n                best_idx = int(np.argmin(archive_F))\n                # if best hasn't improved in last 'window' evaluations\n                if evals - best_idx > window:\n                    sigma = min(sigma * 2.0, sigma_max)\n                    # nudge mean toward historical good point\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                    success_buffer = []\n                    lbfgs_s = []; lbfgs_y = []\n\n            # ensure m in bounds\n            m = np.clip(m, lb, ub)\n\n            # small housekeeping\n            iter_count += 1\n            sigma = float(np.clip(sigma, sigma_min, sigma_max))\n\n        # final fallback\n        if x_best is None:\n            if len(archive_X) > 0:\n                idx = int(np.argmin(archive_F))\n                x_best = archive_X[idx].copy()\n                f_best = float(archive_F[idx])\n            else:\n                x_best = np.clip(m, lb, ub)\n                f_best = float(func(x_best)) if evals < budget else float(f_best)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm MANTA scored 0.215 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "d0f1f6fe-5082-456c-a464-0a4e9d995953", "operator": null, "metadata": {"aucs": [0.11208042339198243, 0.17603730431187448, 0.26128618463142717, 0.2641192502529577, 0.2346295184063225, 0.2578453377238965, 0.23381994080223134, 0.22451347547369094, 0.22139115623853434, 0.16777812659138858]}, "task_prompt": ""}
{"id": "b1a4d099-4d01-4f8c-9ed8-3a534a191c02", "fitness": 0.181364271744021, "name": "EASS", "description": "EASS is an ensemble adaptive subspace search that mixes three mutation modes (isotropic, learned subspace, and per‑coordinate) with a budget‑aware, relatively small population and a conservative initial trust‑region radius (R = 0.15·range), and biases sampling strongly to the learned subspace (p_subspace = 0.60, p_isotropic = 0.20, p_coordinate = 0.20) and a slightly larger-than-sqrt(dim) subspace dimension k. It learns a low‑dimensional basis from a buffer of recent successful normalized steps using weighted PCA/SVD (recent steps upweighted) and maintains per‑coordinate RMS/EMA step scales (beta_rms = 0.07) and a secant memory to estimate curvature, combining a gradient proxy from the buffer with a secant curvature term to form a predicted improvement. Diversity and heavy‑tail exploration are injected via occasional Levy/Cauchy perturbations and DE‑style differences drawn from an archive of evaluated solutions (p_levy = 0.12, p_de = 0.25), while archive_X/archive_F store candidates for selection and difference moves. Selection uses a softmax/temperature annealing over ranks to recombine top offspring into a mean update, and a trust‑region R is adaptively expanded or shrunk based on the ratio rho between actual and predicted improvement with conservative thresholds and multipliers; stagnation triggers larger R and nudges the mean toward archived good points.", "code": "import numpy as np\n\nclass EASS:\n    \"\"\"\n    Ensemble Adaptive Subspace Search (EASS)\n\n    Main ideas / differences vs the provided ASTRE:\n    - Different population sizing heuristic (slightly smaller, budget-aware).\n    - Stronger bias to learned subspace moves (p_subspace larger).\n    - Smaller initial trust-region radius (more conservative start).\n    - Different RMS / EMA parameters and different SVD weighting for subspace learning.\n    - Slightly different DE / Levy probabilities and scaling.\n    - Predicted improvement uses a linear fit plus a simple secant-based curvature correction.\n    - Trust-region update thresholds and expansion/shrink multipliers are changed.\n    - Simpler, slightly more aggressive archive injection / restart rule.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population: smaller baseline than ASTRE, budget-aware in loop\n        self.lambda_ = max(4, int(4 + np.floor(3.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace dimension (biased a bit larger than sqrt(dim))\n        if subspace_k is None:\n            self.k = max(1, int(np.clip(np.ceil((self.dim ** 0.5) * 1.4), 1, self.dim)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # secant memory size (lightweight but slightly larger)\n        self.secant_m = min(10, 3 * self.k + 1)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (support scalar or vector bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        range_mean = np.mean(ub - lb)\n        # different initialization: more conservative\n        R = 0.15 * range_mean\n        R_min = 1e-9\n        R_max = 0.5 * np.max(ub - lb) + 1e-12\n\n        # per-coordinate RMS and EMA of squared steps (different beta)\n        s2_ema = np.ones(n) * (0.5 ** 2)\n        s = np.sqrt(s2_ema)\n\n        # learned subspace basis B (n x k) init random orthonormal\n        if self.k > 0:\n            rand = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand)\n                B = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                B = np.zeros((n, self.k))\n        else:\n            B = np.zeros((n, 0))\n        lambda_sub = np.ones(self.k)\n\n        # success buffer for PCA (store raw steps normalized by R)\n        buffer = []\n        buffer_max = max(6 * self.k, 20)\n\n        # archive for DE-like differences\n        archive_X = []\n        archive_F = []\n\n        # secant memory\n        secant_dx = []\n        secant_df = []\n\n        # mixing probabilities (changed)\n        p_isotropic = 0.20\n        p_subspace = 0.60\n        p_coordinate = 0.20\n\n        # mutation injections\n        p_de = 0.25\n        p_levy = 0.12\n\n        # temperature schedule params for recombination (exponential decay)\n        temp_init = 1.0\n        temp_final = 0.15\n\n        # RMS EMA parameter\n        beta_rms = 0.07\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = float(func(xm))\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_opt = fm\n            x_opt = xm.copy()\n\n        # helper: linear gradient approximation from buffer\n        def approx_grad_from_buffer(buf, fbuf):\n            if len(buf) < 3:\n                return np.zeros(n)\n            Y = np.vstack(buf)  # m x n (steps)\n            Yc = Y - np.mean(Y, axis=0, keepdims=True)\n            if fbuf is None or len(fbuf) != Y.shape[0]:\n                # try approximate fbuf from last archive entries if possible\n                if len(archive_F) >= Y.shape[0]:\n                    fbuf = np.array(archive_F[-Y.shape[0]:])\n                else:\n                    return np.zeros(n)\n            fc = np.array(fbuf) - np.mean(fbuf)\n            reg = 1e-6 + 5e-3 * np.mean(np.var(Yc, axis=0))\n            try:\n                A = Yc.T @ Yc + reg * np.eye(n)\n                b = Yc.T @ fc\n                g = np.linalg.solve(A, b)\n                return g\n            except np.linalg.LinAlgError:\n                return np.zeros(n)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, max(2, remaining))\n            # temperature annealing (exponential)\n            t_frac = evals / max(1, budget)\n            temp = temp_final + (temp_init - temp_final) * np.exp(-5.0 * t_frac)\n\n            arx = np.zeros((lam, n))\n            armut = []\n\n            for i in range(lam):\n                r = np.random.rand()\n                if r < p_isotropic:\n                    z = np.random.randn(n)\n                    step = (R * z) / np.sqrt(n)\n                    mut_mode = 'iso'\n                elif r < p_isotropic + p_subspace and self.k > 0:\n                    zsub = np.random.randn(self.k)\n                    sub = (B @ (np.sqrt(lambda_sub + 1e-12) * zsub))\n                    # different mixing: emphasize subspace more strongly\n                    step = R * (0.85 * sub + 0.15 * (np.random.randn(n) * (s / (np.mean(s) + 1e-12))))\n                    mut_mode = 'sub'\n                else:\n                    z = np.random.randn(n)\n                    step = R * z * (s / (np.mean(s) + 1e-12))\n                    mut_mode = 'coord'\n\n                # Levy/Cauchy heavy tail injection (different exponent)\n                if np.random.rand() < p_levy:\n                    c = np.random.standard_cauchy()\n                    nrm = np.linalg.norm(step) + 1e-12\n                    step = (np.sign(c) * (np.abs(c) ** 0.6)) * (step / nrm) * (R * 3.5)\n                    mut_mode += '+levy'\n\n                x = m + step\n\n                # DE-style archive difference\n                if np.random.rand() < p_de and len(archive_X) >= 3:\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    diff = archive_X[i1] - archive_X[i2]\n                    x = x + 0.8 * diff\n                    mut_mode += '+de'\n\n                # clip\n                x = np.clip(x, lb, ub)\n                arx[i] = x\n                armut.append((step.copy(), mut_mode))\n\n            # evaluate sequentially, respect budget\n            arfit = np.full(lam, np.inf, dtype=float)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                fx = float(func(x))\n                evals += 1\n                arfit[i] = fx\n                archive_X.append(x.copy())\n                archive_F.append(fx)\n                if fx < f_opt:\n                    f_opt = fx\n                    x_opt = x.copy()\n\n            # selection: softmax on negative ranks, but slightly different scaling\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            sorted_idx = valid_idx[np.argsort(arfit[valid_idx])]\n            ranks = np.arange(1, len(sorted_idx) + 1)\n            logits = - (ranks - 1) / max(1e-12, temp * np.sqrt(n) * 0.7)\n            exps = np.exp(logits - np.max(logits))\n            soft_w = exps / np.sum(exps)\n            take = min(self.mu, len(sorted_idx))\n            chosen = sorted_idx[:take]\n            weights = soft_w[:take]\n            if weights.sum() <= 0:\n                weights = np.ones_like(weights) / len(weights)\n            else:\n                weights = weights / np.sum(weights)\n\n            x_sel = arx[chosen]\n            f_sel = arfit[chosen]\n            steps = np.array([armut[j][0] for j in chosen])\n            delta_m = np.sum(weights[:, None] * steps, axis=0)\n            m_new = m + delta_m\n\n            # predicted improvement: linear grad from buffer + secant curvature correction\n            local_buf_len = min(len(buffer), 12)\n            buf_y = [b for b in buffer[-local_buf_len:]]\n            fbuf_local = archive_F[-len(buf_y):] if len(buf_y) > 0 and len(archive_F) >= len(buf_y) else None\n            grad_hat = approx_grad_from_buffer(buf_y, fbuf_local if fbuf_local is not None else None)\n\n            # secant curvature estimate along delta_m (simple ridge regression using secant pairs)\n            gamma = 0.0\n            if len(secant_dx) >= 2:\n                dxs = np.vstack(secant_dx)  # m x n\n                dfs = np.array(secant_df)\n                proj = dxs @ delta_m\n                denom = np.sum(proj ** 2) + 1e-12\n                gamma = np.sum(proj * dfs) / denom  # rough curvature coefficient (df ~ gamma * proj)\n                # clamp gamma to reasonable range to avoid wild second-order corrections\n                gamma = np.clip(gamma, -5.0, 5.0)\n\n            predicted_improv = -np.dot(grad_hat, delta_m) + 0.5 * gamma * (np.dot(delta_m, delta_m))\n\n            # actual improvement: compare current mean fitness (last mean evaluation) vs selected\n            current_mean_f = archive_F[-1] if len(archive_F) > 0 else f_opt\n            mean_fitness_selected = np.mean(f_sel) if len(f_sel) > 0 else current_mean_f\n            actual_improv = max(0.0, (current_mean_f - mean_fitness_selected))\n\n            # ratio\n            eps = 1e-9\n            rho = actual_improv / (abs(predicted_improv) + eps)\n\n            # trust-region update (different thresholds/factors)\n            if predicted_improv > 1e-12 and rho > 1.0:\n                R = min(R * 1.35, R_max)\n            elif rho > 0.6:\n                R = min(R * 1.08, R_max)\n            elif rho < 0.2:\n                R = max(R * 0.6, R_min)\n            else:\n                R = max(R * 0.9, R_min)\n\n            # secant memory update\n            if len(f_sel) > 0:\n                rep_step = delta_m.copy()\n                rep_df = np.mean(f_sel) - current_mean_f\n                if len(secant_dx) >= self.secant_m:\n                    secant_dx.pop(0)\n                    secant_df.pop(0)\n                secant_dx.append(rep_step.copy())\n                secant_df.append(rep_df)\n\n            # update per-coordinate RMS (EMA)\n            if len(steps) > 0:\n                y2 = np.sum(weights[:, None] * (steps ** 2), axis=0)\n                s2_ema = (1.0 - beta_rms) * s2_ema + beta_rms * (y2 + 1e-12)\n                s = np.sqrt(s2_ema + 1e-12)\n                s = np.clip(s, 1e-8, 1e8)\n\n            # record successful step into buffer if any improvement or occasionally for diversity\n            if actual_improv > 1e-12 and len(steps) > 0:\n                avg_step = delta_m.copy() / (R + 1e-12)\n                buffer.append(avg_step)\n                if len(buffer) > buffer_max:\n                    buffer.pop(0)\n            elif len(steps) > 0 and np.random.rand() < 0.12:\n                buffer.append(delta_m.copy() / (R + 1e-12))\n                if len(buffer) > buffer_max:\n                    buffer.pop(0)\n\n            # subspace PCA update (different weighting, more recent emphasis)\n            if len(buffer) >= max(3, self.k) and (evals % max(1, min(8, n)) == 0):\n                M = np.vstack(buffer)\n                mcnt = M.shape[0]\n                # exponential weights: recent higher\n                weights_buf = np.exp(-np.linspace(1.0, 0.0, mcnt) * 2.0)\n                weights_buf = weights_buf / np.sum(weights_buf)\n                M_centered = M - np.average(M, axis=0, weights=weights_buf)\n                Mw = (M_centered.T * np.sqrt(weights_buf)).T\n                try:\n                    U_s, svals, Vt = np.linalg.svd(Mw, full_matrices=False)\n                    k_take = min(self.k, Vt.shape[0])\n                    if k_take > 0:\n                        B_new = Vt[:k_take].T\n                        if B_new.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - B_new.shape[1]))\n                            B = np.hstack([B_new, pad])\n                            svals_norm = svals[:B_new.shape[1]] / (np.sqrt(max(1, mcnt)) + 1e-12)\n                            lambda_sub = np.concatenate([svals_norm, np.ones(self.k - len(svals_norm))])\n                        else:\n                            B = B_new[:, :self.k]\n                            svals_norm = svals[:self.k] / (np.sqrt(max(1, mcnt)) + 1e-12)\n                            lambda_sub = svals_norm.copy()\n                except np.linalg.LinAlgError:\n                    pass\n\n            # accept/update mean: slightly different acceptance criterion\n            if mean_fitness_selected <= current_mean_f + 1e-12 or rho > 0.2:\n                m = m_new.copy()\n            else:\n                alpha = float(np.clip(rho * 0.8, 0.02, 0.5))\n                m = (1.0 - alpha) * m + alpha * m_new\n\n            # stagnation detection: if no improvement in a window, inject diversity\n            stagn_window = 40 + 4 * n\n            if len(archive_F) > stagn_window:\n                best_index = int(np.argmin(archive_F))\n                # if best hasn't improved in last stagn_window evaluations\n                if evals - best_index > stagn_window:\n                    R = min(R * 2.0, R_max)\n                    # nudge mean to a random good archive point slightly\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                    buffer = []\n                    secant_dx = []\n                    secant_df = []\n\n            # ensure bounds\n            m = np.clip(m, lb, ub)\n            R = float(np.clip(R, R_min, R_max))\n\n        # final fallback to archive best if needed\n        if x_opt is None and len(archive_X) > 0:\n            idx = int(np.argmin(archive_F))\n            x_opt = archive_X[idx].copy()\n            f_opt = float(archive_F[idx])\n        elif x_opt is None:\n            x_opt = np.clip(m, lb, ub)\n            if evals < budget:\n                f_opt = float(func(x_opt))\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm EASS scored 0.181 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "d0f1f6fe-5082-456c-a464-0a4e9d995953", "operator": null, "metadata": {"aucs": [0.09143519786561416, 0.15285381684403865, 0.253800261820189, 0.1690132797078553, 0.2084196090650967, 0.21405353762286206, 0.20366559225586256, 0.19753553682429603, 0.18897702332527244, 0.13388886210912332]}, "task_prompt": ""}
{"id": "a173dbd9-64fb-4c61-990c-4d570accd5de", "fitness": 0.23586550608518259, "name": "AGS_QL", "description": "The algorithm blends learned low-dimensional modeling and stochastic block mutations: it adaptively builds a k≈√n SVD/PCA subspace from a rolling buffer (buffer_max ≈ max(30,8k)) and fits a simple per-subspace-coordinate quadratic (y* = −g/(h+eps)) to propose model-minimizer moves (≈45% proposals), while the rest are grouped anisotropic block mutations (G ≈ √n/1.5) or coordinate-wise Gaussian steps. It maintains per-coordinate RMS scaling s (EMA on squared steps) and a diagonal curvature h_diag (secant memory secant_m = min(10,4k) and EMA of positive secant curvatures) to adapt step magnitudes, plus an adaptive trust radius R (init ≈0.25·range_mean, R_max ≈0.6·range) that expands/contracts based on realized vs predicted improvement rho. Exploration enhancements include an archive for differential-like combinations (probability ≈0.18), occasional heavy-tailed Lévy/Cauchy jumps (≈0.07) for global escapes, and budget-aware local refinement via a small Armijo-like line search (≤4 extra evals). Selection/recombination uses a rank-based softmax (temperature decreasing with progress) to form weighted mean shifts, with stagnation detection triggering large injections or reinitialization to restore diversity.", "code": "import numpy as np\n\nclass AGS_QL:\n    \"\"\"\n    Adaptive Grouped-Subspace Quadratic Line-search (AGS-QL)\n\n    Main ideas:\n    - Maintain a small rolling buffer of recent steps and fitnesses to learn a k-dim subspace (SVD/PCA).\n    - In that subspace fit a simple quadratic model f ≈ a + g^T y + 0.5 * (h ∘ y ∘ y) (elementwise 2nd-order),\n      which yields a closed-form per-subspace-coordinate minimizer y* = -g / (h + eps).\n    - Propose model-minimizer moves (projected back to full space) and refine them with a cheap Armijo-like\n      backtracking / few-point line-search (budget aware).\n    - Also propose grouped coordinate mutations: coordinates are clustered into groups by variance-estimates;\n      group-wise step-sizes are adapted separately (allows block moves).\n    - Use an archive for differential-like proposals; occasional Lévy/Cauchy heavy tails for global escapes.\n    - Maintain per-coordinate RMS scaling and a diagonal curvature estimate (ema of secant curvatures).\n    - Adaptive trust radius R governs max step lengths; R expands/contracts based on realized improvement.\n    - All evaluations strictly limited by self.budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, sub_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        # subspace dimension (default ~ sqrt(n))\n        if sub_k is None:\n            self.k = max(1, int(max(1, np.floor(np.sqrt(self.dim)))))\n        else:\n            self.k = min(max(1, int(sub_k)), self.dim)\n\n        # population-ish parameter for batch proposals\n        self.pop = max(4, int(4 + 2 * np.sqrt(self.dim)))\n\n        # small secant memory for curvature estimation\n        self.secant_m = min(10, 4 * self.k)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (support scalar)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center (mean)\n        m = np.random.uniform(lb, ub)\n        range_mean = float(np.mean(ub - lb))\n        R = max(1e-9, 0.25 * range_mean)  # trust radius\n        R_max = 0.6 * np.max(ub - lb) + 1e-12\n        R_min = 1e-9\n\n        # per-coordinate RMS scale (s) and EMA of squared steps\n        s2 = np.ones(n) * ( (np.max(ub-lb) / 10.0) ** 2 )\n        s = np.sqrt(s2)\n\n        # diagonal curvature estimate (positive part) used as h_diag for damping\n        h_diag = np.ones(n) * 1e-3\n\n        # learned subspace basis (n x k) initialized random orthonormal\n        if self.k > 0:\n            rand = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand)\n                B = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                B = np.zeros((n, self.k))\n        else:\n            B = np.zeros((n, 0))\n\n        lambda_sub = np.ones(self.k)\n\n        # buffers and archive\n        buffer = []            # list of (step = x - m, f) recent successful/seen steps (unscaled)\n        buffer_max = max(30, 8 * self.k)\n        archive_X = []\n        archive_F = []\n\n        # secant memory arrays for curvature estimation\n        sec_dx = []\n        sec_df = []\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # initial evaluation\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = float(func(xm))\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = fm; x_best = xm.copy()\n\n        # helper: fit linear model (gradient) from buffer (for predicted improvement)\n        def approx_grad(buf_steps, buf_f):\n            if len(buf_steps) < 3:\n                return np.zeros(n)\n            Y = np.vstack(buf_steps)  # m x n\n            Yc = Y - np.mean(Y, axis=0, keepdims=True)\n            fc = np.array(buf_f) - np.mean(buf_f)\n            reg = 1e-6 + 1e-2 * np.mean(np.var(Yc, axis=0))\n            try:\n                A = Yc.T @ Yc + reg * np.eye(n)\n                b = Yc.T @ fc\n                g = np.linalg.solve(A, b)\n                return g\n            except np.linalg.LinAlgError:\n                return np.zeros(n)\n\n        # helper: update subspace from buffer using weighted SVD\n        def update_subspace():\n            nonlocal B, lambda_sub\n            if len(buffer) < max(3, self.k):\n                return\n            M = np.vstack([step for (step, f) in buffer])  # m x n\n            mcnt = M.shape[0]\n            # decaying weights favor recent\n            w = np.exp(-np.linspace(0, 1.0, mcnt) * 3.0)[::-1]\n            w = w / (np.sum(w) + 1e-12)\n            M_centered = M - np.average(M, axis=0, weights=w)\n            Mw = (M_centered.T * np.sqrt(w)).T  # m x n\n            try:\n                U, svals, Vt = np.linalg.svd(Mw, full_matrices=False)\n                take = min(self.k, Vt.shape[0])\n                if take > 0:\n                    B_new = Vt[:take].T  # n x take\n                    if B_new.shape[1] < self.k:\n                        pad = np.zeros((n, self.k - B_new.shape[1]))\n                        B = np.hstack([B_new, pad])\n                        svals_norm = svals[:B_new.shape[1]] / (np.sqrt(max(1, mcnt)) + 1e-12)\n                        lambda_sub = np.concatenate([svals_norm, np.ones(self.k - len(svals_norm))])\n                    else:\n                        B = B_new[:, :self.k]\n                        lambda_sub = (svals[:self.k] / (np.sqrt(max(1, mcnt)) + 1e-12)).copy()\n            except np.linalg.LinAlgError:\n                pass\n\n        # main optimization loop\n        stagn_counter = 0\n        stagn_limit = max(50 + 10*n, 200)\n        iter_count = 0\n\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n            lam = min(self.pop, max(1, remaining))  # how many proposals to create now\n\n            # refresh subspace occasionally\n            if iter_count % max(1, min(10, n)) == 0:\n                update_subspace()\n\n            # group partitioning: coordinates grouped by s variance (simple greedy)\n            # create G groups: G ≈ max(1, int(sqrt(n)/2))\n            G = max(1, int(max(1, np.floor(np.sqrt(n) / 1.5))))\n            # sort coords by current s (descending) and partition\n            idx_sorted = np.argsort(-s)\n            groups = [idx_sorted[i::G] for i in range(G)]\n\n            # propose lam candidates\n            Xcand = np.zeros((lam, n))\n            steps_meta = []  # store (step, mode, additional_info)\n            for i in range(lam):\n                mode_rand = np.random.rand()\n                step = np.zeros(n)\n                mode = None\n\n                # with some probability, propose from quadratic model (subspace minimizer)\n                if mode_rand < 0.45 and len(buffer) >= max(6, self.k+1) and self.k > 0:\n                    mode = 'model'\n                    # Build dataset in subspace coordinates using buffer\n                    Ys = np.vstack([ (B.T @ sstep) for (sstep, f) in buffer ])  # m x k\n                    fs = np.array([f for (sstep, f) in buffer])\n                    # center features and target\n                    Ym = np.mean(Ys, axis=0, keepdims=True)\n                    Ys_c = Ys - Ym\n                    fs_c = fs - np.mean(fs)\n                    # Build design matrix with columns [y1..yk, 0.5*y1^2 .. 0.5*yk^2]\n                    # Solve linear least squares for params p = [g; h_diag]\n                    mrows = Ys_c.shape[0]\n                    if mrows >= (1 + self.k):\n                        Phi = np.hstack([Ys_c, 0.5 * (Ys_c ** 2)])\n                        # regularized solve\n                        reg = 1e-6 + 1e-3 * np.mean(np.var(Phi, axis=0))\n                        try:\n                            A = Phi.T @ Phi + reg * np.eye(Phi.shape[1])\n                            b = Phi.T @ fs_c\n                            p = np.linalg.solve(A, b)\n                            g_sub = p[:self.k]\n                            h_sub = p[self.k:(2*self.k)]\n                        except np.linalg.LinAlgError:\n                            g_sub = np.zeros(self.k)\n                            h_sub = np.ones(self.k) * 1e-3\n                    else:\n                        g_sub = np.zeros(self.k)\n                        h_sub = np.ones(self.k) * 1e-3\n\n                    # compute subspace minimizer y* = -g / (h + eps)\n                    eps = 1e-8\n                    denom = h_sub + eps\n                    y_star = - g_sub / denom\n                    # limit magnitude in subspace coordinates (avoid huge steps)\n                    max_y_norm = 3.0\n                    ynorm = np.linalg.norm(y_star)\n                    if ynorm > max_y_norm:\n                        y_star = y_star * (max_y_norm / (ynorm + 1e-12))\n\n                    step = B @ y_star  # back to full space\n                    # add small randomization proportional to s to avoid exact step repeats\n                    step += 0.03 * (np.random.randn(n) * (s / (np.mean(s) + 1e-12))) * R\n                elif mode_rand < 0.75:\n                    # grouped block-wise mutation (anisotropic block scaling)\n                    mode = 'grouped'\n                    for gidx, gcoords in enumerate(groups):\n                        # group-wise step magnitude drawn from half-normal scaled by group size and R\n                        group_scale = (1.0 + 0.6 * (gidx / max(1, G-1)))  # varying scale\n                        z = np.random.randn(len(gcoords))\n                        # use per-coord s scaling inside group and group-scale\n                        step[gcoords] = (R * 0.8 * group_scale) * z * (s[gcoords] / (np.mean(s) + 1e-12)) / np.sqrt(n)\n                else:\n                    # simple coordinate-wise anisotropic gaussian\n                    mode = 'coord'\n                    z = np.random.randn(n)\n                    step = R * z * (s / (np.mean(s) + 1e-12))\n\n                # occasional differential archive combination\n                if np.random.rand() < 0.18 and len(archive_X) >= 2:\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    diff = archive_X[i1] - archive_X[i2]\n                    step += 0.6 * diff\n                    mode = (mode or 'mut') + '+de'\n\n                # occasional heavy-tail jump\n                if np.random.rand() < 0.07:\n                    c = np.random.standard_cauchy()\n                    cscale = np.sign(c) * (np.abs(c) ** 0.6)\n                    nrm = np.linalg.norm(step) + 1e-12\n                    step = (step / (nrm)) * (R * 4.0 * cscale)\n                    mode = (mode or 'mut') + '+levy'\n\n                # clip step to trust radius (Euclidean)\n                s_norm = np.linalg.norm(step)\n                if s_norm > R * 2.5:\n                    step = step * ((R * 2.5) / (s_norm + 1e-12))\n\n                x = m + step\n                x = np.clip(x, lb, ub)\n\n                Xcand[i] = x\n                steps_meta.append((step.copy(), mode))\n\n            # evaluate candidates sequentially but limit to remaining budget\n            fvals = np.full(lam, np.inf)\n            order = np.arange(lam)\n            # prioritise model proposals first (heuristic)\n            # sort by mode preference: model > grouped > coord > others\n            mode_priority = {'model': 0, 'model+de':0, 'grouped':1, 'coord':2}\n            pri = np.array([ mode_priority.get(m.split('+')[0], 3) for (_, m) in steps_meta ])\n            order = np.argsort(pri)\n            for idx in order:\n                if evals >= budget:\n                    break\n                x = Xcand[idx]\n                fx = float(func(x))\n                evals += 1\n                fvals[idx] = fx\n                archive_X.append(x.copy()); archive_F.append(fx)\n                # record buffer entry (unscaled step x - m)\n                buffer.append((x - m, fx))\n                if len(buffer) > buffer_max:\n                    buffer.pop(0)\n                # update best\n                if fx < f_best:\n                    f_best = fx; x_best = x.copy()\n                    stagn_counter = 0\n                else:\n                    stagn_counter += 1\n\n            # selection & recombination: softmax on (neg fitness) with diversity penalty\n            valid_idx = np.where(np.isfinite(fvals))[0]\n            if valid_idx.size == 0:\n                break\n            # ranks by increasing f\n            sorted_idx = valid_idx[np.argsort(fvals[valid_idx])]\n            # softmax based on negative rank scaled by temperature decreasing with evals\n            t_frac = evals / max(1, budget)\n            temp = 1.0 * (1.0 - t_frac) + 0.2 * t_frac\n            ranks = np.arange(1, len(sorted_idx) + 1)\n            logits = - (ranks - 1) / max(1e-12, temp * np.sqrt(n))\n            exps = np.exp(logits - np.max(logits))\n            weights_all = exps / np.sum(exps)\n            take = min(max(1, self.pop // 3), len(sorted_idx))\n            chosen = sorted_idx[:take]\n            weights = weights_all[:take]\n            if weights.sum() <= 0:\n                weights = np.ones_like(weights) / len(weights)\n            else:\n                weights = weights / np.sum(weights)\n\n            x_sel = Xcand[chosen]\n            f_sel = fvals[chosen]\n            steps_sel = np.array([steps_meta[int(j)][0] for j in chosen])\n\n            # recombined mean shift\n            delta_m = np.sum(weights[:, None] * steps_sel, axis=0)\n            candidate_m = m + delta_m\n            candidate_m = np.clip(candidate_m, lb, ub)\n\n            # predicted improvement via approximate gradient from buffer\n            buf_steps = [b for (b, f) in buffer[-min(len(buffer), 20):]]\n            buf_f = [f for (b, f) in buffer[-min(len(buffer), 20):]]\n            grad_hat = approx_grad(buf_steps, buf_f)\n            predicted = - np.dot(grad_hat, delta_m)  # predicted positive decrease\n\n            # actual improvement estimate vs current m (use last evaluated m if present)\n            current_m_f = archive_F[-1] if len(archive_F) > 0 else f_best\n            actual_est = max(0.0, current_m_f - np.dot(weights, f_sel)) if len(f_sel)>0 else 0.0\n            rho = actual_est / (abs(predicted) + 1e-8)\n\n            # perform a cheap Armijo-like backtracking line search along delta_m if promising and budget allows\n            moved = False\n            if np.linalg.norm(delta_m) > 1e-12 and evals < budget and (predicted > 1e-8 or rho > 0.2):\n                # alpha candidates (budget-aware)\n                # allocate up to min(4, remaining-1) evaluations for line search\n                remain_now = budget - evals\n                max_ls = min(4, max(1, remain_now))\n                alphas = [1.0, 0.6, 0.3, 0.1][:max_ls]\n                base_f = current_m_f\n                best_local_f = base_f\n                best_local_x = None\n                for a in alphas:\n                    if evals >= budget:\n                        break\n                    x_try = np.clip(m + a * delta_m, lb, ub)\n                    f_try = float(func(x_try))\n                    evals += 1\n                    archive_X.append(x_try.copy()); archive_F.append(f_try)\n                    buffer.append((x_try - m, f_try))\n                    if len(buffer) > buffer_max:\n                        buffer.pop(0)\n                    if f_try < best_local_f:\n                        best_local_f = f_try; best_local_x = x_try.copy()\n                        if f_try < f_best:\n                            f_best = f_try; x_best = x_try.copy()\n                            stagn_counter = 0\n                    else:\n                        stagn_counter += 1\n                if best_local_x is not None and best_local_f <= current_m_f:\n                    # accept the best local point as new mean (aggressive)\n                    m = best_local_x.copy()\n                    moved = True\n                elif rho > 0.15:\n                    # partial move proportional to rho\n                    alpha = max(0.05, min(0.6, rho))\n                    m = (1.0 - alpha) * m + alpha * candidate_m\n                    moved = True\n                else:\n                    # conservative shrink of R\n                    R = max(R * 0.8, R_min)\n            else:\n                # no line search: accept candidate if it improves or rho is decent\n                mean_f_selected = np.mean(f_sel) if len(f_sel) > 0 else current_m_f\n                if mean_f_selected <= current_m_f or rho > 0.3:\n                    m = candidate_m.copy()\n                    moved = True\n                else:\n                    # partial move toward candidate\n                    alpha = max(0.05, min(0.5, rho))\n                    m = (1.0 - alpha) * m + alpha * candidate_m\n\n            # update trust radius R based on rho\n            if predicted > 1e-9 and rho > 1.0:\n                R = min(R * (1.1 + 0.6 * (rho - 1.0)), R_max)\n            elif rho > 0.4:\n                R = min(R * 1.03, R_max)\n            elif rho < 0.05:\n                R = max(R * 0.7, R_min)\n            else:\n                R = np.clip(R * 0.95, R_min, R_max)\n\n            # update per-coordinate RMS (s) from selected steps\n            if steps_sel.size > 0:\n                y2 = np.sum(weights[:, None] * (steps_sel ** 2), axis=0)\n                beta = 0.12\n                s2 = (1 - beta) * s2 + beta * (y2 + 1e-12)\n                s = np.sqrt(s2 + 1e-12)\n                s = np.clip(s, 1e-8, 1e6)\n\n            # update secant memory and diagonal curvature h_diag\n            if steps_sel.size > 0:\n                rep_dx = delta_m.copy()\n                rep_df = np.mean(f_sel) - current_m_f\n                if len(sec_dx) >= self.secant_m:\n                    sec_dx.pop(0); sec_df.pop(0)\n                sec_dx.append(rep_dx.copy()); sec_df.append(rep_df)\n                # estimate per-coordinate curvature from secants and update EMA for positive curvature\n                curv_est = np.zeros(n)\n                for dx, df in zip(sec_dx, sec_df):\n                    denom = (dx.copy() + 1e-12)\n                    curv_est += (df / denom)\n                curv_est = curv_est / max(1, len(sec_dx))\n                # keep only positive curvature signals and EMA them\n                pos_curv = np.maximum(0.0, curv_est)\n                h_diag = 0.85 * h_diag + 0.15 * pos_curv\n                h_diag = np.clip(h_diag, 1e-8, 1e8)\n\n            # decide storing successful normalized steps into buffer for subspace update\n            if moved and np.linalg.norm(delta_m) > 1e-12:\n                normalized = (delta_m.copy()) / (R + 1e-12)\n                buffer.append((normalized, np.mean(f_sel) if len(f_sel)>0 else archive_F[-1]))\n                if len(buffer) > buffer_max:\n                    buffer.pop(0)\n\n            # occasional stigma-based injection if stagnated\n            if stagn_counter > stagn_limit and evals < budget:\n                # heavy injection: enlarge R and reinitialize m near a random archive point or random location\n                R = min(R * 2.0, R_max)\n                if len(archive_X) > 0 and np.random.rand() < 0.8:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                else:\n                    m = np.random.uniform(lb, ub)\n                buffer = []; sec_dx = []; sec_df = []\n                stagn_counter = 0\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n            R = float(np.clip(R, R_min, R_max))\n\n            # occasionally refresh subspace\n            if iter_count % max(1, 5) == 0:\n                update_subspace()\n\n            # loop until budget consumed\n\n        # final fallback best\n        if x_best is None:\n            if len(archive_X) > 0:\n                idx = int(np.argmin(archive_F))\n                x_best = archive_X[idx].copy(); f_best = float(archive_F[idx])\n            else:\n                x_best = np.clip(m, lb, ub)\n                if evals < budget:\n                    f_best = float(func(x_best))\n                    evals += 1\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AGS_QL scored 0.236 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "d0f1f6fe-5082-456c-a464-0a4e9d995953", "operator": null, "metadata": {"aucs": [0.1309976636554705, 0.20799958431102006, 0.29547358678341096, 0.31678712255718244, 0.23404101650974252, 0.2868560322680985, 0.24825216806756356, 0.2514740823025646, 0.22058464763710817, 0.16618915675966506]}, "task_prompt": ""}
{"id": "a9a17607-ce17-4cdc-ba10-2e7f6eaa5d21", "fitness": 0.20770720100835693, "name": "EDAR", "description": "EDAR is an ensemble, budget-aware heuristic that mixes three mutation modes (high-weight isotropic mutations p_isotropic=0.50, learned low-dimensional subspace mutations p_subspace=0.25 with log-scaled subspace k, and coordinate-wise RMS-scaled mutations p_coordinate=0.25) plus occasional Levy tails and small DE-like archive differences for rare large jumps. It learns a compact subspace from a temporal buffer (SVD on recent normalized successful steps) and adapts per-coordinate scales with an EMA (s2_ema, beta=0.25) so mutations combine global, subspace, and coordinate structure while keeping secant memory small (secant_m ≤ 6). Selection uses a temperature-annealed softmax over ranked candidates (temperature schedule from temp_init to temp_final) to produce weighted recombination (mu larger than usual), and a trust-region radius R is adapted by comparing predicted improvement from a simple linear model (approx_grad) to actual improvement (rho) with conservative expansion/contraction factors. Finally, the algorithm uses budget-aware population sizing (lambda ~ log(dim)), periodic aggressive restarts on stagnation, archive-driven DE moves, and safeguards/clipping to remain robust inside [-5,5] bounds.", "code": "import numpy as np\n\nclass EDAR:\n    \"\"\"\n    Ensemble Directional Adaptive Restart (EDAR)\n    - Mixture of isotropic, coordinate-wise, and learned subspace mutations.\n    - Subspace dimension scales with log(dim) (different from sqrt(dim) in ASTRE).\n    - More isotropic emphasis and larger recombination group (mu) than ASTRE.\n    - Different mixing probabilities, RMS updates, and trust-region adaptation rules.\n    - Budget-aware population sizing and aggressive restart policy.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population sizing heuristic (different from ASTRE)\n        self.lambda_ = max(8, int(np.floor(4 + 3.0 * np.log(max(2, self.dim)))))\n        self.mu = max(2, self.lambda_ // 2)   # larger recombination group\n\n        # subspace dimension: log-scaled choice (different formula)\n        if subspace_k is None:\n            self.k = max(1, min(self.dim, int(np.clip(np.ceil(np.log1p(self.dim) * 1.5), 1, self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # secant memory (slightly smaller)\n        self.secant_m = min(6, self.k + 2)\n\n        # mixing probabilities (different emphasis)\n        self.p_isotropic = 0.50\n        self.p_subspace = 0.25\n        self.p_coordinate = 0.25\n\n        # mutation extras probabilities (different)\n        self.p_de = 0.12\n        self.p_levy = 0.04\n\n        # temperature schedule endpoints (different)\n        self.temp_init = 1.0\n        self.temp_final = 0.05\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds handling (support scalar bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center sampled uniformly\n        m = np.random.uniform(lb, ub)\n        range_mean = np.mean(ub - lb)\n        R = max(1e-8, 0.15 * range_mean)  # smaller initial trust-region than ASTRE\n        R_min = 1e-9\n        R_max = 0.6 * np.max(ub - lb) + 1e-12\n\n        # per-coordinate RMS scale and EMA of square steps (different beta)\n        s2_ema = np.ones(n) * (0.5 ** 2)\n        s = np.sqrt(s2_ema)\n\n        # initialize subspace basis (n x k) orthonormal if possible\n        if self.k > 0:\n            rand = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand)\n                B = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                B = np.zeros((n, self.k))\n        else:\n            B = np.zeros((n, 0))\n        lambda_sub = np.ones(self.k)\n\n        # buffers and memories\n        buffer = []               # store recent normalized successful steps\n        buffer_max = max(6 * self.k, 20)\n        archive_X = []\n        archive_F = []\n        secant_dx = []\n        secant_df = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = float(func(xm))\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_opt = fm\n            x_opt = xm.copy()\n\n        # helper: robust linear gradient approx from buffered (x-m) steps and corresponding f values\n        def approx_grad(buf_steps, fbuf):\n            if len(buf_steps) < 3:\n                return np.zeros(n)\n            Y = np.vstack(buf_steps)    # m x n\n            # center\n            Yc = Y - np.mean(Y, axis=0, keepdims=True)\n            fc = np.array(fbuf) - np.mean(fbuf)\n            reg = 1e-4 + 1e-2 * np.mean(np.var(Yc, axis=0))\n            try:\n                A = Yc.T @ Yc + reg * np.eye(n)\n                b = Yc.T @ fc\n                g = np.linalg.solve(A, b)\n                return g\n            except np.linalg.LinAlgError:\n                return np.zeros(n)\n\n        # main optimization loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, max(2, remaining))\n\n            # anneal temperature (nonlinear slightly)\n            frac = evals / max(1, budget)\n            temp = self.temp_init * (1.0 - frac ** 0.9) + self.temp_final * (frac ** 0.9)\n\n            # create candidates\n            arx = np.zeros((lam, n))\n            armut = []\n            for i in range(lam):\n                r = np.random.rand()\n                if r < self.p_isotropic:\n                    z = np.random.randn(n)\n                    step = (R * z) / np.sqrt(n)\n                    mode = 'iso'\n                elif r < self.p_isotropic + self.p_subspace and self.k > 0:\n                    zsub = np.random.randn(self.k)\n                    sub = B @ (np.sqrt(lambda_sub + 1e-12) * zsub)\n                    # different mixing: more weight to orthogonal part occasionally\n                    alpha_sub = 0.8\n                    step = R * (alpha_sub * sub + (1.0 - alpha_sub) * (np.random.randn(n) * (s / (np.mean(s) + 1e-12))))\n                    mode = 'sub'\n                else:\n                    z = np.random.randn(n)\n                    step = R * z * (s / (np.mean(s) + 1e-12))\n                    mode = 'coord'\n\n                # occasional Levy-like heavy tail (less frequent than ASTRE)\n                if np.random.rand() < self.p_levy:\n                    c = np.random.standard_cauchy()\n                    power = 0.85\n                    nrm = np.linalg.norm(step) + 1e-12\n                    step = (np.sign(c) * (np.abs(c) ** power)) * (step / nrm) * (R * 3.5)\n                    mode += '+levy'\n\n                x = m + step\n\n                # occasional DE-like archive difference with smaller scale\n                if np.random.rand() < self.p_de and len(archive_X) >= 3:\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    diff = archive_X[i1] - archive_X[i2]\n                    x = x + 0.4 * diff\n                    mode += '+de'\n\n                x = np.clip(x, lb, ub)\n                arx[i] = x\n                armut.append((step.copy(), mode))\n\n            # evaluate sequentially until budget exhausted\n            arfit = np.full(lam, np.inf, dtype=float)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                fx = float(func(arx[i]))\n                evals += 1\n                arfit[i] = fx\n                archive_X.append(arx[i].copy())\n                archive_F.append(fx)\n                if fx < f_opt:\n                    f_opt = fx\n                    x_opt = arx[i].copy()\n\n            # selection: softmax on negative fitness scaled by temp and dim^0.25 (different scaling)\n            valid = np.where(np.isfinite(arfit))[0]\n            if valid.size == 0:\n                break\n            sorted_idx = valid[np.argsort(arfit[valid])]\n            ranks = np.arange(1, len(sorted_idx) + 1)\n            logits = - (ranks - 1) / max(1e-12, temp * (n ** 0.25))\n            exps = np.exp(logits - np.max(logits))\n            soft_w = exps / np.sum(exps)\n            take = min(self.mu, len(sorted_idx))\n            chosen = sorted_idx[:take]\n            weights = soft_w[:take]\n            if weights.sum() <= 0:\n                weights = np.ones_like(weights) / len(weights)\n            else:\n                weights = weights / np.sum(weights)\n\n            x_sel = arx[chosen]\n            f_sel = arfit[chosen]\n            steps = np.array([armut[j][0] for j in chosen])  # steps relative to m\n            delta_m = np.sum(weights[:, None] * steps, axis=0)\n            m_new = m + delta_m\n\n            # build a small local buffer and approximate gradient\n            local_len = min(len(buffer), 10)\n            if local_len > 0 and len(archive_F) >= local_len:\n                buf_steps = buffer[-local_len:]\n                fbuf = archive_F[-local_len:]\n            else:\n                buf_steps = []\n                fbuf = []\n            grad_hat = approx_grad(buf_steps, fbuf)\n            predicted = -np.dot(grad_hat, delta_m)\n\n            # actual improvement: compare mean selected fitness to current mean evaluation\n            current_mean_f = archive_F[-1] if len(archive_F) > 0 else f_opt\n            mean_selected = np.mean(f_sel) if len(f_sel) > 0 else current_mean_f\n            actual = max(0.0, current_mean_f - mean_selected)\n\n            # trust-region ratio and update with different thresholds\n            eps = 1e-9\n            rho = actual / (abs(predicted) + eps)\n            if predicted > 1e-10 and rho > 1.0:\n                R = min(R * 1.3, R_max)\n            elif rho > 0.5:\n                R = min(R * 1.08, R_max)\n            elif rho < 0.2:\n                R = max(R * 0.6, R_min)\n            else:\n                R = max(R * 0.9, R_min)\n\n            # secant memory update (representative)\n            if len(f_sel) > 0:\n                rep_step = delta_m.copy()\n                rep_df = np.mean(f_sel) - current_mean_f\n                if len(secant_dx) >= self.secant_m:\n                    secant_dx.pop(0); secant_df.pop(0)\n                secant_dx.append(rep_step.copy())\n                secant_df.append(rep_df)\n\n            # RMS update with different beta\n            if len(steps) > 0:\n                y2 = np.sum(weights[:, None] * (steps ** 2), axis=0)\n                beta = 0.25\n                s2_ema = (1.0 - beta) * s2_ema + beta * (y2 + 1e-12)\n                s = np.sqrt(s2_ema + 1e-12)\n                s = np.clip(s, 1e-8, 1e8)\n\n            # push improvement into buffer (normalized)\n            if actual > 1e-12 and len(steps) > 0:\n                buffer.append((delta_m / (R + 1e-12)).copy())\n                if len(buffer) > buffer_max:\n                    buffer.pop(0)\n\n            # update subspace from buffer periodically (different frequency)\n            if len(buffer) >= max(3, self.k) and (evals % max(2, min(8, n)) == 0):\n                M = np.vstack(buffer)  # m x n\n                mcnt = M.shape[0]\n                # exponential temporal weights favoring recent (weaker than ASTRE)\n                w = np.exp(-np.linspace(0, 1.0, mcnt) * 1.8)\n                w = w / np.sum(w)\n                M_centered = M - np.average(M, axis=0, weights=w)\n                Mw = (M_centered.T * np.sqrt(w)).T\n                try:\n                    U, svals, Vt = np.linalg.svd(Mw, full_matrices=False)\n                    k_take = min(self.k, Vt.shape[0])\n                    if k_take > 0:\n                        B_new = Vt[:k_take].T\n                        if B_new.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - B_new.shape[1]))\n                            B = np.hstack([B_new, pad])\n                            svals_norm = svals[:B_new.shape[1]] / (np.sqrt(max(1, mcnt)) + 1e-12)\n                            lambda_sub = np.concatenate([svals_norm, np.ones(self.k - len(svals_norm))])\n                        else:\n                            B = B_new[:, :self.k]\n                            lambda_sub = (svals[:self.k] / (np.sqrt(max(1, mcnt)) + 1e-12)).copy()\n                except np.linalg.LinAlgError:\n                    pass\n\n            # mean acceptance with different acceptance rule\n            if mean_selected <= current_mean_f or rho > 0.2:\n                m = m_new.copy()\n            else:\n                alpha = 0.20\n                m = (1.0 - alpha) * m + alpha * m_new\n\n            # occasional random buffer add for diversity\n            if len(steps) > 0 and np.random.rand() < 0.12:\n                buffer.append((delta_m / (R + 1e-12)).copy())\n                if len(buffer) > buffer_max:\n                    buffer.pop(0)\n\n            # stagnation detection and aggressive restart (different thresholds)\n            stagn_window = 40 + 4 * n\n            if len(archive_F) > stagn_window:\n                best_idx = int(np.argmin(archive_F))\n                # if best hasn't improved in last stagn_window evals\n                if evals - best_idx > stagn_window:\n                    # aggressive restart: reset R to a moderate size, nudge mean toward best plus noise\n                    R = min(max(0.10 * range_mean, R * 0.5), R_max)\n                    pick = np.random.randint(len(archive_X))\n                    # shift mean halfway to a random archive point plus small gaussian\n                    m = 0.5 * m + 0.5 * archive_X[pick] + np.random.randn(n) * (0.05 * range_mean)\n                    # clear buffer and secant memory\n                    buffer = []\n                    secant_dx = []; secant_df = []\n                    # small perturbation to RMS to avoid locking\n                    s2_ema = s2_ema * (1.0 + 0.1 * np.random.rand(n))\n\n            # ensure mean stays inside bounds\n            m = np.clip(m, lb, ub)\n            R = float(np.clip(R, R_min, R_max))\n\n        # final fallback to best archive if needed\n        if x_opt is None:\n            if len(archive_X) > 0:\n                idx = int(np.argmin(archive_F))\n                x_opt = archive_X[idx].copy()\n                f_opt = float(archive_F[idx])\n            else:\n                x_opt = np.clip(m, lb, ub)\n                if evals < budget:\n                    f_opt = float(func(x_opt))\n                else:\n                    f_opt = float(f_opt)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm EDAR scored 0.208 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "d0f1f6fe-5082-456c-a464-0a4e9d995953", "operator": null, "metadata": {"aucs": [0.11092092954528765, 0.16101895440093372, 0.29893299258616957, 0.2626845167269849, 0.19700527687220193, 0.2698976984671061, 0.22384552085668785, 0.23567193201550918, 0.19004247188488432, 0.12705171672780435]}, "task_prompt": ""}
{"id": "77f843c8-de9c-48e1-9e48-048cbd91cfc7", "fitness": 0.356470270666716, "name": "SPARROW", "description": "SPARROW is a hybrid CMA-style heuristic that mixes a fast per-coordinate RMS scaling (s from an EMA of squared steps) with a learned low-rank search subspace U·S (SVD on a buffer of recent successful normalized steps, k ≈ ceil(√dim)) so sampling combines diagonal, subspace and isotropic moves while population size λ scales slowly with dimension (≈4+3·log(dim)). Sampling diversity is increased via mirrored pairs, occasional heavy‑tailed Cauchy jumps, and archive‑based differential moves (DE-style extra differences), with strict bound clipping and archive maintenance to reuse past points. Selection uses a temperature‑annealed softmax recombination of top candidates (temp from 1→0.2) and a trust-ish mechanism that compares predicted vs actual improvement (approximate local gradient from buffer) to adapt sigma conservatively and accept either full or partial mean shifts. Stability comes from CMA-like evolution paths (ps, pc), sigma control (cs, damps, chi_n), periodic reconditioning to compute an inverse‑sqrt covariance from diag+low‑rank decomposition, and simple stagnation handling (sigma inflation and nudging toward archive points) while always respecting the evaluation budget.", "code": "import numpy as np\n\nclass SPARROW:\n    \"\"\"\n    SPARROW: Subspace-Path-length-Archive-RMS Online Whitened search\n\n    Key ideas:\n      - Diagonal RMS per-coordinate scaling (fast adaptation) + learned low-rank subspace (SVD on successful steps).\n      - CMA-like path-length sigma adaptation (ps), mirrored sampling, occasional Cauchy jumps, and DE-style archive differences.\n      - Softmax (temperature-annealed) recombination; predicted-vs-actual ratio to adapt sigma conservatively and accept partial mean moves.\n      - Periodic full reconditioning of an approximate covariance (diag + U S^2 U^T) to compute a stable inverse-sqrt for better ps updates.\n      - Strict budget respecting, bound clipping, and stagnation handling (inflate sigma, nudge mean).\n    One-line: Hybrid diagonal+subspace CMA-style optimizer with archive-DE, mirrored & heavy-tailed sampling, softmax recombination and trust-ish sigma control.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population heuristics like CMA-ES\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # subspace dimension (low-rank)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (expect [-5,5] but respect func.bounds)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # strategy params\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        # log-weights\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # CMA-like constants\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        c1 = 2.0 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1 - c1, 2 * (mu_eff - 2 + 1 / mu_eff) / ((n + 2) ** 2 + mu_eff))\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.2 * np.mean(ub - lb)\n\n        # diagonal per-coordinate RMS (std-like)\n        s2_ema = np.ones(n)  # squared EMA\n        s = np.sqrt(s2_ema)\n\n        # subspace U (n x k) and scales S (k)\n        if self.k > 0:\n            rand = np.random.randn(n, self.k)\n            try:\n                Q, _ = np.linalg.qr(rand)\n                U = Q[:, :self.k]\n            except np.linalg.LinAlgError:\n                U = np.zeros((n, self.k))\n        else:\n            U = np.zeros((n, 0))\n        S = np.ones(self.k)\n\n        # evolution paths\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n\n        # buffers and archive\n        success_buffer = []   # store recent weighted steps y_w (shape n)\n        buffer_max = max(10 * self.k, 30)\n        archive_X = []\n        archive_F = []\n\n        # reconditioning\n        eig_every = max(50, 10 * n)\n        eigen_counter = 0\n        invsqrt_approx = np.eye(n)\n        have_invsqrt = False\n\n        # control probabilities\n        p_de = 0.18\n        F_de = 0.7\n        p_cauchy = 0.10\n        mirrored = True\n\n        # softmax temperature schedule\n        temp_init = 1.0\n        temp_final = 0.2\n\n        # stagnation tracking\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n        last_improve = 0\n\n        # initial evaluation of mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = float(func(xm))\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_opt = fm; x_opt = xm.copy()\n            last_improve = evals\n\n        # helper: approximate gradient from buffer (linear fit)\n        def approx_grad(buf_steps, buf_f):\n            if len(buf_steps) < 3:\n                return np.zeros(n)\n            Y = np.vstack(buf_steps)  # m x n\n            Ys = Y - np.mean(Y, axis=0, keepdims=True)\n            fvec = np.array(buf_f) - np.mean(buf_f)\n            reg = 1e-6 + 1e-2 * np.mean(np.var(Ys, axis=0))\n            try:\n                A = Ys.T @ Ys + reg * np.eye(n)\n                b = Ys.T @ fvec\n                g = np.linalg.solve(A, b)\n                return g\n            except np.linalg.LinAlgError:\n                return np.zeros(n)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            # ensure mirrored pairing even\n            if mirrored and current_lambda % 2 == 1 and current_lambda > 1:\n                current_lambda -= 1\n\n            # temperature annealing\n            t_frac = evals / max(1.0, budget)\n            temp = temp_init * (1.0 - t_frac) + temp_final * t_frac\n\n            # generate candidates\n            arx = np.zeros((current_lambda, n))\n            az = np.zeros((current_lambda, n))  # stored \"y\" proposals (x - m)/sigma\n            mut_meta = []\n\n            for k_idx in range(current_lambda):\n                # choose mixture of diagonal vs subspace vs isotropic\n                mode_rand = np.random.rand()\n                if (self.k > 0) and (mode_rand < 0.55):\n                    # subspace move\n                    zsub = np.random.randn(self.k)\n                    sub = (U @ (S * zsub))\n                    # combine with small diagonal noise\n                    diag_noise = (s * np.random.randn(n)) * 0.15\n                    y = 0.9 * sub + diag_noise\n                    mut = 'sub'\n                elif mode_rand < 0.85:\n                    # coordinate-scaled\n                    y = s * np.random.randn(n)\n                    mut = 'coord'\n                else:\n                    # isotropic\n                    y = np.random.randn(n)\n                    mut = 'iso'\n\n                # mirrored option\n                if mirrored and (k_idx % 2 == 1):\n                    y = -y\n\n                # occasional heavy-tailed jump\n                if np.random.rand() < p_cauchy:\n                    c = np.random.standard_cauchy()\n                    nrm = np.linalg.norm(y) + 1e-12\n                    y = (np.sign(c) * (np.abs(c) ** 0.8)) * (y / nrm) * np.mean(s)\n                    mut += '+cauchy'\n\n                x = m + sigma * y\n\n                # DE-style archive mutation\n                if np.random.rand() < p_de and len(archive_X) >= 2:\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    diff = archive_X[i1] - archive_X[i2]\n                    x = x + F_de * diff\n                    mut += '+de'\n\n                # clip and store\n                x = np.clip(x, lb, ub)\n                arx[k_idx] = x\n                az[k_idx] = y\n                mut_meta.append(mut)\n\n            # evaluate sequentially until budget or all candidates done\n            arfit = np.full(current_lambda, np.inf)\n            for k_idx in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[k_idx]\n                f = float(func(x))\n                evals += 1\n                arfit[k_idx] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                # cap archive\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = f; x_opt = x.copy(); last_improve = evals\n\n            # selection & softmax recombination (temperature scaled on negative ranks)\n            valid_idx = np.where(np.isfinite(arfit))[0]\n            if valid_idx.size == 0:\n                break\n            sorted_idx = valid_idx[np.argsort(arfit[valid_idx])]\n            ranks = np.arange(1, len(sorted_idx) + 1)\n            logits = - (ranks - 1) / max(1e-12, temp * np.sqrt(n))\n            exps = np.exp(logits - np.max(logits))\n            soft_w = exps / np.sum(exps)\n            take = min(mu, len(sorted_idx))\n            chosen = sorted_idx[:take]\n            weights_soft = soft_w[:take]\n            if weights_soft.sum() <= 0:\n                weights_soft = np.ones_like(weights_soft) / len(weights_soft)\n            else:\n                weights_soft = weights_soft / np.sum(weights_soft)\n\n            x_sel = arx[chosen]\n            y_sel = az[chosen]\n            f_sel = arfit[chosen]\n\n            # recombined mean shift (in x-space)\n            delta_m = np.sum(weights_soft[:, None] * (x_sel - m), axis=0)\n            m_candidate = m + delta_m\n\n            # predict improvement using local buffer\n            if len(success_buffer) >= 3:\n                # use last few evaluations' steps and fitnesses\n                buf_steps = success_buffer[-min(len(success_buffer), 20):]\n                # approximate fbuf by selecting last len(buf_steps) archive_F (coarse)\n                fbuf = archive_F[-len(buf_steps):] if len(archive_F) >= len(buf_steps) else []\n                ghat = approx_grad(buf_steps, fbuf)\n                predicted_drop = -np.dot(ghat, delta_m)\n            else:\n                predicted_drop = 0.0\n\n            # actual improvement estimate: compare mean of selected vs current mean eval\n            mean_selected_f = np.mean(f_sel) if len(f_sel) > 0 else f_opt\n            current_mean_f = archive_F[-1] if len(archive_F) > 0 else f_opt\n            actual_drop = max(0.0, current_mean_f - mean_selected_f)\n\n            # trust-ish ratio and sigma adaptation (conservative)\n            eps = 1e-10\n            rho = actual_drop / (abs(predicted_drop) + eps)\n            # path-length style sigma adaptation using ps (requires invsqrt)\n            # compute y_w in y-space (approx (m_candidate - m)/sigma)\n            y_w = delta_m / (sigma + 1e-20)\n            # apply invsqrt approximation: prefer full invsqrt if available\n            if have_invsqrt:\n                ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrt_approx @ y_w)\n            else:\n                # approximate by diag inverse using s\n                invdiag = 1.0 / (s + 1e-20)\n                ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n            # CMA-style sigma update based on ps\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n\n            # additional trust-ish sigma scaling based on rho\n            if predicted_drop > 1e-12 and rho > 0.8:\n                sigma *= 1.15\n            elif rho < 0.2:\n                sigma *= 0.85\n\n            # bound sigma\n            sigma = float(np.clip(sigma, 1e-12, 1e2 * np.mean(ub - lb) + 1e-12))\n\n            # acceptance of mean: accept fully if selects improved, else partial by rho\n            if (mean_selected_f <= current_mean_f) or (rho > 0.3):\n                m = np.clip(m_candidate, lb, ub)\n                accepted = True\n            else:\n                alpha = max(0.05, min(0.6, rho))\n                m = np.clip((1 - alpha) * m + alpha * m_candidate, lb, ub)\n                accepted = False\n\n            # update diagonal s via EMA on selected y's (in coordinate space)\n            if y_sel.shape[0] > 0:\n                # compute second moment of chosen steps (x - m)/sigma approximations\n                y2 = np.sum(weights_soft[:, None] * (y_sel ** 2), axis=0)\n                beta = 0.18\n                s2_ema = (1 - beta) * s2_ema + beta * (y2 + 1e-20)\n                s = np.sqrt(s2_ema + 1e-20)\n\n            # store weighted successful step into buffer (if actual improvement)\n            if actual_drop > 1e-12 and np.linalg.norm(delta_m) > 0:\n                # store in raw step space (x - m)/sigma style\n                success_buffer.append((delta_m / (sigma + 1e-20)).copy())\n                if len(success_buffer) > buffer_max:\n                    success_buffer.pop(0)\n\n            # update subspace U and S via SVD on buffer occasionally\n            if len(success_buffer) >= max(3, self.k) and (evals % max(1, min(10, n)) == 0):\n                Y = np.vstack(success_buffer)  # m x n (steps normalized)\n                # center rows\n                Yc = Y - np.mean(Y, axis=0, keepdims=True)\n                try:\n                    # compute SVD on transpose (n x m) cheaper if m small; use economy SVD\n                    U_new, svals, Vt = np.linalg.svd(Yc.T, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U = U_new[:, :k_take]\n                        S = (svals[:k_take] / (np.sqrt(max(1, Yc.shape[0])) + 1e-12)).copy()\n                        if U.shape[1] < self.k:\n                            pad = np.zeros((n, self.k - U.shape[1]))\n                            U = np.hstack([U, pad])\n                            S = np.concatenate([S, np.ones(self.k - len(S))])\n                except np.linalg.LinAlgError:\n                    pass\n\n            # occasional reconditioning: build approximate covariance and compute eigen-decomp\n            eigen_counter += current_lambda\n            if eigen_counter >= eig_every:\n                eigen_counter = 0\n                try:\n                    cov_approx = np.diag(s ** 2)\n                    if self.k > 0:\n                        cov_approx = cov_approx + (U @ np.diag(S ** 2) @ U.T)\n                    cov_approx = 0.5 * (cov_approx + cov_approx.T)\n                    Dvals, B = np.linalg.eigh(cov_approx)\n                    Dvals = np.maximum(Dvals, 1e-20)\n                    D_sqrt = np.sqrt(Dvals)\n                    invsqrt_approx = (B * (1.0 / D_sqrt)) @ B.T\n                    have_invsqrt = True\n                except np.linalg.LinAlgError:\n                    have_invsqrt = False\n                    invsqrt_approx = np.eye(n)\n\n            # stagnation handling: if no improvement for long window, inflate sigma and nudge mean\n            if evals - last_improve > max(50, 8 * n):\n                sigma *= 2.0\n                if len(archive_X) > 0:\n                    pick = np.random.randint(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                success_buffer = []\n                last_improve = evals\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # loop continues until budget exhausted\n\n        # final fallback if no x_opt (shouldn't happen)\n        if x_opt is None:\n            if len(archive_X) > 0:\n                idx = int(np.argmin(archive_F))\n                x_opt = archive_X[idx].copy()\n                f_opt = float(archive_F[idx])\n            else:\n                x_opt = np.clip(m, lb, ub)\n                if evals < budget:\n                    f_opt = float(func(x_opt))\n                else:\n                    f_opt = float(f_opt)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm SPARROW scored 0.356 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "d0f1f6fe-5082-456c-a464-0a4e9d995953", "operator": null, "metadata": {"aucs": [0.22344074446089113, 0.1791254370659412, 0.43050404992354485, 0.7271682689880842, 0.4254522551214509, 0.39902093166186436, 0.27306239425965617, 0.341086944569049, 0.3325230204818901, 0.2333186601347884]}, "task_prompt": ""}
{"id": "0f5dca95-c50e-45f9-861a-2154c14e41de", "fitness": "-inf", "name": "SALT", "description": "SALT is a hybrid heuristic that mixes a CMA-like generation (lambda/mu sized by log rules, evolution paths, covariance and sigma adaptation with periodic eigendecomposition every ~10*n) with surrogate-guided trust-region proposals and memory-accelerated Langevin probes in small random affine subspaces, using an archive and an LRU of successful unit directions to bias exploration. It fits a cheap separable quadratic surrogate to nearby archive points (requires many neighbors, uses diagonal curvature, clamps delta to trust_radius) and performs budget-aware short golden‑section line-searches (typically ≤6 evals) to intensify promising surrogate or Langevin directions. Global diversity is supported by DE-style difference mutations on offspring, occasional heavy‑tailed Cauchy jumps (prob ~0.08), aggressive step adaptation (grow=1.22, shrink=0.72, initial sigma/step = 0.25*domain_mean, guarded min/max), archive pruning, and small local Gaussian polishing. All operations are strictly budget-aware via safe_eval, with stagnation detection and soft restarts (up to a few restarts, then final polishing) to maintain robustness across Many Affine BBOB problems.", "code": "import numpy as np\n\nclass SALT:\n    \"\"\"\n    SALT: Surrogate-Accelerated Langevin Trust\n\n    One-line:\n      Hybrid surrogate-guided trust-region + CMA-like generations where, between\n      generations, memory-accelerated Langevin proposals are performed in small\n      random affine subspaces with aggressive step adaptation and cheap 1D\n      intensifications (short line-searches).\n\n    Key components:\n      - separable-quadratic surrogate proposals inside a trust-region (cheap local models)\n      - CMA-like generations (lambda, mu, evolution paths, rank-one/rank-mu updates)\n      - LRU memory of successful unit directions and Langevin-style subspace probes\n      - aggressive step adaptation (grow/shrink), short budget-aware line-searches\n      - archive for DE-style difference moves and surrogate fitting, strict budget enforcement\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=12, seed_offset=0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed + seed_offset)\n        self.memory_size = int(memory_size)\n\n        # default CMA-like sizing\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng = ub - lb\n        domain_mean = float(np.mean(rng))\n\n        # safe eval bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x\n\n        # initial incumbent\n        x_cur = np.random.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.array(x_best, dtype=float)\n\n        # CMA-like state\n        lam = max(1, self.lambda_)\n        mu = max(1, min(self.mu, lam))\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        c1 = 2 / ((n + 1.3) ** 2 + mu_eff)\n        cmu = min(1 - c1, 2 * (mu_eff - 2 + 1 / mu_eff) / ((n + 2) ** 2 + mu_eff))\n        damps = 1 + 2 * max(0, np.sqrt((mu_eff - 1) / (n + 1)) - 1) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        m = np.random.uniform(lb, ub)  # CMA mean (kept coordinated with x_cur sometimes)\n        sigma = max(1e-12, 0.25 * domain_mean)\n        C = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        eigen_eval_counter = 0\n        eig_every = max(1, int(10 * n))\n\n        # surrogate/trust params\n        trust_frac = 0.35\n        trust_radius = np.maximum(trust_frac * rng, 1e-9)\n        trust_min_frac = 1e-6\n        trust_max_frac = 2.0\n\n        # Langevin subspace / step adaptation params (inspired by MASTL)\n        step = 0.25 * domain_mean\n        min_step = 1e-6 * max(1.0, domain_mean)\n        grow = 1.22\n        shrink = 0.72\n        max_step = 5.0 * domain_mean\n\n        # memory and archive\n        dir_memory = []\n        archive_X = []\n        archive_F = []\n\n        # short budget-aware golden-section style line-search (direction d from x0)\n        def short_line_search(x0, f0, d, init_step, max_evals=6):\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            remaining_allowed = max(0, self.budget - evals)\n            if remaining_allowed <= 0:\n                return None, None\n            s = float(init_step)\n            # try +s\n            xa = clip(x0 + s * d)\n            fa, _ = safe_eval(xa)\n            if fa is None:\n                return None, None\n            # if +s improved\n            if fa < f0:\n                a, b = 0.0, s\n                fa_val, fb_val = f0, fa\n            else:\n                # try -s\n                xb = clip(x0 - s * d)\n                fb, _ = safe_eval(xb)\n                if fb is None:\n                    return None, None\n                if fb < f0:\n                    a, b = -s, 0.0\n                    fa_val, fb_val = fb, f0\n                else:\n                    return None, None\n            gr = (np.sqrt(5) - 1) / 2\n            c = b - gr * (b - a)\n            d_alpha = a + gr * (b - a)\n            xc = clip(x0 + c * d)\n            fc, _ = safe_eval(xc)\n            if fc is None:\n                return None, None\n            xd = clip(x0 + d_alpha * d)\n            fd, _ = safe_eval(xd)\n            if fd is None:\n                return None, None\n            best_f = f0\n            best_x = x0.copy()\n            for val, px in ((fa_val, xa if 'xa' in locals() else None),\n                            (fb_val, xb if 'xb' in locals() else None),\n                            (fc, xc), (fd, xd)):\n                try:\n                    if val is not None and val < best_f:\n                        best_f = val; best_x = px.copy()\n                except Exception:\n                    pass\n            iters = 0\n            remaining = max(0, self.budget - evals)\n            while remaining > 0 and iters < max_evals:\n                iters += 1\n                if fc < fd:\n                    b = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    c = b - gr * (b - a)\n                    xc = clip(x0 + c * d)\n                    fc, _ = safe_eval(xc)\n                    if fc is None:\n                        break\n                else:\n                    a = c\n                    c = d_alpha\n                    fc = fd\n                    d_alpha = a + gr * (b - a)\n                    xd = clip(x0 + d_alpha * d)\n                    fd, _ = safe_eval(xd)\n                    if fd is None:\n                        break\n                remaining = max(0, self.budget - evals)\n                for val, px in ((fc, xc), (fd, xd)):\n                    if val is not None and val < best_f:\n                        best_f = val; best_x = px.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # helper: separable quadratic surrogate fit (weighted least squares)\n        def try_surrogate_proposal(x_center):\n            # returns (f_model, x_model) or (None, None)\n            if len(archive_X) < max(2 * n + 1, 8 * n):\n                return None, None\n            X_arr = np.asarray(archive_X)\n            F_arr = np.asarray(archive_F)\n            dists = np.linalg.norm(X_arr - x_center, axis=1)\n            if np.all(np.isinf(dists)):\n                return None, None\n            idx = np.argsort(dists)[:min(len(X_arr), max(2 * n + 1, 8 * n))]\n            X_nei = X_arr[idx]\n            F_nei = F_arr[idx]\n            dx = X_nei - x_center\n            rows = dx.shape[0]\n            # design matrix: [1, linear terms, 0.5 * squared terms]\n            M = np.ones((rows, 1 + 2 * n))\n            M[:, 1:1 + n] = dx\n            M[:, 1 + n:1 + 2 * n] = 0.5 * (dx ** 2)\n            # weights by proximity (closer gets higher weight)\n            w = 1.0 / (dists[idx] + 1e-12)\n            w = w / (np.max(w) + 1e-12)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            b = W * F_nei\n            ridge = 1e-8 * np.eye(M.shape[1])\n            try:\n                sol, *_ = np.linalg.lstsq(A.T @ A + ridge, A.T @ b, rcond=None)\n                sol = sol.flatten()\n                a0 = sol[0]\n                b_lin = sol[1:1 + n]\n                h_diag = sol[1 + n:1 + 2 * n]\n                # regularize curvature\n                h_reg = np.copy(h_diag)\n                h_reg[h_reg < 1e-8] = 1e-8\n                delta = -b_lin / (h_reg + 1e-20)\n                delta_limited = np.clip(delta, -trust_radius, trust_radius)\n                x_model = clip(x_center + delta_limited)\n                return x_model\n            except Exception:\n                return None\n\n        # stagnation control\n        no_improve = 0\n        stagnation_limit = max(15, int(8 * np.log(1 + n)))\n        restarts = 0\n        max_restarts = 8\n\n        iteration = 0\n        # main loop\n        while evals < self.budget:\n            iteration += 1\n            improved = False\n\n            # attempt surrogate-based step from current mean m (if available)\n            x_model = try_surrogate_proposal(m)\n            if x_model is not None and evals < self.budget:\n                out = safe_eval(x_model)\n                if out is not None:\n                    f_model, x_model_eval = out\n                    archive_X.append(x_model_eval.copy()); archive_F.append(f_model)\n                    if f_model < f_cur - 1e-12:\n                        # accept surrogate-driven improvement\n                        x_prev = x_cur.copy()\n                        f_prev = f_cur\n                        x_cur = x_model_eval.copy()\n                        f_cur = f_model\n                        # update CMA-like mean and paths using this step as success\n                        y_w = (x_cur - m) / (sigma + 1e-20)\n                        m = x_cur.copy()\n                        ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n                        norm_ps = np.linalg.norm(ps)\n                        hsig = 1.0 if (norm_ps / np.sqrt(1 - (1 - cs) ** (2 * (evals / max(1, lam) + 1))) / chi_n) < (1.4 + 2 / (n + 1)) else 0.0\n                        pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n                        rank_one = np.outer(pc, pc)\n                        rank_mu = np.outer(y_w, y_w)\n                        C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n                        sigma *= np.exp((cs / damps) * (np.linalg.norm(ps) / chi_n - 1))\n                        # expand trust and step\n                        trust_radius = np.minimum(trust_radius * 1.4, trust_max_frac * rng)\n                        step = min(step * grow, max_step)\n                        improved = True\n                        no_improve = 0\n                    else:\n                        # surrogate failed: shrink trust conservatively\n                        trust_radius = np.maximum(trust_radius * 0.7, trust_min_frac * rng)\n                        step = max(step * shrink, min_step)\n\n            # if surrogate did not yield, perform a CMA-like generation\n            if not improved and evals < self.budget:\n                remaining = self.budget - evals\n                current_lambda = min(lam, remaining)\n                arz = np.random.randn(current_lambda, n)\n                ary = arz * D[np.newaxis, :]  # scale by D\n                ary = ary @ B.T\n                arx = m + sigma * ary\n                # DE-style difference moves from archive\n                p_de = 0.20\n                F_de = 0.8\n                for k in range(current_lambda):\n                    if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                        i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                        de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                        arx[k] = arx[k] + de_mut\n                    arx[k] = clip(arx[k])\n                # evaluate offspring until budget allows\n                arfit = np.full(current_lambda, np.inf)\n                for k in range(current_lambda):\n                    if evals >= self.budget:\n                        break\n                    out = safe_eval(arx[k])\n                    if out is None:\n                        break\n                    f_k, xk = out\n                    arfit[k] = f_k\n                    archive_X.append(xk.copy()); archive_F.append(f_k)\n                # selection & recombination if any evaluated\n                valid_idx = np.where(np.isfinite(arfit))[0]\n                if valid_idx.size > 0:\n                    idx_sorted = valid_idx[np.argsort(arfit[valid_idx])]\n                    sel = idx_sorted[:mu]\n                    x_sel = arx[sel]\n                    y_sel = (x_sel - m[np.newaxis, :]) / (sigma + 1e-20)\n                    m_old = m.copy()\n                    m = np.sum(weights[:, None] * x_sel, axis=0)\n                    y_w = np.sum(weights[:, None] * y_sel, axis=0)\n                    # update evolution paths and covariance\n                    ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (invsqrtC @ y_w)\n                    norm_ps = np.linalg.norm(ps)\n                    hsig = 1.0 if (norm_ps / np.sqrt(1 - (1 - cs) ** (2 * (evals / max(1, lam) + 1))) / chi_n) < (1.4 + 2 / (n + 1)) else 0.0\n                    pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mu_eff) * y_w\n                    rank_one = np.outer(pc, pc)\n                    rank_mu = np.zeros((n, n))\n                    for i in range(y_sel.shape[0]):\n                        yi = y_sel[i][:, None]\n                        rank_mu += weights[i] * (yi @ yi.T)\n                    C = (1 - c1 - cmu) * C + c1 * (rank_one + (1 - hsig) * cc * (2 - cc) * C) + cmu * rank_mu\n                    sigma *= np.exp((cs / damps) * (np.linalg.norm(ps) / chi_n - 1))\n                    # if any offspring improved incumbent, accept best and expand step\n                    best_off = np.min(arfit[valid_idx])\n                    if best_off < f_cur - 1e-12:\n                        idx_best = valid_idx[np.argmin(arfit[valid_idx])]\n                        x_best_off = arx[idx_best]\n                        # evaluate exact best (may already be evaluated)\n                        f_best_off = arfit[idx_best]\n                        x_prev = x_cur.copy()\n                        f_prev = f_cur\n                        x_cur = clip(x_best_off.copy())\n                        f_cur = float(f_best_off)\n                        # store direction\n                        dir_succ = x_cur - x_prev\n                        dn = np.linalg.norm(dir_succ)\n                        if dn > 0:\n                            dir_succ = dir_succ / dn\n                            dir_memory.insert(0, dir_succ.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                        step = min(step * grow, max_step)\n                        improved = True\n                        no_improve = 0\n                    else:\n                        step = max(step * shrink, min_step)\n\n            # After generation, run a block of memory-accelerated Langevin subspace probes around x_cur\n            if evals < self.budget:\n                # subspace dimension roughly n^(2/3)\n                k = max(1, int(np.ceil(n ** (2.0 / 3.0))))\n                probes = max(6, 3 * k)\n                # build basis reusing up to half from memory\n                use_mem = min(len(dir_memory), max(1, k // 2))\n                basis = np.zeros((n, 0))\n                if use_mem > 0:\n                    mem_sel = np.array(dir_memory[:use_mem])\n                    basis = np.column_stack((basis, mem_sel.T))\n                needed = k - basis.shape[1]\n                if needed > 0:\n                    R = np.random.randn(n, needed)\n                    if basis.size > 0:\n                        R = np.column_stack((basis, R))\n                    Q, _ = np.linalg.qr(R)\n                    basis = Q[:, :k]\n                else:\n                    Q, _ = np.linalg.qr(basis)\n                    basis = Q[:, :k]\n\n                # momentum from most recent memory\n                momentum = np.zeros(n)\n                if len(dir_memory) > 0:\n                    momentum = dir_memory[0] * 0.5\n\n                for p in range(probes):\n                    if evals >= self.budget:\n                        break\n                    coeffs = np.random.normal(scale=1.0, size=basis.shape[1])\n                    dir_sub = basis @ coeffs\n                    dn = np.linalg.norm(dir_sub)\n                    if dn == 0:\n                        continue\n                    dir_sub = dir_sub / dn\n                    alpha_m = 0.6\n                    candidate_dir = (alpha_m * momentum + (1 - alpha_m) * dir_sub)\n                    cdn = np.linalg.norm(candidate_dir)\n                    if cdn == 0:\n                        candidate_dir = dir_sub\n                        cdn = 1.0\n                    candidate_dir = candidate_dir / cdn\n                    # Langevin-type length (uniform + gaussian noise)\n                    noise = np.random.randn() * 0.15 * step\n                    length = np.random.uniform(-step, step) + noise\n                    x_prop = clip(x_cur + length * candidate_dir)\n                    f_prop, x_prop = safe_eval(x_prop)\n                    if f_prop is None:\n                        break\n                    if f_prop < f_cur - 1e-12:\n                        # success: store direction, intensify via short line-search\n                        dir_succ = x_prop - x_cur\n                        dn2 = np.linalg.norm(dir_succ)\n                        if dn2 > 0:\n                            dir_succ = dir_succ / dn2\n                            dir_memory.insert(0, dir_succ.copy())\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                        # limited short line search\n                        remaining_budget = max(0, self.budget - evals)\n                        line_budget = min(6, remaining_budget)\n                        f_line, x_line = short_line_search(x_cur, f_cur, dir_succ, init_step=abs(length) + 0.5 * step, max_evals=line_budget)\n                        if f_line is not None and f_line < f_prop - 1e-12:\n                            f_prop = f_line\n                            x_prop = x_line\n                        # accept\n                        x_prev = x_cur.copy()\n                        f_prev = f_cur\n                        x_cur = x_prop.copy()\n                        f_cur = f_prop\n                        improved = True\n                        no_improve = 0\n                        # adapt step aggressively\n                        step = min(step * grow, max_step)\n                        # update CMA mean slightly toward new incumbent\n                        m = 0.9 * m + 0.1 * x_cur\n                    else:\n                        # occasional focused short line-search even on failure\n                        if np.random.rand() < 0.06 and (self.budget - evals) >= 3:\n                            f_line, x_line = short_line_search(x_cur, f_cur, candidate_dir, init_step=step, max_evals=min(5, max(1, self.budget - evals)))\n                            if f_line is not None and f_line < f_cur - 1e-12:\n                                dir_succ = x_line - x_cur\n                                dn3 = np.linalg.norm(dir_succ)\n                                if dn3 > 0:\n                                    dir_succ = dir_succ / dn3\n                                    dir_memory.insert(0, dir_succ.copy())\n                                    if len(dir_memory) > self.memory_size:\n                                        dir_memory.pop()\n                                x_cur = x_line.copy()\n                                f_cur = f_line\n                                improved = True\n                                no_improve = 0\n                                step = min(step * grow, max_step)\n\n                # after probes adapt step if no improvement\n                if not improved:\n                    no_improve += 1\n                    step = max(step * shrink, min_step)\n                else:\n                    # local Gaussian polishing\n                    extras = min(6, max(1, int(np.ceil(np.log(1 + n)))))\n                    for _ in range(extras):\n                        if evals >= self.budget:\n                            break\n                        d = np.random.randn(n)\n                        d = d / (np.linalg.norm(d) + 1e-20)\n                        a = np.random.uniform(-0.5 * step, 0.5 * step)\n                        f_try, x_try = safe_eval(clip(x_cur + a * d))\n                        if f_try is None:\n                            break\n                        if f_try < f_cur - 1e-12:\n                            dir_succ = x_try - x_cur\n                            dn4 = np.linalg.norm(dir_succ)\n                            if dn4 > 0:\n                                dir_succ = dir_succ / dn4\n                                dir_memory.insert(0, dir_succ.copy())\n                                if len(dir_memory) > self.memory_size:\n                                    dir_memory.pop()\n                            x_cur = x_try.copy()\n                            f_cur = f_try\n\n            # periodic orthonormalization of CMA directions\n            eigen_eval_counter += 1\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # occasional heavy-tailed jump to escape traps\n            if evals < self.budget and (np.random.rand() < 0.08):\n                scale = 0.35 * np.maximum(rng, 1e-9)\n                jump = np.clip(np.random.standard_cauchy(size=n), -10, 10)\n                x_jump = clip(m + jump * scale)\n                out = safe_eval(x_jump)\n                if out is not None:\n                    f_jump, x_jump_eval = out\n                    if f_jump < f_cur - 1e-12:\n                        x_cur = x_jump_eval.copy()\n                        f_cur = f_jump\n                        m = 0.9 * m + 0.1 * x_cur\n                        step = min(step * grow, max_step)\n                        no_improve = 0\n\n            # archive bookkeeping\n            # (some evaluations already appended inside safe_eval; double-check)\n            # prune archive if too large\n            max_archive = max(2000, 50 * n)\n            if len(archive_X) > max_archive:\n                idx_sorted = np.argsort(archive_F)\n                keep_best = idx_sorted[:200]\n                rest_idx = idx_sorted[200:]\n                keep_rest = rest_idx[::max(1, len(rest_idx) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # stagnation handling and restarts\n            if no_improve >= stagnation_limit:\n                restarts += 1\n                if restarts > max_restarts:\n                    # final polishing: reduce step and try local tiny perturbations around best\n                    step = max(step * 0.5, min_step)\n                    tries = min(10, self.budget - evals)\n                    for _ in range(tries):\n                        if evals >= self.budget:\n                            break\n                        dx = np.random.randn(n) * (0.02 * domain_mean)\n                        out = safe_eval(clip(x_best + dx))\n                        if out is None:\n                            break\n                    # if still no improvement, break out\n                    if no_improve > stagnation_limit * 3:\n                        break\n                    no_improve = 0\n                else:\n                    # soft restart around best\n                    if x_best is not None:\n                        perturb = np.random.randn(n) * (0.4 * domain_mean)\n                        x_cur = clip(x_best + perturb)\n                        out = safe_eval(x_cur)\n                        if out is None:\n                            break\n                        f_cur, x_cur = out\n                        m = x_cur.copy()\n                    else:\n                        x_cur = np.random.uniform(lb, ub)\n                        out = safe_eval(x_cur)\n                        if out is None:\n                            break\n                        f_cur, x_cur = out\n                        m = x_cur.copy()\n                    dir_memory = []\n                    step = min(max_step, step * 1.8)\n                    no_improve = 0\n\n            # safeguard step and sigma\n            step = max(step, min_step)\n            sigma = max(sigma, 1e-12)\n\n            # early exit if extremely good\n            if f_best <= 1e-12:\n                break\n\n        # return best found\n        if x_best is None:\n            return float(f_cur), np.array(x_cur, dtype=float)\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 52, in clip, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (10,) \nOn line: return np.minimum(np.maximum(x, lb), ub)", "error": "In the code, line 52, in clip, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (10,) \nOn line: return np.minimum(np.maximum(x, lb), ub)", "parent_ids": "d0f1f6fe-5082-456c-a464-0a4e9d995953", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "fed2c949-ca57-4604-ac0e-55499ec95d55", "fitness": 0.2137643616519609, "name": "AMTRP", "description": "AMTRP maintains a small pool of anchors (n_anchors) with independent trust radii (initialized ~0.18*range) and temperatures to explore multiple basins in parallel, selecting anchors by fitness-biased probabilities and adapting trust by success/failure (trust_expand/trust_shrink, target_success). Candidate generation is a mixed subspace sampler: a learned low-rank projection P (k ~ ceil(sqrt(dim)/1.4)) is estimated from recent successful normalized steps (success_buf via incremental SVD/PCA) and combined with an orthogonal complement to produce concentrated proposals, augmented by mirrored Gaussian sampling, occasional isotropic Cauchy jumps (p_cauchy=0.10), DE-style difference mutations (p_de=0.16), and small combinatorial/co-ordinate perturbations for diversity. Local exploitation uses a lightweight RBF kernel-ridge surrogate on recent local archive points (up to ~30 samples) to propose low-cost moves evaluated under temperature-based acceptance, while per-dimension scales s are adaptively updated via an EMA of squared successful steps to bias anisotropic proposals. Robustness is enforced by global/local archives, bounded archive sizes, occasional coordinate sweeps for fine search, stagnation-handling reseeding and gentle randomization of P/anchors, and parameter choices that scale with dimension (population ~ pop_factor * log(dim), success_buf_max ~ O(sqrt(dim))).", "code": "import numpy as np\n\nclass AMTRP:\n    \"\"\"\n    Adaptive Mixture Trust Region with Projections (AMTRP)\n\n    Key ideas (novel combination):\n    - Maintain an anchor pool (a few local centers) with independent trust radii and temperatures.\n    - Learn a low-rank projection subspace from recent successful steps via incremental PCA-like SVD\n      (small buffer) and use it to generate concentrated subspace proposals and orthogonal exploration.\n    - Use small mirrored Gaussian populations, occasional isotropic Cauchy (Lévy) jumps, and DE-style\n      differences drawn from a global archive.\n    - Fit a local RBF surrogate (kernel ridge) on recent points near a chosen anchor to propose a low-cost\n      exploitation trial; accept using predicted improvement + trust heuristics.\n    - Do cheap coordinate/block sweeps periodically to exploit separable structure.\n    - Adaptive per-dimension scaling via EMA of squared steps and trust-radii adaptation via success rates.\n    - Mild restarts / anchor reallocation on stagnation.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, anchors=3, pop_factor=2.5):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(int(seed))\n\n        # population size roughly ~ pop_factor * log(dim)\n        self.lambda_ = max(6, int(np.ceil(np.log(max(2, self.dim)) * pop_factor)))\n        # anchors: multiple local centers to keep multimodality\n        self.n_anchors = max(1, int(anchors))\n        # low-rank subspace rank\n        self.k = max(1, int(np.clip(int(np.ceil(np.sqrt(self.dim) / 1.4)), 1, self.dim)))\n        # buffers\n        self.success_buf_max = max(20, 6 * int(np.ceil(np.sqrt(self.dim))))\n        self.local_archive_max = 160\n        self.global_archive_max = 5000\n\n        # probabilities and scales\n        self.p_cauchy = 0.10\n        self.cauchy_scale = 1.0\n        self.p_de = 0.16\n        self.F_de = 0.5\n        self.mirrored = True\n\n        # adaptation\n        self.s_ema_rate = 0.15\n        self.trust_expand = 1.18\n        self.trust_shrink = 0.85\n        self.target_success = 0.2\n\n        # stagnation thresholds\n        self.stagn_limit = max(25, int(8 * self.dim / max(1, self.n_anchors)))\n        self.reseed_prob = 0.25\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (-5 .. 5 typically) but read from func\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center(s)\n        anchors_x = [np.random.uniform(lb, ub) for _ in range(self.n_anchors)]\n        anchors_f = [np.inf] * self.n_anchors\n        anchors_trust = [0.18 * np.mean(ub - lb)] * self.n_anchors  # trust radius per anchor\n        anchors_temp = [1.0] * self.n_anchors\n\n        # global state\n        s = np.ones(n)               # per-dim scale\n        sigma = 0.14 * np.mean(ub - lb)\n        # orthonormal rotation basis: start identity\n        R = np.eye(n)\n        # low-rank projection matrix (n x k)\n        P = np.eye(n)[:, :self.k].copy()\n\n        # archives and buffers\n        success_buf = []\n        local_archive_X = []\n        local_archive_F = []\n        global_archive_X = []\n        global_archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # Evaluate each initial anchor at least once up to budget\n        for i in range(self.n_anchors):\n            if evals >= budget:\n                break\n            x = np.clip(anchors_x[i], lb, ub)\n            f = func(x)\n            evals += 1\n            anchors_f[i] = float(f)\n            if f < f_opt:\n                f_opt = float(f); x_opt = x.copy()\n            global_archive_X.append(x.copy()); global_archive_F.append(f)\n            local_archive_X.append(x.copy()); local_archive_F.append(f)\n        last_improv = evals\n\n        gen = 0\n        # main loop:\n        while evals < budget:\n            gen += 1\n            # pick an anchor by fitness-biased probability (better anchors more often)\n            ranks = np.argsort(anchors_f)\n            # softmax on negative rank to favor best ones\n            invrank = -np.arange(len(ranks))\n            probs = np.exp(0.6 * (len(ranks) - np.arange(1, len(ranks) + 1)))\n            probs = probs / np.sum(probs)\n            a_idx = np.random.choice(len(anchors_x), p=probs)\n            anchor = anchors_x[a_idx].copy()\n            anchor_f = anchors_f[a_idx]\n            trust = anchors_trust[a_idx]\n            temp = anchors_temp[a_idx]\n\n            # generate population in a mixed subspace: projected (P) and orthogonal complement\n            lam = min(self.lambda_, budget - evals)\n            if lam <= 0:\n                break\n            # draw gaussian in reduced coords and complement\n            Z_sub = np.random.randn(lam, self.k)\n            Z_orth = np.random.randn(lam, n - self.k) if (n - self.k) > 0 else np.zeros((lam, 0))\n            Xcand = np.zeros((lam, n))\n            cand_local_steps = []\n            for i in range(lam):\n                z_sub = Z_sub[i]\n                z_orth = Z_orth[i] if (n - self.k) > 0 else np.array([])\n                # mirrored\n                if self.mirrored and (i % 2 == 1):\n                    z_sub = -z_sub\n                    z_orth = -z_orth\n                # construct in original space: subspace component + orth component\n                sub_comp = P @ z_sub  # (n,)\n                if (n - self.k) > 0:\n                    # create orth complement basis Q via QR of a random matrix projected orthogonally to P\n                    # But to keep cheap, build complement vectors once per gen (below)\n                    pass\n                # we'll create orth complement Q once per generation\n                # scaled local step in basis before rotation\n                # compose pre-rotation local vector (in original coordinates)\n                local_vec = sub_comp.copy()\n                if (n - self.k) > 0:\n                    # generate orth complement basis Qgen per generation\n                    # we'll fill later\n                    local_vec = local_vec  # placeholder\n                cand_local_steps.append(local_vec)\n\n            # Build orth complement basis once for generation if needed\n            if n - self.k > 0:\n                # make random complement and orthonormalize against P\n                rnd = np.random.randn(n, n - self.k)\n                proj = P @ (P.T @ rnd)\n                comp = rnd - proj\n                # orthonormalize comp\n                Qc, _ = np.linalg.qr(comp)\n                Qc = Qc[:, : (n - self.k)]\n                # now form candidates combining subspace and orth components\n                for i in range(lam):\n                    zs = Z_sub[i]\n                    zo = Z_orth[i]\n                    vec = P @ zs + Qc @ zo\n                    cand_local_steps[i] = vec\n            else:\n                for i in range(lam):\n                    zs = Z_sub[i]\n                    cand_local_steps[i] = P @ zs\n\n            # Now scale local steps elementwise by s and by trust radius (to control magnitude)\n            Xcand = np.zeros((lam, n))\n            Fvals = np.full(lam, np.inf)\n            for i in range(lam):\n                base = cand_local_steps[i]\n                # normalize direction and scale to trust radius times gaussian magnitude\n                norm = np.linalg.norm(base) + 1e-12\n                direction = base / norm\n                # magnitude drawn from normal with mean trust and per-dim scaling\n                mag = np.random.randn() * (trust / 1.8)  # keep typical steps inside trust\n                step = direction * mag * s  # elementwise scaling\n                # occasional Cauchy global jump\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    step = (direction * r * np.mean(s) * trust)\n                x = anchor + step\n                # DE-like mutation using two archive points\n                if (np.random.rand() < self.p_de) and (len(global_archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(global_archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (global_archive_X[i1] - global_archive_X[i2])\n                    x = x + de_mut\n                # small block-wise shuffle for combinatorial bias: swap random pair of coords with small prob\n                if np.random.rand() < 0.08:\n                    i1, i2 = np.random.choice(n, size=2, replace=False)\n                    tmp = x[i1]; x[i1] = x[i2]; x[i2] = tmp\n                x = np.clip(x, lb, ub)\n                Xcand[i] = x\n\n            # Evaluate candidates (counting budget)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                # update archives\n                global_archive_X.append(xi.copy()); global_archive_F.append(fi)\n                local_archive_X.append(xi.copy()); local_archive_F.append(fi)\n                if len(global_archive_X) > self.global_archive_max:\n                    global_archive_X.pop(0); global_archive_F.pop(0)\n                if len(local_archive_X) > self.local_archive_max:\n                    local_archive_X.pop(0); local_archive_F.pop(0)\n                # update best\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy(); last_improv = evals\n\n            # selection: pick top mu from candidates\n            mu = max(1, lam // 3)\n            sel_idx = np.argsort(Fvals)[:mu]\n            X_sel = Xcand[sel_idx]\n            F_sel = Fvals[sel_idx]\n\n            # recombine to form a proposed new anchor position (weighted mean with fitness weights)\n            # use rank-based weights but based on functional improvement relative to anchor\n            rel = (F_sel - anchor_f)\n            # convert to positive weights: better (smaller f) gets larger weight\n            scores = np.maximum(1e-12, -rel + np.max(-rel) + 1e-6)\n            w = scores / np.sum(scores)\n            m_proposed = np.sum(w[:, None] * X_sel, axis=0)\n\n            # Try local surrogate: fit RBF kernel ridge on recent points near anchor\n            # Use up to m_rbf samples from local_archive_X sorted by distance to anchor\n            if (len(local_archive_X) >= 6) and (evals < budget):\n                Xs = np.vstack(local_archive_X[-min(len(local_archive_X), 80):])\n                Fs = np.array(local_archive_F[-min(len(local_archive_F), 80):])\n                # compute distances\n                dists = np.linalg.norm(Xs - anchor, axis=1)\n                order = np.argsort(dists)\n                m_rbf = min(30, len(order))\n                idxs = order[:m_rbf]\n                Xfit = Xs[idxs]\n                Ffit = Fs[idxs]\n                # RBF lengthscale heuristic\n                if m_rbf > 1:\n                    pair_d = np.mean(np.linalg.norm(Xfit - Xfit.mean(axis=0), axis=1)) + 1e-12\n                    length = max(1e-3, pair_d)\n                else:\n                    length = 1.0\n                # Kernel matrix\n                D2 = np.sum((Xfit[:, None, :] - Xfit[None, :, :]) ** 2, axis=2)\n                K = np.exp(-0.5 * D2 / (length ** 2))\n                # regularize and solve for alpha in ridge regression\n                lam_ridge = 1e-6 * np.var(Ffit) + 1e-8\n                try:\n                    alpha = np.linalg.solve(K + lam_ridge * np.eye(m_rbf), Ffit)\n                    # predict for m_proposed and anchor-neighbor points to estimate predicted improvement\n                    k_vec = np.exp(-0.5 * np.sum((Xfit - m_proposed) ** 2, axis=1) / (length ** 2))\n                    pred = float(k_vec @ alpha)\n                    k_anchor = np.exp(-0.5 * np.sum((Xfit - anchor) ** 2, axis=1) / (length ** 2))\n                    pred_anchor = float(k_anchor @ alpha)\n                    # if predicted improvement significant, evaluate surrogate proposal\n                    if pred < pred_anchor - 1e-8 and evals < budget:\n                        probe = np.clip(m_proposed, lb, ub)\n                        fp = func(probe)\n                        evals += 1\n                        global_archive_X.append(probe.copy()); global_archive_F.append(fp)\n                        local_archive_X.append(probe.copy()); local_archive_F.append(fp)\n                        if fp < f_opt:\n                            f_opt = float(fp); x_opt = probe.copy(); last_improv = evals\n                        # acceptance by trust and temperature\n                        if fp <= anchor_f or (np.random.rand() < np.exp(-(fp - anchor_f) / max(1e-12, temp))):\n                            anchors_x[a_idx] = probe.copy()\n                            anchors_f[a_idx] = float(fp)\n                            # push step to success buffer (normalized local)\n                            step_local = (probe - anchor) / (trust + 1e-12)\n                            success_buf.append(step_local.copy())\n                            if len(success_buf) > self.success_buf_max:\n                                success_buf.pop(0)\n                            # expand trust if improved\n                            anchors_trust[a_idx] = min(2.0 * np.mean(ub - lb), anchors_trust[a_idx] * self.trust_expand)\n                        else:\n                            anchors_trust[a_idx] *= self.trust_shrink\n                    else:\n                        # fallback: directly consider moving anchor towards m_proposed (without eval if budget low)\n                        if evals < budget:\n                            xm = np.clip(m_proposed, lb, ub)\n                            fm = func(xm)\n                            evals += 1\n                            global_archive_X.append(xm.copy()); global_archive_F.append(fm)\n                            local_archive_X.append(xm.copy()); local_archive_F.append(fm)\n                            if fm < f_opt:\n                                f_opt = float(fm); x_opt = xm.copy(); last_improv = evals\n                            # accept based on temperature / improvement\n                            if fm <= anchor_f or (np.random.rand() < np.exp(-(fm - anchor_f) / max(1e-12, temp))):\n                                anchors_x[a_idx] = xm.copy(); anchors_f[a_idx] = float(fm)\n                                anchors_trust[a_idx] = min(2.0 * np.mean(ub - lb), anchors_trust[a_idx] * self.trust_expand)\n                                success_buf.append((xm - anchor) / (anchors_trust[a_idx] + 1e-12))\n                                if len(success_buf) > self.success_buf_max:\n                                    success_buf.pop(0)\n                            else:\n                                anchors_trust[a_idx] *= self.trust_shrink\n                except np.linalg.LinAlgError:\n                    # ignore surrogate if numerical issues\n                    pass\n            else:\n                # no surrogate fit possible, directly test m_proposed with small probability\n                if evals < budget and np.random.rand() < 0.45:\n                    xm = np.clip(m_proposed, lb, ub)\n                    fm = func(xm)\n                    evals += 1\n                    global_archive_X.append(xm.copy()); global_archive_F.append(fm)\n                    local_archive_X.append(xm.copy()); local_archive_F.append(fm)\n                    if fm < f_opt:\n                        f_opt = float(fm); x_opt = xm.copy(); last_improv = evals\n                    if fm <= anchor_f or (np.random.rand() < np.exp(-(fm - anchor_f) / max(1e-12, temp))):\n                        anchors_x[a_idx] = xm.copy(); anchors_f[a_idx] = float(fm)\n                        anchors_trust[a_idx] = min(2.0 * np.mean(ub - lb), anchors_trust[a_idx] * self.trust_expand)\n                        success_buf.append((xm - anchor) / (anchors_trust[a_idx] + 1e-12))\n                        if len(success_buf) > self.success_buf_max:\n                            success_buf.pop(0)\n                    else:\n                        anchors_trust[a_idx] *= self.trust_shrink\n\n            # Update per-dim scales s using EMA of squared normalized local steps from success_buf\n            if len(success_buf) > 0:\n                Y = np.vstack(success_buf[-min(len(success_buf), self.success_buf_max):])\n                v2 = np.mean(Y ** 2, axis=0)\n                s2 = (1.0 - self.s_ema_rate) * (s ** 2) + self.s_ema_rate * (v2 + 1e-20)\n                s = np.sqrt(s2)\n                s = np.clip(s, 1e-6, 1e3)\n\n            # Update global projection P occasionally using small SVD on success buffer (incremental PCA style)\n            if (len(success_buf) >= max(6, self.k)) and (gen % max(1, int(2 + n / 18)) == 0):\n                Ymat = np.vstack(success_buf[-min(len(success_buf), self.success_buf_max):])\n                Yc = Ymat - np.mean(Ymat, axis=0, keepdims=True)\n                # small SVD on Yc (samples x dims) -> get top-k right singular vectors\n                try:\n                    U_s, S_s, Vt = np.linalg.svd(Yc, full_matrices=False)\n                    V = Vt.T\n                    topk = min(self.k, V.shape[1])\n                    P_new = V[:, :topk]\n                    # gently mix projection\n                    beta = 0.18\n                    # re-orthonormalize mix\n                    M = np.zeros((n, topk))\n                    M[:] = (1.0 - beta) * P[:, :topk] + beta * P_new\n                    Q, _ = np.linalg.qr(M)\n                    P = Q[:, :topk]\n                except np.linalg.LinAlgError:\n                    # small random drift\n                    P = P + 0.01 * np.random.randn(n, self.k)\n                    Q, _ = np.linalg.qr(P)\n                    P = Q[:, :self.k]\n\n            # Stagnation handling: if no improvement for long, re-seed anchors / perturb P\n            if (evals - last_improv) > self.stagn_limit:\n                # pick worst anchor and reset it near current best with jitter\n                worst = int(np.argmax(anchors_f))\n                if x_opt is not None:\n                    anchors_x[worst] = np.clip(x_opt + 0.1 * np.mean(ub - lb) * np.random.randn(n), lb, ub)\n                    anchors_f[worst] = float(f_opt)\n                else:\n                    anchors_x[worst] = np.random.uniform(lb, ub)\n                    anchors_f[worst] = np.inf\n                anchors_trust[worst] = 0.2 * np.mean(ub - lb)\n                # randomize P a little\n                P = P + 0.06 * np.random.randn(n, self.k)\n                Q, _ = np.linalg.qr(P)\n                P = Q[:, :self.k]\n                # maybe reseed some global archive entries\n                if np.random.rand() < self.reseed_prob and len(global_archive_X) > 0:\n                    # shuffle archive subset\n                    subset = min(5, len(global_archive_X))\n                    for _ in range(subset):\n                        ii = np.random.randint(len(global_archive_X))\n                        global_archive_X[ii] = np.random.uniform(lb, ub)\n                        global_archive_F[ii] = np.inf\n                last_improv = evals\n\n            # occasional coordinate-wise sweep for fine-grained exploitation\n            if gen % max(6, int(np.ceil(n / 4))) == 0 and evals < budget:\n                # pick an anchor and try tiny coordinate steps\n                for c in range(min(n, 6)):\n                    if evals >= budget:\n                        break\n                    coord = np.random.randint(n)\n                    anchor = anchors_x[a_idx].copy()\n                    step = np.zeros(n)\n                    step[coord] = 0.6 * anchors_trust[a_idx] * (np.random.choice([-1.0, 1.0]))\n                    probe = np.clip(anchor + step, lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    global_archive_X.append(probe.copy()); global_archive_F.append(fp)\n                    local_archive_X.append(probe.copy()); local_archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = float(fp); x_opt = probe.copy(); last_improv = evals\n                    if fp < anchors_f[a_idx] or (np.random.rand() < np.exp(-(fp - anchors_f[a_idx]) / max(1e-12, anchors_temp[a_idx]))):\n                        anchors_x[a_idx] = probe.copy()\n                        anchors_f[a_idx] = float(fp)\n                        anchors_trust[a_idx] = min(2.0 * np.mean(ub - lb), anchors_trust[a_idx] * self.trust_expand)\n                        success_buf.append((probe - anchor) / (anchors_trust[a_idx] + 1e-12))\n                        if len(success_buf) > self.success_buf_max:\n                            success_buf.pop(0)\n                    else:\n                        anchors_trust[a_idx] *= self.trust_shrink\n\n            # cool anchor temperatures slowly\n            anchors_temp = [max(1e-6, t * 0.996) for t in anchors_temp]\n\n            # safety: keep archives bounded\n            if len(global_archive_X) > self.global_archive_max:\n                global_archive_X = global_archive_X[-self.global_archive_max:]\n                global_archive_F = global_archive_F[-self.global_archive_max:]\n            if len(local_archive_X) > self.local_archive_max:\n                local_archive_X = local_archive_X[-self.local_archive_max:]\n                local_archive_F = local_archive_F[-self.local_archive_max:]\n\n            # stop if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AMTRP scored 0.214 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "dd5952f4-6c1c-4ad0-88fc-9c8573e0aa0f", "operator": null, "metadata": {"aucs": [0.07925486633351331, 0.15318135430238633, 0.283421264966185, 0.28387792908131826, 0.22843918980416622, 0.3161381684578284, 0.23774561497135438, 0.22373088622730397, 0.21639649870957756, 0.11545784366597533]}, "task_prompt": ""}
{"id": "ebe61591-30b2-40c7-ade3-c443f33190b4", "fitness": "-inf", "name": "AMMS", "description": "AMMS is a hybrid multi-scale memetic search that generates a small population (lambda ~ pop_factor * log(dim)) of candidates using RMSprop-normalized Gaussian steps, mirrored sampling and occasional Student-t heavy-tail draws (p_t=0.10, df=3) and augments them with DE‑style best-difference mutations (p_de=0.22, F_de=0.7) and occasional within-block coordinate permutations (p_blockswap=0.09) to inject both global and local diversity. Per-coordinate adaptation is RMSProp-like (g2 with s_lr=0.12 → s = 1/sqrt(g2)) while the global step-size sigma is adapted multiplicatively on the log-scale toward a relatively high target success rate (sigma_target=0.28, sigma_lr=0.6) to control exploration-exploitation. The algorithm learns a low-rank search subspace online via an exponentially forgotten covariance (subspace_forget=0.94, k ≈ k_factor*sqrt(dim)), mixes the top-k eigenvectors into an orthonormal rotation R (subspace_mix=0.22) to bias proposals, and runs a robust 1-D quadratic surrogate along the principal direction (Huber-weighted fit, periodic probes) to exploit found structure. Elitist selection (mu = lam//3) with softmax recombination, logistic Metropolis-like acceptance, success buffers and bounded local/global archives provide memory for recombination and DE steps, and stagnation triggers reheating/perturbation (reheat_factor=1.8) and mild restarts to recover from premature convergence.", "code": "import numpy as np\n\nclass AMMS:\n    \"\"\"\n    Adaptive Multi-Scale Memetic Search (AMMS)\n\n    Short summary of main tunable parameters (from the original AS3 to identify and change):\n      - population size / lambda (controls parallel sampling)\n      - per-coordinate scale learning rate (s_ema_rate vs RMSprop alpha)\n      - global step-size adaptation rules (sigma_inflate/deflate vs multiplicative schedule)\n      - heavy-tail probability and distribution (p_cauchy vs p_t with df)\n      - DE-like probability and factor (p_de, F_de)\n      - subspace rank k and subspace mixing strength (subspace_mix)\n      - block-swap probability (p_blockswap) and stagnation controls\n      - surrogate probe frequency and window sizes\n\n    This implementation intentionally changes many equations and parameterizations:\n      - Uses Student-t heavy tails (df=3) with different probability.\n      - Updates per-coordinate scales with an RMSProp-like denominator and explicit learning rate.\n      - Adapts sigma via an exponential rule using psucc-target feedback.\n      - Learns a low-rank subspace incrementally with exponential forgetting (online covariance).\n      - Proposes samples mirrored and non-mirrored; uses DE diffs from the best archive member.\n      - Uses a 1D trust-region quadratic surrogate along the top learned direction, but with robust Huber weighting.\n      - Block structure is randomized initially and can be partially re-sampled on stagnation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, pop_factor=2.5, k_factor=0.6):\n        \"\"\"\n        Args:\n            budget (int): evaluation budget (must not be exceeded).\n            dim (int): problem dimension.\n            seed (int|None): RNG seed.\n            pop_factor (float): multiplier controlling population size ~ pop_factor * log(dim).\n            k_factor (float): fraction of sqrt(dim) to use for low-rank k (smaller than AS3).\n        \"\"\"\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population size (different equation)\n        self.lambda_ = max(4, int(np.ceil(pop_factor * np.log(max(2, self.dim + 1)))))\n\n        # subspace rank (smaller proportion)\n        self.k = max(1, min(self.dim, int(np.ceil(k_factor * np.sqrt(self.dim)))))\n\n        # heavy-tail sampling: Student-t df (changed from Cauchy)\n        self.p_t = 0.10         # probability of heavy tail\n        self.t_df = 3.0        # degrees of freedom for Student-t (heavier than Gaussian but lighter than Cauchy)\n        self.t_scale = 1.2\n\n        # DE-like mutation parameters (use best-based difference)\n        self.p_de = 0.22\n        self.F_de = 0.7\n        self.use_best_de = True  # difference uses current best and a random archive element\n\n        # block permutation specifics (randomized initial blocks)\n        # number of blocks is different heuristic: round(sqrt(dim)/2) (coarser blocks)\n        self.n_blocks = max(1, int(np.round(np.sqrt(self.dim) / 2.0)))\n        # block-swap probability (slightly lower)\n        self.p_blockswap = 0.09\n\n        # mirrored sampling toggle (mix mirrored and non-mirrored)\n        self.mirrored = True\n        self.mirrored_frac = 0.55  # fraction of samples that are mirrored (instead of simple alternating)\n\n        # per-coordinate scale adaptation (RMSProp-like)\n        self.s_lr = 0.12           # learning rate for RMSprop accumulator (different from s_ema_rate)\n        self.s_eps = 1e-8\n\n        # sigma adaptation: target success and learning rate for log-sigma update (multiplicative)\n        self.sigma = None  # initial set later relative to bounds\n        self.sigma_target = 0.28   # target success rate (different from AS3 ~0.2)\n        self.sigma_lr = 0.6        # learning rate for log-sigma update\n\n        # subspace mixing: incremental covariance forgetting factor and mix strength\n        self.subspace_forget = 0.94   # exponential forgetting for online covariance (different)\n        self.subspace_mix = 0.22      # how strongly to rotate towards learned subspace (larger than AS3)\n\n        # success buffer & archives limits\n        self.success_buf_max = max(15, 5 * int(np.ceil(np.sqrt(self.dim))))\n        self.global_archive_max = 4000\n        self.local_archive_max = 300\n\n        # stagnation and restart\n        self.stagn_limit = max(30, int(5 * self.dim / max(1, self.n_blocks)))\n        self.reheat_factor = 1.8\n\n        # surrogate parameters\n        self.surrogate_freq = max(9, int(5 + self.dim / 6))\n        self.surrogate_window = 48\n\n        # internal small clamp\n        self.min_sigma = 1e-12\n        self.max_sigma_factor = 6.0\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (expect -5..5 but read from func)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initialize center uniformly in bounds\n        m = self.rng.uniform(lb, ub)\n\n        # set sigma relative to range (different initial constant)\n        self.sigma = 0.10 * np.linalg.norm(ub - lb) / np.sqrt(n) + 1e-12\n\n        # per-coordinate scaling accumulator (RMSProp denom) and scale vector s\n        g2 = np.ones(n) * 1e-6   # running second moment of local steps\n        s = np.ones(n)           # local scales (derived from g2)\n\n        # orthonormal rotation R and low-rank basis U (start as identity first k columns)\n        R = np.eye(n)\n        U = np.eye(n)[:, :self.k].copy()\n\n        # incremental covariance for subspace learning (we store small matrix in original coordinates)\n        Cov = np.zeros((n, n))\n        cov_count = 0.0\n\n        # archives/buffers\n        success_buf = []\n        local_archive_X = []\n        local_archive_F = []\n        global_archive_X = []\n        global_archive_F = []\n\n        # tracking\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial center evaluation (counts)\n        x0 = np.clip(m, lb, ub)\n        f0 = func(x0)\n        evals += 1\n        f_center = float(f0)\n        f_opt = float(f0)\n        x_opt = x0.copy()\n\n        global_archive_X.append(x0.copy()); global_archive_F.append(f0)\n        local_archive_X.append(x0.copy()); local_archive_F.append(f0)\n        last_improv = evals\n        stagn_count = 0\n        gen = 0\n\n        # Helper: sample Student-t scaled vector in local coordinates\n        def sample_heavy_local():\n            # Student's t with df self.t_df, scaled and then modulated by s\n            t = self.rng.standard_t(self.t_df, size=n)\n            return self.t_scale * t * s\n\n        # Helper: propose population in one generation but ensure not to evaluate more than budget\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)  # will be conservative; we'll still evaluate at most lam\n            Z = self.rng.randn(lam, n)\n            Xcand = np.zeros((lam, n))\n            local_pre_rot = np.zeros((lam, n))  # local pre-rotation vectors (for learning)\n            Fvals = np.full(lam, np.inf)\n\n            # decide which indices will be mirrored according to mirrored_frac\n            if self.mirrored:\n                mirrored_flags = self.rng.rand(lam) < self.mirrored_frac\n                # but ensure pairs are mirrored where possible\n                # simple: if odd and last mirrored, invert last to keep variety\n            else:\n                mirrored_flags = np.zeros(lam, dtype=bool)\n\n            for i in range(lam):\n                z = Z[i].copy()\n                if mirrored_flags[i]:\n                    z = -z  # mirrored variant\n\n                # local scaled vector using RMSprop-like normalization\n                # derive local step: normalize by sqrt(g2) to avoid runaway dims, then scale by s\n                denom = np.sqrt(g2 + self.s_eps)\n                y_local = (z / denom) * s\n\n                # occasional heavy-tail sample replacing y_local\n                if self.rng.rand() < self.p_t:\n                    y_local = sample_heavy_local()\n\n                # rotate to original coordinates\n                y_rot = R.dot(y_local)\n\n                # base candidate\n                x = m + self.sigma * y_rot\n\n                # DE-style best-difference mutation (use global archive best)\n                if (self.rng.rand() < self.p_de) and (len(global_archive_X) >= 2):\n                    if self.use_best_de:\n                        # choose a random archive member different from best\n                        best_idx = int(np.argmin(global_archive_F))\n                        idx_rand = self.rng.randint(0, len(global_archive_X))\n                        # avoid identical\n                        if idx_rand == best_idx:\n                            idx_rand = (idx_rand + 1) % len(global_archive_X)\n                        de_mut = self.F_de * (global_archive_X[best_idx] - global_archive_X[idx_rand])\n                    else:\n                        i1, i2 = self.rng.choice(len(global_archive_X), size=2, replace=False)\n                        de_mut = self.F_de * (global_archive_X[i1] - global_archive_X[i2])\n                    x = x + de_mut\n\n                # block swap: randomly permute entries within one random block with some probability\n                if (self.rng.rand() < self.p_blockswap) and (self.n_blocks > 0):\n                    # create blocks on the fly deterministically from dimension (randomized initial partition)\n                    # blocks are contiguous segments (but offsets randomized by gen to vary structure)\n                    block_sizes = []\n                    base = n // self.n_blocks\n                    rem = n % self.n_blocks\n                    for b in range(self.n_blocks):\n                        block_sizes.append(base + (1 if b < rem else 0))\n                    # compute block indices\n                    idxs = []\n                    pos = 0\n                    for bsize in block_sizes:\n                        idxs.append(np.arange(pos, pos + bsize))\n                        pos += bsize\n                    bsel = self.rng.randint(0, len(idxs))\n                    b = idxs[bsel]\n                    if b.size > 1:\n                        perm = self.rng.permutation(b.size)\n                        xx = x.copy()\n                        xx[b] = x[b][perm]\n                        x = xx\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                Xcand[i] = x\n                # store pre-rotation local vector normalized by sigma for learning\n                local_pre_rot[i] = (R.T @ ((x - m) / (self.sigma + 1e-20)))\n\n            # Evaluate candidates sequentially (counting)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n\n                # update archives with bounded size\n                global_archive_X.append(xi.copy()); global_archive_F.append(float(fi))\n                local_archive_X.append(xi.copy()); local_archive_F.append(float(fi))\n                if len(global_archive_X) > self.global_archive_max:\n                    global_archive_X.pop(0); global_archive_F.pop(0)\n                if len(local_archive_X) > self.local_archive_max:\n                    local_archive_X.pop(0); local_archive_F.pop(0)\n\n                # update best\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy()\n                    last_improv = evals\n\n            # selection: take top-mu candidates by objective\n            mu = max(1, lam // 3)  # different fraction than AS3 (more elitism)\n            idx_sorted = np.argsort(Fvals)\n            sel_idx = idx_sorted[:mu]\n            X_sel = Xcand[sel_idx]\n            Ylocal_sel = local_pre_rot[sel_idx]  # mu x n\n\n            # recombination: use softmax of negative ranks with different temperature\n            ranks = np.arange(1, mu + 1)\n            tau = 0.8\n            w_raw = np.exp(-tau * (ranks - 1))\n            weights = w_raw / np.sum(w_raw)\n\n            # compute candidate mean (in original space)\n            m_proposed = np.sum(weights[:, None] * X_sel, axis=0)\n\n            # accept or temper the mean with a simple Metropolis-like criteria but with logistic temp\n            # build a single evaluation for the proposed mean if budget allows\n            if evals < budget:\n                xm = np.clip(m_proposed, lb, ub)\n                fm = func(xm)\n                evals += 1\n                global_archive_X.append(xm.copy()); global_archive_F.append(float(fm))\n                local_archive_X.append(xm.copy()); local_archive_F.append(float(fm))\n                if len(global_archive_X) > self.global_archive_max:\n                    global_archive_X.pop(0); global_archive_F.pop(0)\n                if len(local_archive_X) > self.local_archive_max:\n                    local_archive_X.pop(0); local_archive_F.pop(0)\n\n                if fm < f_center:\n                    accept = True\n                else:\n                    # logistic acceptance to keep occasional uphill moves:\n                    delta = float(fm) - float(f_center)\n                    T = max(1e-6, 0.8 * np.exp(-0.03 * gen) + 0.05 * stagn_count)\n                    p_accept = 1.0 / (1.0 + np.exp(delta / (T + 1e-12)))\n                    accept = (self.rng.rand() < p_accept)\n\n                if accept:\n                    m = xm.copy()\n                    f_center = float(fm)\n                    # push successful local deltas into success_buf\n                    for j in range(min(mu, len(Ylocal_sel))):\n                        success_buf.append(Ylocal_sel[j].copy())\n                        if len(success_buf) > self.success_buf_max:\n                            success_buf.pop(0)\n                else:\n                    # small conservative move towards proposal to keep diversity\n                    if self.rng.rand() < 0.08:\n                        m = np.clip(0.95 * m + 0.05 * m_proposed, lb, ub)\n            else:\n                # budget exhausted, perform trustless move\n                m = np.clip(m_proposed, lb, ub)\n\n            # ----------------------------\n            # Update per-coordinate RMSprop accumulator and s\n            # compute weighted second moment from selected local steps\n            if Ylocal_sel.shape[0] > 0:\n                # weigh by weights and compute mean squared local delta\n                msq = np.sum(weights[:, None] * (Ylocal_sel ** 2), axis=0)\n                # RMSprop-style update (exponential moving average of squared grads)\n                g2 = (1.0 - self.s_lr) * g2 + self.s_lr * msq\n                # scale s inversely proportional to sqrt of g2 (so large variance dims get smaller steps)\n                s = 1.0 / (np.sqrt(g2) + 1e-12)\n                # clamp scales\n                s = np.clip(s, 1e-6, 1e3)\n            else:\n                # decay g2 slowly to allow exploration\n                g2 *= (1.0 - 0.01)\n\n            # ----------------------------\n            # Update incremental covariance for subspace learning using success_buf or selected local steps\n            # We'll use selected local deltas Ylocal_sel as samples (normalized)\n            if Ylocal_sel.shape[0] > 0:\n                Ys = Ylocal_sel.copy()\n                # center Ys\n                Ys_mean = Ys.mean(axis=0)\n                Ys_cent = Ys - Ys_mean\n                # update Cov with forgetting\n                Cov = self.subspace_forget * Cov + (1.0 - self.subspace_forget) * (Ys_cent.T @ Ys_cent) / max(1.0, Ys_cent.shape[0])\n                cov_count = self.subspace_forget * cov_count + (1.0 - self.subspace_forget) * Ys_cent.shape[0]\n            elif len(success_buf) >= max(4, self.k):\n                # occasional use of success_buf to refresh Cov\n                Ys = np.vstack(success_buf[-min(len(success_buf), 30):])\n                Ys_mean = Ys.mean(axis=0)\n                Ys_cent = Ys - Ys_mean\n                Cov = self.subspace_forget * Cov + (1.0 - self.subspace_forget) * (Ys_cent.T @ Ys_cent) / max(1.0, Ys_cent.shape[0])\n                cov_count = max(cov_count, Ys_cent.shape[0])\n\n            # extract subspace occasionally and mix into R\n            if (gen % max(1, int(2 + n / 18)) == 0) and (cov_count > 0.0):\n                try:\n                    vals, vecs = np.linalg.eigh(Cov)\n                    order = np.argsort(vals)[::-1]\n                    topk = min(self.k, vecs.shape[1])\n                    U_new = vecs[:, order[:topk]]\n                    # construct Qcand and mix with R using different equation (orthogonal Procrustes-like heavy mixing)\n                    Qcand = np.zeros((n, n))\n                    Qcand[:, :topk] = U_new\n                    if topk < n:\n                        rnd = self.rng.randn(n, n - topk)\n                        proj = U_new @ (U_new.T @ rnd)\n                        comp = rnd - proj\n                        Qcand[:, topk:] = comp\n                    Qr, _ = np.linalg.qr(Qcand)\n                    Qcand = Qr[:, :n]\n                    # mix via orthonormal interpolation (Slerp-like on rotation: here linear + orthonormalize)\n                    alpha = self.subspace_mix\n                    R = (1.0 - alpha) * R + alpha * Qcand\n                    Qr, _ = np.linalg.qr(R)\n                    R = Qr[:, :n]\n                    U = R[:, :self.k].copy()\n                except np.linalg.LinAlgError:\n                    # slight perturbation fallback\n                    R = R + 0.02 * self.rng.randn(n, n)\n                    Qr, _ = np.linalg.qr(R)\n                    R = Qr[:, :n]\n                    U = R[:, :self.k].copy()\n\n            # ----------------------------\n            # Sigma adaptation: exponential update on log(sigma) using observed psucc\n            successes = np.sum(Fvals < (f_center - 1e-12))\n            psucc = successes / max(1, lam)\n            # update log-sigma towards target success rate\n            log_sigma = np.log(self.sigma + 1e-20)\n            log_sigma += self.sigma_lr * (psucc - self.sigma_target)\n            self.sigma = np.exp(log_sigma)\n            # clamp sigma\n            self.sigma = np.clip(self.sigma, self.min_sigma, self.max_sigma_factor * np.mean(ub - lb) + 1e-12)\n\n            # ----------------------------\n            # Stagnation detection and gentle restarts\n            if (evals - last_improv) > self.stagn_limit:\n                stagn_count += 1\n                last_improv = evals\n                # reheat sigma multiplicatively\n                self.sigma *= self.reheat_factor\n                # randomize block structure mildly (resample n_blocks with small prob)\n                if self.rng.rand() < 0.5:\n                    # randomly choose new number of blocks nearby\n                    nb = max(1, int(np.round(self.n_blocks * (1 + 0.5 * (self.rng.rand() - 0.5)))))\n                    self.n_blocks = max(1, min(self.dim, nb))\n                # perturb R\n                R = R + 0.05 * self.rng.randn(n, n)\n                Qr, _ = np.linalg.qr(R)\n                R = Qr[:, :n]\n                # clear some memory\n                success_buf = []\n                # slightly reset accumulator to boost exploration\n                g2 *= 0.7\n\n            # ----------------------------\n            # Periodic 1D surrogate along principal direction(s)\n            if (gen % self.surrogate_freq == 0) and (evals < budget) and (len(local_archive_X) >= 8):\n                # use top direction d = U[:,0]\n                d = U[:, 0] if U.shape[1] >= 1 else R[:, 0]\n                # gather recent local archive samples\n                W = min(len(local_archive_X), self.surrogate_window)\n                Xs = np.vstack(local_archive_X[-W:])\n                Fs = np.array(local_archive_F[-W:])\n                ts = (Xs - m) @ d\n                # robustly pick samples around zero (small |t|)\n                order = np.argsort(np.abs(ts))\n                kfit = min(18, len(ts))\n                sel = order[:kfit]\n                t_sel = ts[sel]\n                f_sel = Fs[sel]\n                # Fit quadratic with Huber-like weighting to reduce effect of outliers\n                A = np.vstack([t_sel ** 2, t_sel, np.ones_like(t_sel)]).T\n                # robust weighted least squares: weights are clipped residual-proportional\n                try:\n                    # initial LS\n                    coeffs, *_ = np.linalg.lstsq(A, f_sel, rcond=None)\n                    a, b, c = coeffs\n                    # compute residuals and Huber weights\n                    resid = f_sel - (A @ coeffs)\n                    scale = max(1e-6, np.median(np.abs(resid)) * 1.4826)\n                    hub_thr = 1.5 * scale\n                    hub_w = np.where(np.abs(resid) <= hub_thr, 1.0, hub_thr / np.abs(resid))\n                    # solve weighted LS\n                    Wmat = np.sqrt(hub_w)[:, None]\n                    coeffs_w, *_ = np.linalg.lstsq(Wmat * A, Wmat.flatten()[:, None] * f_sel, rcond=None)\n                    a, b, c = coeffs_w\n                    if a > 1e-14:\n                        t_star = -b / (2.0 * a)\n                        # trust region clamping for t_star\n                        tstd = max(1e-6, np.std(ts))\n                        tmax = max(1.5 * self.sigma * np.mean(s), 3.0 * tstd)\n                        t_star = np.clip(t_star, -tmax, tmax)\n                        probe = np.clip(m + t_star * d, lb, ub)\n                        if evals < budget:\n                            fp = func(probe)\n                            evals += 1\n                            global_archive_X.append(probe.copy()); global_archive_F.append(float(fp))\n                            local_archive_X.append(probe.copy()); local_archive_F.append(float(fp))\n                            if fp < f_opt:\n                                f_opt = float(fp); x_opt = probe.copy(); last_improv = evals\n                            # accept if better or by small probabilistic acceptance\n                            if fp < f_center or (self.rng.rand() < 0.12):\n                                m = probe.copy(); f_center = float(fp)\n                                # incorporate step into buffers\n                                v_local = (R.T @ ((probe - m) / max(1e-20, self.sigma)))\n                                success_buf.append(v_local)\n                                if len(success_buf) > self.success_buf_max:\n                                    success_buf.pop(0)\n                except np.linalg.LinAlgError:\n                    pass\n\n            # ensure center remains feasible\n            m = np.clip(m, lb, ub)\n\n            gen += 1\n\n            # safety: break if exhausted\n            if evals >= budget:\n                break\n\n        # final return best seen\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 446, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if a > 1e-14:", "error": "In the code, line 446, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if a > 1e-14:", "parent_ids": "dd5952f4-6c1c-4ad0-88fc-9c8573e0aa0f", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "81e000ad-c98d-48c2-b6cd-d0e47baccb39", "fitness": 0.29365202543058344, "name": "AMELD", "description": "The algorithm maintains a search center m with a global radius rho and per-coordinate amplitudes p, explores in a rotated basis Q whose first k columns U are learned via PCA on a rolling success buffer, and tracks bandit-like scores for those top-k directions to bias directional probes. Candidate generation is a heterogeneous mixture: mirrored anisotropic Gaussian draws in local coordinates, occasional heavy-tailed Cauchy (Lévy) jumps (p_cauchy≈0.14), DE-style difference mutations from a global archive (p_de≈0.20), and dynamic block-swaps (p_blockswap≈0.12), with population size lambda_ scaled by dimension and a default k ≈ ceil(sqrt(n)). Updates use rank-weighted recombination to propose a new center, EMA of per-coordinate variances to adapt p, small stochastic Givens rotations (and periodic QR) to smoothly evolve Q, and smooth blending of PCA-derived subspace into Q/U; directional rewards are accumulated to form sampling probabilities. Robustness/adaptation features include rho inflation/deflation by observed success rate, temperature-based acceptance, budget-aware 1D parabolic probes and finite-difference directional checks, archive-driven mild restarts with reheating on stagnation, and aggressive truncation/orthonormalization to keep the scheme stable.", "code": "import numpy as np\n\nclass AMELD:\n    \"\"\"\n    Adaptive Mirror Ensemble with Learned Directions and Lévy Restarts (AMELD)\n\n    Main novel elements:\n    - Maintain center m, global radius rho and per-coordinate scales p.\n    - Maintain an orthonormal rotation Q updated via small random Givens-style rotations.\n    - Learn a low-rank subspace U (top-k directions) from a small success buffer via PCA.\n    - Maintain directional statistics (bandit-like scores) for top directions; allocate\n      limited directional probes to directions with high expected improvement.\n    - Candidate generation: mixture of rotated Gaussian (mirrored), Cauchy (Lévy) jumps,\n      DE-style differences from a bounded archive, and dynamic block shuffles.\n    - 1D parabolic probes (like surrogate) along high-score directions performed occasionally.\n    - Adaptive updates: per-coordinate EMA scales, rho adjustment by success-rate,\n      small Givens rotations on Q, and mild restarts when stagnation detected.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, pop_factor=3.5, k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(seed)\n\n        # population size heuristic\n        self.lambda_ = max(4, int(max(4, np.log(max(2, self.dim))) * pop_factor))\n\n        # low-rank dimension\n        if k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(k)), self.dim)\n\n        # probabilities and scales\n        self.p_cauchy = 0.14\n        self.cauchy_scale = 1.2\n        self.p_de = 0.20\n        self.F_de = 0.7\n        self.p_blockswap = 0.12\n        self.mirrored = True\n\n        # buffers\n        self.success_buf_max = max(24, 6 * int(np.ceil(np.sqrt(self.dim))))\n        self.local_archive_max = 300\n        self.global_archive_max = 5000\n\n        # adaptation rates\n        self.p_ema_rate = 0.16\n        self.rho_inflate = 1.22\n        self.rho_deflate = 0.88\n\n        # stagnation and restart\n        self.stagn_limit = max(18, int(5 * self.dim / max(1, int(np.sqrt(self.dim)))))\n        self.reheat_mult = 1.8\n\n        # bandit-like allocation for directional probing\n        self.dir_score_decay = 0.92\n        self.dir_min_prob = 0.05\n\n        # Givens rotation step magnitude\n        self.givens_alpha = 0.06\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (assume func.bounds.lb/ub exist)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial state\n        m = self.rng.uniform(lb, ub)                 # center\n        rho = 0.18 * np.mean(ub - lb)                # global radius\n        p = np.ones(n)                               # per-coordinate amplitude scales\n        Q = np.eye(n)                                # rotation basis\n        U = Q[:, :self.k].copy()                     # learned directions\n\n        # archives\n        success_buf = []             # store local-projected successful steps (in local coords)\n        local_archive_X = []\n        local_archive_F = []\n        global_archive_X = []\n        global_archive_F = []\n\n        # directional bandit stats for top-k directions\n        dir_scores = np.ones(self.k) * 1e-6  # small positive\n        dir_counts = np.ones(self.k) * 1e-6\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of center\n        x0 = np.clip(m, lb, ub)\n        f0 = func(x0)\n        evals += 1\n        f_center = float(f0)\n        f_opt = float(f0)\n        x_opt = x0.copy()\n        local_archive_X.append(x0.copy()); local_archive_F.append(f0)\n        global_archive_X.append(x0.copy()); global_archive_F.append(f0)\n        last_improv = evals\n        stagn_count = 0\n        gen = 0\n        Temp = 1.0\n\n        # helper functions\n        def safe_eval(x):\n            nonlocal evals, f_opt, x_opt, last_improv\n            if evals >= budget:\n                return None\n            x_cl = np.clip(x, lb, ub)\n            f = func(x_cl)\n            evals += 1\n            # update archives\n            global_archive_X.append(x_cl.copy()); global_archive_F.append(f)\n            local_archive_X.append(x_cl.copy()); local_archive_F.append(f)\n            if len(global_archive_X) > self.global_archive_max:\n                global_archive_X.pop(0); global_archive_F.pop(0)\n            if len(local_archive_X) > self.local_archive_max:\n                local_archive_X.pop(0); local_archive_F.pop(0)\n            if f < f_opt:\n                f_opt = float(f); x_opt = x_cl.copy(); last_improv = evals\n            return float(f)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, max(2, remaining // 2))  # keep some room for other probes\n            # prepare Gaussian draws in local coordinates\n            Z = self.rng.randn(lam, n)\n            Xcand = np.zeros((lam, n))\n            cand_y_local = []  # store local pre-rotation normalized steps for learning\n            Fvals = np.full(lam, np.inf)\n\n            for i in range(lam):\n                z = Z[i].copy()\n                if self.mirrored and (i % 2 == 1):\n                    z = -z\n\n                # scale with per-coordinate p (elementwise) and possible anisotropic amplification\n                y_local = p * z\n\n                # rotate to original coordinates\n                y_rot = Q.dot(y_local)\n\n                # occasional Cauchy (Lévy) heavy jump\n                if self.rng.rand() < self.p_cauchy:\n                    r = self.rng.standard_cauchy() * self.cauchy_scale\n                    # normalize z to unit local direction then scale\n                    nz = np.linalg.norm(z) + 1e-12\n                    dir_unit = z / nz\n                    y_rot = Q.dot(p * dir_unit) * (r * np.mean(p) * 0.8)\n\n                # form candidate\n                x = m + rho * y_rot\n\n                # DE-style mutation occasionally using archive\n                if (self.rng.rand() < self.p_de) and (len(global_archive_X) >= 2):\n                    i1, i2 = self.rng.choice(len(global_archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (global_archive_X[i1] - global_archive_X[i2])\n                    x = x + de_mut\n\n                # dynamic block-swap: form a random block partition for this candidate\n                if (self.rng.rand() < self.p_blockswap) and (n > 1):\n                    # create between 2 and min(6,n) blocks randomly\n                    nb = int(self.rng.randint(2, min(6, n) + 1))\n                    # choose block boundaries\n                    perm_idx = self.rng.permutation(n)\n                    block_sizes = [n // nb] * nb\n                    for t in range(n % nb):\n                        block_sizes[t] += 1\n                    # apply permutation inside a randomly chosen block to simulate combinatorial changes\n                    b = self.rng.randint(nb)\n                    start = sum(block_sizes[:b])\n                    end = start + block_sizes[b]\n                    sub = perm_idx[start:end]\n                    if len(sub) > 1:\n                        xp = x.copy()\n                        xp[sub] = x[sub][self.rng.permutation(len(sub))]\n                        x = xp\n\n                # clip\n                x = np.clip(x, lb, ub)\n\n                Xcand[i] = x\n                # map rotated back to local coordinates normalized by rho\n                cand_y_local.append((Q.T @ ((x - m) / (rho + 1e-20))))\n\n            # Evaluate candidates (ensure we don't exceed budget)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = safe_eval(xi)\n                if fi is None:\n                    break\n                Fvals[i] = fi\n\n            # selection: pick top mu\n            mu = max(1, lam // 3)\n            idx = np.argsort(Fvals)\n            sel_idx = idx[:mu]\n            X_sel = Xcand[sel_idx]\n            Ylocals_sel = np.vstack([cand_y_local[j] for j in sel_idx])  # mu x n\n\n            # rank weights (exponential by fitness rank, decaying)\n            ranks = np.arange(1, mu + 1)\n            tau = 0.55\n            w_raw = np.exp(-tau * (ranks - 1))\n            weights = w_raw / np.sum(w_raw)\n\n            # recombine to propose new center candidate (in original coordinates)\n            m_proposed = np.sum(weights[:, None] * X_sel, axis=0)\n\n            # Evaluate proposed mean with temperature acceptance (budget permitting)\n            if evals < budget:\n                fm = safe_eval(m_proposed)\n                if fm is not None:\n                    delta = fm - f_center\n                    if delta <= 0.0 or self.rng.rand() < np.exp(-delta / max(1e-12, Temp)):\n                        # accept\n                        m = np.clip(m_proposed, lb, ub)\n                        f_center = float(fm)\n                    else:\n                        # small biased step toward m_proposed to maintain diversity\n                        if self.rng.rand() < 0.08:\n                            m = np.clip(0.90 * m + 0.10 * m_proposed, lb, ub)\n\n            # update per-coordinate p via EMA of squared selected local steps\n            y2 = np.sum(weights[:, None] * (Ylocals_sel ** 2), axis=0)\n            p2 = (1.0 - self.p_ema_rate) * (p ** 2) + self.p_ema_rate * (y2 + 1e-20)\n            p = np.sqrt(np.clip(p2, 1e-8, 1e6))\n\n            # push some of best local steps into success buffer\n            half = max(1, mu // 2)\n            for j in range(half):\n                success_buf.append(Ylocals_sel[j].copy())\n                if len(success_buf) > self.success_buf_max:\n                    success_buf.pop(0)\n\n            # Update rotation Q via small random Givens-like rotations to introduce smooth change\n            # pick a few random index pairs and apply small rotations in their subspace\n            n_givens = max(1, int(0.5 * np.log(1 + n)))\n            for _ in range(n_givens):\n                i, j = self.rng.choice(n, size=2, replace=False)\n                theta = (self.rng.randn() * self.givens_alpha)\n                c = np.cos(theta); s_g = np.sin(theta)\n                # apply rotation to columns i and j of Q (right-multiplying by G gives new basis)\n                qi = Q[:, i].copy()\n                qj = Q[:, j].copy()\n                Q[:, i] = c * qi - s_g * qj\n                Q[:, j] = s_g * qi + c * qj\n            # re-orthonormalize occasionally\n            if gen % 7 == 0:\n                try:\n                    Q, _ = np.linalg.qr(Q)\n                except np.linalg.LinAlgError:\n                    Q = np.eye(n)\n\n            # Subspace learning: use success_buf to compute top-k directions via PCA, but with weighted decay\n            if (len(success_buf) >= max(6, self.k)) and (gen % max(1, int(2 + n / 30)) == 0):\n                Ymat = np.vstack(success_buf)\n                Yc = Ymat - np.mean(Ymat, axis=0, keepdims=True)\n                try:\n                    C = (Yc.T @ Yc) / max(1.0, Yc.shape[0] - 1)\n                    vals, vecs = np.linalg.eigh(C)\n                    order = np.argsort(vals)[::-1]\n                    topk = min(self.k, vecs.shape[1])\n                    U_new = vecs[:, order[:topk]]\n                    # smoothly update U by interpolation and then set first k columns of Q accordingly\n                    beta = 0.18\n                    # assemble candidate Qcand with U_new as first cols and random orthonormal complement\n                    Qcand = np.zeros((n, n))\n                    Qcand[:, :topk] = U_new\n                    if topk < n:\n                        rnd = self.rng.randn(n, n - topk)\n                        proj = U_new @ (U_new.T @ rnd)\n                        comp = rnd - proj\n                        Qcand[:, topk:] = comp\n                    Qcand, _ = np.linalg.qr(Qcand)\n                    # blend Q toward Qcand in column space\n                    Q = (1.0 - beta) * Q + beta * Qcand\n                    Q, _ = np.linalg.qr(Q)\n                    U = Q[:, :self.k].copy()\n                except np.linalg.LinAlgError:\n                    # fallback tiny perturbation\n                    Q = Q + 0.01 * self.rng.randn(n, n)\n                    Q, _ = np.linalg.qr(Q)\n                    U = Q[:, :self.k].copy()\n\n            # update directional bandit scores using successes projected on U\n            # compute improvements of selected points relative to center and attribute to directions\n            baseline = f_center\n            for idx_sel in sel_idx:\n                fi = Fvals[idx_sel]\n                if np.isfinite(fi):\n                    imp = max(0.0, baseline - fi)\n                    # project step onto top-k directions\n                    yl = cand_y_local[idx_sel]\n                    proj = U.T @ yl[:n]  # shape (k,)\n                    # allocate reward proportional to projection magnitude * improvement\n                    rewards = np.abs(proj) * imp\n                    dir_scores = dir_scores * self.dir_score_decay + rewards\n                    dir_counts = dir_counts * self.dir_score_decay + (np.abs(proj) > 1e-12).astype(float)\n\n            # Normalize directional probabilities\n            dir_prior = dir_scores / (dir_counts + 1e-12)\n            # ensure positivity\n            dir_prior = np.maximum(dir_prior, 0.0)\n            if np.sum(dir_prior) <= 0:\n                dir_prob = np.ones_like(dir_prior) / len(dir_prior)\n            else:\n                dir_prob = dir_prior / np.sum(dir_prior)\n            # floor probabilities to ensure exploration\n            dir_prob = (1.0 - self.dir_min_prob * self.k) * dir_prob + self.dir_min_prob\n\n            # Adjust global radius rho by success-rate heuristic on current generation\n            successes = np.sum(Fvals < (f_center - 1e-12))\n            psucc = successes / max(1, lam)\n            if psucc > 0.28:\n                rho *= self.rho_deflate\n            elif psucc < 0.07:\n                rho *= self.rho_inflate\n            rho = np.clip(rho, 1e-10, 8.0 * np.mean(ub - lb) + 1e-12)\n\n            # stagnation detection & mild restart\n            if (evals - last_improv) > self.stagn_limit:\n                stagn_count += 1\n                # reheat radius and perform heavy-tail sampling around best archive member\n                rho *= self.reheat_mult\n                # pick a random good archive member or random point\n                if len(global_archive_X) > 0 and self.rng.rand() < 0.7:\n                    sel_idx_arch = np.argmin(global_archive_F[-min(len(global_archive_F), 40):])\n                    m = np.array(global_archive_X[-min(len(global_archive_X), 40):][sel_idx_arch]).copy()\n                else:\n                    m = self.rng.uniform(lb, ub)\n                # perturb Q strongly a bit\n                Q = Q + 0.12 * self.rng.randn(n, n)\n                try:\n                    Q, _ = np.linalg.qr(Q)\n                except np.linalg.LinAlgError:\n                    Q = np.eye(n)\n                # clear some buffers\n                success_buf = []\n                dir_scores = np.ones(self.k) * 1e-6\n                dir_counts = np.ones(self.k) * 1e-6\n                last_improv = evals\n\n            # Occasional 1D parabolic/directional probe along a high-score direction\n            # budget-aware: only if we have at least 1 eval left after candidate generation\n            if (gen % max(6, int(6 + n / 10)) == 0) and (evals < budget):\n                # sample a direction from the bandit distribution over U columns\n                try:\n                    chosen = self.rng.choice(self.k, p=dir_prob / np.sum(dir_prob))\n                except Exception:\n                    chosen = int(self.rng.randint(0, self.k))\n                d = U[:, chosen]\n                # collect samples near m from local archive to fit 1D quadratic\n                L = min(len(local_archive_X), 36)\n                if L >= 6:\n                    Xs = np.vstack(local_archive_X[-L:])\n                    Fs = np.array(local_archive_F[-L:])\n                    ts = (Xs - m) @ d\n                    # pick points with smallest |t| (close to center) for stability\n                    order = np.argsort(np.abs(ts))\n                    kfit = min(12, len(ts))\n                    sel = order[:kfit]\n                    t_sel = ts[sel]; f_sel = Fs[sel]\n                    A = np.vstack([t_sel ** 2, t_sel, np.ones_like(t_sel)]).T\n                    try:\n                        coeffs, *_ = np.linalg.lstsq(A, f_sel, rcond=None)\n                        a, b, c = coeffs\n                        if a > 1e-12:\n                            t_star = -b / (2.0 * a)\n                            # clamp t_star\n                            tmax = min(4.0 * rho * np.mean(p), max(1.5, 4.0 * np.std(ts) + 1e-12))\n                            t_star = np.clip(t_star, -tmax, tmax)\n                            probe = np.clip(m + t_star * d, lb, ub)\n                            if evals < budget:\n                                fp = safe_eval(probe)\n                                if fp is not None:\n                                    # accept if improving or probabilistically so\n                                    if fp < f_center or self.rng.rand() < np.exp(-(fp - f_center) / max(1e-12, Temp)):\n                                        m = probe.copy()\n                                        f_center = float(fp)\n                                        # incorporate normalized local step into success buffer\n                                        v_local = Q.T @ ((probe - m) / max(1e-20, rho))\n                                        success_buf.append(v_local)\n                                        if len(success_buf) > self.success_buf_max:\n                                            success_buf.pop(0)\n                                        # reward chosen direction\n                                        dir_scores[chosen] += max(0.0, f_center - fp)\n                                        dir_counts[chosen] += 1.0\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # small random directional finite-difference gradient estimation occasionally\n            if (gen % max(4, int(3 + n / 40)) == 0) and (evals + 2 <= budget) and (len(global_archive_X) >= 4):\n                # pick a top direction by bandit\n                try:\n                    chosen = int(np.where(np.random.multinomial(1, dir_prob / np.sum(dir_prob)))[0][0])\n                except Exception:\n                    chosen = int(self.rng.randint(0, self.k))\n                d = U[:, chosen]\n                eps = 0.9 * rho * np.mean(p) * (0.5 + 0.5 * self.rng.rand())\n                x_plus = np.clip(m + eps * d, lb, ub)\n                x_minus = np.clip(m - eps * d, lb, ub)\n                f_plus = safe_eval(x_plus)\n                f_minus = None\n                if evals < budget:\n                    f_minus = safe_eval(x_minus)\n                if (f_plus is not None) and (f_minus is not None):\n                    grad_est = (f_plus - f_minus) / (2.0 * eps + 1e-20)\n                    # take a small quasi-gradient step along -d if promising\n                    if grad_est > 0:\n                        step = -0.6 * (rho * np.mean(p)) * (grad_est / (abs(grad_est) + 1e-8))\n                        candidate = np.clip(m + step * d, lb, ub)\n                        if evals < budget:\n                            fc = safe_eval(candidate)\n                            if (fc is not None) and (fc < f_center or self.rng.rand() < np.exp(-(fc - f_center) / max(1e-12, Temp))):\n                                m = candidate.copy()\n                                f_center = float(fc)\n\n            # cool the temperature slowly\n            Temp = max(1e-6, Temp * 0.996)\n\n            # ensure m bounded\n            m = np.clip(m, lb, ub)\n\n            gen += 1\n\n            # safety break if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AMELD scored 0.294 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "dd5952f4-6c1c-4ad0-88fc-9c8573e0aa0f", "operator": null, "metadata": {"aucs": [0.1668301281653235, 0.20463759393412606, 0.3469971844092359, 0.37965202204176884, 0.4012885846984263, 0.3626359330854867, 0.25608952563564724, 0.3428253663309947, 0.28275389315919075, 0.19281002284563442]}, "task_prompt": ""}
{"id": "95d582c3-9a62-4adf-b228-1b583d7a9d2a", "fitness": "-inf", "name": "MACE_B", "description": "The algorithm combines per-coordinate RMSProp-style scale adaptation (high s_rms_beta=0.88, bias-corrected second moment g2 → s) with a global multiplicative sigma update driven smoothly by the observed batch success rate (sigma *= exp((psucc−target)*scale) with sigma_update_scale=0.6 and target_success=0.25) to balance local anisotropy and global step-size. Exploration mixes several mutation modes — mirrored Gaussian sampling in a rotated space, occasional Cauchy heavy tails (p_cauchy=0.08), differential-evolution style archive differences (p_de=0.25, F_de=0.8), block-wise swaps/jitters, and a population size heuristic lambda driven by pop_factor to trade off exploration and evaluation cost. Subspace learning is incremental: an exponentially-weighted covariance C_inc (cov_alpha=0.12) is periodically decomposed to extract top-k directions which are gently mixed into the orthonormal rotation R (subspace_mix=0.35) to bias search along learned principal directions, plus occasional 1D surrogate probing along the leading eigenvector. Robustness and diversification are provided by bounded local/global archives, simulated-annealing acceptance for proposed means, stagnation detection with reheating and block repartition, and conservative clipping to the problem bounds.", "code": "import numpy as np\n\nclass MACE_B:\n    \"\"\"\n    Multi-Scale Adaptive Coordinate-Block Explorer (MACE-B)\n\n    Key ideas and how this differs from the provided AS3:\n    - Uses RMS-style per-coordinate scale adaptation (bias-corrected RMS) instead of a plain EMA\n      of squared local steps. This gives a different effective timescale and bias-correction.\n    - Sigma (global step-size) adapted with a smooth exponential update based on deviation from\n      a target success rate (multiplicative update: sigma *= exp((psucc - target)*scale)),\n      rather than simple inflations/deflations at thresholds.\n    - Subspace learning is done incrementally via an exponentially-weighted covariance matrix\n      (no full-success-buffer eig every time), and SVD is used occasionally for stability.\n      This is lighter-weight and smoother than re-computing covariance from scratch every cycle.\n    - Uses a slightly smaller/larger population sizing heuristic, different probabilities for\n      Cauchy/DE/block-swap, and stronger mixing into R to make the learned subspace more\n      influential; also uses a randomized block repartitioning on stagnation.\n    - Surrogate probing along the principal learned direction uses a weighted robust quadratic\n      fit (weights favor nearby samples) and a modest trust radius scaled to RMS scales.\n    - Uses an RNG local to the instance for reproducibility (RandomState).\n    - Parameter choices intentionally different from the provided algorithm (see parameters below).\n\n    Main tunable parameters (examples chosen differently from AS3):\n      - pop_factor: population scale factor (affects lambda)\n      - p_cauchy (default 0.08) vs AS3 0.12\n      - p_de (default 0.25) vs AS3 0.18\n      - F_de (default 0.8) vs AS3 0.6\n      - p_blockswap (default 0.10) vs AS3 0.14\n      - mirrored sampling is enabled\n      - s_rms_beta: RMS-propagation rate (default 0.88) vs AS3 s_ema_rate 0.18\n      - subspace_mix: stronger mixing (default 0.35) vs AS3 0.15\n      - sigma_update_scale: smoothness for multiplicative update (default 0.6)\n      - cov_alpha: incremental covariance update rate (default 0.12)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, blocks=None, pop_factor=2.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.RandomState(None if seed is None else int(seed))\n\n        # population and structural choices\n        self.pop_factor = float(pop_factor)\n        self.lambda_ = max(4, int(max(4, np.log(max(2, self.dim))) * self.pop_factor))\n\n        # block partition default heuristic different from AS3 (more blocks for large dims)\n        if blocks is None:\n            self.n_blocks = max(1, int(np.ceil(np.sqrt(self.dim) / 1.2)))\n        else:\n            self.n_blocks = max(1, min(int(blocks), self.dim))\n\n        # initial contiguous blocks\n        sizes = [self.dim // self.n_blocks] * self.n_blocks\n        for i in range(self.dim % self.n_blocks):\n            sizes[i] += 1\n        idx = 0\n        self.blocks = []\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n        # subspace rank (mildly different heuristic)\n        self.k = min(max(1, int(np.ceil(np.sqrt(self.dim) / 1.0))), self.dim)\n\n        # mutation probabilities / coefficients (changed)\n        self.p_cauchy = 0.08\n        self.cauchy_scale = 1.0\n        self.p_de = 0.25\n        self.F_de = 0.8\n        self.p_blockswap = 0.10\n        self.mirrored = True\n\n        # buffers and windows\n        self.success_buf_max = max(16, 5 * int(np.ceil(np.sqrt(self.dim))))\n        self.local_archive_max = 300\n        self.global_archive_max = 5000\n\n        # adaptation parameters (different style)\n        self.s_rms_beta = 0.88          # RMSProp-like smoothing for per-coordinate scales\n        self.s_rms_eps = 1e-9\n        self.subspace_mix = 0.35        # stronger mix into R than AS3\n        self.sigma_update_scale = 0.6   # multiplier in exp((psucc-target)*scale)\n        self.target_success = 0.25      # desired success rate\n        self.cov_alpha = 0.12           # incremental covariance update rate\n\n        # stagnation controls (different heuristic)\n        self.stagn_limit = max(25, int(8 * self.dim / max(1, self.n_blocks)))\n        self.reheat_mult = 1.8\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (assume BBOB style -5..5 but read from func)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # state\n        m = self.rng.uniform(lb, ub)                       # center\n        sigma = 0.08 * np.mean(ub - lb)                    # initial step-size (different)\n        s = np.ones(n, dtype=float)                        # per-coordinate RMS scales\n        R = np.eye(n)                                      # rotation\n        U = np.eye(n)[:, :self.k].copy()                   # principal subspace columns (init)\n\n        # incremental covariance for subspace learning\n        C_inc = np.zeros((n, n), dtype=float)\n\n        # RMS prop helper for per-coordinate scaling (second moment)\n        g2 = np.ones(n, dtype=float) * (1e-6)\n\n        # buffers\n        success_buf = []\n        local_archive_X = []\n        local_archive_F = []\n        global_archive_X = []\n        global_archive_F = []\n\n        # acceptance temperature\n        Temp = 0.9\n        Tmin = 1e-8\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial eval\n        x0 = np.clip(m, lb, ub)\n        f0 = func(x0)\n        evals += 1\n        f_center = float(f0)\n        f_opt = float(f0)\n        x_opt = x0.copy()\n        local_archive_X.append(x0.copy()); local_archive_F.append(f0)\n        global_archive_X.append(x0.copy()); global_archive_F.append(f0)\n        last_improv = evals\n        stagn_count = 0\n        gen = 0\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            Z = self.rng.randn(lam, n)\n            Xcand = np.zeros((lam, n), dtype=float)\n            cand_info_local = []\n            Fvals = np.full(lam, np.inf)\n\n            # generate population\n            for i in range(lam):\n                z = Z[i].copy()\n                if self.mirrored and (i % 2 == 1):\n                    z = -z\n\n                # local scaled vector (RMS-scaled)\n                y_local = s * z\n\n                # occasional Cauchy heavy tail along local direction (scale relative to RMS)\n                if self.rng.rand() < self.p_cauchy:\n                    r = self.rng.standard_cauchy() * self.cauchy_scale\n                    # cap r to avoid infinities\n                    r = np.clip(r, -1e3, 1e3)\n                    nz = np.linalg.norm(z) + 1e-20\n                    dir_unit = z / nz\n                    y_local = (np.mean(s) * dir_unit) * (r * 0.7)\n\n                # rotate\n                y_rot = R.dot(y_local)\n                x = m + sigma * y_rot\n\n                # DE-like mutation from global archive (different prob and strength)\n                if (self.rng.rand() < self.p_de) and (len(global_archive_X) >= 2):\n                    i1, i2 = self.rng.choice(len(global_archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (global_archive_X[i1] - global_archive_X[i2])\n                    x = x + de_mut\n\n                # cheap intra-block permutation (swap within block)\n                if (self.rng.rand() < self.p_blockswap) and (len(self.blocks) > 0):\n                    bidx = self.rng.randint(len(self.blocks))\n                    b = self.blocks[bidx]\n                    if len(b) > 1:\n                        perm = self.rng.permutation(len(b))\n                        xb = x[b].copy()\n                        x[b] = xb[perm]\n\n                # small blockwise Gaussian jitter to encourage combinatorial mixing\n                if self.rng.rand() < 0.06 and len(self.blocks) > 0:\n                    bidx = self.rng.randint(len(self.blocks))\n                    b = self.blocks[bidx]\n                    x[b] += 0.5 * sigma * (self.rng.randn(len(b)) * s[b])\n\n                # clip\n                x = np.clip(x, lb, ub)\n\n                Xcand[i] = x\n                # store local (pre-rotation) normalized by sigma for learning\n                cand_info_local.append((R.T @ y_rot) / (sigma + 1e-20))\n\n            # Evaluate candidates until budget or population done\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n\n                # update archives (bounded)\n                global_archive_X.append(xi.copy()); global_archive_F.append(fi)\n                local_archive_X.append(xi.copy()); local_archive_F.append(fi)\n                if len(global_archive_X) > self.global_archive_max:\n                    global_archive_X.pop(0); global_archive_F.pop(0)\n                if len(local_archive_X) > self.local_archive_max:\n                    local_archive_X.pop(0); local_archive_F.pop(0)\n\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy(); last_improv = evals\n\n            # selection: choose top mu (different fraction)\n            mu = max(1, int(np.ceil(lam / 3.0)))\n            idx = np.argsort(Fvals)\n            sel_idx = idx[:mu]\n            X_sel = Xcand[sel_idx]\n            Ylocals_sel = np.vstack([cand_info_local[j] for j in sel_idx])  # mu x n\n\n            # linear rank weights (different weighting)\n            ranks = np.arange(1, mu + 1)\n            w_raw = (mu + 0.5) - ranks  # descending linear weights\n            w_raw = np.maximum(w_raw, 0.0) + 1e-12\n            weights = w_raw / np.sum(w_raw)\n\n            # recombination - propose new mean\n            m_proposed = np.sum(weights[:, None] * X_sel, axis=0)\n\n            # Evaluate the proposed mean if budget allows (careful with budget)\n            accept_mean = True\n            if evals < budget:\n                xm = np.clip(m_proposed, lb, ub)\n                fm = func(xm)\n                evals += 1\n                global_archive_X.append(xm.copy()); global_archive_F.append(fm)\n                local_archive_X.append(xm.copy()); local_archive_F.append(fm)\n                if fm < f_opt:\n                    f_opt = float(fm); x_opt = xm.copy(); last_improv = evals\n\n                delta = float(fm) - float(f_center)\n                if delta <= 0.0:\n                    accept_mean = True\n                    m = xm.copy(); f_center = float(fm)\n                else:\n                    # simulated annealing acceptance with temperature\n                    p = np.exp(-delta / max(1e-12, Temp))\n                    if self.rng.rand() < p:\n                        accept_mean = True\n                        m = xm.copy(); f_center = float(fm)\n                    else:\n                        accept_mean = False\n                        # small biased move toward proposal occasionally to maintain diversity\n                        if self.rng.rand() < 0.08:\n                            m = np.clip(0.90 * m + 0.10 * m_proposed, lb, ub)\n                # cool temperature gently\n                Temp = max(Tmin, Temp * 0.993)\n            else:\n                # no budget to evaluate mean; move fractionally\n                m = np.clip(0.9 * m + 0.1 * m_proposed, lb, ub)\n\n            # Update per-coordinate RMS second moment g2 and scales s\n            # compute weighted second moment from selected local steps\n            y2 = np.sum(weights[:, None] * (Ylocals_sel ** 2), axis=0)\n            # RMSProp-like update (bias-corrected)\n            beta = self.s_rms_beta\n            g2 = beta * g2 + (1.0 - beta) * y2\n            # bias correction factor approximately 1/(1-beta^t) but we don't track t; keep simple scaling\n            s = np.sqrt(g2 + self.s_rms_eps)\n            s = np.clip(s, 1e-6, 1e3)\n\n            # push best half of selected local normalized steps into success buffer\n            half = max(1, mu // 2)\n            for j in range(half):\n                success_buf.append(Ylocals_sel[j].copy())\n                if len(success_buf) > self.success_buf_max:\n                    success_buf.pop(0)\n\n            gen += 1\n\n            # Incremental covariance update for subspace (smooth) and occasional SVD\n            if len(success_buf) > 0:\n                # use mean outer product of current success_buf as signal\n                Ys = np.vstack(success_buf)\n                Yc = Ys - np.mean(Ys, axis=0, keepdims=True)\n                M = (Yc.T @ Yc) / max(1.0, Yc.shape[0] - 1)\n                # exponential update of incremental covariance\n                C_inc = (1.0 - self.cov_alpha) * C_inc + self.cov_alpha * M\n\n                # every few generations do an SVD to extract principal directions\n                if gen % max(1, int(2 + n / 25)) == 0:\n                    try:\n                        # economy SVD is stable for symmetric positive semidefinite C_inc\n                        vals, vecs = np.linalg.eigh(C_inc)\n                        order = np.argsort(vals)[::-1]\n                        topk = min(self.k, vecs.shape[1])\n                        U_new = vecs[:, order[:topk]]\n                        # create candidate orthonormal rotation with U_new first\n                        Qcand = np.zeros((n, n))\n                        Qcand[:, :topk] = U_new\n                        if topk < n:\n                            rnd = self.rng.randn(n, n - topk)\n                            proj = U_new @ (U_new.T @ rnd)\n                            comp = rnd - proj\n                            Qcand[:, topk:] = comp\n                        Qr, _ = np.linalg.qr(Qcand)\n                        Qcand = Qr[:, :n]\n                        # gently mix into R and reorthogonalize\n                        alpha = self.subspace_mix\n                        R = (1.0 - alpha) * R + alpha * Qcand\n                        Qr, _ = np.linalg.qr(R)\n                        R = Qr[:, :n]\n                        U = R[:, :self.k].copy()\n                    except np.linalg.LinAlgError:\n                        # fallback perturbation\n                        R = R + 0.02 * self.rng.randn(n, n)\n                        Qr, _ = np.linalg.qr(R)\n                        R = Qr[:, :n]\n                        U = R[:, :self.k].copy()\n\n            # Adaptive sigma update: smooth multiplicative update based on current batch success rate\n            successes = np.sum(Fvals < (f_center - 1e-12))\n            psucc = successes / max(1, lam)\n            # multiplicative exponential update (smooth)\n            sigma *= np.exp((psucc - self.target_success) * self.sigma_update_scale)\n            sigma = np.clip(sigma, 1e-12, 6.0 * np.mean(ub - lb) + 1e-12)\n\n            # stagnation detection and gentle restart-like moves\n            if (evals - last_improv) > self.stagn_limit:\n                stagn_count += 1\n                Temp = max(Temp, 0.4 + 0.25 * stagn_count)\n                sigma *= self.reheat_mult\n                # random block repartition with small probability\n                if self.rng.rand() < 0.75 and len(self.blocks) > 1:\n                    # reshuffle indices and build new balanced blocks\n                    perm = self.rng.permutation(n)\n                    sizes = [n // self.n_blocks] * self.n_blocks\n                    for i in range(n % self.n_blocks):\n                        sizes[i] += 1\n                    blocks_new = []\n                    idx = 0\n                    for s in sizes:\n                        blocks_new.append(list(perm[idx:idx + s]))\n                        idx += s\n                    self.blocks = blocks_new\n                # perturb R and U\n                R = R + 0.04 * self.rng.randn(n, n)\n                Qr, _ = np.linalg.qr(R)\n                R = Qr[:, :n]\n                U = R[:, :self.k].copy()\n                # reset success buffer to encourage new directions\n                success_buf = []\n                last_improv = evals\n\n            # Periodic 1D surrogate probe along principal direction U[:,0]\n            if (gen % max(6, int(5 + n / 12)) == 0) and (evals < budget) and (len(local_archive_X) >= 8):\n                d = U[:, 0]\n                # gather recent local archive\n                L = min(len(local_archive_X), 60)\n                Xs = np.vstack(local_archive_X[-L:])\n                Fs = np.array(local_archive_F[-L:])\n                ts = (Xs - m) @ d\n                # pick samples with smallest |t| (near center) but keep diversity\n                order = np.argsort(np.abs(ts))\n                kfit = min(16, len(ts))\n                sel = order[:kfit]\n                t_sel = ts[sel]; f_sel = Fs[sel]\n                # build weighted least squares with weights decreasing with |t|\n                w = 1.0 / (1.0 + (np.abs(t_sel) / (0.5 * sigma * np.mean(s) + 1e-12)) ** 2)\n                W = np.sqrt(w)\n                A = np.vstack([t_sel ** 2, t_sel, np.ones_like(t_sel)]).T * W[:, None]\n                b = f_sel * W\n                try:\n                    coeffs, *_ = np.linalg.lstsq(A, b, rcond=None)\n                    a, bcoef, c = coeffs\n                    if a > 1e-12:\n                        t_star = -bcoef / (2.0 * a)\n                        # clamp t_star to trust region (based on sigma and s RMS)\n                        tmax = min(3.0 * sigma * np.mean(s), max(1.5, 3.0 * np.std(ts) + 1e-12))\n                        t_star = np.clip(t_star, -tmax, tmax)\n                        probe = np.clip(m + t_star * d, lb, ub)\n                        if evals < budget:\n                            fp = func(probe)\n                            evals += 1\n                            global_archive_X.append(probe.copy()); global_archive_F.append(fp)\n                            local_archive_X.append(probe.copy()); local_archive_F.append(fp)\n                            if len(global_archive_X) > self.global_archive_max:\n                                global_archive_X.pop(0); global_archive_F.pop(0)\n                            if fp < f_opt:\n                                f_opt = float(fp); x_opt = probe.copy(); last_improv = evals\n                            # accept probe if improving or probabilistically by temperature\n                            if fp < f_center or (self.rng.rand() < np.exp(-(fp - f_center) / max(1e-12, Temp))):\n                                # We must ensure v_local refers to the step relative to old center (use t_star)\n                                m = probe.copy(); f_center = float(fp)\n                                # store normalized local step for success buffer (map back)\n                                v_local = (R.T @ ((probe - m) / max(1e-20, sigma)))\n                                success_buf.append(v_local)\n                                if len(success_buf) > self.success_buf_max:\n                                    success_buf.pop(0)\n                except np.linalg.LinAlgError:\n                    pass\n\n            # keep center feasible\n            m = np.clip(m, lb, ub)\n\n            # check budget safety\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 194, in __call__, the following error occurred:\nTypeError: 'int' object is not subscriptable\nOn line: x[b] += 0.5 * sigma * (self.rng.randn(len(b)) * s[b])", "error": "In the code, line 194, in __call__, the following error occurred:\nTypeError: 'int' object is not subscriptable\nOn line: x[b] += 0.5 * sigma * (self.rng.randn(len(b)) * s[b])", "parent_ids": "dd5952f4-6c1c-4ad0-88fc-9c8573e0aa0f", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "cfd546bd-deec-4239-a160-13dee95d74b4", "fitness": "-inf", "name": "HLRC_SALR", "description": "The algorithm is a hybrid diagonal+low-rank CMA-like search that samples with per-coordinate scales D and a learned low-rank subspace U (k ≈ ceil(sqrt(dim))) using mirrored draws and a modest population (lambda ≈ max(6, 4+3 log dim), mu ≈ lambda/2) and an initial sigma = 0.18 * range to balance global search and local refinement. Adaptation uses a CMA-style evolution path ps with an elementwise inverse-diagonal approximation for invsqrt(C), plus an EMA update of D (c_d≈0.25) to adapt per-coordinate step-sizes while periodically recomputing U by SVD on a success buffer to capture principal search directions. Exploration is boosted by stochastic diversifiers (Cauchy jumps with p_cauchy=0.12, DE-style archive mutations with p_de=0.18, mirrored sampling), stagnation handling (sigma reheating and mild random perturbations of D/U), and bounded archives to maintain history. Exploitation is enhanced by a surrogate-assisted quadratic probe along the top principal direction and budget-aware short local CMA refinements (local_refine_budget_frac≈0.03) that opportunistically spend small remaining budget to intensify search near promising points.", "code": "import numpy as np\n\nclass HLRC_SALR:\n    \"\"\"\n    Hybrid Low-Rank CMA with Surrogate-Assisted Local Refinement (HLRC-SALR)\n    One-line: Diagonal+low-rank sampling with mirrored draws, DE/Cauchy jumps, ps-based sigma control,\n    periodic SVD to learn subspace, surrogate quadratic probes on principal directions, and budget-aware\n    short local-CMA refinements for fast exploitation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population similar to CMA heuristics but modest\n        self.lambda_ = max(6, int(np.ceil(4 + 3 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n        # subspace rank\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # get bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng = np.random\n\n        # strategy params\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # ps path constants (CMA-like)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initial state\n        m = rng.uniform(lb, ub)\n        sigma = 0.18 * np.mean(ub - lb)\n        # diagonal per-coordinate stds\n        D = np.ones(n)\n        # orthonormal low-rank subspace (n x k)\n        rand = np.random.randn(n, self.k)\n        try:\n            U, _ = np.linalg.qr(rand)\n            U = U[:, :self.k]\n        except:\n            U = np.eye(n)[:, :self.k]\n\n        ps = np.zeros(n)  # path for sigma (we will use diag inv approx)\n        invdiag = 1.0 / (D + 1e-20)\n\n        # buffers / archives\n        global_archive_X = []\n        global_archive_F = []\n        success_buf = []  # store local pre-rotation steps (rows) for SVD\n        local_archive_X = []\n        local_archive_F = []\n\n        # control params\n        p_cauchy = 0.12\n        p_de = 0.18\n        F_de = 0.7\n        mirrored = True\n        max_global_archive = 5000\n        success_buf_max = max(20, 6 * int(np.ceil(np.sqrt(n))))\n        subspace_update_every = max(1, int(4))\n        surrogate_every = max(5, int(8 - n // 10))\n        local_refine_budget_frac = 0.03  # fraction of remaining budget to spend on local CMA when promising\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial seed samples to populate archives (but respect budget)\n        init_seeds = min(max(4, 2 * n), budget)\n        for _ in range(init_seeds):\n            x = rng.uniform(lb, ub)\n            f = func(x); evals += 1\n            global_archive_X.append(x.copy()); global_archive_F.append(f)\n            local_archive_X.append(x.copy()); local_archive_F.append(f)\n            if f < f_opt:\n                f_opt = f; x_opt = x.copy()\n\n        # main loop\n        gen = 0\n        last_improv = 0\n        stagn_count = 0\n        while evals < budget:\n            remaining = budget - evals\n            lam_cur = min(lam, remaining)\n            Z = np.random.randn(lam_cur, n)\n            Ys = np.zeros((lam_cur, n))\n            Xs = np.zeros((lam_cur, n))\n            Fvals = np.full(lam_cur, np.inf)\n\n            for i in range(lam_cur):\n                z = Z[i].copy()\n                if mirrored and (i % 2 == 1):\n                    z = -z\n                # low-rank sample\n                if self.k > 0:\n                    zl = np.random.randn(self.k)\n                    low = U @ zl\n                    beta = 0.75 * np.mean(D)\n                    y = D * z + beta * low\n                else:\n                    y = D * z\n\n                # occasional Cauchy jump\n                if rng.rand() < p_cauchy:\n                    r = np.random.standard_cauchy() * 1.0\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = r * (z / nz) * np.mean(D)\n\n                x = m + sigma * y\n\n                # DE-style archive mutation sometimes\n                if (rng.rand() < p_de) and (len(global_archive_X) >= 2):\n                    i1, i2 = rng.choice(len(global_archive_X), size=2, replace=False)\n                    de = F_de * (global_archive_X[i1] - global_archive_X[i2])\n                    x = x + de\n\n                # clip and store\n                x = np.clip(x, lb, ub)\n                Xs[i] = x\n                Ys[i] = y\n\n            # evaluate candidates while respecting budget\n            for i in range(lam_cur):\n                if evals >= budget:\n                    break\n                xi = Xs[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                global_archive_X.append(xi.copy()); global_archive_F.append(fi)\n                local_archive_X.append(xi.copy()); local_archive_F.append(fi)\n                if len(global_archive_X) > max_global_archive:\n                    global_archive_X.pop(0); global_archive_F.pop(0)\n                if len(local_archive_X) > 200:\n                    local_archive_X.pop(0); local_archive_F.pop(0)\n                if fi < f_opt - 1e-15:\n                    f_opt = fi; x_opt = xi.copy(); last_improv = evals\n\n            # selection & recombination\n            idx = np.argsort(Fvals)\n            sel = idx[:mu]\n            X_sel = Xs[sel]\n            Y_sel = Ys[sel]\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * X_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(weights[:, None] * Y_sel, axis=0)\n\n            # ps update: approximate invsqrtC by 1/D elementwise; keep ps as n-vector\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = np.clip(sigma, 1e-12, 1e2 * np.mean(ub - lb) + 1e-12)\n\n            # update diagonal D via EMA of squared selected y\n            y2 = np.sum(weights[:, None] * (Y_sel ** 2), axis=0)\n            c_d = 0.25\n            D2 = (1 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n            D = np.clip(D, 1e-6, 1e3)\n\n            # store successes for subspace learning (weighted mean step)\n            success_buf.append(y_w.copy())\n            if len(success_buf) > success_buf_max:\n                success_buf.pop(0)\n\n            gen += 1\n            # periodically update low-rank subspace U via SVD on success buffer\n            if (len(success_buf) >= max(3, self.k)) and (gen % subspace_update_every == 0):\n                Ymat = np.vstack(success_buf)  # rows samples x n\n                Yc = Ymat - np.mean(Ymat, axis=0, keepdims=True)\n                try:\n                    # compute top-k left singular vectors\n                    U_s, svals, Vt = np.linalg.svd(Yc, full_matrices=False)\n                    k_take = min(self.k, U_s.shape[1])\n                    if k_take > 0:\n                        Ucand = np.zeros((n, self.k))\n                        Ucand[:, :k_take] = U_s[:, :k_take]\n                        if k_take < self.k:\n                            # fill remaining with random orthonormal complement\n                            rnd = np.random.randn(n, self.k - k_take)\n                            proj = Ucand[:, :k_take] @ (Ucand[:, :k_take].T @ rnd)\n                            comp = rnd - proj\n                            Qc, _ = np.linalg.qr(comp)\n                            Ucand[:, k_take:] = Qc[:, :self.k - k_take]\n                        # gently mix into U\n                        alpha = 0.25\n                        U = (1 - alpha) * U + alpha * Ucand\n                        # orthonormalize columns of U\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                except np.linalg.LinAlgError:\n                    pass\n\n            # surrogate quadratic probe along principal direction from U[:,0]\n            if (gen % surrogate_every == 0) and (len(local_archive_X) >= max(6, 3 * n)) and (evals < budget):\n                # construct local dataset near current m\n                Xs_arr = np.vstack(local_archive_X[-min(len(local_archive_X), 80):])\n                Fs_arr = np.array(local_archive_F[-min(len(local_archive_F), 80):])\n                # principal direction\n                d = U[:, 0]\n                ts = (Xs_arr - m) @ d\n                # choose points with smallest |t| for robust fit\n                order = np.argsort(np.abs(ts))\n                kfit = min(20, len(ts))\n                sel = order[:kfit]\n                t_sel = ts[sel]; f_sel = Fs_arr[sel]\n                # fit quadratic f = a t^2 + b t + c\n                A = np.vstack([t_sel ** 2, t_sel, np.ones_like(t_sel)]).T\n                try:\n                    coeffs, *_ = np.linalg.lstsq(A, f_sel, rcond=None)\n                    a, b, c = coeffs\n                    if a > 1e-12:\n                        t_star = -b / (2 * a)\n                        # clamp step\n                        tmax = max(2.0 * sigma * np.mean(D), 1.5 * np.std(ts) + 1e-12)\n                        t_star = np.clip(t_star, -tmax, tmax)\n                        probe = np.clip(m + t_star * d, lb, ub)\n                        if evals < budget:\n                            fp = func(probe); evals += 1\n                            global_archive_X.append(probe.copy()); global_archive_F.append(fp)\n                            local_archive_X.append(probe.copy()); local_archive_F.append(fp)\n                            if fp < f_opt - 1e-15:\n                                f_opt = fp; x_opt = probe.copy(); last_improv = evals\n                                # small local refinement (short local CMA) with budget proportional to remaining\n                                # allocate small chunk but don't exceed overall budget\n                                local_budget = min(max(10, int(local_refine_budget_frac * (budget - evals))), budget - evals)\n                                if local_budget >= 8:\n                                    self._local_cma(func, probe, fp, local_budget, lb, ub, evals, D, sigma, x_opt, global_archive_X, global_archive_F)\n                                    # local_cma subtracts from budget by calling func directly; we cannot know its internal updates to evals here,\n                                    # so we keep global correctness by letting that method manage budget-aware calls.\n                except np.linalg.LinAlgError:\n                    pass\n\n            # stagnation handling: if no improvement in many evals, reheat sigma and perturb U/D\n            if (evals - last_improv) > max(50, 6 * n):\n                stagn_count += 1\n                sigma *= (1.2 + 0.15 * stagn_count)\n                # mild perturbation of D and U\n                D = np.clip(D * (1.0 + 0.08 * np.random.randn(n)), 1e-6, 1e3)\n                U = U + 0.02 * np.random.randn(*U.shape)\n                Q, _ = np.linalg.qr(U)\n                U = Q[:, :self.k]\n                # inject a few random archive samples as new center candidates\n                if len(global_archive_X) > 0:\n                    pick = rng.randint(0, len(global_archive_X))\n                    m = 0.6 * m + 0.4 * np.asarray(global_archive_X[pick])\n                last_improv = evals\n\n            # ensure arrays trimmed\n            if len(global_archive_X) > max_global_archive:\n                global_archive_X = global_archive_X[-max_global_archive:]\n                global_archive_F = global_archive_F[-max_global_archive:]\n\n            # enforce bound on mean\n            m = np.clip(m, lb, ub)\n\n            # loop end\n            if evals >= budget:\n                break\n\n        # final return\n        return float(f_opt), np.array(x_opt, dtype=float)\n\n    def _local_cma(self, func, center, f_center, local_budget, lb, ub, evals_start, D_init, sigma_init, x_opt_ref, global_archive_X, global_archive_F):\n        \"\"\"\n        Very short local CMA-like refinement centered at 'center' using diagonal+small rank-1 adaption.\n        This helper uses budget-aware calls to func and returns immediately; it attempts a few\n        opportunistic improvements near center. It purposely uses few evaluations.\n        \"\"\"\n        # note: this helper assumes we may call func up to local_budget times, but caller must ensure not to exceed overall budget externally.\n        n = center.size\n        rng = np.random\n        D = D_init.copy()\n        sigma = max(1e-12, 0.5 * sigma_init)\n        ps = np.zeros(n)\n        lam_local = max(6, min(20, int(4 + np.floor(2 * np.log(max(2, n))))))\n        mu_local = max(1, lam_local // 2)\n        weights = np.log(mu_local + 0.5) - np.log(np.arange(1, mu_local + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        evals = 0\n        m = center.copy()\n        f_m = f_center\n        best_x = x_opt_ref.copy() if x_opt_ref is not None else m.copy()\n        best_f = f_m\n\n        while evals < local_budget:\n            lamc = min(lam_local, local_budget - evals)\n            Z = np.random.randn(lamc, n)\n            Xcand = np.zeros((lamc, n))\n            Y = np.zeros((lamc, n))\n            for i in range(lamc):\n                z = Z[i]\n                y = D * z\n                x = m + sigma * y\n                x = np.clip(x, lb, ub)\n                Xcand[i] = x\n                Y[i] = y\n            # evaluate\n            Fvals = np.full(lamc, np.inf)\n            for i in range(lamc):\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                global_archive_X.append(xi.copy()); global_archive_F.append(fi)\n                if fi < best_f:\n                    best_f = fi; best_x = xi.copy()\n            # update m via weighted recombination\n            idx = np.argsort(Fvals)\n            sel = idx[:mu_local]\n            Xs = Xcand[sel]; Ys = Y[sel]\n            m = np.sum(weights[:, None] * Xs, axis=0)\n            y_w = np.sum(weights[:, None] * Ys, axis=0)\n            # update ps and sigma\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = np.clip(sigma, 1e-12, 1e2)\n            # update D slightly\n            y2 = np.sum(weights[:, None] * (Ys ** 2), axis=0)\n            c_d = 0.2\n            D2 = (1 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n            D = np.clip(D, 1e-6, 1e3)\n        # no return required; this method only opportunistically calls func to improve global archives\n        return", "configspace": "", "generation": 0, "feedback": "In the code, line 202, in __call__, the following error occurred:\nValueError: could not broadcast input array from shape (4,2) into shape (2,2)\nOn line: Ucand[:, :k_take] = U_s[:, :k_take]", "error": "In the code, line 202, in __call__, the following error occurred:\nValueError: could not broadcast input array from shape (4,2) into shape (2,2)\nOn line: Ucand[:, :k_take] = U_s[:, :k_take]", "parent_ids": "dd5952f4-6c1c-4ad0-88fc-9c8573e0aa0f", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "c6a73a25-f48d-45d3-a020-c892a33c5a36", "fitness": 0.1102810209708885, "name": "CASLS", "description": "The algorithm is a CMA-inspired hybrid that adapts a global step and per-coordinate variances (coord_var) while keeping a short memory of recent successful unit directions (mem) and an evolution path (ps) to steer search; defaults emphasize moderate memory (memory_size=12), modest initial step (init_step_scale=0.25) and gentle step growth/shrink (grow=1.25, shrink=0.7). It samples inside small orthonormal subspaces that mix recent directions and random vectors (subspace size k ≈ ceil(n^subspace_exp) with subspace_exp=0.6) and generates mirrored probe pairs (probes_factor*k) with occasional heavy-tailed (Cauchy) coefficient draws and anisotropic per-coordinate scaling. Promising directions trigger an inexpensive 3‑point parabolic line minimizer (quick_parabola), and a periodic diagonal quadratic (ridge) model is fit from an archive (model_every=25) to propose coordinate-wise Newton-like corrections limited by trust radii derived from coord_var. Budget safety, box clipping and archive pruning are enforced, and successful moves update coord_var, mem and ps to bias future sampling while failures cause controlled step shrinkage and occasional perturbations.", "code": "import numpy as np\n\nclass CASLS:\n    \"\"\"\n    CMA-inspired Adaptive Subspace Lévy Search (CASLS)\n\n    Key ideas:\n    - Use adaptive per-coordinate scaling (diag-cov) and an evolution path to adapt a global step size.\n    - Build small orthonormal subspaces mixing recent successful directions (memory) and random vectors.\n    - Perform mirrored subspace probes (paired +/−) with occasional heavy-tailed coefficients (Cauchy)\n      and immediate parabolic 1-D refinements on promising directions.\n    - Keep an archive and periodically fit a diagonal quadratic (ridge) model for a focused proposal.\n    - Budget-safe, respects box bounds, and uses multiple safeguards for numerical stability.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=12, init_step_scale=0.25, subspace_exp=0.6,\n                 probes_factor=3, grow=1.25, shrink=0.7,\n                 p_cauchy=0.15, cauchy_scale=1.0, model_every=25):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.init_step_scale = float(init_step_scale)\n        self.subspace_exp = float(subspace_exp)\n        self.probes_factor = int(probes_factor)\n        self.grow = float(grow)\n        self.shrink = float(shrink)\n        self.p_cauchy = float(p_cauchy)\n        self.cauchy_scale = float(cauchy_scale)\n        self.model_every = int(model_every)\n        # RNG\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        lb = lb.reshape(n,)\n        ub = ub.reshape(n,)\n\n        domain = np.maximum(1e-12, np.mean(ub - lb))\n\n        evals = 0\n\n        # safe eval wrapper\n        f_best = np.inf\n        x_best = None\n\n        def clip(x):\n            return np.minimum(ub, np.maximum(lb, x))\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= self.budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x.copy()\n\n        # initial point\n        x_cur = self.rng.uniform(lb, ub)\n        out = safe_eval(x_cur)\n        if out[0] is None:\n            return float(f_best), np.asarray(x_best, dtype=float)\n        f_cur = float(out[0])\n        x_cur = out[1]\n\n        # step sizes\n        step = max(self.init_step_scale * domain, 1e-12)\n        min_step = 1e-12 * domain\n        max_step = 2.0 * domain\n\n        # per-coordinate variance (diag-cov proxy), RMS-like\n        coord_var = np.ones(n) * 1e-6\n        coord_alpha = 0.18\n\n        # evolution path (for step adaptation)\n        ps = np.zeros(n)\n        ps_decay = 0.85\n        # small memory of recent directions\n        mem = []\n        # archive for model building (keep points and function values)\n        archive_X = [x_cur.copy()]\n        archive_F = [f_cur]\n\n        iter_count = 0\n        gen_since_model = 0\n\n        # quick 3-point parabolic line minimizer (cheap)\n        def quick_parabola(x0, f0, d, delta, max_evals=4):\n            # returns (f_new, x_new) or (None, None)\n            nonlocal evals\n            if evals >= self.budget:\n                return None, None\n            dn = np.linalg.norm(d)\n            if dn == 0:\n                return None, None\n            d = d / dn\n            rem = self.budget - evals\n            # evaluate +delta and -delta if possible\n            pts = [(0.0, f0, x0.copy())]\n            if rem <= 0:\n                return None, None\n            # +delta\n            if rem >= 1:\n                xp = clip(x0 + delta * d)\n                outp = safe_eval(xp)\n                if outp[0] is None:\n                    return None, None\n                pts.append((delta, outp[0], outp[1]))\n                rem = self.budget - evals\n            if rem >= 1:\n                xm = clip(x0 - delta * d)\n                outm = safe_eval(xm)\n                if outm[0] is None:\n                    return None, None\n                pts.append((-delta, outm[0], outm[1]))\n                rem = self.budget - evals\n            # need at least two sampled points aside from 0 to attempt fit\n            if len(pts) < 3:\n                # return best non-zero if better than f0\n                best = min(pts, key=lambda t: t[1])\n                if best[1] < f0 - 1e-12 and best[0] != 0.0:\n                    return best[1], best[2].copy()\n                return None, None\n            # fit parabola via three points (take first three)\n            alphas = np.array([p[0] for p in pts[:3]])\n            fs = np.array([p[1] for p in pts[:3]])\n            M = np.vstack([alphas**2, alphas, np.ones_like(alphas)]).T\n            try:\n                A, B, C = np.linalg.solve(M, fs)\n            except np.linalg.LinAlgError:\n                # fallback to best sample\n                best = min(pts, key=lambda t: t[1])\n                if best[1] < f0 - 1e-12:\n                    return best[1], best[2].copy()\n                return None, None\n            if A == 0:\n                best = min(pts, key=lambda t: t[1])\n                if best[1] < f0 - 1e-12 and best[0] != 0.0:\n                    return best[1], best[2].copy()\n                return None, None\n            alpha_star = -B / (2.0 * A)\n            # guard alpha_star range\n            span = max(1e-12, np.ptp(alphas))\n            alpha_star = np.clip(alpha_star, alphas.min() - 0.5 * span, alphas.max() + 0.5 * span)\n            if self.budget - evals <= 0:\n                # can't evaluate optimum; fallback\n                best = min(pts, key=lambda t: t[1])\n                if best[1] < f0 - 1e-12 and best[0] != 0.0:\n                    return best[1], best[2].copy()\n                return None, None\n            xopt = clip(x0 + alpha_star * d)\n            outo = safe_eval(xopt)\n            if outo[0] is None:\n                return None, None\n            if outo[0] < f0 - 1e-12:\n                return outo[0], outo[1].copy()\n            # otherwise return best sampled if better\n            best = min(pts + [(alpha_star, outo[0], outo[1])], key=lambda t: t[1])\n            if best[1] < f0 - 1e-12:\n                return best[1], best[2].copy()\n            return None, None\n\n        # model fit: diagonal quadratic (ridge) using best mu from archive\n        def try_model_proposal():\n            nonlocal evals, f_best, x_best, x_cur, f_cur, coord_var\n            if len(archive_X) < n + 2:\n                return False\n            # pick top points\n            arrF = np.array(archive_F)\n            idx = np.argsort(arrF)[:min(len(arrF), 4 * n)]\n            Xm = np.array([archive_X[i] for i in idx])\n            Fm = np.array([archive_F[i] for i in idx])\n            # centre at x_cur to reduce numeric scale\n            dx = Xm - x_cur\n            # design: [dx_i, 0.5*dx_i^2, 1]\n            M = np.hstack([dx, 0.5 * (dx ** 2), np.ones((dx.shape[0], 1))])\n            # weights by proximity\n            dists = np.linalg.norm(dx, axis=1) + 1e-12\n            w = 1.0 / (1.0 + dists)\n            W = np.sqrt(w)[:, None]\n            A = W * M\n            b = W * Fm\n            reg = 1e-6\n            try:\n                params, *_ = np.linalg.lstsq(A.T @ A + reg * np.eye(A.shape[1]), A.T @ b, rcond=None)\n                params = params.flatten()\n                lin = params[:n]\n                quad = params[n:2*n]\n                # stabilize quad\n                quad_safe = np.copy(quad)\n                quad_safe[quad_safe < 1e-8] = 1e-8\n                delta = -lin / (quad_safe + 1e-20)\n                # clamp by coordinate trust (coord_var) and a global trust radius\n                trust = np.sqrt(coord_var) * step * 2.0\n                delta_limited = np.clip(delta, -trust, trust)\n                x_prop = clip(x_cur + delta_limited)\n                if evals >= self.budget:\n                    return False\n                out = safe_eval(x_prop)\n                if out[0] is None:\n                    return False\n                if out[0] < f_cur - 1e-12:\n                    # accept\n                    x_cur[:] = out[1]\n                    f_cur = float(out[0])\n                    # update memory and coord_var\n                    disp = delta_limited.copy()\n                    dn = np.linalg.norm(disp)\n                    if dn > 1e-12:\n                        unit = disp / dn\n                        mem.insert(0, unit.copy())\n                        if len(mem) > self.memory_size:\n                            mem.pop()\n                        # update coord_var via RMS of unit direction\n                        coord_var[:] = (1 - coord_alpha) * coord_var + coord_alpha * (unit ** 2 + 1e-12)\n                    return True\n                else:\n                    # shrink step a bit\n                    return False\n            except Exception:\n                return False\n\n        # main loop\n        while evals < self.budget:\n            iter_count += 1\n            gen_since_model += 1\n            # adapt subspace size\n            k = max(1, int(np.clip(int(np.ceil(n ** self.subspace_exp)), 1, n)))\n            probes = max(6, self.probes_factor * k)\n\n            # build basis from mem + random, orthonormalize\n            chosen = []\n            use_mem = min(len(mem), k // 2)\n            if use_mem > 0:\n                # prefer recent\n                chosen = [mem[i] for i in range(use_mem)]\n            needed = k - len(chosen)\n            if needed > 0:\n                R = self.rng.normal(size=(n, needed))\n                # bias by coord scaling\n                R = R * np.sqrt(coord_var)[:, None]\n                if chosen:\n                    R = np.column_stack((np.column_stack(chosen), R))\n                Q, _ = np.linalg.qr(R)\n                basis = Q[:, :k]\n            else:\n                B = np.column_stack(chosen)\n                Q, _ = np.linalg.qr(B)\n                basis = Q[:, :k]\n\n            improved_round = False\n            # produce mirrored probes in subspace: sample coefficients; sometimes heavy-tailed\n            # We'll generate ceil(probes/2) base samples and mirror them\n            base = (probes + 1) // 2\n            coeffs_list = []\n            for _ in range(base):\n                if self.rng.random() < 0.12:\n                    # heavy tail via cauchy on coefficients\n                    c = np.tan(np.pi * (self.rng.random(k) - 0.5)) * 0.8\n                else:\n                    # gaussian with anisotropic scale from projected coord_var\n                    c = self.rng.normal(scale=1.0, size=k)\n                coeffs_list.append(c)\n\n            # Evaluate candidates\n            for c in coeffs_list:\n                if evals >= self.budget:\n                    break\n                # build direction and scale by coordinate sqrt variance\n                d = basis @ c\n                # apply per-coordinate sqrt scaling (anisotropy)\n                d = d * np.sqrt(coord_var)\n                nd = np.linalg.norm(d) + 1e-20\n                d_unit = d / nd\n                # sample alpha magnitude with folded normal or uniform\n                if self.rng.random() < 0.18:\n                    alpha = abs(self.rng.normal()) * step * (1.0 + 0.8 * self.rng.random())\n                else:\n                    alpha = self.rng.uniform(0.2 * step, 1.0 * step)\n                # mirrored pair: +alpha and -alpha\n                for sign in (+1.0, -1.0):\n                    if evals >= self.budget:\n                        break\n                    x_try = clip(x_cur + sign * alpha * d_unit)\n                    out = safe_eval(x_try)\n                    if out[0] is None:\n                        break\n                    f_try = out[0]; x_ret = out[1]\n                    # store to archive\n                    archive_X.append(x_ret.copy()); archive_F.append(f_try)\n                    # immediate accept\n                    if f_try < f_cur - 1e-12:\n                        # perform quick parabola along d_unit with delta approx alpha\n                        # prefer delta = max(alpha, 0.8*step)\n                        delta = max(alpha, 0.8 * step)\n                        res = quick_parabola(x_cur, f_cur, d_unit, delta=min(delta, 2.0 * step))\n                        if res[0] is not None:\n                            f_new, x_new = res\n                        else:\n                            f_new, x_new = f_try, x_ret\n                        # accept best\n                        if f_new < f_cur - 1e-12:\n                            # update evolution path as displacement\n                            disp = x_new - x_cur\n                            ps = ps_decay * ps + (1 - ps_decay) * disp\n                            # update coord_var RMS\n                            coord_var = (1 - coord_alpha) * coord_var + coord_alpha * ((disp / (np.linalg.norm(disp) + 1e-20)) ** 2 + 1e-12)\n                            # record memory\n                            dn = np.linalg.norm(disp)\n                            if dn > 1e-12:\n                                mem.insert(0, (disp / dn).copy())\n                                if len(mem) > self.memory_size:\n                                    mem.pop()\n                            x_cur = x_new.copy()\n                            f_cur = float(f_new)\n                            improved_round = True\n                        else:\n                            # small accept if raw improved but parabola didn't: accept raw\n                            if f_try < f_cur - 1e-12:\n                                disp = x_ret - x_cur\n                                ps = ps_decay * ps + (1 - ps_decay) * disp\n                                coord_var = (1 - coord_alpha) * coord_var + coord_alpha * ((disp / (np.linalg.norm(disp) + 1e-20)) ** 2 + 1e-12)\n                                dn = np.linalg.norm(disp)\n                                if dn > 1e-12:\n                                    mem.insert(0, (disp / dn).copy())\n                                    if len(mem) > self.memory_size:\n                                        mem.pop()\n                                x_cur = x_ret.copy()\n                                f_cur = float(f_try)\n                                improved_round = True\n                        # adapt step on success\n                        step = min(step * self.grow, max_step)\n                        # occasionally do a Cauchy jump along memory\n                        if mem and self.rng.random() < 0.12:\n                            u = mem[self.rng.integers(len(mem))]\n                            jump = np.tan(np.pi * (self.rng.random() - 0.5)) * self.cauchy_scale * step\n                            xj = clip(x_cur + jump * u)\n                            if evals < self.budget:\n                                outj = safe_eval(xj)\n                                if outj[0] is not None:\n                                    archive_X.append(outj[1].copy()); archive_F.append(outj[0])\n                                    if outj[0] < f_cur - 1e-12:\n                                        disp = outj[1] - x_cur\n                                        dn = np.linalg.norm(disp)\n                                        if dn > 1e-12:\n                                            mem.insert(0, (disp / dn).copy())\n                                            if len(mem) > self.memory_size:\n                                                mem.pop()\n                                        x_cur = outj[1].copy()\n                                        f_cur = float(outj[0])\n                                        step = min(step * self.grow, max_step)\n                        # adjust evolution path-based mild step scaling\n                        ps_n = np.linalg.norm(ps)\n                        if ps_n > 0:\n                            # if persistent direction magnitude grows, slightly increase step\n                            step = np.clip(step * (1.0 + 0.5 * (ps_n / (np.linalg.norm(coord_var) + 1e-12))), min_step, max_step)\n                    else:\n                        # failure case: maybe try rare parabola to exploit curvature\n                        if self.rng.random() < 0.03 and self.budget - evals >= 3:\n                            res = quick_parabola(x_cur, f_cur, d_unit, delta=0.6 * step, max_evals=3)\n                            if res[0] is not None:\n                                f_new, x_new = res\n                                if f_new < f_cur - 1e-12:\n                                    disp = x_new - x_cur\n                                    ps = ps_decay * ps + (1 - ps_decay) * disp\n                                    coord_var = (1 - coord_alpha) * coord_var + coord_alpha * ((disp / (np.linalg.norm(disp) + 1e-20)) ** 2 + 1e-12)\n                                    dn = np.linalg.norm(disp)\n                                    if dn > 1e-12:\n                                        mem.insert(0, (disp / dn).copy())\n                                        if len(mem) > self.memory_size:\n                                            mem.pop()\n                                    x_cur = x_new.copy()\n                                    f_cur = float(f_new)\n                                    improved_round = True\n                                    step = min(step * self.grow, max_step)\n\n            # end probes loop\n\n            # if no improvement this round, shrink step somewhat\n            if not improved_round:\n                step = max(step * self.shrink, min_step)\n                # decay memory effectiveness gently\n                coord_var = np.clip((1 - 0.01) * coord_var + 0.01 * np.ones_like(coord_var) * 1e-6, 1e-12, 1e6)\n\n            # periodic model attempt\n            if gen_since_model >= self.model_every:\n                gen_since_model = 0\n                try_model_proposal()\n\n            # prune archive to reasonable size\n            max_archive = max(200, 40 * n)\n            if len(archive_X) > max_archive:\n                idxs = np.argsort(archive_F)\n                keep = list(idxs[:min(200, len(idxs))])\n                rest = idxs[200:]\n                if len(rest) > 0:\n                    step_sel = max(1, len(rest) // (max_archive - 200))\n                    keep += list(rest[::step_sel])\n                keep = np.unique(keep).tolist()\n                archive_X = [archive_X[i] for i in keep]\n                archive_F = [archive_F[i] for i in keep]\n\n            # small stagnation check and perturbation\n            if iter_count % max(3, int(6 + np.log1p(n))) == 0 and not improved_round:\n                # small anisotropic perturb around best known\n                if x_best is not None and self.rng.random() < 0.25:\n                    perturb = self.rng.normal(scale=0.6 * step, size=n) * np.sqrt(coord_var)\n                    x_new = clip(x_best + perturb)\n                    if evals < self.budget:\n                        outp = safe_eval(x_new)\n                        if outp[0] is not None:\n                            archive_X.append(outp[1].copy()); archive_F.append(outp[0])\n                            if outp[0] < f_cur - 1e-12:\n                                disp = outp[1] - x_cur\n                                dn = np.linalg.norm(disp)\n                                if dn > 1e-12:\n                                    mem.insert(0, (disp / dn).copy())\n                                    if len(mem) > self.memory_size:\n                                        mem.pop()\n                                x_cur = outp[1].copy()\n                                f_cur = float(outp[0])\n                                step = min(step * self.grow, max_step)\n\n            # enforce budget safety and loop\n            if evals >= self.budget:\n                break\n\n        # final return\n        if x_best is None:\n            x_best = x_cur.copy()\n            f_best = float(f_cur)\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm CASLS scored 0.110 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "dd5952f4-6c1c-4ad0-88fc-9c8573e0aa0f", "operator": null, "metadata": {"aucs": [0.04407074795935262, 0.15011846541964924, 0.1981581516002724, 0.14737788073470304, 0.10565410534849318, 0.17026141488711144, 0.00026409194253662616, 0.13409959186730247, 0.014726099325732545, 0.13807966062373145]}, "task_prompt": ""}
{"id": "46d2d1d5-b1df-4518-aa1b-5be4173d534b", "fitness": "-inf", "name": "AEGIS", "description": "AEGIS combines a low-rank adaptive eigen-basis (B, size k ≈ √dim) with a three-channel sampler that mixes eigen-subspace steps, coordinate-wise RMS-scaled complement steps (D = 1/√(S+ε)) and occasional isotropic/Cauchy heavy-tailed jumps (p_cauchy ~ 0.16), using mirrored pairing and a momentum drift to reduce variance and promote directed exploration. It adaptively moves a global step-size sigma multiplicatively toward a success target and learns a mixture weight gamma that biases sampling to eigen directions when those directions correlate with improvements, while S (β_rms=0.88) accumulates second-moment statistics for per-coordinate scaling. The eigen-basis is updated by an improvement-weighted “power-like” push (eig_lr ≈ 0.06) with occasional QR re-orthonormalization and small randomization under stagnation to prevent collapse. An archive supports DE-like recombination (rand-to-best style with F_arch, cr_arch), opposition perturbations, and stagnation-triggered directed Lévy/Cauchy burst trials; practical safeguards (clipping to bounds, budget-aware evaluation, sigma/domain limits and archive size) and population sizing (λ ∝ log(dim), μ = λ/2) stabilize search.", "code": "import numpy as np\n\nclass AEGIS:\n    \"\"\"\n    AEGIS: Adaptive Eigen-Guided Lévy Search\n\n    Key ideas:\n    - Maintain a low-rank eigen-basis B (k ~ dim^0.5) that is continuously biased\n      toward directions that historically produced improvements (power-like updates).\n    - Combine three exploratory channels: (a) eigen-subspace exploration, (b)\n      coordinate-wise (RMS-like) scaled complement exploration, (c) isotropic or\n      Lévy/Cauchy heavy-tailed jumps for escapes.\n    - Two-tier step-size adaptation: global sigma and eigen-scale sigma_eig adapt\n      from recent success statistics. Also adaptive mixture gamma between eigen\n      vs complement directions.\n    - Archive-guided recombination: use archive members to propose differential\n      adjustments and opposition-based perturbations to exploit past good points.\n    - Stagnation handling: directed Lévy bursts + soft restart of mean.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, pop=None, k=None, init_sigma=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        self.seed = seed\n\n        # population heuristics\n        if pop is None:\n            self.lambda_ = max(6, int(np.floor(4 + 3.0 * np.log(max(2, self.dim)))))\n        else:\n            self.lambda_ = int(pop)\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank eigen basis size\n        if k is None:\n            self.k = max(1, int(np.ceil(self.dim ** 0.5)))\n        else:\n            self.k = min(max(1, int(k)), self.dim)\n\n        # initial scales\n        if init_sigma is None:\n            # more conservative by default; user space is [-5,5]\n            self.init_sigma = 0.12 * 10.0\n        else:\n            self.init_sigma = float(init_sigma)\n\n        # RMS-like accumulator parameters\n        self.beta_rms = 0.88\n        self.rms_eps = 1e-8\n        self.rms_init = 5e-4\n\n        # eigen updater\n        self.eig_lr = 0.06      # learning rate for eigen-basis adaptation\n        self.reorthonormalize_prob = 0.12\n\n        # momentum/inertia\n        self.momentum_decay = 0.88\n        self.momentum_scale = 0.45\n\n        # mixture between eigensubspace vs complement (learned)\n        self.gamma = 0.35\n        self.gamma_lr = 0.12\n        self.gamma_target = 0.28\n\n        # heavy-tail parameters\n        self.p_cauchy = 0.16    # probability of Cauchy/Lévy-like jump\n        self.cauchy_scale = 1.2\n\n        # archive & DE-like usage\n        self.archive_limit = 2500\n        self.p_archive = 0.20\n        self.F_arch = 0.65\n        self.cr_arch = 0.55\n\n        # adaptation target success\n        self.success_target = 0.2\n        self.sigma_adapt_strength = 0.5\n\n        # stagnation detection / restart\n        self.stagnation_window = max(30, 4 * self.dim)\n        self.burst_multiplier = 3.5\n        self.max_burst_trials = 10\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds handling (assume -5..5 default)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initial mean uniformly inside bounds\n        m = np.random.uniform(lb, ub)\n        sigma = float(max(1e-12, self.init_sigma))\n\n        # eigen-basis B: orthonormal columns (n x k)\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            B = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            B = rand.copy()\n            for j in range(B.shape[1]):\n                B[:, j] /= (np.linalg.norm(B[:, j]) + 1e-12)\n\n        # RMS-like diagonal scale accumulator and diagonal D = 1/sqrt(S + eps)\n        S = np.ones(n) * float(self.rms_init)\n        D = 1.0 / np.sqrt(S + self.rms_eps)\n\n        # momentum vector\n        v = np.zeros(n)\n\n        # archive\n        archive_X = []\n        archive_F = []\n\n        # rank-to-weight: use soft-exponential (decreasing)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        ranks = np.arange(1, mu + 1)\n        tau = 0.85\n        raw_w = np.exp(-tau * (ranks - 1.0))\n        weights = raw_w / np.sum(raw_w)\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = fm; x_best = xm.copy()\n            last_mean_f = fm\n        else:\n            last_mean_f = np.inf\n\n        # dynamic stats for adaptation\n        recent_successes = []\n        success_window = max(8, lam)\n\n        # auxiliary small helper: sample isotropic Cauchy-Lévy direction scaled to median D\n        def cauchy_jump(scale=1.0):\n            z = np.random.standard_cauchy(n) * scale\n            # clamp extreme tails to avoid inf/nans; rescale robustly\n            z = np.clip(z, -1e6, 1e6)\n            # normalize direction but keep heavy-tailed magnitude distribution by scaling\n            nz = np.linalg.norm(z) + 1e-12\n            return z / nz\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # sample base normals\n            base_z = np.random.randn(current_lambda, n)\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # project into eigen-subspace and complement\n                proj_eig = B @ (B.T @ z)\n                proj_comp = z - proj_eig\n\n                # coordinate-scale complement (elementwise)\n                y_comp = D * proj_comp\n\n                # eigen-subspace scaled by adaptive per-eig scale:\n                # scale eigen part by median(D) times an eigen scale factor (learned through sigma_eig)\n                medD = float(np.median(D))\n                # eigen-scale proportional to current sigma (implicit); allow extra per-iteration factor\n                eig_scale = 1.0  # will be adapted indirectly by gamma and sigma\n                y_eig = medD * eig_scale * proj_eig\n\n                # isotropic normal channel\n                y_iso = z\n\n                # decide on heavy-tailed jump\n                if np.random.rand() < self.p_cauchy:\n                    # use Cauchy direction, but combine with median scaling to keep units sane\n                    y = 0.9 * medD * cauchy_jump(self.cauchy_scale)\n                else:\n                    # mix eigen vs complement vs isotropic using gamma\n                    # gamma biases toward eigen directions\n                    g = self.gamma\n                    # small randomness to avoid determinism in mixing\n                    rr = np.random.rand()\n                    # if rr<0.08 favor isotropic exploratory channel for diversity\n                    if rr < 0.08:\n                        y = y_iso\n                    else:\n                        y = g * y_eig + (1.0 - g) * y_comp\n\n                # occasional mirrored pairing to reduce sampling variance\n                if (i % 2 == 1):\n                    y = -y\n\n                # build candidate with momentum drift\n                x = m + sigma * y + self.momentum_scale * v\n\n                # archive-guided differential recombination (opportunity)\n                if (np.random.rand() < self.p_archive) and (len(archive_X) >= 3):\n                    # select three distinct members from archive (or current best)\n                    ids = np.random.choice(len(archive_X), size=3, replace=False)\n                    xa = archive_X[ids[0]]; xb = archive_X[ids[1]]; xc = archive_X[ids[2]]\n                    # Rand-to-best-like: push toward best while adding differential diversity\n                    if x_best is not None:\n                        de_mut = self.F_arch * (x_best - xb) + 0.25 * (xa - xc)\n                    else:\n                        de_mut = self.F_arch * (xa - xb) + 0.25 * (xc - xb)\n                    # per-dimension crossover\n                    mask = (np.random.rand(n) < self.cr_arch)\n                    x[mask] = np.clip(x[mask] + de_mut[mask], lb[mask], ub[mask])\n\n                    # small opposition-based perturbation: reflect relative to mean with small prob\n                    if np.random.rand() < 0.06:\n                        opp = np.clip(m - 0.6 * (x - m), lb, ub)\n                        # adopt the opposition if it increases diversity (we'll evaluate later)\n                        x = 0.5 * x + 0.5 * opp\n\n                # clip and store\n                x = np.clip(x, lb, ub)\n                arx[i] = x\n                arz[i] = y\n\n            # Evaluate candidates with strict budget checking\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # selection: take best mu candidates\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n            f_sel = arfit[sel]\n\n            # compute weighted recombination into new mean\n            # weights from rank-to-weight earlier\n            # if fewer than mu survivors (rare), renormalize weights\n            if len(sel) < len(weights):\n                w = weights[:len(sel)]\n                w = w / np.sum(w)\n            else:\n                w = weights\n            m_old = m.copy()\n            m = np.sum(w[:, None] * x_sel, axis=0)\n\n            # compute weighted average step in projection space (y)\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # success measure: did any selected candidate beat last_mean_f?\n            sel_best_f = float(np.min(f_sel)) if len(f_sel) > 0 else np.inf\n            improved = 1 if sel_best_f < last_mean_f - 1e-12 else 0\n\n            # update success history window\n            recent_successes.append(improved)\n            if len(recent_successes) > success_window:\n                recent_successes.pop(0)\n            success_rate = float(np.mean(recent_successes)) if recent_successes else 0.0\n\n            # adapt gamma: move toward eigen usage when eigen-projected steps correlated with improvement\n            # compute correlation of projections onto eigen-basis with improvements if possible\n            # measure signal energy along eigen vs complement among winners\n            if y_sel.shape[0] > 0:\n                proj_e_levels = np.sum((B.T @ y_sel.T)**2, axis=0)  # energy per selected in eig-space\n                comp_levels = np.sum((y_sel - (B @ (B.T @ y_sel.T)).T)**2, axis=1)\n                # if energy in eig-space correlates with improvement (smaller f_sel), increase gamma\n                # compute improvement magnitudes relative to last_mean_f\n                imp = np.maximum(0.0, last_mean_f - f_sel)\n                if np.sum(imp) > 0:\n                    score_eig = np.dot(imp, proj_e_levels) / (np.sum(imp) + 1e-12)\n                    score_comp = np.dot(imp, comp_levels) / (np.sum(imp) + 1e-12)\n                    # drive gamma toward ratio\n                    target_gamma = 0.5 * (score_eig / (score_eig + score_comp + 1e-12)) + 0.25\n                    # exponential smoothing move\n                    self.gamma *= np.exp(self.gamma_lr * (target_gamma - self.gamma))\n                    self.gamma = float(np.clip(self.gamma, 0.02, 0.95))\n            else:\n                # mild decay toward prior target\n                self.gamma = float(np.clip(self.gamma * (1.0 - 0.02) + 0.02 * self.gamma_target, 0.02, 0.95))\n\n            # update momentum\n            v = self.momentum_decay * v + (1.0 - self.momentum_decay) * (sigma * y_w)\n\n            # update RMS-like second moment S from selected y's\n            if y_sel.size > 0:\n                y2 = np.sum(w[:, None] * (y_sel ** 2), axis=0)\n            else:\n                y2 = np.zeros(n)\n            S = self.beta_rms * S + (1.0 - self.beta_rms) * (y2 + self.rms_eps)\n            D = 1.0 / np.sqrt(S + self.rms_eps)\n\n            # adapt sigma multiplicatively using soft success target (two-tier)\n            # small factor for stability, stronger when many successes/ failures\n            adapt_exp = (success_rate - self.success_target)\n            adapt_factor = np.exp(self.sigma_adapt_strength * adapt_exp / max(0.06, np.sqrt(1.0 + n / 10.0)))\n            sigma *= adapt_factor\n            # bound sigma to sensible domain-relative ranges\n            domain_scale = np.mean(ub - lb)\n            sigma = float(np.clip(sigma, 1e-12, 60.0 * domain_scale))\n\n            # update eigen-basis B using a power-like update driven by weighted improvement directions\n            # compute improvement-weighted direction (signal)\n            if y_sel.shape[0] > 0:\n                imp = np.maximum(0.0, last_mean_f - f_sel)\n                if np.sum(imp) > 0:\n                    # normalize improvement weights\n                    imp_w = imp / (np.sum(imp) + 1e-12)\n                    signal = np.sum((imp_w[:, None] * y_sel), axis=0)\n                else:\n                    # use magnitude-weighted direction (non-improving but energetic)\n                    magn = np.linalg.norm(y_sel, axis=1) + 1e-12\n                    mag_w = magn / np.sum(magn)\n                    signal = np.sum((mag_w[:, None] * y_sel), axis=0)\n                sig_norm = np.linalg.norm(signal)\n                if sig_norm > 1e-12:\n                    signal = signal / (sig_norm + 1e-12)\n                    # for each eigenvector, nudge toward the signal orthogonally\n                    for j in range(B.shape[1]):\n                        u = B[:, j]\n                        proj = float(np.dot(u, signal))\n                        # incremental orthogonal push with eigen LR\n                        u = u + self.eig_lr * (signal - proj * u)\n                        # normalize\n                        u = u / (np.linalg.norm(u) + 1e-12)\n                        B[:, j] = u\n                    # occasional re-orthonormalize\n                    if np.random.rand() < self.reorthonormalize_prob:\n                        try:\n                            Q, _ = np.linalg.qr(B)\n                            B = Q[:, :self.k]\n                        except np.linalg.LinAlgError:\n                            pass\n\n            # novel local covariance inflation: if recent success very low, slightly randomize B to avoid stagnation\n            if success_rate < 0.02 and np.random.rand() < 0.05:\n                noise = 0.01 * np.random.randn(*B.shape)\n                B += noise\n                try:\n                    Q, _ = np.linalg.qr(B)\n                    B = Q[:, :self.k]\n                except np.linalg.LinAlgError:\n                    pass\n\n            # stagnation detection and burst maneuvers\n            if len(archive_F) >= self.stagnation_window:\n                recent = archive_F[-self.stagnation_window:]\n                # if best in recent window hasn't improved beyond a small tolerance\n                if np.min(recent) >= (f_best - 1e-12):\n                    # perform a burst of directed Cauchy/Lévy trials around mean and around archive extremes\n                    old_sigma = sigma\n                    sigma = min(old_sigma * self.burst_multiplier, 80.0 * domain_scale)\n                    tries = min(self.max_burst_trials, budget - evals)\n                    for t in range(tries):\n                        # blend: some trials pure Cauchy, some hybrid towards distant archive points\n                        if np.random.rand() < 0.5 or len(archive_X) < 3:\n                            y_b = cauchy_jump(self.cauchy_scale * 1.6) * float(np.median(D))\n                        else:\n                            # pick far archive member: maximize distance from current mean\n                            idx_far = np.argmax([np.linalg.norm(x - m) for x in archive_X])\n                            far = archive_X[idx_far]\n                            dir_far = far - m\n                            dir_far = dir_far / (np.linalg.norm(dir_far) + 1e-12)\n                            y_b = dir_far * (np.random.standard_cauchy() * 0.7 * float(np.median(D)))\n                        x_try = np.clip(m + sigma * y_b, lb, ub)\n                        f_try = func(x_try)\n                        evals += 1\n                        archive_X.append(x_try.copy()); archive_F.append(f_try)\n                        if f_try < f_best:\n                            f_best = f_try\n                            x_best = x_try.copy()\n                            # adopt found point as new mean, reset momentum and raise sigma mildly\n                            m = x_try.copy()\n                            v = np.zeros(n)\n                            sigma = max(sigma, old_sigma * 0.8)\n                            break\n                    # restore sigma\n                    sigma = old_sigma\n\n            # update last_mean_f for next iteration\n            last_mean_f = float(np.mean(arfit[sel])) if len(sel) > 0 else last_mean_f\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "23e9cf3f-d0e0-406d-bd6d-72692cfe6f5b", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "2b76690f-1d64-4160-9364-38895e05d483", "fitness": 0.5226583954265656, "name": "STTE_Levy", "description": "The algorithm is a hybrid evolutionary strategy that mixes a low-rank subspace model (k ≈ dim^0.7) with a diagonal (per-coordinate) Adam-like second-moment scaler, uses a small population λ (log-scaled with dim) and mirrored sampling to generate candidates inside the [-5,5] bounds. Step magnitudes are controlled by a relatively large initial sigma (init_sigma ≈ 2.0) that is gently adapted by recent success rate and a decaying temperature, while an additive alpha (start 0.35 → target 0.28, lr 0.07) blends subspace vs diagonal contributions. Search diversity comes from Oja updates to the subspace (oja_eta 0.06, occasional re-orthonormalization), momentum/inertia (momentum 0.85, momentum_scale 0.6), DE-style current-to-pbest/1 recombination with an archive (p_de 0.18, F_de 0.6, cr_de 0.5, p_best 0.2), and heavy-tailed Lévy/Cauchy jumps (p_levy 0.22, levy_scale 1.6) for exploratory probes. Robustness features include an archive (limit 2000), stagnation-triggered large Cauchy probes, QR reorthonormalization of U, bias-corrected S (beta2 0.98, S_init 1e-4), and conservative clipping to bounds.", "code": "import numpy as np\n\nclass STTE_Levy:\n    \"\"\"\n    STTE_Levy: Subspace-Tempered Tuned Evolution with Lévy-like (Cauchy) Jumps\n\n    Key elements (different choices vs the provided SPIRIT):\n    - subspace dimension k ~ dim^0.7\n    - Adam-style second moment (beta2) with bias correction for diagonal scaling\n    - Oja update for subspace with different eta and less frequent re-orthonormalization\n    - Cauchy heavy-tailed jumps (df ~ 1) with different p_levy and scale\n    - current-to-pbest/1 DE-style recombination with different F/cr and p_best\n    - additive alpha adaptation (rather than multiplicative) toward target\n    - tempering factor T that decays, reducing exploratory jump magnitudes over time\n    - step-size sigma adaptation by logistic-like mapping using recent success rate\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 subspace_k=None, init_sigma=None,\n                 p_levy=0.22, levy_scale=1.6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        self.seed = seed\n\n        # population: slightly larger than minimal, different scaling\n        self.lambda_ = max(6, int(4 + 3.0 * np.log(max(2, self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank subspace with exponent 0.7\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(self.dim ** 0.7)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # initial sigma (a bit more exploratory than very conservative)\n        if init_sigma is None:\n            self.init_sigma = 0.2 * 10.0  # 0.2 * range(=10) -> 2.0\n        else:\n            self.init_sigma = float(init_sigma)\n\n        # heavy-tailed (Cauchy-like) jumps\n        self.p_levy = float(p_levy)\n        self.levy_scale = float(levy_scale)\n\n        # DE current-to-pbest/1 style recombination\n        self.p_de = 0.18\n        self.F_de = 0.60\n        self.cr_de = 0.5\n        self.p_best = 0.2  # proportion of archive/pop to consider pbest\n\n        # Oja update parameters (different eta)\n        self.oja_eta = 0.06\n        self.oja_reorth_freq = 0.08\n\n        # momentum / inertia\n        self.momentum = 0.85\n        self.momentum_scale = 0.6\n\n        # mixing alpha between diagonal & subspace (additive update)\n        self.alpha = 0.35\n        self.alpha_target = 0.28\n        self.alpha_lr = 0.07  # additive adjustment per generation\n\n        # Adam-like second moment (beta2) for diagonal scaling\n        self.beta2 = 0.98\n        self.S_eps = 1e-8\n        self.S_init = 1e-4\n\n        # archive\n        self.archive_limit = 2000\n\n        # mirrored sampling\n        self.mirror = True\n\n        # tempering (reduces exploratory magnitude slowly)\n        self.temperature = 1.0\n        self.T_decay = 0.9995  # per generation multiplicative decay\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds: BBOB is [-5,5] but attempt to read func.bounds if provided\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        # initial mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = max(1e-12, float(self.init_sigma))\n\n        # Adam-style second moment accumulator (uncorrected, we'll bias-correct)\n        S = np.ones(n) * float(self.S_init)\n        S_step = 0\n\n        # low-rank orthonormal subspace init\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            U = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = rand.copy()\n            for j in range(U.shape[1]):\n                U[:, j] /= (np.linalg.norm(U[:, j]) + 1e-12)\n\n        # inertia\n        v = np.zeros(n)\n\n        # archive\n        archive_X = []\n        archive_F = []\n\n        # linear rank-to-weight (different to SPIRIT's exponential)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        ranks = np.arange(1, mu + 1)\n        raw_w = (mu + 1 - ranks).astype(float)\n        weights = raw_w / np.sum(raw_w)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = fm; x_best = xm.copy()\n            last_eval_best = fm\n        else:\n            last_eval_best = np.inf\n\n        # success history for sigma adaptation & alpha\n        recent_successes = []\n        success_window = max(8, lam)\n\n        # main loop: generate generations until budget exhausted\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # gaussian base draws\n            base_z = np.random.randn(current_lambda, n)\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # projection onto low-rank subspace and complement\n                proj_low = U @ (U.T @ z)\n                proj_high = z - proj_low\n\n                # diagonal-like scaling from Adam second moment (bias-corrected later)\n                medS = float(np.median(S))\n                D = 1.0 / (np.sqrt(S + self.S_eps))\n\n                # scale low subspace by median and high by diagonal D\n                y_sub = np.sqrt(medS + self.S_eps) * proj_low\n                y_diag = D * proj_high\n\n                # combine via alpha\n                y = (1.0 - self.alpha) * y_diag + self.alpha * y_sub\n\n                # occasional Cauchy (Lévy-like) heavy-tailed jump\n                if np.random.rand() < self.p_levy:\n                    # sample symmetric Cauchy scaled\n                    r = np.random.standard_cauchy() * self.levy_scale\n                    # direction: normalized z\n                    nz = np.linalg.norm(z) + 1e-20\n                    y = (r * (z / nz)) * float(np.median(D)) * self.temperature\n\n                # mirrored sampling pairing\n                if self.mirror and (i % 2 == 1):\n                    y = -y\n\n                # form candidate with momentum\n                x = m + sigma * y + self.momentum_scale * v\n\n                # DE current-to-pbest/1 recombination with archive if available\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                    # build pool of candidates: recently archived bests + archive randoms\n                    pool_size = max(2, int(self.p_best * len(archive_X)))\n                    sorted_idx = np.argsort(archive_F)\n                    p_candidates = [archive_X[idx] for idx in sorted_idx[:pool_size]]\n                    pbest = p_candidates[np.random.randint(len(p_candidates))].copy()\n                    # pick two distinct random archive members\n                    idxs = np.random.choice(len(archive_X), size=2, replace=False)\n                    r1 = archive_X[idxs[0]].copy()\n                    r2 = archive_X[idxs[1]].copy()\n                    # current-to-pbest mutation\n                    de_mut = self.F_de * (pbest - x) + self.F_de * (r1 - r2)\n                    # binomial crossover\n                    mask = (np.random.rand(n) < self.cr_de)\n                    if not np.any(mask):\n                        mask[np.random.randint(n)] = True\n                    x[mask] = x[mask] + de_mut[mask]\n\n                # clip bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates (careful not to exceed budget)\n            prev_best = f_best\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    # remove oldest\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # selection: best mu\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            # recompute mean via linear weights\n            m_old = m.copy()\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # success measure: any of evaluated candidates improved over prev_best\n            success_flag = 1 if np.any(arfit < prev_best) else 0\n            recent_successes.append(success_flag)\n            if len(recent_successes) > success_window:\n                recent_successes.pop(0)\n            success_rate = float(np.mean(recent_successes)) if recent_successes else 0.0\n\n            # additive alpha adaptation toward target (clamped)\n            self.alpha += self.alpha_lr * (success_rate - self.alpha_target)\n            self.alpha = float(np.clip(self.alpha, 0.02, 0.95))\n\n            # update inertia (momentum)\n            v = self.momentum * v + (1.0 - self.momentum) * (sigma * y_w)\n\n            # Adam-style second moment update with bias correction\n            y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0) if y_sel.size > 0 else np.zeros(n)\n            S_step += 1\n            S = self.beta2 * S + (1.0 - self.beta2) * (y2 + self.S_eps)\n            # bias correction\n            S_hat = S / (1.0 - (self.beta2 ** max(1, S_step)))\n            D = 1.0 / (np.sqrt(S_hat) + self.S_eps)\n\n            # step-size (sigma) adaptation — logistic-like gentle update\n            # target success 0.25, map to multiplier\n            adapt_exp = 0.3 * (success_rate - 0.25)\n            sigma *= np.exp(adapt_exp) * (1.0 + 0.01 * np.log1p(n))\n            domain_scale = np.mean(ub - lb)\n            sigma = float(np.clip(sigma, 1e-12, 80.0 * domain_scale))\n\n            # tempering decay per generation\n            self.temperature *= self.T_decay\n            self.temperature = max(self.temperature, 0.25)  # don't vanish completely\n\n            # Oja subspace update using normalized y_w as signal\n            signal = y_w.copy()\n            sig_norm = np.linalg.norm(signal)\n            if sig_norm > 1e-12:\n                signal = signal / (sig_norm + 1e-12)\n                for j in range(U.shape[1]):\n                    u = U[:, j]\n                    proj = float(np.dot(u, signal))\n                    u = u + self.oja_eta * proj * (signal - proj * u)\n                    u = u / (np.linalg.norm(u) + 1e-12)\n                    U[:, j] = u\n                # occasional re-orthonormalize\n                if np.random.rand() < self.oja_reorth_freq:\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # stagnation kicks: if no improvement for long time, try several Cauchy probes\n            stagnation_length = max(40, 4 * n)\n            if len(archive_F) >= stagnation_length and (evals % stagnation_length == 0):\n                recent = archive_F[-stagnation_length:]\n                if len(recent) == stagnation_length and np.min(recent) >= (f_best - 1e-12):\n                    old_sigma = sigma\n                    sigma *= 3.0\n                    tries = min(6, budget - evals)\n                    for _ in range(tries):\n                        z = np.random.randn(n)\n                        nz = np.linalg.norm(z) + 1e-12\n                        r = np.random.standard_cauchy() * (1.2 * self.levy_scale)\n                        y = r * (z / nz) * float(np.median(D)) * self.temperature\n                        x_try = np.clip(m + sigma * y, lb, ub)\n                        f_try = func(x_try)\n                        evals += 1\n                        archive_X.append(x_try.copy()); archive_F.append(f_try)\n                        if len(archive_X) > self.archive_limit:\n                            archive_X.pop(0); archive_F.pop(0)\n                        if f_try < f_best:\n                            f_best = f_try\n                            x_best = x_try.copy()\n                            m = x_try.copy()\n                            v = np.zeros(n)\n                            break\n                    sigma = old_sigma\n\n            # keep mean in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm STTE_Levy scored 0.523 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "23e9cf3f-d0e0-406d-bd6d-72692cfe6f5b", "operator": null, "metadata": {"aucs": [0.15885926225586344, 0.2087306651965607, 0.8363583929166347, 0.9247001743079662, 0.3044297257781382, 0.888104044697697, 0.2979130912261614, 0.5681452631833457, 0.8777931736971896, 0.16155016100609954]}, "task_prompt": ""}
{"id": "4c1e5986-e4b3-43d1-9119-c9a9e3fe7aa6", "fitness": "-inf", "name": "ORBITAL", "description": "ORBITAL builds a hybrid search space by combining a learned low-rank orthonormal basis B (k ≈ n^0.55) with a diagonal complement (RMS accumulator S → D and a per-coordinate trust vector) and uses an adaptive mixing weight alpha to trade off subspace vs coordinate search. Candidates are generated as m + sigma*y + momentum*v where y is formed from projected Gaussian noise, occasional DE-style recombination from an archive, mirrored ± pairs for finite-difference directional estimates, and intermittent heavy‑tailed Lévy (Student‑t) directional bursts (p_levy=0.2, df=1.5) to escape local minima. Adaptation is bandit‑inspired: selected improvements increase per-coordinate trust (trust_lr ≈ 0.12) and produce an Oja-like basis update (basis_eta) that replaces weak basis columns, while an RMS-like accumulator and a smoothed 1/5th-style success rule adapt sigma; momentum and clipping maintain stability. An archive supports DE recombination and nearest-mean fitness lookup, and a stagnation window triggers Lévy-bursts or sigma inflation; overall parameter choices (conservative init_sigma ≈1.2, moderate lambda, small DE probability, large archive_limit) bias the method toward robust global exploration with focused local refinement.", "code": "import numpy as np\n\nclass ORBITAL:\n    \"\"\"\n    ORBITAL optimizer -- hybrid low-rank + coordinate preconditioning,\n    uses mirrored finite-difference signals to build a bandit-like trust\n    vector, an incremental orthonormal subspace (different update rule),\n    DE-style archive recombination, and intermittent Lévy-flight bursts.\n\n    One-line: Orthogonal Rotations with Bandit-Informed Trust-regions And Lévy-flights.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        self.seed = seed\n\n        # population/adaptive sizes\n        # a bit larger than classical 4+floor(3*log(n)) style, scaled with dim^0.7\n        self.lambda_ = max(6, int(6 + np.floor(3.0 * np.log(max(2, self.dim)) * (self.dim ** 0.0))))\n        # ensure at least proportional to sqrt(dim)\n        self.lambda_ = max(self.lambda_, int(max(6, np.ceil(2.5 * (self.dim ** 0.5)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank subspace dimension (novel exponent)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(self.dim ** 0.55)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # initial global step (conservative but not tiny)\n        self.init_sigma = 0.12 * 10.0  # corresponds to 1.2 across [-5,5]\n\n        # heavy-tailed jump parameters (Lévy / Student variants)\n        self.p_levy = 0.20\n        self.levy_df = 1.5  # heavier tails (non-integer allowed for numpy's standard_t)\n        self.levy_scale = 1.4\n\n        # small DE-recombination archive\n        self.p_de = 0.18\n        self.F_de = 0.6\n        self.cr_de = 0.5\n        self.archive_limit = 2500\n\n        # Oja-like but bandit-weighted incremental learning for basis\n        self.basis_eta = 0.06\n\n        # per-dimension trust / preconditioner learning (bandit-like)\n        self.trust_lr = 0.12\n        self.trust_eps = 1e-8\n        self.trust = np.ones(self.dim)\n\n        # RMS-like accumulator for quasi-diagonal scaling (for complement)\n        self.s_beta = 0.88\n        self.s_init = 1e-3\n\n        # mixing between subspace and complement\n        self.alpha = 0.45\n        self.alpha_lr = 0.08\n        self.alpha_target = 0.28\n\n        # inertia / momentum\n        self.momentum = 0.35\n        self.mom_decay = 0.88\n\n        # stagnation / restart parameters\n        self.stag_window = max(30, 4 * self.dim)\n        self.stag_threshold = 1e-12\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds handling\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        domain_scale = np.mean(ub - lb)\n\n        # initialize mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = float(self.init_sigma)\n\n        # initialize orthonormal basis B (n x k)\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            B = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            B = rand.copy()\n            for j in range(B.shape[1]):\n                B[:, j] /= (np.linalg.norm(B[:, j]) + 1e-12)\n\n        # diagonal scaling (for complement directions) via RMS-like accumulator\n        S = np.ones(n) * float(self.s_init)\n        D = 1.0 / (np.sqrt(S) + self.trust_eps)\n\n        # momentum vector\n        v = np.zeros(n)\n\n        # archive for DE-style recombination and reuse\n        archive_X = []\n        archive_F = []\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_best = fm; x_best = xm.copy()\n            m_f = fm\n        else:\n            m_f = np.inf\n\n        # maintain recent no-improvement counter for stagnation detection\n        recent_best_hist = [f_best]\n\n        # window for success-rate adaptation\n        success_history = []\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            # ensure mirrored pairing convenience: produce even count when mirror used\n            if lam % 2 == 1 and lam > 1:\n                lam -= 1\n\n            arx = np.zeros((lam, n))\n            arz = np.zeros((lam, n))\n            arfit = np.full(lam, np.inf)\n            pair_idx = {}  # mapping from pair_idx to its counterpart for diff estimates\n\n            base_noise = np.random.randn(lam, n)\n\n            for i in range(lam):\n                z = base_noise[i].copy()\n\n                # split into subspace and complement\n                proj_sub = B @ (B.T @ z)        # low-rank projection\n                proj_compl = z - proj_sub\n\n                # scaled complement via D (coordinates), subspace scaled by median trust\n                med_trust = float(np.median(self.trust * D))\n                y_sub = med_trust * proj_sub\n                y_compl = (self.trust * D) * proj_compl\n\n                # adaptive mixing alpha (use current self.alpha)\n                y = self.alpha * y_sub + (1.0 - self.alpha) * y_compl\n\n                # DE-style archive recombination occasionally (rand/1 from archive)\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                    i1, i2, i3 = np.random.choice(len(archive_X), size=3, replace=False)\n                    xa = archive_X[i1]; xb = archive_X[i2]; xc = archive_X[i3]\n                    de_mut = self.F_de * (xa - xb)\n                    mask = (np.random.rand(n) < self.cr_de)\n                    mixed = np.copy(m)\n                    mixed[mask] = mixed[mask] + de_mut[mask]\n                    # tame mixed via small interpolation with xc\n                    y = y + 0.06 * (mixed - m)\n\n                # Lévy / Student heavy-tailed shot (directional)\n                if np.random.rand() < self.p_levy:\n                    direction = np.random.randn(n)\n                    direction /= (np.linalg.norm(direction) + 1e-12)\n                    r = np.random.standard_t(self.levy_df) * self.levy_scale\n                    y = r * direction * med_trust\n\n                # mirrored pairing: store sign-flipped direction for every odd index\n                if i % 2 == 1:\n                    y = -y\n                    pair_idx[i] = i - 1\n                    pair_idx[i - 1] = i\n\n                # candidate with momentum drift\n                x = m + sigma * y + self.momentum * v\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates sequentially (respect budget)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n                # update recent history\n                recent_best_hist.append(f_best)\n                if len(recent_best_hist) > self.stag_window:\n                    recent_best_hist.pop(0)\n\n            # compute finite-difference directional derivative estimates for mirrored pairs\n            # when both members of a pair evaluated, approximate directional derivative:\n            # g_est ≈ (f_plus - f_minus) / (2*sigma) * direction\n            derivative_est = np.zeros(n)\n            derivative_count = 0\n            # build index->fit mapping\n            for i in range(lam):\n                j = pair_idx.get(i, None)\n                if j is None:\n                    continue\n                # ensure we only compute once per pair\n                if i < j:\n                    continue\n                # both indices i and j should be evaluated\n                if np.isfinite(arfit[i]) and np.isfinite(arfit[j]):\n                    # i and j are opposite-signed directions: let i be +y, j be -y (or vice versa)\n                    fi = arfit[i]; fj = arfit[j]\n                    yi = arz[i]\n                    # estimate directional derivative along yi (unit scaled)\n                    denom = 2.0 * sigma\n                    if abs(denom) < 1e-12:\n                        continue\n                    gproj = (fi - fj) / denom  # note sign; larger negative gproj -> descent along yi\n                    derivative_est += gproj * yi\n                    derivative_count += 1\n\n            if derivative_count > 0:\n                derivative_est /= derivative_count\n            else:\n                derivative_est[:] = 0.0\n\n            # selection: choose top mu by fitness\n            idx = np.argsort(arfit)\n            sel = idx[:self.mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n            f_sel = arfit[sel]\n\n            # recombine mean with simple weighted average (weights by soft ranks)\n            ranks = np.arange(1, len(sel) + 1)\n            # soft exponential rank weighting\n            beta = 0.7\n            raw_w = np.exp(-beta * (ranks - 1.0))\n            w = raw_w / np.sum(raw_w)\n            m_old = m.copy()\n            m_old_f = m_f\n            m = np.sum(w[:, None] * x_sel, axis=0)\n            # compute composite step in y-space (for basis updates & momentum)\n            y_w = np.sum(w[:, None] * y_sel, axis=0)\n\n            # compute whether selection improved mean's fitness (compare best among selected to previous mean f)\n            sel_best_f = np.min(f_sel) if f_sel.size > 0 else np.inf\n            success_flag = 1 if sel_best_f < m_old_f - 1e-15 else 0\n            success_history.append(success_flag)\n            if len(success_history) > max(8, self.lambda_):\n                success_history.pop(0)\n            success_rate = float(np.mean(success_history)) if success_history else 0.0\n\n            # update alpha (mixing) with logistic-like adjustment toward target using success signal\n            # increase subspace usage if subspace-selected steps produced lower f\n            # compute average subspace energy in selected y's\n            if y_sel.size > 0:\n                sub_energies = np.sum((B.T @ y_sel.T) ** 2, axis=0)  # energy per selected in subspace\n                comp_energies = np.sum((y_sel.T - B @ (B.T @ y_sel.T)) ** 2, axis=0)\n                mean_sub_ratio = np.mean(sub_energies / (sub_energies + comp_energies + 1e-12))\n            else:\n                mean_sub_ratio = 0.5\n            # novel update: alpha moves toward mean_sub_ratio scaled by success feedback\n            self.alpha += self.alpha_lr * (mean_sub_ratio - self.alpha) * (0.5 + success_rate)\n            self.alpha = float(np.clip(self.alpha, 0.02, 0.95))\n\n            # momentum update\n            v = self.mom_decay * v + (1.0 - self.mom_decay) * (sigma * y_w)\n\n            # update diagonal RMS S with squared composite y_w (soft)\n            S = self.s_beta * S + (1.0 - self.s_beta) * (y_w ** 2 + self.trust_eps)\n            D = 1.0 / (np.sqrt(S) + self.trust_eps)\n\n            # update trust vector using bandit-like positive-feedback from selected improvements\n            # if a selected point improved significantly, increase trust in coordinates where its y had magnitude\n            for j_idx, j in enumerate(sel):\n                fj = f_sel[j_idx]\n                yj = y_sel[j_idx]\n                # improvement factor relative to previous mean\n                impro = max(0.0, (m_old_f - fj))\n                # update trust: exponential moving toward |yj| weighted by improvement\n                if impro > 0:\n                    # normalized direction magnitude\n                    mag = np.abs(yj) / (np.max(np.abs(yj)) + 1e-12)\n                    self.trust += self.trust_lr * impro * (mag - self.trust)\n            # keep trust positive and bounded\n            self.trust = np.clip(self.trust, 1e-6, 1e3)\n\n            # update low-rank basis B with a bandit-weighted incremental projection rule (novel)\n            # accumulate an influence vector from selected y's weighted by improvement\n            influ = np.zeros(n)\n            total_w = 0.0\n            for j_idx, j in enumerate(sel):\n                fj = f_sel[j_idx]\n                yj = y_sel[j_idx]\n                impro = max(0.0, (m_old_f - fj))\n                if impro > 0:\n                    # use a smoothed importance weight\n                    weight = impro / (abs(m_old_f) + 1e-12)\n                    influ += weight * (yj / (np.linalg.norm(yj) + 1e-12))\n                    total_w += weight\n            if total_w > 0:\n                influ /= total_w\n                # project out current basis and take component orthogonal to B\n                orth = influ - B @ (B.T @ influ)\n                onorm = np.linalg.norm(orth)\n                if onorm > 1e-12:\n                    orth_unit = orth / onorm\n                    # rotate basis toward this direction: replace smallest explained direction\n                    # compute contributions of columns (cov-like) and replace the one with least variance\n                    cols_var = np.sum((B * (B @ influ)[None, :].T) ** 2, axis=0)\n                    idx_replace = np.argmin(cols_var)\n                    # gently blend new direction into column\n                    new_col = (1.0 - self.basis_eta) * B[:, idx_replace] + self.basis_eta * orth_unit\n                    new_col /= (np.linalg.norm(new_col) + 1e-12)\n                    B[:, idx_replace] = new_col\n                    # occasional orthonormalization to maintain numeric stability\n                    if np.random.rand() < 0.18:\n                        try:\n                            Q, _ = np.linalg.qr(B)\n                            B = Q[:, :self.k]\n                        except np.linalg.LinAlgError:\n                            pass\n\n            # adapt sigma multiplicatively using smoothed 1/5th principle variant\n            # target success ~0.2 ; compute smoothed success over window\n            sr = np.mean(success_history[-max(8, self.lambda_):]) if success_history else 0.0\n            adapt_exp = 0.40 * (sr - 0.2) / max(0.07, np.sqrt(1.0 + n / 8.0))\n            sigma *= np.exp(adapt_exp)\n            sigma = float(np.clip(sigma, 1e-12, 80.0 * domain_scale))\n\n            # update mean fitness m_f if we have a new evaluation for m (try to estimate m_f from archive)\n            # Find closest archive point to m and use its f, otherwise keep old\n            if len(archive_X) > 0:\n                # compute distances\n                arr = np.vstack(archive_X)\n                dists = np.sum((arr - m[None, :]) ** 2, axis=1)\n                idx_closest = int(np.argmin(dists))\n                m_f = archive_F[idx_closest]\n            else:\n                m_f = f_best\n\n            # stagnation detection: if no improvement in recent window, perform Lévy-burst restart\n            if len(recent_best_hist) >= self.stag_window:\n                window = recent_best_hist[-self.stag_window:]\n                if min(window) >= (f_best - self.stag_threshold):\n                    # do a limited burst: try up to B trials with inflated sigma and accept any improvement\n                    burst_sigma = sigma * 3.5\n                    burst_tries = min(6 + int(np.sqrt(n)), budget - evals)\n                    accepted = False\n                    for _ in range(burst_tries):\n                        direction = np.random.randn(n)\n                        direction /= (np.linalg.norm(direction) + 1e-12)\n                        r = np.random.standard_t(self.levy_df) * (self.levy_scale * 1.1)\n                        y = r * direction * np.median(self.trust * D)\n                        x_try = np.clip(m + burst_sigma * y, lb, ub)\n                        if evals >= budget:\n                            break\n                        ftry = func(x_try)\n                        evals += 1\n                        archive_X.append(x_try.copy()); archive_F.append(ftry)\n                        if len(archive_X) > self.archive_limit:\n                            archive_X.pop(0); archive_F.pop(0)\n                        if ftry < f_best:\n                            f_best = ftry\n                            x_best = x_try.copy()\n                            # adopt new mean and reset momentum/trust gently\n                            m = x_try.copy()\n                            v = np.zeros(n)\n                            # increase trust along that jump direction\n                            mag = np.abs(y) / (np.max(np.abs(y)) + 1e-12)\n                            self.trust += 0.5 * (mag - self.trust)\n                            accepted = True\n                            break\n                    # if not accepted, nudge sigma upward slightly to encourage escapes\n                    if not accepted:\n                        sigma *= 1.12\n                        sigma = float(np.clip(sigma, 1e-12, 80.0 * domain_scale))\n                    # clear recent history after burst to avoid immediate repeats\n                    recent_best_hist = [f_best]\n\n            # bound mean\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 334, in __call__, the following error occurred:\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 4)\nOn line: cols_var = np.sum((B * (B @ influ)[None, :].T) ** 2, axis=0)", "error": "In the code, line 334, in __call__, the following error occurred:\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 4)\nOn line: cols_var = np.sum((B * (B @ influ)[None, :].T) ** 2, axis=0)", "parent_ids": "23e9cf3f-d0e0-406d-bd6d-72692cfe6f5b", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "1c45c336-9acd-4f52-bbbf-b8cb6da37a7c", "fitness": 0.1580905775496253, "name": "LEVIATHAN", "description": "LEVIATHAN combines a low-rank adaptive subspace (U, k ≈ dim^0.5) with a diagonal RMS-based preconditioner D = 1/√S (S updated with beta=0.85) and a deliberately large initial step (init_sigma = 0.30·range) so searches are anisotropic and initially exploratory. Sampling mixes subspace and complement directions via an adaptive additive alpha (starts 0.6, α_lr=0.06) and injects heavy-tailed Cauchy hops (p_cauchy=0.28, cauchy_scale=1.6) plus periodic mirroring (every 3rd sample) to encourage large jumps and symmetry. Search is steered by fast weighted recombination (softmax-like weights, learn_mean=0.6), momentum/inertia (momentum_scale=0.7, decay=0.95), and occasional DE-style archive recombination (p_de=0.12, F_de=0.9, cr_de=0.5) to blend global archive information. Online learning rotates U with a lightweight Oja update (oja_eta=0.08), sigma is smoothed multiplicatively by observed success rate, and a stagnation detector triggers targeted Cauchy bursts and mean re-centering from the archive to escape plateaus.", "code": "import numpy as np\n\nclass LEVIATHAN:\n    \"\"\"\n    LEVIATHAN: LEVelled, Inertia-enhanced, Adaptive subspace search with Cauchy Tail Hops\n\n    Main ideas and changed parameter/equation choices vs. the provided SPIRIT:\n    - Subspace dimension k ~ dim^0.5 (different exponent).\n    - Initial sigma larger (more exploratory): init_sigma = 0.30 * range.\n    - Heavy tails via Cauchy (np.random.standard_cauchy) with higher p_cauchy.\n    - RMS-like second moment uses a slightly smaller decay (beta=0.85) and S_init smaller.\n    - Alpha (mix between subspace and diagonal) uses additive update toward target (not multiplicative).\n    - Oja update uses the preconditioned signal U.T @ (D * y) (different projection equation).\n    - Sigma adaptation is a smoothed 1/5th style but with different exponent and normalization.\n    - Mirrored pairing retained but mirrored sign toggles every third sample (different mirroring scheme).\n    - DE-like archive recombination kept, but lower p_de and softer mixture.\n    - Stagnation uses targeted Cauchy restarts and optionally re-centers mean to archive bests.\n    - Many parameter values intentionally changed to produce different dynamics.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, init_sigma=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        self.seed = seed\n\n        # population size: slightly different heuristic\n        self.lambda_ = max(6, int(6 + np.floor(3.0 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank dimension (different exponent)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(self.dim ** 0.5)))  # changed exponent\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # initial sigma (more exploratory)\n        if init_sigma is None:\n            self.init_sigma = 0.30 * 10.0  # 0.30 of domain range (domain is [-5,5] => 10)\n        else:\n            self.init_sigma = float(init_sigma)\n\n        # Heavy-tailed jump parameters (Cauchy)\n        self.p_cauchy = 0.28\n        self.cauchy_scale = 1.6\n\n        # DE-like archive recombination params (different)\n        self.p_de = 0.12\n        self.F_de = 0.9\n        self.cr_de = 0.5\n        self.archive_limit = 5000\n\n        # mirroring behavior: flip every 3rd sample (different from pairing)\n        self.mirror_period = 3\n\n        # Oja learning rate (different)\n        self.oja_eta = 0.08\n\n        # momentum/inertia\n        self.momentum_decay = 0.95\n        self.momentum_scale = 0.7\n\n        # alpha mixing: additive update toward target\n        self.alpha = 0.6\n        self.alpha_lr = 0.06\n        self.alpha_target = 0.5\n\n        # RMS-like accumulator\n        self.rms_beta = 0.85\n        self.S_eps = 1e-8\n        self.S_init = 1e-4\n\n        # sigma adaptation parameters (different scaling)\n        self.sigma_adapt_base = 0.8\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (assume -5..5 if not provided)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        domain_scale = np.mean(ub - lb)\n\n        # initialize mean uniformly in bounds\n        m = np.random.uniform(lb, ub)\n        sigma = max(1e-12, float(self.init_sigma))\n\n        # RMS second moment accumulator and diagonal preconditioner D = 1/sqrt(S)\n        S = np.ones(n) * float(self.S_init)\n        D = 1.0 / np.sqrt(S + self.S_eps)\n\n        # low-rank subspace U orthonormal init\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            U = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = rand.copy()\n            for j in range(U.shape[1]):\n                U[:, j] /= (np.linalg.norm(U[:, j]) + 1e-12)\n\n        # momentum/inertia vector\n        v = np.zeros(n)\n\n        # archive for DE-like recombination\n        archive_X = []\n        archive_F = []\n\n        # simple rank-to-weight: softmax-like weights for selected mu (different mapping)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        ranks = np.arange(1, mu + 1)\n        tau = 0.7\n        raw = np.exp(- (ranks - 1) / tau)\n        weights = raw / np.sum(raw)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # evaluate initial mean once\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_best = fm\n            x_best = xm.copy()\n            last_mean_f = fm\n        else:\n            last_mean_f = np.inf\n\n        # recent success window for alpha and sigma adaptation\n        recent_successes = []\n        success_window = max(8, lam)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # produce base gaussian draws\n            base_z = np.random.randn(current_lambda, n)\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # preconditioned projection: place diag-scaling inside subspace projection\n                Dz = D * z\n                # subspace-projected preconditioned component\n                proj_low = U @ (U.T @ Dz)\n                # complementary component using original geometry\n                proj_high = z - U @ (U.T @ z)\n                y_sub = proj_low\n                y_diag = D * proj_high  # elementwise scaling on complement\n\n                # combine using alpha (alpha is fraction on subspace)\n                y = self.alpha * y_sub + (1.0 - self.alpha) * y_diag\n\n                # occasional Cauchy heavy-tailed jump (direction from z)\n                if np.random.rand() < self.p_cauchy:\n                    nz = np.linalg.norm(z) + 1e-20\n                    r = np.random.standard_cauchy() * self.cauchy_scale\n                    # scale by median(D) to mix subspace scaling\n                    medD = float(np.median(D))\n                    y = r * (z / nz) * medD\n\n                # alternate mirroring: flip sign every mirror_period-th sample\n                if (i % self.mirror_period) == (self.mirror_period - 1):\n                    y = -y\n\n                # candidate with inertia (momentum contributes as additive drift)\n                x = m + sigma * y + self.momentum_scale * v\n\n                # DE-style archive recombination with soft blending\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                    # pick three distinct archive members\n                    i1, i2, i3 = np.random.choice(len(archive_X), size=3, replace=False)\n                    xa = archive_X[i1]; xb = archive_X[i2]; xc = archive_X[i3]\n                    de_mut = self.F_de * (xa - xb)\n                    # crossover mask\n                    mask = (np.random.rand(n) < self.cr_de)\n                    # softly add mutation to x's masked coords\n                    x[mask] = x[mask] + de_mut[mask]\n                    # blend slightly toward the third vector\n                    x = x + 0.03 * (xc - x)\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # sequential evaluation of candidates (stop at budget)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = f\n                archive_X.append(x.copy())\n                archive_F.append(f)\n                # enforce archive limit FIFO\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = f\n                    x_best = x.copy()\n\n            # selection: best mu\n            idx = np.argsort(arfit)\n            sel = idx[:mu]\n            x_sel = arx[sel]\n            y_sel = arz[sel]\n\n            # weighted recombination (soft update toward new mean)\n            new_mean = np.sum(weights[:, None] * x_sel, axis=0)\n            # update mean with aggressive learning rate to allow jumps\n            learn_mean = 0.6\n            m_old = m.copy()\n            m = (1.0 - learn_mean) * m + learn_mean * new_mean\n\n            # weighted mean in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # success measure: any selected better than last recorded mean fitness?\n            sel_best_f = float(np.min(arfit[sel])) if len(sel) > 0 else np.inf\n            # success comparison to previous best known (strict improvement)\n            success_flag = 1 if sel_best_f < f_best else 0\n            recent_successes.append(1 if sel_best_f < f_best else 0)\n            if len(recent_successes) > success_window:\n                recent_successes.pop(0)\n            success_rate = float(np.mean(recent_successes)) if recent_successes else 0.0\n\n            # adapt alpha additively toward target (different update)\n            self.alpha += self.alpha_lr * (success_rate - self.alpha_target)\n            self.alpha = float(np.clip(self.alpha, 0.02, 0.98))\n\n            # update momentum (decayed plus y-driven impulse)\n            v = self.momentum_decay * v + (1.0 - self.momentum_decay) * (sigma * y_w)\n\n            # update RMS second moment S with weighted y^2 (preconditioner)\n            if y_sel.size > 0:\n                y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            else:\n                y2 = np.zeros(n)\n            S = self.rms_beta * S + (1.0 - self.rms_beta) * (y2 + self.S_eps)\n            D = 1.0 / np.sqrt(S + self.S_eps)\n\n            # adapt sigma multiplicatively (smoothed 1/5th-like but different scale)\n            # target success 0.2, use base factor and normalization by dimension\n            denom = np.sqrt(1.0 + n / 4.0)\n            adapt_factor = np.exp(self.sigma_adapt_base * (success_rate - 0.2) / max(0.05, denom))\n            sigma *= adapt_factor\n            sigma = float(np.clip(sigma, 1e-12, 100.0 * domain_scale))\n\n            # Online Oja update for U using D-weighted signal projection (different projection)\n            # Construct a normalized signal inside the subspace coordinates\n            signal = U.T @ (D * y_w)  # k-vector\n            sig_norm = np.linalg.norm(signal)\n            if sig_norm > 1e-12:\n                signal = signal / (sig_norm + 1e-12)\n                # update each column vector using Oja in subspace-coordinates then map back\n                for j in range(U.shape[1]):\n                    u = U[:, j]\n                    # projection of signal onto u in original space through U\n                    proj = float(np.dot(u, U @ signal))\n                    # Oja-like step in ambient space via mapped signal\n                    mapped = U @ signal\n                    u = u + self.oja_eta * proj * (mapped - proj * u)\n                    u = u / (np.linalg.norm(u) + 1e-12)\n                    U[:, j] = u\n                # periodic orthonormalize with low probability\n                if np.random.rand() < 0.10:\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # stagnation detection and targeted Cauchy bursts: if no improvement in a while\n            stagnation_length = max(25, 3 * n)\n            if len(archive_F) >= stagnation_length and (evals % stagnation_length == 0):\n                recent = archive_F[-stagnation_length:]\n                if len(recent) == stagnation_length and np.min(recent) >= (f_best - 1e-12):\n                    # attempt several Cauchy-directed tries with larger scale\n                    old_sigma = sigma\n                    sigma *= 3.5\n                    tries = min(6 + n // 5, budget - evals)\n                    improved = False\n                    for _ in range(tries):\n                        z = np.random.randn(n)\n                        nz = np.linalg.norm(z) + 1e-12\n                        r = np.random.standard_cauchy() * (1.8 * self.cauchy_scale)\n                        y = r * (z / nz) * float(np.median(D))\n                        x_try = np.clip(m + sigma * y, lb, ub)\n                        f_try = func(x_try)\n                        evals += 1\n                        archive_X.append(x_try.copy()); archive_F.append(f_try)\n                        if f_try < f_best:\n                            f_best = f_try\n                            x_best = x_try.copy()\n                            # adopt new mean and reset momentum to focus local search\n                            m = x_try.copy()\n                            v = np.zeros(n)\n                            improved = True\n                            break\n                        if evals >= budget:\n                            break\n                    if not improved:\n                        # if none helped, nudge mean toward archive best with small blending\n                        if len(archive_X) > 0:\n                            best_idx = int(np.argmin(archive_F))\n                            m = 0.9 * m + 0.1 * archive_X[best_idx]\n                    sigma = old_sigma\n\n            # keep mean in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm LEVIATHAN scored 0.158 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "23e9cf3f-d0e0-406d-bd6d-72692cfe6f5b", "operator": null, "metadata": {"aucs": [0.050774998430035745, 0.1366276808953576, 0.15628273616094535, 0.10136758051426853, 0.21225555280737451, 0.17332543527187416, 0.25865944967918186, 0.15545641311229508, 0.24877184106536088, 0.08738408755955929]}, "task_prompt": ""}
{"id": "70795728-0ee0-4640-bbf0-1b785281886d", "fitness": "-inf", "name": "HALO", "description": "HALO is a hybrid strategy that combines an online low-rank adaptive subspace (Oja updates, k ≈ n^0.55) with a RMS-style diagonal scaler (S with beta=0.92) so steps are a convex mix (alpha) of subspace and diagonal-scaled components; alpha is adapted to favor the subspace when recent success is high. It uses small-to-moderate population sizes (λ = max(4, 4+⌊3 log n⌋), μ ≈ λ/2) with CMA-like log weights and mirrored sampling to reduce variance, plus momentum (decay 0.88) to exploit inertia. Exploration diversity is injected via occasional heavy-tailed Student-t moves (p_heavy=0.14), DE-style archive mutations/crossover from a FIFO archive (limit 3000), and stagnation-triggered heavy bursts that temporarily inflate σ. Step-size σ is robustly controlled by dual mechanisms — a path-length (CMA-like ps) update and a success-rate soft 1/5th rule — with sensible safeguards (σ_init=0.2·domain_scale, clamping, max factor), while other practical touches include bound clipping, re-orthonormalization of U, and light randomization in Oja learning.", "code": "import numpy as np\n\nclass HALO:\n    \"\"\"\n    HALO: HYbrid Adaptive Low-rank Optimizer\n    One-line: Online low-rank (Oja) + RMS-diagonal scaling + momentum + mirrored sampling,\n    with DE-style archive mutations and dual sigma adaptation (path-length + success-rate).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population and selection\n        self.lambda_ = max(4, int(4 + np.floor(3 * np.log(self.dim))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank subspace dimension (balance sqrt and n^0.6)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(self.dim ** 0.55)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # Oja params\n        self.oja_eta = 0.06\n\n        # RMS-like diagonal accumulator\n        self.rms_beta = 0.92\n        self.S_eps = 1e-8\n        self.S_init = 1e-3\n\n        # momentum\n        self.momentum_decay = 0.88\n        self.momentum_scale = 0.5\n\n        # DE-style / heavy tails\n        self.p_de = 0.20\n        self.F_de = 0.75\n        self.cr_de = 0.6\n        self.p_heavy = 0.14\n        self.heavy_df = 3.0\n        self.heavy_scale = 1.2\n\n        # archive\n        self.archive_limit = 3000\n\n        # sigma safeguards\n        self.sigma_min = 1e-12\n        self.sigma_max_factor = 80.0  # times domain scale\n\n        # alpha mixing initial (subspace vs diag)\n        self.alpha = 0.5\n        self.alpha_lr = 0.08\n        self.alpha_target = 0.25\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # get bounds from func if available, else default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape == ():\n                lb = np.full(n, lb)\n            if ub.shape == ():\n                ub = np.full(n, ub)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n\n        domain_scale = float(np.mean(ub - lb))\n        # initialize mean and sigma\n        m = np.random.uniform(lb, ub)\n        sigma = 0.2 * domain_scale\n\n        # initialize low-rank U (orthonormal)\n        rand = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand)\n            U = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = rand.copy()\n            for j in range(U.shape[1]):\n                U[:, j] /= (np.linalg.norm(U[:, j]) + 1e-12)\n\n        # RMS accumulator and derived diagonal scale D\n        S = np.ones(n) * float(self.S_init)\n        D = 1.0 / np.sqrt(S + self.S_eps)\n\n        # momentum and other states\n        v = np.zeros(n)\n        ps = np.zeros(n)  # path-length like vector for sigma control\n        mu = min(self.mu, self.lambda_)\n\n        # recombination weights (log-based like CMA)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length constants (CMA-like)\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # bookkeeping\n        evals = 0\n        f_best = np.inf\n        x_best = None\n        last_improvement_eval = 0\n\n        # archives\n        archive_X = []\n        archive_F = []\n\n        # initial evaluate mean\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy())\n            archive_F.append(fm)\n            f_best = float(fm)\n            x_best = xm.copy()\n            last_improvement_eval = evals\n\n        # short-term success tracking for alpha / sigma\n        recent_successes = []\n        success_window = max(8, self.lambda_)\n\n        # helper: approximate invsqrtC @ y using diagonal and subspace scales\n        def approx_invsqrt(y, D_local, U_local):\n            # project onto subspace and complement\n            proj_low = U_local @ (U_local.T @ y)\n            proj_high = y - proj_low\n            medD = float(np.median(D_local) + 1e-20)\n            # scale inverses: high comp divided by D, low comp divided by medD\n            return proj_high / (D_local + 1e-20) + proj_low / (medD + 1e-20)\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(self.lambda_, remaining)\n\n            # prepare containers\n            arx = np.zeros((current_lambda, n))\n            arz = np.zeros((current_lambda, n))\n            arfit = np.full(current_lambda, np.inf)\n\n            # base gaussian draws\n            base_z = np.random.randn(current_lambda, n)\n\n            for i in range(current_lambda):\n                z = base_z[i].copy()\n\n                # decompose z into low-rank and complement\n                proj_low = U @ (U.T @ z)\n                proj_high = z - proj_low\n\n                # diag-scaled high component and median-scaled low component\n                y_diag = D * proj_high\n                medD = float(np.median(D))\n                y_sub = medD * proj_low\n\n                # combined step\n                y = (1.0 - self.alpha) * y_diag + self.alpha * y_sub\n\n                # mirrored sampling\n                if i % 2 == 1:\n                    y = -y\n\n                # occasional heavy-tailed move (Student-t)\n                if np.random.rand() < self.p_heavy:\n                    nz = np.linalg.norm(z) + 1e-20\n                    r = np.random.standard_t(self.heavy_df) * self.heavy_scale\n                    y = r * (z / nz) * medD\n\n                # candidate with inertia\n                x = m + sigma * y + self.momentum_scale * v\n\n                # DE-style archive mutation with crossover\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 3):\n                    i1, i2, i3 = np.random.choice(len(archive_X), size=3, replace=False)\n                    a = archive_X[i1]; b = archive_X[i2]; c = archive_X[i3]\n                    de_mut = self.F_de * (a - b)\n                    mask = (np.random.rand(n) < self.cr_de)\n                    # add mutation on masked positions\n                    x[mask] = x[mask] + de_mut[mask]\n                    # small mixing to diversify\n                    x = x + 0.03 * (c - x)\n\n                # ensure bounds\n                x = np.clip(x, lb, ub)\n\n                arx[i] = x\n                arz[i] = y\n\n            # evaluate candidates (respect budget)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                x = arx[i]\n                f = func(x)\n                evals += 1\n                arfit[i] = float(f)\n                archive_X.append(x.copy())\n                archive_F.append(float(f))\n                # FIFO archive bound\n                if len(archive_X) > self.archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_best:\n                    f_best = float(f)\n                    x_best = x.copy()\n                    last_improvement_eval = evals\n\n            # selection and recombination\n            idx = np.argsort(arfit)\n            sel_idx = idx[:mu]\n            x_sel = arx[sel_idx]\n            y_sel = arz[sel_idx]\n\n            m_old = m.copy()\n            # recombine to new mean\n            m = np.sum(weights[:, None] * x_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(weights[:, None] * y_sel, axis=0)\n\n            # update path vector ps using approx invsqrt\n            inv_y = approx_invsqrt(y_w, D, U)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * inv_y\n            norm_ps = np.linalg.norm(ps)\n\n            # sigma adaptation: combine CMA-path-length and success-rate (soft 1/5th)\n            sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n            # compute success flag: improvement among selected vs global best at selection time\n            sel_best_f = np.min(arfit[sel_idx]) if sel_idx.size > 0 else np.inf\n            success = 1 if sel_best_f < f_best else 0\n            recent_successes.append(success)\n            if len(recent_successes) > success_window:\n                recent_successes.pop(0)\n            success_rate = float(np.mean(recent_successes)) if recent_successes else 0.0\n\n            # soft 1/5th multiplicative adjustment\n            kappa = 0.45 / max(0.05, np.sqrt(1.0 + n / 8.0))\n            sigma *= np.exp(kappa * (success_rate - 0.2))\n\n            # clamp sigma\n            sigma = float(np.clip(sigma, self.sigma_min, self.sigma_max_factor * domain_scale))\n\n            # update RMS S and D\n            if y_sel.size > 0:\n                y2 = np.sum(weights[:, None] * (y_sel ** 2), axis=0)\n            else:\n                y2 = np.zeros(n)\n            S = self.rms_beta * S + (1.0 - self.rms_beta) * (y2 + self.S_eps)\n            D = 1.0 / np.sqrt(S + self.S_eps)\n\n            # update alpha: favor subspace when success high\n            self.alpha *= np.exp(self.alpha_lr * (success_rate - self.alpha_target))\n            self.alpha = float(np.clip(self.alpha, 0.03, 0.97))\n            alpha = self.alpha\n\n            # update momentum v\n            v = self.momentum_decay * v + (1.0 - self.momentum_decay) * (sigma * y_w)\n\n            # online Oja update for U from normalized y_w\n            signal = y_w.copy()\n            sig_norm = np.linalg.norm(signal)\n            if sig_norm > 1e-12:\n                signal = signal / (sig_norm + 1e-12)\n                for j in range(U.shape[1]):\n                    u = U[:, j]\n                    proj = float(np.dot(u, signal))\n                    u = u + self.oja_eta * proj * (signal - proj * u)\n                    u = u / (np.linalg.norm(u) + 1e-12)\n                    U[:, j] = u\n                # occasional re-orthonormalize\n                if np.random.rand() < 0.15:\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # stagnation detection: if no improvement for a while, perform heavy burst\n            stagnation_limit = max(40, 4 * n)\n            if (evals - last_improvement_eval) > stagnation_limit:\n                # perform controlled heavy-tailed exploration (without blowing budget)\n                old_sigma = sigma\n                sigma = min(self.sigma_max_factor * domain_scale, sigma * 3.5)\n                tries = min(6, budget - evals)\n                for _ in range(tries):\n                    z = np.random.randn(n)\n                    nz = np.linalg.norm(z) + 1e-20\n                    r = np.random.standard_t(self.heavy_df) * (1.8 * self.heavy_scale)\n                    y = r * (z / nz) * float(np.median(D))\n                    x_try = np.clip(m + sigma * y, lb, ub)\n                    f_try = func(x_try)\n                    evals += 1\n                    archive_X.append(x_try.copy()); archive_F.append(float(f_try))\n                    if f_try < f_best:\n                        f_best = float(f_try)\n                        x_best = x_try.copy()\n                        m = x_try.copy()  # adopt new mean\n                        v = np.zeros(n)\n                        last_improvement_eval = evals\n                        break\n                sigma = old_sigma\n                # reset some state to encourage new direction learning\n                recent_successes = []\n                ps = np.zeros_like(ps)\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n\n            # safety: if x_best is None (shouldn't happen) set from archive\n            if x_best is None and len(archive_X) > 0:\n                x_best = np.array(archive_X[np.argmin(archive_F)], dtype=float)\n                f_best = float(np.min(archive_F))\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 227, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,1) (3,10) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "error": "In the code, line 227, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,1) (3,10) \nOn line: m = np.sum(weights[:, None] * x_sel, axis=0)", "parent_ids": "23e9cf3f-d0e0-406d-bd6d-72692cfe6f5b", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5c196ced-d955-4175-8268-3a5153d4fbbc", "fitness": 0.1505542430319697, "name": "PCAUCB", "description": "The algorithm maintains a compact low‑rank search basis D (default p ≈ ceil(sqrt(dim))) that mixes random orthonormal directions and PCA of recent successful unit moves so searches focus on promising low‑dimensional subspaces. Each basis column is treated as an “arm” with per‑arm UCB statistics (counts, cumulative rewards, tunable ucb_beta) and adaptive step sizes (step_growth=1.20, step_shrink=0.82, min_step_frac tiny) so the sampler balances exploration/exploitation and self‑tunes step magnitudes. Successful probes are frequently (parabola_prob=0.9) refined by a cheap 3‑point parabolic fit along the direction, and the basis is periodically rebuilt or rotated (rebuild_every, rotate_every) with worst arms replaced using recent moves to keep directions relevant. Additional practical designs include budget‑safe clipped evaluations, occasional large/random global jumps to escape, decaying memory/trust weights for PCA bias, stagnation detection that triggers soft restarts or final polishing, and QR orthonormalization to maintain numerical stability.", "code": "import numpy as np\n\nclass PCAUCB:\n    \"\"\"\n    PCAUCB: Bandit-guided Low-Rank Directional Search with PCA-biased arms and parabolic refinement.\n\n    One-line: Maintain per-arm UCB stats over a low-rank basis that mixes PCA of recent successes and random directions;\n    probe arms with adaptive per-arm step sizes, refine successful probes by cheap 3-point parabolic fits, adapt and\n    rebuild the basis periodically, and perform budget-safe restarts and small polishing.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 subspace_size=None, memory_size=16, ucb_beta=1.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            self.rng = np.random.RandomState(seed)\n        else:\n            self.rng = np.random.RandomState()\n        # number of arms (basis size)\n        self.p = subspace_size if subspace_size is not None else max(2, int(np.ceil(np.sqrt(self.dim))))\n        self.memory_size = int(memory_size)\n        self.ucb_beta = float(ucb_beta)\n\n        # adaptation parameters\n        self.step_growth = 1.20\n        self.step_shrink = 0.82\n        self.min_step_frac = 1e-8\n        self.max_step_mult = 5.0\n\n        # refinement parameters\n        self.parabola_prob = 0.9  # refine most successful probes\n        self.parabola_delta_frac = 0.9\n\n        # rebuild/rotate frequency\n        self.rebuild_every = max(6, self.p)  # iterations\n        self.rotate_every = max(20, self.p * 4)\n\n    def __call__(self, func):\n        n = int(self.dim)\n        budget = int(self.budget)\n\n        # bounds from func\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        lb = lb.reshape(n,)\n        ub = ub.reshape(n,)\n\n        range_mean = float(np.mean(ub - lb))\n        # helper clip\n        def clip(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # safe eval wrapper\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best\n            x = clip(np.asarray(x, dtype=float))\n            if evals >= budget:\n                return None, None\n            try:\n                f = func(x)\n            except Exception:\n                f = np.inf\n            evals += 1\n            if f < f_best:\n                f_best = float(f)\n                x_best = x.copy()\n            return float(f), x\n\n        # initialization: start random\n        x_cur = self.rng.uniform(lb, ub)\n        f_cur, x_cur = safe_eval(x_cur)\n        if f_cur is None:\n            return float(f_best), np.asarray(x_best, dtype=float)\n        x_cur = np.asarray(x_cur, dtype=float)\n        f_cur = float(f_cur)\n\n        # memory of recent successful unit moves (for PCA bias)\n        mem_steps = []  # list of unit vectors\n        mem_trust = []  # trust weights\n\n        # initialize basis D (n x p) as random orthonormal\n        p = min(self.p, n)\n        R = self.rng.randn(n, p)\n        Q, _ = np.linalg.qr(R)\n        D = Q[:, :p].copy()\n\n        # per-arm statistics\n        counts = np.zeros(p, dtype=int)\n        rewards = np.zeros(p, dtype=float)     # cumulative improvement\n        steps = np.full(p, 0.35 * range_mean, dtype=float)\n        min_step = max(self.min_step_frac * max(1.0, range_mean), 1e-12)\n\n        # small recent moves memory to rotate directions\n        recent_moves = []\n\n        iteration = 0\n        stagnation = 0\n        stagnation_limit = max(12, int(10 + np.log1p(n) * 4))\n        restarts = 0\n        max_restarts = 6\n\n        # helper: rebuild basis mixing PCA of mem_steps and random vectors\n        def rebuild_basis():\n            nonlocal D, p\n            k = p\n            if len(mem_steps) >= 2:\n                M = np.vstack(mem_steps)  # (m, n)\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                try:\n                    U, S, Vt = np.linalg.svd(Mc, full_matrices=False)\n                    pcs = Vt.T  # n x r\n                    take = min(k, pcs.shape[1])\n                    basis_cols = pcs[:, :take]\n                    if take < k:\n                        Rn = self.rng.randn(n, k - take)\n                        stacked = np.column_stack((basis_cols, Rn))\n                        Qb, _ = np.linalg.qr(stacked)\n                        D = Qb[:, :k].copy()\n                    else:\n                        Qb, _ = np.linalg.qr(basis_cols)\n                        D = Qb[:, :k].copy()\n                except np.linalg.LinAlgError:\n                    Rn = self.rng.randn(n, k)\n                    Qb, _ = np.linalg.qr(Rn)\n                    D = Qb[:, :k].copy()\n            else:\n                Rn = self.rng.randn(n, k)\n                Qb, _ = np.linalg.qr(Rn)\n                D = Qb[:, :k].copy()\n\n        # parabolic refinement along direction d anchored at x0\n        def parabolic_refine(x0, f0, d, delta=None):\n            nonlocal evals\n            if delta is None:\n                delta = max(self.parabola_delta_frac * np.mean(steps), min_step)\n            dn = np.linalg.norm(d)\n            if dn == 0 or evals >= budget:\n                return None, None\n            d_unit = d / (dn + 1e-20)\n            # evaluate +delta and -delta\n            x_p = clip(x0 + delta * d_unit)\n            fp, _ = safe_eval(x_p)\n            if fp is None:\n                return None, None\n            if evals >= budget:\n                # only one extra sample available\n                if fp < f0 - 1e-12:\n                    return fp, x_p\n                return None, None\n            x_m = clip(x0 - delta * d_unit)\n            fm, _ = safe_eval(x_m)\n            if fm is None:\n                return None, None\n            # quadratic fit\n            denom = 2.0 * delta * delta\n            A = (fp + fm - 2.0 * f0) / denom\n            B = (fp - fm) / (2.0 * delta)\n            if not np.isfinite(A) or abs(A) < 1e-16 or A <= 0:\n                # pick best of bracket\n                best_f = f0\n                best_x = x0.copy()\n                if fp < best_f:\n                    best_f = fp; best_x = x_p.copy()\n                if fm < best_f:\n                    best_f = fm; best_x = x_m.copy()\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            alpha_star = -B / (2.0 * A)\n            # trust region for alpha_star\n            if abs(alpha_star) > 4.0 * delta:\n                # accept best sampled only\n                best_f = f0\n                best_x = x0.copy()\n                if fp < best_f:\n                    best_f = fp; best_x = x_p.copy()\n                if fm < best_f:\n                    best_f = fm; best_x = x_m.copy()\n                if best_f < f0 - 1e-12:\n                    return best_f, best_x\n                return None, None\n            x_s = clip(x0 + alpha_star * d_unit)\n            fs, _ = safe_eval(x_s)\n            if fs is None:\n                return None, None\n            if fs < f0 - 1e-12:\n                return fs, x_s\n            # fallback\n            best_f = f0\n            best_x = x0.copy()\n            if fp < best_f:\n                best_f = fp; best_x = x_p.copy()\n            if fm < best_f:\n                best_f = fm; best_x = x_m.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # UCB selector\n        def select_ucb():\n            total = counts.sum()\n            if total == 0:\n                untried = np.where(counts == 0)[0]\n                if len(untried) > 0:\n                    return int(self.rng.choice(untried))\n                return int(self.rng.randint(0, p))\n            avg = np.zeros_like(rewards)\n            nonz = counts > 0\n            avg[nonz] = rewards[nonz] / counts[nonz]\n            bonus = np.sqrt(np.log(1.0 + total) / (1.0 + counts))\n            scores = avg + self.ucb_beta * bonus\n            return int(np.argmax(scores))\n\n        # main loop\n        while evals < budget:\n            iteration += 1\n            # occasionally rebuild basis from PCA of mem_steps\n            if iteration % self.rebuild_every == 0:\n                rebuild_basis()\n\n            improved_round = False\n            probes = min(max(2, 2 * p), budget - evals)\n\n            # probe loop\n            for _ in range(probes):\n                if evals >= budget:\n                    break\n                arm = select_ucb()\n                d = D[:, arm].copy()\n                if np.linalg.norm(d) == 0:\n                    d = self.rng.randn(n)\n                    d /= (np.linalg.norm(d) + 1e-20)\n                    D[:, arm] = d.copy()\n                # sample alpha from normal with scale = steps[arm] but allow occasional larger tries\n                if self.rng.rand() < 0.12:\n                    alpha = self.rng.uniform(-2.0 * steps[arm], 2.0 * steps[arm])\n                else:\n                    alpha = self.rng.normal(0.0, steps[arm])\n                if abs(alpha) < 1e-20:\n                    alpha = (0.5 - self.rng.rand()) * steps[arm]\n                x_try = clip(x_cur + alpha * d)\n                f_try, x_try = safe_eval(x_try)\n                if f_try is None:\n                    break\n                counts[arm] += 1\n                reward = max(0.0, f_cur - f_try)\n                rewards[arm] += reward\n\n                if f_try < f_cur - 1e-12:\n                    # success: optionally refine by parabolic fit anchored at x_cur along d\n                    if self.rng.rand() < self.parabola_prob and (budget - evals) >= 2:\n                        delta = max(abs(alpha), self.parabola_delta_frac * steps[arm], min_step)\n                        fres, xres = parabolic_refine(x_cur, f_cur, d, delta=delta)\n                        if fres is not None and fres < f_try - 1e-12:\n                            f_try = fres\n                            x_try = xres.copy()\n                    # accept\n                    x_prev = x_cur.copy()\n                    f_prev = f_cur\n                    x_cur = x_try.copy()\n                    f_cur = float(f_try)\n                    improved_round = True\n                    stagnation = 0\n                    # record successful move\n                    mv = x_cur - x_prev\n                    mvn = np.linalg.norm(mv)\n                    if mvn > 0:\n                        unit_mv = mv / mvn\n                        mem_steps.insert(0, unit_mv.copy())\n                        mem_trust.insert(0, 1.0)\n                        recent_moves.insert(0, unit_mv.copy())\n                        if len(mem_steps) > self.memory_size:\n                            mem_steps.pop(); mem_trust.pop()\n                        if len(recent_moves) > self.memory_size:\n                            recent_moves.pop()\n                        # soft update direction toward the move\n                        gamma = 0.30\n                        D[:, arm] = (1.0 - gamma) * D[:, arm] + gamma * unit_mv\n                        D[:, arm] /= (np.linalg.norm(D[:, arm]) + 1e-20)\n                    # adapt step up\n                    steps[arm] = min(steps[arm] * self.step_growth, self.max_step_mult * range_mean)\n                    # boost reward and trust\n                    rewards[arm] += max(0.0, f_prev - f_cur)\n                    if mem_trust:\n                        mem_trust = [t * 0.97 for t in mem_trust]\n                        mem_trust[0] = mem_trust[0] + 0.2\n                else:\n                    # no improvement -> shrink step\n                    steps[arm] = max(steps[arm] * self.step_shrink, min_step)\n                    stagnation += 1\n                    # slight decay of mem_trust\n                    mem_trust = [t * 0.98 for t in mem_trust]\n\n                # occasional small probabilistic acceptance of nearly-equal worse moves\n                if (not improved_round) and (self.rng.rand() < 0.01):\n                    if f_try <= f_cur + 1e-8:\n                        x_cur = x_try.copy(); f_cur = float(f_try)\n\n                # occasional global random jump to escape\n                if self.rng.rand() < 0.02 and (budget - evals) > 0:\n                    sigma = 0.4 * range_mean * (0.4 + self.rng.rand())\n                    xg = clip(x_cur + self.rng.randn(n) * sigma)\n                    fg, xg = safe_eval(xg)\n                    if fg is None:\n                        break\n                    if fg < f_cur - 1e-12:\n                        # accept and nudge worst arm toward this move\n                        mv = xg - x_cur\n                        mvn = np.linalg.norm(mv)\n                        x_cur = xg.copy(); f_cur = float(fg)\n                        if mvn > 1e-20:\n                            worst = int(np.argmin((rewards + 1e-12) / (1 + counts)))\n                            D[:, worst] = 0.75 * D[:, worst] + 0.25 * (mv / mvn)\n                            D[:, worst] /= (np.linalg.norm(D[:, worst]) + 1e-20)\n                            steps[worst] = max(steps[worst], 0.8 * sigma)\n\n            # end probes\n\n            # periodic orthonormalize to keep numerical stability\n            if iteration % 7 == 0:\n                try:\n                    Q, _ = np.linalg.qr(D)\n                    D = Q[:, :p].copy()\n                except Exception:\n                    # regenerate if QR fails\n                    Rn = self.rng.randn(n, p)\n                    Q, _ = np.linalg.qr(Rn)\n                    D = Q[:, :p].copy()\n\n            # rotate/replace worst arms occasionally\n            if iteration % self.rotate_every == 0:\n                avg_score = np.zeros_like(rewards)\n                nonz = counts > 0\n                avg_score[nonz] = rewards[nonz] / counts[nonz]\n                # choose lowest performing quarter\n                worst_idx = np.argsort(avg_score)[:max(1, p // 4)]\n                for wi in worst_idx:\n                    rnd = self.rng.randn(n)\n                    if len(recent_moves) > 0 and self.rng.rand() < 0.6:\n                        cand = 0.5 * rnd + 0.5 * recent_moves[self.rng.randint(len(recent_moves))]\n                    else:\n                        cand = rnd\n                    cand /= (np.linalg.norm(cand) + 1e-20)\n                    D[:, wi] = cand\n                    counts[wi] = 0\n                    rewards[wi] = 0.0\n                    steps[wi] = 0.35 * range_mean\n\n            # stagnation handling and soft restarts\n            if stagnation >= stagnation_limit:\n                restarts += 1\n                stagnation = 0\n                if restarts > max_restarts:\n                    # final local polishing: try a few tiny random radial probes\n                    tries = min(20, budget - evals)\n                    for _ in range(tries):\n                        if evals >= budget:\n                            break\n                        d = self.rng.randn(n)\n                        d /= (np.linalg.norm(d) + 1e-20)\n                        alpha = 0.02 * range_mean * (0.5 + self.rng.rand())\n                        x_try = clip(x_best + alpha * d) if x_best is not None else clip(x_cur + alpha * d)\n                        ft, _ = safe_eval(x_try)\n                        if ft is None:\n                            break\n                    break\n                else:\n                    # soft restart around best with moderate perturbation and rebuild basis\n                    center = x_best.copy() if x_best is not None else self.rng.uniform(lb, ub)\n                    radius = min(0.6 * range_mean, 0.6 * np.mean(steps))\n                    x_new = clip(center + self.rng.randn(n) * radius)\n                    fn, x_new = safe_eval(x_new)\n                    if fn is None:\n                        break\n                    x_cur = x_new.copy(); f_cur = float(fn)\n                    # clear memory lightly but keep best\n                    mem_steps = mem_steps[:max(1, len(mem_steps)//3)]\n                    mem_trust = mem_trust[:max(1, len(mem_trust)//3)]\n                    recent_moves = recent_moves[:max(1, len(recent_moves)//3)]\n                    rebuild_basis()\n                    # enlarge some steps\n                    steps = np.minimum(steps * 1.6, self.max_step_mult * range_mean)\n                    # continue\n\n            # ensure steps not too small\n            steps = np.maximum(steps, min_step)\n\n            # quick exit if extremely good\n            if f_best <= 1e-12:\n                break\n\n        # final return\n        if x_best is None:\n            x_best = x_cur.copy()\n            f_best = float(f_cur)\n        return float(f_best), np.asarray(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm PCAUCB scored 0.151 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "23e9cf3f-d0e0-406d-bd6d-72692cfe6f5b", "operator": null, "metadata": {"aucs": [0.10349742708793241, 0.1472177004198768, 0.25908047582225746, 0.14612425441871102, 0.17692204070856155, 0.13093149616295163, 0.1731373942231358, 0.10255622826227451, 0.1870376410711424, 0.07903777214285346]}, "task_prompt": ""}
{"id": "ad9d5254-6b65-4a26-880e-32e3817630bb", "fitness": "-inf", "name": "CAMO_DE", "description": "CAMO-DE maintains an ensemble of k≈2·√dim local centers with per-center trust radii (initially 0.25·range) and tracks per-center success/trial counts to multiplicatively adapt radii (shrink 0.90 or expand 1.12 toward a 25% target success rate). It learns a small orthogonal subspace from a bounded success buffer (PCA/SVD, k_sub = ceil(subspace_frac·dim) up to 4) and biases sampling into that subspace while also mixing blockwise trust-region perturbations, DE-style archive-difference jumps, and occasional Lévy-like long jumps (p_de≈0.18, p_levy≈0.08), with mirrored sampling and two candidates per center. An archive of evaluated points fuels DE moves and recombination: every few generations the worst center is replaced by a softmax-weighted recombination of the top centers (with a small perturbation), and occasional 1-D quasi-Newton probes along the dominant learned direction accelerate local convergence. Additional diversity mechanisms include randomized block reshuffles, periodic reseeding on stagnation, caps on archive size and evaluations to respect the budget, and lightweight bookkeeping (archive_cap=3000, success_buf size ≳ max(32,8√dim)).", "code": "import numpy as np\n\nclass CAMO_DE:\n    \"\"\"\n    CAMO-DE: Cascade Adaptive Memory-guided Orthogonal Differential Ensemble\n\n    Main idea:\n      - Maintain an ensemble of local centers (modes), each with its own trust radius.\n      - Learn a small orthogonal subspace from a buffer of recent successful steps and bias sampling inside it.\n      - Generate candidates by a mixture of (i) subspace-directed moves, (ii) blockwise trust-region perturbations,\n        (iii) archive-difference (DE-style) jumps, and (iv) occasional Lévy-like long jumps.\n      - Use per-center success tracking to adapt trust radii and replace poor centers by recombination of the best ones.\n      - Occasionally perform a 1-D quasi-Newton probe along the dominant learned direction to accelerate local convergence.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 ensemble_factor=2.0, init_radius=0.25,\n                 p_levy=0.08, levy_df=3.0,\n                 p_de=0.18, F_de=0.6,\n                 subspace_frac=0.25, success_buf_max=None,\n                 p_quasi_newton=0.06, mirror=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # ensemble size (number of centers)\n        self.k = max(3, int(np.ceil(ensemble_factor * np.sqrt(max(1, self.dim)))))\n        # initial trust radius (fraction of range)\n        self.init_radius = float(init_radius)\n        self.p_levy = float(p_levy)\n        self.levy_df = float(levy_df)\n        self.p_de = float(p_de)\n        self.F_de = float(F_de)\n        self.subspace_frac = float(subspace_frac)\n        self.p_quasi_newton = float(p_quasi_newton)\n        self.mirror = bool(mirror)\n\n        # success buffer size\n        if success_buf_max is None:\n            self.buf_max = max(32, 8 * int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.buf_max = int(success_buf_max)\n\n        # block partition for blockwise moves (simple contiguous blocks)\n        self.n_blocks = max(1, int(np.ceil(np.sqrt(self.dim))))\n        sizes = [self.dim // self.n_blocks] * self.n_blocks\n        for i in range(self.dim % self.n_blocks):\n            sizes[i] += 1\n        self.blocks = []\n        idx = 0\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # global scale estimate from bounds\n        range_mean = np.mean(ub - lb)\n        # initial ensemble centers sampled uniformly\n        centers = [np.random.uniform(lb, ub) for _ in range(self.k)]\n        center_f = [np.inf] * self.k\n        # per-center trust radii (absolute scale units)\n        radii = [self.init_radius * range_mean for _ in range(self.k)]\n        # per-center recent success rates\n        succ_counts = [0] * self.k\n        trial_counts = [1] * self.k\n\n        # archive for DE differences\n        archive_X = []\n        archive_F = []\n\n        # success step buffer (rows are local steps in original coordinates)\n        success_buf = []\n\n        # tracking best\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # evaluate initial centers (but be careful with budget)\n        for i in range(self.k):\n            if evals >= budget:\n                break\n            x = np.clip(centers[i], lb, ub)\n            f = func(x)\n            evals += 1\n            center_f[i] = f\n            archive_X.append(x.copy()); archive_F.append(f)\n            if f < f_opt:\n                f_opt = f; x_opt = x.copy()\n\n        # ensure archive size cap\n        archive_cap = 3000\n\n        gen = 0\n\n        # how many top components to learn in subspace\n        k_sub = max(1, min(4, int(np.ceil(self.subspace_frac * n))))\n        # target success rate for each center (for radius adaptation)\n        target_succ = 0.25\n\n        # main loop\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n\n            # Build a learned orthogonal subspace U from success buffer (rows are steps)\n            U = None\n            if len(success_buf) >= max(8, k_sub + 2):\n                try:\n                    S = np.vstack(success_buf)  # m x n\n                    S = S - np.mean(S, axis=0, keepdims=True)\n                    # small randomized SVD via econ SVD (rows << n usually)\n                    # compute covariance on rows if m < n to be efficient\n                    if S.shape[0] < n:\n                        # compute S * S^T (m x m), then form eigenvectors\n                        M = S @ S.T\n                        # eig decomposition\n                        vals, vecs = np.linalg.eigh(M)\n                        # take top k_sub eigenvectors mapped back\n                        top_idx = np.argsort(vals)[-k_sub:][::-1]\n                        Vsmall = vecs[:, top_idx]  # m x k_sub\n                        U = S.T @ Vsmall\n                        # orthonormalize U\n                        U, _ = np.linalg.qr(U)\n                        U = U[:, :k_sub]\n                    else:\n                        # perform economy SVD\n                        U_svd, _, _ = np.linalg.svd(S, full_matrices=False)\n                        U = U_svd[:, :k_sub]\n                except Exception:\n                    U = None\n\n            # generate candidates per center (one or two each) while respecting budget\n            # choose total candidates budgeted this gen\n            per_center = 2  # candidates per center\n            lam = min(sum([per_center for _ in range(len(centers))]), remaining)\n            # but we will iterate centers and break when budget reached\n            candidates = []\n            cand_origin = []  # which center idx produced it\n            cand_step = []    # store step (for success buffer)\n            cand_vals = []\n\n            # candidate generation\n            for ci, c in enumerate(centers):\n                for t in range(per_center):\n                    if evals + len(candidates) >= budget:\n                        break\n                    # choose operator probabilistically\n                    u = np.random.rand()\n                    # basic scaled normal in full space but scaled by center radius\n                    if U is not None and u < 0.45:\n                        # biased subspace sampling: sample in subspace and small orthogonal noise\n                        z_sub = np.random.randn(k_sub)\n                        step_sub = U @ z_sub\n                        orth = np.random.randn(n)\n                        # project orth onto orth complement\n                        orth = orth - U @ (U.T @ orth)\n                        step = 0.9 * step_sub + 0.15 * orth\n                        # normalize and scale by radii[ci]\n                        step = step / (np.linalg.norm(step) + 1e-12)\n                        step = step * radii[ci] * np.abs(np.mean(z_sub))  # magnitude informed by sub coordinates\n                    elif u < 0.75:\n                        # blockwise trust-region perturbation\n                        bidx = np.random.randint(len(self.blocks))\n                        b = self.blocks[bidx]\n                        step = np.zeros(n)\n                        # random normal on block and small noise elsewhere\n                        step_block = np.random.randn(len(b))\n                        step[b] = step_block * radii[ci]\n                        # small global jitter\n                        step += 0.08 * radii[ci] * np.random.randn(n)\n                    else:\n                        # DE-style archive difference or Lévy-like long jump\n                        if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                            i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                            de = self.F_de * (archive_X[i1] - archive_X[i2])\n                            step = de\n                        else:\n                            # Lévy-like scaled Student-T direction\n                            t_s = np.random.standard_t(self.levy_df)\n                            dirv = np.random.randn(n)\n                            dirv /= (np.linalg.norm(dirv) + 1e-12)\n                            step = dirv * radii[ci] * (1.6 * t_s)\n\n                    # mirrored sampling optional\n                    if self.mirror and (t % 2 == 1):\n                        step = -step\n\n                    x = c + step\n                    x = np.clip(x, lb, ub)\n                    candidates.append(x)\n                    cand_origin.append(ci)\n                    cand_step.append(step)\n\n            # Evaluate candidates up to budget\n            for j, x in enumerate(candidates):\n                if evals >= budget:\n                    break\n                f = func(x)\n                evals += 1\n                cand_vals.append(f)\n                archive_X.append(x.copy()); archive_F.append(f)\n                if len(archive_X) > archive_cap:\n                    archive_X.pop(0); archive_F.pop(0)\n                # update global best\n                if f < f_opt:\n                    f_opt = f; x_opt = x.copy()\n\n            # if no candidates evaluated (budget consumed), break\n            if len(cand_vals) == 0:\n                break\n\n            # Assign improvements back to centers\n            # For each candidate, compare to its origin center; if improved, update center\n            improved_flags = [False] * len(centers)\n            for idx_cand, f in enumerate(cand_vals):\n                ci = cand_origin[idx_cand]\n                x = candidates[idx_cand]\n                step = cand_step[idx_cand]\n                if f < center_f[ci]:\n                    # accept as new center location (greedy)\n                    centers[ci] = x.copy()\n                    center_f[ci] = f\n                    improved_flags[ci] = True\n                    succ_counts[ci] += 1\n                    if len(success_buf) >= self.buf_max:\n                        success_buf.pop(0)\n                    # store the step (in original coords) scaled to unit of range_mean\n                    success_buf.append((step / (range_mean + 1e-20)).copy())\n                    if f < f_opt:\n                        f_opt = f; x_opt = x.copy()\n                trial_counts[ci] += 1\n\n            # Adaptive radius update per center (multiplicative towards target)\n            for ci in range(len(centers)):\n                psucc = succ_counts[ci] / max(1, trial_counts[ci])\n                # small adjustments: if very successful, shrink radius to exploit; else enlarge\n                if psucc > target_succ + 0.05:\n                    radii[ci] *= 0.90\n                elif psucc < target_succ - 0.05:\n                    radii[ci] *= 1.12\n                # bounds\n                radii[ci] = np.clip(radii[ci], 1e-6 * range_mean, 2.5 * range_mean)\n                # decay trial counters slowly\n                trial_counts[ci] = max(1, int(0.9 * trial_counts[ci]))\n                succ_counts[ci] = max(0, int(0.85 * succ_counts[ci]))\n\n            # Periodic recombination: replace worst center with recombination of top centers\n            if gen % 5 == 0:\n                # identify top and worst\n                order = np.argsort(center_f)\n                top = order[:max(1, len(order)//3)]\n                worst = order[-1]\n                # softmax-weighted recombination using top centers (weights from fitness)\n                vals = np.array([center_f[i] for i in top], dtype=float)\n                # stabilize\n                vals = vals - np.min(vals) + 1e-12\n                inv = 1.0 / vals\n                weights = inv / np.sum(inv)\n                new_center = np.sum([weights[i_idx] * centers[top[i_idx]] for i_idx in range(len(top))], axis=0)\n                # random small perturbation\n                new_center = np.clip(new_center + 0.02 * range_mean * np.random.randn(n), lb, ub)\n                # evaluate new center if budget allows\n                if evals < budget:\n                    f_new = func(new_center)\n                    evals += 1\n                    archive_X.append(new_center.copy()); archive_F.append(f_new)\n                    if len(archive_X) > archive_cap:\n                        archive_X.pop(0); archive_F.pop(0)\n                    # replace worst if better or probabilistically to maintain diversity\n                    if (f_new < center_f[worst]) or (np.random.rand() < 0.12):\n                        centers[worst] = new_center.copy()\n                        center_f[worst] = f_new\n                        radii[worst] = np.mean(radii[top]) * 0.9\n                        trial_counts[worst] = 1\n                        succ_counts[worst] = 0\n                        if f_new < f_opt:\n                            f_opt = f_new; x_opt = new_center.copy()\n\n            # Quasi-Newton 1-D probe along dominant direction (global) occasionally\n            if (np.random.rand() < self.p_quasi_newton) and (len(success_buf) >= max(6, k_sub)):\n                # compute dominant direction via simple PCA on success_buf\n                try:\n                    S = np.vstack(success_buf)\n                    S = S - np.mean(S, axis=0, keepdims=True)\n                    # top eigenvector\n                    u, svals, _ = np.linalg.svd(S, full_matrices=False)\n                    d = u[:, 0]\n                except Exception:\n                    d = np.random.randn(n)\n                    d /= (np.linalg.norm(d) + 1e-12)\n                # choose a promising center (best center)\n                best_idx = int(np.argmin(center_f))\n                m = centers[best_idx].copy()\n                # probe at -h, 0, h scaled by center radius\n                h = 0.6 * radii[best_idx]\n                probes = [np.clip(m - h * d, lb, ub),\n                          m.copy(),\n                          np.clip(m + h * d, lb, ub)]\n                # evaluate probes only if budget\n                f_probes = []\n                for p in probes:\n                    if evals >= budget:\n                        break\n                    fp = func(p)\n                    evals += 1\n                    archive_X.append(p.copy()); archive_F.append(fp)\n                    f_probes.append(fp)\n                    if fp < f_opt:\n                        f_opt = fp; x_opt = p.copy()\n                if len(f_probes) == 3:\n                    A, B, C = f_probes\n                    # quadratic interpolation: a = (A - 2B + C) / (2 h^2); b = (C - A) / (2h)\n                    denom = (A - 2.0 * B + C)\n                    if np.abs(denom) > 1e-12:\n                        a = denom / (2.0 * (h ** 2))\n                        b = (C - A) / (2.0 * h)\n                        t_star = -b / (2.0 * a)\n                        # clamp t_star within reasonable bounds\n                        t_star = np.clip(t_star, -2.0 * h, 2.0 * h)\n                        probe = np.clip(m + t_star * d, lb, ub)\n                        if evals < budget:\n                            fp = func(probe)\n                            evals += 1\n                            archive_X.append(probe.copy()); archive_F.append(fp)\n                            if fp < center_f[best_idx]:\n                                centers[best_idx] = probe.copy()\n                                center_f[best_idx] = fp\n                                # record step\n                                step = probe - m\n                                if len(success_buf) >= self.buf_max:\n                                    success_buf.pop(0)\n                                success_buf.append((step / (range_mean + 1e-20)).copy())\n                                succ_counts[best_idx] += 1\n                            if fp < f_opt:\n                                f_opt = fp; x_opt = probe.copy()\n\n            # small random block reshuffle occasionally to encourage new block decompositions\n            if gen % 37 == 0:\n                if np.random.rand() < 0.5 and len(self.blocks) > 1:\n                    # swap few indices among blocks\n                    b1, b2 = np.random.choice(len(self.blocks), size=2, replace=False)\n                    if len(self.blocks[b1]) and len(self.blocks[b2]):\n                        i1 = np.random.choice(self.blocks[b1])\n                        i2 = np.random.choice(self.blocks[b2])\n                        # swap\n                        self.blocks[b1].remove(i1); self.blocks[b1].append(i2)\n                        self.blocks[b2].remove(i2); self.blocks[b2].append(i1)\n\n            # if stagnation (no improvement for many evals), re-seed one center randomly\n            if evals > 50 and (evals - np.min([np.argmin(archive_F) if len(archive_F)>0 else 0])) > max(200, 10 * n):\n                # randomize one center\n                idx_r = np.random.randint(len(centers))\n                centers[idx_r] = np.random.uniform(lb, ub)\n                radii[idx_r] = 0.9 * np.mean(radii)\n                # evaluate new center if allowed\n                if evals < budget:\n                    fnew = func(centers[idx_r])\n                    evals += 1\n                    archive_X.append(centers[idx_r].copy()); archive_F.append(fnew)\n                    center_f[idx_r] = fnew\n                    if fnew < f_opt:\n                        f_opt = fnew; x_opt = centers[idx_r].copy()\n\n            # keep archival bounds\n            if len(archive_X) > archive_cap:\n                archive_X = archive_X[-archive_cap:]\n                archive_F = archive_F[-archive_cap:]\n\n            # safety: if budget nearly exhausted, break\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 287, in __call__, the following error occurred:\nTypeError: only integer scalar arrays can be converted to a scalar index\nOn line: radii[worst] = np.mean(radii[top]) * 0.9", "error": "In the code, line 287, in __call__, the following error occurred:\nTypeError: only integer scalar arrays can be converted to a scalar index\nOn line: radii[worst] = np.mean(radii[top]) * 0.9", "parent_ids": "e256ddcf-bcdf-4570-b35c-49b54c5c01d2", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "b97883e8-3378-4b76-a32b-29a58abf1de5", "fitness": 0.25538810449409693, "name": "ASGE", "description": "The ASGE algorithm combines adaptive subspace sampling (randomly selecting a fraction of coordinates per candidate, with mirrored sampling and occasional intra-block shuffles) with per-coordinate ADAM-like scale adaptation (m1/m2 moments, per_scale clipped to [1e-6,1e2]) to focus step sizes where they matter. It learns a dominant search direction via an online Oja update (oja_lr scaled by dim) and amplifies steps along that eigenvector, while injecting heavy tails via Gamma-length scaling and occasional Cauchy jumps (p_cauchy) plus archive-based DE deltas (p_de, F_de) for long-range exploration. Recombination uses rank-weighted linear combination blended with a per-dimension weighted median to reduce outlier effects, sigma is adapted in log-space toward a target success rate, and subspace fraction self-adjusts based on recent success. Robustness features include bounded sigma/init scales, an archive cap, stagnation detection with reheating and partial restarts, and conservative acceptance of new centers (fitness-based or probabilistic with a decaying temperature).", "code": "import numpy as np\n\nclass ASGE:\n    \"\"\"\n    Adaptive Subspace Gradient-free Ensemble (ASGE)\n\n    Main idea (one-liner above): adaptive subspace sampling with learned dominant\n    direction (Oja updates), Gamma-length scaling + occasional Cauchy jumps,\n    ADAM-like per-coordinate scale adaptation, and small DE-like archive deltas.\n\n    Main tunable parameters (defaults chosen differently from the provided ABLE_DE):\n      - budget (required) : total function evaluations (int)\n      - dim (required)    : problem dimension (int)\n      - pop_factor        : controls population size (default 1.6; different scaling)\n      - init_sigma_scale  : initial sigma relative to range mean (default 0.12 vs 0.20)\n      - n_blocks          : number of coordinate blocks (if None auto sqrt(dim)*0.9)\n      - p_cauchy          : probability to use Cauchy heavy jump (default 0.09)\n      - p_de              : probability to apply archive-based DE delta (default 0.18)\n      - F_de              : DE weight for archive delta (default 0.6; different)\n      - p_shuffle         : probability to shuffle entries inside one block (default 0.08)\n      - mirror            : mirrored sampling (default True)\n      - sub_frac_init     : initial fraction of dims sampled per candidate (default 0.35)\n      - archive_cap       : maximum archive length (default 2500)\n      - seed              : RNG seed (optional)\n    \"\"\"\n    def __init__(self, budget, dim, seed=None,\n                 pop_factor=1.6, init_sigma_scale=0.12, n_blocks=None,\n                 p_cauchy=0.09, p_de=0.18, F_de=0.6, p_shuffle=0.08,\n                 mirror=True, sub_frac_init=0.35, archive_cap=2500):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size (different heuristic than ABLE_DE)\n        self.lambda_ = max(4, int(np.ceil(pop_factor * max(4, np.log(1 + self.dim)))))\n        # blocks: slightly fewer blocks than sqrt(dim) (different)\n        if n_blocks is None:\n            self.n_blocks = max(1, int(np.maximum(1, np.floor(np.sqrt(self.dim) * 0.9))))\n        else:\n            self.n_blocks = max(1, min(int(n_blocks), self.dim))\n\n        # create contiguous blocks initially\n        sizes = [self.dim // self.n_blocks] * self.n_blocks\n        for i in range(self.dim % self.n_blocks):\n            sizes[i] += 1\n        self.blocks = []\n        idx = 0\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n        # hyper-params\n        self.init_sigma_scale = float(init_sigma_scale)\n        self.p_cauchy = float(p_cauchy)\n        self.p_de = float(p_de)\n        self.F_de = float(F_de)\n        self.p_shuffle = float(p_shuffle)\n        self.mirror = bool(mirror)\n\n        # subspace fraction (adapts slowly)\n        self.sub_frac = float(np.clip(sub_frac_init, 0.05, 0.9))\n\n        # archive cap\n        self.archive_cap = int(archive_cap)\n\n        # adaptation rates (different equations)\n        self.target_success = 0.22       # close to 1/5 but different\n        self.log_sigma_lr = 0.08         # learning rate in log-space\n        self.adam_beta1 = 0.85           # for per-coordinate first moment (different)\n        self.adam_beta2 = 0.98           # for per-coordinate second moment (different)\n        self.eps = 1e-12\n\n        # Oja (online PCA) learning rate for principal direction\n        self.oja_lr = max(0.02, 0.7 / np.sqrt(max(1, self.dim)))\n        # stagnation thresholds\n        self.stagn_limit_base = max(25, 6 * self.dim // max(1, len(self.blocks)))\n        self.max_restarts = 3\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds should exist; assume [-5,5] as default fallback\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = -5.0 * np.ones(n)\n            ub = 5.0 * np.ones(n)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center uniformly in bounds\n        m = np.random.uniform(lb, ub)\n\n        # initial sigma in absolute units (different scale)\n        sigma = self.init_sigma_scale * np.mean(ub - lb)\n        log_sigma = np.log(max(self.eps, sigma))\n\n        # ADAM-like per-coordinate moments for scale adaptation (we maintain an adaptive per-dim step multiplier)\n        m1 = np.zeros(n)   # first moment (mean of abs steps)\n        m2 = np.zeros(n)   # second moment (mean of squared abs steps)\n        per_scale = np.ones(n)  # per-dim multiplicative scale (like learning rates)\n\n        # rotation/learned dominant direction (initialize random unit)\n        # We'll maintain a single dominant eigenvector 'p' learned via Oja\n        p = np.random.randn(n)\n        p /= (np.linalg.norm(p) + 1e-20)\n\n        # simple small archive for DE-like deltas\n        archive_X = []\n        archive_F = []\n\n        # tracking\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # evaluate initial center\n        x0 = np.clip(m, lb, ub)\n        f0 = func(x0)\n        evals += 1\n        archive_X.append(x0.copy()); archive_F.append(f0)\n        if len(archive_X) > self.archive_cap:\n            archive_X.pop(0); archive_F.pop(0)\n        f_opt = f0; x_opt = x0.copy()\n        f_center = f0\n        last_improv = evals\n        stagn_count = 0\n        restart_count = 0\n\n        gen = 0\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            # sample base normals; mirrored sampling handled later\n            Z = np.random.randn(lam, n)\n            X = np.zeros((lam, n))\n            Y_local = np.zeros((lam, n))  # store local (pre-rotation) steps used for updates\n            Fvals = np.full(lam, np.inf)\n\n            # adaptively choose subspace size for this generation (small random jitter)\n            sub_frac = np.clip(self.sub_frac * (1.0 + 0.06 * (np.random.rand() - 0.5)), 0.05, 0.9)\n            sub_k = max(1, int(np.round(sub_frac * n)))\n\n            for i in range(lam):\n                z = Z[i].copy()\n                if self.mirror and (i % 2 == 1):\n                    z = -z\n\n                # sample a subset of coordinates to move (subspace)\n                if sub_k < n:\n                    idxs = np.random.choice(n, size=sub_k, replace=False)\n                    mask = np.zeros(n, dtype=bool)\n                    mask[idxs] = True\n                else:\n                    mask = np.ones(n, dtype=bool)\n\n                # build local direction: scaled by per_scale\n                local = per_scale * z\n                # keep only subspace dimensions\n                local[~mask] = 0.0\n\n                # rotate by learned dominant direction by projecting onto p and orthogonal complement\n                # Projection component along p is amplified slightly (different approach)\n                proj = np.dot(local, p) * p\n                orth = local - proj\n                y_rot = 1.0 * orth + 1.15 * proj  # amplify along learned direction\n\n                # occasional Cauchy heavy jump (different heavy-tail scheme than ABLE_DE)\n                if np.random.rand() < self.p_cauchy:\n                    # sample per-dimension Cauchy perturbation limited to preserve budget-likely magnitudes\n                    cauch = np.random.standard_cauchy(size=n)\n                    # scale by median absolute per_scale and sigma\n                    med = np.median(np.abs(per_scale)) + 1e-20\n                    y_rot = y_rot + 0.8 * med * cauch\n\n                # Gamma-length scaling: draw a scalar from Gamma to modulate step length (adds skew)\n                # parameters chosen to produce moderate heavy right tail\n                g_shape = 2.2\n                g_scale = 0.6\n                g = np.random.gamma(shape=g_shape, scale=g_scale)\n                step = sigma * g * y_rot\n\n                x = m + step\n\n                # DE-like archive delta (different weighting): if archive available and coin flip\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    delta = self.F_de * (archive_X[i1] - archive_X[i2])\n                    # apply delta but restrict to subspace mask to preserve locality\n                    delta[~mask] = 0.0\n                    x = x + delta\n\n                # intra-block shuffle mutation occasionally (keeps coordinates but permutes)\n                if (np.random.rand() < self.p_shuffle) and (len(self.blocks) > 0):\n                    bidx = np.random.randint(len(self.blocks))\n                    b = self.blocks[bidx]\n                    if len(b) > 1:\n                        perm = np.random.permutation(len(b))\n                        x_block = x[b].copy()\n                        x[b] = x_block[perm]\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                X[i] = x\n                # store local pre-rotation tiny-normalized vector for learning and ADAM\n                # normalize by sigma and gamma to get unit-free steps\n                Y_local[i] = (y_rot * g) / (1.0 + np.abs(sigma))\n\n            # evaluate candidates, respecting budget\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if len(archive_X) > self.archive_cap:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi; x_opt = xi.copy(); last_improv = evals\n\n            # selection: pick top mu (linear rank-weighted recombination different from softmax)\n            mu = max(1, lam // 2)\n            idxs_sorted = np.argsort(Fvals)\n            sel = idxs_sorted[:mu]\n            X_sel = X[sel]\n            Y_sel = Y_local[sel]\n\n            # linear rank weights: best gets weight mu, worst gets 1, normalized\n            ranks = np.arange(mu, 0, -1)  # mu,...,1\n            weights = ranks / np.sum(ranks)\n\n            # recombine to new center with weighted median smoothing:\n            # compute weighted average and also weighted median to reduce outlier effect\n            m_avg = np.sum(weights[:, None] * X_sel, axis=0)\n            # weighted median per-dim (approx via sorting)\n            m_med = np.zeros(n)\n            for d in range(n):\n                vals = X_sel[:, d]\n                order = np.argsort(vals)\n                cumw = np.cumsum(weights[order])\n                idx_med = np.searchsorted(cumw, 0.5)\n                idx_med = np.clip(idx_med, 0, mu - 1)\n                m_med[d] = vals[order[idx_med]]\n            # combine average and median (different recomb)\n            m_new = 0.7 * m_avg + 0.3 * m_med\n\n            # acceptance: only accept after evaluating candidate center if budget allows\n            if evals < budget:\n                xm = np.clip(m_new, lb, ub)\n                fm = func(xm)\n                evals += 1\n                archive_X.append(xm.copy()); archive_F.append(fm)\n                if len(archive_X) > self.archive_cap:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fm < f_opt:\n                    f_opt = fm; x_opt = xm.copy(); last_improv = evals\n                # Accept if better than center or with small probability proportional to improvement\n                if fm <= f_center:\n                    m = xm.copy(); f_center = fm\n                else:\n                    # acceptance prob scaled by relative improvement magnitude and a slowly decaying temperature\n                    Temp = max(1e-6, 0.8 - 0.0003 * gen)\n                    # smaller probability for worse solutions\n                    prob_accept = 1.0 / (1.0 + np.exp((fm - f_center) / (Temp + 1e-12)))\n                    if np.random.rand() < prob_accept:\n                        m = xm.copy(); f_center = fm\n                    else:\n                        # small nudge toward xm occasionally\n                        if np.random.rand() < 0.06:\n                            m = np.clip(0.9 * m + 0.1 * xm, lb, ub)\n            else:\n                # no budget to evaluate center, do not change center (safer)\n                pass\n\n            # ADAM-like per-coordinate adaptation using selected steps (Y_sel)\n            # compute weighted mean absolute step and squared\n            mean_abs = np.sum(weights[:, None] * np.abs(Y_sel), axis=0)\n            mean_sq = np.sum(weights[:, None] * (Y_sel ** 2), axis=0)\n\n            # update biased moments\n            m1 = self.adam_beta1 * m1 + (1.0 - self.adam_beta1) * mean_abs\n            m2 = self.adam_beta2 * m2 + (1.0 - self.adam_beta2) * mean_sq\n\n            # bias correction\n            # approximate generation count for bias correction (use gen+1)\n            bc1 = 1.0 - self.adam_beta1 ** (gen + 1 + 1e-12)\n            bc2 = 1.0 - self.adam_beta2 ** (gen + 1 + 1e-12)\n            m1_hat = m1 / (bc1 + 1e-20)\n            m2_hat = m2 / (bc2 + 1e-20)\n\n            # per-scale update (different equation than RMSprop)\n            per_scale = 0.9 * per_scale + 0.1 * (m1_hat / (np.sqrt(m2_hat) + self.eps) + 1e-6)\n            # clip scales to reasonable range\n            per_scale = np.clip(per_scale, 1e-6, 1e2)\n\n            # Oja update for principal direction p using the best few steps to steer direction\n            top_k_for_oja = max(1, min(mu, 3))\n            # average of top-k Y_sel rows\n            top_vec = np.mean(Y_sel[:top_k_for_oja], axis=0)\n            # Oja's rule: p <- p + lr * ( (x (x^T p)) - (p (p^T x)^2) ) simplified\n            # We'll do a normalized incremental update to keep p unit\n            if np.linalg.norm(top_vec) > 1e-16:\n                proj_coeff = np.dot(top_vec, p)\n                p = p + self.oja_lr * (proj_coeff * top_vec - (proj_coeff ** 2) * p)\n                # renormalize\n                p /= (np.linalg.norm(p) + 1e-20)\n\n            gen += 1\n\n            # sigma adaptation in log-space (different equation): move log_sigma toward direction indicated by psucc - target\n            successes = np.sum(Fvals < f_center - 1e-12)\n            psucc = successes / max(1, lam)\n            # proportional control in log space\n            log_sigma += self.log_sigma_lr * (self.target_success - psucc)\n            sigma = float(np.exp(log_sigma))\n            # hard bounds on sigma\n            sigma = float(np.clip(sigma, 1e-12, 6.0 * np.mean(ub - lb)))\n            log_sigma = np.log(sigma)\n\n            # adapt subspace fraction slowly: if many successes increase subspace to explore more dims\n            if psucc > self.target_success + 0.05:\n                self.sub_frac = np.clip(self.sub_frac * 1.03, 0.05, 0.9)\n            elif psucc < self.target_success - 0.05:\n                self.sub_frac = np.clip(self.sub_frac * 0.97, 0.05, 0.9)\n\n            # stagnation detection and light reheating (different actions)\n            stagn_limit = max(10, int(self.stagn_limit_base * (1 + 0.2 * stagn_count)))\n            if (evals - last_improv) > stagn_limit:\n                stagn_count += 1\n                # inflate sigma moderately\n                sigma *= (1.15 + 0.08 * stagn_count)\n                log_sigma = np.log(sigma)\n                # randomize a small fraction of coordinates of center\n                kflip = max(1, int(0.06 * n))\n                flip_idx = np.random.choice(n, size=kflip, replace=False)\n                for ii in flip_idx:\n                    m[ii] = np.random.uniform(lb[ii], ub[ii])\n                # perturb principal vector a little\n                p = p + 0.06 * np.random.randn(n)\n                p /= (np.linalg.norm(p) + 1e-20)\n                # clear a fraction of archive to encourage fresh deltas\n                if len(archive_X) > 10:\n                    keep = max(5, int(0.6 * len(archive_X)))\n                    # keep best elements by value\n                    order = np.argsort(archive_F)\n                    keep_idx = set(order[:keep])\n                    archive_X = [archive_X[i] for i in range(len(archive_X)) if i in keep_idx]\n                    archive_F = [archive_F[i] for i in range(len(archive_F)) if i in keep_idx]\n                # evaluate new center if budget allows\n                if evals < budget:\n                    m_clip = np.clip(m, lb, ub)\n                    fm = func(m_clip)\n                    evals += 1\n                    archive_X.append(m_clip.copy()); archive_F.append(fm)\n                    if len(archive_X) > self.archive_cap:\n                        archive_X.pop(0); archive_F.pop(0)\n                    if fm < f_opt:\n                        f_opt = fm; x_opt = m_clip.copy(); last_improv = evals\n                    f_center = fm\n                # if too many restarts, perform a stronger random restart\n                if stagn_count > 0 and stagn_count % 4 == 0:\n                    restart_count += 1\n                    if restart_count <= self.max_restarts:\n                        # reinitialize a portion of m and shrink sigma a bit\n                        idxs = np.random.choice(n, size=max(1, n // 4), replace=False)\n                        for ii in idxs:\n                            m[ii] = np.random.uniform(lb[ii], ub[ii])\n                        sigma = max(sigma * 0.6, 1e-12)\n                        log_sigma = np.log(sigma)\n\n                last_improv = evals\n\n            # ensure center in bounds\n            m = np.clip(m, lb, ub)\n\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ASGE scored 0.255 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e256ddcf-bcdf-4570-b35c-49b54c5c01d2", "operator": null, "metadata": {"aucs": [0.15193774982843233, 0.20345833847299732, 0.3192170478218903, 0.3436733405463118, 0.25417788779075157, 0.3215682698313176, 0.2479966956743097, 0.2860195330134766, 0.2493007257347417, 0.17653145622674016]}, "task_prompt": ""}
{"id": "379187d1-7319-4545-96e7-88447ab9171b", "fitness": "-inf", "name": "CODASS", "description": "The algorithm maintains a central solution m and samples a small population of candidate perturbations in an adaptive low-dimensional subspace U, recombines the top-ranked candidates into a new center and keeps a large archive of evaluated points and the global best. U is learned from recent successful normalized step-deltas via a PCA-like update with an adaptive dimension k (k grows when progress is recent, shrinks otherwise), and sampling mixes subspace directions with an orthogonal complement plus an Ornstein–Uhlenbeck velocity for temporal correlation. Global exploration comes from occasional heavy-tailed Student-t jumps along dominant subspace directions and differential-style perturbations using the evaluation archive (p_levy ≈ 0.1, p_diff ≈ 0.18, F_diff ≈ 0.6), while local exploitation uses lightweight linear/quadratic fits and periodic parabolic line probes. Step-size sigma is initialized proportional to the search range and adapted multiplicatively by observed success rate (sigma_up=1.10, sigma_down=0.92) with reheating and subspace randomization on stagnation; population and memory sizes are set by simple heuristics (lambda_ ≥ 6, success_mem_max ≈ 6·√dim).", "code": "import numpy as np\n\nclass CODASS:\n    \"\"\"\n    Cooperative Directional Adaptive Subspace Search (CODASS)\n\n    Key ideas:\n      - Maintain a center m and a small swarm of candidate perturbations sampled in an adaptive subspace U\n      - Subspace U is learned from recent successful step-deltas (PCA-like) but with an adaptive dimension k\n      - Use velocity (Ornstein-Uhlenbeck style) for temporally-correlated proposals (smooth search)\n      - Occasional Student-T (heavy-tailed) jumps along dominant subspace directions for global escapes\n      - Lightweight curvature/gradient estimate from recent evaluated points to drive short quadratic line probes\n      - Archive of past points used for differential-like perturbations\n      - Adaptive sigma (step-size) controlled by success rate, with reheating on stagnation\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10,\n                 pop_factor=2.0, init_step=0.18,\n                 p_levy=0.10, levy_df=3.0,\n                 p_diff=0.18, F_diff=0.6,\n                 mirror=False, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_factor = float(pop_factor)\n        self.init_step = float(init_step)\n        self.p_levy = float(p_levy)\n        self.levy_df = float(levy_df)\n        self.p_diff = float(p_diff)\n        self.F_diff = float(F_diff)\n        self.mirror = bool(mirror)\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size heuristic\n        self.lambda_ = max(6, int(np.ceil(self.pop_factor * (2 + np.log(max(2, self.dim))))))\n        # memory sizes\n        self.success_mem_max = max(20, 6 * int(np.ceil(np.sqrt(self.dim))))\n        self.archive_max = 3000\n\n        # subspace parameters\n        self.k_min = 1\n        self.k_max = min(self.dim, max(2, int(np.ceil(np.sqrt(self.dim)))))\n        # parameters for OU velocity\n        self.ou_theta = 0.15\n        self.ou_sigma_vel = 0.8\n\n        # sigma adaptation\n        self.target_success = 0.25\n        self.sigma_up = 1.10\n        self.sigma_down = 0.92\n        self.sigma_min = 1e-12\n\n        # stagnation\n        self.stagn_threshold_factor = 8.0\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # read bounds (assume func.bounds.lb, ub available)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center uniformly sampled\n        m = np.random.uniform(lb, ub)\n        # initial step-size sigma scaled to problem range\n        sigma = self.init_step * np.mean(ub - lb)\n\n        # velocity for proposals (OU)\n        v = np.zeros(n)\n\n        # initial subspace: unit coordinate axes first k_min\n        k = self.k_min\n        U = np.eye(n)[:, :k]  # n x k\n\n        # success buffer: store local step deltas in local coords (w.r.t. scale sigma)\n        success_buf = []\n\n        # archive of evaluated points\n        archive_X = []\n        archive_F = []\n\n        # tracking\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # evaluate initial center\n        x0 = np.clip(m, lb, ub)\n        f0 = func(x0)\n        evals += 1\n        archive_X.append(x0.copy()); archive_F.append(f0)\n        f_opt = float(f0); x_opt = x0.copy()\n        f_center = float(f0)\n        last_improv_eval = evals\n        stagnation_limit = max(40, int(self.stagn_threshold_factor * n / max(1, k)))\n        stagn_count = 0\n\n        gen = 0\n\n        # lightweight recent history for curvature/gradient estimate\n        recent_X = [x0.copy()]\n        recent_F = [f0]\n        recent_max = 40\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n\n            # adjust population size down if budget small\n            lam = max(1, lam)\n\n            # decide subspace dimension adaptively:\n            # if progress recently, try to increase k to exploit; otherwise reduce k\n            if (evals - last_improv_eval) < max(20, 2*n):\n                # some growth tendency\n                k = min(self.k_max, k + (1 if np.random.rand() < 0.15 else 0))\n            else:\n                # shrink to focus\n                k = max(self.k_min, k - (1 if np.random.rand() < 0.20 else 0))\n\n            # ensure U has correct columns; if U too small, expand with random orthonormal vectors\n            if U.shape[1] != k:\n                # try to build U by recomputing PCA on success buffer if available\n                if len(success_buf) >= max(6, k):\n                    S = np.vstack(success_buf)\n                    S = S - np.mean(S, axis=0, keepdims=True)\n                    # compute top k eigenvectors via small covariance\n                    try:\n                        C = (S.T @ S) / max(1, S.shape[0])\n                        eigvals, eigvecs = np.linalg.eigh(C)\n                        idx = np.argsort(eigvals)[::-1]\n                        V = eigvecs[:, idx[:k]]\n                        # orthonormalize\n                        Q, _ = np.linalg.qr(V)\n                        U = Q[:, :k]\n                    except np.linalg.LinAlgError:\n                        # fallback random orthonormal\n                        Q, _ = np.linalg.qr(np.random.randn(n, k))\n                        U = Q[:, :k]\n                else:\n                    Q, _ = np.linalg.qr(np.random.randn(n, k))\n                    U = Q[:, :k]\n\n            # sampling: build lam candidate points\n            Xcand = np.zeros((lam, n))\n            cand_steps_local = np.zeros((lam, n))  # local coordinates (coeffs) relative to sigma\n            Fs = np.full(lam, np.inf)\n\n            for i in range(lam):\n                # Ornstein-Uhlenbeck update for velocity (temporal correlation)\n                # v <- v + theta*(0 - v) + sigma_v * N(0,1)\n                v = v + self.ou_theta * (-v) + self.ou_sigma_vel * np.sqrt(2 * self.ou_theta) * np.random.randn(n)\n\n                # sample coefficients in subspace (k dims)\n                z_sub = np.random.randn(k)\n                # occasional mirror to maintain symmetry if desired\n                if self.mirror and (i % 2 == 1):\n                    z_sub = -z_sub\n\n                # orthogonal complement random noise (n-k dims)\n                if k < n:\n                    z_perp = np.random.randn(n - k)\n                else:\n                    z_perp = np.array([])\n\n                # combine into full local coordinate y_local = U * coeffs + W * coeffs_perp\n                # scale subspace coefficients by per-dim weights (here uniform, but could adapt)\n                coeff_sub = z_sub\n                # generate orthonormal basis for perp if needed\n                if k < n:\n                    # complete orthonormal basis by QR of random matrix with U blocked out\n                    # compute projection residual of random matrix and orthonormalize\n                    Rrand = np.random.randn(n, n - k)\n                    # orthogonalize w.r.t. U\n                    proj = U @ (U.T @ Rrand)\n                    Rres = Rrand - proj\n                    Qp, _ = np.linalg.qr(Rres + 1e-12 * np.random.randn(n, n - k))\n                    W = Qp[:, : (n - k)]\n                    coeff_perp = z_perp\n                    y_local = U.dot(coeff_sub) + W.dot(coeff_perp * 0.6)  # less magnitude in perp\n                else:\n                    y_local = U.dot(coeff_sub)\n\n                # scale local step to sigma and add velocity smoothing\n                step = sigma * y_local + 0.5 * v  # velocity nudges the sample\n                # occasionally apply a heavy-tailed jump strictly along dominant subspace vector\n                if np.random.rand() < self.p_levy:\n                    t = np.random.standard_t(self.levy_df)\n                    d0 = U[:, 0] if U.shape[1] > 0 else np.random.randn(n)\n                    step = step + (1.8 * t * sigma * np.mean(np.abs(step) + 1e-12)) * d0\n\n                # small differential archive perturbation\n                if (np.random.rand() < self.p_diff) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de = self.F_diff * (archive_X[i1] - archive_X[i2])\n                    step = step + de\n\n                x = m + step\n                x = np.clip(x, lb, ub)\n\n                Xcand[i] = x\n                # store local coordinate normalized by sigma for learning (approx)\n                cand_steps_local[i] = (x - m) / (sigma + 1e-20)\n\n            # Evaluate candidates with careful budget checking\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fs[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if len(archive_X) > self.archive_max:\n                    archive_X.pop(0); archive_F.pop(0)\n                # update best\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy(); last_improv_eval = evals\n\n                # update recent history\n                recent_X.append(xi.copy()); recent_F.append(fi)\n                if len(recent_X) > recent_max:\n                    recent_X.pop(0); recent_F.pop(0)\n\n            # selection: pick top half of candidates (or at least 1)\n            mu = max(1, lam // 2)\n            idx = np.argsort(Fs)\n            sel_idx = idx[:mu]\n            X_sel = Xcand[sel_idx]\n            steps_sel = cand_steps_local[sel_idx]\n\n            # compute success proportion relative to current center f_center\n            successes = np.sum(Fs[:lam] < (f_center - 1e-12))\n            psucc = successes / max(1.0, lam)\n\n            # recombine: compute a convex move towards a weighted average of selected\n            # use rank-based weights with slight entropy regularization\n            ranks = np.arange(mu)\n            scores = (mu - ranks).astype(float)\n            temp = max(0.5, 0.8 - 0.01 * gen)\n            expw = np.exp(scores / temp)\n            weights = expw / np.sum(expw)\n            m_new = np.sum(weights[:, None] * X_sel, axis=0)\n\n            # try to accept new center if improved or with probability depending on psucc\n            accepted = False\n            # only evaluate m_new if budget allows\n            if evals < budget:\n                m_new_clip = np.clip(m_new, lb, ub)\n                fm = func(m_new_clip)\n                evals += 1\n                archive_X.append(m_new_clip.copy()); archive_F.append(fm)\n                if fm < f_opt:\n                    f_opt = float(fm); x_opt = m_new_clip.copy(); last_improv_eval = evals\n                # accept if improvement\n                if fm <= f_center:\n                    m = m_new_clip.copy(); f_center = float(fm); accepted = True\n                else:\n                    # probabilistic acceptance based on success proportion and magnitude\n                    # compute relative improvement expectation\n                    weight_accept = min(0.6, psucc + 0.2)\n                    if np.random.rand() < weight_accept:\n                        m = m_new_clip.copy(); f_center = float(fm); accepted = True\n                    else:\n                        # small nudging toward m_new if rejected\n                        if np.random.rand() < 0.10:\n                            m = np.clip(0.9 * m + 0.1 * m_new_clip, lb, ub)\n            else:\n                # no evaluations left for center, accept the clipped average\n                m = np.clip(m_new, lb, ub)\n\n            # update success buffer with best step deltas (store local steps normalized by sigma)\n            q = max(1, mu // 3)\n            good_steps = steps_sel[:q]\n            for sstep in good_steps:\n                success_buf.append(sstep.copy())\n                if len(success_buf) > self.success_mem_max:\n                    success_buf.pop(0)\n\n            # adjust U more directly if many successes: recompute PCA on success_buf\n            if (len(success_buf) >= max(6, k)) and (np.random.rand() < 0.6):\n                S = np.vstack(success_buf)\n                S = S - np.mean(S, axis=0, keepdims=True)\n                try:\n                    C = (S.T @ S) / max(1, S.shape[0])\n                    eigvals, eigvecs = np.linalg.eigh(C)\n                    idxe = np.argsort(eigvals)[::-1]\n                    V = eigvecs[:, idxe[:k]]\n                    Q, _ = np.linalg.qr(V)\n                    # gently mix with previous U to avoid abrupt changes\n                    alpha = 0.28\n                    U = (1 - alpha) * U + alpha * Q[:, :k]\n                    U, _ = np.linalg.qr(U)  # orthonormalize\n                    U = U[:, :k]\n                except np.linalg.LinAlgError:\n                    # perturb randomly\n                    Q, _ = np.linalg.qr(np.random.randn(n, k))\n                    U = Q[:, :k]\n\n            # lightweight curvature & gradient estimate via local quadratic fit if we have enough recent points\n            grad_est = np.zeros(n)\n            hdiag_est = np.ones(n) * 1e-6\n            if len(recent_X) >= 6:\n                # center recent points around m\n                RX = np.vstack(recent_X)\n                RF = np.array(recent_F)\n                Xc = RX - m[None, :]\n                # solve least squares for linear model F ~ a + g^T x + 0.5 x^T H_diag x (diagonal Hessian)\n                # use design matrix with linear terms and squared terms diag\n                A_lin = Xc  # m x n\n                A_quad = 0.5 * (Xc ** 2)\n                A = np.hstack([np.ones((A_lin.shape[0], 1)), A_lin, A_quad])  # m x (1+n+n)\n                try:\n                    sol, *_ = np.linalg.lstsq(A, RF, rcond=None)\n                    a0 = sol[0]\n                    g = sol[1:1+n]\n                    Hdiag = sol[1+n:1+n+n]\n                    grad_est = g\n                    # keep reasonable hdiag\n                    hdiag_est = np.clip(Hdiag, 1e-8, 1e3)\n                except np.linalg.LinAlgError:\n                    pass\n\n            # periodic 1D quadratic line probe along promising direction (use grad_est or dominant U[:,0])\n            if (gen % max(5, int(5 + n / 6)) == 0) and (evals < budget):\n                # choose probe direction\n                if np.linalg.norm(grad_est) > 1e-12:\n                    d = -grad_est  # go downhill\n                else:\n                    d = U[:, 0] if U.shape[1] > 0 else np.random.randn(n)\n                d = d / (np.linalg.norm(d) + 1e-20)\n                # generate three probes: at -a, 0, +b scaled adaptively\n                a = 0.5 * sigma * (1.0 + 0.1 * np.random.randn())\n                b = 0.8 * sigma * (1.0 + 0.1 * np.random.randn())\n                alphas = [-a, 0.0, b]\n                probes = []\n                fprobes = []\n                for alpha in alphas:\n                    if evals >= budget:\n                        break\n                    xprobe = np.clip(m + alpha * d, lb, ub)\n                    fp = func(xprobe)\n                    evals += 1\n                    probes.append(xprobe.copy())\n                    fprobes.append(fp)\n                    archive_X.append(xprobe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = float(fp); x_opt = xprobe.copy(); last_improv_eval = evals\n                # if three probes collected, try parabolic interpolation to estimate minimum\n                if (len(fprobes) == 3) and (evals < budget):\n                    f1, f2, f3 = fprobes\n                    x1, x2, x3 = alphas\n                    denom = (x1 - x2) * (x1 - x3) * (x2 - x3) + 1e-20\n                    # compute parabola vertex t = ( (f1*(x2**2 - x3**2) + f2*(x3**2 - x1**2) + f3*(x1**2 - x2**2)) ) / (2 * (f1*(x2 - x3) + f2*(x3 - x1) + f3*(x1 - x2)) )\n                    num = (f1*(x2**2 - x3**2) + f2*(x3**2 - x1**2) + f3*(x1**2 - x2**2))\n                    den = 2.0 * (f1*(x2 - x3) + f2*(x3 - x1) + f3*(x1 - x2)) + 1e-20\n                    t = num / den\n                    # if t is reasonable (within span), probe it\n                    if np.isfinite(t) and (min(alphas) - 1e-8 <= t <= max(alphas) + 1e-8) and (evals < budget):\n                        xpar = np.clip(m + t * d, lb, ub)\n                        fp = func(xpar)\n                        evals += 1\n                        archive_X.append(xpar.copy()); archive_F.append(fp)\n                        if fp < f_opt:\n                            f_opt = float(fp); x_opt = xpar.copy(); last_improv_eval = evals\n                        # accept if better than center\n                        if fp <= f_center:\n                            m = xpar.copy(); f_center = float(fp)\n\n            # Adapt sigma according to success proportion\n            if psucc > self.target_success + 0.05:\n                sigma = max(self.sigma_min, sigma * self.sigma_down)\n            elif psucc < self.target_success - 0.05:\n                sigma = sigma * self.sigma_up\n            else:\n                # slight random jitter to avoid lock\n                sigma = max(self.sigma_min, sigma * (1.0 + 0.005 * (np.random.rand() - 0.5)))\n\n            # stagnation handling\n            if (evals - last_improv_eval) > stagnation_limit:\n                stagn_count += 1\n                # reheat step-size and diversify subspace\n                sigma *= (1.4 + 0.15 * stagn_count)\n                # randomize a fraction of U columns\n                for _ in range(min(3, max(1, k))):\n                    if np.random.rand() < 0.6:\n                        col = np.random.randint(0, k)\n                        rnd = np.random.randn(n)\n                        # orthogonalize against existing U\n                        for j in range(k):\n                            rnd = rnd - U[:, j] * (U[:, j] @ rnd)\n                        rnd = rnd / (np.linalg.norm(rnd) + 1e-12)\n                        U[:, col] = rnd\n                U, _ = np.linalg.qr(U)\n                # perturb center slightly\n                m = np.clip(m + sigma * 0.6 * np.random.randn(n), lb, ub)\n                # evaluate new center if budget allows\n                if evals < budget:\n                    fm = func(m)\n                    evals += 1\n                    archive_X.append(m.copy()); archive_F.append(fm)\n                    if fm < f_opt:\n                        f_opt = float(fm); x_opt = m.copy(); last_improv_eval = evals\n                    f_center = float(fm)\n                # clear some memory\n                success_buf = []\n                recent_X = [m.copy()]\n                recent_F = [f_center]\n                last_improv_eval = evals\n\n            # keep archive size bounded\n            if len(archive_X) > self.archive_max:\n                extra = len(archive_X) - self.archive_max\n                archive_X = archive_X[extra:]\n                archive_F = archive_F[extra:]\n\n            gen += 1\n\n            # safety: ensure center is within bounds\n            m = np.clip(m, lb, ub)\n\n            # stop if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "e256ddcf-bcdf-4570-b35c-49b54c5c01d2", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "15a47a64-ef3b-4e30-bc58-eac867b81a5b", "fitness": 0.20779896842576936, "name": "AdaOrthoEnsemble", "description": "The algorithm combines per-coordinate adaptive scaling (Adam-like first/second moments with beta1=0.9, beta2=0.999 and small eps) with an online exponentially-decayed covariance and cheap power-iteration PCA (cov_decay=0.88, pow_iters=6, top_k) to bias search along learned orthonormal directions. It generates a modest population (lambda from pop_factor=1.6) of candidates using random subset masks (sparse block updates), mixing mostly Gaussian local steps with occasional heavy-tailed Student‑t Lévy steps (p_levy≈0.18, levy_df≈2.5) scaled by a trust-radius (init_radius_factor≈0.18) and augmented sometimes by an archive-based differential perturbation (p_archive≈0.22, adaptive F). Selection uses harmonic-ranked weighting to recombine a new center, with probabilistic acceptance and multiplicative radius adaptation toward a target success rate (0.25; radius ×1.10/0.88), plus deterministic probes along the top eigenvector and conservative restart/shrink when stagnating. Practical safeguards include clipped bounds, archive capping (~2500), optional mirroring, and moderate randomness injection when PCA fails to keep exploration stable.", "code": "import numpy as np\n\nclass AdaOrthoEnsemble:\n    \"\"\"\n    Adaptive Orthonormal Ensemble Search (AOES)\n\n    Key ideas (novel compared to provided algorithm):\n      - per-coordinate adaptive scaling using Adam-like first/second moment estimates\n      - online low-cost covariance estimate (exponential decay) and occasional power-iteration\n        to extract dominant orthonormal search directions (Oja-like / power-method hybrid)\n      - mixture of Gaussian and Student-T (Lévy-like) perturbations, but with different df and\n        a magnitude schedule tied to a trust-radius variable (called radius)\n      - trust-radius adapts toward a higher target success (0.25) using multiplicative updates\n      - archive-based rand/1-style perturbation but with adaptive F based on recent successes\n      - block/coordinate masks chosen randomly each generation (not fixed contiguous blocks)\n      - controlled restarts/shrinking on stagnation\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 pop_factor=1.6, init_radius_factor=0.18,\n                 p_levy=0.18, levy_df=2.5,\n                 p_archive=0.22, F_archive=0.6,\n                 adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-8,\n                 cov_decay=0.88, pow_iters=6, top_k=1,\n                 mirror=False):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # population size heuristic (different from ABLE-DE)\n        self.lambda_ = max(6, int(np.ceil(pop_factor * max(3, np.sqrt(self.dim) * 2.0))))\n        # initial radius relative to range (different scale)\n        self.init_radius_factor = float(init_radius_factor)\n\n        # heavy-tail mixing\n        self.p_levy = float(p_levy)\n        self.levy_df = float(levy_df)\n\n        # archive DE\n        self.p_archive = float(p_archive)\n        self.F_archive = float(F_archive)\n\n        # Adam-like params for per-dim adaptation (different equations)\n        self.adam_beta1 = float(adam_beta1)\n        self.adam_beta2 = float(adam_beta2)\n        self.adam_eps = float(adam_eps)\n\n        # online covariance / PCA settings\n        self.cov_decay = float(cov_decay)\n        self.pow_iters = int(pow_iters)\n        self.top_k = max(1, min(int(top_k), self.dim))\n\n        self.mirror = bool(mirror)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (Many BBOB uses [-5,5], but read generically)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        rng = np.random\n\n        # initial center uniformly in bounds\n        center = rng.uniform(lb, ub)\n\n        # trust-radius (controls step magnitudes)\n        radius = self.init_radius_factor * np.mean(ub - lb)\n\n        # Adam-like moments (first and second) for per-dim scaling (different equations)\n        m1 = np.zeros(n)\n        v2 = np.ones(n) * 1e-6\n\n        # online covariance estimate for local steps (keeps n x n diagonal+lowrank implicitly)\n        # we'll keep a full small covariance matrix but with decay (cost O(n^2) occasionally)\n        cov = np.zeros((n, n))\n\n        # orthonormal basis (initially identity)\n        U = np.eye(n)\n\n        # archive\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # evaluate initial center\n        x0 = np.clip(center, lb, ub)\n        f0 = func(x0)\n        evals += 1\n        archive_X.append(x0.copy()); archive_F.append(f0)\n        f_opt = f0; x_opt = x0.copy()\n        f_center = f0\n\n        gen = 0\n        last_improv = evals\n        stagn_limit = max(40, 6 * n)\n\n        # target success for radius adaptation (different value 0.25)\n        target_success = 0.25\n\n        # small history buffer for adaptive F scaling\n        recent_success_counts = []\n\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n\n            Z = rng.randn(lam, n)\n            candidates = np.zeros((lam, n))\n            step_locals = np.zeros((lam, n))  # local coordinates before rotation/scaling\n            fvals = np.full(lam, np.inf)\n\n            # adaptively choose proportion of dims to change per candidate: mix wide/narrow probes\n            # We ensure variety by drawing mask sizes from [1..n] with bias to small blocks\n            for i in range(lam):\n                z = Z[i].copy()\n                if self.mirror and (i % 2 == 1):\n                    z = -z\n\n                # normalize direction to unit vector\n                znorm = np.linalg.norm(z) + 1e-20\n                dir_unit = z / znorm\n\n                # choose block mask (random subset) — different from fixed contiguous blocks\n                mask_size = max(1, int(np.round(np.clip(rng.poisson(lam=1.8) + 1, 1, n))))\n                mask_idx = rng.choice(n, size=mask_size, replace=False)\n                mask = np.zeros(n, dtype=bool); mask[mask_idx] = True\n\n                # mixture: Gaussian small step or Lévy large-tail step\n                if rng.rand() < self.p_levy:\n                    # Student-T scalar magnitude (heavy-tailed) scaled to radius and mean adaptive scale\n                    t = rng.standard_t(self.levy_df)\n                    # relative per-dim scale from Adam moments\n                    per_dim_scale = (np.sqrt(v2) + self.adam_eps)\n                    step = dir_unit * (t * radius * 1.8) * (per_dim_scale / np.mean(per_dim_scale))\n                else:\n                    # Gaussian step scaled by radius and Adam per-dim scale\n                    per_dim_scale = (np.sqrt(v2) + self.adam_eps)\n                    step = z * radius * 0.6 * (per_dim_scale / np.mean(per_dim_scale))\n\n                # apply mask: only mutate masked coords; other coords zero-step\n                step_masked = np.zeros_like(step)\n                step_masked[mask] = step[mask]\n\n                # rotate into original coordinate system using learned basis U (only top_k used more strongly)\n                # amplify movement along U[:, :top_k]\n                lowrank_component = U[:, :self.top_k] @ (U[:, :self.top_k].T @ step_masked)\n                remainder = step_masked - lowrank_component\n                # combine so that lowrank directions get slightly higher weight\n                step_world = 1.12 * lowrank_component + 0.88 * remainder\n\n                # archive-based differential perturbation (rand/1) with adaptive small chance\n                if (rng.rand() < self.p_archive) and (len(archive_X) >= 3):\n                    a, b, c = rng.choice(len(archive_X), size=3, replace=False)\n                    Xa = archive_X[a]; Xb = archive_X[b]; Xc = archive_X[c]\n                    # adaptive F: if recent success high, be bolder\n                    recent_success = np.mean(recent_success_counts[-8:]) if recent_success_counts else 0.0\n                    F_adapt = self.F_archive * (1.0 + 0.6 * np.clip(recent_success - target_success, -0.5, 1.0))\n                    de_step = F_adapt * (Xa - Xb) + 0.25 * (Xc - center)\n                    step_world = step_world + de_step\n\n                x = center + step_world\n                x = np.clip(x, lb, ub)\n                candidates[i] = x\n                step_locals[i] = step_world  # store world step for updates\n\n            # evaluate candidates (respect budget)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = candidates[i]\n                fi = func(xi)\n                evals += 1\n                fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                # bound archive size modestly\n                if len(archive_X) > 2500:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi; x_opt = xi.copy(); last_improv = evals\n\n            # selection: choose top mu (rank-weighted with harmonic weights, different from softmax)\n            mu = max(1, lam // 2)\n            order = np.argsort(fvals)\n            sel = order[:mu]\n            X_sel = candidates[sel]\n            steps_sel = step_locals[sel]\n\n            # harmonic weights: w_k proportional to 1/(rank+1)\n            ranks = np.arange(1, mu + 1)\n            weights = 1.0 / ranks\n            weights = weights / np.sum(weights)\n\n            # recombine center as weighted median-like (using weighted average)\n            m_new = np.sum(weights[:, None] * X_sel, axis=0)\n\n            # optionally evaluate new center (only if budget remains)\n            moved = False\n            if evals < budget:\n                m_clip = np.clip(m_new, lb, ub)\n                fm = func(m_clip)\n                evals += 1\n                archive_X.append(m_clip.copy()); archive_F.append(fm)\n                if fm < f_center:\n                    center = m_clip.copy()\n                    f_center = fm\n                    moved = True\n                    if fm < f_opt:\n                        f_opt = fm; x_opt = m_clip.copy(); last_improv = evals\n                else:\n                    # accept with small probability proportional to relative improvement potential\n                    diff = fm - f_center\n                    Temp = 0.8 * max(1e-8, 1.0 - 0.0006 * gen)\n                    p_accept = 1.0 / (1.0 + np.exp(diff / (Temp + 1e-12)))\n                    if rng.rand() < p_accept:\n                        center = m_clip.copy()\n                        f_center = fm\n                        moved = True\n                    else:\n                        # gentle move towards m_new sometimes\n                        if rng.rand() < 0.06:\n                            center = np.clip(0.9 * center + 0.1 * m_new, lb, ub)\n\n            else:\n                # no budget to evaluate center, adopt weighted mean (safe)\n                center = np.clip(m_new, lb, ub)\n\n            # compute success count this generation (relative to center at sample time)\n            successes = np.sum(fvals < f_center - 1e-12)\n            psucc = successes / max(1, lam)\n            recent_success_counts.append(psucc)\n\n            # update Adam-like moments using weighted average of selected step magnitudes (world steps)\n            # we update using the weighted mean step (directional)\n            step_mean = np.sum(weights[:, None] * steps_sel, axis=0)\n            g = step_mean  # treat step as \"gradient-like\" direction to adapt scales\n            m1 = self.adam_beta1 * m1 + (1.0 - self.adam_beta1) * g\n            v2 = self.adam_beta2 * v2 + (1.0 - self.adam_beta2) * (g * g + 1e-12)\n            # bias correction\n            t = gen + 1\n            m1_hat = m1 / (1.0 - (self.adam_beta1 ** t))\n            v2_hat = v2 / (1.0 - (self.adam_beta2 ** t))\n            # per-dim effective scale used for next generation (different equation)\n            per_dim_scale = np.sqrt(v2_hat) + self.adam_eps\n\n            # update online covariance estimate (using selected steps) with exponential decay\n            # use outer product of step_mean to bias principal directions\n            cov = self.cov_decay * cov + (1.0 - self.cov_decay) * np.outer(step_mean, step_mean)\n\n            # periodically extract top_k eigenvectors via power iteration on cov (cheap)\n            gen += 1\n            if gen % max(3, int(6 + n // 6)) == 0:\n                try:\n                    # initialize V as current U columns\n                    k = min(self.top_k, n)\n                    V = U[:, :k].copy()\n                    # randomize a bit\n                    V += 0.02 * rng.randn(n, k)\n                    # power iterations with covariance\n                    for _ in range(self.pow_iters):\n                        Zp = cov @ V\n                        V, _ = np.linalg.qr(Zp)\n                    # mix into U gently\n                    alpha = 0.3\n                    U[:, :k] = (1.0 - alpha) * U[:, :k] + alpha * V[:, :k]\n                    # re-orthonormalize\n                    Q, _ = np.linalg.qr(U)\n                    U = Q[:, :n]\n                except np.linalg.LinAlgError:\n                    U = U + 0.03 * rng.randn(n, n)\n                    Q, _ = np.linalg.qr(U)\n                    U = Q[:, :n]\n\n            # adapt trust-radius using success vs target (different multipliers)\n            if psucc > target_success + 0.05:\n                radius *= 1.10  # more success -> be bolder\n            elif psucc < target_success - 0.05:\n                radius *= 0.88  # fewer successes -> shrink\n            else:\n                radius *= 1.0\n            # bound radius to reasonable multiples of range\n            max_radius = 6.0 * np.mean(ub - lb)\n            radius = np.clip(radius, 1e-12, max_radius)\n\n            # stagnation handling: if no improvement for many evaluations, perform partial restart / shrink\n            if (evals - last_improv) > stagn_limit:\n                # shrink radius and partially randomize center\n                radius *= 0.5\n                # randomize a small fraction of coordinates\n                kflip = max(1, n // 8)\n                idxs = rng.choice(n, size=kflip, replace=False)\n                for ii in idxs:\n                    center[ii] = rng.uniform(lb[ii], ub[ii])\n                # clear some history and nudge covariance\n                cov *= 0.3\n                U = np.eye(n)\n                last_improv = evals\n                # evaluate new center if budget allows\n                if evals < budget:\n                    mc = np.clip(center, lb, ub)\n                    fm = func(mc)\n                    evals += 1\n                    archive_X.append(mc.copy()); archive_F.append(fm)\n                    if fm < f_opt:\n                        f_opt = fm; x_opt = mc.copy(); last_improv = evals\n                    f_center = fm\n\n            # small deterministic local probes along top direction occasionally (different magnitudes)\n            if (gen % max(10, int(12 + n // 4)) == 0) and (evals < budget):\n                d = U[:, 0]\n                for alpha in [0.25, -0.25, 0.6, -0.6]:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(center + alpha * radius * d, lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = fp; x_opt = probe.copy(); last_improv = evals\n                        f_center = fp\n\n            # ensure center in bounds\n            center = np.clip(center, lb, ub)\n\n            # conservative cap on archive memory (already bounded above)\n            # continue until budget exhausted\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaOrthoEnsemble scored 0.208 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e256ddcf-bcdf-4570-b35c-49b54c5c01d2", "operator": null, "metadata": {"aucs": [0.09936777464889979, 0.1739256343795177, 0.2780440155678574, 0.2572977546873261, 0.20267607073610872, 0.25423066263770466, 0.22370908947747992, 0.23992210736159414, 0.20792322787077766, 0.14089334689042754]}, "task_prompt": ""}
{"id": "ec9edca9-d728-4646-88b6-e844ed4acc8a", "fitness": 0.2758238043934757, "name": "HARMONY", "description": "HARMONY is a hybrid heuristic that blends CMA-like path-length sigma control, a diagonal RMS-style per-coordinate scale D, and a learned low-rank subspace U (via SVD on recent successful steps) to get both coordinate-wise and subspace-adaptive search. Sampling mixes mirrored Gaussian draws scaled by D with a low-rank contribution from U (k ≈ ceil(sqrt(n))), occasional heavy‑tailed Cauchy jumps (p_cauchy=0.12) and archive-driven DE-style difference mutations (p_de=0.18, F_de=0.7); population size λ is set heuristically as 4+⌊3 ln(dim)⌋ and initial σ = 0.18·mean_range. Adaptation rules include weighted recombination with log-weights and μ_eff, σ updated by a path-length ps (cs, damps, χn like CMA), D updated by EMA of squared selected steps, and U refreshed from a buffered set of success directions. Robustness mechanisms include an archive for reseeding, stagnation-detection reheating (inflate σ, perturb U/D and nudge toward the best archive point), occasional focused 1‑D probes along the top mode, probabilistic acceptance of new center, and strict bound clipping for feasibility.", "code": "import numpy as np\n\nclass HARMONY:\n    \"\"\"\n    HARMONY: HYbrid Adaptive Rotational-Memory Optimizer for Noiseless BBOB\n\n    One-line: Hybrid Subspace-Path CMA-DE combining diagonal RMS, a learned low-rank\n    subspace, mirrored sampling, archive DE-deltas, heavy-tailed jumps, path-length\n    sigma control and stagnation reheating for robust bounded-budget continuous optimization.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, pop_factor=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population heuristic\n        if pop_factor is None:\n            self.lambda_ = max(4, int(4 + np.floor(3 * np.log(max(2, self.dim)))))\n        else:\n            self.lambda_ = max(4, int(np.ceil(pop_factor * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n        # low-rank dimension\n        self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        # basic problem scale\n        rng = np.maximum(ub - lb, 1e-12)\n        mean_range = float(np.mean(rng))\n\n        # strategy params (CMA-like)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length control constants\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize dynamic state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.18 * mean_range\n        # diagonal RMS-like scales (std per coordinate)\n        D = np.ones(n)\n        # low-rank orthonormal basis (n x k)\n        k = min(self.k, n)\n        rand_mat = np.random.randn(n, k)\n        try:\n            U, _ = np.linalg.qr(rand_mat)\n            U = U[:, :k]\n        except np.linalg.LinAlgError:\n            U = np.zeros((n, k))\n        # evolution path for sigma\n        ps = np.zeros(n)\n\n        # buffers and archive\n        success_buffer = []  # store local y vectors (pre-sampling scaling) for subspace updates\n        buf_max = max(20, 8 * k)\n        archive_X = []\n        archive_F = []\n        max_archive = 4000\n\n        # other controls\n        p_cauchy = 0.12\n        p_de = 0.18\n        F_de = 0.7\n        mirrored = True\n        stagnation_thresh = max(8 * n, 200)\n        last_improve_eval = 0\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of center\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_opt = fm; x_opt = xm.copy(); last_improve_eval = evals\n            f_center = fm\n\n        # helper to update archive safely\n        def add_archive(x, f):\n            archive_X.append(x.copy()); archive_F.append(f)\n            if len(archive_X) > max_archive:\n                # pop oldest\n                del archive_X[0]; del archive_F[0]\n\n        gen = 0\n        # main loop\n        while evals < budget:\n            gen += 1\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n\n            # generate standard normals\n            Z = np.random.randn(current_lambda, n)\n            Zlow = np.random.randn(current_lambda, k) if k > 0 else np.zeros((current_lambda, 0))\n\n            X = np.zeros((current_lambda, n))\n            Y_local = np.zeros((current_lambda, n))  # store pre-rotation/local scaled step y (for learning)\n            # adapt mixing weight of low-rank part using its recent singular values' magnitude heuristic\n            alpha_low = 0.8 * np.mean(D) if k > 0 else 0.0\n            # produce candidates\n            for i in range(current_lambda):\n                z = Z[i].copy()\n                # mirrored sampling for noise reduction\n                if mirrored and (i % 2 == 1):\n                    z = -z\n                # local pre-rotation step = D * z (elementwise)\n                y_local = D * z\n                # low-rank contribution\n                if k > 0:\n                    y_low = U.dot(Zlow[i])\n                    # mix low-rank with local diagonal step\n                    y = y_local + alpha_low * y_low\n                else:\n                    y = y_local\n                # occasional heavy-tailed jump (Cauchy-like) in learned subspace or random direction\n                if np.random.rand() < p_cauchy:\n                    # use t-dist scaled along top subspace if available else random\n                    if k > 0:\n                        t = np.random.standard_cauchy()\n                        dir_low = U[:, 0] if U.shape[1] >= 1 else (z / (np.linalg.norm(z) + 1e-20))\n                        # create jump primarily along top mode + small orthogonal noise\n                        y = (np.mean(D) * (0.8 * dir_low + 0.2 * z / (np.linalg.norm(z) + 1e-20))) * (3.0 * t)\n                    else:\n                        t = np.random.standard_cauchy()\n                        y = (D * z / (np.linalg.norm(z) + 1e-20)) * (3.0 * t)\n                # final candidate\n                x = m + sigma * y\n                # occasional DE-style archive difference mutation\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n                X[i] = x\n                Y_local[i] = y  # store pre-scaling (in sigma units)\n\n            # evaluate candidates (sequentially to respect budget)\n            Fvals = np.full(current_lambda, np.inf)\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                add_archive(xi, fi)\n                if fi < f_opt:\n                    f_opt = fi; x_opt = xi.copy(); last_improve_eval = evals\n            # if no candidate evaluated (shouldn't happen) continue\n            if np.all(np.isinf(Fvals)):\n                break\n\n            # selection and recombination\n            idx = np.argsort(Fvals)\n            sel_idx = idx[:mu]\n            X_sel = X[sel_idx]\n            Y_sel = Y_local[sel_idx]  # steps in sigma units\n            # recombine weighted mean\n            m_new = np.sum(weights[:, None] * X_sel, axis=0)\n            # compute weighted mean step y_w\n            y_w = np.sum(weights[:, None] * Y_sel, axis=0)\n\n            # update sigma using path-length ps (approximate inverse sqrt cov by 1/D)\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n\n            # also mild 1/5-style correction based on success proportion in this generation\n            successes = np.sum(Fvals < (f_center - 1e-12)) if 'f_center' in locals() else np.sum(Fvals < f_opt + 1e-12)\n            psucc = successes / max(1, current_lambda)\n            if psucc > 0.25:\n                sigma *= 0.97\n            elif psucc < 0.10:\n                sigma *= 1.06\n            # clamp sigma\n            sigma = float(np.clip(sigma, 1e-12, 5.0 * mean_range + 1e-12))\n\n            # accept new mean with greedy + occasional probabilistic nudging if center worsens,\n            # but avoid extra evaluations for acceptance to respect budget: use archive-based surrogate\n            # Accept if recombined center is inside archive and good, else accept if weighted improvement among sampled\n            # Simple rule: if average of selected fitness is better than f_center, accept, else accept with small prob\n            avg_sel_f = np.mean(Fvals[sel_idx])\n            accept = True\n            if 'f_center' in locals():\n                if avg_sel_f <= f_center:\n                    accept = True\n                else:\n                    # allow probabilistic acceptance decreasing with gap\n                    gap = avg_sel_f - f_center\n                    if np.random.rand() < np.exp(-gap / (1e-6 + max(1e-2, sigma / mean_range))):\n                        accept = True\n                    else:\n                        accept = False\n            if accept:\n                m = np.clip(m_new, lb, ub)\n                if 'f_center' in locals() and avg_sel_f < f_center:\n                    f_center = avg_sel_f\n            else:\n                # small nudge toward m_new occasionally\n                if np.random.rand() < 0.08:\n                    m = np.clip(0.92 * m + 0.08 * m_new, lb, ub)\n\n            # update per-coordinate scales D via EMA on squared selected steps (y in sigma units)\n            c_d = 0.25\n            y2 = np.sum(weights[:, None] * (Y_sel ** 2), axis=0)\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n            # keep D in reasonable range\n            D = np.clip(D, 1e-8, 1e3)\n\n            # store weighted success direction in buffer (project local y back to local pre-scaling)\n            success_buffer.append(y_w.copy())\n            if len(success_buffer) > buf_max:\n                success_buffer.pop(0)\n\n            # update low-rank U periodically via SVD of success_buffer\n            if (len(success_buffer) >= max(3, k)) and (gen % max(1, int(3)) == 0):\n                Ymat = np.vstack(success_buffer).T  # n x m\n                # center\n                Ymat = Ymat - np.mean(Ymat, axis=1, keepdims=True)\n                try:\n                    Unew, svals, _ = np.linalg.svd(Ymat, full_matrices=False)\n                    k_take = min(k, Unew.shape[1])\n                    if k_take > 0:\n                        U = Unew[:, :k_take]\n                except np.linalg.LinAlgError:\n                    # small perturbation fallback\n                    U = U + 0.01 * np.random.randn(*U.shape)\n                    try:\n                        U, _ = np.linalg.qr(U)\n                        U = U[:, :k]\n                    except Exception:\n                        pass\n\n            # stagnation detection and reheating\n            if (evals - last_improve_eval) > stagnation_thresh:\n                # inflate sigma, perturb U and D, and perform a targeted archive-guided jump\n                sigma *= (1.5 + 0.1 * np.random.rand())\n                # perturb U\n                if k > 0:\n                    U = U + 0.06 * np.random.randn(*U.shape)\n                    try:\n                        U, _ = np.linalg.qr(U)\n                        U = U[:, :k]\n                    except Exception:\n                        pass\n                # boost diagonal scales mildly\n                D *= (1.0 + 0.2 * np.random.rand(n))\n                D = np.clip(D, 1e-8, 1e3)\n                # nudges: pick a good archive point and move toward it\n                if len(archive_X) > 0:\n                    pick = np.argmin(archive_F)\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                    m = np.clip(m, lb, ub)\n                    # evaluate new center if budget allows (useful to escape stagnation)\n                    if evals < budget:\n                        fm = func(m)\n                        evals += 1\n                        add_archive(m, fm)\n                        if fm < f_opt:\n                            f_opt = fm; x_opt = m.copy(); last_improve_eval = evals\n                        f_center = fm\n                # clear buffer to allow fresh subspace discovery\n                success_buffer = []\n                last_improve_eval = evals\n\n            # occasional focused 1D probes along top learned direction\n            if (gen % max(8, int(6 + n/10))) == 0 and (evals + 3 < budget):\n                # pick top direction\n                if k > 0:\n                    d = U[:, 0]\n                else:\n                    # fallback to largest D coordinate\n                    d = np.zeros(n); d[np.argmax(D)] = 1.0\n                probes = [0.6, 0.3, -0.3, -0.6]\n                for alpha in probes:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + sigma * alpha * d * np.mean(D), lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    add_archive(probe, fp)\n                    if fp < f_opt:\n                        f_opt = fp; x_opt = probe.copy(); last_improve_eval = evals\n\n            # keep archive size bounded\n            if len(archive_X) > max_archive:\n                del archive_X[:len(archive_X) - max_archive]\n                del archive_F[:len(archive_F) - max_archive]\n\n            # ensure center in bounds\n            m = np.clip(m, lb, ub)\n\n            # loop continues until budget exhausted\n\n        if x_opt is None:\n            # fallback: sample random point if nothing found\n            x = np.random.uniform(lb, ub)\n            fx = func(x) if evals < budget else np.inf\n            if fx < f_opt:\n                f_opt = fx; x_opt = x.copy()\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm HARMONY scored 0.276 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "e256ddcf-bcdf-4570-b35c-49b54c5c01d2", "operator": null, "metadata": {"aucs": [0.10853037841583824, 0.1618844052477444, 0.21510248661794718, 0.17109983434151343, 0.14524799504164043, 0.20258733392112205, 0.3183530149845486, 0.33552872698703884, 0.9729313994058708, 0.1269724689714934]}, "task_prompt": ""}
{"id": "a90dac81-0e73-455d-9c0e-562b15e3c49d", "fitness": "-inf", "name": "LARSPS", "description": "LARSPS mixes randomized low-dimensional subspace probing (subspace dim ~ ceil(sqrt(n)), probes per round ~ max(4,2k)) with mirrored sampling and an LRU memory of successful unit directions to focus search cheaply under tight budgets. It layers several exploration mechanisms — occasional DE-like archive differences (p_de=0.28), Mantegna–Levy heavy‑tailed jumps (p_levy=0.12) and Levy restarts on stagnation — together with a lightweight rank‑one/cumulation covariance update (C, ps, pc, c1, cmu, cs, cc derived from classical CMA-like formulas) to bias sampling and adapt a step-size sigma (init 0.25·mean_range, mild multiplicative growth on success and shrink on failure). Local exploitation is enforced by short safe golden‑section line searches and small local Gaussian polishing triggered on improvements, while safe_eval and archive pruning enforce strict budget accounting and memory control. Eigen updates are performed infrequently (eig_every = O(n)) to keep matrix adaptations cheap, and many parameter choices (pop_scale→lam, mu_eff, damps, chi_n) follow robust CMA‑style heuristics to balance exploration/exploitation.", "code": "import numpy as np\n\nclass LARSPS:\n    \"\"\"\n    LARSPS: Levy-Adaptive Random Subspace + Polished Search\n    One-line: Hybrid random-subspace probing + short line-search polishing with lightweight\n    covariance cumulation, archive-based DE differences, mirrored sampling and occasional\n    Mantegna-Levy jumps for robust exploration under strict budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, memory_size=8, pop_scale=5):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.pop_scale = pop_scale\n        if seed is not None:\n            self.rng = np.random.RandomState(seed)\n        else:\n            self.rng = np.random.RandomState()\n\n    # Mantegna's Levy for heavy tails\n    def _levy_mantegna(self, n, alpha=1.5, scale=1.0):\n        sigma_u = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                   (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n        u = self.rng.normal(0, sigma_u, size=n)\n        v = self.rng.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1.0 / alpha))\n        return scale * step\n\n    def __call__(self, func):\n        n = self.dim\n        rng = self.rng\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        domain_range = ub - lb\n        mean_range = np.mean(domain_range)\n\n        # state and parameters (blend of ARSS style and LARDE ideas)\n        evals = 0\n        f_best = np.inf\n        x_best = None\n\n        # start point\n        x_cur = rng.uniform(lb, ub)\n        f_cur = func(x_cur)\n        evals += 1\n        f_best = float(f_cur)\n        x_best = x_cur.copy()\n\n        # archive for DE-like differences and reuse\n        archive_X = [x_cur.copy()]\n        archive_F = [float(f_cur)]\n\n        # memory of successful directions (LRU)\n        dir_memory = []\n\n        # covariance-ish lightweight state (rank-one like)\n        C = np.eye(n)\n        B = np.eye(n)\n        D = np.ones(n)\n        invsqrtC = np.eye(n)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n\n        # population-like and cumulation constants (simple, robust choices)\n        lam = max(4, int(self.pop_scale + np.floor(5.0 * np.log(max(1, n)))))\n        mu = max(1, lam // 2)\n        # recombination-like weights (descending)\n        weights = np.log(np.arange(1, mu + 1) + 0.5)\n        weights = (weights[::-1])\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        cc = 2.0 / (n + 2.0)\n        cs = 0.4 * (mu_eff / (n + mu_eff + 1))\n        c1 = 1.0 / max(1.0, (n + 3.0) ** 2)\n        cmu = 0.4 * (1.0 - c1) * min(1.0, mu_eff / (2.0 * n))\n        damps = 1.0 + 0.3 * cs + 0.2 * np.sqrt(max(0.0, mu_eff / n))\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # step-size and safeguards\n        sigma = max(1e-12, 0.25 * mean_range)\n        sigma_min = 1e-12\n        sigma_max = 5.0 * mean_range\n\n        # eigen recompute frequency\n        eig_every = max(1, int(8 * n))\n        eigen_eval_counter = 0\n\n        # probabilities and params\n        p_de = 0.28\n        p_levy = 0.12\n        use_mirroring = True\n\n        # short 1D golden-section search (budget-respecting)\n        def line_search(x0, f0, d, init_step=1.0, max_evals=10):\n            # normalized direction\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            # bracket try\n            remain = max(0, self.budget - evals)\n            if remain <= 0:\n                return None, None\n            a = 0.0\n            fa = f0\n            b = init_step\n            xb = np.clip(x0 + b * d, lb, ub)\n            try:\n                fb = func(xb)\n            except Exception:\n                fb = np.inf\n            # count eval\n            nonlocal_eval_increment(1)\n            # if both directions not better, try negative\n            if fb >= fa:\n                b = -init_step\n                xb = np.clip(x0 + b * d, lb, ub)\n                try:\n                    fb = func(xb)\n                except Exception:\n                    fb = np.inf\n                nonlocal_eval_increment(1)\n                if fb >= fa:\n                    return None, None\n            # expand a little\n            expansion = 1.5\n            expand_count = 0\n            while self.budget - evals > 0 and expand_count < 3:\n                new_b = b * expansion\n                xb = np.clip(x0 + new_b * d, lb, ub)\n                try:\n                    fnew = func(xb)\n                except Exception:\n                    fnew = np.inf\n                nonlocal_eval_increment(1)\n                if fnew < fb:\n                    b = new_b\n                    fb = fnew\n                    expand_count += 1\n                else:\n                    break\n            # golden-section between a and b\n            remain = max(0, self.budget - evals)\n            if remain <= 0:\n                return (fb, np.clip(x0 + b * d, lb, ub)) if fb < fa else (None, None)\n            gr = (np.sqrt(5) - 1) / 2\n            left, right = a, b\n            c = right - gr * (right - left)\n            d_alpha = left + gr * (right - left)\n            xc = np.clip(x0 + c * d, lb, ub)\n            xd = np.clip(x0 + d_alpha * d, lb, ub)\n            try:\n                fc = func(xc); nonlocal_eval_increment(1)\n            except Exception:\n                fc = np.inf\n            try:\n                fd = func(xd); nonlocal_eval_increment(1)\n            except Exception:\n                fd = np.inf\n            best_f = fa\n            best_x = x0.copy()\n            if fc < best_f:\n                best_f = fc; best_x = xc.copy()\n            if fd < best_f:\n                best_f = fd; best_x = xd.copy()\n            iters = 0\n            while self.budget - evals > 0 and iters < max_evals and abs(right - left) > 1e-12:\n                iters += 1\n                if fc < fd:\n                    right = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    c = right - gr * (right - left)\n                    xc = np.clip(x0 + c * d, lb, ub)\n                    try:\n                        fc = func(xc); nonlocal_eval_increment(1)\n                    except Exception:\n                        fc = np.inf\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    left = c\n                    c = d_alpha\n                    fc = fd\n                    d_alpha = left + gr * (right - left)\n                    xd = np.clip(x0 + d_alpha * d, lb, ub)\n                    try:\n                        fd = func(xd); nonlocal_eval_increment(1)\n                    except Exception:\n                        fd = np.inf\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0:\n                return best_f, best_x\n            return None, None\n\n        # helper to increment evals and update archive/best safely when we call func inside nested functions\n        def nonlocal_eval_increment(count):\n            # placeholder to satisfy closure; actual increments handled at call sites below\n            pass\n\n        # We'll instead centralize safe eval to maintain archive and counting\n        def safe_eval(x):\n            nonlocal evals, f_best, x_best, archive_X, archive_F, x_cur, f_cur\n            if evals >= self.budget:\n                return None, None\n            x_clipped = np.clip(np.asarray(x, dtype=float), lb, ub)\n            try:\n                f = func(x_clipped)\n            except Exception:\n                f = np.inf\n            evals += 1\n            archive_X.append(x_clipped.copy())\n            archive_F.append(float(f))\n            if f < f_best:\n                f_best = float(f)\n                x_best = x_clipped.copy()\n            return float(f), x_clipped\n\n        # Replace line_search to use safe_eval internally (budget safe)\n        def line_search_safe(x0, f0, d, init_step=1.0, max_evals=10):\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            if evals >= self.budget:\n                return None, None\n            a = 0.0\n            fa = f0\n            b = init_step\n            xb = np.clip(x0 + b * d, lb, ub)\n            out = safe_eval(xb)\n            if out[0] is None:\n                return None, None\n            fb, xb = out\n            if fb >= fa:\n                b = -init_step\n                xb = np.clip(x0 + b * d, lb, ub)\n                out = safe_eval(xb)\n                if out[0] is None:\n                    return None, None\n                fb, xb = out\n                if fb >= fa:\n                    return None, None\n            expansion = 1.5\n            expand_count = 0\n            while evals < self.budget and expand_count < 3:\n                new_b = b * expansion\n                xb = np.clip(x0 + new_b * d, lb, ub)\n                out = safe_eval(xb)\n                if out[0] is None:\n                    break\n                fnew, xb = out\n                if fnew < fb:\n                    b = new_b\n                    fb = fnew\n                    expand_count += 1\n                else:\n                    break\n            if evals >= self.budget:\n                return (fb, np.clip(x0 + b * d, lb, ub)) if fb < fa else (None, None)\n            gr = (np.sqrt(5) - 1) / 2\n            left, right = a, b\n            c = right - gr * (right - left)\n            d_alpha = left + gr * (right - left)\n            xc = np.clip(x0 + c * d, lb, ub)\n            xd = np.clip(x0 + d_alpha * d, lb, ub)\n            out = safe_eval(xc)\n            if out[0] is None:\n                return None, None\n            fc, xc = out\n            out = safe_eval(xd)\n            if out[0] is None:\n                return None, None\n            fd, xd = out\n            best_f = fa\n            best_x = x0.copy()\n            if fc < best_f:\n                best_f = fc; best_x = xc.copy()\n            if fd < best_f:\n                best_f = fd; best_x = xd.copy()\n            iters = 0\n            while evals < self.budget and iters < max_evals and abs(right - left) > 1e-12:\n                iters += 1\n                if fc < fd:\n                    right = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    c = right - gr * (right - left)\n                    xc = np.clip(x0 + c * d, lb, ub)\n                    out = safe_eval(xc)\n                    if out[0] is None:\n                        break\n                    fc, xc = out\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    left = c\n                    c = d_alpha\n                    fc = fd\n                    d_alpha = left + gr * (right - left)\n                    xd = np.clip(x0 + d_alpha * d, lb, ub)\n                    out = safe_eval(xd)\n                    if out[0] is None:\n                        break\n                    fd, xd = out\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0:\n                return best_f, best_x\n            return None, None\n\n        # main loop: subspace probing rounds\n        while evals < self.budget:\n            remaining = self.budget - evals\n            # subspace dimension somewhat like sqrt(n), at least 1, at most n\n            k = max(1, int(min(n, int(np.ceil(np.sqrt(n))))))\n            probes = max(4, 2 * k)\n            improved_this_round = False\n\n            # Build basis: reuse some memory directions if available\n            use_mem = min(len(dir_memory), k // 2)\n            basis_cols = []\n            if use_mem > 0:\n                for i in range(use_mem):\n                    basis_cols.append(dir_memory[i].copy())\n            needed = k - len(basis_cols)\n            if needed > 0:\n                R = rng.randn(n, needed)\n                if basis_cols:\n                    R = np.column_stack((np.column_stack(basis_cols), R))\n                Q, _ = np.linalg.qr(R)\n                basis = Q[:, :k]\n            else:\n                basis = np.column_stack(basis_cols)[:, :k]\n                # orthonormalize\n                Q, _ = np.linalg.qr(basis)\n                basis = Q[:, :k]\n\n            # mirrored sampling in subspace coefficients if even\n            if use_mirroring and (probes % 2 == 0):\n                half = probes // 2\n                coeffs_half = rng.randn(half, k)\n                coeffs = np.vstack([coeffs_half, -coeffs_half])\n            else:\n                coeffs = rng.randn(probes, k)\n\n            # generate probes\n            for pi in range(probes):\n                if evals >= self.budget:\n                    break\n                coeff = coeffs[pi]\n                d = basis @ coeff\n                dn = np.linalg.norm(d)\n                if dn == 0:\n                    continue\n                d = d / dn\n                # base step scaled by sigma and trust-like factor\n                alpha = rng.uniform(-1.0, 1.0) * sigma * np.sqrt(k)  # scale with sigma and subspace dim\n                # small Gaussian noise shaped by current covariance\n                z_noise = rng.randn(n)\n                noise = (B * D) @ z_noise  # B @ (D * z)\n                # form candidate\n                x_try = x_cur + alpha * d + 0.6 * sigma * (noise / (np.linalg.norm(noise) + 1e-12))\n                # DE-like archive differential occasionally\n                if (rng.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = rng.choice(len(archive_X), size=2, replace=False)\n                    Fk = np.exp(rng.normal(np.log(0.8), 0.4))\n                    x_try = x_try + Fk * (archive_X[i1] - archive_X[i2])\n                # occasional Levy jump added to candidate\n                if rng.rand() < p_levy:\n                    levy_step = self._levy_mantegna(n, alpha=1.5, scale=0.6 * sigma)\n                    x_try = x_try + levy_step\n                # clip and evaluate\n                out = safe_eval(x_try)\n                if out[0] is None:\n                    break\n                f_try, x_ret = out\n                # update eigen counter roughly when we evaluated a probe\n                eigen_eval_counter += 1\n\n                # If improvement, accept and refine with a short line-search\n                if f_try < f_cur - 1e-12:\n                    dir_succ = x_ret - x_cur\n                    norm_dir = np.linalg.norm(dir_succ)\n                    if norm_dir > 1e-16:\n                        dir_unit = dir_succ / norm_dir\n                    else:\n                        dir_unit = d.copy()\n                    # store direction LRU\n                    dir_memory.insert(0, dir_unit.copy())\n                    if len(dir_memory) > self.memory_size:\n                        dir_memory.pop()\n                    # polish with line search (safe)\n                    ls_init = max(0.6 * sigma, 0.1 * mean_range)\n                    ls_out = line_search_safe(x_cur, f_cur, dir_unit, init_step=ls_init, max_evals=min(8, self.budget - evals))\n                    if ls_out[0] is not None:\n                        f_polish, x_polish = ls_out\n                        if f_polish < f_try:\n                            f_try = f_polish\n                            x_ret = x_polish.copy()\n                    # accept improved point\n                    step_vec = (x_ret - x_cur) / (sigma + 1e-20)\n                    # update paths and covariance (rank-one style)\n                    y_w = step_vec.copy()\n                    ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (invsqrtC @ y_w)\n                    norm_ps = np.linalg.norm(ps)\n                    hsig = 1.0 if (norm_ps / (np.sqrt(1.0 - (1.0 - cs) ** (2.0 * max(1, evals / max(1, lam)))) + 1e-20) / chi_n) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n                    pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n                    rank_one = np.outer(pc, pc)\n                    C = (1.0 - c1 - cmu) * C + c1 * (rank_one + (1.0 - hsig) * cc * (2.0 - cc) * C) + cmu * np.outer(y_w, y_w)\n                    # adapt sigma with cumulation and success multiplier\n                    sigma *= np.exp((cs / damps) * (norm_ps / (chi_n + 1e-20) - 1.0))\n                    sigma = min(max(sigma, sigma_min), sigma_max)\n                    # mild growth on success\n                    sigma *= 1.12\n                    # accept point\n                    x_cur = x_ret.copy()\n                    f_cur = float(f_try)\n                    improved_this_round = True\n                else:\n                    # small shrink on failed probe\n                    sigma *= 0.985\n                    if sigma < sigma_min:\n                        sigma = sigma_min\n\n            # occasional Levy restart around best if stagnation (light)\n            if (not improved_this_round) and (rng.rand() < 0.05) and (evals < self.budget):\n                jump = self._levy_mantegna(n, alpha=1.5, scale=1.2 * mean_range)\n                x_jump = np.clip(x_best + jump, lb, ub)\n                out = safe_eval(x_jump)\n                if out[0] is not None:\n                    fj, xj = out\n                    if fj < f_cur:\n                        x_cur = xj.copy()\n                        f_cur = float(fj)\n                        sigma = max(sigma, 0.3 * mean_range)\n\n            # occasional small local intensification around best\n            if improved_this_round and (evals < self.budget):\n                # small local gaussian polishing\n                for _ in range(min(3, self.budget - evals)):\n                    step_vec = rng.randn(n)\n                    step_vec = step_vec / (np.linalg.norm(step_vec) + 1e-16) * (0.4 * sigma)\n                    out = safe_eval(np.clip(x_cur + step_vec, lb, ub))\n                    if out[0] is None:\n                        break\n                    f_try, x_try = out\n                    if f_try < f_cur - 1e-12:\n                        x_cur = x_try.copy()\n                        f_cur = f_try\n                        # update memory and mild sigma up\n                        dir_succ = (x_try - x_cur)\n                        if np.linalg.norm(dir_succ) > 1e-12:\n                            dir_memory.insert(0, dir_succ / (np.linalg.norm(dir_succ)))\n                            if len(dir_memory) > self.memory_size:\n                                dir_memory.pop()\n                        sigma = min(sigma * 1.07, sigma_max)\n\n            # recompute eigendecomposition occasionally\n            if eigen_eval_counter >= eig_every:\n                eigen_eval_counter = 0\n                # symmetrize C\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    D2, B = np.linalg.eigh(C)\n                    D2 = np.maximum(D2, 1e-20)\n                    D = np.sqrt(D2)\n                    invsqrtC = (B * (1.0 / D)) @ B.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(n)\n                    B = np.eye(n)\n                    D = np.ones(n)\n                    invsqrtC = np.eye(n)\n\n            # ensure sigma bounds\n            sigma = min(max(sigma, sigma_min), sigma_max)\n\n            # prune archive occasionally keep it manageable\n            if len(archive_X) > max(800, 50 * n):\n                idx_sorted = np.argsort(archive_F)\n                keep_n = max(400, 20 * n)\n                keep_idx = idx_sorted[:keep_n].tolist()\n                archive_X = [archive_X[i] for i in keep_idx]\n                archive_F = [archive_F[i] for i in keep_idx]\n\n            # quick termination if nearly optimal\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "e256ddcf-bcdf-4570-b35c-49b54c5c01d2", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "2293986d-a7d5-428f-a8df-80ab4975b975", "fitness": 0.3170011218562857, "name": "ARPM", "description": "The algorithm centers search in a momentum-biased center m and generates proposals by blending a low-rank structured component (orthonormal basis B, k_sub ≈ max(1, dim//4)) with full-space/diagonal noise (per-dimension sigma), using a trimmed-mean recombination of a small population (lam ≈ 3+√n) to propose center updates. A small portfolio of specialized operators (rot_gauss, levy_jump, pocket_hop, coord_mix, surrogate_step) is soft-selected via a bandit over log-weights (softmax temp≈0.35, update α≈0.2) so that operator probabilities adapt to normalized improvement rewards. Acceptance and adaptation are governed by a tempered Metropolis rule (initial T≈0.5·span, slow decay) and an adaptive trust radius r (shrink on many successes, expand on stagnation) while sigma is updated by weighted second moments of selected steps. Learning and diversification use a pocket archive of elite centers for hops, an incremental PCA update of B from recent successes, a linear surrogate for directional probes and periodic line searches, heavy-tailed Levy jumps for long-range moves, and explicit stagnation reinjection (radius inflation, basis jitter, pocket jumps).", "code": "import numpy as np\n\nclass ARPM:\n    \"\"\"\n    ARPM (Adaptive Rotational Population with Momentum and Pockets)\n\n    Key ideas / novel elements:\n      - Low-rank rotational sampler: maintain an orthonormal basis B (k << n) and sample steps\n        as a blend of low-rank (structured) and full-space (exploratory) components.\n      - Momentum on the center: keeps a velocity vector that biases proposals, giving inertia.\n      - Pocket archive: a few elite \"pockets\" (local centers) that the search can hop to for diversification.\n      - Tempered Metropolis acceptance: uphill moves accepted with probability exp(-(df)/T); T decays.\n      - Adaptive operator soft-selection: operators' log-weights are updated from normalized rewards.\n      - Directional surrogate probe: fit a ridge linear surrogate on recent successes for guided 1D moves.\n      - Stagnation reinjection: randomized low-rank restart + radius inflation.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_r_frac=0.22, k_sub=None, pocket_size=4):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = np.random.default_rng(seed)\n        self.init_r_frac = float(init_r_frac)\n        self.k_sub = min(self.dim, 1 + (self.dim // 4)) if k_sub is None else min(self.dim, k_sub)\n        self.pocket_size = int(max(1, pocket_size))\n        # operator portfolio\n        self.operators = ['rot_gauss', 'levy_jump', 'pocket_hop', 'coord_mix', 'surrogate_step']\n        self.k_ops = len(self.operators)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (respect func.bounds if provided)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, float(lb))\n        if ub.shape == ():\n            ub = np.full(n, float(ub))\n\n        span = np.mean(ub - lb)\n        # initialize center uniformly\n        m = self.rng.uniform(lb, ub)\n        m = np.clip(m, lb, ub)\n\n        # initial trust radius\n        r = float(self.init_r_frac * span)\n        r = max(r, 1e-8)\n\n        # per-dimension scale (diagonal exploration)\n        sigma = np.ones(n) * 0.8\n\n        # low-rank orthonormal basis B (n x k)\n        k = max(1, min(self.k_sub, n))\n        Rand = self.rng.standard_normal((n, k))\n        Q, _ = np.linalg.qr(Rand)\n        B = Q[:, :k]\n\n        # momentum (velocity) in global coords\n        mom = np.zeros(n)\n        mu_m = 0.65  # momentum retention\n\n        # pockets: list of (center, f)\n        pockets = []\n        pocket_cap = self.pocket_size\n\n        # small success buffer for surrogate and PCA\n        success_X = []   # steps relative to center\n        success_df = []\n        buf_max = max(40, 8 * n)\n\n        # operator bandit (log-weights)\n        op_logw = np.zeros(self.k_ops)\n        op_age = np.zeros(self.k_ops)\n\n        # temperature for Metropolis acceptance\n        T = max(1e-6, 0.5 * span)\n        T_decay = 0.996\n\n        # other hyperparams\n        lam = max(6, int(3 + np.ceil(np.sqrt(n))))  # population size per generation\n        levy_scale = 1.2\n        coord_frac = min(0.35, 6.0 / n)\n        surrogate_lambda = 1e-3\n        line_probe_every = max(12, int(6 * (n / 10 + 1)))\n        gen = 0\n\n        # evaluate initial center\n        f_center = float(func(np.clip(m, lb, ub)))\n        evals = 1\n        f_opt = f_center\n        x_opt = m.copy()\n        last_improv = evals\n\n        # helpers\n        def softmax(logw, temp=0.4):\n            z = (logw - np.max(logw)) / max(1e-12, temp)\n            e = np.exp(z)\n            return e / (np.sum(e) + 1e-20)\n\n        def sample_levy_vector():\n            # symmetric Levy-like heavy tail via Cauchy scaled and truncated\n            v = self.rng.standard_cauchy(n) * levy_scale\n            # truncate extreme tails to avoid blow-ups\n            v = np.clip(v, -10.0, 10.0)\n            return v\n\n        def fit_linear_surrogate(buf_X, buf_df):\n            # solve ridge: df ≈ g^T dx  => g = (A^T A + λI)^{-1} A^T y\n            if len(buf_X) < min(6, n//2 + 2):\n                return None\n            A = np.vstack(buf_X)\n            y = np.array(buf_df, dtype=float)\n            ATA = A.T @ A\n            reg = surrogate_lambda * np.eye(n)\n            try:\n                g = np.linalg.solve(ATA + reg, A.T @ y)\n            except np.linalg.LinAlgError:\n                g = np.linalg.pinv(ATA + reg) @ (A.T @ y)\n            return g\n\n        def orthonormalize_columns(M):\n            Q, _ = np.linalg.qr(M)\n            return Q[:, :min(Q.shape[1], n)]\n\n        # main loop - ensure we don't exceed budget\n        while evals < budget:\n            remaining = budget - evals\n            batch = min(lam, remaining)\n            # policy for operator selection\n            probs = softmax(op_logw, temp=0.35)\n            Xcand = np.zeros((batch, n))\n            Fcand = np.full(batch, np.inf)\n            op_used = np.zeros(batch, dtype=int)\n            step_vecs = np.zeros((batch, n))\n\n            # pre-sample normals to reduce RNG overhead\n            Z = self.rng.standard_normal((batch, n))\n\n            for i in range(batch):\n                op_idx = self.rng.choice(self.k_ops, p=probs)\n                op_used[i] = int(op_idx)\n                z = Z[i]\n\n                # low-rank vs high-rank blending parameter (adaptive-ish)\n                alpha = 0.6  # favor low-rank structure\n                # construct structured step:\n                # local scaled noise\n                local = sigma * z\n                # low-rank projection (in basis B)\n                low = B @ (B.T @ local)\n                high = local - low\n                # velocity-injected step\n                step = r * (alpha * low + (1.0 - alpha) * high) + 0.5 * mom\n\n                # operator-specific perturbations\n                if self.operators[op_idx] == 'rot_gauss':\n                    # small randomized scaling\n                    step = step * (0.8 + 0.4 * self.rng.random())\n\n                elif self.operators[op_idx] == 'levy_jump':\n                    # heavy-tailed jump component added\n                    lj = sample_levy_vector()\n                    step = step + (r * 0.9) * (lj / (np.linalg.norm(lj) + 1e-12))\n\n                elif self.operators[op_idx] == 'pocket_hop':\n                    # hop toward a random pocket with noise\n                    if pockets and self.rng.random() < 0.85:\n                        p_idx = self.rng.integers(len(pockets))\n                        pocket_center, pocket_f = pockets[p_idx]\n                        vec_to_pocket = pocket_center - m\n                        step = 0.6 * vec_to_pocket + 0.4 * step + r * 0.1 * self.rng.standard_normal(n)\n                    else:\n                        # fallback to rot_gauss\n                        step = step * (0.9 + 0.2 * self.rng.random())\n\n                elif self.operators[op_idx] == 'coord_mix':\n                    # mix coordinates with the best known (x_opt) on a random subset\n                    mask = self.rng.random(n) < coord_frac\n                    candidate = m + step\n                    # replace masked coords with those of x_opt but with noise\n                    if x_opt is not None:\n                        blend = x_opt.copy()\n                        blend[mask] = blend[mask] + 0.05 * r * self.rng.standard_normal(np.count_nonzero(mask))\n                        candidate[mask] = blend[mask]\n                    step = candidate - m\n\n                elif self.operators[op_idx] == 'surrogate_step':\n                    # guided step along surrogate gradient if available\n                    g = fit_linear_surrogate(success_X, success_df)\n                    if g is not None:\n                        ng = np.linalg.norm(g) + 1e-12\n                        # use adaptive step length from recent successes\n                        step_dir = - (g / ng)\n                        lam_scale = (0.25 + 0.75 * self.rng.random())\n                        step = r * lam_scale * step_dir + 0.3 * step  # mix with rotational noise\n                    else:\n                        # fallback\n                        step = step * (0.85 + 0.3 * self.rng.random())\n\n                # propose candidate and clip\n                x = m + step\n                x = np.clip(x, lb, ub)\n                Xcand[i, :] = x\n                step_vecs[i, :] = step\n\n            # evaluate candidates (stop if budget runs out)\n            for i in range(batch):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = float(func(xi))\n                evals += 1\n                Fcand[i] = fi\n                # update global best\n                if fi < f_opt:\n                    f_opt = fi\n                    x_opt = xi.copy()\n                    last_improv = evals\n\n            # selection: pick best few candidates and compute weighted aggregate\n            mu = max(1, batch // 3)\n            idx_sorted = np.argsort(Fcand)\n            sel_idx = idx_sorted[:mu]\n            Xsel = Xcand[sel_idx]\n            Fsel = Fcand[sel_idx]\n\n            # robust recombination: use trimmed mean along each coordinate\n            # (drop extremes -> more robust than mean + median blend)\n            def trimmed_mean(arr, trim_frac=0.2):\n                k = max(1, int(np.floor(arr.shape[0] * trim_frac)))\n                out = np.zeros(arr.shape[1])\n                for d in range(arr.shape[1]):\n                    col = np.sort(arr[:, d])\n                    out[d] = np.mean(col[k:arr.shape[0]-k]) if arr.shape[0] - 2*k > 0 else np.mean(col)\n                return out\n\n            rec = trimmed_mean(Xsel, trim_frac=0.2)\n            # candidate center proposal mixes with momentum and a small random rotation\n            m_prop = np.clip(0.7 * m + 0.3 * rec + 0.4 * mom, lb, ub)\n\n            # evaluate proposed center (if budget allows)\n            f_prop = None\n            if evals < budget:\n                f_prop = float(func(m_prop))\n                evals += 1\n                if f_prop < f_center:\n                    accepted = True\n                else:\n                    # Metropolis-like acceptance with temperature T\n                    df = f_prop - f_center\n                    p_accept = np.exp(-max(0.0, df) / (T + 1e-12))\n                    accepted = (self.rng.random() < p_accept)\n                if accepted:\n                    # update momentum based on move\n                    mom = mu_m * mom + (1.0 - mu_m) * (m_prop - m)\n                    m = m_prop.copy()\n                    f_center = f_prop\n                    # register to pockets if good\n                    if len(pockets) < pocket_cap or f_center < max(p[1] for p in pockets):\n                        pockets.append((m.copy(), f_center))\n                        # keep pockets sorted best-first and trim\n                        pockets = sorted(pockets, key=lambda t: t[1])[:pocket_cap]\n                else:\n                    # decayed momentum if reject\n                    mom = mu_m * mom\n            else:\n                # no budget to evaluate m_prop: conservatively move partially towards rec\n                m = np.clip(0.95 * m + 0.05 * rec, lb, ub)\n                mom = mu_m * mom\n\n            # compute improvement statistics relative to f_center (conservative)\n            improvements = np.sum(Fcand < f_center - 1e-12)\n            psucc = improvements / max(1, batch)\n\n            # adaptive radius update: encourage exploitation if many improvements, expand if none\n            if psucc > 0.4:\n                r *= 0.88\n            elif psucc < 0.08:\n                r *= 1.18\n            r = float(np.clip(r, 1e-8, 8.0 * span))\n\n            # update sigma per-dimension using exponential second moment of selected steps (in global coords)\n            if len(sel_idx) > 0:\n                S = step_vecs[sel_idx] / (r + 1e-20)\n                # weighted second moment (weights favor better Fsel)\n                ranks = np.arange(1, len(Fsel) + 1)\n                w = (len(Fsel) + 1 - ranks) / (np.sum(len(Fsel) + 1 - ranks))\n                s2 = np.sum((S ** 2) * w[:, None], axis=0)\n                beta_s = 0.18\n                sigma = np.sqrt((1.0 - beta_s) * (sigma ** 2) + beta_s * (s2 + 1e-20))\n                sigma = np.clip(sigma, 1e-6, 1e3)\n\n            # update success buffer with top half of selections (relative to current center)\n            top_h = max(1, len(sel_idx) // 2)\n            for j in range(top_h):\n                idxj = sel_idx[j]\n                dx = (Xcand[idxj] - m).copy()\n                df = max(0.0, (f_center - Fcand[idxj]))\n                if len(success_X) >= buf_max:\n                    success_X.pop(0); success_df.pop(0)\n                success_X.append(dx)\n                success_df.append(df)\n\n            # update low-rank basis B via PCA-like update on successes (incremental)\n            if len(success_X) >= min(6, n):\n                M = np.vstack(success_X[-min(200, len(success_X)):])\n                M -= np.mean(M, axis=0, keepdims=True)\n                # covariance of steps\n                try:\n                    C = (M.T @ M) / max(1, M.shape[0] - 1)\n                    eigvals, eigvecs = np.linalg.eigh(C)\n                    order = np.argsort(eigvals)[::-1]\n                    newB = eigvecs[:, order[:k]]\n                    # align newB to B via Procrustes-ish SVD and convexly mix\n                    U, _, Vt = np.linalg.svd(newB.T @ B)\n                    align = newB @ (U @ Vt)\n                    mix = 0.22\n                    B = orthonormalize_columns((1.0 - mix) * B + mix * align)\n                except Exception:\n                    # small jitter if numerical issues\n                    B = orthonormalize_columns(B + 0.02 * self.rng.standard_normal(B.shape))\n\n            # update operator log-weights using average normalized reward per operator\n            op_rewards = np.zeros(self.k_ops)\n            op_seen = np.zeros(self.k_ops)\n            baseline = f_center\n            for i in range(batch):\n                oi = op_used[i]\n                if np.isfinite(Fcand[i]):\n                    improv = max(0.0, baseline - Fcand[i])\n                    step_len = np.linalg.norm(step_vecs[i]) + 1e-20\n                    # reward = improvement scaled by inverse step length and squashed\n                    rw = improv / (1.0 + 0.5 * step_len)\n                    op_rewards[oi] += rw\n                    op_seen[oi] += 1\n            alpha_bw = 0.20\n            for oi in range(self.k_ops):\n                if op_seen[oi] > 0:\n                    avg = op_rewards[oi] / op_seen[oi]\n                    # log-target: smoother transform to avoid overreaction\n                    target = np.log1p(avg)\n                    op_logw[oi] = (1.0 - alpha_bw) * op_logw[oi] + alpha_bw * target\n                    op_age[oi] = 0\n                else:\n                    op_age[oi] += 1\n                    # slowly decay unused ops\n                    op_logw[oi] *= (0.999 - 1e-6 * op_age[oi])\n\n            # periodic directional probe along surrogate gradient / top basis vector\n            gen += 1\n            if gen % line_probe_every == 0 and evals < budget:\n                g = fit_linear_surrogate(success_X, success_df)\n                dirs = []\n                if g is not None:\n                    ng = np.linalg.norm(g) + 1e-12\n                    dirs.append(-g / ng)\n                # also probe along principal low-rank direction\n                dirs.append(B[:, 0])\n                alphas = [0.0, 0.15, -0.15, 0.35, -0.35]\n                for d in dirs:\n                    for a in alphas:\n                        if evals >= budget:\n                            break\n                        probe = np.clip(m + r * a * d, lb, ub)\n                        fp = float(func(probe))\n                        evals += 1\n                        if fp < f_opt:\n                            f_opt = fp; x_opt = probe.copy(); last_improv = evals\n                        if fp < f_center:\n                            f_center = fp; m = probe.copy()\n                    if evals >= budget:\n                        break\n\n            # stagnation detection -> reinjection + pocket hop\n            if (evals - last_improv) > max(80, 6 * n):\n                # inflate radius and add jitter to basis and sigma\n                r *= 1.6\n                sigma = sigma * (1.0 + 0.4 * (self.rng.random(n) - 0.5))\n                B = orthonormalize_columns(B + 0.08 * self.rng.standard_normal(B.shape))\n                # try jumping to a random pocket if exists\n                if pockets and self.rng.random() < 0.7:\n                    pidx = self.rng.integers(len(pockets))\n                    m = pockets[pidx][0].copy()\n                    f_center = pockets[pidx][1]\n                # reset momentum a bit\n                mom *= 0.4\n                last_improv = evals\n\n            # gently cool temperature\n            T = max(1e-8, T * T_decay)\n\n            # enforce center bounds\n            m = np.clip(m, lb, ub)\n\n            # (loop ends when evals >= budget)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ARPM scored 0.317 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "3e29d308-29b0-4a2b-9d16-8e6068e82e1b", "operator": null, "metadata": {"aucs": [0.1333992704262149, 0.1693190386871276, 0.41091720924125785, 0.677581649533809, 0.3206949299433225, 0.45430185612104246, 0.25507875386900125, 0.3046970479931269, 0.2698182826878003, 0.17420318006015423]}, "task_prompt": ""}
{"id": "96cd9d9b-e0b2-4f99-a28e-d31efd0c4e14", "fitness": 0.1979863714463253, "name": "ARCS", "description": "The algorithm is an ensemble-based local search that generates candidates in a learned rotated local basis and selects among multiple mutation operators (gauss, cauchy, DE-like, low‑dimensional subspace, block coordinate permutation) via Thompson-style sampling from online posterior estimates. Spatial adaptation is multiplicative and data-driven: a trust radius r is adjusted exponentially from the observed success rate, per-dimension scales s are updated toward the MAD of selected local coordinates, and a PCA-derived rotation R is gently mixed with eigenvectors learned from a success buffer. Exploration/exploitation is controlled by a compact population (λ), trimmed-mean recombination toward promising candidates, occasional DE double-difference and subspace steps, probabilistic uphill acceptance, and periodic line probes along top eigenvectors. Robustness features include log-normalized operator rewards for bandit updates, an archive and bounded evaluations to respect the budget, and stagnation-driven inflation/jitter of r, s and R to reintroduce exploration.", "code": "import numpy as np\n\nclass ARCS:\n    \"\"\"\n    Adaptive Rotated Cyclic Search (ARCS)\n\n    One-line: A compact trust-region ensemble that mixes Gaussian/Cauchy/DE/subspace/coordinate operators,\n    selects operators via Thompson-style sampling, adapts radius multiplicatively from success rate,\n    updates per-dimension scales multiplicatively, and gently updates a learned PCA subspace with cyclic probes.\n\n    Main tunable parameters (exposed or in-code):\n      - budget, dim, seed\n      - lambda_: population size (compact, grows mildly with dim)\n      - init_r_factor: initial trust radius factor relative to mean(range)\n      - bandit priors: op_mu0, op_var0 (Thompson sampling normal priors for operators)\n      - p_de, F_de: DE-like operator probability and scale\n      - p_coord: coordinate-permute operator probability\n      - c_scale: multiplicative smoothing for per-dim scales\n      - alpha_sub: PCA mixing factor for subspace updates\n      - line_every: cadence for cyclic subspace probes\n      - buf_max: size of success buffer for surrogate/subspace fitting\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 init_r_factor=0.20,\n                 p_de=0.25, F_de=0.9,\n                 p_coord=0.15,\n                 c_scale=0.18,\n                 alpha_sub=0.28,\n                 line_every_factor=10):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # population size: compact but slightly different scaling than the reference\n        self.lambda_ = max(7, int(6 + np.ceil(np.sqrt(self.dim) * 1.2)))\n\n        # blocks (different heuristic: more, smaller blocks)\n        n_blocks = max(1, int(np.ceil(self.dim ** 0.5)))\n        sizes = [self.dim // n_blocks] * n_blocks\n        for i in range(self.dim % n_blocks):\n            sizes[i] += 1\n        self.blocks = []\n        idx = 0\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n        # algorithmic hyperparams (some exposed)\n        self.init_r_factor = float(init_r_factor)\n        self.p_de = float(p_de)\n        self.F_de = float(F_de)\n        self.p_coord = float(p_coord)\n        self.c_scale = float(c_scale)\n        self.alpha_sub = float(alpha_sub)\n        self.line_every_factor = int(max(4, line_every_factor))\n\n        # bandit priors for Thompson-style selection (normal)\n        self.operators = ['gauss', 'cauchy', 'de2', 'subspace', 'coordperm']\n        k = len(self.operators)\n        self.op_mu = np.zeros(k)        # posterior mean estimate of reward\n        self.op_var = np.ones(k) * 1.0  # posterior variance (un-normalized)\n        self.op_count = np.zeros(k, dtype=int)\n        self.op_prior_var = 2.0         # prior variance for Thompson sampling\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center uniform in bounds\n        m = self.rng.uniform(lb, ub)\n        # initial trust radius\n        r = float(self.init_r_factor * np.mean(ub - lb))\n        # per-dimension relative scales (multiplicative)\n        s = np.ones(n)\n\n        # PCA subspace (identity initial)\n        R = np.eye(n)\n\n        # small archive and success buffer (store absolute x and dx)\n        archive_X = []\n        archive_F = []\n        success_buf_dx = []\n        success_buf_df = []\n        buf_max = max(40, 8 * int(np.ceil(np.sqrt(n))))\n\n        # bookkeeping\n        evals = 0\n        gen = 0\n\n        # evaluate initial center\n        m = np.clip(m, lb, ub)\n        f_center = func(m)\n        evals += 1\n        archive_X.append(m.copy()); archive_F.append(f_center)\n        f_opt = float(f_center); x_opt = m.copy()\n        last_improv = evals\n\n        # derived params\n        lam_base = self.lambda_\n        line_every = max(8, int(self.line_every_factor * (n / 8.0 + 1.0)))\n\n        # helpers\n        def sample_cauchy_vec():\n            # independent standard Cauchy samples\n            u = self.rng.standard_normal(n)\n            v = self.rng.standard_normal(n)\n            # standard Cauchy = normal / normal\n            # avoid division by zero\n            v = np.where(np.abs(v) < 1e-12, 1e-12, v)\n            return u / v\n\n        def fit_linear_ridge(buf_dx, buf_df, reg=1e-3):\n            if len(buf_dx) < 4:\n                return None\n            A = np.vstack(buf_dx)\n            y = np.array(buf_df)\n            ATA = A.T @ A\n            try:\n                g = np.linalg.solve(ATA + reg * np.eye(n), A.T @ y)\n            except np.linalg.LinAlgError:\n                g = np.linalg.pinv(ATA + reg * np.eye(n)) @ (A.T @ y)\n            return g\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(lam_base, remaining)\n            Xcand = np.zeros((lam, n))\n            Fcand = np.full(lam, np.inf)\n            cand_op_idx = np.zeros(lam, dtype=int)\n            cand_dx = np.zeros((lam, n))  # x - m\n\n            # pre-generate randomness\n            Z = self.rng.standard_normal((lam, n))\n\n            for i in range(lam):\n                # Thompson-style selection: sample from Normal(op_mu, op_var + prior)\n                samples = self.rng.normal(self.op_mu, np.sqrt(self.op_var + self.op_prior_var))\n                op_idx = int(np.argmax(samples))  # pick argmax\n                op = self.operators[op_idx]\n                cand_op_idx[i] = op_idx\n\n                z = Z[i].copy()\n                # basic rotated-local vector\n                y_local = s * z\n                y_global = R.dot(y_local)\n                x = m + r * y_global\n\n                # operator specifics (different equations/choices)\n                if op == 'gauss':\n                    # conservative gaussian step: scale by 0.9\n                    x = m + r * 0.9 * y_global\n\n                elif op == 'cauchy':\n                    # heavy-tailed independent cauchy in rotated local basis\n                    c = sample_cauchy_vec()\n                    yg = R.dot(s * c)\n                    # cauchy steps amplified and clipped\n                    x = m + r * np.clip(1.6 * yg, -3.0, 3.0)\n\n                elif op == 'de2':\n                    # double-difference mutation from archive if enough points, else fallback\n                    if len(archive_X) >= 4 and self.rng.random() < self.p_de:\n                        idxs = self.rng.choice(len(archive_X), size=4, replace=False)\n                        a, b, c, d = [archive_X[j] for j in idxs]\n                        de_mut = self.F_de * ((a - b) + 0.5 * (c - d))\n                        x = x + de_mut\n                    else:\n                        # small gaussian perturbation fallback\n                        x = m + r * 0.6 * R.dot(s * self.rng.standard_normal(n))\n\n                elif op == 'subspace':\n                    # sample steps only in a random low-dimensional subspace (k_sub dims)\n                    k_sub = min(3, n)\n                    # pick random top-k columns of R or contiguous ones\n                    cols = self.rng.choice(n, size=k_sub, replace=False)\n                    coeffs = self.rng.laplace(0.0, 0.6, size=k_sub)\n                    step = np.sum([coeffs[jj] * R[:, cols[jj]] * s[cols[jj]] for jj in range(k_sub)], axis=0)\n                    x = m + r * step\n\n                elif op == 'coordperm':\n                    # small permutation within a randomly chosen block\n                    x = x.copy()\n                    if (len(self.blocks) > 0) and (self.rng.random() < self.p_coord):\n                        b = self.blocks[self.rng.integers(0, len(self.blocks))]\n                        if len(b) > 1:\n                            perm = self.rng.permutation(len(b))\n                            xb = x[b].copy()\n                            x[b] = xb[perm]\n\n                # clip into bounds\n                x = np.clip(x, lb, ub)\n                Xcand[i] = x\n                cand_dx[i] = (x - m)\n\n            # Evaluate candidates carefully under budget\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fcand[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy(); last_improv = evals\n\n            # Select top mu candidates (using robust trimmed mean)\n            mu = max(1, lam // 2)\n            idx_sorted = np.argsort(Fcand)\n            sel_idx = idx_sorted[:mu]\n            X_sel = Xcand[sel_idx]\n            F_sel = Fcand[sel_idx]\n\n            # recombination: trimmed mean (drop worst 20% among selected) and bias toward center\n            trim_k = max(0, int(np.floor(0.2 * mu)))\n            if mu - trim_k > 0:\n                good_idx = sel_idx[:(mu - trim_k)]\n                recomb = np.mean(Xcand[good_idx], axis=0)\n            else:\n                recomb = np.mean(X_sel, axis=0)\n            # new candidate center is interpolated toward recomb with a stronger step than baseline\n            alpha_move = 0.62\n            m_new = np.clip((1.0 - alpha_move) * m + alpha_move * recomb, lb, ub)\n\n            # Evaluate potential center (if budget allows) using conservative check\n            accepted_center_move = False\n            if evals < budget:\n                fm_new = func(m_new)\n                evals += 1\n                archive_X.append(m_new.copy()); archive_F.append(fm_new)\n                if fm_new < f_center:\n                    accepted_center_move = True\n                    f_center = fm_new\n                    m = m_new.copy()\n                else:\n                    # probabilistic uphill acceptance; temperature decays cyclically\n                    T = 0.08 * (1.0 + 0.5 * np.sin(gen / max(1.0, 40.0)))\n                    delta = fm_new - f_center\n                    if self.rng.random() < np.exp(-delta / (T + 1e-12)):\n                        accepted_center_move = True\n                        f_center = fm_new\n                        m = m_new.copy()\n                    else:\n                        # weak move toward m_new occasionally\n                        if self.rng.random() < 0.07:\n                            m = np.clip(0.9 * m + 0.1 * m_new, lb, ub)\n            else:\n                # no budget for evaluating m_new: conservative adoption if mean selected better\n                mean_sel = np.mean(F_sel)\n                if mean_sel < f_center:\n                    m = m_new.copy()\n                    accepted_center_move = True\n\n            # Compute successes relative to center snapshot before adaptation (conservative)\n            improvements = np.sum(Fcand < f_center - 1e-12)\n            psucc = improvements / max(1, lam)\n\n            # multiplicative trust-radius adaptation (different equation)\n            target_ps = 0.20\n            # exponential update: if psucc above target shrink, below expand\n            r *= float(np.exp(0.35 * (target_ps - psucc)))\n            r = float(np.clip(r, 1e-8, 6.0 * np.mean(ub - lb)))\n\n            # update per-dimension scales multiplicatively using observed local variance\n            if r > 0 and len(sel_idx) > 0:\n                # compute rotated local coordinates for selected (relative to possibly-updated m)\n                Ylocal_sel = (R.T @ ((X_sel - m).T)).T / (r + 1e-20)\n                # robust scale estimate: median absolute deviation per dim\n                mad = np.median(np.abs(Ylocal_sel - np.median(Ylocal_sel, axis=0, keepdims=True)), axis=0) + 1e-12\n                # multiplicative update towards observed mad (log-space smoothing)\n                beta = self.c_scale\n                s = s * np.exp(beta * (np.log(mad + 1e-12) - np.log(s + 1e-12)))\n                s = np.clip(s, 1e-6, 1e3)\n\n            # update success buffer with top half of selected (store dx relative to current m)\n            h = max(1, (mu // 2))\n            for jj in range(h):\n                idx_j = sel_idx[jj]\n                dx = (Xcand[idx_j] - m).copy()\n                df = max(0.0, (f_center - Fcand[idx_j]))\n                if len(success_buf_dx) >= buf_max:\n                    success_buf_dx.pop(0); success_buf_df.pop(0)\n                success_buf_dx.append(dx)\n                success_buf_df.append(df)\n\n            # Update PCA subspace from success buffer with alpha_sub mixing (different gentle mixing)\n            if len(success_buf_dx) >= min(6, n):\n                M = np.vstack(success_buf_dx)\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                try:\n                    C = (Mc.T @ Mc) / max(1, Mc.shape[0] - 1)\n                    eigvals, eigvecs = np.linalg.eigh(C)\n                    order = np.argsort(eigvals)[::-1]\n                    k_sub = min(4, n)\n                    top = eigvecs[:, order[:k_sub]]\n                    # align subspace with current R[:,:k_sub] via SVD and convex blend\n                    A = R[:, :k_sub]\n                    Msmall = top.T @ A\n                    U, _, Vt = np.linalg.svd(Msmall, full_matrices=False)\n                    Q = U @ Vt\n                    newblock = top @ Q\n                    R[:, :k_sub] = (1.0 - self.alpha_sub) * A + self.alpha_sub * newblock\n                    # orthonormalize whole R\n                    Qr, _ = np.linalg.qr(R)\n                    R = Qr[:, :n]\n                except np.linalg.LinAlgError:\n                    # mild random perturbation and orthonormalize\n                    R = R + 0.05 * self.rng.standard_normal(R.shape)\n                    Qr, _ = np.linalg.qr(R)\n                    R = Qr[:, :n]\n\n            # Update operator posterior estimates (online mean/var) using the candidate batch rewards\n            # reward = improvement normalized by step norm to prefer efficient small steps\n            op_rewards = np.zeros(len(self.operators))\n            op_seen = np.zeros(len(self.operators), dtype=int)\n            # use baseline as current f_center\n            baseline = f_center\n            for i in range(lam):\n                opi = cand_op_idx[i]\n                fi = Fcand[i]\n                if np.isfinite(fi):\n                    improv = max(0.0, baseline - fi)\n                    step_norm = np.linalg.norm(cand_dx[i]) + 1e-20\n                    reward = improv / (1.0 + 0.6 * step_norm)\n                else:\n                    reward = 0.0\n                op_rewards[opi] += reward\n                op_seen[opi] += 1\n\n            for oi in range(len(self.operators)):\n                if op_seen[oi] > 0:\n                    avg_r = op_rewards[oi] / op_seen[oi]\n                    # online update of mean and variance (Welford-ish)\n                    prev_mu = self.op_mu[oi]\n                    prev_var = self.op_var[oi]\n                    c = op_seen[oi]\n                    # simple exponential update to keep memory\n                    tau = 0.22\n                    target = np.log1p(avg_r + 1e-12)\n                    self.op_mu[oi] = (1.0 - tau) * prev_mu + tau * target\n                    # update a pseudo-variance toward squared deviation\n                    dev = (target - prev_mu) ** 2\n                    self.op_var[oi] = (1.0 - tau) * prev_var + tau * dev + 1e-6\n                    self.op_count[oi] += int(op_seen[oi])\n\n            # cyclic subspace probes (periodically perform a small set of line probes along top eigenvector)\n            gen += 1\n            if (gen % line_every == 0) and (evals < budget):\n                d = R[:, 0]\n                # small symmetric probes with decreasing amplitudes\n                amps = [0.0, 0.15, -0.15, 0.35, -0.35]\n                for a in amps:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + r * a * d, lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = float(fp); x_opt = probe.copy(); last_improv = evals\n                    if fp < f_center:\n                        f_center = fp; m = probe.copy()\n\n            # stagnation detection and larger perturbation to reintroduce exploration\n            if (evals - last_improv) > max(80, 10 * n):\n                # inflate radius, jitter scales and subspace\n                r *= 1.7\n                s = s * (1.0 + 0.5 * (self.rng.random(n) - 0.5))\n                R = R + 0.08 * self.rng.standard_normal(R.shape)\n                Qr, _ = np.linalg.qr(R)\n                R = Qr[:, :n]\n                # reshuffle one element between two random blocks\n                if len(self.blocks) > 1 and self.rng.random() < 0.6:\n                    b1, b2 = self.rng.choice(len(self.blocks), size=2, replace=False)\n                    if self.blocks[b1] and self.blocks[b2]:\n                        i1 = self.rng.choice(self.blocks[b1])\n                        i2 = self.rng.choice(self.blocks[b2])\n                        try:\n                            self.blocks[b1].remove(i1); self.blocks[b1].append(i2)\n                            self.blocks[b2].remove(i2); self.blocks[b2].append(i1)\n                        except ValueError:\n                            pass\n                last_improv = evals\n\n            # trim archive to avoid memory blowup\n            if len(archive_X) > 5000:\n                archive_X = archive_X[-5000:]\n                archive_F = archive_F[-5000:]\n\n            # keep center in bounds\n            m = np.clip(m, lb, ub)\n\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ARCS scored 0.198 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "3e29d308-29b0-4a2b-9d16-8e6068e82e1b", "operator": null, "metadata": {"aucs": [0.05247215399561844, 0.14659718478196448, 0.26661348129998197, 0.24063254142297752, 0.20386030618262385, 0.2942696746994803, 0.24124082123158108, 0.22044013791888384, 0.18330455813858437, 0.13043285479155686]}, "task_prompt": ""}
{"id": "ebc7e118-5ee9-46ce-818d-a407a6f5b852", "fitness": 0.31605030582307403, "name": "ARLEO", "description": "The algorithm maintains a single search center m with a global step-size sigma, per-dimension local scales s, and an orthonormal rotation Q to express anisotropic searches in a rotated coordinate system, with population size lambda growing ~sqrt(dim) and a bounded archive for past evaluations. It samples a compact ensemble of candidates from a diverse operator set (gauss, levy, de, orthog, surrogate) and selects operators via a simple Thompson-style bandit (Gaussian draws from per-operator reward estimates) to balance exploration and exploitation. Adaptation is driven by a target success rate (target_psucc≈0.2) that multiplicatively adjusts sigma, second-moment updates of s in the rotated local frame, gentle Givens rotations and success-biased nudges to Q, plus occasional power-iteration to extract a dominant success direction for 1D line probes. Candidate recombination (weighted mean + median), Metropolis-style acceptance with annealing, archive- and surrogate-guided DE-like moves, and stagnation restarts provide robustness while strictly respecting the allowed budget.", "code": "import numpy as np\n\nclass ARLEO:\n    \"\"\"\n    Adaptive Rotated Lévy Ensemble with Orthogonal Transform (ARLEO)\n\n    Key ideas:\n      - Maintain a single center m, global scale sigma, per-dimension scales s and an orthonormal rotation Q.\n      - Generate a compact ensemble of candidates each generation from multiple diverse operators:\n          'gauss'   : local Gaussian anisotropic step\n          'levy'    : heavy-tailed (Cauchy-like) jump with anisotropy\n          'de'      : differential jump from small archive (DE-style)\n          'orthog'  : rotate a coordinate pair (Givens) and take a directed coordinate step\n          'surrogate': short guided step from a linear surrogate fit on successful deltas\n      - Operator selection by Gaussian-Thompson sampling over per-operator mean rewards.\n      - Update Q gently via random Givens rotations seeded from successful directions; periodically extract top direction via power iteration of success covariance for 1D probes.\n      - Adaptive sigma via success-rate feedback toward a target success rate, and per-dim scales s updated by second-moment statistics in rotated-local coords.\n      - All evaluations strictly respect the provided budget.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # population size: compact, grows sublinearly with dim\n        self.lambda_ = max(6, int(4 + 2 * np.ceil(np.sqrt(self.dim))))\n        # small archive / buffers\n        self.archive_limit = 2000\n\n        # Operator list\n        self.operators = ['gauss', 'levy', 'de', 'orthog', 'surrogate']\n        self.k_ops = len(self.operators)\n\n        # Thompson-style bandit parameters (per-operator reward stats)\n        self.op_count = np.zeros(self.k_ops, dtype=int)\n        self.op_sum_reward = np.zeros(self.k_ops)\n        self.op_mean = np.zeros(self.k_ops)\n        # small prior variance to encourage early exploration\n        self.op_prior_var = np.full(self.k_ops, 1.0)\n\n        # default hyperparameters (somewhat robust defaults)\n        self.target_psucc = 0.20\n        self.sigma_init = 0.20  # fraction of search range\n        self.s_min = 1e-6\n        self.s_max = 1e3\n        self.c_s = 0.18  # smoothing for per-dim second moment\n        self.de_prob = 0.35\n        self.de_F = 0.9\n        self.levy_scale = 2.0\n        self.surrogate_reg = 1e-3\n        self.power_iter_steps = 6\n        self.line_probe_alphas = [0.0, 0.15, -0.15, 0.35, -0.35, 0.7, -0.7]\n        self.line_probe_every = max(10, int(6 * (self.dim / 12 + 1)))\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (support scalar bounds same as arrays)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initialize center in bounds\n        m = self.rng.uniform(lb, ub)\n        # initial sigma (absolute)\n        sigma = float(self.sigma_init * np.mean(ub - lb))\n        # per-dim local scales\n        s = np.ones(n)\n\n        # orthonormal rotation matrix Q\n        Q = np.eye(n)\n\n        # small archive of evaluated points\n        archive_X = []\n        archive_F = []\n\n        # success buffer: store dx (global coords) and df (improvement)\n        success_X = []\n        success_df = []\n        buf_max = max(50, 8 * int(np.ceil(np.sqrt(n))))\n\n        evals = 0\n        # evaluate initial center\n        m = np.clip(m, lb, ub)\n        f_center = func(m)\n        evals += 1\n        archive_X.append(m.copy()); archive_F.append(f_center)\n        f_best = float(f_center); x_best = m.copy()\n        last_improv_evals = evals\n\n        gen = 0\n\n        # helper: Thompson sampling selection for operator index\n        def select_operator():\n            # draw samples from Normal(mean, var_prior/ (1 + count) ) to encourage exploration\n            samples = np.zeros(self.k_ops)\n            for i in range(self.k_ops):\n                count = max(1, self.op_count[i])\n                var = (self.op_prior_var[i] / count) + 1e-6\n                samples[i] = self.rng.normal(loc=self.op_mean[i], scale=np.sqrt(var))\n            # pick argmax sample\n            return int(np.argmax(samples))\n\n        # helper: fit a linear surrogate (gradient) to predict df from dx in global coords\n        def fit_linear_surrogate(buf_X, buf_df):\n            if len(buf_X) < 4:\n                return None\n            A = np.vstack(buf_X)  # m x n\n            y = np.array(buf_df)\n            ATA = A.T @ A\n            reg = self.surrogate_reg * np.eye(n)\n            try:\n                g = np.linalg.solve(ATA + reg, A.T @ y)\n            except np.linalg.LinAlgError:\n                g = np.linalg.pinv(ATA + reg) @ (A.T @ y)\n            return g  # gradient approx of improvement vs dx: df ≈ g^T dx\n\n        # helper: small Givens rotation applied to Q for two indices\n        def apply_random_givens(Qmat, strength=0.12):\n            Qnew = Qmat.copy()\n            i, j = self.rng.choice(n, size=2, replace=False)\n            theta = (self.rng.uniform(-1.0, 1.0) * strength * np.pi)\n            c = np.cos(theta); s_g = np.sin(theta)\n            # apply on columns i and j (right-multiply)\n            Qi = Qnew[:, i].copy()\n            Qj = Qnew[:, j].copy()\n            Qnew[:, i] = c * Qi - s_g * Qj\n            Qnew[:, j] = s_g * Qi + c * Qj\n            # re-orthonormalize a few columns via local QR to mitigate drift\n            try:\n                Qnew, _ = np.linalg.qr(Qnew)\n            except np.linalg.LinAlgError:\n                Qnew = Qmat + 1e-3 * self.rng.standard_normal(Qmat.shape)\n                Qnew, _ = np.linalg.qr(Qnew)\n            return Qnew\n\n        # helper: compute top eigenvector via power iteration on success covariance\n        def top_success_direction(buf_X):\n            if len(buf_X) < 3:\n                return None\n            M = np.vstack(buf_X)\n            M = M - np.mean(M, axis=0, keepdims=True)\n            # multiply covariance by vector: C v = (M^T (M v)) / (m-1)\n            v = self.rng.standard_normal(n)\n            v = v / (np.linalg.norm(v) + 1e-12)\n            for _ in range(self.power_iter_steps):\n                u = M @ v  # m-vector\n                v_new = M.T @ u\n                norm_v = np.linalg.norm(v_new) + 1e-12\n                v = v_new / norm_v\n            return v\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            # prepare storage\n            Xcand = np.zeros((lam, n))\n            Fcand = np.full(lam, np.inf)\n            cand_op = np.zeros(lam, dtype=int)\n            cand_step = np.zeros((lam, n))  # x - m\n\n            # pre-generate standard normals\n            Z = self.rng.standard_normal((lam, n))\n\n            # baseline for reward calculation uses current center f_center snapshot\n            baseline = f_center\n\n            # generate candidates\n            for i in range(lam):\n                op_idx = select_operator()\n                cand_op[i] = op_idx\n                op = self.operators[op_idx]\n\n                z = Z[i].copy()\n\n                # local anisotropic sample in rotated frame\n                y_local = s * z\n                y_global = (Q @ y_local)\n                x = m + sigma * y_global  # default Gaussian-like step\n\n                if op == 'gauss':\n                    # leave as-is, but add small randomized scaling\n                    x = m + sigma * (0.9 + 0.2 * self.rng.random()) * y_global\n\n                elif op == 'levy':\n                    # heavy-tailed jump: Cauchy-like scaled by levy_scale\n                    cauchy = self.rng.standard_cauchy(size=n)\n                    # blend anisotropy with cauchy direction\n                    levy_vec = 0.6 * y_global + 0.4 * (Q @ (s * cauchy))\n                    x = m + (sigma * self.levy_scale) * levy_vec\n\n                elif op == 'de':\n                    # difference-of-archive vectors\n                    if (len(archive_X) >= 2) and (self.rng.random() < self.de_prob):\n                        i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                        de_mut = self.de_F * (archive_X[i1] - archive_X[i2])\n                        x = x + de_mut\n                    else:\n                        # fallback to gaussian perturbation\n                        x = m + sigma * 0.8 * y_global\n\n                elif op == 'orthog':\n                    # take coordinate step aligned with a random column of current Q (rotated coordinate)\n                    idx_col = self.rng.integers(0, n)\n                    dir_col = Q[:, idx_col]\n                    step_scale = sigma * (0.6 + 1.4 * self.rng.random())\n                    # optionally perturb Q before stepping (local rotation)\n                    if self.rng.random() < 0.25:\n                        Q = apply_random_givens(Q, strength=0.09)\n                    x = m + step_scale * dir_col\n\n                elif op == 'surrogate':\n                    # guided step from linear surrogate g: df ≈ g^T dx => descent direction -g\n                    if len(success_X) >= 6:\n                        g = fit_linear_surrogate(success_X, success_df)\n                        if g is not None:\n                            ng = np.linalg.norm(g) + 1e-12\n                            # compute a normalized guided step; randomize step size\n                            step_len = sigma * (0.2 + 1.2 * self.rng.random())\n                            x = m - (step_len * (g / ng))\n                        else:\n                            x = m + sigma * y_global\n                    else:\n                        x = m + sigma * y_global\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n                Xcand[i] = x\n                cand_step[i] = x - m\n\n            # Evaluate candidates (respect budget)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fcand[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if fi < f_best:\n                    f_best = float(fi); x_best = xi.copy(); last_improv_evals = evals\n\n            # selection: pick champion via fitness with distance penalization (prefer close improvements)\n            # compute penalized score = f + lambda_pen * (||dx|| / (sigma + eps))\n            lam_eff = np.sum(np.isfinite(Fcand))\n            if lam_eff == 0:\n                break\n            lam_eff = int(lam_eff)\n            dist_norms = np.linalg.norm(cand_step, axis=1)\n            lambda_pen = 0.05  # small penalty on distance\n            penalized_scores = Fcand + lambda_pen * (dist_norms / (sigma + 1e-12))\n            champion_idx = int(np.argmin(penalized_scores))\n\n            # propose center move via robust recombination of top half of candidates (median+mean blend)\n            mu = max(1, lam // 2)\n            sorted_idx = np.argsort(Fcand)\n            chosen_idx = sorted_idx[:mu]\n            Xsel = Xcand[chosen_idx]\n            weights = np.linspace(mu, 1, mu)  # stronger weighting to better candidates\n            weights = weights / np.sum(weights)\n            wmean = np.sum((weights[:, None] * Xsel), axis=0)\n            median = np.median(Xsel, axis=0)\n            m_proposed = 0.6 * wmean + 0.4 * median\n            m_proposed = np.clip(m_proposed, lb, ub)\n\n            # acceptance: accept if better, else Metropolis style with annealing temperature\n            accepted = False\n            if evals < budget:\n                f_prop = func(m_proposed)\n                evals += 1\n                archive_X.append(m_proposed.copy()); archive_F.append(f_prop)\n                if f_prop < f_center:\n                    accepted = True\n                    f_center = f_prop\n                    m = m_proposed.copy()\n                    last_improv_evals = evals\n                else:\n                    delta = f_prop - f_center\n                    # temperature decays with generations and current sigma\n                    T = 0.08 * (1.0 + 0.5 * (sigma / (np.mean(ub - lb) + 1e-12))) * np.exp(-gen / 1200.0)\n                    if self.rng.random() < np.exp(-delta / (T + 1e-12)):\n                        accepted = True\n                        f_center = f_prop\n                        m = m_proposed.copy()\n                    else:\n                        # occasional partial move toward m_proposed to keep memory\n                        if self.rng.random() < 0.06:\n                            m = 0.88 * m + 0.12 * m_proposed\n            else:\n                # if no budget, conservatively accept if penalized champion beats center\n                if penalized_scores[champion_idx] < f_center:\n                    m = Xcand[champion_idx].copy()\n                    f_center = Fcand[champion_idx]\n                    accepted = True\n\n            # compute psucc as fraction of candidates that improved beyond baseline (center before potential update)\n            improvements = np.sum(Fcand < baseline - 1e-12)\n            psucc = improvements / max(1, lam)\n\n            # adapt sigma toward target success rate\n            adapt_factor = 1.0\n            if psucc > (self.target_psucc + 0.05):\n                adapt_factor = 0.85\n            elif psucc < (self.target_psucc - 0.08):\n                adapt_factor = 1.15\n            # gentle adaptation scaled by log of sigma to avoid runaway\n            sigma *= adapt_factor\n            sigma = float(np.clip(sigma, 1e-8, 6.0 * np.mean(ub - lb)))\n\n            # update per-dimension scales s using weighted second moments in rotated-local coords\n            try:\n                # use selected set Xsel (mu points) to compute local variances\n                Ylocal = (Q.T @ ((Xsel - m).T)).T / (sigma + 1e-20)\n                y2 = np.mean(Ylocal ** 2, axis=0)\n                s2 = (1.0 - self.c_s) * (s ** 2) + self.c_s * (y2 + 1e-20)\n                s = np.sqrt(s2)\n                s = np.clip(s, self.s_min, self.s_max)\n            except Exception:\n                pass\n\n            # update success buffer using top improving candidates\n            # store dx = x - m (global coords) and df = max(0, baseline - f)\n            h = max(1, mu // 2)\n            for j in range(h):\n                idxj = chosen_idx[j]\n                dx = (Xcand[idxj] - m).copy()\n                df = max(0.0, baseline - Fcand[idxj])\n                if len(success_X) >= buf_max:\n                    success_X.pop(0); success_df.pop(0)\n                success_X.append(dx)\n                success_df.append(df)\n\n            # update rotation Q mildly using Givens seeded from successes\n            if len(success_X) >= 4:\n                # pick a random successful dx (biased towards larger df)\n                df_arr = np.array(success_df)\n                if df_arr.sum() > 0:\n                    probs = (df_arr + 1e-12) / (df_arr.sum() + 1e-12 * len(df_arr))\n                else:\n                    probs = np.ones(len(df_arr)) / len(df_arr)\n                idx_pick = int(self.rng.choice(len(success_X), p=probs))\n                dx_pick = success_X[idx_pick]\n                # form a Givens rotation trying to align a random Q column with dx_pick\n                if np.linalg.norm(dx_pick) > 1e-12:\n                    # compute a desired unit direction in global coords and rotate Q towards it\n                    desired = dx_pick / (np.linalg.norm(dx_pick) + 1e-12)\n                    # pick a column to nudge\n                    col_idx = self.rng.integers(0, n)\n                    current_col = Q[:, col_idx]\n                    # compose small rotation that moves current_col toward desired\n                    # create orthonormal basis {current_col, orth} and rotate in their plane\n                    orth = desired - (current_col @ desired) * current_col\n                    if np.linalg.norm(orth) > 1e-12:\n                        orth = orth / np.linalg.norm(orth)\n                        angle = 0.08 * (1.0 + 4.0 * (success_df[idx_pick] / (1.0 + success_df[idx_pick])))\n                        c = np.cos(angle); s_g = np.sin(angle)\n                        new_col = c * current_col + s_g * orth\n                        Q[:, col_idx] = new_col\n                        # re-orthonormalize Q\n                        try:\n                            Q, _ = np.linalg.qr(Q)\n                        except np.linalg.LinAlgError:\n                            Q = Q + 0.02 * self.rng.standard_normal(Q.shape)\n                            Q, _ = np.linalg.qr(Q)\n\n            # update per-operator reward statistics using candidate results\n            # reward = improvement / (1 + distance / (sigma + eps))\n            for i in range(lam):\n                opi = cand_op[i]\n                if not np.isfinite(Fcand[i]):\n                    continue\n                improv = max(0.0, baseline - Fcand[i])\n                step_norm = np.linalg.norm(cand_step[i]) + 1e-12\n                reward = improv / (1.0 + (step_norm / (sigma + 1e-12)))\n                # incremental mean update\n                self.op_count[opi] += 1\n                c = self.op_count[opi]\n                prev_mean = self.op_mean[opi]\n                new_mean = prev_mean + (reward - prev_mean) / c\n                self.op_mean[opi] = new_mean\n                self.op_sum_reward[opi] += reward\n                # keep a modest prior variance decay\n                self.op_prior_var[opi] = max(1e-4, 0.9 * self.op_prior_var[opi] + 0.1 * max(0.1, np.var([reward, prev_mean])))\n\n            # periodic 1D probes along top success direction (power iteration)\n            gen += 1\n            if (gen % self.line_probe_every == 0) and (evals < budget):\n                d = top_success_direction(success_X)\n                if d is not None:\n                    for a in self.line_probe_alphas:\n                        if evals >= budget:\n                            break\n                        probe = np.clip(m + sigma * a * d, lb, ub)\n                        fp = func(probe)\n                        evals += 1\n                        archive_X.append(probe.copy()); archive_F.append(fp)\n                        if fp < f_best:\n                            f_best = float(fp); x_best = probe.copy(); last_improv_evals = evals\n                        if fp < f_center:\n                            f_center = fp; m = probe.copy()\n\n            # stagnation handling: if no improvement for long, perform larger randomized restart-ish tweaks\n            if (evals - last_improv_evals) > max(80, 12 * n):\n                # inflate sigma, randomize some scales, jitter Q moderately\n                sigma *= 1.6\n                s = s * (1.0 + 0.5 * (self.rng.random(n) - 0.5))\n                Q = Q + 0.06 * self.rng.standard_normal(Q.shape)\n                try:\n                    Q, _ = np.linalg.qr(Q)\n                except Exception:\n                    Q = np.eye(n)\n                # mix in a random candidate into center occasionally\n                if len(archive_X) > 0 and self.rng.random() < 0.35:\n                    candidate = archive_X[self.rng.integers(0, len(archive_X))]\n                    m = 0.6 * m + 0.4 * candidate\n                    m = np.clip(m, lb, ub)\n                    f_center = func(m); evals += 1\n                    archive_X.append(m.copy()); archive_F.append(f_center)\n                    if f_center < f_best:\n                        f_best = float(f_center); x_best = m.copy(); last_improv_evals = evals\n                last_improv_evals = evals\n\n            # trim archive to avoid memory blowup\n            if len(archive_X) > self.archive_limit:\n                archive_X = archive_X[-self.archive_limit:]\n                archive_F = archive_F[-self.archive_limit:]\n\n            # ensure center in bounds\n            m = np.clip(m, lb, ub)\n\n            # safety: break if budget exhausted\n            if evals >= budget:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ARLEO scored 0.316 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "3e29d308-29b0-4a2b-9d16-8e6068e82e1b", "operator": null, "metadata": {"aucs": [0.15939654895614308, 0.6786974242089581, 0.2915821128188243, 0.4377104735512032, 0.28369944108000444, 0.3224954571080557, 0.2722419017093646, 0.2782108466974116, 0.25395247046451574, 0.18251638163625927]}, "task_prompt": ""}
{"id": "58ae67d9-6cc5-4035-9b18-bed4d5f92c01", "fitness": 0.23510080293782085, "name": "REMAS", "description": "REMAS maintains a compact rotated ensemble around a trust center m: it samples anisotropic perturbations using a rotation matrix R and per-dimension scales s (EMA-smoothed) with a small initial trust radius r = 0.15 * mean(range), storing recent trials in an archive and a success buffer for learning. It uses a small operator portfolio (gaussian, heavy-tailed cauchy, DE-style differences, ridge-surrogate steps, and block-permutations) and selects operators by Thompson-sampling on decayed running reward statistics, while the surrogate is a ridge linear model (λ=1e-2) fit on successful dx→df pairs to propose aggressive guided steps. Promising candidates are recombined into a trial center via a robust blend of fitness-weighted mean and median, accepted by a Metropolis-like rule with a decaying temperature, and the trust radius is adapted by simple psucc thresholds (shrink at <0.20, expand at >0.55). A light PCA on recent successes (k_sub up to 5, mixing α=0.35) updates R to bias search directions, periodic 1D probes, and stronger stagnation resets (r doubled, R randomized, operator-history decay) keep exploration alive while bounded buffers and archive trimming control memory.", "code": "import numpy as np\n\nclass REMAS:\n    \"\"\"\n    Rotated Ensemble Metropolis Adaptive Search (REMAS)\n\n    Main idea (one-liner):\n      Use a small rotated ensemble of anisotropic samples, choose operators via Thompson-sampling\n      on per-operator reward posteriors, recombine promising samples into a trial center, accept\n      with a decaying Metropolis rule, adapt trust-radius and per-dimension scales, and update a\n      light PCA subspace from recent successes.\n\n    Key (tunable) parameters and differences to the provided ASES:\n      - lambda_: population = max(8, 6 + ceil(2*log(dim+1))) (different scaling -> more compact in very high dims)\n      - init_r_factor = 0.15 (smaller than ASES' 0.25)\n      - operator portfolio: ['gauss','cauchy','de','surrogate','permute'] (Cauchy instead of Student-t)\n      - operator selector: Thompson-sampling on estimated normal reward posteriors (different from softmax bandit)\n      - per-dim scale smoothing: EMA on absolute rotated coords with c_s = 0.15 (instead of 2nd-moment)\n      - radius adaptation: shrink when too many failures (psucc < 0.2 -> r *= 0.9), expand for many successes (psucc > 0.55 -> r *= 1.08)\n      - surrogate: ridge linear model with stronger reg (1e-2) and step length scaling different\n      - PCA mixing alpha_sub = 0.35 (more aggressive than ASES)\n      - acceptance: Metropolis-like with tau decaying with generation\n      - stagnation resets are stronger (r doubled, full randomize of R with orthonormalization)\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # population size: different compact formula (tends to be slightly larger at low dims)\n        self.lambda_ = max(8, int(6 + np.ceil(2.0 * np.log(self.dim + 1.0))))\n        # create coarse blocks for small permutations\n        n_blocks = max(1, int(np.ceil(self.dim ** 0.55)))\n        sizes = [self.dim // n_blocks] * n_blocks\n        for i in range(self.dim % n_blocks):\n            sizes[i] += 1\n        self.blocks = []\n        idx = 0\n        for s in sizes:\n            self.blocks.append(list(range(idx, idx + s)))\n            idx += s\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (respect func.bounds if present)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial center uniform in bounds\n        m = self.rng.uniform(lb, ub)\n        # initial trust radius (different)\n        init_r_factor = 0.15\n        r = float(init_r_factor * np.mean(ub - lb))\n        # per-dimension relative scales\n        s = np.ones(n)\n\n        # rotated subspace (identity initially)\n        R = np.eye(n)\n\n        # small archive and success buffer\n        archive_X = []\n        archive_F = []\n        success_buf_X = []   # dx in global coords\n        success_buf_df = []\n\n        # operators and Thompson-sampling stats (mean, var, count)\n        operators = ['gauss', 'cauchy', 'de', 'surrogate', 'permute']\n        k_ops = len(operators)\n        op_count = np.zeros(k_ops, dtype=int)\n        op_mean = np.zeros(k_ops, dtype=float)     # running mean of rewards\n        op_M2 = np.zeros(k_ops, dtype=float)       # running second moment accumulator for variance\n\n        # operator hyperparameters (different choices)\n        p_de = 0.4\n        F_de = 0.6\n        cauchy_scale = 1.2\n        permute_prob = 0.2\n        lin_reg_lambda = 1e-2  # stronger regularization than ASES\n        buf_max = max(60, 12 * int(np.ceil(np.sqrt(n))))  # larger buffer\n        k_sub = min(5, n)\n        line_search_every = max(10, int(5 * (n / 10.0 + 1.0)))\n        c_s = 0.15  # EMA smoothing for per-dim scales\n        gen = 0\n\n        evals = 0\n        # evaluate initial center\n        m = np.clip(m, lb, ub)\n        f_center = func(m)\n        evals += 1\n        archive_X.append(m.copy()); archive_F.append(f_center)\n        f_opt = float(f_center); x_opt = m.copy()\n        last_improv = evals\n\n        # helpers\n        def safe_clip(x):\n            return np.clip(x, lb, ub)\n\n        def fit_ridge(buf_X, buf_df):\n            if len(buf_X) < 4:\n                return None\n            A = np.vstack(buf_X)  # rows are dx\n            y = np.array(buf_df)\n            ATA = A.T @ A\n            reg = lin_reg_lambda * np.eye(n)\n            try:\n                w = np.linalg.solve(ATA + reg, A.T @ y)\n            except np.linalg.LinAlgError:\n                w = np.linalg.pinv(ATA + reg) @ (A.T @ y)\n            return w\n\n        def sample_operator_thompson():\n            # sample reward estimate from normal posterior approx: mean +/- sqrt(var/ count)\n            # for unseen ops choose randomly with small prob\n            unseen = np.where(op_count == 0)[0]\n            if len(unseen) > 0 and self.rng.random() < 0.15:\n                return int(self.rng.choice(unseen))\n            # build samples\n            samples = np.zeros(k_ops)\n            for i in range(k_ops):\n                if op_count[i] <= 1:\n                    # add jitter for poorly observed ops\n                    samples[i] = op_mean[i] + 0.1 * self.rng.standard_normal()\n                else:\n                    var = max(1e-8, op_M2[i] / (op_count[i] - 1))\n                    # posterior variance approx var / count\n                    post_std = np.sqrt(var / max(1, op_count[i]))\n                    samples[i] = op_mean[i] + post_std * self.rng.standard_normal()\n            # pick operator with highest sampled reward\n            return int(np.argmax(samples))\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            if lam <= 0:\n                break\n\n            # snapshot baseline f_center for reward computations (before any center update this gen)\n            f_center_pre = float(f_center)\n\n            Xcand = np.zeros((lam, n))\n            Fcand = np.full(lam, np.inf)\n            cand_op_idx = np.zeros(lam, dtype=int)\n            cand_step = np.zeros((lam, n))\n\n            # pre-generate normals for efficiency\n            Z = self.rng.standard_normal((lam, n))\n\n            for i in range(lam):\n                op_idx = sample_operator_thompson()\n                cand_op_idx[i] = op_idx\n                op = operators[op_idx]\n                z = Z[i].copy()\n\n                # local scaled vector and rotation\n                y_local = s * z\n                y_global = R.dot(y_local)\n                x = m + r * 0.6 * y_global   # Gaussian baseline with smaller multiplier\n\n                if op == 'gauss':\n                    # leave as-is (small gaussian)\n                    pass\n\n                elif op == 'cauchy':\n                    # Cauchy-like along direction z: draw scalar from Cauchy via tan(pi*(u-0.5))\n                    u = self.rng.random()\n                    scalar = cauchy_scale * np.tan(np.pi * (u - 0.5))\n                    # scale and rotate\n                    yg = R.dot(s * (z / (np.linalg.norm(z) + 1e-20))) * scalar\n                    x = m + r * yg\n\n                elif op == 'de':\n                    if len(archive_X) >= 2 and self.rng.random() < p_de:\n                        i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                        de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                        x = x + de_mut\n\n                elif op == 'surrogate':\n                    if len(success_buf_X) >= 6:\n                        g = fit_ridge(success_buf_X, success_buf_df)\n                        if g is not None:\n                            ng = np.linalg.norm(g) + 1e-20\n                            # aggressive surrogate step but scaled by a decaying factor\n                            frac = 0.4 + 0.6 * self.rng.random()\n                            step = - (r * (g / ng)) * frac * 1.2\n                            x = m + step\n                    # else fallback to gaussian\n\n                elif op == 'permute':\n                    if (len(self.blocks) > 0) and (self.rng.random() < permute_prob):\n                        bidx = int(self.rng.integers(0, len(self.blocks)))\n                        b = self.blocks[bidx]\n                        if len(b) > 1:\n                            perm = self.rng.permutation(len(b))\n                            x_block = x[b].copy()\n                            x[b] = x_block[perm]\n\n                x = safe_clip(x)\n                Xcand[i] = x\n                cand_step[i] = (x - m)\n\n            # Evaluate candidates (watch budget)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = Xcand[i]\n                fi = func(xi)\n                evals += 1\n                Fcand[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy(); last_improv = evals\n\n            # Selection: choose top mu candidates (top third with minimum 1)\n            mu = max(1, lam // 3)\n            idx_sorted = np.argsort(Fcand)\n            sel_idx = idx_sorted[:mu]\n            X_sel = Xcand[sel_idx]\n            F_sel = Fcand[sel_idx]\n\n            # recombination: use soft-weights from fitness differences and median\n            fmin = np.min(F_sel)\n            # beta adjusts sensitivity; choose adaptively based on r and n\n            beta = 0.8 + 0.2 * (np.clip(r, 1e-8, np.mean(ub - lb)) / (np.mean(ub - lb)))\n            scores = np.exp(-beta * (F_sel - fmin))\n            weights = scores / (np.sum(scores) + 1e-20)\n            weighted_mean = np.sum(weights[:, None] * X_sel, axis=0)\n            median = np.median(X_sel, axis=0)\n            # more median-robust than ASES (50/50 blend)\n            m_new = 0.5 * weighted_mean + 0.5 * median\n            m_new = safe_clip(m_new)\n\n            # center acceptance: Metropolis with decaying temperature\n            accepted_center_move = False\n            fm_new = None\n            if evals < budget:\n                fm_new = func(m_new)\n                evals += 1\n                archive_X.append(m_new.copy()); archive_F.append(fm_new)\n                if fm_new < f_center:\n                    accepted_center_move = True\n                    f_center = fm_new\n                    m = m_new.copy()\n                else:\n                    delta = fm_new - f_center\n                    # decaying tau (different schedule)\n                    tau0 = 0.12\n                    tau = tau0 / (1.0 + 0.002 * gen)\n                    if self.rng.random() < np.exp(-delta / (tau + 1e-12)):\n                        accepted_center_move = True\n                        f_center = fm_new\n                        m = m_new.copy()\n                    else:\n                        # occasional small move toward m_new\n                        if self.rng.random() < 0.08:\n                            m = 0.9 * m + 0.1 * m_new\n            else:\n                # no budget: accept if average selected fitness is better than center\n                mean_sel = np.nanmean(F_sel)\n                if mean_sel < f_center:\n                    m = m_new.copy()\n                    accepted_center_move = True\n\n            # Count improvements relative to pre-snapshot baseline\n            improvements = np.sum(Fcand < (f_center_pre - 1e-12))\n            psucc = improvements / max(1, lam)\n            # adapt radius with different multipliers\n            if psucc < 0.20:\n                r *= 0.90\n            elif psucc > 0.55:\n                r *= 1.08\n            # clamp r\n            r = float(np.clip(r, 1e-8, 8.0 * np.mean(ub - lb)))\n\n            # Update per-dimension scales s using EMA on abs rotated-local coords\n            if r > 0:\n                # Use selected samples relative to current center m\n                Ylocal_sel = (R.T @ ((X_sel - m).T)).T / (r + 1e-20)\n                mean_abs = np.mean(np.abs(Ylocal_sel), axis=0)\n                # EMA update: s <- (1-c_s)*s + c_s*(mean_abs + eps)\n                s = (1.0 - c_s) * s + c_s * (mean_abs + 1e-8)\n                s = np.clip(s, 1e-6, 1e3)\n\n            # Update success buffer with top half of selected (relative to possibly updated center)\n            h = max(1, mu // 2)\n            for j in range(h):\n                idx_j = sel_idx[j]\n                dx = (Xcand[idx_j] - m)\n                df = max(0.0, (f_center - Fcand[idx_j]))\n                if len(success_buf_X) >= buf_max:\n                    success_buf_X.pop(0); success_buf_df.pop(0)\n                success_buf_X.append(dx.copy())\n                success_buf_df.append(df)\n\n            # Update PCA subspace R via PCA on success buffer with different mixing\n            if len(success_buf_X) >= max(8, n // 2):\n                M = np.vstack(success_buf_X)\n                Mc = M - np.mean(M, axis=0, keepdims=True)\n                try:\n                    C = (Mc.T @ Mc) / max(1, Mc.shape[0] - 1)\n                    eigvals, eigvecs = np.linalg.eigh(C)\n                    order = np.argsort(eigvals)[::-1]\n                    top = eigvecs[:, order[:k_sub]]\n                    alpha_sub = 0.35\n                    A = R[:, :k_sub]\n                    Msmall = top.T @ A\n                    try:\n                        U, _, Vt = np.linalg.svd(Msmall)\n                        Q = U @ Vt\n                        newblock = top @ Q\n                        R[:, :k_sub] = (1.0 - alpha_sub) * A + alpha_sub * newblock\n                        # orthonormalize R\n                        Qr, _ = np.linalg.qr(R)\n                        R = Qr[:, :n]\n                    except np.linalg.LinAlgError:\n                        R = R + 0.06 * self.rng.standard_normal(R.shape)\n                        Qr, _ = np.linalg.qr(R)\n                        R = Qr[:, :n]\n                except np.linalg.LinAlgError:\n                    pass\n\n            # Update operator Thompson-sampling stats using rewards observed this generation\n            # Reward definition: improvement over baseline per candidate normalized by step length\n            op_rewards = np.zeros(k_ops)\n            op_seen = np.zeros(k_ops, dtype=int)\n            for i in range(lam):\n                opi = cand_op_idx[i]\n                if not np.isfinite(Fcand[i]):\n                    continue\n                improv = max(0.0, f_center_pre - Fcand[i])\n                step_norm = np.linalg.norm(cand_step[i]) + 1e-20\n                reward = improv / (1.0 + 0.7 * step_norm)\n                op_rewards[opi] += reward\n                op_seen[opi] += 1\n            # update running mean/variance with decay (light forgetting)\n            decay = 0.96\n            for oi in range(k_ops):\n                if op_seen[oi] > 0:\n                    avg_reward = op_rewards[oi] / op_seen[oi]\n                    # apply exponential decay to stored moments to allow nonstationarity\n                    # convert stored M2/mean/count to incremental update\n                    # We'll collapse past into a pseudo-count via decay\n                    prev_count = op_count[oi]\n                    # effective new count\n                    new_count = int(prev_count * decay) + op_seen[oi]\n                    # merge by simple incremental update (safe and simple)\n                    # update mean and M2 using observed batch avg\n                    # for stability, we do a small incremental step towards avg_reward\n                    step = min(0.6, 1.0 / max(1.0, new_count))\n                    # update mean with small step\n                    op_mean[oi] = (1.0 - step) * op_mean[oi] + step * avg_reward\n                    # update M2 with observed batch variance proxy (here approximate by avg_reward^2)\n                    # keep M2 positive\n                    op_M2[oi] = (1.0 - step) * op_M2[oi] + step * (avg_reward - op_mean[oi])**2\n                    op_count[oi] = new_count\n\n            # periodic 1D line probes along leading eigenvector (different cadence)\n            gen += 1\n            if (gen % line_search_every == 0) and (evals < budget):\n                d = R[:, 0]\n                alphas = [0.0, 0.15, -0.15, 0.35, -0.35, 0.7, -0.7]\n                for a in alphas:\n                    if evals >= budget:\n                        break\n                    probe = safe_clip(m + r * a * d)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = float(fp); x_opt = probe.copy(); last_improv = evals\n                    if fp < f_center:\n                        f_center = fp; m = probe.copy()\n\n            # stagnation detection and reset (different thresholds)\n            if (evals - last_improv) > max(50, 6 * n):\n                # stronger exploration injection\n                r *= 2.0\n                # jitter scales\n                s = s * (1.0 + 0.6 * (self.rng.random(n) - 0.5))\n                # randomize R and orthonormalize\n                R = np.eye(n) + 0.12 * self.rng.standard_normal((n, n))\n                Qr, _ = np.linalg.qr(R)\n                R = Qr[:, :n]\n                # reshuffle blocks more aggressively\n                if len(self.blocks) > 1 and self.rng.random() < 0.8:\n                    self.rng.shuffle(self.blocks)\n                # decay operator history to encourage exploration\n                op_count = (op_count * 0.4).astype(int)\n                op_mean *= 0.6\n                op_M2 *= 0.6\n                last_improv = evals\n\n            # trim archive memory\n            if len(archive_X) > 6000:\n                archive_X = archive_X[-6000:]\n                archive_F = archive_F[-6000:]\n\n            # ensure center in bounds\n            m = safe_clip(m)\n\n            # loop end condition\n            if evals >= budget:\n                break\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm REMAS scored 0.235 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "3e29d308-29b0-4a2b-9d16-8e6068e82e1b", "operator": null, "metadata": {"aucs": [0.1368984064704576, 0.16114705843121369, 0.3654577529096733, 0.6442839088616532, 0.1986482176789961, 0.1466808927299561, 0.24452698357449332, 0.19098624565499267, 0.14907389108723712, 0.11330467197953564]}, "task_prompt": ""}
{"id": "1472e1e1-84b2-4758-a700-538454d998e3", "fitness": 0.14507039446752115, "name": "EALCMA_BAND", "description": "The algorithm is a hybrid CMA-style sampler that mixes a diagonal adaptation (D) with a learned low-rank subspace (U,Svec) to propose points (k defaults to ceil(sqrt(dim)), sigma initialized to 0.22 * range, population lambda ≈ 4+3·log(dim), mu≈lambda/2). It maintains an operator portfolio (gauss, subspace, de, cauchy, mixed) with a softmax bandit (temp=0.6) whose log-weights (op_logw) are updated from improvement-based rewards (bandit_alpha=0.18) so the sampler adaptively favors successful mutation types. To concentrate evaluations it generates a large candidate pool (pool_mult=4, pool_cap=120), ranks candidates by a cheap ridge-linear surrogate fitted on a success buffer (fallback: distance-to-mean heuristic), and only evaluates the top lambda, with occasional DE-differences and Cauchy perturbations applied. Evolution paths (ps,pc), D adaptation via weighted second moments, low-rank updates by SVD of recent successes (alpha_sub≈0.25), sigma control via path-length rules (cs,damps,chi_n), plus periodic line-search probes and stagnation perturbations (sigma boost, U/D/noisy mean moves) provide robustness and escape from traps.", "code": "import numpy as np\n\nclass EALCMA_BAND:\n    \"\"\"\n    Ensemble Adaptive Low-rank CMA with Bandit Operators, Surrogate Prefiltering and DE-Archive (EAL-CMA-BAND)\n    One-line: Hybrid diagonal+low-rank CMA-style sampler with an adaptive operator bandit, surrogate prefiltering\n    and DE-archive mutations to concentrate evaluations on promising, diverse and affine-robust proposals.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n        # population size (compact, CMA-inspired)\n        self.lambda_ = max(6, int(4 + np.floor(3 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank subspace dimension\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (assume scalar or per-dim)\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # strategy parameters (CMA-like for recombination and sigma control)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        cs = (mu_eff + 2) / (n + mu_eff + 5)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n\n        # initialize dynamic state\n        m = self.rng.uniform(lb, ub)                           # mean\n        sigma = 0.22 * np.mean(ub - lb)                        # global step-size\n        D = np.ones(n)                                         # diagonal scales (std dev approx)\n        # low-rank subspace: orthonormal columns (n x k), with per-mode scale Svec\n        rand_mat = self.rng.standard_normal((n, self.k))\n        try:\n            U, _ = np.linalg.qr(rand_mat)\n            U = U[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = np.zeros((n, self.k))\n        Svec = np.ones(self.k) * 0.8\n\n        # evolution paths (approx, using invsqrt ~ 1/D on diag)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n\n        # operator portfolio and bandit log-weights\n        operators = ['gauss', 'subspace', 'de', 'cauchy', 'mixed']\n        k_ops = len(operators)\n        op_logw = np.zeros(k_ops)\n        op_count = np.zeros(k_ops, dtype=int)\n\n        # archive and success buffer\n        archive_X = []\n        archive_F = []\n        success_buf_X = []   # store dx (x - m) for successes\n        success_buf_f = []   # store f(x) for successes\n        buf_max = max(50, 10 * self.k)\n\n        # other hyperparams\n        p_de = 0.28\n        F_de = 0.8\n        p_cauchy = 0.10\n        pool_mult = 4             # generate pool_mult * lambda candidates, evaluate top lambda after surrogate ranking\n        pool_cap = 120            # cap total pool size\n        subspace_update_every = max(1, 4)\n        line_search_every = max(20, int(8 * (n / 12 + 1)))\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of center\n        if evals < budget:\n            xm = np.clip(m, lb, ub)\n            fm = func(xm)\n            evals += 1\n            archive_X.append(xm.copy()); archive_F.append(fm)\n            f_opt = float(fm); x_opt = xm.copy()\n            f_center = float(fm)\n        else:\n            f_center = np.inf\n\n        gen = 0\n        stagn_counter = 0\n        last_improv_eval = evals\n\n        # helper: surrogate linear ridge regression (predict f from dx rows)\n        def fit_surrogate(buf_X_rows, buf_f_vals, lam_reg=1e-6):\n            if len(buf_X_rows) < 5:\n                return None\n            A = np.vstack(buf_X_rows)  # m x n\n            y = np.array(buf_f_vals)\n            # center columns to improve conditioning\n            Amean = np.mean(A, axis=0, keepdims=True)\n            Acent = A - Amean\n            ATA = Acent.T @ Acent\n            try:\n                w = np.linalg.solve(ATA + lam_reg * np.eye(n), Acent.T @ (y - np.mean(y)))\n                # return (w, bias, Amean) to predict f(x) = w^T (x - Amean) + mean(y)\n                return (w, float(np.mean(y)), Amean.ravel())\n            except np.linalg.LinAlgError:\n                return None\n\n        # main loop (budget-aware)\n        while evals < budget:\n            remaining = budget - evals\n            # generation population to evaluate this generation\n            gen_lambda = min(lam, remaining)\n\n            # build pool of candidate proposals (free to generate many, only top gen_lambda evaluated)\n            pool_size = int(min(pool_cap, max(gen_lambda, pool_mult * gen_lambda)))\n            Z_pool = self.rng.standard_normal((pool_size, n))\n            cand_X = np.zeros((pool_size, n))\n            cand_meta = [None] * pool_size  # (operator_idx, y vector pre-scale)\n            # prepare surrogate if enough data\n            surrogate = None\n            if len(success_buf_X) >= 6:\n                surrogate = fit_surrogate(success_buf_X, success_buf_f)\n\n            # operator sampling probabilities\n            # use softmax temp that favors exploration slightly\n            temp_bandit = 0.6\n            zlw = op_logw - np.max(op_logw)\n            op_probs = np.exp(zlw / max(1e-12, temp_bandit))\n            op_probs = op_probs / np.sum(op_probs)\n\n            for i in range(pool_size):\n                z = Z_pool[i].copy()\n                # mirrored sampling for even-odd pairs\n                if i % 2 == 1:\n                    z = -z\n\n                # choose an operator\n                op_idx = self.rng.choice(k_ops, p=op_probs)\n                op = operators[op_idx]\n\n                # base diagonal + low-rank mixing\n                z_low = self.rng.standard_normal(self.k) if self.k > 0 else np.zeros(0)\n                # diagonal part\n                y_diag = D * z\n                # low-rank part\n                y_low = (U @ (Svec * z_low)) if self.k > 0 else 0.0\n                # mixing factor adaptively set: if U has meaningful modes, give some weight\n                alpha = 0.7 if np.mean(Svec) > 1e-6 else 0.0\n                y = (1.0 - alpha) * y_diag + alpha * y_low\n\n                # operator modifications\n                if op == 'gauss':\n                    pass\n                elif op == 'subspace':\n                    # biased more to low-rank modes\n                    y = 0.4 * y_diag + 1.6 * y_low if self.k > 0 else y\n                elif op == 'de':\n                    if (len(archive_X) >= 2) and (self.rng.random() < p_de):\n                        i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                        de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                        # apply de mutation after scaling by sigma\n                        # we include the de mutation by adding to candidate after full step\n                        pass  # will add after compute x\n                elif op == 'cauchy':\n                    if self.rng.random() < p_cauchy:\n                        # sample a standard Cauchy scalar and apply in direction of z\n                        r = self.rng.standard_cauchy() * 0.8\n                        nz = np.linalg.norm(z) + 1e-20\n                        y = r * (z / nz) * np.mean(D)\n                    else:\n                        # fallback to gaussian behavior\n                        pass\n                elif op == 'mixed':\n                    # mix gaussian + occasional strong low-rank move\n                    if self.rng.random() < 0.4 and self.k > 0:\n                        z_low2 = self.rng.standard_normal(self.k)\n                        y = 0.3 * y_diag + 1.2 * (U @ (Svec * z_low2))\n\n                xcand = m + sigma * y\n\n                # apply DE difference if operator was de\n                if op == 'de' and (len(archive_X) >= 2) and (self.rng.random() < p_de):\n                    i1, i2 = self.rng.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    xcand = xcand + de_mut\n\n                # clip candidate\n                xcand = np.clip(xcand, lb, ub)\n\n                cand_X[i] = xcand\n                cand_meta[i] = (op_idx, y)\n\n            # if surrogate exists, use it to rank pool, otherwise rank by distance to mean as proxy\n            rank_scores = np.zeros(pool_size)\n            if surrogate is not None:\n                w_sur, bias_sur, meanvec = surrogate\n                # predict f ~= w^T (x - meanvec) + bias\n                Xcent = cand_X - meanvec[np.newaxis, :]\n                preds = Xcent @ w_sur + bias_sur\n                # lower predicted f is better\n                rank_scores = preds\n            else:\n                # prefer candidates closer to mean (exploit) but also with moderate norm for exploration\n                norms = np.linalg.norm((cand_X - m), axis=1)\n                rank_scores = norms  # smaller norms first (heuristic)\n                # add small rand to break ties\n                rank_scores += 1e-8 * self.rng.standard_normal(pool_size)\n\n            # select top-gen_lambda candidates to actually evaluate\n            sel_idx_pool = np.argsort(rank_scores)[:gen_lambda]\n            arx = cand_X[sel_idx_pool]\n            meta_sel = [cand_meta[i] for i in sel_idx_pool]\n\n            # Evaluate selected candidates (budget-aware)\n            arfit = np.full(gen_lambda, np.inf)\n            for k_i in range(gen_lambda):\n                if evals >= budget:\n                    break\n                x = arx[k_i]\n                f = func(x)\n                evals += 1\n                arfit[k_i] = f\n                archive_X.append(x.copy()); archive_F.append(f)\n                # maintain small archive size\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if f < f_opt:\n                    f_opt = float(f); x_opt = x.copy(); last_improv_eval = evals\n\n            # If no evaluations (budget exhausted exactly), break\n            if gen_lambda == 0:\n                break\n\n            # selection and recombination (use top-mu among evaluated)\n            idx_sorted = np.argsort(arfit)\n            sel = idx_sorted[:max(1, min(mu, np.sum(np.isfinite(arfit))))]  # ensure at least one\n            X_sel = arx[sel]\n            Y_sel = []\n            for j in sel:\n                # get corresponding y from meta (if missing, compute approx)\n                op_idx, yvec = meta_sel[j]\n                Y_sel.append(yvec if yvec is not None else (X_sel[j] - m) / max(1e-12, sigma))\n            Y_sel = np.vstack(Y_sel)\n\n            # recombine new mean (log weights)\n            # reuse weights computed earlier but adapt length if mu changed\n            mu_eff_local = 1.0 / np.sum(weights[:len(sel)] ** 2) if len(sel) > 0 else mu_eff\n            w_local = weights[:len(sel)]\n            w_local = w_local / np.sum(w_local)\n            m_old = m.copy()\n            m = np.sum(w_local[:, np.newaxis] * X_sel, axis=0)\n\n            # compute weighted mean step in y-space for path updates\n            y_w = np.sum(w_local[:, np.newaxis] * Y_sel, axis=0)  # approx (m_new - m_old) / sigma\n\n            # update evolution paths (approx invsqrtC via diag D and low-rank ignored for speed)\n            invdiag = 1.0 / (D + 1e-20)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff_local) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n            # compute hsig similar criterion\n            hsig = 1.0 if (norm_ps / np.sqrt(1 - (1 - cs) ** (2 * (gen + 1))) / chi_n) < (1.4 + 2.0 / (n + 1)) else 0.0\n            cc = (4 + mu_eff_local / n) / (n + 4 + 2 * mu_eff_local / n)\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff_local) * y_w\n\n            # adapt covariance-like statistics: update diagonal D by second moment of selected y_local\n            # transform selected Y_sel back to local pre-scale coords approx: local ≈ y_diag component = Y_sel*(something)\n            # For a cheap update, use squared of Y_sel as proxy to update D\n            y2 = np.sum(w_local[:, np.newaxis] * (Y_sel ** 2), axis=0)\n            c_d = 0.18\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + 1e-20)\n            D = np.sqrt(D2)\n            D = np.clip(D, 1e-8, 1e3)\n\n            # update low-rank scales / subspace using success buffer: push best selected into buffer\n            # success criterion: those better than current f_center or relative improvement\n            # Use fcandidate comparisons against f_center snapshot prior to center update\n            # store top half of selected candidates into success buffer\n            half = max(1, len(sel) // 2)\n            for j in range(half):\n                idxj = sel[j]\n                dx = (arx[idxj] - m_old)  # relative to previous mean\n                fj = arfit[idxj]\n                if len(success_buf_X) >= buf_max:\n                    success_buf_X.pop(0); success_buf_f.pop(0)\n                success_buf_X.append(dx.copy())\n                success_buf_f.append(float(fj))\n\n            # update low-rank U,Svec by SVD on success buffer occasionally\n            gen += 1\n            if (gen % subspace_update_every == 0) and (len(success_buf_X) >= max(3, self.k)):\n                try:\n                    Ymat = np.vstack(success_buf_X).T  # n x m\n                    Ymat = Ymat - np.mean(Ymat, axis=1, keepdims=True)\n                    U_new, svals, _ = np.linalg.svd(Ymat, full_matrices=False)\n                    k_take = min(self.k, U_new.shape[1])\n                    if k_take > 0:\n                        U_top = U_new[:, :k_take]\n                        s_top = np.maximum(svals[:k_take] / np.sqrt(max(1, Ymat.shape[1])), 1e-8)\n                        # gentle mixing of subspace and scales\n                        alpha_sub = 0.25\n                        # align current U block size with U_top\n                        # create target block of size k_take and pad if necessary\n                        A = U[:, :k_take]\n                        try:\n                            Msmall = U_top.T @ A\n                            U_m, _, Vt_m = np.linalg.svd(Msmall)\n                            Qsmall = U_m @ Vt_m\n                            newblock = U_top @ Qsmall\n                            U[:, :k_take] = (1.0 - alpha_sub) * A + alpha_sub * newblock\n                            # re-orthonormalize U\n                            Qr, _ = np.linalg.qr(U)\n                            U = Qr[:, :self.k]\n                        except np.linalg.LinAlgError:\n                            # fallback small perturb\n                            U[:, :k_take] = U[:, :k_take] + 0.02 * self.rng.standard_normal((n, k_take))\n                            Qr, _ = np.linalg.qr(U)\n                            U = Qr[:, :self.k]\n                        # update Svec gently\n                        Svec[:k_take] = (1.0 - alpha_sub) * Svec[:k_take] + alpha_sub * s_top[:k_take]\n                        Svec = np.clip(Svec, 1e-8, 1e3)\n                except np.linalg.LinAlgError:\n                    pass\n\n            # sigma adaptation via path length control\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            sigma = float(np.clip(sigma, 1e-12, 1e2 * np.mean(ub - lb) + 1e-12))\n\n            # operator bandit update: compute rewards for operators used in evaluated candidates\n            op_rewards = np.zeros(k_ops)\n            op_seen = np.zeros(k_ops, dtype=int)\n            baseline = f_center if 'f_center' in locals() else f_opt\n            for i_local in range(len(sel)):\n                global_idx = sel[i_local]\n                op_idx, _ = meta_sel[global_idx] if isinstance(meta_sel[global_idx], tuple) else (0, None)\n                # improvement relative to baseline\n                improv = max(0.0, baseline - arfit[global_idx]) if np.isfinite(arfit[global_idx]) else 0.0\n                op_rewards[op_idx] += improv\n                op_seen[op_idx] += 1\n            bandit_alpha = 0.18\n            for oi in range(k_ops):\n                if op_seen[oi] > 0:\n                    avg = op_rewards[oi] / op_seen[oi]\n                    target = np.log1p(avg + 1e-12)\n                    op_logw[oi] = (1.0 - bandit_alpha) * op_logw[oi] + bandit_alpha * target\n                    op_count[oi] += int(op_seen[oi])\n\n            # periodic short 1D line-search / probe along leading subspace direction\n            if (gen % line_search_every == 0) and (evals < budget):\n                # take principal direction as first column of U (if available) else random direction\n                if self.k > 0:\n                    dvec = U[:, 0]\n                else:\n                    dvec = self.rng.standard_normal(n)\n                    dvec = dvec / (np.linalg.norm(dvec) + 1e-20)\n                alphas = [0.0, 0.25, -0.25, 0.5, -0.5]\n                for a in alphas:\n                    if evals >= budget:\n                        break\n                    probe = np.clip(m + sigma * a * dvec, lb, ub)\n                    fp = func(probe)\n                    evals += 1\n                    archive_X.append(probe.copy()); archive_F.append(fp)\n                    if fp < f_opt:\n                        f_opt = float(fp); x_opt = probe.copy(); last_improv_eval = evals\n                    if fp < f_center:\n                        f_center = fp; m = probe.copy()\n\n            # stagnation detection and mild perturbation\n            if (evals - last_improv_eval) > max(60, 8 * n):\n                stagn_counter += 1\n                sigma *= 1.5\n                # perturb U and D slightly\n                U = U + 0.06 * self.rng.standard_normal(U.shape)\n                Qr, _ = np.linalg.qr(U)\n                U = Qr[:, :self.k]\n                D = D * (1.0 + 0.25 * (self.rng.random(n) - 0.5))\n                # random small move of mean toward an archive point to escape flat trap\n                if len(archive_X) > 0:\n                    pick = self.rng.integers(len(archive_X))\n                    m = 0.6 * m + 0.4 * archive_X[pick]\n                    m = np.clip(m, lb, ub)\n                last_improv_eval = evals\n\n            # maintain f_center (approx current center function value)\n            # If center was not evaluated recently, try to re-evaluate it occasionally (budget permitting)\n            if 'f_center' not in locals() or (evals < budget and (gen % 50 == 0)):\n                xmclip = np.clip(m, lb, ub)\n                if evals < budget:\n                    fcenter_try = func(xmclip)\n                    evals += 1\n                    archive_X.append(xmclip.copy()); archive_F.append(fcenter_try)\n                    f_center = float(fcenter_try)\n                    if f_center < f_opt:\n                        f_opt = float(f_center); x_opt = xmclip.copy(); last_improv_eval = evals\n\n            # safety clamp mean into bounds\n            m = np.clip(m, lb, ub)\n\n            # break if budget exhausted\n            if evals >= budget:\n                break\n\n        # final return\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm EALCMA_BAND scored 0.145 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "3e29d308-29b0-4a2b-9d16-8e6068e82e1b", "operator": null, "metadata": {"aucs": [0.07251132554764628, 0.13707437211673223, 0.20361102253481422, 0.16006023983760354, 0.14055276280500695, 0.17412685992472454, 0.1692852794892843, 0.13753717932211618, 0.13769315188568243, 0.11825175121160103]}, "task_prompt": ""}
{"id": "243e48e6-bc70-42e0-8cf6-651d6fa9d793", "fitness": "-inf", "name": "HSMLS", "description": "1) The algorithm performs hybrid memory-guided random-subspace probing: it searches in a low-dimensional subspace of size k ≈ ceil(√n) (probes ≈ 2k) built from an LRU direction memory plus random orthogonal directions (QR) to reuse recent successful directions while keeping exploration.  \n2) Mirrored sampling of normalized subspace directions reduces variance/bias, and occasional archive-driven DE diffs (p_de ≈ 0.22) and Lévy Mantegna jumps (p_levy ≈ 0.12) inject focused mutations and rare long jumps for diversity.  \n3) Successful probes update a compact k×k covariance S and a cumulation path ps (controlled by cs and cov_beta) used to adapt the global step size via the norm of ps and an empirical success-rate multiplier (succ_grow/succ_shrink), concentrating search where improvements occurred.  \n4) Budget-aware intensification is achieved with short 1-D golden-ish line searches, cheap subspace quadratic fitting from archived nearest neighbors to propose focused minimizers, plus archive pruning and occasional Lévy restarts and safeguards (clipping, min_step, eval-count checks) to keep the method robust and efficient.", "code": "import numpy as np\n\nclass HSMLS:\n    \"\"\"\n    Hybrid Subspace Memory Line Search (HSMLS)\n\n    One-line: Memory-guided random-subspace probing with a compact subspace covariance\n    cumulation path, mirrored sampling, archive-based DE diffs, occasional Lévy nudges,\n    and budget-aware short 1-D line searches for intensification.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None,\n                 memory_size=8, p_de=0.22, F_de=0.8, p_levy=0.12,\n                 cs=0.35, cov_beta=0.18, succ_grow=1.12, succ_shrink=0.88,\n                 max_archive_factor=50):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.memory_size = int(memory_size)\n        self.p_de = float(p_de)\n        self.F_de = float(F_de)\n        self.p_levy = float(p_levy)\n        self.cs = float(cs)\n        self.cov_beta = float(cov_beta)\n        self.succ_grow = float(succ_grow)\n        self.succ_shrink = float(succ_shrink)\n        self.max_archive_factor = int(max_archive_factor)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def _levy_mantegna(self, n, alpha=1.5, scale=1.0):\n        # Mantegna symmetric alpha-stable step (approx)\n        sigma_u = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                   (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n        u = np.random.normal(0, sigma_u, size=n)\n        v = np.random.normal(0, 1.0, size=n)\n        step = u / (np.abs(v) ** (1.0 / alpha))\n        return scale * step\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds\n        lb = np.asarray(func.bounds.lb, dtype=float)\n        ub = np.asarray(func.bounds.ub, dtype=float)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n        rng_range = ub - lb\n        mean_range = np.mean(rng_range)\n\n        # safe eval wrapper\n        evals = 0\n        X_archive = []\n        F_archive = []\n\n        def safe_eval(x):\n            nonlocal evals\n            if evals >= budget:\n                return None, None\n            x_clipped = np.clip(x, lb, ub)\n            try:\n                f = float(func(x_clipped))\n            except Exception:\n                f = np.inf\n            evals += 1\n            X_archive.append(x_clipped.copy())\n            F_archive.append(f)\n            return f, x_clipped\n\n        # initial point\n        x_cur = np.random.uniform(lb, ub)\n        out = safe_eval(x_cur)\n        if out[0] is None:\n            return float(np.inf), np.zeros(n)\n        f_cur, x_cur = out\n        f_best = float(f_cur)\n        x_best = x_cur.copy()\n\n        # step-size and safeguards\n        step = 0.4 * mean_range\n        min_step = 1e-9 * max(1.0, mean_range)\n\n        # subspace dimension\n        k = max(1, int(np.ceil(np.sqrt(n))))\n        probes = max(4, 2 * k)\n\n        # compact subspace covariance and cumulation\n        S = np.eye(k)\n        ps = np.zeros(k)\n        B_sub = np.eye(k)\n        D_sub = np.ones(k)\n        invsqrtS = np.eye(k)\n\n        # memory of good directions (LRU)\n        dir_memory = []\n\n        # small budget-aware 1D line-search (golden-ish), few evals\n        def short_line_search(x0, f0, d, init_step, max_evals=8):\n            nonlocal evals\n            if evals >= budget:\n                return None, None\n            dnrm = np.linalg.norm(d)\n            if dnrm == 0:\n                return None, None\n            d = d / dnrm\n            remaining = budget - evals\n            if remaining <= 0:\n                return None, None\n            remaining = min(remaining, max_evals)\n            # try + and - init_step\n            a0 = 0.0\n            fa = f0\n            b = init_step\n            xb = np.clip(x0 + b * d, lb, ub)\n            out = safe_eval(xb)\n            if out[0] is None:\n                return None, None\n            fb, xb = out\n            remaining -= 1\n            if fb >= fa:\n                b = -init_step\n                xb = np.clip(x0 + b * d, lb, ub)\n                out = safe_eval(xb)\n                if out[0] is None:\n                    return None, None\n                fb2, xb2 = out\n                remaining -= 1\n                if fb2 >= fa:\n                    return None, None\n                fb, xb = fb2, xb2\n            # small golden-search between 0 and b with few steps\n            gr = (np.sqrt(5) - 1) / 2\n            a = 0.0\n            c = b - gr * (b - a)\n            d_alpha = a + gr * (b - a)\n            xc = np.clip(x0 + c * d, lb, ub)\n            out = safe_eval(xc)\n            if out[0] is None:\n                return None, None\n            fc, xc = out\n            remaining -= 1\n            xd = np.clip(x0 + d_alpha * d, lb, ub)\n            out = safe_eval(xd)\n            if out[0] is None:\n                return None, None\n            fd, xd = out\n            remaining -= 1\n            best_f = fa\n            best_x = x0.copy()\n            if fc < best_f:\n                best_f = fc; best_x = xc.copy()\n            if fd < best_f:\n                best_f = fd; best_x = xd.copy()\n            it = 0\n            max_it = max(1, remaining)\n            while it < max_it:\n                it += 1\n                if fc < fd:\n                    b = d_alpha\n                    d_alpha = c\n                    fd = fc\n                    c = b - gr * (b - a)\n                    xc = np.clip(x0 + c * d, lb, ub)\n                    out = safe_eval(xc)\n                    if out[0] is None:\n                        break\n                    fc, xc = out\n                    if fc < best_f:\n                        best_f = fc; best_x = xc.copy()\n                else:\n                    a = c\n                    c = d_alpha\n                    fc = fd\n                    d_alpha = a + gr * (b - a)\n                    xd = np.clip(x0 + d_alpha * d, lb, ub)\n                    out = safe_eval(xd)\n                    if out[0] is None:\n                        break\n                    fd, xd = out\n                    if fd < best_f:\n                        best_f = fd; best_x = xd.copy()\n            if best_f < f0 - 1e-12:\n                return best_f, best_x\n            return None, None\n\n        # main loop\n        while evals < budget:\n            # recompute k/probes to allow dimension variation\n            k = max(1, int(np.ceil(np.sqrt(n))))\n            probes = max(4, 2 * k)\n\n            # build basis using memory directions if available\n            use_mem = min(len(dir_memory), k // 2)\n            basis_cols = []\n            if use_mem > 0:\n                for i in range(use_mem):\n                    basis_cols.append(dir_memory[i].copy())\n            needed = k - len(basis_cols)\n            if needed > 0:\n                R = np.random.randn(n, needed)\n                if basis_cols:\n                    R = np.column_stack((np.column_stack(basis_cols), R))\n                Q, _ = np.linalg.qr(R)\n                basis = Q[:, :k]\n            else:\n                Q, _ = np.linalg.qr(np.column_stack(basis_cols))\n                basis = Q[:, :k]\n\n            # ensure S matches k\n            if S.shape[0] != k:\n                S_new = np.eye(k)\n                m = min(S.shape[0], k)\n                S_new[:m, :m] = S[:m, :m]\n                S = S_new\n                ps = np.zeros(k)\n                B_sub = np.eye(k)\n                D_sub = np.ones(k)\n                invsqrtS = np.eye(k)\n\n            # eigendecompose small S for invsqrt\n            try:\n                vals, vecs = np.linalg.eigh(S)\n                vals = np.maximum(vals, 1e-20)\n                D_sub = np.sqrt(vals)\n                B_sub = vecs\n                invsqrtS = (B_sub * (1.0 / D_sub)) @ B_sub.T\n            except np.linalg.LinAlgError:\n                invsqrtS = np.eye(k)\n\n            # mirrored probing\n            if probes % 2 == 0:\n                half = probes // 2\n                coeffs_list = [np.random.randn(k) for _ in range(half)]\n                coeffs_list = coeffs_list + [-c for c in coeffs_list]\n            else:\n                coeffs_list = [np.random.randn(k) for _ in range(probes)]\n\n            successes = 0\n            success_y_list = []\n\n            for coeffs in coeffs_list:\n                if evals >= budget:\n                    break\n                # map to full space\n                d = basis @ coeffs\n                dnrm = np.linalg.norm(d)\n                if dnrm == 0:\n                    continue\n                d = d / dnrm\n\n                # sample alpha\n                alpha = np.random.uniform(-step, step)\n                x_try = x_cur + alpha * d\n\n                # occasional DE diff mutation from archive\n                if (np.random.rand() < self.p_de) and (len(X_archive) >= 2):\n                    i1, i2 = np.random.choice(len(X_archive), size=2, replace=False)\n                    de = self.F_de * (X_archive[i1] - X_archive[i2])\n                    x_try = x_try + 0.5 * de\n\n                # occasional Levy long jump\n                if np.random.rand() < self.p_levy:\n                    levy = self._levy_mantegna(n, alpha=1.5, scale=0.5 * step)\n                    x_try = x_try + levy\n\n                x_try = np.clip(x_try, lb, ub)\n                out = safe_eval(x_try)\n                if out[0] is None:\n                    break\n                f_try, x_try = out\n\n                if f_try < f_cur - 1e-12:\n                    prev_x = x_cur.copy()\n                    x_cur = x_try.copy()\n                    f_cur = f_try\n                    successes += 1\n                    # compute subspace coordinate y using coeffs direction\n                    z = coeffs.copy()\n                    zn = np.linalg.norm(z)\n                    if zn > 0:\n                        z = z / zn\n                        y = (alpha / (step + 1e-20)) * z\n                    else:\n                        y = np.zeros(k)\n                    success_y_list.append(y.copy())\n\n                    # store direction memory\n                    dir_succ = x_cur - prev_x\n                    dn = np.linalg.norm(dir_succ)\n                    if dn > 0:\n                        dir_unit = dir_succ / dn\n                        dir_memory.insert(0, dir_unit.copy())\n                        if len(dir_memory) > self.memory_size:\n                            dir_memory.pop()\n\n                    # short local line search along successful direction for intensification\n                    remaining_budget = budget - evals\n                    if remaining_budget >= 2:\n                        ls_max = min(10, remaining_budget)\n                        ls_out = short_line_search(x_cur, f_cur, dir_unit if dn > 0 else d, init_step=abs(alpha) if abs(alpha) > 1e-12 else step, max_evals=ls_max)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                f_cur = f_ls\n                                x_cur = x_ls.copy()\n                    # update best\n                    if f_cur < f_best:\n                        f_best = f_cur\n                        x_best = x_cur.copy()\n                else:\n                    # small chance of focused short line search from current point along -d/+d\n                    if np.random.rand() < 0.04 and (budget - evals) >= 3:\n                        ls_out = short_line_search(x_cur, f_cur, d, init_step=step, max_evals=4)\n                        if ls_out[0] is not None:\n                            f_ls, x_ls = ls_out\n                            if f_ls < f_cur - 1e-12:\n                                prev_x = x_cur.copy()\n                                x_cur = x_ls.copy()\n                                f_cur = f_ls\n                                # record direction memory\n                                dir_succ = x_cur - prev_x\n                                dn = np.linalg.norm(dir_succ)\n                                if dn > 0:\n                                    dir_unit = dir_succ / dn\n                                    dir_memory.insert(0, dir_unit.copy())\n                                    if len(dir_memory) > self.memory_size:\n                                        dir_memory.pop()\n                                # add to success list for covariance update\n                                z = coeffs.copy()\n                                zn = np.linalg.norm(z)\n                                if zn > 0:\n                                    z = z / zn\n                                    y = (alpha / (step + 1e-20)) * z\n                                else:\n                                    y = np.zeros(k)\n                                success_y_list.append(y.copy())\n                                successes += 1\n                                if f_cur < f_best:\n                                    f_best = f_cur\n                                    x_best = x_cur.copy()\n\n            # end probes\n\n            attempted = len(coeffs_list)\n            success_rate = successes / max(1, attempted)\n\n            # cumulation update\n            if success_y_list:\n                y_mean = np.mean(np.vstack(success_y_list), axis=0)\n                if np.linalg.norm(invsqrtS) > 0:\n                    ps = (1.0 - self.cs) * ps + np.sqrt(self.cs * (2.0 - self.cs)) * (invsqrtS @ y_mean)\n                else:\n                    ps = (1.0 - self.cs) * ps + np.sqrt(self.cs * (2.0 - self.cs)) * y_mean\n            else:\n                ps = (1.0 - self.cs) * ps\n\n            # step-size adaptation (cumulation + success-rate)\n            chi_k = np.sqrt(max(1, k)) * (1.0 - 1.0 / (4.0 * max(1, k)) + 1.0 / (21.0 * max(1, k) ** 2))\n            norm_ps = np.linalg.norm(ps)\n            step *= np.exp(0.25 * (norm_ps / (chi_k + 1e-20) - 1.0) / max(1.0, k))\n            if success_rate > 0.2:\n                step *= self.succ_grow\n            elif success_rate < 0.05:\n                step *= self.succ_shrink\n            step = max(step, min_step)\n\n            # update compact covariance S from successes\n            if success_y_list:\n                y_mean = np.mean(np.vstack(success_y_list), axis=0)\n                rank = np.outer(y_mean, y_mean)\n                S = (1.0 - self.cov_beta) * S + self.cov_beta * (rank + 1e-12 * np.eye(k))\n\n            # occasional small subspace quadratic fit to propose a focused local minimizer (cheap)\n            if len(X_archive) >= max(10, n) and (budget - evals) >= 1:\n                # pick nearest neighbors to x_best in full space and project to current basis\n                X_arr = np.asarray(X_archive)\n                F_arr = np.asarray(F_archive)\n                dists = np.linalg.norm(X_arr - x_best, axis=1)\n                m = min(len(X_arr), max(8, 6 * n // 5))\n                idx = np.argsort(dists)[:m]\n                Z = (X_arr[idx] - x_best) @ basis  # project into subspace\n                # fit simple ridge quadratic in subspace on target values\n                try:\n                    # design: [1, z, 0.5*z^2 diag], keep compact\n                    mpts = Z.shape[0]\n                    P = 1 + k + k\n                    Phi = np.ones((mpts, P))\n                    Phi[:, 1:1 + k] = Z\n                    Phi[:, 1 + k:1 + 2 * k] = 0.5 * (Z ** 2)\n                    yvals = F_arr[idx]\n                    lam = 1e-6\n                    lhs = Phi.T @ Phi + lam * np.eye(P)\n                    rhs = Phi.T @ yvals\n                    sol = np.linalg.solve(lhs, rhs)\n                    g_lin = sol[1:1 + k]  # linear part\n                    h_diag = sol[1 + k:1 + 2 * k]  # quadratic diag (half)\n                    # compute minimizer in subspace z* = -g / (2*h_diag) with small damping\n                    denom = 2.0 * (h_diag + 1e-8)\n                    z_star = -g_lin / denom\n                    step_vec = basis @ z_star\n                    z_norm = np.linalg.norm(step_vec)\n                    if z_norm > 0:\n                        scale = min(1.0, step / (z_norm + 1e-16))\n                    else:\n                        scale = 1.0\n                    x_prop = np.clip(x_best + scale * step_vec, lb, ub)\n                    if evals < budget:\n                        outp = safe_eval(x_prop)\n                        if outp[0] is not None:\n                            f_prop, x_prop = outp\n                            if f_prop < f_best:\n                                f_best = f_prop\n                                x_best = x_prop.copy()\n                                # accept as current to intensify\n                                x_cur = x_prop.copy()\n                                f_cur = f_prop\n                except Exception:\n                    pass\n\n            # archive pruning\n            max_archive = max(2000, self.max_archive_factor * n)\n            if len(X_archive) > max_archive:\n                idx_sorted = np.argsort(F_archive)\n                keep_best = idx_sorted[:200]\n                rest = idx_sorted[200:]\n                keep_rest = rest[::max(1, len(rest) // (max_archive - 200))]\n                keep_idx = np.concatenate([keep_best, keep_rest])\n                X_archive = [X_archive[i] for i in keep_idx]\n                F_archive = [F_archive[i] for i in keep_idx]\n\n            # occasional Levy restart when stagnating (small probability)\n            if np.random.rand() < 0.03 and (budget - evals) > 0 and f_best < np.inf:\n                perturb = self._levy_mantegna(n, alpha=1.5, scale=0.8 * step)\n                x_new = np.clip(x_best + perturb, lb, ub)\n                out = safe_eval(x_new)\n                if out[0] is not None:\n                    f_new, x_new = out\n                    if f_new < f_cur:\n                        x_cur = x_new.copy()\n                        f_cur = f_new\n                        if f_new < f_best:\n                            f_best = f_new\n                            x_best = x_new.copy()\n\n            # small termination heuristic\n            if f_best <= 1e-12:\n                break\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 792, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": "3e29d308-29b0-4a2b-9d16-8e6068e82e1b", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "74059849-f852-4872-a866-a5ba15c416af", "fitness": "-inf", "name": "AROMA", "description": "AROMA centers search on an adaptive trust-region (center m, radius rho) and alternates cheap model-guided Newton-like steps with population sampling, using a reduction-ratio rule to expand/contract rho and accept/blend model moves. The surrogate is a ridge-regularized quadratic fitted on recent archive points (constant + gradient + diagonal Hessian), with distance-weighting toward m, clamped curvature and adaptive damping so model steps are stable. Sampling mixes Gaussian perturbations scaled by a per-coordinate scale D and a low-rank PCA subspace B (k ≈ sqrt(n)), mirrored samples, occasional heavy-tailed Cauchy jumps and DE-style archive differences, and recombines elites with log-rank weights (CMA-like) to update m. Adaptation is lightweight and online: D is updated by EMA of squared steps (c_d=0.20), B is refreshed by PCA on elites, archive size and budget checks are enforced, and heuristics (rho init = 0.15·range, p_cauchy≈0.12, p_de≈0.18, pop size scaling with log(dim), stagnation reheating and periodic center reevaluation) balance exploration vs exploitation.", "code": "import numpy as np\n\nclass AROMA:\n    \"\"\"\n    AROMA: Adaptive Ridge-Orthogonal Model-Assisted search\n\n    Main ingredients:\n    - Maintain a center m and an adaptive trust radius `rho`.\n    - Build a cheap online surrogate f(x) ≈ a + g^T dx + 0.5 * sum(h_i * dx_i^2)\n      by ridge regression on recent archive points (dx = x - m). This gives an\n      approximate gradient g and diagonal Hessian h_diag.\n    - Use an approximate Newton-like step delta = -g / (h_diag + reg) clipped to rho\n      as a model-guided probe and adapt rho using the reduction ratio (trust-region rule).\n    - Sampling ensemble: Gaussian scaled by diagonal D, low-rank PCA basis B (k ~ sqrt(n)),\n      mirrored sampling, occasional Cauchy jumps and DE-style archive differences.\n    - Update D by EMA of squared steps; update B by PCA on top archived point differences.\n    - Stagnation handling: reheating rho and restarts nudging to archived elites.\n    - Budget-aware: never exceeds the allowed number of function evaluations.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, pop_factor=3.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        self.pop_factor = pop_factor\n\n        # population & recombination sizes (small ensemble)\n        self.lambda_ = max(4, int(np.floor(pop_factor + 2.5 * np.log(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n\n        # low-rank subspace dimension (PCA)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds fallback - Many BBOB: [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # basic strategy params\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # initial center m and trust radius rho\n        m = np.random.uniform(lb, ub)\n        range_mean = np.mean(ub - lb)\n        rho = 0.15 * range_mean  # trust radius\n        rho_min = 1e-8\n        rho_max = 5.0 * range_mean\n\n        # diagonal scales (std proxies), low-rank PCA basis B\n        D = np.ones(n)\n        rand_mat = np.random.randn(n, self.k)\n        try:\n            B, _ = np.linalg.qr(rand_mat)\n            B = B[:, :self.k]\n        except np.linalg.LinAlgError:\n            B = np.zeros((n, self.k))\n\n        # surrogate regularization\n        ridge_reg = 1e-6\n\n        # archive\n        archive_X = []\n        archive_F = []\n        archive_limit = max(2000, 20 * n)\n\n        # EMA param for D update\n        c_d = 0.20\n\n        # operator probabilities\n        p_cauchy = 0.12\n        p_de = 0.18\n        F_de = 0.7\n        mirrored = True\n\n        # internal tracking\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation\n        xm = np.clip(m, lb, ub)\n        f = func(xm)\n        evals += 1\n        archive_X.append(xm.copy()); archive_F.append(f)\n        f_opt = f; x_opt = xm.copy()\n        last_improv_eval = evals\n\n        gen = 0\n        stagn_count = 0\n        stagn_thresh = max(8, int(8 * n / max(1, self.k)))\n\n        EPS = 1e-12\n\n        # helper: build surrogate from recent archive (centered at m)\n        def fit_quadratic_diagonal(arch_X, arch_F, max_points=10 * n):\n            \"\"\"\n            Fit model: f ≈ a + g^T dx + 0.5 * sum(h_i * dx_i^2) using ridge regression.\n            Returns a, g (n,), h_diag (n,).\n            If insufficient data, return None.\n            \"\"\"\n            if len(arch_X) < 2:\n                return None\n            # choose recent points, prioritize those near m (within radius 4*rho)\n            Xs = np.array(arch_X[-max_points:])\n            Fs = np.array(arch_F[-max_points:])\n            dxs = Xs - m[None, :]\n            dists = np.linalg.norm(dxs / (D[None, :] + 1e-8), axis=1)\n            # weights favor small distances\n            w = np.exp(-0.5 * (dists / (max(1.0, rho / (np.mean(D) + EPS)))) ** 2)\n            # design: [ones, dx (n), 0.5 * dx^2 (n)]\n            M = np.hstack([np.ones((dxs.shape[0], 1)), dxs, 0.5 * (dxs ** 2)])\n            # weighted ridge solve: (M^T W M + reg I) beta = M^T W y\n            W = np.diag(w + 1e-8)\n            MTW = M.T.dot(W)\n            A = MTW.dot(M)\n            reg = ridge_reg * np.eye(A.shape[0]) * (1.0 + 0.1 * np.mean(Fs ** 2))\n            b = MTW.dot(Fs)\n            try:\n                beta = np.linalg.solve(A + reg, b)\n            except np.linalg.LinAlgError:\n                try:\n                    beta = np.linalg.lstsq(A + reg, b, rcond=None)[0]\n                except Exception:\n                    return None\n            a = float(beta[0])\n            g = beta[1:1 + n].copy()\n            hdiag = beta[1 + n:1 + 2 * n].copy()\n            # clamp hdiag to avoid wild curvature estimates\n            hdiag = np.clip(hdiag, -1e3, 1e3)\n            return a, g, hdiag\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            lam_eff = current_lambda\n\n            # attempt to fit surrogate\n            surf = fit_quadratic_diagonal(archive_X, archive_F, max_points=min(len(archive_X), 12 * n))\n            model_used = False\n\n            # if surrogate available, propose model-based Newton-like step\n            if surf is not None:\n                a, g, hdiag = surf\n                # regularize diagonal Hessian estimate\n                h_reg = hdiag.copy()\n                # ensure positive curvature for stable Newton; if negative, increase damping\n                pos_mask = h_reg <= 0\n                h_reg[pos_mask] = np.abs(h_reg[pos_mask]) + 1.0\n                reg = 1e-4 + 0.1 * np.mean(np.abs(h_reg))  # adaptive reg\n                denom = h_reg + reg\n                delta = - g / (denom + EPS)\n                # limit step size by rho (trust region)\n                norm_delta = np.linalg.norm(delta)\n                if norm_delta > 1e-12:\n                    if norm_delta > rho:\n                        delta = delta * (rho / (norm_delta + EPS))\n                # predicted reduction by model (positive means predicted decrease)\n                pred_red = -(g.dot(delta) + 0.5 * np.sum(hdiag * (delta ** 2)))\n                # apply only if predicted reduction meaningful\n                if pred_red > 1e-8:\n                    candidate = np.clip(m + delta, lb, ub)\n                    if evals < budget:\n                        fc = func(candidate)\n                        evals += 1\n                        archive_X.append(candidate.copy()); archive_F.append(fc)\n                        if len(archive_X) > archive_limit:\n                            archive_X.pop(0); archive_F.pop(0)\n                        if fc < f_opt:\n                            f_opt = fc; x_opt = candidate.copy(); last_improv_eval = evals\n                        # compute actual reduction using current center evaluation if available in archive\n                        # approximate f_center by nearest archived center if needed\n                        # use the last recorded f at m if present, else approximate by surrogate a\n                        # find last archived point equal to clipped m\n                        idxm = None\n                        for i in range(len(archive_X)-1, -1, -1):\n                            if np.allclose(archive_X[i], np.clip(m, lb, ub)):\n                                idxm = i; break\n                        f_center = archive_F[idxm] if idxm is not None else a\n                        actual_red = f_center - fc\n                        if pred_red > 0:\n                            rho_ratio = actual_red / (pred_red + 1e-12)\n                        else:\n                            rho_ratio = -1.0\n                        # trust-region update\n                        if actual_red > 0 and rho_ratio > 0.75:\n                            rho = min(rho_max, rho * 2.0)\n                        elif rho_ratio < 0.25:\n                            rho = max(rho_min, rho * 0.5)\n                        # if model made progress, center m towards the accepted candidate\n                        if fc < f_center:\n                            # conservative update: blend\n                            m = 0.7 * m + 0.3 * candidate\n                        model_used = True\n\n            # prepare sampling amplitude sigma proportional to rho and diag scale D\n            sigma = rho / (np.mean(D) + EPS)\n\n            # sample standard normals\n            Z = np.random.randn(current_lambda, n)\n            X = np.empty((current_lambda, n))\n            Ys = np.empty((current_lambda, n))\n            Fvals = np.full(current_lambda, np.inf)\n\n            for i in range(current_lambda):\n                z = Z[i].copy()\n                if mirrored and (i % 2 == 1):\n                    z = -z\n                # low-rank component\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = B.dot(z_low)\n                    beta = 0.85 * np.mean(D)\n                    y = D * z + beta * low\n                else:\n                    y = D * z\n\n                # occasional heavy-tailed jump along random direction (Cauchy)\n                if np.random.rand() < p_cauchy:\n                    r = np.random.standard_cauchy()\n                    nz = np.linalg.norm(z) + EPS\n                    y = (z / nz) * r * np.mean(D)\n\n                x = m + sigma * y\n\n                # occasional DE-style archive difference mutation\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                X[i] = x\n                Ys[i] = y\n\n            # Evaluate candidates\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if len(archive_X) > archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi; x_opt = xi.copy(); last_improv_eval = evals\n\n            # selection & recombination\n            idx = np.argsort(Fvals)\n            sel = idx[:mu]\n            X_sel = X[sel]\n            Y_sel = Ys[sel]\n            w = weights.copy()\n            if w.shape[0] != len(sel):\n                mu_local = len(sel)\n                w = np.log(mu_local + 0.5) - np.log(np.arange(1, mu_local + 1))\n                w = w / np.sum(w)\n            m_old = m.copy()\n            m = np.sum(w[:, None] * X_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(w[:, None] * Y_sel, axis=0)\n\n            # update diagonal scales D by EMA of weighted second moments\n            y2 = np.sum(w[:, None] * (Y_sel ** 2), axis=0)\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + EPS)\n            D = np.sqrt(D2)\n            D = np.clip(D, 1e-8, 1e3)\n\n            # update low-rank basis B using PCA on differences of top archive elites\n            if len(archive_X) >= max(5, self.k + 2) and (gen % 4 == 0):\n                # take top 4*self.k elites\n                elites_idx = np.argsort(archive_F)[:min(len(archive_F), 4 * self.k + 8)]\n                Xe = np.array(archive_X)[elites_idx]\n                diffs = Xe - np.mean(Xe, axis=0, keepdims=True)\n                try:\n                    U, S, Vt = np.linalg.svd(diffs.T, full_matrices=False)\n                    k_take = min(self.k, U.shape[1])\n                    B_new = U[:, :k_take]\n                    # gentle blending to avoid abrupt switches\n                    blend = 0.5\n                    if B.shape[1] == B_new.shape[1]:\n                        B = (1 - blend) * B + blend * B_new\n                    else:\n                        # reinitialize to B_new with small noise\n                        B = B_new + 0.01 * np.random.randn(*B_new.shape)\n                    # orthonormalize\n                    try:\n                        Q, _ = np.linalg.qr(B)\n                        B = Q[:, :self.k]\n                    except Exception:\n                        pass\n                except np.linalg.LinAlgError:\n                    pass\n\n            # short local probes along principal PCA direction and along averaged selected step\n            if (gen % max(6, int(3 + n // 6)) == 0) and (evals < budget):\n                probes = [0.6, 0.3, -0.3, -0.6]\n                directions = []\n                # use first PCA basis vector if exists\n                if self.k > 0:\n                    directions.append(B[:, 0])\n                # also use direction of weighted step\n                nd = np.linalg.norm(y_w)\n                if nd > EPS:\n                    directions.append(y_w / (nd + EPS))\n                for d in directions:\n                    for alpha in probes:\n                        if evals >= budget:\n                            break\n                        probe = np.clip(m + rho * alpha * d, lb, ub)\n                        fp = func(probe)\n                        evals += 1\n                        archive_X.append(probe.copy()); archive_F.append(fp)\n                        if len(archive_X) > archive_limit:\n                            archive_X.pop(0); archive_F.pop(0)\n                        if fp < f_opt:\n                            f_opt = fp; x_opt = probe.copy(); last_improv_eval = evals\n                    if evals >= budget:\n                        break\n\n            # stagnation detection and reheating\n            if (evals - last_improv_eval) > stagn_thresh * max(1, lam):\n                stagn_count += 1\n                # expand rho and re-randomize center slightly towards a good archived point\n                rho = min(rho_max, rho * (1.5 + 0.2 * stagn_count))\n                if len(archive_X) > 0:\n                    top_idx = np.argsort(archive_F)[:max(1, min(5, len(archive_F)))]\n                    pick = np.random.choice(top_idx)\n                    m = 0.5 * m + 0.5 * archive_X[pick]\n                # perturb B and D lightly\n                if self.k > 0:\n                    B = B + 0.05 * np.random.randn(*B.shape)\n                    try:\n                        Q, _ = np.linalg.qr(B)\n                        B = Q[:, :self.k]\n                    except Exception:\n                        pass\n                D = np.maximum(1e-8, D * (1.0 + 0.06 * (np.random.randn(n))))\n                last_improv_eval = evals\n\n            # small center re-evaluation every few generations to keep center accurate (budget allowing)\n            if (gen % 7 == 0) and (evals < budget):\n                xm_clip = np.clip(m, lb, ub)\n                fm = func(xm_clip)\n                evals += 1\n                archive_X.append(xm_clip.copy()); archive_F.append(fm)\n                if len(archive_X) > archive_limit:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fm < f_opt:\n                    f_opt = fm; x_opt = xm_clip.copy(); last_improv_eval = evals\n\n            # keep m in bounds\n            m = np.clip(m, lb, ub)\n\n            gen += 1\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "595a468f-de25-42dd-900c-96969989bf23", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "0054525b-bc21-4974-955e-95afcb036eb3", "fitness": 0.1440601708671572, "name": "ASDS", "description": "The ASDS design combines per-coordinate RMSProp-like scaling (S/D with beta2=0.88 and small eps) with a learned low-rank subspace U (rank k ≈ sqrt(n)) so mutations are a blend of adaptive diagonal scaling and a compact directional subspace, while population size and subspace rank both scale with √n to keep cost moderate. It injects exploration via moderate-probability heavy-tailed Cauchy jumps (p_cauchy=0.16), occasional DE-style archive differences (p_de=0.22, F_de=0.8) and small stochastic jitter, and it keeps a bounded archive (max 6000) for global mixing and reseeding. Step-size sigma is adapted multiplicatively from generation success rate toward target_succ=0.25 with a dimension-aware learning rate (sigma_eta ∝ 1/√n), plus stagnation reheating (factor ~1.8) and periodic directional probes along a learned principal vector v and U[:,0]. The low-rank U is updated cheaply with an Oja/power-like replacement of the weakest column using weighted selected steps, recombination uses linear decreasing weights, and all evaluations are budget-aware and clipped to the problem bounds.", "code": "import numpy as np\n\nclass ASDS:\n    \"\"\"\n    Adaptive Subspace Differential Search (ASDS)\n\n    Highlights:\n    - Population sampling combining per-coordinate RMSProp-like scaling (D),\n      a learned incremental low-rank subspace U (rank k ~ sqrt(n)), and occasional\n      heavy-tailed Cauchy jumps.\n    - Sigma (global step-size) adapted multiplicatively from generation success-rate\n      using a logistic/exp rule (different from CMA path-length).\n    - Subspace U updated incrementally via a power/Q-like step (cheap) instead of SVD.\n    - Archive-driven DE-style differences for global moves.\n    - Budget-aware evaluations, clipping to box [-5,5] (or func.bounds), and light stagnation reheating.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, pop_factor=2.0, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(seed)\n        # Population sizing: different from HASCE (smaller, scales with sqrt)\n        self.lambda_ = max(4, int(np.ceil(pop_factor * np.sqrt(max(2, self.dim)))))\n        self.mu = max(1, self.lambda_ // 2)\n        # Subspace rank ~ sqrt(n) by default but capped\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(self.dim))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # Hyperparameters (deliberately different from HASCE)\n        self.p_cauchy = 0.16       # heavier chance to try Cauchy jumps\n        self.p_de = 0.22          # DE-difference usage probability\n        self.F_de = 0.8           # DE factor\n        self.mirrored = False     # don't rely on mirrored sampling here\n        # RMSProp-like constants for D update\n        self.beta2 = 0.88\n        self.eps = 1e-10\n        # sigma adaptation constants (success-rate multiplicative)\n        self.target_succ = 0.25\n        self.sigma_eta = 0.6 / np.sqrt(self.dim)  # learning rate scaled with dimension\n        # subspace incremental learning rate\n        self.upa = 0.18  # how strongly to push U towards new direction\n        # stagnation handling\n        self.stagn_reheat = 1.8\n        self.max_archive = 6000\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # read bounds (default [-5,5])\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # initial mean at random location inside bounds\n        m = np.random.uniform(lb, ub)\n        # sigma initialized proportional to box size but different factor from HASCE\n        sigma = 0.12 * np.mean(ub - lb)\n\n        # RMSProp accumulators for squared steps (per-coordinate)\n        S = np.ones(n) * (0.5 ** 2)  # initial variance estimate per coordinate\n        D = np.sqrt(S)\n\n        # low-rank subspace U: initialize orthonormal (n x k)\n        rand_mat = np.random.randn(n, self.k)\n        try:\n            Q, _ = np.linalg.qr(rand_mat)\n            U = Q[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = np.zeros((n, self.k))\n\n        # principal probe direction (unit)\n        v = np.random.randn(n)\n        v /= (np.linalg.norm(v) + self.eps)\n\n        # archive\n        archive_X = []\n        archive_F = []\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # Evaluate initial center\n        xm = np.clip(m, lb, ub)\n        f = func(xm)\n        evals += 1\n        archive_X.append(xm.copy()); archive_F.append(f)\n        f_opt = f; x_opt = xm.copy()\n        last_improv = evals\n\n        gen = 0\n        stagn_count = 0\n        stagn_thresh = max(6, int(6 * n / max(1, self.k)))\n\n        # precompute linear weights for recombination (different: linear decreasing)\n        def recomb_weights(mu_local):\n            w = np.linspace(mu_local, 1.0, num=mu_local)\n            w = w[::-1]  # highest weight for best\n            w = w / np.sum(w)\n            return w\n\n        # helper: Levy-like step via scaled Cauchy truncated\n        def levy_like(scale):\n            # Cauchy heavy-tail but truncated to avoid infinities\n            r = np.random.standard_cauchy()\n            r = np.clip(r, -50.0, 50.0)\n            return r * scale\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            lam = min(self.lambda_, remaining)\n            mu = min(self.mu, lam)\n\n            # sample gaussian base\n            Z = np.random.randn(lam, n)\n            Y = np.zeros((lam, n))\n            X = np.zeros((lam, n))\n            Fvals = np.full(lam, np.inf)\n\n            # per-generation random gamma to weight subspace vs diag (different schedule)\n            gamma = 0.65 * (0.9 + 0.1 * np.random.rand())  # slightly stochastic gamma\n\n            for i in range(lam):\n                z = Z[i].copy()\n\n                # low-rank sample\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U.dot(z_low)\n                else:\n                    low = 0.0\n\n                # blend: diag-scaled plus projected low-rank scaled by gamma and mean(D)\n                y = D * z + gamma * np.mean(D) * low\n\n                # occasional heavy-tailed jump (Cauchy/Levy-like) along random direction or v\n                if np.random.rand() < self.p_cauchy:\n                    # pick direction: v or random\n                    if np.random.rand() < 0.5:\n                        direc = v\n                    else:\n                        direc = np.random.randn(n)\n                        direc /= (np.linalg.norm(direc) + self.eps)\n                    step = levy_like(np.mean(D))\n                    y = direc * step\n\n                x = m + sigma * y\n\n                # occasional DE-style archive difference (global mixing)\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # clip\n                x = np.clip(x, lb, ub)\n\n                X[i, :] = x\n                Y[i, :] = y\n\n            # Evaluate candidates (budget-aware)\n            for i in range(lam):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                if len(archive_X) > self.max_archive:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = fi; x_opt = xi.copy(); last_improv = evals\n\n            # selection\n            idx = np.argsort(Fvals)\n            sel = idx[:mu]\n            X_sel = X[sel]\n            Y_sel = Y[sel]\n\n            # recombination weights (different linear weights)\n            w = recomb_weights(len(sel))\n\n            # update mean m (weighted average of selected candidates)\n            m_old = m.copy()\n            m = np.sum(w[:, None] * X_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(w[:, None] * Y_sel, axis=0)\n\n            # sigma adaptation (multiplicative using success fraction)\n            # measure successes: beats previous center's f (not necessarily best) to encourage progress\n            f_center = archive_F[-1] if len(archive_F) > 0 else f_opt\n            gen_successes = np.sum(Fvals < (f_center - 1e-12))\n            frac = gen_successes / max(1, lam)\n\n            # logistic style modulation (different equation): sigma <- sigma * exp(eta * (frac - target) / sqrt(n))\n            sigma *= np.exp(self.sigma_eta * (frac - self.target_succ))\n            # small stochastic jitter to encourage escape\n            if np.random.rand() < 0.03:\n                sigma *= (1.0 + 0.04 * (np.random.rand() - 0.5))\n\n            # clip sigma into reasonable range based on box\n            sigma = float(np.clip(sigma, 1e-12, 6.0 * np.mean(ub - lb)))\n\n            # Update per-coordinate RMSProp-like second moment S from selected Y (different update rule)\n            # compute weighted second moment of y's\n            y2 = np.sum(w[:, None] * (Y_sel ** 2), axis=0)\n            S = self.beta2 * S + (1.0 - self.beta2) * (y2 + self.eps)\n            # bias-correct and convert to D\n            # apply simple bias correction akin to Adam but only for sqrt\n            bias_corr = 1.0 - (self.beta2 ** (gen + 1))\n            D = np.sqrt(S / (bias_corr + 1e-20))\n            D = np.clip(D, 1e-8, 1e3)\n\n            # incremental low-rank subspace update (power-ish / Oja-like but different parameters)\n            if Y_sel.shape[0] >= 1 and self.k > 0:\n                # make a target direction from weighted selected y's projected orthogonally to current U\n                candidate = y_w.copy()\n                # remove components already in U (project out)\n                proj = U.dot(U.T.dot(candidate))\n                residual = candidate - proj\n                if np.linalg.norm(residual) > 1e-12:\n                    u_new = residual / (np.linalg.norm(residual) + self.eps)\n                    # incorporate into U by replacing smallest-variance column (cheap heuristic)\n                    # compute column variances to find least contributing column\n                    col_strength = np.sum(np.abs(U), axis=0)\n                    j_replace = np.argmin(col_strength)\n                    # gentle replace/blend\n                    U[:, j_replace] = (1.0 - self.upa) * U[:, j_replace] + self.upa * u_new\n                    # re-orthonormalize\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        # fallback: re-randomize small perturbation\n                        rand_mat = np.random.randn(n, self.k)\n                        Q, _ = np.linalg.qr(rand_mat)\n                        U = Q[:, :self.k]\n\n            # update principal probe direction v using a robust median-of-y technique (cheap)\n            if Y_sel.shape[0] >= 1:\n                # take coordinate-wise median of selected steps, then normalize\n                med = np.median(Y_sel, axis=0)\n                if np.linalg.norm(med) > 1e-12:\n                    v = 0.7 * v + 0.3 * (med / (np.linalg.norm(med) + self.eps))\n                    v /= (np.linalg.norm(v) + self.eps)\n\n            # occasional directional probes along v and first U column (cheap line-search-like)\n            if (gen % max(6, int(2 + n // 10)) == 0) and (evals < budget):\n                probes = [0.5, -0.3, 0.2]\n                directions = [v]\n                if self.k > 0:\n                    directions.append(U[:, 0])\n                for d in directions:\n                    for a in probes:\n                        if evals >= budget:\n                            break\n                        probe = np.clip(m + sigma * a * d * np.mean(D), lb, ub)\n                        fp = func(probe)\n                        evals += 1\n                        archive_X.append(probe.copy()); archive_F.append(fp)\n                        if len(archive_X) > self.max_archive:\n                            archive_X.pop(0); archive_F.pop(0)\n                        if fp < f_opt:\n                            f_opt = fp; x_opt = probe.copy(); last_improv = evals\n                    if evals >= budget:\n                        break\n\n            # stagnation detection & mild reheating (different schedule)\n            if (evals - last_improv) > stagn_thresh:\n                stagn_count += 1\n                sigma *= (self.stagn_reheat + 0.12 * stagn_count)\n                # nudge mean toward a good archived point (probabilistic)\n                if len(archive_X) > 0 and np.random.rand() < 0.7:\n                    # pick from top-10% archive\n                    kpick = max(1, int(0.1 * len(archive_X)))\n                    # if archive small fallback to random\n                    if kpick < len(archive_X):\n                        ids = np.argsort(archive_F)[:kpick]\n                        pick = np.random.choice(ids)\n                    else:\n                        pick = np.random.randint(len(archive_X))\n                    m = 0.5 * m + 0.5 * archive_X[pick]\n                # perturb U and v lightly\n                if self.k > 0:\n                    U = U + 0.05 * np.random.randn(*U.shape)\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n                v = v + 0.06 * np.random.randn(n)\n                v /= (np.linalg.norm(v) + self.eps)\n                last_improv = evals\n\n            # small periodic re-evaluate center to keep track (every few gens)\n            if (gen % 7 == 0) and (evals < budget):\n                xm_clip = np.clip(m, lb, ub)\n                fm = func(xm_clip)\n                evals += 1\n                archive_X.append(xm_clip.copy()); archive_F.append(fm)\n                if fm < f_opt:\n                    f_opt = fm; x_opt = xm_clip.copy(); last_improv = evals\n\n            gen += 1\n            # keep mean within bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ASDS scored 0.144 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "595a468f-de25-42dd-900c-96969989bf23", "operator": null, "metadata": {"aucs": [0.06450037007735065, 0.12975960288239763, 0.19328814651314863, 0.13784252147930554, 0.14182998560023363, 0.16676220843053058, 0.17520914049868708, 0.16013557227987896, 0.14968409110207304, 0.12159006980796616]}, "task_prompt": ""}
{"id": "ec17fdb2-45f6-4520-a9bf-9972d31fe39b", "fitness": "-inf", "name": "MANTA", "description": "The algorithm runs a small population of \"walkers\" (p ≈ max(4,⌈2√dim⌉)) each with its own adaptive trust radius s (initialized to 0.12·domain_scale) and a per-dimension anisotropic scale S (initialized ~0.25·(ub−lb)), is budget-aware and clips all proposals to the [-5,5] bounds. Proposals are drawn from a mixture of kernels (probabilities p_levy=0.10, p_de=0.18, p_surrogate=0.22, rest Gaussian/subspace): Lévy/Cauchy jumps often along the top PCA direction, DE-style moves using archive differences, a lightweight surrogate quadratic (diagonal-Hessian) solved in a learned low-rank subspace P (k≈⌈√dim⌉) built from accepted displacements, and Gaussian + subspace blends, plus occasional line probes along principal directions; an archive of evaluated points (capped) feeds DE, surrogate fitting and nudges. Adaptation/robustness features include success-driven step inflation (×1.15) and shrinkage on rejection (×0.85), EMA updates of S from accepted moves, periodic PCA reblending/orthonormalization, stochastic uphill acceptance via a decaying temperature T, stagnation detection that reheats and respawns walkers near the best, and conservative bookkeeping (archive caps, PCA cadence, minimal model sizes, trust-scaling of surrogate steps) to balance exploration and local exploitation.", "code": "import numpy as np\n\nclass MANTA:\n    \"\"\"\n    MANTA: Multiscale Adaptive Neighborhood & Trust-region Algorithm\n\n    Main ideas:\n    - Small population (\"walkers\") each with its own adaptive step-size (trust radius).\n    - A shared learned low-rank subspace P (via incremental PCA/SVD on accepted displacements)\n      used for efficient subspace search and surrogate modeling.\n    - Mixed proposal kernels: coordinate-wise Gaussian, subspace-focused Gaussian, Lévy (Cauchy)\n      jumps, DE-style archive differences, and surrogate-guided Newton-like steps in subspace.\n    - Per-dimension scale S updated from recent accepted moves for anisotropic search.\n    - Occasional 1D/line probes along the top principal direction derived from P.\n    - Archive of evaluated points used for model building, DE moves, and nudging.\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, pop=None, subspace_k=None):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        if seed is not None:\n            np.random.seed(int(seed))\n        self.seed = seed\n        self.pop = pop\n        if self.pop is None:\n            # population scales with dimension but remains small\n            self.pop = max(4, int(np.ceil(2.0 * np.sqrt(max(1, self.dim)))))\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(np.sqrt(max(1, self.dim)))))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds fallback to -5..5\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # small numerical epsilon\n        EPS = 1e-12\n\n        # Initialize population (walkers)\n        p = min(self.pop, budget)  # cannot have more initial walkers than budget\n        X = np.random.uniform(lb, ub, size=(p, n))\n        fvals = np.full(p, np.inf)\n\n        # Evaluate initial walkers (budget-aware)\n        evals = 0\n        archive_X = []\n        archive_F = []\n\n        for i in range(p):\n            if evals >= budget:\n                break\n            xi = np.clip(X[i], lb, ub)\n            fi = func(xi)\n            fvals[i] = float(fi)\n            archive_X.append(xi.copy()); archive_F.append(float(fi))\n            evals += 1\n\n        # If we couldn't fill p due to small budget, reduce p\n        p = min(p, len(archive_F))\n        X = X[:p]\n        fvals = fvals[:p]\n\n        # Best-so-far\n        best_idx = int(np.argmin(fvals))\n        f_best = float(fvals[best_idx])\n        x_best = X[best_idx].copy()\n        last_improv_eval = evals\n\n        # per-walker step sizes (trust radii) initialised to a fraction of domain\n        domain_scale = np.mean(ub - lb)\n        s = np.full(p, 0.12 * domain_scale)  # step-size per walker\n\n        # per-dimension anisotropic scale (S) initial -> use domain scale\n        S = np.full(n, 0.25 * (ub - lb) + 1e-6)\n\n        # learned low-rank subspace P (n x k) orthonormal\n        rand_mat = np.random.randn(n, self.k)\n        try:\n            P, _ = np.linalg.qr(rand_mat)\n            P = P[:, :self.k]\n        except np.linalg.LinAlgError:\n            P = np.zeros((n, self.k))\n\n        # buffer of accepted displacements for PCA / subspace learning\n        accepted_disp = []  # stores y = (x_new - x_old) / s_scale or raw displacements\n        disp_buffer = max(50, 6 * self.k)\n\n        # hyperparameters / probabilities\n        p_levy = 0.10\n        p_de = 0.18\n        p_surrogate = 0.22\n        p_gauss = 0.5  # remainder for Gaussian/subspace mixed\n        F_de = 0.6\n\n        # acceptance temperature for stochastic acceptance of worse moves (simulated-anneal like)\n        T0 = 0.02 * max(1.0, abs(f_best))\n        T = T0\n        T_decay = 0.9995\n\n        # smoothing params for S updates\n        alphaS = 0.12\n\n        # PCA update cadence\n        pca_update_cadence = 4\n\n        # For surrogate building: minimal archive points\n        min_model_points = max(6, 4 * self.k)\n\n        # stagnation detection\n        stagn_no_improve_limit = max(20, 20 * int(np.ceil(n / max(1, self.k))))\n        stagn_counter = 0\n\n        gen = 0\n        # main loop: asynchronous proposals until budget exhausted\n        walker_idx = 0\n        while evals < budget:\n            # pick a walker in round-robin\n            i = walker_idx % p\n            walker_idx += 1\n\n            x_cur = X[i].copy()\n            f_cur = float(fvals[i])\n            step_scale = s[i]\n\n            # choose proposal strategy stochastically\n            r = np.random.rand()\n            propose = None\n            if (r < p_levy) and (np.random.rand() < p_levy + p_de + p_surrogate + p_gauss):\n                # Lévy (Cauchy) jump, often along top principal direction or random dir\n                if self.k > 0 and np.random.rand() < 0.6:\n                    dir_vec = P[:, 0]\n                else:\n                    dir_vec = np.random.randn(n)\n                    dir_vec /= (np.linalg.norm(dir_vec) + EPS)\n                c = np.random.standard_cauchy()\n                propose = x_cur + step_scale * c * dir_vec * np.mean(np.abs(S))\n            elif r < p_levy + p_de and len(archive_X) >= 2:\n                # DE-style move from archive differences\n                a, b = np.random.choice(len(archive_X), size=2, replace=False)\n                de_move = F_de * (archive_X[a] - archive_X[b])\n                # combine with a small Gaussian perturbation\n                z = np.random.randn(n)\n                propose = x_cur + step_scale * 0.6 * S * z + de_move\n            elif (r < p_levy + p_de + p_surrogate) and (len(archive_X) >= min_model_points):\n                # Surrogate-guided step in subspace: fit a lightweight quadratic in P-subspace\n                # gather nearest points to x_cur from archive\n                A = np.asarray(archive_X)\n                F = np.asarray(archive_F)\n                diffs = A - x_cur[None, :]\n                dists = np.sum((diffs / (S[None, :] + 1e-9)) ** 2, axis=1)\n                idxs = np.argsort(dists)[:min(3 * min_model_points, len(dists))]\n                Z = diffs[idxs].dot(P) if self.k > 0 else diffs[idxs]  # m x k\n                fs = F[idxs]\n                # build design matrix: columns [ones, Z (k), 0.5*Z**2 (k)] -> allow diagonal Hessian approximation\n                m_pts = Z.shape[0]\n                if m_pts >= (1 + 2 * self.k):\n                    ones = np.ones((m_pts, 1))\n                    Z_lin = Z\n                    Z_quad = 0.5 * (Z ** 2)\n                    A_design = np.hstack([ones, Z_lin, Z_quad])  # m x (1+2k)\n                    # linear least squares with small ridge\n                    ridge = 1e-6 * np.eye(A_design.shape[1])\n                    try:\n                        beta, *_ = np.linalg.lstsq(A_design.T.dot(A_design) + ridge, A_design.T.dot(fs), rcond=None)\n                        # extract gradient and diagonal Hessian terms:\n                        g_sub = beta[1:1 + self.k]  # subspace gradient approx\n                        q_sub = beta[1 + self.k:1 + 2 * self.k]  # 0.5 * diagH\n                        diagH = 2.0 * q_sub\n                        # ensure Hessian positive definite by regularization\n                        reg = 1e-3 * np.mean(np.abs(diagH) + 1.0)\n                        H_sub = np.diag(diagH + reg)\n                        # Newton-like step in subspace: delta = - H^{-1} g\n                        try:\n                            delta_sub = -np.linalg.solve(H_sub, g_sub)\n                        except np.linalg.LinAlgError:\n                            delta_sub = -g_sub / (np.diag(H_sub) + 1e-8)\n                        # scale step to trust radius\n                        max_norm = np.linalg.norm(P.dot(delta_sub)) if self.k > 0 else np.linalg.norm(delta_sub)\n                        if max_norm > 1e-12:\n                            scale_to_trust = (step_scale * np.mean(S)) / (max_norm + 1e-12)\n                        else:\n                            scale_to_trust = 1.0\n                        propose = x_cur + P.dot(delta_sub) * scale_to_trust if self.k > 0 else x_cur + delta_sub * scale_to_trust\n                    except Exception:\n                        propose = None\n                else:\n                    propose = None\n            else:\n                # Gaussian + subspace mixed proposal (default)\n                z = np.random.randn(n)\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    propose = x_cur + step_scale * (S * z + 0.6 * P.dot(z_low))\n                else:\n                    propose = x_cur + step_scale * (S * z)\n\n            # fall back to pure Gaussian if propose None\n            if propose is None:\n                z = np.random.randn(n)\n                propose = x_cur + step_scale * (S * z)\n\n            # clip to bounds\n            propose = np.clip(propose, lb, ub)\n\n            # one more random tiny perturb with small probability to avoid exact duplicates\n            if np.random.rand() < 0.03:\n                propose += 1e-6 * np.random.randn(n)\n                propose = np.clip(propose, lb, ub)\n\n            # Evaluate proposal if budget allows\n            if evals >= budget:\n                break\n            fp = func(propose)\n            evals += 1\n            archive_X.append(propose.copy()); archive_F.append(float(fp))\n            # keep archive size bounded\n            if len(archive_X) > 5000:\n                archive_X.pop(0); archive_F.pop(0)\n\n            accept = False\n            if fp < f_cur - 1e-15:\n                accept = True\n            else:\n                # stochastic acceptance for uphill moves based on temperature T\n                # scaled by relative objective magnitudes\n                delta = float(fp - f_cur)\n                prob = np.exp(-max(1e-12, delta) / (T + 1e-12))\n                if np.random.rand() < prob:\n                    accept = True\n\n            # update temperature\n            T *= T_decay\n\n            if accept:\n                # accept move\n                X[i] = propose.copy()\n                fvals[i] = float(fp)\n                # update per-walker step-size (reward success)\n                s[i] *= 1.15\n                # record displacement normalized by scale for PCA\n                disp = (propose - x_cur).copy()\n                # store raw displacement scaled to domain for PCA\n                accepted_disp.append(disp / (np.linalg.norm(disp) + EPS))\n                if len(accepted_disp) > disp_buffer:\n                    accepted_disp.pop(0)\n                # update per-dimension scale S via EMA of squared accepted absolute moves\n                S2_new = disp ** 2\n                S = np.sqrt((1.0 - alphaS) * (S ** 2) + alphaS * (S2_new + EPS))\n                S = np.clip(S, 1e-8 * (ub - lb + 1e-12), 2.0 * (ub - lb + 1e-12))\n                # update best\n                if fp < f_best - 1e-15:\n                    f_best = float(fp)\n                    x_best = propose.copy()\n                    last_improv_eval = evals\n                    stagn_counter = 0\n                else:\n                    stagn_counter += 0\n            else:\n                # rejected move -> shrink trust radius\n                s[i] *= 0.85\n                stagn_counter += 1\n\n            # clip step sizes\n            s[i] = np.clip(s[i], 1e-9, 4.0 * domain_scale)\n\n            # occasional PCA/subspace update from accepted_disp\n            if (gen % pca_update_cadence == 0) and (len(accepted_disp) >= max(4, self.k)):\n                Dmat = np.vstack(accepted_disp).T  # n x m\n                # center columns\n                Dmat = Dmat - np.mean(Dmat, axis=1, keepdims=True)\n                try:\n                    U, Svals, Vt = np.linalg.svd(Dmat, full_matrices=False)\n                    k_take = min(self.k, U.shape[1])\n                    if k_take > 0:\n                        P_new = U[:, :k_take]\n                        # gentle blending to avoid abrupt subspace changes\n                        blend = 0.5\n                        if P.shape != P_new.shape:\n                            # reinitialize to new shape\n                            randm = np.random.randn(n, self.k)\n                            P, _ = np.linalg.qr(randm)\n                        else:\n                            P = (1.0 - blend) * P + blend * np.hstack([P_new, np.zeros((n, max(0, self.k - k_take)))])\n                            # orthonormalize\n                            Q, _ = np.linalg.qr(P)\n                            P = Q[:, :self.k]\n                except np.linalg.LinAlgError:\n                    pass\n\n            # occasional line probes along top principal direction starting from best\n            if (gen % max(7, int(3 + n // 8)) == 0) and (evals + 4 <= budget):\n                # probes along top PCA and coordinate scaled principal\n                directions = []\n                if self.k > 0:\n                    directions.append(P[:, 0])\n                # also probe the vector from mean of archive to best\n                if len(archive_X) >= 3:\n                    mean_arch = np.mean(np.asarray(archive_X[-min(200, len(archive_X)):]), axis=0)\n                    dmean = x_best - mean_arch\n                    if np.linalg.norm(dmean) > 1e-12:\n                        directions.append(dmean / (np.linalg.norm(dmean) + EPS))\n                probes = [0.5, 0.2, -0.2, -0.5]\n                for d in directions:\n                    for alpha in probes:\n                        if evals >= budget:\n                            break\n                        probe = np.clip(x_best + alpha * np.mean(s) * np.mean(S) * d, lb, ub)\n                        fp_line = func(probe)\n                        evals += 1\n                        archive_X.append(probe.copy()); archive_F.append(float(fp_line))\n                        if fp_line < f_best:\n                            f_best = float(fp_line)\n                            x_best = probe.copy()\n                            last_improv_eval = evals\n                        if len(archive_X) > 5000:\n                            archive_X.pop(0); archive_F.pop(0)\n                    if evals >= budget:\n                        break\n\n            # stagnation handling: if many evals without improvement, reheating and nudging\n            if (evals - last_improv_eval) > stagn_no_improve_limit:\n                # reheat a fraction of walkers by resetting near-best with larger step sizes\n                n_reheat = max(1, int(np.ceil(0.25 * p)))\n                for j in range(n_reheat):\n                    idx_r = np.random.randint(0, p)\n                    # nudge close to best with random perturbation\n                    X[idx_r] = np.clip(x_best + 0.2 * domain_scale * np.random.randn(n), lb, ub)\n                    fvals[idx_r] = float(func(np.clip(X[idx_r], lb, ub)))\n                    evals += 1\n                    archive_X.append(X[idx_r].copy()); archive_F.append(float(fvals[idx_r]))\n                    s[idx_r] = 0.25 * domain_scale\n                    if evals >= budget:\n                        break\n                # perturb subspace a little\n                if self.k > 0:\n                    P = P + 0.06 * np.random.randn(*P.shape)\n                    Q, _ = np.linalg.qr(P)\n                    P = Q[:, :self.k]\n                last_improv_eval = evals\n                stagn_counter = 0\n\n            gen += 1\n\n            # ensure best stays tracked and mean bounds\n            # occasionally re-evaluate best center to keep f_best honest (budget permitting)\n            if (gen % 17 == 0) and (evals < budget):\n                xm = np.clip(x_best, lb, ub)\n                fm = func(xm)\n                evals += 1\n                archive_X.append(xm.copy()); archive_F.append(float(fm))\n                if fm < f_best:\n                    f_best = float(fm)\n                    x_best = xm.copy()\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n\n        return float(f_best), np.array(x_best, dtype=float)", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 600 seconds.", "error": "Evaluation timed out after 600 seconds.", "parent_ids": "595a468f-de25-42dd-900c-96969989bf23", "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "99124cac-3c91-4754-90ec-ac08a837a304", "fitness": 0.37197840923292147, "name": "ARMS", "description": "ARMS is a hybrid, population-based continuous optimizer that mixes CMA-style recombination and path adaptation with low-rank subspace learning, per-coordinate RMS scaling, DE-style archive differences and occasional heavy-tailed (Cauchy) jumps to balance exploration and exploitation. Population sizing grows weakly with dimension (lambda ∝ sqrt(dim) with pop_factor=2.2) and uses fewer parents (mu≈lambda/3) with simple linear descending weights and mu_eff feeding path/adaptation constants (cs, cc, damps); sigma starts relatively large (0.25·range) and is adapted by a combination of a path-length term and a smoothed success fraction (alpha_smooth=0.12, target_success=0.18). Per-coordinate scale D is maintained with an RMSprop-like EMA (beta_rms decays with generation) and a learned low-rank subspace U (rank k≈dim^0.45) updated incrementally by an Oja-like rule and occasionally orthonormalized to inject correlated directions into sampling. Other purposeful designs include mirrored sampling for variance reduction, periodic 1-D probes along a learned probe vector v and U columns, archive-guided DE mutations (p_de=0.22, F_de=0.6), sporadic Cauchy jumps (p_cauchy=0.08) for heavy tails, and explicit stagnation escapes (multiplicative reheating, archive nudges, subspace perturbation) while strictly enforcing bounds and the evaluation budget.", "code": "import numpy as np\n\nclass ARMS:\n    \"\"\"\n    ARMS: Adaptive Rank-Multiplicative Search\n\n    Main algorithm parameters (explicit, and intentionally different formulae vs HASCE):\n    - lambda_ (population size): set using a sqrt(dim)-scaled heuristic (different constant multiplier).\n    - mu (parents): ~ lambda_/3 (instead of lambda_/2).\n    - weights: linear descending weights (w_i ∝ mu - i + 1) rather than log-weights.\n    - mu_eff: effective selection mass from weights (same concept).\n    - cs (path learning rate): alternative formula cs = 0.4 * mu_eff/(n + mu_eff).\n    - damps (damping for sigma): damps = 1.5 + 0.3 * (mu_eff / max(1,n)).\n    - sigma: initial global step-size set to 0.25 * mean(range).\n    - D: per-coordinate RMS-like scales updated with an adaptive beta decreasing with generations.\n    - U: low-rank subspace (k ~ ceil(n^(0.45))) learned with Oja-like incremental updates every generation (instead of periodic SVD blending).\n    - sigma adaptation: multiplicative 1/5-style nudging combined with a small exponential factor from a path-length-like ps but using invsqrt(D).\n    - p_cauchy, p_de: probabilities for heavy-tailed and DE-style moves (different defaults).\n    - stagnation handling: multiplicative reheating and archive-guided nudges with small Gaussian perturbations.\n    - Many internal constants chosen to be different from HASCE (for exploration-exploitation balance).\n\n    The implementation respects the evaluation budget and bounds [-5,5] (or func.bounds).\n    \"\"\"\n\n    def __init__(self, budget=10000, dim=10, seed=None, subspace_k=None, pop_factor=2.2):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n        # population sizing different from HASCE: weaker growth with dim\n        self.lambda_ = max(6, int(np.ceil(pop_factor * np.sqrt(max(2, self.dim)))))\n        # fewer parents to encourage diversity\n        self.mu = max(1, self.lambda_ // 3)\n        # subspace rank choice different exponent (slightly smaller than sqrt)\n        if subspace_k is None:\n            self.k = max(1, int(np.ceil(self.dim ** 0.45)))\n        else:\n            self.k = min(max(1, int(subspace_k)), self.dim)\n\n        # alternative default probabilities and DE weight\n        self.p_cauchy = 0.08\n        self.p_de = 0.22\n        self.F_de = 0.6\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n\n        # bounds (fallback -5..5)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # strategy parameters (different formulas)\n        lam = self.lambda_\n        mu = min(self.mu, lam)\n        # linear descending weights (different from log-weights)\n        ranks = np.arange(1, mu + 1)\n        lin_weights = (mu - ranks + 1).astype(float)\n        weights = lin_weights / np.sum(lin_weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path/adaptation constants (different)\n        cs = 0.4 * (mu_eff / (n + mu_eff))\n        damps = 1.5 + 0.3 * (mu_eff / max(1, n))\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n        cc = 0.8 * (mu_eff / (n + mu_eff))  # different softness\n\n        # initial state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.25 * np.mean(ub - lb)  # slightly larger starting sigma\n        # per-coordinate RMS scale (start small)\n        D = np.ones(n) * 0.8\n        # low-rank subspace U orthonormal (incremental Oja will update)\n        rand_mat = np.random.randn(n, self.k)\n        try:\n            U, _ = np.linalg.qr(rand_mat)\n            U = U[:, :self.k]\n        except np.linalg.LinAlgError:\n            U = np.zeros((n, self.k))\n        # principal probe direction\n        v = np.random.randn(n)\n        v /= (np.linalg.norm(v) + 1e-20)\n\n        # evolution paths\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n\n        # archive\n        archive_X = []\n        archive_F = []\n\n        # smoothing for success fraction (different alpha)\n        success_smoothed = 0.0\n        alpha_smooth = 0.12\n        target_success = 0.18  # slightly lower than 0.2\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation (center)\n        xm = np.clip(m, lb, ub)\n        f = func(xm)\n        evals += 1\n        archive_X.append(xm.copy()); archive_F.append(f)\n        f_opt = f; x_opt = xm.copy()\n        f_center = f\n        last_improv_eval = evals\n\n        gen = 0\n        stagn_count = 0\n        stagn_thresh = max(6, int(6 * n / max(1, self.k)))\n\n        EPS = 1e-12\n\n        # convenience helper to evaluate safely wrt budget\n        def safe_eval(x):\n            nonlocal evals, f_opt, x_opt, last_improv_eval\n            if evals >= budget:\n                return None\n            fx = func(x)\n            evals += 1\n            archive_X.append(x.copy()); archive_F.append(fx)\n            # keep archive manageable\n            if len(archive_X) > 6000:\n                archive_X.pop(0); archive_F.pop(0)\n            if fx < f_opt:\n                f_opt = fx; x_opt = x.copy(); last_improv_eval = evals\n            return fx\n\n        # main loop\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            lam_eff = current_lambda\n\n            # sample N(0,I) matrix\n            Z = np.random.randn(current_lambda, n)\n            X = np.empty((current_lambda, n))\n            Ys = np.empty((current_lambda, n))\n            Fvals = np.full(current_lambda, np.inf)\n\n            # per-generation adaptive beta for RMS update (decreasing with gen)\n            beta_rms = 0.25 / (1.0 + 0.015 * gen)  # different decay than HASCE\n\n            for i in range(current_lambda):\n                z = Z[i].copy()\n                # mirrored sampling for variance reduction but with random mirror chance\n                if (i % 2 == 1) and (np.random.rand() < 0.85):\n                    z = -z\n\n                # low-rank component via current U\n                if self.k > 0:\n                    z_low = np.random.randn(self.k)\n                    low = U.dot(z_low)\n                    # different blending factor: depend on mean(D) and gen\n                    beta = 0.75 * np.mean(D) * (1.0 / (1.0 + 0.002 * gen))\n                    y = D * z + beta * low\n                else:\n                    y = D * z\n\n                # occasional heavy-tailed jump projecting onto v (different weight)\n                if np.random.rand() < self.p_cauchy:\n                    r = np.random.standard_cauchy()\n                    nz = np.linalg.norm(z) + EPS\n                    y = (z / nz) * r * (0.9 * np.mean(D))\n\n                x = m + sigma * y\n\n                # DE-style archive difference mutation (different scale)\n                if (np.random.rand() < self.p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    de_mut = self.F_de * (archive_X[i1] - archive_X[i2])\n                    x = x + de_mut\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                X[i] = x\n                Ys[i] = y\n\n            # evaluate candidates\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                fi = safe_eval(X[i])\n                Fvals[i] = fi if fi is not None else np.inf\n\n            # selection & recombination\n            idx = np.argsort(Fvals)\n            sel = idx[:mu]\n            X_sel = X[sel]\n            Y_sel = Ys[sel]\n            # ensure weights match current mu\n            w = weights.copy()\n            if w.shape[0] != len(sel):\n                mu_local = len(sel)\n                lin_w = (mu_local - np.arange(1, mu_local + 1) + 1).astype(float)\n                w = lin_w / np.sum(lin_w)\n            m_old = m.copy()\n            # recombine in search-space (linear weights)\n            m = np.sum(w[:, None] * X_sel, axis=0)\n\n            # weighted mean step in y-space\n            y_w = np.sum(w[:, None] * Y_sel, axis=0)\n\n            # path-like update but using inv sqrt(D) (different approx)\n            inv_sqrt = 1.0 / (np.sqrt(D) + EPS)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * inv_sqrt)\n            norm_ps = np.linalg.norm(ps)\n\n            # alternative hsig heuristic slightly stricter\n            hsig = 1.0 if (norm_ps / np.sqrt(1.0 - (1.0 - cs) ** (2.0 * (gen + 1))) / chi_n) < (1.3 + 1.5 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # sigma update: multiplicative success-based nudging + small path-term\n            # small path influence (different exponent factor)\n            sigma *= np.exp(0.7 * (cs / damps) * (norm_ps / (chi_n + EPS) - 1.0))\n            # generation success fraction vs previous center\n            gen_successes = np.sum(Fvals < (f_center - 1e-12))\n            frac = gen_successes / max(1, lam_eff)\n            success_smoothed = (1 - alpha_smooth) * success_smoothed + alpha_smooth * frac\n            # multiplicative nudging towards target (different multipliers)\n            if success_smoothed > target_success:\n                sigma *= 0.985\n            elif success_smoothed < 0.6 * target_success:\n                sigma *= 1.045\n\n            # clamp sigma\n            sigma = np.clip(sigma, 1e-12, 6.0 * np.mean(ub - lb) + 1e-12)\n\n            # update per-coordinate RMS D (RMSprop-like EMA) with adaptive beta\n            y2 = np.sum(w[:, None] * (Y_sel ** 2), axis=0)\n            D2 = (1.0 - beta_rms) * (D ** 2) + beta_rms * (y2 + EPS)\n            D = np.sqrt(D2)\n            # keep D moderate\n            D = np.clip(D, 1e-8, 1e2)\n\n            # incremental Oja update for low-rank subspace U every generation (different approach than HASCE SVD)\n            if self.k > 0 and Y_sel.shape[0] >= 1:\n                # batch Oja: for each selected y, update U columns\n                eta_oja = 0.09 / (1.0 + 0.015 * gen)  # learning rate decays slightly with gen\n                # use the weighted average direction for Oja to save ops\n                y_ave = np.sum(w[:, None] * Y_sel, axis=0)\n                for j in range(self.k):\n                    uj = U[:, j]\n                    # Oja update: uj <- uj + eta * (y_ave * (y_ave^T uj) - (uj * ||y_ave||^2) * small_factor)\n                    proj = np.dot(y_ave, uj)\n                    uj = uj + eta_oja * (y_ave * proj - 0.01 * uj * (np.dot(y_ave, y_ave) + EPS))\n                    # re-normalize\n                    uj /= (np.linalg.norm(uj) + EPS)\n                    U[:, j] = uj\n                # occasionally orthonormalize the subspace\n                if (gen % 6) == 0:\n                    try:\n                        Q, _ = np.linalg.qr(U)\n                        U = Q[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n\n            # update probe vector v by simple covariance accumulation (different gains)\n            if Y_sel.shape[0] >= 1:\n                eta_v = 0.07 / (1.0 + 0.02 * gen)\n                yproj = Y_sel.dot(v)\n                cov_v = (Y_sel.T.dot(yproj)) / max(1, Y_sel.shape[0])\n                v = v + eta_v * cov_v\n                v /= (np.linalg.norm(v) + EPS)\n\n            # occasional 1D lightweight probes along v and U[:,0] but with different probe magnitudes\n            if (gen % max(5, int(2 + n // 10)) == 0) and (evals < budget):\n                probes = [0.5, 0.2, -0.2, -0.5]\n                directions = [v]\n                if self.k > 0:\n                    directions.append(U[:, 0])\n                for d in directions:\n                    for alpha in probes:\n                        if evals >= budget:\n                            break\n                        probe = np.clip(m + sigma * alpha * d * (0.95 * np.mean(D)), lb, ub)\n                        fp = safe_eval(probe)\n                        if fp is None:\n                            break\n                    if evals >= budget:\n                        break\n\n            # stagnation detection and escape (different thresholds/actions)\n            if (evals - last_improv_eval) > stagn_thresh * max(1, lam):\n                stagn_count += 1\n                # stronger reheating multiplicative factor but capped\n                sigma *= min(5.0, 2.0 + 0.35 * stagn_count)\n                # nudge mean toward a good archive point with probability\n                if len(archive_X) > 0 and (np.random.rand() < 0.9):\n                    # pick a top-10 archive point if available\n                    topk = min(10, len(archive_X))\n                    idxs = np.argsort(archive_F)[:topk]\n                    pick = np.random.choice(idxs)\n                    m = 0.55 * m + 0.45 * archive_X[pick]\n                # perturb subspace and v gently\n                if self.k > 0:\n                    U = U + 0.08 * np.random.randn(*U.shape)\n                    try:\n                        Qr, _ = np.linalg.qr(U)\n                        U = Qr[:, :self.k]\n                    except np.linalg.LinAlgError:\n                        pass\n                v = v + 0.12 * np.random.randn(n)\n                v /= (np.linalg.norm(v) + EPS)\n                # reset some buffers/states\n                ps[:] = 0.0\n                success_smoothed = 0.0\n                last_improv_eval = evals\n\n            # periodic small re-evaluation of center to keep f_center accurate\n            if (gen % 7 == 0) and (evals < budget):\n                xm_clip = np.clip(m, lb, ub)\n                fm = safe_eval(xm_clip)\n                if fm is not None:\n                    f_center = min(f_center, fm)\n\n            gen += 1\n            # keep mean in bounds\n            m = np.clip(m, lb, ub)\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm ARMS scored 0.372 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "595a468f-de25-42dd-900c-96969989bf23", "operator": null, "metadata": {"aucs": [0.179195592256572, 0.30037340440654403, 0.3983097599828489, 0.5959064812334831, 0.4235697303693179, 0.5235112315378151, 0.27159858719086793, 0.44871286033086866, 0.42048288775504816, 0.1581235572658488]}, "task_prompt": ""}
{"id": "27b6167b-7779-4a97-af31-f2fc160309a9", "fitness": 0.28623538550443195, "name": "AMUSE", "description": "AMUSE mixes a diagonal per-coordinate adaptive scaling D (updated by an EMA of selected step second-moments) with an ensemble of low-rank subspaces (k ≈ sqrt(n)) to form candidate steps, where ensemble directions are blended (beta ≈ 0.85·mean(D)) and ensemble weights are adapted from projection energies. Sampling uses mirrored Gaussian draws for variance reduction, occasional Cauchy heavy-tail jumps and DE-style differences from an archive (p_cauchy≈0.12, p_de≈0.18, F_de=0.7) to boost exploration and diversify moves. The algorithm borrows CMA-like mechanics (log-based recombination weights, path-length ps/pc, cs/cc/damps/chi_n) to adapt sigma via an exponential update, with an additional smoothed success-rate nudging to bias step-size growth/shrink. Subspaces are learned from small per-subspace buffers via periodic SVD and gentle blending/re-orthonormalization, augmented by periodic 1D probes along principal directions and archive-guided reheating/nudging when stagnation is detected, while all points are clipped to bounds and evaluations are strictly budget-aware.", "code": "import numpy as np\n\nclass AMUSE:\n    \"\"\"\n    AMUSE: Adaptive Multi-Scale Subspace Ensemble\n\n    Key ideas:\n    - Diagonal per-coordinate scaling D + ensemble of low-rank subspaces U_j (k ~ sqrt(n)).\n    - Mirrored sampling for variance reduction; occasional Cauchy jumps and DE-style archive differences.\n    - CMA-like path-length sigma update combined with smoothed 1/5-style success nudging.\n    - Each subspace updated softly from a small buffer via SVD/Oja-style blending to avoid abrupt changes.\n    - Ensemble weights adapt by energy of selected steps projected to each subspace; stagnation reheating,\n      nudging to archived points and periodic 1D probes along principal directions for acceleration.\n    \"\"\"\n    def __init__(self, budget=10000, dim=10, seed=None, ensemble=3, pop_factor=3.0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.seed = seed\n        self.ensemble = int(max(1, ensemble))\n        self.pop_factor = float(pop_factor)\n        if seed is not None:\n            np.random.seed(seed)\n\n    def __call__(self, func):\n        n = self.dim\n        budget = int(self.budget)\n        seed = self.seed\n        if seed is not None:\n            np.random.seed(seed)\n\n        # read bounds (BBOB: typically -5..5)\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n        except Exception:\n            lb = np.full(n, -5.0)\n            ub = np.full(n, 5.0)\n        if lb.shape == ():\n            lb = np.full(n, lb)\n        if ub.shape == ():\n            ub = np.full(n, ub)\n\n        # population sizing (small, CMA-like)\n        lam = max(4, int(4 + np.floor(3 * np.log(max(2, n)))))\n        lam = max(4, int(np.round(self.pop_factor * max(4, np.log(max(2, n))))) ) if self.pop_factor != 3.0 else lam\n        mu = max(1, lam // 2)\n        # recombination weights (log-based)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # path-length constants\n        cs = (mu_eff + 2.0) / (n + mu_eff + 5.0)\n        damps = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (n + 1.0)) - 1.0) + cs\n        chi_n = np.sqrt(n) * (1.0 - 1.0 / (4.0 * n) + 1.0 / (21.0 * n ** 2))\n        cc = (4 + mu_eff / n) / (n + 4 + 2 * mu_eff / n)\n\n        # initial state\n        m = np.random.uniform(lb, ub)\n        sigma = 0.18 * np.mean(ub - lb)\n        D = np.ones(n)  # per-coordinate std proxies\n        k = max(1, int(np.ceil(np.sqrt(n))))  # subspace rank\n        # ensemble of subspaces U_j (n x k)\n        U_list = []\n        for j in range(self.ensemble):\n            rand = np.random.randn(n, k)\n            try:\n                Q, _ = np.linalg.qr(rand)\n                U_list.append(Q[:, :k])\n            except np.linalg.LinAlgError:\n                U_list.append(np.eye(n, k))\n        # ensemble weights (for mixing low-rank components)\n        ensemble_w = np.ones(self.ensemble) / float(self.ensemble)\n\n        # evolution path for sigma (approx using diag inv)\n        ps = np.zeros(n)\n        pc = np.zeros(n)\n\n        # buffers for subspaces\n        buffers = [[] for _ in range(self.ensemble)]\n        buffer_max = max(10 * k, 30)\n\n        # archive for DE, keep some bests\n        archive_X = []\n        archive_F = []\n\n        # control params\n        p_cauchy = 0.12\n        p_de = 0.18\n        F_de = 0.7\n        mirrored = True\n        success_smoothed = 0.0\n        alpha_smooth = 0.2\n        target_success = 0.2\n        EPS = 1e-12\n\n        evals = 0\n        f_opt = np.inf\n        x_opt = None\n\n        # initial evaluation of center\n        xm = np.clip(m, lb, ub)\n        f_m = func(xm)\n        evals += 1\n        archive_X.append(xm.copy()); archive_F.append(f_m)\n        f_opt = float(f_m); x_opt = xm.copy()\n        f_center = float(f_m)\n        last_improv = evals\n\n        gen = 0\n        stagn_no_improve = 0\n        stagn_thresh = max(10, int(6 * n / max(1, k)))\n\n        # main loop: produce generations respecting budget\n        while evals < budget:\n            remaining = budget - evals\n            current_lambda = min(lam, remaining)\n            Z = np.random.randn(current_lambda, n)\n            X = np.empty((current_lambda, n))\n            Ys = np.empty((current_lambda, n))\n            Fvals = np.full(current_lambda, np.inf)\n\n            for i in range(current_lambda):\n                z = Z[i].copy()\n                if mirrored and (i % 2 == 1):\n                    z = -z\n\n                # diagonal component\n                y_diag = D * z\n\n                # ensemble low-rank mixture\n                low_sum = np.zeros(n)\n                for j, U in enumerate(U_list):\n                    # sample projection coefficients\n                    z_low = np.random.randn(U.shape[1])\n                    low = U.dot(z_low)\n                    low_sum += ensemble_w[j] * low\n\n                beta = 0.85 * np.mean(D)  # weight for low-rank mixture\n                y = y_diag + beta * low_sum\n\n                # occasional Cauchy heavy-tail\n                if np.random.rand() < p_cauchy:\n                    r = np.random.standard_cauchy()\n                    nz = np.linalg.norm(z) + EPS\n                    y = (z / nz) * r * np.mean(D)\n\n                x = m + sigma * y\n\n                # DE-style archive difference\n                if (np.random.rand() < p_de) and (len(archive_X) >= 2):\n                    i1, i2 = np.random.choice(len(archive_X), size=2, replace=False)\n                    x = x + F_de * (archive_X[i1] - archive_X[i2])\n\n                # clip to bounds\n                x = np.clip(x, lb, ub)\n\n                X[i] = x\n                Ys[i] = y\n\n            # Evaluate candidates budget-aware\n            for i in range(current_lambda):\n                if evals >= budget:\n                    break\n                xi = X[i]\n                fi = func(xi)\n                evals += 1\n                Fvals[i] = fi\n                archive_X.append(xi.copy()); archive_F.append(fi)\n                # bound archive size\n                if len(archive_X) > 5000:\n                    archive_X.pop(0); archive_F.pop(0)\n                if fi < f_opt:\n                    f_opt = float(fi); x_opt = xi.copy(); last_improv = evals\n\n            # selection\n            idx = np.argsort(Fvals)\n            sel = idx[:mu]\n            X_sel = X[sel]\n            Y_sel = Ys[sel]\n\n            # recombine new mean\n            w = weights.copy()\n            if w.shape[0] != len(sel):\n                # adapt weights if mu changed due to small lambda\n                mu_loc = max(1, len(sel))\n                w = np.log(mu_loc + 0.5) - np.log(np.arange(1, mu_loc + 1))\n                w = w / np.sum(w)\n            m_old = m.copy()\n            m = np.sum(w[:, None] * X_sel, axis=0)\n\n            # compute weighted mean step in y-space\n            y_w = np.sum(w[:, None] * Y_sel, axis=0)\n\n            # path-length update (approx invsqrtC by 1/D)\n            invdiag = 1.0 / (D + EPS)\n            ps = (1.0 - cs) * ps + np.sqrt(cs * (2.0 - cs) * mu_eff) * (y_w * invdiag)\n            norm_ps = np.linalg.norm(ps)\n            # heuristic hsig (like CMA)\n            hsig = 1.0 if (norm_ps / np.sqrt(1.0 - (1.0 - cs) ** (2.0 * (gen + 1))) / chi_n) < (1.4 + 2.0 / (n + 1.0)) else 0.0\n            pc = (1.0 - cc) * pc + hsig * np.sqrt(cc * (2.0 - cc) * mu_eff) * y_w\n\n            # sigma update: path-length + smoothed success-rate nudging\n            sigma *= np.exp((cs / damps) * (norm_ps / chi_n - 1.0))\n            # success fraction this generation (relative to f_center)\n            gen_successes = np.sum(Fvals < (f_center - 1e-12))\n            frac = gen_successes / max(1, current_lambda)\n            success_smoothed = (1 - alpha_smooth) * success_smoothed + alpha_smooth * frac\n            if success_smoothed > target_success:\n                sigma *= 0.98\n            elif success_smoothed < 0.5 * target_success:\n                sigma *= 1.03\n            sigma = np.clip(sigma, 1e-12, 5.0 * np.mean(ub - lb) + 1e-12)\n\n            # update diagonal D by EMA of weighted second moments from selected y\n            y2 = np.sum(w[:, None] * (Y_sel ** 2), axis=0)\n            c_d = 0.22\n            D2 = (1.0 - c_d) * (D ** 2) + c_d * (y2 + EPS)\n            D = np.sqrt(D2)\n            D = np.clip(D, 1e-8, 1e3)\n\n            # update subspace buffers and ensemble weights\n            # compute projection energy of each selected y onto each subspace to credit subspaces\n            energies = np.zeros(self.ensemble)\n            for j, U in enumerate(U_list):\n                # project weighted mean step y_w onto U_j\n                proj = U.T.dot(y_w)\n                energies[j] = np.sum(proj ** 2) + 1e-20\n            if energies.sum() > 0:\n                ensemble_w = ensemble_w * 0.9 + 0.1 * (energies / energies.sum())\n                # normalize\n                ensemble_w = np.maximum(ensemble_w, 1e-8)\n                ensemble_w = ensemble_w / np.sum(ensemble_w)\n\n            # add y_w and top selected projections to respective buffers\n            # for diversity, give each buffer a copy proportionally to its ensemble weight\n            for j in range(self.ensemble):\n                if np.random.rand() < (0.6 * ensemble_w[j] + 0.05):\n                    buffers[j].append(y_w.copy())\n                    if len(buffers[j]) > buffer_max:\n                        buffers[j].pop(0)\n\n            # periodically update subspaces via blended SVD\n            if gen % 3 == 0:\n                for j in range(self.ensemble):\n                    buf = buffers[j]\n                    if len(buf) >= min(3, k):\n                        Ymat = np.vstack(buf).T  # n x m\n                        Ymat = Ymat - np.mean(Ymat, axis=1, keepdims=True)\n                        try:\n                            U_new, svals, _ = np.linalg.svd(Ymat, full_matrices=False)\n                            k_take = min(k, U_new.shape[1])\n                            if k_take > 0:\n                                Utop = U_new[:, :k_take]\n                                # blend old U and Utop gently to avoid abrupt switch\n                                blend = 0.5\n                                # if shapes mismatch, pad Utop\n                                if U_list[j].shape[1] != k_take:\n                                    padded = np.zeros((n, k))\n                                    padded[:, :k_take] = Utop\n                                    Utop = padded\n                                U_list[j] = (1 - blend) * U_list[j] + blend * Utop[:, :k]\n                                # re-orthonormalize\n                                try:\n                                    Q, _ = np.linalg.qr(U_list[j])\n                                    U_list[j] = Q[:, :k]\n                                except np.linalg.LinAlgError:\n                                    # small random perturbation fallback\n                                    U_list[j] = U_list[j] + 0.01 * np.random.randn(*U_list[j].shape)\n                                    Q, _ = np.linalg.qr(U_list[j])\n                                    U_list[j] = Q[:, :k]\n                        except np.linalg.LinAlgError:\n                            pass\n\n            # periodic 1D probes along top ensemble directions (first principal vector of top-weight subspace)\n            if (gen % max(7, int(4 + n/8)) == 0) and (evals < budget):\n                # pick the subspace with highest ensemble weight\n                top_j = int(np.argmax(ensemble_w))\n                dlist = []\n                # principal direction from top subspace\n                dlist.append(U_list[top_j][:, 0])\n                # also try direction from combined weighted low-rank (weighted sum of first columns)\n                comb = np.zeros(n)\n                for j in range(self.ensemble):\n                    comb += ensemble_w[j] * U_list[j][:, 0]\n                if np.linalg.norm(comb) > EPS:\n                    dlist.append(comb / (np.linalg.norm(comb) + EPS))\n                alphas = [0.6, 0.3, -0.3, -0.6]\n                for d in dlist:\n                    for a in alphas:\n                        if evals >= budget:\n                            break\n                        probe = np.clip(m + sigma * a * d * np.mean(D), lb, ub)\n                        fp = func(probe)\n                        evals += 1\n                        archive_X.append(probe.copy()); archive_F.append(fp)\n                        if len(archive_X) > 5000:\n                            archive_X.pop(0); archive_F.pop(0)\n                        if fp < f_opt:\n                            f_opt = float(fp); x_opt = probe.copy(); last_improv = evals\n                    if evals >= budget:\n                        break\n\n            # stagnation detection: if no improvement for a while, reheat and nudge to archive\n            if (evals - last_improv) > stagn_thresh:\n                stagn_no_improve += 1\n                # reheating sigma\n                sigma *= min(4.0, 1.6 + 0.15 * stagn_no_improve)\n                # nudge mean towards a random archived good point (prefer lower F)\n                if len(archive_X) > 0:\n                    # pick among best 10% if available\n                    cutoff = max(1, int(0.1 * len(archive_F)))\n                    idx_sorted = np.argsort(archive_F)\n                    pick_idx = idx_sorted[np.random.randint(0, min(cutoff, len(idx_sorted)))]\n                    m = 0.6 * m + 0.4 * archive_X[pick_idx]\n                # perturb subspaces lightly\n                for j in range(self.ensemble):\n                    U_list[j] = U_list[j] + 0.06 * np.random.randn(*U_list[j].shape)\n                    try:\n                        Q, _ = np.linalg.qr(U_list[j])\n                        U_list[j] = Q[:, :k]\n                    except np.linalg.LinAlgError:\n                        pass\n                # slightly reset ps to encourage exploration\n                ps *= 0.3\n                last_improv = evals\n\n            # periodically evaluate the center to update f_center\n            if (gen % 5 == 0) and (evals < budget):\n                xm_clip = np.clip(m, lb, ub)\n                fm = func(xm_clip)\n                evals += 1\n                archive_X.append(xm_clip.copy()); archive_F.append(fm)\n                if fm < f_opt:\n                    f_opt = float(fm); x_opt = xm_clip.copy(); last_improv = evals\n                f_center = min(f_center, float(fm))\n\n            # ensure mean in bounds\n            m = np.clip(m, lb, ub)\n            gen += 1\n\n        return float(f_opt), np.array(x_opt, dtype=float)", "configspace": "", "generation": 0, "feedback": "The algorithm AMUSE scored 0.286 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": "595a468f-de25-42dd-900c-96969989bf23", "operator": null, "metadata": {"aucs": [0.08677506969659332, 0.15956893349395962, 0.208248737913318, 0.15896636031219746, 0.1674162547642305, 0.8228292114988197, 0.2119759729835745, 0.1767870845852767, 0.7485575112842661, 0.12122871851208372]}, "task_prompt": ""}

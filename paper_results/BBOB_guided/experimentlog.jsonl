{"method_name": "ES", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "6f749bec-8fd8-4ef6-8b99-9ec09a8cf60e", "fitness": 0.6228707352367312, "name": "AdaptiveCauchyDifferentialLocalSearchV2", "description": "Adaptive Cauchy‑Differential Cooperative Search v2 — hybrid DE+heavy‑tailed global jumps with JADE‑style adaptive F/CR memory, per‑dimension success scaling, archive‑assisted mutation, budget‑aware parabolic directional probes and staged population resizing for robust exploration-to-exploitation shift.", "code": "import numpy as np\n\nclass AdaptiveCauchyDifferentialLocalSearchV2:\n    \"\"\"\n    Adaptive Cauchy-Differential Cooperative Search v2 (ACDCSv2).\n\n    Key elements:\n      - JADE-like adaptive sampling of F and CR (adaptive differential weight & crossover)\n      - Archive of elites for diversity-aware mutation (DE/current-to-pbest/1-like)\n      - Per-dimension success counters -> gentle per-dimension sigma adaptation\n      - Occasional heavy-tailed multivariate Cauchy jumps centered at best for basin escape\n      - Budget-aware parabolic directional probes around the best (2-point probe + parabola)\n      - Staged population resizing: gradual population reduction as budget dwindles\n      - Strict budget accounting (never exceed self.budget evaluations)\n    Interface:\n      optim = AdaptiveCauchyDifferentialLocalSearchV2(budget=10000, dim=10, seed=0)\n      f_best, x_best = optim(func)\n    \"\"\"\n\n    def __init__(\n        self,\n        budget,\n        dim,\n        pop_size=None,\n        memory_size=8,          # JADE memory size for F/CR adaptation\n        p_best=0.2,             # top-p for p-best selection\n        initial_step=0.2,       # initial per-dimension relative sigma\n        c_adapt=0.1,            # learning rate for JADE memory updates\n        p_cauchy=0.18,          # probability of heavy-tail jump around best\n        parabolic_every=150,    # perform parabolic probe around best every this many evals\n        parabolic_samples=2,    # number of radii samples for parabola (kept 2)\n        local_gauss_every=200,  # periodic small Gaussian local sampling around best\n        local_gauss_samples=12,\n        pop_size_min=6,\n        seed=None\n    ):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop_size = pop_size  # may be None; auto-set below\n        self.memory_size = int(memory_size)\n        self.p_best = float(p_best)\n        self.initial_step = float(initial_step)\n        self.c_adapt = float(c_adapt)\n        self.p_cauchy = float(p_cauchy)\n        self.parabolic_every = int(parabolic_every)\n        self.parabolic_samples = int(parabolic_samples)\n        self.local_gauss_every = int(local_gauss_every)\n        self.local_gauss_samples = int(local_gauss_samples)\n        self.pop_size_min = int(pop_size_min)\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n\n    def __call__(self, func):\n        # Determine bounds: prefer func.bounds if provided, else [-5,5]^dim\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.shape[0] != self.dim or ub.shape[0] != self.dim:\n                raise Exception(\"bounds dimension mismatch\")\n        except Exception:\n            lb = np.full(self.dim, -5.0, dtype=float)\n            ub = np.full(self.dim, 5.0, dtype=float)\n\n        bounds_width = ub - lb\n        # Population size auto heuristics: scale with dim and budget\n        if self.pop_size is None:\n            # prefer between [pop_size_min, 12*dim] but constrained by budget\n            p_guess = int(min(max(self.pop_size_min, 6 + self.dim), 12 * self.dim))\n            p_budget = max(self.pop_size_min, self.budget // 25)\n            p = int(min(p_guess, p_budget))\n            p = max(self.pop_size_min, p)\n            self.pop_size = p\n        else:\n            self.pop_size = max(self.pop_size_min, min(int(self.pop_size), self.budget))\n\n        # State variables\n        pop = self.rng.uniform(lb, ub, size=(self.pop_size, self.dim))\n        f_vals = np.full(self.pop_size, np.inf)\n        evals = 0\n\n        # Strict evaluation wrapper\n        def eval_point(x):\n            nonlocal evals\n            if evals >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            fx = func(x)\n            evals += 1\n            return float(fx)\n\n        # initial evaluations\n        for i in range(self.pop_size):\n            if evals >= self.budget:\n                break\n            f_vals[i] = eval_point(pop[i])\n\n        # archive (for JADE-like mutation) keeps previous successful vectors\n        archive = np.zeros((0, self.dim))\n        # per-dimension adaptive sigma (global, then per-dim small multipliers)\n        sigma_dim = np.full(self.dim, self.initial_step)  # relative to bounds width\n\n        # JADE memory for F/CR\n        H = max(1, self.memory_size)\n        mem_F = np.full(H, 0.5)\n        mem_CR = np.full(H, 0.5)\n        mem_pos = 0\n\n        # helper for sampling F/CR similar to JADE\n        def sample_F_CR():\n            # pick an index from memory\n            idx = self.rng.integers(0, H)\n            mu_F = mem_F[idx]\n            mu_CR = mem_CR[idx]\n            # F from Cauchy(mu_F, 0.1)\n            for _ in range(10):\n                F = self.rng.standard_cauchy() * 0.1 + mu_F\n                if F > 0:\n                    break\n            F = np.clip(F, 1e-6, 1.0)\n            # CR from normal\n            CR = self.rng.normal(loc=mu_CR, scale=0.1)\n            CR = np.clip(CR, 0.0, 1.0)\n            return float(F), float(CR), idx\n\n        # track global best\n        best_idx = int(np.argmin(f_vals))\n        f_best = float(f_vals[best_idx])\n        x_best = pop[best_idx].copy()\n\n        # bookkeeping for parabolic/local probes\n        last_parabolic_at = 0\n        last_local_gauss_at = 0\n\n        # Main loop\n        stagnation = 0\n        iter_num = 0\n        # store successful F and CRs with delta fitness for memory update\n        while evals < self.budget:\n            iter_num += 1\n            improved = False\n            # dynamic population shrinking as budget decreases (staged exploitation)\n            remaining = max(1, self.budget - evals)\n            target_pop = max(self.pop_size_min, int(np.ceil(self.pop_size * (0.6 + 0.4 * remaining / self.budget))))\n            # ensure target_pop at most current pop size\n            if target_pop < pop.shape[0]:\n                # prune worst individuals to speed up local search, keep elites\n                worst_ids = np.argsort(f_vals)[- (pop.shape[0] - target_pop):]\n                pop = np.delete(pop, worst_ids, axis=0)\n                f_vals = np.delete(f_vals, worst_ids, axis=0)\n                # no change to archive or mem\n            pop_n = pop.shape[0]\n\n            # prepare arrays for next generation replacement\n            new_pop = pop.copy()\n            new_f = f_vals.copy()\n\n            successful_F = []\n            successful_CR = []\n            delta_f_list = []\n            successful_diffs = []  # for weighted means if needed\n\n            for i in range(pop_n):\n                if evals >= self.budget:\n                    break\n\n                # occasionally do a heavy-tailed Cauchy jump around best\n                if self.rng.random() < self.p_cauchy:\n                    # multivariate heavy-tail: standard_cauchy per-dim scaled by sigma_dim and individual's scale\n                    scale_vec = sigma_dim * bounds_width\n                    raw = self.rng.standard_cauchy(size=self.dim)\n                    cand = x_best + raw * scale_vec * (0.6 + 0.8 * self.rng.random())\n                    cand = np.clip(cand, lb, ub)\n                    # evaluate\n                    try:\n                        fc = eval_point(cand)\n                    except StopIteration:\n                        break\n                    # accept if better than individual's f or better than best\n                    if fc < f_vals[i]:\n                        new_pop[i] = cand\n                        new_f[i] = fc\n                        # add to archive\n                        archive = np.vstack([archive, pop[i:i+1]])\n                    if fc < f_best:\n                        f_best = fc\n                        x_best = cand.copy()\n                        improved = True\n                    continue  # next individual\n\n                # sample F and CR from memory\n                F, CR, mem_idx = sample_F_CR()\n\n                # mutation: current-to-pbest/1 strategy with archive\n                p_num = max(2, int(np.ceil(self.p_best * pop_n)))\n                p_idxs = np.argsort(f_vals)[:p_num]\n                p_best_idx = self.rng.choice(p_idxs)\n                # select two distinct vectors from union of pop and archive\n                union = np.vstack([pop, archive]) if archive.size else pop\n                u_n = union.shape[0]\n                # ensure indices distinct and not equal to i or p_best_idx if from pop\n                all_idx = np.arange(u_n)\n                # pick r1,r2 distinct and not pointing to current\n                r1 = r2 = None\n                # map i to its index in union if necessary:\n                # we must avoid picking the i-th individual from pop (if union contains it)\n                # to keep simple: pick until r1,r2 are distinct and not equal to p and not equal to i (if applicable)\n                for _ in range(50):\n                    r1 = int(self.rng.integers(0, u_n))\n                    r2 = int(self.rng.integers(0, u_n))\n                    if r1 != r2:\n                        break\n                # construct donor\n                xr1 = union[r1]\n                xr2 = union[r2]\n                xpi = pop[p_best_idx]\n                # current vector\n                xi = pop[i]\n                donor = xi + F * (xpi - xi) + F * (xr1 - xr2)\n                # crossover to produce trial vector\n                mask = self.rng.random(self.dim) < CR\n                if not mask.any():\n                    mask[self.rng.integers(0, self.dim)] = True\n                trial = np.where(mask, donor, xi)\n                trial = np.clip(trial, lb, ub)\n\n                # evaluate trial\n                try:\n                    f_trial = eval_point(trial)\n                except StopIteration:\n                    break\n\n                # selection\n                if f_trial <= f_vals[i]:\n                    new_pop[i] = trial\n                    new_f[i] = f_trial\n                    # store successful F/CR and improvement amount for memory update\n                    successful_F.append(F)\n                    successful_CR.append(CR)\n                    delta_f = max(1e-12, f_vals[i] - f_trial)\n                    delta_f_list.append(delta_f)\n                    successful_diffs.append((xi - trial).copy())\n                    # add xi to archive for diversity (JADE style)\n                    archive = np.vstack([archive, xi.reshape(1, -1)]) if archive.size else xi.reshape(1, -1)\n                    # update per-dimension sigma: reward dims that changed and improved\n                    diff = np.abs(xi - trial)\n                    # normalized diff per-dim relative to bounds\n                    rel = diff / (bounds_width + 1e-12)\n                    # increase sigma_dim in dims that changed proportionally to improvement\n                    if np.sum(rel) > 0:\n                        sigma_dim += 0.1 * (rel / (np.mean(rel) + 1e-12)) * (delta_f / (abs(f_vals[i]) + 1e-12))\n                        # clip sigma\n                        sigma_dim = np.clip(sigma_dim, 1e-7, 1.0)\n                else:\n                    # small decay on failure for dims where trial tried to change\n                    diff = np.abs(pop[i] - trial)\n                    rel = diff / (bounds_width + 1e-12)\n                    sigma_dim = np.maximum(1e-7, sigma_dim * (1.0 - 0.03 * rel))\n                    # optionally replace some archive entries randomly to maintain diversity\n                    if archive.shape[0] > 0 and self.rng.random() < 0.02:\n                        ridx = self.rng.integers(0, archive.shape[0])\n                        archive[ridx] = trial\n\n                # update best\n                if f_trial < f_best:\n                    f_best = f_trial\n                    x_best = trial.copy()\n                    improved = True\n\n                # keep archive at reasonable size\n                max_archive = max(5, pop_n)\n                if archive.shape[0] > max_archive:\n                    # remove random entries\n                    rem = archive.shape[0] - max_archive\n                    if rem > 0:\n                        keep_idx = np.setdiff1d(np.arange(archive.shape[0]), self.rng.choice(np.arange(archive.shape[0]), size=rem, replace=False))\n                        archive = archive[keep_idx]\n\n            # apply new population\n            pop = new_pop\n            f_vals = new_f\n\n            # memory update for JADE: Lehmer mean weighted by delta_f\n            if len(successful_F) > 0:\n                sF = np.array(successful_F)\n                sCR = np.array(successful_CR)\n                w = np.array(delta_f_list)\n                w /= np.sum(w)\n                # Lehmer mean for F\n                num = np.sum(w * sF * sF)\n                den = np.sum(w * sF) + 1e-12\n                new_mu_F = num / den\n                new_mu_CR = np.sum(w * sCR)\n                # update memory at position mem_pos\n                mem_F[mem_pos] = (1 - self.c_adapt) * mem_F[mem_pos] + self.c_adapt * new_mu_F\n                mem_CR[mem_pos] = (1 - self.c_adapt) * mem_CR[mem_pos] + self.c_adapt * new_mu_CR\n                mem_pos = (mem_pos + 1) % H\n\n            # parabolic directional probe around best (budget-aware)\n            if (evals - last_parabolic_at) >= self.parabolic_every and evals < self.budget:\n                last_parabolic_at = evals\n                # choose a random direction (diff of two pop points or random Gaussian)\n                if pop.shape[0] >= 2:\n                    a, b = self.rng.choice(pop.shape[0], size=2, replace=False)\n                    direction = pop[a] - pop[b]\n                else:\n                    direction = self.rng.normal(size=self.dim)\n                nd = np.linalg.norm(direction)\n                if nd < 1e-12:\n                    direction = self.rng.normal(size=self.dim)\n                    nd = np.linalg.norm(direction)\n                direction = direction / nd\n                # radii relative to bounds: use median sigma_dim magnitude\n                base = max(1e-8, float(np.median(sigma_dim))) * np.linalg.norm(bounds_width)\n                r1 = base * (0.6 + 0.8 * self.rng.random())\n                r2 = base * (1.2 + 1.0 * self.rng.random())\n                # two probe points\n                p1 = np.clip(x_best + direction * r1, lb, ub)\n                p2 = np.clip(x_best + direction * r2, lb, ub)\n                # evaluate sequentially if budget permits\n                probes = []\n                for p in (p1, p2):\n                    if evals >= self.budget:\n                        break\n                    try:\n                        fp = eval_point(p)\n                    except StopIteration:\n                        break\n                    probes.append((p.copy(), fp))\n                    if fp < f_best:\n                        f_best = fp\n                        x_best = p.copy()\n                        improved = True\n                # if we have both probes and the center best f, fit parabola along 1D r to estimate min\n                if len(probes) == 2:\n                    f0 = f_best\n                    f1 = probes[0][1]\n                    f2 = probes[1][1]\n                    # radii values\n                    x0 = 0.0\n                    x1 = r1\n                    x2 = r2\n                    # fit parabola ax^2+bx+c through (x0,f0),(x1,f1),(x2,f2)\n                    denom = (x1 - x0) * (x2 - x0) * (x2 - x1)\n                    if abs(denom) > 1e-18:\n                        A = ( (f2 - f0)/(x2 - x0) - (f1 - f0)/(x1 - x0) ) / (x2 - x1)\n                        if A != 0:\n                            x_min = - ( (f1 - f0)/(x1 - x0) - A*(x1 + x0) ) / (2*A) if False else None\n                            # robust formula: vertex of parabola from quadratic fit via Lagrange or direct coefficients:\n                            # compute coefficients by solving 3x3 small system (stable enough)\n                            M = np.array([[x0*x0, x0, 1.0],\n                                          [x1*x1, x1, 1.0],\n                                          [x2*x2, x2, 1.0]])\n                            y = np.array([f0, f1, f2])\n                            try:\n                                a_coef, b_coef, c_coef = np.linalg.solve(M, y)\n                                if a_coef != 0:\n                                    x_min = -b_coef / (2.0 * a_coef)\n                                else:\n                                    x_min = None\n                            except np.linalg.LinAlgError:\n                                x_min = None\n\n                            if x_min is not None:\n                                # clamp x_min to between min and max of probes times a safety factor\n                                xmin_clamped = np.clip(x_min, min(0.0, x1, x2) - 0.1 * max(1.0, abs(x1), abs(x2)),\n                                                       max(0.0, x1, x2) + 0.1 * max(1.0, abs(x1), abs(x2)))\n                                cand = np.clip(x_best + direction * xmin_clamped, lb, ub)\n                                if evals < self.budget:\n                                    try:\n                                        f_cand = eval_point(cand)\n                                    except StopIteration:\n                                        f_cand = np.inf\n                                    if f_cand < f_best:\n                                        f_best = f_cand\n                                        x_best = cand.copy()\n                                        improved = True\n                                        # inject into population by replacing worst\n                                        worst = int(np.argmax(f_vals))\n                                        pop[worst] = cand.copy()\n                                        f_vals[worst] = f_cand\n                                        # shrink sigma slightly to refine\n                                        sigma_dim = np.maximum(1e-8, sigma_dim * 0.95)\n\n            # periodic small gaussian refinement around best\n            if (evals - last_local_gauss_at) >= self.local_gauss_every and evals < self.budget:\n                last_local_gauss_at = evals\n                nloc = min(self.local_gauss_samples, self.budget - evals)\n                # use per-dim sigma for local sampling\n                local_scale = np.maximum(1e-12, sigma_dim * 0.5) * bounds_width\n                # sample batch\n                if nloc > 0:\n                    samples = x_best + self.rng.normal(scale=local_scale, size=(nloc, self.dim))\n                    samples = np.clip(samples, lb, ub)\n                    for s in samples:\n                        if evals >= self.budget:\n                            break\n                        try:\n                            fs = eval_point(s)\n                        except StopIteration:\n                            break\n                        if fs < f_best:\n                            f_best = fs\n                            x_best = s.copy()\n                            improved = True\n                            # replace worst\n                            worst = int(np.argmax(f_vals))\n                            pop[worst] = s.copy()\n                            f_vals[worst] = fs\n                            # slightly increase sigma on success\n                            sigma_dim = np.minimum(1.0, sigma_dim * 1.05)\n\n            # stagnation handling: if no improvement, enforce rediversify using small random injections\n            if not improved:\n                stagnation += 1\n            else:\n                stagnation = 0\n\n            if stagnation >= 6 and evals < self.budget:\n                # rediversify a small fraction of population\n                nrep = max(1, int(0.2 * pop.shape[0]))\n                for _ in range(nrep):\n                    if evals >= self.budget:\n                        break\n                    idx = int(self.rng.integers(0, pop.shape[0]))\n                    new_x = self.rng.uniform(lb, ub)\n                    try:\n                        f_new = eval_point(new_x)\n                    except StopIteration:\n                        break\n                    # replace worst if better or replace random with some prob\n                    worst = int(np.argmax(f_vals))\n                    if f_new < f_vals[worst] or self.rng.random() < 0.15:\n                        pop[worst] = new_x\n                        f_vals[worst] = f_new\n                        # refresh sigma at that index (reflected by global sigma change)\n                        sigma_dim = np.minimum(1.0, sigma_dim * 1.02)\n                        if f_new < f_best:\n                            f_best = f_new\n                            x_best = new_x.copy()\n                stagnation = 0\n\n            # lightly decay sigma to move towards exploitation slowly if budget decreasing\n            sigma_dim = np.maximum(1e-8, sigma_dim * (1.0 - 0.0005))\n\n            # keep archive bounded\n            if archive.shape[0] > max(5, pop_n * 3):\n                # randomly prune\n                keep = self.rng.choice(archive.shape[0], size=max(5, pop_n), replace=False)\n                archive = archive[keep]\n\n            # safety: break if budget exhausted (loop condition will handle)\n            if evals >= self.budget:\n                break\n\n        # final best may not be in pop; ensure we report best found\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best, dtype=float)\n        self.evals = int(evals)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveCauchyDifferentialLocalSearchV2 scored 0.623 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["63eda163-e0c8-4f20-9d5c-14035b1a6900"], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9174381790060083}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9121696960739151}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9123277881014654}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8135352188993}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.8385096628771826}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.853019785808916}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.31501608097383205}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.4038558233690458}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.11866795173481837}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.1225109176691005}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.1103221159827874}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.17608717474469027}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9590130394807097}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9720735521664711}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9185140416627235}], "aucs": [0.9174381790060083, 0.9121696960739151, 0.9123277881014654, 0.8135352188993, 0.8385096628771826, 0.853019785808916, 0.31501608097383205, 0.4038558233690458, 0.11866795173481837, 0.1225109176691005, 0.1103221159827874, 0.17608717474469027, 0.9590130394807097, 0.9720735521664711, 0.9185140416627235]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-BBOB-2", "seed": 2}
{"method_name": "ES", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "0918a7d7-29bd-4e9f-87ca-6c62a3a09fd5", "fitness": 0.7532578784608799, "name": "DOEPSE_X", "description": "DOEPSE‑X+ — enhanced Directional Orthogonal Ensemble with adaptive operator bandit, consensus‑recombination from pooled curvature, and improved trust scheduling for robust global/local balance.", "code": "import numpy as np\n\nclass DOEPSE_X:\n    \"\"\"\n    DOEPSE-X (refined): Directional Orthogonal Ensemble with adaptive operator\n    selection, consensus recombination using pooled curvature, and enhanced trust.\n\n    Required:\n    - budget: int, maximum number of function evaluations\n    - dim: int, problem dimensionality\n\n    Optional:\n    - pop: number of anchor points (default 12)\n    - seed: RNG seed (default None)\n    - init_samples: number of initial evaluations (default = pop)\n    - mem_size: memory for recent successful directions (default 60)\n    - base_scale: trust radius fraction of domain (default 0.16)\n    - max_subspace: maximum subspace dimension (default 8)\n    - p_global: base probability of heavy global jump (default 0.06)\n    - levy_df: degrees of freedom for Student-t heavy tail (default 2.5)\n    - block_fraction: fraction of coordinates for coordinate tweaks (default 0.14)\n    - reg: small regularizer for computations (default 1e-8)\n    - stagnation_replace: fraction of budget to consider stagnation reseed (default 0.12)\n    - bandit_lr: learning rate for operator bandit adaptation (default 0.12)\n    \"\"\"\n    def __init__(self, budget, dim, pop=12, seed=None, init_samples=None,\n                 mem_size=60, base_scale=0.16, max_subspace=8,\n                 p_global=0.06, levy_df=2.5, block_fraction=0.14,\n                 reg=1e-8, stagnation_replace=0.12, bandit_lr=0.12):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.pop = max(4, int(pop))\n        self.init_samples = self.pop if init_samples is None else int(init_samples)\n        self.rng = np.random.default_rng(seed)\n        self.mem_size = int(mem_size)\n        self.base_scale = float(base_scale)\n        self.max_subspace = max(1, int(max_subspace))\n        self.p_global = float(p_global)\n        self.levy_df = float(levy_df)\n        self.block_fraction = float(block_fraction)\n        self.reg = float(reg)\n        self.stagnation_replace = float(stagnation_replace)\n        self.bandit_lr = float(bandit_lr)\n\n        # results\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        rng = self.rng\n        dim = self.dim\n        budget = self.budget\n\n        lb = np.full(dim, -5.0)\n        ub = np.full(dim, 5.0)\n        span = ub - lb\n        span_mean = float(np.mean(span))\n        min_scale_abs = 1e-9 * span_mean\n\n        evals = 0\n        cache = {}\n\n        def clamp(x):\n            return np.minimum(np.maximum(x, lb), ub)\n\n        def key_of(x):\n            return np.asarray(x, dtype=np.float64).tobytes()\n\n        def eval_one(x):\n            nonlocal evals\n            x = np.asarray(x, dtype=float)\n            x = clamp(x)\n            key = key_of(x)\n            if key in cache:\n                return cache[key], x\n            if evals >= budget:\n                raise RuntimeError(\"Evaluation budget exceeded\")\n            fx = float(func(x))\n            evals += 1\n            cache[key] = fx\n            if fx < self.f_opt or self.x_opt is None:\n                self.f_opt = fx\n                self.x_opt = x.copy()\n            return fx, x\n\n        # Initialize anchors\n        anchors = []\n        n_init = min(self.init_samples, budget)\n        # stratified pseudo-Latin initial sampling for spread\n        if n_init > 1:\n            # generate n_init stratified points in each dimension\n            strata = rng.uniform(size=(n_init, dim))\n            perm = np.arange(n_init)\n            for d in range(dim):\n                rng.shuffle(perm)\n                strata[:, d] = (perm + strata[:, d]) / float(n_init)\n            pts = lb + strata * (ub - lb)\n        else:\n            pts = rng.uniform(lb, ub, size=(n_init, dim))\n\n        for i in range(n_init):\n            x0 = pts[i]\n            try:\n                f0, x0 = eval_one(x0)\n            except RuntimeError:\n                break\n            anchors.append({\n                'x': x0.copy(),\n                'f': f0,\n                'scale': span_mean * self.base_scale * (0.75 + 0.5 * rng.random()),\n                'age': 0,\n                'widen': 0,\n                'diag_h': np.full(dim, 1e-6),  # quasi-diagonal curvature memory\n                'succ': 1.0  # success score for simple anchor weighting\n            })\n            if evals >= budget:\n                break\n\n        # fill remaining anchors if budget remains\n        while len(anchors) < self.pop and evals < budget:\n            x0 = rng.uniform(lb, ub)\n            try:\n                f0, x0 = eval_one(x0)\n            except RuntimeError:\n                break\n            anchors.append({\n                'x': x0.copy(),\n                'f': f0,\n                'scale': span_mean * self.base_scale * (0.75 + 0.4 * rng.random()),\n                'age': 0,\n                'widen': 0,\n                'diag_h': np.full(dim, 1e-6),\n                'succ': 1.0\n            })\n\n        if evals >= budget:\n            return self.f_opt, self.x_opt\n\n        # Directional memory\n        dir_mem = []\n\n        # Operator bandit setup - adaptive selection among operators\n        operators = ['subspace_quad', 'coord_tweak', 'anisotropic_mut', 'consensus_proposal', 'heavy_jump']\n        op_stats = {op: {'score': 1.0, 'trials': 1, 'successes': 0} for op in operators}\n\n        def affordable(k):\n            return (budget - evals) >= k\n\n        def choose_anchor():\n            # weighted sampling by softmin of f and anchor success score\n            fs = np.array([a['f'] for a in anchors], dtype=float)\n            best = fs.min()\n            temp = max(1e-6, np.std(fs))\n            rel = np.exp(-np.clip(fs - best, 0, 50) / (0.8 * temp + 1e-12))\n            succ = np.array([a.get('succ', 1.0) for a in anchors], dtype=float)\n            p = rel * (0.6 + 0.4 * (succ / (succ.mean() + 1e-12)))\n            p = p / p.sum()\n            return int(rng.choice(len(anchors), p=p))\n\n        def replace_worst_if_better(x_try, f_try):\n            worst_idx = int(np.argmax([a['f'] for a in anchors]))\n            if f_try < anchors[worst_idx]['f']:\n                anchors[worst_idx] = {'x': x_try.copy(), 'f': f_try,\n                                      'scale': span_mean * self.base_scale * (0.75 + 0.25 * rng.random()),\n                                      'age': 0, 'widen': 0, 'diag_h': np.full(dim, 1e-6),\n                                      'succ': 1.0}\n                return True\n            return False\n\n        # robust parabolic 3-point fit (from original, slightly tightened)\n        def parabolic_three_point(fm, f0, fp, sm, sp):\n            sm = float(sm); sp = float(sp)\n            if abs(sm - sp) < 1e-12:\n                s = sm\n                a = (fp + fm - 2.0 * f0) / (2.0 * s * s)\n                b = (fp - fm) / (2.0 * s)\n                c = f0\n            else:\n                A = np.array([[sm * sm, -sm, 1.0],\n                              [0.0, 0.0, 1.0],\n                              [sp * sp, sp, 1.0]], dtype=float)\n                y = np.array([fm, f0, fp], dtype=float)\n                try:\n                    sol = np.linalg.solve(A, y)\n                    a, b, c = sol[0], sol[1], sol[2]\n                except Exception:\n                    return None\n            if abs(a) < 1e-16:\n                return None\n            t_star = -b / (2.0 * a)\n            f_star = a * t_star * t_star + b * t_star + c\n            return {'a': float(a), 'b': float(b), 'c': float(c),\n                    't_star': float(t_star), 'f_star': float(f_star)}\n\n        # stable orthonormal basis builder biased by recent successful directions\n        def build_local_basis(anchor_idx, k):\n            anchor = anchors[anchor_idx]\n            x0 = anchor['x']\n            vecs = []\n            # include directions to several good anchors\n            sorted_idx = np.argsort([a['f'] for a in anchors])\n            for idx in sorted_idx[:min(6, len(anchors))]:\n                if idx == anchor_idx:\n                    continue\n                dv = anchors[idx]['x'] - x0\n                nrm = np.linalg.norm(dv)\n                if nrm > 1e-12:\n                    vecs.append(dv / nrm)\n                if len(vecs) >= k:\n                    break\n            # use recent direction memory with bias toward recent successes\n            for v in reversed(dir_mem[-self.mem_size:]):\n                if len(vecs) >= k:\n                    break\n                vecs.append(v)\n            # random fill with slight bias to coordinate axes if dim small\n            while len(vecs) < max(1, k):\n                v = rng.normal(size=dim)\n                nrm = np.linalg.norm(v)\n                if nrm < 1e-12:\n                    continue\n                vecs.append(v / nrm)\n            M = np.vstack(vecs)\n            # orthonormalize using QR of centered set for numerical stability\n            try:\n                Q, _ = np.linalg.qr((M - M.mean(axis=0, keepdims=True)).T)\n                B = Q[:, :k]\n            except Exception:\n                # fallback SVD\n                try:\n                    U, S, Vt = np.linalg.svd(M - M.mean(axis=0, keepdims=True), full_matrices=False)\n                    B = Vt.T[:, :k]\n                except Exception:\n                    # Gram-Schmidt fallback\n                    Bcols = []\n                    for v in vecs:\n                        vv = np.array(v, dtype=float)\n                        for u in Bcols:\n                            vv = vv - (u @ vv) * u\n                        nv = np.linalg.norm(vv)\n                        if nv > 1e-12:\n                            Bcols.append(vv / nv)\n                        if len(Bcols) >= k:\n                            break\n                    if len(Bcols) == 0:\n                        v = rng.normal(size=dim)\n                        v = v / (np.linalg.norm(v) + 1e-12)\n                        B = np.atleast_2d(v).T\n                    else:\n                        B = np.column_stack(Bcols[:k])\n            # ensure correct shape\n            if B.shape[1] < k:\n                extra = k - B.shape[1]\n                for _ in range(extra):\n                    v = rng.normal(size=dim)\n                    for i in range(B.shape[1]):\n                        v = v - (B[:, i] @ v) * B[:, i]\n                    nv = np.linalg.norm(v)\n                    if nv < 1e-12:\n                        v = rng.normal(size=dim)\n                        nv = np.linalg.norm(v) + 1e-12\n                    B = np.column_stack((B, v / nv))\n            return B\n\n        # Subspace quadratic (similar to original) but returns predicted decrease for operator bandit reward\n        def subspace_quadratic_step(anchor_idx, m):\n            nonlocal evals\n            anchor = anchors[anchor_idx]\n            x0 = anchor['x'].copy()\n            f0 = anchor['f']\n            scale = max(min_scale_abs, anchor.get('scale', span_mean * self.base_scale))\n            B = build_local_basis(anchor_idx, m)\n            m = min(m, B.shape[1])\n            if not affordable(2 * m + 1):\n                return False, 0.0\n            s_list = []\n            for i in range(m):\n                col = B[:, i]\n                diag_h = anchor.get('diag_h', np.full(dim, 1e-6))\n                approx_h = max(1e-9, float((diag_h * (col ** 2)).sum()))\n                s = scale * (0.35 + 1.2 * rng.random()) / (1.0 + 4.0 * approx_h)\n                s = max(min_scale_abs, float(s))\n                s_list.append(s)\n            a_list = np.zeros(m, dtype=float)\n            b_list = np.zeros(m, dtype=float)\n            valid = np.zeros(m, dtype=bool)\n            f_pred = np.full(m, np.inf)\n\n            for i in range(m):\n                if not affordable(2):\n                    break\n                u = B[:, i]\n                s = s_list[i]\n                xp = clamp(x0 + s * u)\n                xm = clamp(x0 - s * u)\n                try:\n                    fp, _ = eval_one(xp)\n                except RuntimeError:\n                    break\n                if not affordable(1):\n                    break\n                try:\n                    fm, _ = eval_one(xm)\n                except RuntimeError:\n                    break\n                res = parabolic_three_point(fm, f0, fp, s, s)\n                if res is None:\n                    # treat as small curvature but record directional slope\n                    if fm < fp and fm < f0:\n                        t_pred = -s\n                        f_pred[i] = fm\n                        a_list[i] = 1e-8\n                        b_list[i] = (fm - f0) / (-s)\n                        valid[i] = True\n                    elif fp < f0:\n                        t_pred = s\n                        f_pred[i] = fp\n                        a_list[i] = 1e-8\n                        b_list[i] = (fp - f0) / s\n                        valid[i] = True\n                    else:\n                        valid[i] = False\n                else:\n                    a = max(1e-12, float(res['a']))\n                    b = float(res['b'])\n                    tstar = float(res['t_star'])\n                    fstar = float(res['f_star'])\n                    cap = max(3.0 * s, 1.5 * abs(tstar))\n                    if abs(tstar) > cap:\n                        tstar = np.sign(tstar) * cap\n                        fstar = a * tstar * tstar + b * tstar + res['c']\n                    a_list[i] = a\n                    b_list[i] = b\n                    f_pred[i] = fstar\n                    valid[i] = True\n\n            if not np.any(valid):\n                return False, 0.0\n\n            g = b_list.copy()\n            H = np.diag(2.0 * a_list + self.reg)\n            try:\n                t_star = -g / (np.diag(H) + self.reg)\n            except Exception:\n                t_star = -np.linalg.solve(H + np.eye(m) * (self.reg * 10.0), g)\n            step = B[:, :m] @ t_star\n            step_norm = np.linalg.norm(step)\n            max_step = max(scale * 2.2, span_mean * 0.5)\n            if step_norm > max_step and step_norm > 0:\n                step *= (max_step / step_norm)\n                try:\n                    t_star, *_ = np.linalg.lstsq(B[:, :m], step, rcond=None)\n                except Exception:\n                    pass\n\n            x_cand = clamp(x0 + step)\n            if not affordable(1):\n                return False, 0.0\n            try:\n                f_cand, x_cand = eval_one(x_cand)\n            except RuntimeError:\n                return False, 0.0\n\n            preds = np.maximum(0.0, f0 - f_pred[valid])\n            quad_pred = float((g @ t_star) + 0.5 * (t_star @ (H @ t_star)))\n            quad_pred = max(1e-12, -quad_pred)\n            pred_comb = max(1e-12, np.sqrt(max(preds.sum(), 1e-12) * quad_pred))\n\n            actual = max(0.0, f0 - f_cand)\n            ratio = actual / (pred_comb + 1e-12)\n\n            accepted = False\n            if f_cand + 1e-12 < anchor['f']:\n                accepted = True\n                anchors[anchor_idx]['x'] = x_cand.copy()\n                anchors[anchor_idx]['f'] = f_cand\n                anchors[anchor_idx]['age'] = 0\n                # update diag_h\n                for i in range(m):\n                    if valid[i]:\n                        col = B[:, i]\n                        curv = 2.0 * a_list[i]\n                        anchors[anchor_idx]['diag_h'] = anchors[anchor_idx].get('diag_h',\n                                                                                np.full(dim, 1e-6)) * 0.72 + (curv * (col ** 2)) * 0.28\n                # adapt scale and widen\n                if ratio > 0.8:\n                    anchors[anchor_idx]['scale'] = min(span_mean * 2.5, scale * (1.3 + 0.06 * rng.random()))\n                    anchors[anchor_idx]['widen'] = min(self.max_subspace, anchor.get('widen', 0) + 1)\n                elif ratio > 0.25:\n                    anchors[anchor_idx]['scale'] = min(span_mean * 1.4, scale * (1.08 + 0.06 * rng.random()))\n                else:\n                    anchors[anchor_idx]['scale'] = max(min_scale_abs, scale * (0.86 + 0.06 * rng.random()))\n                    anchors[anchor_idx]['widen'] = max(0, anchors[anchor_idx].get('widen', 0) - 1)\n                # record direction\n                dv = x_cand - x0\n                ndv = np.linalg.norm(dv)\n                if ndv > 1e-12:\n                    dvn = dv / ndv\n                    dir_mem.append(dvn)\n                    if len(dir_mem) > self.mem_size:\n                        dir_mem.pop(0)\n                # boost anchor success score\n                anchors[anchor_idx]['succ'] = anchors[anchor_idx].get('succ', 1.0) * 1.06\n                return True, actual\n            else:\n                # occasionally replace worst\n                if replace_worst_if_better(x_cand, f_cand) or rng.random() < 0.04:\n                    return True, max(0.0, anchors[anchor_idx]['f'] - f_cand)\n                anchors[anchor_idx]['scale'] = max(min_scale_abs, scale * (0.82 + 0.06 * rng.random()))\n                anchors[anchor_idx]['age'] = anchors[anchor_idx].get('age', 0) + 1\n                anchors[anchor_idx]['succ'] = max(0.2, anchors[anchor_idx].get('succ', 1.0) * 0.94)\n                return False, 0.0\n\n        # coordinate tweaks (1D parabolic)\n        def local_coordinate_tweak(anchor_idx):\n            nonlocal evals\n            anchor = anchors[anchor_idx]\n            x0 = anchor['x'].copy()\n            f0 = anchor['f']\n            scale = max(min_scale_abs, anchor.get('scale', span_mean * self.base_scale))\n            block_size = max(1, int(np.ceil(self.block_fraction * dim)))\n            coords = rng.choice(dim, size=block_size, replace=False)\n            improved_total = 0.0\n            improved = False\n            for j in coords:\n                if not affordable(2):\n                    break\n                e = np.zeros(dim)\n                e[j] = 1.0\n                s = scale * (0.45 + 0.9 * rng.random())\n                xp = clamp(x0 + s * e)\n                xm = clamp(x0 - s * e)\n                try:\n                    fp, _ = eval_one(xp)\n                except RuntimeError:\n                    break\n                if not affordable(1):\n                    break\n                try:\n                    fm, _ = eval_one(xm)\n                except RuntimeError:\n                    break\n                res = parabolic_three_point(fm, f0, fp, s, s)\n                if res is not None:\n                    t_star = res['t_star']\n                    if abs(t_star) > 3.0 * s:\n                        t_star = np.sign(t_star) * 3.0 * s\n                    x_star = clamp(x0 + t_star * e)\n                    if affordable(1):\n                        try:\n                            f_star, x_star = eval_one(x_star)\n                        except RuntimeError:\n                            f_star = None\n                    else:\n                        f_star = None\n                    candidates = [(fp, xp), (fm, xm)]\n                    if f_star is not None:\n                        candidates.append((f_star, x_star))\n                    best = min(candidates, key=lambda z: z[0])\n                    if best[0] + 1e-12 < anchor['f']:\n                        anchors[anchor_idx]['x'] = best[1].copy()\n                        anchors[anchor_idx]['f'] = best[0]\n                        anchors[anchor_idx]['scale'] = min(span_mean * 1.5, anchor.get('scale', scale) * (1.06 + 0.08 * rng.random()))\n                        dv = best[1] - x0\n                        ndv = np.linalg.norm(dv)\n                        if ndv > 1e-12:\n                            dvn = dv / ndv\n                            dir_mem.append(dvn)\n                            if len(dir_mem) > self.mem_size:\n                                dir_mem.pop(0)\n                        improved_total += (f0 - best[0])\n                        improved = True\n                        x0 = anchors[anchor_idx]['x'].copy()\n                        f0 = anchors[anchor_idx]['f']\n                    else:\n                        anchors[anchor_idx]['scale'] = max(min_scale_abs, anchor.get('scale', scale) * (0.88 + 0.06 * rng.random()))\n                else:\n                    best_sample = min((fp, xp), (fm, xm), key=lambda z: z[0])\n                    if best_sample[0] + 1e-12 < anchor['f']:\n                        anchors[anchor_idx]['x'] = best_sample[1].copy()\n                        anchors[anchor_idx]['f'] = best_sample[0]\n                        anchors[anchor_idx]['scale'] = max(min_scale_abs, anchor.get('scale', scale) * (1.04 + 0.06 * rng.random()))\n                        dv = best_sample[1] - x0\n                        ndv = np.linalg.norm(dv)\n                        if ndv > 1e-12:\n                            dvn = dv / ndv\n                            dir_mem.append(dvn)\n                            if len(dir_mem) > self.mem_size:\n                                dir_mem.pop(0)\n                        improved_total += (f0 - best_sample[0])\n                        improved = True\n                        x0 = anchors[anchor_idx]['x'].copy()\n                        f0 = anchors[anchor_idx]['f']\n                    else:\n                        anchors[anchor_idx]['scale'] = max(min_scale_abs, anchor.get('scale', scale) * (0.9 + 0.05 * rng.random()))\n            if improved:\n                anchors[anchor_idx]['succ'] = anchors[anchor_idx].get('succ', 1.0) * (1.03 + 0.01 * rng.random())\n            else:\n                anchors[anchor_idx]['succ'] = max(0.2, anchors[anchor_idx].get('succ', 1.0) * 0.96)\n            return improved, improved_total\n\n        # anisotropic gaussian mutation around anchor\n        def anisotropic_mutation(anchor_idx):\n            nonlocal evals\n            anchor = anchors[anchor_idx]\n            if not affordable(1):\n                return False, 0.0\n            cov_vecs = np.array(dir_mem[-min(len(dir_mem), 8):]) if len(dir_mem) > 0 else None\n            if cov_vecs is None or cov_vecs.size == 0:\n                z = rng.normal(size=dim)\n            else:\n                mix = np.mean(cov_vecs, axis=0)\n                z = rng.normal(size=dim) + 0.8 * mix\n            scale = anchors[anchor_idx].get('scale', span_mean * self.base_scale) * (0.16 + 0.9 * rng.random())\n            x_try = clamp(anchors[anchor_idx]['x'] + (z / (np.std(z) + 1e-12)) * scale)\n            try:\n                f_try, x_try = eval_one(x_try)\n            except RuntimeError:\n                return False, 0.0\n            if f_try + 1e-12 < anchors[anchor_idx]['f']:\n                anchors[anchor_idx]['x'] = x_try.copy()\n                anchors[anchor_idx]['f'] = f_try\n                anchors[anchor_idx]['scale'] = min(span_mean * 2.0, anchors[anchor_idx].get('scale', span_mean * self.base_scale) * (1.12 + 0.18 * rng.random()))\n                dv = x_try - anchor['x']\n                ndv = np.linalg.norm(dv)\n                if ndv > 1e-12:\n                    dvn = dv / ndv\n                    dir_mem.append(dvn)\n                    if len(dir_mem) > self.mem_size:\n                        dir_mem.pop(0)\n                anchors[anchor_idx]['succ'] = anchors[anchor_idx].get('succ', 1.0) * 1.04\n                return True, (anchor['f'] - f_try)\n            else:\n                # sometimes replace worst\n                worst_idx = int(np.argmax([a['f'] for a in anchors]))\n                if f_try < anchors[worst_idx]['f'] or rng.random() < 0.035:\n                    anchors[worst_idx] = {'x': x_try.copy(), 'f': f_try,\n                                          'scale': max(min_scale_abs, anchors[anchor_idx].get('scale', span_mean * self.base_scale) * 0.6),\n                                          'age': 0, 'widen': 0, 'diag_h': np.full(dim, 1e-6),\n                                          'succ': 1.0}\n                    return True, (anchors[anchor_idx]['f'] - f_try)\n                anchors[anchor_idx]['scale'] = max(min_scale_abs, anchors[anchor_idx].get('scale', span_mean * self.base_scale) * (0.86 + 0.06 * rng.random()))\n                anchors[anchor_idx]['succ'] = max(0.2, anchors[anchor_idx].get('succ', 1.0) * 0.96)\n                return False, 0.0\n\n        # consensus recombination proposal using pooled curvature (cheap: 1 eval)\n        def consensus_proposal():\n            nonlocal evals\n            if not affordable(1):\n                return False, 0.0\n            fs = np.array([a['f'] for a in anchors], dtype=float)\n            best = fs.min()\n            T = max(1e-6, np.std(fs)) + 1e-12\n            weights = np.exp(-np.clip(fs - best, 0, 50) / (0.6 * T + 1e-12))\n            # bias weights using anchor success\n            succ = np.array([a.get('succ', 1.0) for a in anchors], dtype=float)\n            weights *= (0.5 + 0.5 * (succ / (succ.mean() + 1e-12)))\n            weights = weights / weights.sum()\n            center = sum(w * a['x'] for w, a in zip(weights, anchors))\n            # pooled diagonal curvature (weighted)\n            pooled_diag = np.sum([w * a.get('diag_h', np.full(dim, 1e-6)) for w, a in zip(weights, anchors)], axis=0)\n            pooled_diag = np.maximum(pooled_diag, 1e-9)\n            cov_diag = 1.0 / (pooled_diag + self.reg)\n            avg_scale = np.mean([a.get('scale', span_mean * self.base_scale) for a in anchors])\n            # generate anisotropic sample using diag covariance\n            z = rng.normal(size=dim)\n            noise = z * np.sqrt(cov_diag) * (avg_scale * (0.28 + 1.2 * rng.random()))\n            # add directional memory bias occasionally\n            if len(dir_mem) >= 3 and rng.random() < 0.5:\n                mix = np.mean(np.array(dir_mem[-min(len(dir_mem), 8):]), axis=0)\n                noise = noise + 0.8 * avg_scale * 0.2 * mix\n            x_try = clamp(center + noise)\n            # small chance to uniform reseed\n            if rng.random() < 0.04:\n                x_try = rng.uniform(lb, ub)\n            try:\n                f_try, x_try = eval_one(x_try)\n            except RuntimeError:\n                return False, 0.0\n            # replace worst or keep if promising\n            if replace_worst_if_better(x_try, f_try) or (f_try < np.median(fs) and rng.random() < 0.28):\n                return True, max(0.0, min(fs) - f_try)\n            return False, 0.0\n\n        # heavy global jump similar to previous\n        def heavy_global_jump():\n            nonlocal evals\n            if not affordable(1):\n                return False, 0.0\n            best_idx = int(np.argmin([a['f'] for a in anchors]))\n            center = anchors[best_idx]['x']\n            avg_scale = np.mean([a.get('scale', span_mean * self.base_scale) for a in anchors])\n            z = rng.standard_t(self.levy_df, size=dim)\n            if len(dir_mem) >= 3 and rng.random() < 0.6:\n                mix = np.mean(np.array(dir_mem[-min(len(dir_mem), 8):]), axis=0)\n                z = z + 0.6 * mix\n            scale = avg_scale * (0.4 + 3.0 * rng.random()) if rng.random() < 0.18 else avg_scale * (0.5 + 1.2 * rng.random())\n            x_try = clamp(center + (z / (np.std(z) + 1e-12)) * scale)\n            if rng.random() < 0.06:\n                x_try = rng.uniform(lb, ub)\n            try:\n                f_try, x_try = eval_one(x_try)\n            except RuntimeError:\n                return False, 0.0\n            if replace_worst_if_better(x_try, f_try) or rng.random() < 0.12:\n                return True, max(0.0, anchors[best_idx]['f'] - f_try)\n            return False, 0.0\n\n        # Main loop with adaptive operator bandit\n        iter_count = 0\n        last_improve_eval = evals\n        recent_best = self.f_opt\n        stall_window = max(8, int(0.02 * budget))\n        while evals < budget:\n            iter_count += 1\n            remaining = budget - evals\n            fs = np.array([a['f'] for a in anchors], dtype=float)\n            best_idx = int(np.argmin(fs))\n            worst_idx = int(np.argmax(fs))\n            avg_age = float(np.mean([a.get('age', 0) for a in anchors]))\n            stagnation_factor = 1.0 + min(3.0, avg_age / 4.0)\n            pglob = min(0.6, self.p_global * stagnation_factor + 0.02 * rng.random())\n\n            # dynamic reseed if stagnation\n            if (evals - last_improve_eval) > max(10, int(self.stagnation_replace * self.budget)):\n                n_replace = max(1, int(0.12 * self.pop))\n                worst_ord = np.argsort([a['f'] for a in anchors])[::-1][:n_replace]\n                for wi in worst_ord:\n                    if not affordable(1):\n                        break\n                    x_new = rng.uniform(lb, ub)\n                    try:\n                        f_new, x_new = eval_one(x_new)\n                    except RuntimeError:\n                        break\n                    anchors[wi] = {'x': x_new.copy(), 'f': f_new,\n                                   'scale': span_mean * self.base_scale * (0.6 + 0.6 * rng.random()),\n                                   'age': 0, 'widen': 0, 'diag_h': np.full(dim, 1e-6),\n                                   'succ': 1.0}\n                last_improve_eval = evals\n\n            # global jump with some probability influenced by stagnation\n            if rng.random() < pglob and affordable(1):\n                ok, gain = heavy_global_jump()\n                # update operator bandit stats\n                op_stats['heavy_jump']['trials'] += 1\n                if ok:\n                    op_stats['heavy_jump']['successes'] += 1\n                    op_stats['heavy_jump']['score'] += 1.0 + 0.5 * (gain / (1.0 + abs(recent_best)))\n                    last_improve_eval = evals\n                for a in anchors:\n                    a['age'] = a.get('age', 0) + 1\n                continue\n\n            # choose operator by softmax over scores\n            scores = np.array([op_stats[op]['score'] / (op_stats[op]['trials'] + 1e-12) for op in operators], dtype=float)\n            probs = np.exp(scores - scores.max())\n            probs = probs / probs.sum()\n            op = rng.choice(operators, p=probs)\n\n            ai = choose_anchor()\n            anchors[ai]['age'] = anchors[ai].get('age', 0) + 1\n\n            # adaptive operator dispatch\n            if op == 'subspace_quad' and affordable(3):\n                # decide subspace size\n                widen = anchors[ai].get('widen', 0)\n                m = 1 + int(min(self.max_subspace - 1, widen + (0 if rng.random() < 0.6 else 0)))\n                if anchors[ai]['f'] < np.median(fs) and rng.random() < 0.45:\n                    m = min(self.max_subspace, m + 1)\n                ok, gain = subspace_quadratic_step(ai, m)\n                op_stats['subspace_quad']['trials'] += 1\n                if ok and gain > 0:\n                    op_stats['subspace_quad']['successes'] += 1\n                    op_stats['subspace_quad']['score'] += 1.0 + 0.6 * (gain / (1.0 + abs(recent_best)))\n                    last_improve_eval = evals\n                else:\n                    op_stats['subspace_quad']['score'] *= (1.0 - 0.02 * self.bandit_lr)\n                continue\n\n            if op == 'coord_tweak':\n                ok, gain = local_coordinate_tweak(ai)\n                op_stats['coord_tweak']['trials'] += 1\n                if ok and gain > 0.0:\n                    op_stats['coord_tweak']['successes'] += 1\n                    op_stats['coord_tweak']['score'] += 0.8 + 0.4 * (gain / (1.0 + abs(recent_best)))\n                    last_improve_eval = evals\n                else:\n                    op_stats['coord_tweak']['score'] *= (1.0 - 0.01 * self.bandit_lr)\n                continue\n\n            if op == 'anisotropic_mut':\n                ok, gain = anisotropic_mutation(ai)\n                op_stats['anisotropic_mut']['trials'] += 1\n                if ok and gain > 0.0:\n                    op_stats['anisotropic_mut']['successes'] += 1\n                    op_stats['anisotropic_mut']['score'] += 0.6 + 0.6 * (gain / (1.0 + abs(recent_best)))\n                    last_improve_eval = evals\n                else:\n                    op_stats['anisotropic_mut']['score'] *= (1.0 - 0.01 * self.bandit_lr)\n                continue\n\n            if op == 'consensus_proposal':\n                ok, gain = consensus_proposal()\n                op_stats['consensus_proposal']['trials'] += 1\n                if ok and gain > 0.0:\n                    op_stats['consensus_proposal']['successes'] += 1\n                    op_stats['consensus_proposal']['score'] += 0.9 + 0.5 * (gain / (1.0 + abs(recent_best)))\n                    last_improve_eval = evals\n                else:\n                    op_stats['consensus_proposal']['score'] *= (1.0 - 0.02 * self.bandit_lr)\n                continue\n\n            # fallback to heavy jump if nothing matched\n            ok, gain = heavy_global_jump()\n            op_stats['heavy_jump']['trials'] += 1\n            if ok and gain > 0.0:\n                op_stats['heavy_jump']['successes'] += 1\n                op_stats['heavy_jump']['score'] += 1.0 + 0.5 * (gain / (1.0 + abs(recent_best)))\n                last_improve_eval = evals\n            else:\n                op_stats['heavy_jump']['score'] *= (1.0 - 0.02 * self.bandit_lr)\n\n            # occasional light housekeeping: lower success staleness\n            if iter_count % 37 == 0:\n                for a in anchors:\n                    a['succ'] = max(0.2, a.get('succ', 1.0) * 0.995)\n\n            # refresh recent_best for bandit scaling\n            recent_best = min(recent_best, self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 24, "feedback": "The algorithm DOEPSE_X scored 0.753 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["75f755c4-63ff-4b60-9624-bc9f377a3f78"], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9382100632650793}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9532459205095452}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9411056805206441}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.7447495975272538}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.7290380297323541}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.6888423745975327}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.5577211778125979}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.5006416382402318}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.546646733481615}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.5246737832265719}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.6842282774646877}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.5317065803088483}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9843791464795741}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9830006380042908}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9906785357423706}], "aucs": [0.9382100632650793, 0.9532459205095452, 0.9411056805206441, 0.7447495975272538, 0.7290380297323541, 0.6888423745975327, 0.5577211778125979, 0.5006416382402318, 0.546646733481615, 0.5246737832265719, 0.6842282774646877, 0.5317065803088483, 0.9843791464795741, 0.9830006380042908, 0.9906785357423706]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-BBOB-5", "seed": 5}
{"method_name": "ES", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "d9fe439b-2d4e-4c80-9ad4-87943950bf22", "fitness": 0.5570201910350188, "name": "ASRES", "description": "Adaptive Subspace Regression with Ellipsoidal Shrinkage (ASRES) — combine cheap local linear-gradient regression, principal-axis ellipsoidal proposals, per-axis trust radii that adapt from successes/failures, 3‑point directional parabolic refinement and mirrored reflections to quickly focus and exploit promising basins under tight budgets.", "code": "import numpy as np\n\nclass ASRES:\n    \"\"\"\n    Adaptive Subspace Regression with Ellipsoidal Shrinkage (ASRES)\n\n    Main ideas:\n      - Use a small Latin-hypercube initial archive.\n      - Fit a cheap local linear model (finite-sample least squares) on top-k points to estimate a descent direction (approx gradient).\n      - Maintain an ellipsoidal trust region via per-axis trust radii derived from weighted covariance of good points.\n      - Generate proposals from a mixture: principal axes of the ellipsoid, gradient-descent direction, random orthonormal directions, coordinate jumps, and ellipsoid-sampled global probes.\n      - For directional proposals use inexpensive 3-point parabolic probing (center +/- s along direction then 1D minimizer).\n      - Adapt per-axis trust radii: enlarge on success, shrink on failure; occasionally reflect successful steps across the current best to exploit structure.\n      - Maintain archive with best points and enforce strict function-evaluation budget.\n    \"\"\"\n    def __init__(self, budget, dim,\n                 n_init=None,\n                 per_iter=8,\n                 rng=None,\n                 scales=None,\n                 max_archive=3000,\n                 trust_init=0.25,\n                 trust_grow=1.2,\n                 trust_shrink=0.7,\n                 topk=12):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.per_iter = max(1, int(per_iter))\n        if n_init is None:\n            # keep initial samples modest relative to budget and dimension\n            self.n_init = max(4, min(40, self.budget // 10, 6 * self.dim))\n        else:\n            self.n_init = int(n_init)\n        self.rng = np.random.default_rng() if rng is None else (rng if isinstance(rng, np.random.Generator) else np.random.default_rng(rng))\n        self.scales = np.array(scales) if scales is not None else np.array([0.02, 0.08, 0.28, 0.9])\n        self.max_archive = int(max_archive)\n        self.trust_init = float(trust_init)\n        self.trust_grow = float(trust_grow)\n        self.trust_shrink = float(trust_shrink)\n        self.topk = int(topk)\n\n        # state\n        self.f_opt = None\n        self.x_opt = None\n\n    def __call__(self, func):\n        # bounds check / fallback\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float)\n            ub = np.asarray(func.bounds.ub, dtype=float)\n            if lb.size == 1:\n                lb = np.full(self.dim, lb.item())\n            if ub.size == 1:\n                ub = np.full(self.dim, ub.item())\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        lb = lb.reshape(self.dim)\n        ub = ub.reshape(self.dim)\n        span = ub - lb\n        if np.any(span <= 0):\n            raise ValueError(\"Invalid bounds\")\n        span_mean = float(np.mean(span))\n\n        # strict budget-aware evaluator\n        evals = 0\n        def eval_x_raw(x):\n            nonlocal evals\n            if evals >= self.budget:\n                raise RuntimeError(\"budget exhausted\")\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            x = np.minimum(np.maximum(x, lb), ub)\n            f = func(x)\n            evals += 1\n            return float(f), x\n\n        # small helper: clipped gaussian for proposals\n        def clip(x):\n            x = np.asarray(x, dtype=float).reshape(self.dim)\n            return np.minimum(np.maximum(x, lb), ub)\n\n        # initialize archive via latin-hypercube-like sampling\n        n_init = min(self.n_init, max(1, self.budget - 1))\n        X_list = []\n        F_list = []\n        if n_init > 0:\n            base = np.zeros((n_init, self.dim))\n            for d in range(self.dim):\n                perm = self.rng.permutation(n_init)\n                base[:, d] = (perm + self.rng.random(n_init)) / float(n_init)\n            for i in range(n_init):\n                if evals >= self.budget:\n                    break\n                x0 = lb + base[i] * span\n                try:\n                    f0, x0c = eval_x_raw(x0)\n                except RuntimeError:\n                    break\n                X_list.append(x0c.copy())\n                F_list.append(float(f0))\n        if len(F_list) == 0:\n            # can't evaluate anything\n            self.f_opt = np.inf\n            self.x_opt = lb.copy()\n            return self.f_opt, self.x_opt\n\n        X = np.vstack(X_list)\n        F = np.array(F_list)\n\n        # best so far\n        best_idx = int(np.argmin(F))\n        x_best = X[best_idx].copy()\n        f_best = float(F[best_idx])\n\n        # per-axis trust radii (relative to span_mean)\n        trusts = np.full(self.dim, self.trust_init * span_mean)\n        # keep small lower bound\n        min_trust = 1e-6 * span_mean\n\n        stagnation = 0\n        iteration = 0\n\n        # helper: compute weighted covariance and principal axes from top-k points\n        def ellipsoid_axes(center, k=None):\n            if X.shape[0] <= 2:\n                # fallback: orthonormal random basis\n                Q, _ = np.linalg.qr(self.rng.normal(size=(self.dim, self.dim)))\n                scales = np.ones(self.dim) * span_mean * 0.5\n                return Q, scales\n            k = k or self.topk\n            idx = np.argsort(F)[:min(k, X.shape[0])]\n            Y = X[idx]\n            # weights: better points get larger weight\n            w = np.maximum(0.0, (np.max(F[idx]) - F[idx]) + 1e-12)\n            w = w / (np.sum(w) + 1e-12)\n            mean = np.sum(Y * w[:, None], axis=0)\n            Z = (Y - mean)\n            # weighted covariance\n            C = (Z.T * w) @ Z + 1e-8 * np.eye(self.dim)\n            try:\n                S, V = np.linalg.eigh(C)  # S ascending\n                order = np.argsort(-S)\n                S = S[order]\n                V = V[:, order]\n                axes_scales = np.sqrt(np.maximum(S, 1e-12))\n            except Exception:\n                V = np.eye(self.dim)\n                axes_scales = np.ones(self.dim) * span_mean * 0.5\n            # axes_scales -> absolute scale suggestion\n            return V, axes_scales\n\n        # helper: estimate a cheap local gradient by linear regression on top-k points around center\n        def approx_grad(center, k=None):\n            if X.shape[0] <= 3:\n                return None\n            k = k or self.topk\n            # pick nearest k\n            dists = np.linalg.norm(X - center.reshape(1, -1), axis=1)\n            idx = np.argsort(dists)[:min(k, X.shape[0])]\n            Y = X[idx] - center.reshape(1, -1)\n            dy = F[idx] - float(center_f)  # center_f must be available in closure\n            # solve Y g = dy in least squares (overdetermined or small)\n            if np.linalg.matrix_rank(Y) < min(Y.shape):\n                # regularize\n                reg = 1e-6 * np.mean(np.var(Y, axis=0) + 1e-12)\n            else:\n                reg = 0.0\n            try:\n                if reg > 0:\n                    A = Y.T @ Y + reg * np.eye(self.dim)\n                    b = Y.T @ dy\n                    g = np.linalg.solve(A, b)\n                else:\n                    g, *_ = np.linalg.lstsq(Y, dy, rcond=None)\n                    g = g\n                return g\n            except Exception:\n                return None\n\n        # heavy mirror step: reflect new good points across the best to test symmetric structure\n        def mirror_across_best(center, newpt, factor=1.0):\n            # reflect newpt around best: x_ref = x_best + factor*(x_best - center)\n            x_ref = x_best + factor * (x_best - center)\n            return clip(x_ref)\n\n        # main loop\n        while evals < self.budget:\n            iteration += 1\n\n            # choose center: usually the current best, occasionally a sampled good archive point\n            if self.rng.random() < 0.8 or X.shape[0] < 6:\n                center = x_best.copy()\n                center_f = f_best\n            else:\n                K = min(self.topk, X.shape[0])\n                idxs = np.argsort(F)[:K]\n                # prefer better ones but with a soft selection\n                scores = -F[idxs]\n                if np.std(scores) < 1e-12:\n                    p = np.ones_like(scores) / len(scores)\n                else:\n                    ex = np.exp((scores - np.max(scores)) / (0.5 * (np.std(scores) + 1e-12)))\n                    p = ex / (np.sum(ex) + 1e-12)\n                pick = self.rng.choice(len(idxs), p=p)\n                idx = idxs[pick]\n                center = X[idx].copy()\n                center_f = float(F[idx])\n\n            # prepare ellipsoid axes and gradient approx\n            V, axes_scales = ellipsoid_axes(center, k=self.topk)\n            # combine axes_scales with current trusts to get absolute proposals\n            axis_abs_scales = np.maximum(axes_scales * span_mean * 0.5, min_trust)\n            # approximate gradient (need center_f in closure)\n            try:\n                # ensure center measurement exists in archive; if not, evaluate it\n                presence = np.allclose(X, center.reshape(1, -1), atol=1e-12).any()\n            except Exception:\n                presence = False\n            if not presence and evals < self.budget:\n                try:\n                    cf, center = eval_x_raw(center)\n                    center_f = float(cf)\n                    X = np.vstack([X, center])\n                    F = np.concatenate([F, np.array([center_f])])\n                    if center_f < f_best:\n                        f_best = center_f\n                        x_best = center.copy()\n                except RuntimeError:\n                    break\n\n            g = approx_grad(center, k=self.topk)\n            # build a pool of directions to try this iteration\n            directions = []\n            # 1) negative approximate gradient if available\n            if g is not None and np.linalg.norm(g) > 1e-12:\n                dg = -g\n                directions.append(dg / (np.linalg.norm(dg) + 1e-12))\n            # 2) principal axes directions (first few)\n            n_axes = min(self.dim, max(1, int(np.ceil(self.per_iter / 3))))\n            for j in range(n_axes):\n                v = V[:, j]\n                directions.append(v / (np.linalg.norm(v) + 1e-12))\n                directions.append(-v / (np.linalg.norm(v) + 1e-12))\n            # 3) a few random orthonormal directions\n            R = self.rng.normal(size=(self.dim, min(self.per_iter, 3)))\n            Q, _ = np.linalg.qr(R)\n            for j in range(min(Q.shape[1], 3)):\n                directions.append(Q[:, j] / (np.linalg.norm(Q[:, j]) + 1e-12))\n            # 4) coordinate biased directions\n            kcoords = max(1, min(self.dim, int(1 + self.rng.poisson(1.2))))\n            idxs = self.rng.choice(self.dim, size=kcoords, replace=False)\n            v = np.zeros(self.dim)\n            v[idxs] = 1.0\n            directions.append(v / (np.linalg.norm(v) + 1e-12))\n            # deduplicate directions (normalize and unique by angle)\n            D = []\n            for d in directions:\n                d = np.asarray(d, dtype=float).reshape(self.dim)\n                d = d / (np.linalg.norm(d) + 1e-12)\n                # reject duplicates by angle\n                if not any(np.dot(d, dd) > 1 - 1e-6 or np.dot(d, dd) < -1 + 1e-6 for dd in D):\n                    D.append(d)\n            # clip number of proposals to per_iter and budget\n            proposals = min(len(D), self.per_iter, max(1, self.budget - evals))\n            # rank directions with a simple UCB-like score: prefer directions aligned with trust & variance\n            scores = []\n            for d in D:\n                # score = projection on top axis scales weighted by trusts magnitude\n                proj = np.abs((d @ V) * (axes_scales + 1e-12))\n                score = np.sum(proj * (trusts + 1e-12))\n                # add small random tie-breaker\n                score = score + 0.01 * self.rng.random()\n                scores.append(score)\n            order = np.argsort(-np.array(scores))\n            chosen_dirs = [D[i] for i in order[:proposals]]\n\n            any_improved = False\n            # for each chosen direction do a multi-scale 3-point parabola with trust-per-axis influence\n            for d in chosen_dirs:\n                if evals >= self.budget:\n                    break\n                # choose scale multiplier: combine per-axis average trust projected on d with global scales\n                proj_trust = float(np.sum(np.abs(d) * trusts) / (np.sum(np.abs(d)) + 1e-12))\n                # sample a scale coefficient from scales with bias toward small when trust small\n                scale_coeff = float(self.rng.choice(self.scales))\n                s = float(scale_coeff * max(proj_trust, 0.5 * span_mean))\n                s = max(s, 1e-12)\n\n                # form three probe points: center - s*d, center + s*d\n                x_m = clip(center - d * s)\n                x_p = clip(center + d * s)\n\n                # avoid degenerate clipping: jitter a bit\n                if np.allclose(x_m, center, atol=1e-14):\n                    x_m = clip(center - d * (0.5 * s))\n                if np.allclose(x_p, center, atol=1e-14):\n                    x_p = clip(center + d * (0.5 * s))\n\n                # evaluate two side probes if budget allows\n                if evals >= self.budget:\n                    break\n                try:\n                    f_m, xm = eval_x_raw(x_m)\n                except RuntimeError:\n                    break\n                if evals >= self.budget:\n                    # append and break\n                    X = np.vstack([X, xm])\n                    F = np.concatenate([F, np.array([f_m])])\n                    break\n                try:\n                    f_p, xp = eval_x_raw(x_p)\n                except RuntimeError:\n                    # store xm and break\n                    X = np.vstack([X, xm])\n                    F = np.concatenate([F, np.array([f_m])])\n                    break\n\n                X = np.vstack([X, xm, xp])\n                F = np.concatenate([F, np.array([f_m, f_p])])\n\n                # fit parabola along t in {-s,0,s}\n                a = (f_p + f_m - 2.0 * center_f) / (2.0 * (s ** 2) + 1e-20)\n                b = (f_p - f_m) / (2.0 * s + 1e-20)\n                if abs(a) > 1e-12:\n                    tstar = -b / (2.0 * a)\n                else:\n                    tstar = -np.sign(b) * 0.8 * s\n                tcap = 3.0 * s\n                tstar = float(np.clip(tstar, -tcap, tcap))\n                x_star = clip(center + d * tstar)\n\n                # evaluate candidate\n                if evals < self.budget:\n                    try:\n                        f_star, x_star_c = eval_x_raw(x_star)\n                    except RuntimeError:\n                        break\n                    X = np.vstack([X, x_star_c])\n                    F = np.concatenate([F, np.array([f_star])])\n                else:\n                    f_star = np.inf\n\n                # find best among center and the probes\n                cand = [(center_f, center.copy()), (f_m, xm.copy()), (f_p, xp.copy()), (f_star, x_star_c.copy())]\n                cand_best = min(cand, key=lambda t: t[0])\n\n                # update global if improved\n                if cand_best[0] < f_best - 1e-15:\n                    # success: enlarge trusts along the direction components\n                    # update trusts proportionally to absolute direction weights\n                    w = np.abs(d)\n                    trusts = trusts * (1.0 + 0.1 * w)  # small global nudge first\n                    # targeted grow: increase axes where d has magnitude\n                    trusts = trusts + self.trust_grow * (w * np.linalg.norm(trusts) * 0.03 + 1e-12)\n                    # ensure within reasonable bounds\n                    trusts = np.clip(trusts, min_trust, 4.0 * span_mean)\n                    f_best = float(cand_best[0])\n                    x_best = cand_best[1].copy()\n                    any_improved = True\n                    stagnation = 0\n                    # mirrored exploitation: try reflecting this center across current best\n                    if evals < self.budget and self.rng.random() < 0.6:\n                        xr = mirror_across_best(center, cand_best[1], factor=1.0 + 0.2 * self.rng.random())\n                        try:\n                            fr, xr_c = eval_x_raw(xr)\n                        except RuntimeError:\n                            fr = np.inf\n                        X = np.vstack([X, xr_c])\n                        F = np.concatenate([F, np.array([fr])])\n                        if fr < f_best:\n                            # further grow trusts along the mirror direction\n                            delta = xr_c - center\n                            if np.linalg.norm(delta) > 1e-12:\n                                w2 = np.abs(delta / (np.linalg.norm(delta) + 1e-12))\n                                trusts = trusts + self.trust_grow * 0.5 * w2 * span_mean\n                                trusts = np.clip(trusts, min_trust, 4.0 * span_mean)\n                            f_best = float(fr)\n                            x_best = xr_c.copy()\n                else:\n                    # failure: shrink trusts on dimensions of d\n                    w = np.abs(d)\n                    trusts = trusts * (1.0 - 0.05 * w)\n                    trusts = np.clip(trusts, min_trust, 4.0 * span_mean)\n                    stagnation += 0.5  # fractional stagnation increment\n\n                # mild local exploitation: if star improved beyond center, attempt a shorter conjugate step\n                if cand_best[0] < center_f - 1e-12 and evals < self.budget:\n                    # step in same direction but reduced\n                    s2 = 0.7 * s\n                    x_follow = clip(center + d * (tstar + np.sign(tstar) * 0.6 * s2))\n                    try:\n                        f_follow, xf = eval_x_raw(x_follow)\n                    except RuntimeError:\n                        f_follow = np.inf\n                    X = np.vstack([X, xf])\n                    F = np.concatenate([F, np.array([f_follow])])\n                    if f_follow < f_best:\n                        f_best = float(f_follow)\n                        x_best = xf.copy()\n                        trusts = trusts * (1.0 + 0.05 * np.abs(d))\n                        trusts = np.clip(trusts, min_trust, 4.0 * span_mean)\n                        stagnation = 0\n                        any_improved = True\n\n            # end chosen directions loop\n\n            # occasional ellipsoidal sample conditioned on trusts for exploration\n            if evals < self.budget and self.rng.random() < 0.18:\n                # create covariance from V and trusts (project trusts to principal axes)\n                # transform trusts into axis variances heuristically\n                axis_var = np.abs(V.T @ trusts) + 1e-12\n                cov = (V * axis_var[None, :]) @ V.T\n                # draw a sample scaled by a random multiplier\n                scale_mult = float(self.rng.choice(self.scales) * 0.6 + 0.4)\n                # use small regularizer for stability\n                try:\n                    z = self.rng.multivariate_normal(np.zeros(self.dim), cov * (scale_mult ** 2) + 1e-12 * np.eye(self.dim))\n                except Exception:\n                    z = self.rng.normal(size=self.dim) * span_mean * 0.1\n                x_e = clip(x_best + z)\n                try:\n                    f_e, xe = eval_x_raw(x_e)\n                except RuntimeError:\n                    break\n                X = np.vstack([X, xe])\n                F = np.concatenate([F, np.array([f_e])])\n                if f_e < f_best:\n                    f_best = float(f_e)\n                    x_best = xe.copy()\n                    trusts = trusts * (1.0 + 0.08)\n                    trusts = np.clip(trusts, min_trust, 4.0 * span_mean)\n                    stagnation = 0\n\n            # maintain archive size\n            if X.shape[0] > self.max_archive:\n                keepk = min(800, X.shape[0])\n                best_idx = np.argsort(F)[:keepk]\n                recent_idx = np.arange(max(0, X.shape[0] - (self.max_archive - keepk)), X.shape[0])\n                keep = np.unique(np.concatenate([best_idx, recent_idx]))\n                X = X[keep]\n                F = F[keep]\n\n            # stagnation handling: if stagnating, do a few global jittered ellipsoid probes\n            if stagnation >= max(4, self.dim // 2) and evals < self.budget:\n                stagnation = 0\n                n_jumps = min(6, self.budget - evals)\n                for _ in range(n_jumps):\n                    if evals >= self.budget:\n                        break\n                    # heavy jitter along principal axes combined with random sign\n                    j = self.rng.integers(0, self.dim)\n                    noise = np.zeros(self.dim)\n                    noise[j] = (self.rng.standard_cauchy() * 0.8 + self.rng.normal() * 0.2) * trusts[j]\n                    x_jump = clip(x_best + noise)\n                    try:\n                        f_jump, xj = eval_x_raw(x_jump)\n                    except RuntimeError:\n                        break\n                    X = np.vstack([X, xj])\n                    F = np.concatenate([F, np.array([f_jump])])\n                    if f_jump < f_best:\n                        f_best = float(f_jump)\n                        x_best = xj.copy()\n                        trusts = trusts * (1.0 + 0.1)\n                        trusts = np.clip(trusts, min_trust, 4.0 * span_mean)\n\n            # small annealing: slowly reduce scales if improving frequently\n            if any_improved:\n                self.scales = np.clip(self.scales * (0.92 + 0.12 * self.rng.random()), 1e-6, 5.0)\n                stagnation = max(0, stagnation - 1)\n            else:\n                stagnation += 0.25\n\n        # finalize\n        self.f_opt = float(f_best)\n        self.x_opt = np.asarray(x_best).copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 22, "feedback": "The algorithm ASRES scored 0.557 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["42af3700-d987-4c91-becb-ce9a633cabbe"], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9487063120586403}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9431710457754121}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9326338042319768}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.6812346535254727}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.6192235123214856}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.6412988648141902}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.1254444602716116}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.13495143744404592}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.15304943343297217}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.06193989662143107}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.04346651417701963}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.1057369240718925}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.988296895435137}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9918692793586871}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.984279831985307}], "aucs": [0.9487063120586403, 0.9431710457754121, 0.9326338042319768, 0.6812346535254727, 0.6192235123214856, 0.6412988648141902, 0.1254444602716116, 0.13495143744404592, 0.15304943343297217, 0.06193989662143107, 0.04346651417701963, 0.1057369240718925, 0.988296895435137, 0.9918692793586871, 0.984279831985307]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-BBOB-3", "seed": 3}
{"method_name": "ES", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "882877a5-668a-497d-bedb-72462a1b0145", "fitness": 0.8603882397336443, "name": "MultiScaleDirectionalMemeticV2_RefinedX", "description": "Adaptive Multi-Scale Directional Memetic V2X — trust-aware, score-driven directional exploration with orthogonalized direction bank, budget-aware compact line-bracketing (parabola+golden fallback), lightweight low-rank quasi-Newton intensification and per-direction success/failure adaptation for fast noiseless box-constrained continuous optimization in [-5,5]^dim.", "code": "import numpy as np\n\nclass MultiScaleDirectionalMemeticV2_RefinedX:\n    \"\"\"\n    Adaptive Multi-Scale Directional Memetic V2X\n\n    Highlights of improvements over the prior variant:\n    - Direction bank uses UCB-like selection (score + exploration bonus) and orthogonalization to maintain diverse, informative directions.\n    - Per-direction trust radius and success/failure counters adaptively scale step multipliers.\n    - Compact, budget-aware bracketed line search: try small symmetric probes, expand geometrically up to a capped budget,\n      fit a parabola when possible; fallback to golden-section refinement over the bracket if parabola is unreliable.\n    - Lightweight low-rank quasi-Newton intensification: maintain a small set of (s,y) pairs from successful steps to form\n      an L-BFGS-like apply_invH on local linear model gradient to propose promising steps.\n    - Budget-aware mode scheduling: early diverse exploration, mid targeted directional exploitation, late quasi-Newton intensification.\n    - Strict budget enforcement and robust handling of boundary clipping.\n    \"\"\"\n\n    def __init__(self, budget=1000, dim=10, rng=None, max_archive=2000, verbose=0):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.rng = rng if isinstance(rng, np.random.Generator) else np.random.default_rng(rng)\n        self.max_archive = int(max_archive)\n        self.verbose = int(verbose)\n\n        # global best\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # adaptation / memory\n        self.dir_bank = []   # entries: {'d':unit, 'score':float, 'age':int, 'mult':float, 'succ':int, 'fail':int}\n        self.cov_diag = np.ones(self.dim) * 1e-3\n        self.recent_steps = []   # s vectors (x_new - x_old) for low-rank updates\n        self.recent_diffs = []   # y vectors (f_old - f_new)\n        self.axis_mult = np.ones(self.dim)  # per-axis multipliers to prioritize coordinate directions\n\n    # small Latin-hypercube-like uniform spread\n    def _lhs_samples(self, n, lb, ub):\n        samples = np.empty((n, self.dim))\n        for j in range(self.dim):\n            perm = self.rng.permutation(n)\n            u = (perm + self.rng.random(size=n)) / n\n            samples[:, j] = lb[j] + u * (ub[j] - lb[j])\n        return samples\n\n    # robust 3-point parabolic minimizer; returns t or None\n    def _parabolic_minimizer(self, ts, fs):\n        try:\n            # ensure arrays are float\n            ts = np.asarray(ts, dtype=float)\n            fs = np.asarray(fs, dtype=float)\n            if ts.size < 3:\n                return None\n            # shift to improve numeric stability\n            t0 = ts.mean()\n            tt = ts - t0\n            # fit quadratic a*tt^2 + b*tt + c\n            coeff = np.polyfit(tt, fs, 2)\n            a, b = coeff[0], coeff[1]\n            if abs(a) < 1e-16:\n                return None\n            tmin = -b / (2.0 * a) + t0\n            return float(tmin)\n        except Exception:\n            return None\n\n    # orthonormalize candidate directions using modified Gram-Schmidt and drop near-collinears\n    def _orthonormalize_unique(self, dirs, tol=1e-3):\n        U = []\n        for v in dirs:\n            v = np.asarray(v, dtype=float)\n            n = np.linalg.norm(v)\n            if n <= 1e-12:\n                continue\n            u = v / n\n            # subtract projections\n            for w in U:\n                u = u - np.dot(w, u) * w\n            nu = np.linalg.norm(u)\n            if nu > tol:\n                U.append(u / nu)\n        return U\n\n    # compute PCA-like principal directions from recent successful steps\n    def _pca_dirs(self, k=3, max_samples=60):\n        if len(self.recent_steps) < 3:\n            return []\n        M = np.vstack(self.recent_steps[-min(len(self.recent_steps), max_samples):])\n        M = M - M.mean(axis=0, keepdims=True)\n        try:\n            U, S, VT = np.linalg.svd(M, full_matrices=False)\n            dirs = []\n            for i in range(min(k, VT.shape[0])):\n                v = VT[i]\n                n = np.linalg.norm(v)\n                if n > 1e-12:\n                    dirs.append(v / n)\n            return dirs\n        except Exception:\n            return []\n\n    def __call__(self, func):\n        # bounds if provided else default [-5,5]\n        try:\n            lb = np.asarray(func.bounds.lb, dtype=float).reshape(self.dim)\n            ub = np.asarray(func.bounds.ub, dtype=float).reshape(self.dim)\n        except Exception:\n            lb = np.full(self.dim, -5.0)\n            ub = np.full(self.dim, 5.0)\n        span = ub - lb\n        if np.any(span <= 0.0):\n            raise ValueError(\"Invalid bounds\")\n\n        evals = 0\n\n        def remaining():\n            return self.budget - evals\n\n        # archives\n        X_archive = []\n        F_archive = []\n\n        def trim_archive():\n            if len(F_archive) > self.max_archive:\n                # simple eviction: remove oldest half of worst region\n                idx = int(np.argmax(F_archive))\n                del X_archive[idx]\n                del F_archive[idx]\n\n        def eval_point(x):\n            nonlocal evals\n            if remaining() <= 0:\n                return None\n            x = np.asarray(x, dtype=float)\n            x = np.minimum(np.maximum(x, lb), ub)\n            try:\n                val = float(func(x))\n            except Exception:\n                # in case function throws, count eval and return large penalty\n                evals += 1\n                val = float(np.inf)\n                X_archive.append(x.copy()); F_archive.append(val)\n                trim_archive()\n                return val\n            evals += 1\n            X_archive.append(x.copy()); F_archive.append(val)\n            trim_archive()\n            if val < self.f_opt:\n                self.f_opt = val; self.x_opt = x.copy()\n            return val\n\n        # initial sampling: small LHS\n        init_n = min(max(4, 2 * self.dim), max(6, self.budget // 30))\n        samples = self._lhs_samples(init_n, lb, ub)\n        for i in range(init_n):\n            if remaining() <= 0:\n                break\n            f = eval_point(samples[i])\n            if f is None:\n                break\n\n        if remaining() <= 0:\n            if self.x_opt is None and len(F_archive) > 0:\n                idx = int(np.argmin(F_archive))\n                return float(F_archive[idx]), X_archive[idx].copy()\n            return float(self.f_opt), self.x_opt.copy()\n\n        # multi-scale schedule (geometric)\n        max_scale = 0.6 * span.max()\n        min_scale = max(1e-6, 1e-5 * span.max())\n        n_scales = max(5, min(12, 4 + self.dim // 3))\n        scales = max_scale * np.exp(np.linspace(0, np.log(min_scale / max_scale), n_scales))\n\n        # control params\n        stagn = 0\n        stagn_threshold = max(30, 12 * self.dim)\n        max_dir_bank = max(10, 3 * self.dim)\n        total_iters = 0\n\n        # helpers: record successes and update direction bank, low-rank pairs\n        def record_success(step_vec, improvement, f_before=None, f_after=None):\n            # step_vec = x_new - x_old\n            sv = np.asarray(step_vec, dtype=float)\n            norm = np.linalg.norm(sv)\n            if norm <= 0:\n                return\n            u = sv / norm\n            # update cov diag (EMAv)\n            self.cov_diag = 0.9 * self.cov_diag + 0.1 * (sv**2 + 1e-12)\n            # small low-rank quasi-Newton memory: (s,y) where y = f_old - f_new (positive improvement)\n            if f_before is not None and f_after is not None:\n                y = float(f_before) - float(f_after)\n                if abs(y) > 1e-12:\n                    self.recent_steps.append(sv.copy())\n                    self.recent_diffs.append(y * np.sign(np.sum(sv)) * (sv / (norm + 1e-12)))  # directional y aligned\n                    # cap memory\n                    if len(self.recent_steps) > 12:\n                        self.recent_steps.pop(0); self.recent_diffs.pop(0)\n            # update/merge into direction bank with orthogonalization policy\n            merged = False\n            for e in self.dir_bank:\n                sim = abs(np.dot(e['d'], u))\n                if sim > 0.9:\n                    # reinforce\n                    e['d'] = (0.85 * e['d'] + 0.15 * u)\n                    e['d'] = e['d'] / max(1e-12, np.linalg.norm(e['d']))\n                    e['mult'] = min(20.0, 1.06 * e['mult'] + 0.04 * norm / (1.0 + norm))\n                    e['score'] = 0.92 * e['score'] + 0.08 * (improvement / (1.0 + norm))\n                    e['succ'] = e.get('succ', 0) + 1\n                    e['age'] = 0\n                    merged = True\n                    break\n            if not merged:\n                self.dir_bank.append({'d': u.copy(), 'score': float(improvement) / (1.0 + norm), 'age': 0,\n                                      'mult': min(10.0, 1.0 + 0.2 * (norm / (1.0 + norm))), 'succ': 1, 'fail': 0})\n            # age and trim\n            for e in self.dir_bank:\n                e['age'] = e.get('age', 0) + 1\n            if len(self.dir_bank) > max_dir_bank:\n                # remove lowest effective score (score / (1 + age penalty))\n                eff = np.array([e['score'] / (1.0 + 0.02 * e.get('age', 0)) for e in self.dir_bank])\n                idx = int(np.argmin(eff))\n                del self.dir_bank[idx]\n            # re-orthonormalize bank occasionally to remove redundancy\n            if len(self.dir_bank) > 1 and np.random.rand() < 0.05:\n                dirs = [e['d'] for e in self.dir_bank]\n                U = self._orthonormalize_unique(dirs, tol=1e-3)\n                # rebuild bank preserving top scores order\n                new_bank = []\n                s_sorted = sorted(self.dir_bank, key=lambda e: -e['score'])\n                for u in U:\n                    # find best matching original entry for stats\n                    best = None\n                    best_sim = -1.0\n                    for e in s_sorted:\n                        sim = abs(np.dot(e['d'], u))\n                        if sim > best_sim:\n                            best_sim = sim; best = e\n                    if best is not None:\n                        new_bank.append({'d': u.copy(), 'score': best['score'], 'age': best.get('age', 0),\n                                         'mult': best.get('mult', 1.0), 'succ': best.get('succ', 0), 'fail': best.get('fail', 0)})\n                self.dir_bank = new_bank\n\n        # simple local linear gradient estimator (weighted regression)\n        def local_linear(center, f_center, scale):\n            if len(F_archive) < max(6, self.dim + 2):\n                return None\n            X = np.asarray(X_archive)\n            F = np.asarray(F_archive)\n            diffs = X - center\n            # normalized distances\n            dists = np.linalg.norm(diffs / (span + 1e-12), axis=1)\n            k = min(len(F), max(self.dim + 2, 3 * self.dim))\n            idxs = np.argsort(dists)[:k]\n            A = diffs[idxs]\n            b = F[idxs] - f_center\n            # weights\n            w = np.exp(- (dists[idxs] / (max(scale, 1e-12)))**2)\n            W = np.sqrt(w)[:, None]\n            Aw = A * W\n            bw = b * W.ravel()\n            ridge = max(1e-8, 1e-4 * (scale / (span.max() + 1e-12)))\n            try:\n                AtA = Aw.T @ Aw + ridge * np.eye(self.dim)\n                rhs = Aw.T @ bw\n                g = np.linalg.solve(AtA, rhs)\n                return g\n            except Exception:\n                return None\n\n        # apply a compact two-loop L-BFGS-like inverse Hessian on small (s,y) memory to approximate invH * g\n        def apply_invH_to_vec(g):\n            m = len(self.recent_steps)\n            if m == 0:\n                return g\n            # store s,y in lists\n            s_list = [np.asarray(s, dtype=float) for s in self.recent_steps]\n            # y stored as scaled directional vector; reconstruct approximate y scalar per s by dot with s\n            y_list = []\n            for i, s in enumerate(s_list):\n                # estimate scalar y as dot(recent_diffs[i], s) / (||s||^2) if possible\n                yvec = self.recent_diffs[i]\n                denom = (np.dot(s, s) + 1e-12)\n                y_scalar = np.dot(yvec, s) / denom if denom > 0 else 0.0\n                # produce a y vector aligned with s\n                if abs(y_scalar) < 1e-12:\n                    y_list.append(y_scalar * (s / (np.linalg.norm(s) + 1e-12)))\n                else:\n                    y_list.append(y_scalar * (s / (np.linalg.norm(s) + 1e-12)))\n            # simplified two-loop but with diagonal scaling from cov_diag\n            q = g.copy()\n            alpha = [0.0] * m\n            rho = [0.0] * m\n            for i in range(m-1, -1, -1):\n                s = s_list[i]; y = y_list[i]\n                sy = np.dot(s, y) + 1e-12\n                rho[i] = 1.0 / sy if sy != 0 else 1e12\n                alpha[i] = rho[i] * np.dot(s, q)\n                q = q - alpha[i] * y\n            # initial H0: use diagonal from cov_diag (more cov => smaller curvature -> larger step)\n            H0 = np.diag(1.0 / (np.maximum(1e-8, np.abs(self.cov_diag))))\n            r = q.copy() * np.sqrt(np.maximum(1e-8, np.mean(np.diag(H0))))  # scaled\n            for i in range(m):\n                s = s_list[i]; y = y_list[i]\n                beta = rho[i] * np.dot(y, r)\n                r = r + s * (alpha[i] - beta)\n            return r\n\n        # bracket-and-refine line search that is gentle on budget:\n        # - try symmetric small probes +/- t\n        # - expand geometrically if improving\n        # - attempt parabola fit on triple, else golden-section on last bracket\n        def line_search(center, f_center, u, step_scale_vec, max_expansions=4, max_steps_budget=12):\n            \"\"\"\n            center: base point\n            f_center: function value at center\n            u: unit direction\n            step_scale_vec: per-dimension scale or scalar used to multiply u component-wise\n            Returns (best_f, best_x, used_evals)\n            \"\"\"\n            used = 0\n            best_f = f_center\n            best_x = center.copy()\n            if remaining() <= 0:\n                return best_f, best_x, used\n\n            u = np.asarray(u, dtype=float)\n            nu = np.linalg.norm(u)\n            if nu <= 0:\n                return best_f, best_x, used\n            u = u / nu\n\n            # determine nominal scalar magnitude\n            svec = np.asarray(step_scale_vec, dtype=float)\n            if svec.ndim == 0 or svec.size == 1:\n                base_mag = float(svec)\n                svec_full = base_mag * np.ones(self.dim)\n            else:\n                svec_full = svec\n                base_mag = float(np.mean(np.abs(svec_full * u)) + 1e-12)\n\n            # initial probes at t = +/-1\n            probes = []\n            for t in (+1.0, -1.0):\n                x_t = np.minimum(np.maximum(center + u * (svec_full * t), lb), ub)\n                if remaining() <= 0:\n                    break\n                ft = eval_point(x_t); used += 1\n                if ft is None:\n                    return best_f, best_x, used\n                probes.append((t, float(ft), x_t.copy()))\n                if ft < best_f:\n                    best_f = ft; best_x = x_t.copy()\n\n            if remaining() <= 0 or len(probes) == 0:\n                return best_f, best_x, used\n\n            # for each sign, expand geometrically up to max_expansions\n            for init in probes:\n                t_prev, f_prev, x_prev = 0.0, f_center, center.copy()\n                t_cur, f_cur, x_cur = init\n                last_good = (t_cur, f_cur, x_cur)\n                expansion = 0\n                while expansion < max_expansions and remaining() > 0 and abs(t_cur) <= 64.0:\n                    t_next = t_cur * 2.0\n                    x_next = np.minimum(np.maximum(center + u * (svec_full * t_next), lb), ub)\n                    if remaining() <= 0:\n                        break\n                    f_next = eval_point(x_next); used += 1\n                    if f_next is None:\n                        return best_f, best_x, used\n                    # keep track\n                    if f_next < best_f:\n                        best_f = f_next; best_x = x_next.copy()\n                    # decide whether we improved relative to previous on this ray\n                    if f_next < f_cur - 1e-15:\n                        # progress, shift window\n                        t_prev, f_prev = t_cur, f_cur\n                        t_cur, f_cur, x_cur = t_next, f_next, x_next.copy()\n                        last_good = (t_cur, f_cur, x_cur)\n                        expansion += 1\n                        continue\n                    else:\n                        # bracket found: (t_prev, t_cur, t_next) with f_prev, f_cur, f_next\n                        ts = np.array([t_prev, t_cur, t_next])\n                        fs = np.array([f_prev, f_cur, f_next])\n                        tpar = self._parabolic_minimizer(ts, fs)\n                        if tpar is not None and abs(tpar) <= abs(t_next) * 1.2:\n                            x_par = np.minimum(np.maximum(center + u * (svec_full * tpar), lb), ub)\n                            if remaining() <= 0:\n                                break\n                            f_par = eval_point(x_par); used += 1\n                            if f_par is None:\n                                return best_f, best_x, used\n                            if f_par < best_f:\n                                best_f = f_par; best_x = x_par.copy()\n                        else:\n                            # fallback: golden-section on bracket [t_prev, t_next] minimizing f(t)\n                            a, c = t_prev, t_next\n                            fa, fc = f_prev, f_next\n                            # initialize b somewhere near t_cur\n                            b = t_cur\n                            fb = f_cur\n                            gr = (np.sqrt(5) - 1) / 2.0\n                            # small golden loop with strict budget cap\n                            golden_iters = 0\n                            while golden_iters < 6 and remaining() > 0:\n                                # choose interior point\n                                if abs(c - b) > abs(b - a):\n                                    xg = c - gr * (c - b)\n                                else:\n                                    xg = a + gr * (b - a)\n                                x_point = np.minimum(np.maximum(center + u * (svec_full * xg), lb), ub)\n                                if remaining() <= 0:\n                                    break\n                                fg = eval_point(x_point); used += 1\n                                if fg is None:\n                                    return best_f, best_x, used\n                                if fg < best_f:\n                                    best_f = fg; best_x = x_point.copy()\n                                # update bracket\n                                if xg < b:\n                                    if fg <= fb:\n                                        c, fc = b, fb\n                                        b, fb = xg, fg\n                                    else:\n                                        a, fa = xg, fg\n                                else:\n                                    if fg <= fb:\n                                        a, fa = b, fb\n                                        b, fb = xg, fg\n                                    else:\n                                        c, fc = xg, fg\n                                golden_iters += 1\n                        break  # stop expanding on this sign\n\n            return best_f, best_x, used\n\n        # main loop: iterative cycles until budget or stagnation\n        while remaining() > 0 and stagn < 8 * stagn_threshold:\n            total_iters += 1\n            improved_cycle = False\n\n            # choose center (current best if available)\n            if self.x_opt is None and len(F_archive) > 0:\n                idx = int(np.argmin(F_archive))\n                center = X_archive[idx].copy(); f_center = float(F_archive[idx])\n            elif self.x_opt is not None:\n                center = self.x_opt.copy(); f_center = float(self.f_opt)\n            else:\n                # fallback random center\n                center = self.rng.uniform(lb, ub); f_center = eval_point(center)\n                if f_center is None:\n                    break\n\n            budget_frac = remaining() / max(1.0, float(self.budget))\n\n            # construct direction pool: coordinate axes, bank, PCA, sampled anisotropic normals\n            axes = [np.eye(1, self.dim, i).reshape(self.dim) for i in range(self.dim)]\n            bank_dirs = [e['d'] for e in self.dir_bank]\n            pca_dirs = self._pca_dirs(k=3)\n            samp_cnt = min(6, max(2, self.dim // 2 + 1))\n            L = np.sqrt(self.cov_diag + 1e-12)\n            samp = self.rng.normal(size=(samp_cnt, self.dim)) * L[None, :]\n            sample_dirs = []\n            for r in samp:\n                nr = np.linalg.norm(r)\n                if nr > 1e-12:\n                    sample_dirs.append(r / nr)\n\n            pool = axes + bank_dirs + pca_dirs + sample_dirs\n            pool = self._orthonormalize_unique(pool, tol=1e-4)\n            # score and sort pool with UCB-like formula\n            dir_scores = []\n            total_attempts = sum((e.get('succ', 0) + e.get('fail', 0) + 1) for e in self.dir_bank) + 1\n            for u in pool:\n                # find matching bank entry if exists\n                match = None\n                best_sim = -1.0\n                for e in self.dir_bank:\n                    sim = abs(np.dot(e['d'], u))\n                    if sim > best_sim:\n                        best_sim = sim; match = e\n                base_score = 0.1 + 0.0 * self.rng.random()\n                if match is not None:\n                    plays = match.get('succ', 0) + match.get('fail', 0) + 1\n                    base_score = match['score']\n                    # exploration bonus\n                    bonus = 1.2 * np.sqrt(np.log(1 + total_attempts) / plays)\n                    base_score = base_score + bonus\n                # axis boost\n                axis_idx = int(np.argmax(np.abs(u)))\n                if np.allclose(np.abs(u), np.eye(self.dim)[axis_idx], atol=1e-6):\n                    base_score = base_score * (1.0 + 0.12 * self.axis_mult[axis_idx])\n                dir_scores.append((base_score, u))\n            # sort descending and take top-k\n            dir_scores.sort(key=lambda x: -x[0])\n            max_dirs = max(3, min(len(dir_scores), 6 + self.dim // 4))\n            selected_dirs = [u for _, u in dir_scores[:max_dirs]]\n\n            # per-cycle adapt axis multipliers decay/expand\n            if len(self.recent_steps) > 0 and (len(self.recent_steps) % 6 == 0):\n                self.axis_mult = np.minimum(20.0, 1.03 * self.axis_mult)\n            else:\n                self.axis_mult = np.maximum(1e-3, 0.993 * self.axis_mult)\n\n            # scales to probe: coarse to fine biased by budget left\n            coarse_pref = max_scale * (0.6 + 0.4 * self.rng.random()) * (1.0 + 0.5 * (1.0 - budget_frac))\n            base_scales = np.array(scales) * (0.3 + 0.7 * budget_frac)\n            scales_local = np.maximum(min_scale, np.concatenate(([coarse_pref], base_scales)))\n            self.rng.shuffle(scales_local)\n\n            # iterate scales\n            for scale in scales_local:\n                if remaining() <= 0:\n                    break\n\n                # anisotropic per-dim scaling by cov_diag\n                local_scale_vec = scale * (1.0 + 0.6 * (self.cov_diag / (np.max(self.cov_diag) + 1e-12)))\n\n                # mid/late budget: try quasi-Newton suggestion\n                if budget_frac < 0.6 and len(F_archive) >= max(8, self.dim + 2):\n                    g = local_linear(center, f_center, scale)\n                    if g is not None and np.linalg.norm(g) > 1e-10:\n                        # approximate inverse-H via recent s,y\n                        step_dir = apply_invH_to_vec(g)\n                        sdir_norm = np.linalg.norm(step_dir)\n                        if sdir_norm > 1e-12:\n                            step_mag = scale * min(1.2, 1.0 / (1.0 + np.linalg.norm(g)))\n                            x_q = np.minimum(np.maximum(center - (step_dir / sdir_norm) * step_mag, lb), ub)\n                            if remaining() <= 0:\n                                break\n                            fq = eval_point(x_q)\n                            if fq is None:\n                                break\n                            if fq < f_center - 1e-12:\n                                record_success(x_q - center, f_center - fq, f_before=f_center, f_after=fq)\n                                center = x_q.copy(); f_center = float(fq)\n                                improved_cycle = True\n                                stagn = 0\n                                if f_center < self.f_opt:\n                                    self.f_opt = f_center; self.x_opt = center.copy()\n\n                # loop over selected directions\n                for u in selected_dirs:\n                    if remaining() <= 0:\n                        break\n                    # determine multiplier from axis/bank\n                    mult = 1.0\n                    axis_idx = int(np.argmax(np.abs(u)))\n                    if np.allclose(np.abs(u), np.eye(self.dim)[axis_idx], atol=1e-6):\n                        mult = self.axis_mult[axis_idx]\n                    else:\n                        # find bank match\n                        best_e = None\n                        best_sim = 0.0\n                        for e in self.dir_bank:\n                            sim = np.dot(e['d'], u)\n                            if sim > best_sim:\n                                best_sim = sim; best_e = e\n                        if best_e is not None and best_sim > 0.7:\n                            mult = best_e.get('mult', 1.0)\n\n                    step_vec = u * (mult * local_scale_vec)\n\n                    # budget-aware conservative cap for line search evaluations\n                    per_line_budget = min(10, max(3, remaining() // (3 + len(selected_dirs))))\n                    # perform line search (ensures it uses eval_point)\n                    bf, bx, used = line_search(center, f_center, u, step_vec, max_expansions=4)\n                    # if no progress or no evals, continue\n                    if used == 0:\n                        continue\n                    # update bank stats: match best entry and record success/fail\n                    matched = None\n                    best_sim = 0.0\n                    for e in self.dir_bank:\n                        sim = abs(np.dot(e['d'], u))\n                        if sim > best_sim:\n                            best_sim = sim; matched = e\n                    if bf < f_center - 1e-12:\n                        # improvement\n                        improvement = f_center - bf\n                        record_success(bx - center, improvement, f_before=f_center, f_after=bf)\n                        # update matched entry if exists\n                        if matched is not None and best_sim > 0.75:\n                            matched['mult'] = min(20.0, 1.06 * matched.get('mult', 1.0))\n                            matched['score'] = 0.95 * matched['score'] + 0.05 * (improvement / (1.0 + np.linalg.norm(bx - center)))\n                        else:\n                            # small chance to add new direction if not matched\n                            if matched is None and self.rng.random() < 0.3:\n                                self.dir_bank.append({'d': u.copy(), 'score': float(improvement) / (1.0 + np.linalg.norm(bx - center)),\n                                                      'age': 0, 'mult': min(8.0, 1.0 + 0.2 * (improvement / (1.0 + improvement))),\n                                                      'succ': 1, 'fail': 0})\n                        center = bx.copy(); f_center = float(bf)\n                        improved_cycle = True\n                        stagn = 0\n                        if f_center < self.f_opt:\n                            self.f_opt = f_center; self.x_opt = center.copy()\n                    else:\n                        # no improvement: penalize matched direction gently\n                        if matched is not None:\n                            matched['fail'] = matched.get('fail', 0) + 1\n                            matched['mult'] = max(1e-4, 0.95 * matched.get('mult', 1.0))\n                            matched['score'] = 0.98 * matched['score'] - 0.01\n                        # if axis direction, decay axis_mult\n                        if np.allclose(np.abs(u), np.eye(self.dim)[axis_idx], atol=1e-6):\n                            self.axis_mult[axis_idx] = max(1e-4, 0.97 * self.axis_mult[axis_idx])\n\n                # occasional small gaussian perturbation to escape flat areas\n                if remaining() > 0 and self.rng.random() < (0.10 + 0.25 * (1.0 - budget_frac)):\n                    noise = 0.18 * scale * self.rng.normal(size=self.dim) * np.sqrt(self.cov_diag / (np.max(self.cov_diag) + 1e-12))\n                    xn = np.minimum(np.maximum(center + noise, lb), ub)\n                    fn = eval_point(xn)\n                    if fn is None:\n                        break\n                    if fn < f_center - 1e-12:\n                        record_success(xn - center, f_center - fn, f_before=f_center, f_after=fn)\n                        center = xn.copy(); f_center = float(fn)\n                        improved_cycle = True\n                        stagn = 0\n                        if f_center < self.f_opt:\n                            self.f_opt = f_center; self.x_opt = center.copy()\n\n            # recombination intensification (mix elites)\n            if remaining() > 0 and len(F_archive) >= 4 and self.rng.random() < (0.10 + 0.25 * (1.0 - budget_frac)):\n                Farr = np.asarray(F_archive)\n                ranks = np.argsort(Farr)\n                topk = max(2, min(len(ranks), 6))\n                i1, i2 = self.rng.choice(ranks[:topk], size=2, replace=False)\n                w = self.rng.random()\n                off = w * np.asarray(X_archive[i1]) + (1 - w) * np.asarray(X_archive[i2])\n                off += 0.012 * span * (self.rng.normal(size=self.dim) * np.sqrt(self.cov_diag / (np.max(self.cov_diag) + 1e-12)))\n                off = np.minimum(np.maximum(off, lb), ub)\n                if remaining() > 0:\n                    _ = eval_point(off)\n\n            if improved_cycle:\n                stagn = 0\n            else:\n                stagn += 1\n\n            # restart / heavy jump if stagnating\n            if stagn > stagn_threshold and remaining() > 0:\n                # select base from best-so-far\n                if len(F_archive) > 0:\n                    idx = int(np.argmin(F_archive))\n                    base = np.asarray(X_archive[idx]).copy()\n                else:\n                    base = self.rng.uniform(lb, ub)\n                # jump direction: highest PCA or random\n                pdirs = self._pca_dirs(k=2)\n                if len(pdirs) > 0 and self.rng.random() < 0.8:\n                    dir_jump = pdirs[0]\n                else:\n                    dir_jump = self.rng.normal(size=self.dim)\n                    nrm = np.linalg.norm(dir_jump)\n                    if nrm > 1e-12:\n                        dir_jump = dir_jump / nrm\n                # heavy-tailed factor but scaled to avoid exceeding bounds\n                raw = self.rng.standard_cauchy(size=1)[0]\n                raw = np.tanh(raw)  # clip extremes\n                cscale = 0.25 * span.max() * (1.0 + min(3.0, stagn / (stagn_threshold + 1.0)))\n                cand = np.minimum(np.maximum(base + dir_jump * (cscale * raw * np.sqrt(self.cov_diag / (np.max(self.cov_diag) + 1e-12))), lb), ub)\n                if remaining() > 0:\n                    f_c = eval_point(cand)\n                    if f_c is None:\n                        break\n                # relax axis multipliers and age bank\n                self.axis_mult = np.maximum(1e-4, 0.5 * self.axis_mult + 0.5)\n                for e in self.dir_bank:\n                    e['age'] = e.get('age', 0) + 5\n                stagn = 0\n\n        # final intensification near best: small L-BFGS-ish steps + coordinate polishing\n        if remaining() > 0 and self.x_opt is not None:\n            center = self.x_opt.copy()\n            f_center = float(self.f_opt)\n            # try a handful of quasi-Newton proposals using local linear model\n            for _ in range(6):\n                if remaining() <= 0:\n                    break\n                g = local_linear(center, f_center, max(1e-8, 0.05 * span.max()))\n                if g is None:\n                    break\n                # apply inverse approx\n                step_dir = apply_invH_to_vec(g)\n                if np.linalg.norm(step_dir) < 1e-12:\n                    break\n                step = - (step_dir / (np.linalg.norm(step_dir) + 1e-12)) * (0.02 * span.max())\n                x_try = np.minimum(np.maximum(center + step, lb), ub)\n                f_try = eval_point(x_try)\n                if f_try is None:\n                    break\n                if f_try < f_center - 1e-12:\n                    record_success(x_try - center, f_center - f_try, f_before=f_center, f_after=f_try)\n                    center = x_try.copy(); f_center = float(f_try)\n                    if f_center < self.f_opt:\n                        self.f_opt = f_center; self.x_opt = center.copy()\n                else:\n                    # small coordinate polish\n                    improved_local = False\n                    local_scale = 1e-5 * span.max()\n                    for i in range(self.dim):\n                        if remaining() <= 0:\n                            break\n                        for sgn in (+1.0, -1.0):\n                            step_c = np.zeros(self.dim)\n                            step_c[i] = sgn * local_scale * (1.0 + 0.5 * (self.cov_diag[i] / (np.max(self.cov_diag) + 1e-12)))\n                            x_c = np.minimum(np.maximum(center + step_c, lb), ub)\n                            f_c = eval_point(x_c)\n                            if f_c is None:\n                                break\n                            if f_c < f_center - 1e-15:\n                                record_success(x_c - center, f_center - f_c, f_before=f_center, f_after=f_c)\n                                center = x_c.copy(); f_center = float(f_c)\n                                if f_center < self.f_opt:\n                                    self.f_opt = f_center; self.x_opt = center.copy()\n                                improved_local = True\n                                break\n                        if improved_local:\n                            break\n                    if not improved_local:\n                        break\n\n        # ensure best exists\n        if self.x_opt is None and len(F_archive) > 0:\n            idx = int(np.argmin(F_archive))\n            self.x_opt = np.asarray(X_archive[idx]).copy()\n            self.f_opt = float(F_archive[idx])\n\n        return float(self.f_opt), np.asarray(self.x_opt).copy()", "configspace": "", "generation": 18, "feedback": "The algorithm MultiScaleDirectionalMemeticV2_RefinedX scored 0.860 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ba82828d-37cc-48dd-9998-7dc579a0f246"], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9745083358776262}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9564963982566674}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9699408842602605}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8181701990325942}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9136661845627716}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8878509888291715}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.6834488898130726}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.7326742200971056}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.8068071462360037}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.8338728738680065}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.7551076889351335}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.7793291252602571}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.923572698686549}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9352136214336149}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.935164340855829}], "aucs": [0.9745083358776262, 0.9564963982566674, 0.9699408842602605, 0.8181701990325942, 0.9136661845627716, 0.8878509888291715, 0.6834488898130726, 0.7326742200971056, 0.8068071462360037, 0.8338728738680065, 0.7551076889351335, 0.7793291252602571, 0.923572698686549, 0.9352136214336149, 0.935164340855829]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-BBOB-1", "seed": 1}
{"method_name": "ES", "problem_name": "BBOB", "llm_name": "gpt-5-mini-2025-08-07", "method": {"method_name": "ES", "budget": 200, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before."], "n_parents": 8, "n_offspring": 8, "elitism": true}}, "problem": {"name": "BBOB", "dims": [10], "training_instances": [[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3], [4, 1], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3]], "test_instances": [[1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9]], "budget_factor": 2000, "problem_type": "BBOB", "specific_fid": null, "specific_group": null}, "llm": {"model": "gpt-5-mini-2025-08-07", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "74f5f6c7-3a00-4493-84ef-8c77bf793300", "fitness": 0.6396362350528833, "name": "FractalSurrogateExplorerRefined", "description": "Ensemble Thompson-Fractal Surrogate — multi-center adaptive surrogate ensemble with per-center trust regions, Thompson+UCB move selection, density-regularized candidate pre-screening and adaptive covariance polishing with stagnation-triggered restarts.", "code": "import numpy as np\n\nclass FractalSurrogateExplorerRefined:\n    \"\"\"\n    Refined FractalSurrogateExplorer\n\n    Improvements over prior variant:\n    - Explicit multi-center management: keep several promising centers with per-center trust radii\n      and adapt radii on success/failure. This encourages robust multi-start behavior.\n    - Better surrogate ensemble: weighted diagonal-quadratic + low-rank quadratic + residual-based\n      local uncertainty proxy (heteroscedastic via kernel weights).\n    - Candidate pre-screening without extra evaluations: combine surrogate mean & uncertainty into\n      Thompson-style sampled scores and UCB; choose few best for opportunistic true evaluations.\n    - Density-penalty to avoid sampling in crowded regions.\n    - Robust budget-aware parabolic/bracketing 1D polishing, and a light covariance-adaptive polishing\n      (micro-CMA-like) only at the very end or when stagnation detected.\n    - Restart (LHS reseed) when long stagnation observed.\n    \"\"\"\n\n    def __init__(self, budget=200, dim=10, rng=None, verbose=False,\n                 init_frac=0.06, centers=6, min_trust=0.03, max_trust=0.6):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.verbose = bool(verbose)\n        self.rng = np.random.default_rng(rng)\n\n        # algorithm knobs\n        self.init_frac = float(init_frac)\n        self.n_centers = int(max(1, centers))\n        self.min_trust = float(min_trust)\n        self.max_trust = float(max_trust)\n\n        # Moves repertoire\n        self.move_types = [\n            'diag_quad',        # exploit via diagonal quadratic surrogate\n            'subspace_quad',    # exploit in low-rank subspace\n            'cov_mutation',     # covariance-adaptive gaussian mutation\n            'heavy_jump',       # heavy-tailed global jump\n            'fractal_noise',    # fractal multi-scale jitter\n            'coord_polish',     # coordinate subset polish\n            'recombine'         # recombination of archive members\n        ]\n\n        # Thompson-like move posteriors: keep decayed mean and variance proxies\n        self.move_stats = {m: {'N': 1.0, 'm': 0.0, 'v': 1.0} for m in self.move_types}\n        self.move_decay = 0.90\n\n        # archive\n        self.X = []  # list of np arrays\n        self.y = []\n\n        # covariance for mutation polishing\n        self.cov = np.eye(self.dim) * (0.07 ** 2)\n        self.path = np.zeros(self.dim)\n        self.cov_lr = 0.12\n        self.path_decay = 0.85\n\n        # surrogate controls\n        self.surr_min_pts = max(4, self.dim + 1)\n        self.surr_max_pts = max(40, 14 * min(4, self.dim))\n        self.surr_reg = 1e-8\n\n        # trust centers: list of dicts {x, f, trust_radius}\n        self.centers = []\n\n        # fractal noise\n        self.fractal_levels = 5\n        self.fractal_base = 0.5\n\n        # exploration temperature\n        self.temp = 1.0\n        self.min_temp = 0.06\n        self.max_temp = 6.0\n\n        # stagnation tracking\n        self.stagnation = 0\n        self.last_improve_at = 0\n\n        # archive capacity\n        self.max_archive = max(1000, 80 * self.dim)\n\n    # ----------------- utility models -----------------\n    def _latin_hypercube(self, n):\n        d = self.dim\n        cut = np.linspace(0., 1., n + 1)\n        u = self.rng.random((n, d))\n        samples = np.zeros((n, d))\n        for j in range(d):\n            order = self.rng.permutation(n)\n            samples[:, j] = cut[:-1] + u[order, j] * (cut[1] - cut[0])\n        return samples\n\n    def _weighted_diag_quad(self, center, Xpts, ypts, kernel_scale):\n        # Fit y ≈ b + g^T (x-c) + 0.5 * h_diag * (x-c)^2 using weighted least squares\n        X = Xpts - center.reshape(1, -1)\n        n, d = X.shape\n        if n < max(self.surr_min_pts, d + 2):\n            return None\n        dists = np.linalg.norm(X, axis=1)\n        scale = float(max(kernel_scale, 1e-8))\n        w = np.exp(-0.5 * (dists / (scale + 1e-12)) ** 2)\n        Wsqrt = np.sqrt(w)[:, None]\n        A = np.hstack([np.ones((n, 1)), X, 0.5 * (X ** 2)])\n        Aw = A * Wsqrt\n        wy = ypts * Wsqrt.ravel()\n        try:\n            p, *_ = np.linalg.lstsq(Aw, wy, rcond=None)\n        except Exception:\n            return None\n        bias = float(p[0])\n        grad = p[1:1 + d]\n        hdiag = p[1 + d:1 + 2 * d]\n        preds = A.dot(p)\n        resid = ypts - preds\n        resid_var = float(np.clip(np.sum(w * resid ** 2) / (np.sum(w) + 1e-12), 1e-10, 1e6))\n        return {'bias': bias, 'grad': grad.reshape(d), 'hdiag': hdiag.reshape(d), 'resid_var': resid_var}\n\n    def _lowrank_quad(self, center, Xpts, ypts, sub_k=3, kernel_scale=None):\n        # Fit quadratic in low-dim subspace: project using weighted covariance eigenvectors\n        B = Xpts - center.reshape(1, -1)\n        n, d = B.shape\n        if n < max(self.surr_min_pts, sub_k + 3):\n            return None\n        dists = np.linalg.norm(B, axis=1)\n        if kernel_scale is None:\n            kernel_scale = max(1e-8, np.median(np.clip(dists, 1e-8, np.max(dists) + 1e-8)))\n        w = np.exp(-0.5 * (dists / (kernel_scale + 1e-12)) ** 2)\n        W = np.sqrt(w)[:, None]\n        try:\n            cov = (B * w[:, None]).T @ B\n            cov = 0.5 * (cov + cov.T) + 1e-12 * np.eye(d)\n            vals, vecs = np.linalg.eigh(cov)\n            idx = np.argsort(vals)[-sub_k:]\n            U = vecs[:, idx]\n        except Exception:\n            U = self.rng.normal(size=(d, sub_k))\n            U, _, _ = np.linalg.svd(U, full_matrices=False)\n            U = U[:, :sub_k]\n        Z = B @ U  # n x sub_k\n        A = np.hstack([np.ones((n, 1)), Z, 0.5 * (Z ** 2)])\n        Aw = A * W\n        wy = ypts * W.ravel()\n        try:\n            p, *_ = np.linalg.lstsq(Aw, wy, rcond=None)\n        except Exception:\n            return None\n        bias = float(p[0])\n        grad_z = p[1:1 + U.shape[1]]\n        hdiag_z = p[1 + U.shape[1]:1 + 2 * U.shape[1]]\n        preds = A.dot(p)\n        resid = ypts - preds\n        resid_var = float(np.clip(np.sum(w * resid ** 2) / (np.sum(w) + 1e-12), 1e-10, 1e6))\n        grad_full = U @ grad_z\n        H = U @ np.diag(hdiag_z) @ U.T\n        return {'bias': bias, 'grad_full': grad_full, 'H': H, 'U': U, 'resid_var': resid_var}\n\n    # ----------------- line search -----------------\n    def _parabolic_bracket(self, x0, dir_unit, f0, eval_fn, lb, ub, allowed):\n        # Bracket then parabolic interpolation within remaining allowed evals\n        used = 0\n        if allowed <= 0:\n            return x0, f0, used\n        # initial bracket points\n        s = max(1e-6, 0.08 * np.linalg.norm(ub - lb))\n        alphas = [0.0]; fvals = [f0]\n        # probe symmetric points of increasing radius\n        probes = [0.4 * s, 1.0 * s, 2.2 * s]\n        for a in probes:\n            if used + 1 > allowed:\n                break\n            xa = np.clip(x0 + a * dir_unit, lb, ub)\n            fa = eval_fn(xa); used += 1\n            if fa is None: break\n            alphas.append(a); fvals.append(fa)\n            if used + 1 > allowed: break\n            xb = np.clip(x0 - a * dir_unit, lb, ub)\n            fb = eval_fn(xb); used += 1\n            if fb is None: break\n            alphas.append(-a); fvals.append(fb)\n            # if we have at least 3 distinct alphas, try parabolic fit\n            if len(alphas) >= 3:\n                try:\n                    idx = np.argsort(fvals)[:3]\n                    A = np.array([alphas[i] for i in idx])\n                    B = np.array([fvals[i] for i in idx])\n                    coeffs = np.polyfit(A, B, 2)\n                    a2, a1, a0 = coeffs\n                    if abs(a2) > 1e-16:\n                        amin = -a1 / (2 * a2)\n                        if np.isfinite(amin) and abs(amin) < 3.0 * s:\n                            x_try = np.clip(x0 + amin * dir_unit, lb, ub)\n                            if used + 1 > allowed: continue\n                            f_try = eval_fn(x_try); used += 1\n                            if f_try is None: break\n                            alphas.append(amin); fvals.append(f_try)\n                except Exception:\n                    pass\n        best_idx = int(np.argmin(fvals))\n        best_alpha = alphas[best_idx]\n        best_x = np.clip(x0 + best_alpha * dir_unit, lb, ub)\n        best_f = fvals[best_idx]\n        return best_x, best_f, used\n\n    # ----------------- move selection -----------------\n    def _select_move(self):\n        # Thompson-like sampling from move posteriors combined with softmax temperature\n        samples = []\n        for m in self.move_types:\n            s = self.move_stats[m]\n            mean = s['m']\n            var = max(1e-6, s['v'] / (s['N'] + 1e-12))\n            samp = self.rng.normal(mean, np.sqrt(var + 1e-9))\n            samples.append(samp)\n        arr = np.array(samples)\n        # sharpen/shuffle via temperature\n        arr = arr - arr.max()\n        temp = np.clip(self.temp, 0.3, 3.0)\n        probs = np.exp(arr / temp)\n        probs = probs / (probs.sum() + 1e-12)\n        return self.rng.choice(self.move_types, p=probs)\n\n    # ----------------- bookkeeping -----------------\n    def _eval_and_record(self, func, x, lb, ub, evals, budget_counter):\n        # budget_counter is a one-element list acting as mutable integer\n        if budget_counter[0] >= self.budget:\n            return None\n        x = np.clip(np.asarray(x, dtype=float).reshape(self.dim), lb, ub)\n        try:\n            fv = float(func(x))\n        except Exception:\n            fv = float(np.inf)\n        self.X.append(x.copy()); self.y.append(fv)\n        budget_counter[0] += 1\n        return fv\n\n    def _ensure_centers(self):\n        # maintain list of top centers (unique by distance)\n        if len(self.X) == 0:\n            return\n        Xarr = np.asarray(self.X)\n        yarr = np.asarray(self.y)\n        idxs = np.argsort(yarr)[:min(len(yarr), self.n_centers)]\n        centers = []\n        for i in idxs:\n            centers.append({'x': Xarr[i].copy(), 'f': float(yarr[i]), 'trust': 0.18})\n        # if too few, add some random archive ones\n        while len(centers) < self.n_centers and len(self.X) > 0:\n            i = self.rng.integers(0, len(self.X))\n            centers.append({'x': np.asarray(self.X[i]).copy(), 'f': float(self.y[i]), 'trust': 0.18})\n        self.centers = centers\n\n    # ----------------- main optimizer -----------------\n    def __call__(self, func):\n        rng = self.rng\n\n        # bounds - default -5..5\n        lb = np.full(self.dim, -5.0)\n        ub = np.full(self.dim, 5.0)\n        # try to read func.bounds if exists\n        if hasattr(func, 'bounds') and getattr(func.bounds, 'lb', None) is not None:\n            try:\n                lb_c = np.asarray(func.bounds.lb, dtype=float).reshape(-1)\n                ub_c = np.asarray(func.bounds.ub, dtype=float).reshape(-1)\n                if lb_c.size == self.dim and ub_c.size == self.dim:\n                    lb = lb_c.copy(); ub = ub_c.copy()\n            except Exception:\n                pass\n        span = ub - lb\n        span = np.where(span <= 0, 1.0, span)\n\n        # evaluation bookkeeping with mutable counter\n        evals = [0]\n\n        # wrapper eval function to obey budget\n        def eval_and_record(x):\n            return self._eval_and_record(func, x, lb, ub, evals, evals)\n\n        # Initial design: Latin hypercube + center + random\n        init_n = int(np.clip(max(12, int(self.init_frac * self.budget)), 8, max(12, self.budget // 8)))\n        lh = self._latin_hypercube(init_n)\n        for i in range(init_n):\n            if evals[0] >= self.budget:\n                break\n            x = lb + lh[i] * span\n            eval_and_record(x)\n        if evals[0] < self.budget:\n            eval_and_record(lb + 0.5 * span)\n        if evals[0] < self.budget:\n            eval_and_record(rng.uniform(lb, ub))\n        if evals[0] == 0:\n            return float(np.inf), np.zeros(self.dim)\n\n        Xarr = np.asarray(self.X)\n        yarr = np.asarray(self.y)\n        best_idx = int(np.argmin(yarr))\n        f_best = float(yarr[best_idx]); x_best = Xarr[best_idx].copy()\n\n        self._ensure_centers()\n        self.last_improve_at = evals[0]\n        self.stagnation = 0\n\n        # main loop\n        while evals[0] < self.budget:\n            remaining = self.budget - evals[0]\n\n            # update centers periodically\n            if rng.random() < 0.18 or len(self.centers) == 0:\n                self._ensure_centers()\n\n            # tune temperature by remaining budget and stagnation\n            if remaining <= max(6, int(0.03 * self.budget)):\n                self.temp = max(self.min_temp, self.temp * 0.86)\n            elif self.stagnation > max(6, int(0.02 * self.budget)):\n                self.temp = min(self.max_temp, self.temp * 1.12)\n            else:\n                self.temp = float(np.clip(self.temp * 0.99, self.min_temp, self.max_temp))\n\n            # select a center (multistart): probabilistically choose among centers biased to goodness\n            if len(self.centers) > 0:\n                scores = np.array([c['f'] for c in self.centers])\n                inv = (scores.max() - scores + 1e-12)\n                probs = inv / (inv.sum() + 1e-12)\n                cidx = int(np.searchsorted(np.cumsum(probs), rng.random() * probs.sum()))\n                center = self.centers[min(cidx, len(self.centers)-1)]\n            else:\n                center = {'x': x_best.copy(), 'f': f_best, 'trust': 0.18}\n\n            center_x = center['x'].copy()\n            trust = float(np.clip(center.get('trust', 0.18), self.min_trust, self.max_trust))\n            trust_scale = trust * span\n\n            # neighborhood selection for surrogate\n            allX = np.asarray(self.X)\n            dists = np.linalg.norm(allX - center_x.reshape(1, -1), axis=1)\n            k = min(self.surr_max_pts, max(self.surr_min_pts, int(len(allX) * 0.5)))\n            sel = np.argsort(dists)[:k]\n            ptsX = allX[sel]\n            ptsY = np.asarray(self.y)[sel]\n            kernel_scale = float(np.median(np.clip(dists[sel], 1e-8, np.max(dists[sel]) + 1e-8)))\n\n            # build candidate pool\n            candidates = []\n            meta = []  # tuples (pred_mean, pred_uncertainty, source)\n\n            # diagonal surrogate suggestion\n            sdiag = self._weighted_diag_quad(center_x, ptsX, ptsY, kernel_scale if kernel_scale > 0 else span.mean())\n            if sdiag is not None:\n                g = sdiag['grad']; h = sdiag['hdiag']\n                denom = h + 1e-6 * (1.0 + np.abs(h))\n                denom = np.where(denom > 1e-8, denom, (np.abs(denom) + 1e-8))\n                step = -g / (denom + 1e-12)\n                # cap step to trust\n                step = np.sign(step) * np.minimum(np.abs(step), 1.5 * trust_scale)\n                cand = np.clip(center_x + step, lb, ub)\n                pred = sdiag['bias'] + g.dot(step) + 0.5 * (h * (step ** 2)).sum()\n                unc = np.sqrt(sdiag['resid_var']) * (1.0 + np.linalg.norm(step) / (1.0 + np.linalg.norm(span)))\n                candidates.append(cand); meta.append((pred, unc, 'diag_quad'))\n\n            # low-rank surrogate suggestion\n            subk = min(max(1, int(0.12 * self.dim) + 1), min(6, self.dim))\n            ssub = self._lowrank_quad(center_x, ptsX, ptsY, sub_k=subk, kernel_scale=kernel_scale)\n            if ssub is not None:\n                grad = ssub['grad_full']; H = ssub['H']\n                # damped Newton in subspace via eigen-decomposition\n                try:\n                    vals, vecs = np.linalg.eigh(H + 1e-8 * np.eye(self.dim))\n                    inv = (vecs * (1.0 / (vals + 1e-10))) @ vecs.T\n                    step = -inv.dot(grad)\n                except Exception:\n                    step = -grad\n                step = np.sign(step) * np.minimum(np.abs(step), 1.5 * trust_scale)\n                cand = np.clip(center_x + step, lb, ub)\n                pred = ssub['bias'] + grad.dot(step) + 0.5 * step.dot(H).dot(step)\n                unc = np.sqrt(ssub['resid_var']) * (1.0 + np.linalg.norm(step) / (1.0 + np.linalg.norm(span)))\n                candidates.append(cand); meta.append((pred, unc, 'subspace_quad'))\n\n            # move selection influences random proposals\n            move = self._select_move()\n\n            # move-specific proposals (diverse set)\n            if move == 'cov_mutation':\n                mean = x_best.copy() if rng.random() < 0.8 else center_x.copy()\n                if rng.random() < 0.4 and np.linalg.norm(self.path) > 1e-12:\n                    mean = mean + 0.5 * span.mean() * self.path\n                for s in [0.10 * self.temp, 0.35 * self.temp]:\n                    try:\n                        L = np.linalg.cholesky(self.cov + 1e-12 * np.eye(self.dim))\n                        z = rng.normal(size=self.dim)\n                        cand = mean + (s * span.mean()) * (L @ z)\n                    except Exception:\n                        cand = mean + rng.normal(0, s * span * 0.9)\n                    candidates.append(np.clip(cand, lb, ub)); meta.append((np.inf, 1.0, 'cov_mutation'))\n\n            elif move == 'heavy_jump':\n                center_h = x_best if rng.random() < 0.75 else self.X[rng.integers(0, len(self.X))]\n                scale = 0.32 * (1.0 + 0.6 * min(1.5, self.stagnation / 15.0)) * (0.4 + rng.random() * 0.8)\n                raw = rng.standard_cauchy(self.dim)\n                step = raw * (scale * span)\n                step = np.clip(step, -5 * span, 5 * span)\n                cand = np.clip(center_h + step, lb, ub)\n                candidates.append(cand); meta.append((np.inf, 1.0, 'heavy_jump'))\n\n            elif move == 'fractal_noise':\n                levels = int(rng.integers(1, self.fractal_levels + 1))\n                total = np.zeros(self.dim)\n                for lvl in range(levels):\n                    sigma = (self.fractal_base ** lvl) * span * (0.8 + 0.9 * rng.random()) * self.temp\n                    total += rng.normal(0, sigma)\n                cand = np.clip(x_best + total * 0.9, lb, ub)\n                candidates.append(cand); meta.append((np.inf, 1.0, 'fractal_noise'))\n\n            elif move == 'coord_polish':\n                sdim = max(1, int(0.12 * self.dim))\n                dims = rng.choice(self.dim, size=sdim, replace=False)\n                center_cp = x_best if rng.random() < 0.9 else center_x\n                dscale = 0.12 * span[dims] * max(0.35, 0.9 * self.temp)\n                for sgn in [-1.0, 1.0]:\n                    cand = center_cp.copy()\n                    cand[dims] = cand[dims] + sgn * dscale * (0.6 + 0.9 * rng.random())\n                    candidates.append(np.clip(cand, lb, ub)); meta.append((np.inf, 1.0, 'coord_polish'))\n                cand2 = center_cp.copy()\n                cand2[dims] += rng.normal(0, 0.03 * span[dims] * self.temp)\n                candidates.append(np.clip(cand2, lb, ub)); meta.append((np.inf, 1.0, 'coord_polish'))\n\n            elif move == 'recombine':\n                if len(self.X) >= 2:\n                    a = self.X[rng.integers(0, len(self.X))]\n                    b = self.X[rng.integers(0, len(self.X))]\n                    w = rng.random()\n                    child = w * a + (1.0 - w) * b + rng.normal(0, 0.045 * span) * self.temp\n                    candidates.append(np.clip(child, lb, ub)); meta.append((np.inf, 1.0, 'recombine'))\n                else:\n                    candidates.append(rng.uniform(lb, ub)); meta.append((np.inf, 1.0, 'recombine'))\n\n            # fallback global if none\n            if len(candidates) == 0:\n                candidates.append(np.clip(x_best + rng.normal(0, 0.06 * span), lb, ub)); meta.append((np.inf, 1.0, 'global'))\n\n            # pre-screen candidates via surrogate ensemble (no extra true evals)\n            scores = []\n            for i, cand in enumerate(candidates):\n                pred_vals = []\n                sig_vals = []\n                # diag pred if available\n                if sdiag is not None:\n                    delta = cand - center_x\n                    pred_d = sdiag['bias'] + sdiag['grad'].dot(delta) + 0.5 * (sdiag['hdiag'] * (delta ** 2)).sum()\n                    unc_d = np.sqrt(sdiag['resid_var']) * (1.0 + np.linalg.norm(delta) / (1.0 + np.linalg.norm(span)))\n                    pred_vals.append(pred_d); sig_vals.append(unc_d)\n                if ssub is not None:\n                    delta = cand - center_x\n                    pred_s = ssub['bias'] + ssub['grad_full'].dot(delta) + 0.5 * delta.dot(ssub['H']).dot(delta)\n                    unc_s = np.sqrt(ssub['resid_var']) * (1.0 + np.linalg.norm(delta) / (1.0 + np.linalg.norm(span)))\n                    pred_vals.append(pred_s); sig_vals.append(unc_s)\n                # combine preds (if none, treat as unknown)\n                if len(pred_vals) > 0:\n                    # combine means by inverse-uncertainty weights\n                    w = np.array([1.0 / (sv + 1e-12) for sv in sig_vals])\n                    mean_comb = np.sum(np.array(pred_vals) * w) / (np.sum(w) + 1e-12)\n                    # combined unc\n                    unc_comb = np.sqrt(np.sum(np.array(sig_vals) ** 2) / max(1.0, len(sig_vals)))\n                    # add density penalty: prefer far from archive\n                    dmin = np.min(np.linalg.norm(np.asarray(self.X) - cand.reshape(1, -1), axis=1)) if len(self.X) > 0 else np.inf\n                    density_pen = 0.05 * np.exp(-dmin / (0.12 * np.linalg.norm(span) + 1e-12))\n                    # Thompson-style sample plus small UCB bias\n                    th = rng.normal(mean_comb, max(1e-6, unc_comb)) - 0.8 * unc_comb\n                    score = th + density_pen\n                else:\n                    # no surrogate info: use distance and randomness\n                    dmin = np.min(np.linalg.norm(np.asarray(self.X) - cand.reshape(1, -1), axis=1)) if len(self.X) > 0 else np.inf\n                    score = -0.1 * dmin + rng.random() * 1e-6\n                scores.append(score)\n\n            # order by score ascending (lower better)\n            order = np.argsort(scores)\n\n            improved = False\n            before = f_best\n            # opportunistically evaluate top candidates until improvement or until small budget left\n            max_probe = min(6, remaining)\n            for j in range(min(len(order), max_probe)):\n                if evals[0] >= self.budget:\n                    break\n                idx = int(order[j])\n                cand = np.clip(candidates[idx], lb, ub)\n                fv = eval_and_record(cand)\n                if fv is None:\n                    break\n                # update archive if necessary\n                if fv < f_best - 1e-12:\n                    step = cand - x_best\n                    # update covariance path\n                    if np.linalg.norm(step) > 1e-14:\n                        s = step / (np.linalg.norm(step) + 1e-12)\n                        self.path = self.path_decay * self.path + (1.0 - self.path_decay) * s\n                        R = np.outer(s, s)\n                        self.cov = (1.0 - self.cov_lr) * self.cov + self.cov_lr * R\n                        self.cov = 0.5 * (self.cov + self.cov.T) + 1e-12 * np.eye(self.dim)\n                    f_best = fv; x_best = cand.copy()\n                    improved = True\n                    self.stagnation = 0\n                    self.last_improve_at = evals[0]\n                    # reward for the move\n                    reward = (before - f_best) / (1.0 + abs(before))\n                    # small local polish along step if budget allows\n                    if evals[0] < self.budget:\n                        dirv = step\n                        if np.linalg.norm(dirv) > 1e-12:\n                            diru = dirv / (np.linalg.norm(dirv) + 1e-12)\n                            allowed = min(5, self.budget - evals[0])\n                            if allowed > 0:\n                                def wrap_eval(z): return eval_and_record(z)\n                                x_new, f_new, used = self._parabolic_bracket(x_best, diru, f_best, wrap_eval, lb, ub, allowed)\n                                if used > 0 and f_new < f_best:\n                                    f_best = f_new; x_best = x_new.copy()\n                                    reward += (before - f_best) / (1.0 + abs(before))\n                    # reduce trust for other centers slightly, grow current center trust\n                    center['trust'] = float(np.clip(center.get('trust', 0.18) * 1.08, self.min_trust, self.max_trust))\n                    # end opportunistic\n                    break\n                else:\n                    # no improvement: small negative reward (penalize wasted eval)\n                    reward = -0.005\n                # update move stats after each candidate (we consider the selected 'move')\n                S = self.move_stats.get(move, None)\n                if S is not None:\n                    # decayed online update of mean/variance\n                    S['m'] = S['m'] * self.move_decay + reward * (1.0 - self.move_decay)\n                    S['v'] = max(1e-6, S['v'] * self.move_decay + (reward - S['m']) ** 2 * (1.0 - self.move_decay))\n                    S['N'] = S['N'] + 1.0\n\n            # stagnation update\n            if improved:\n                self.temp = max(self.min_temp, self.temp * 0.94)\n                self.stagnation = 0\n            else:\n                self.stagnation += 1\n                # penalize center trust if repeated failures\n                center['trust'] = float(np.clip(center.get('trust', 0.18) * 0.94, self.min_trust, self.max_trust))\n                if evals[0] - self.last_improve_at > max(12, int(0.06 * self.budget)):\n                    # escalate exploration temperature\n                    self.temp = min(self.max_temp, self.temp * 1.15)\n                    self.last_improve_at = evals[0]\n                    # if severe stagnation, do a small LHS reseed\n                    if self.stagnation > max(40, int(0.25 * self.budget)):\n                        reseed = min(6, max(2, int(0.03 * self.budget)))\n                        lh = self._latin_hypercube(reseed)\n                        for i in range(reseed):\n                            if evals[0] >= self.budget: break\n                            x = lb + lh[i] * span\n                            fv = eval_and_record(x)\n                            if fv is not None and fv < f_best:\n                                f_best = fv; x_best = x.copy()\n                        self.stagnation = 0\n\n            # update centers list with new function values\n            self._ensure_centers()\n\n            # prune archive to cap\n            if len(self.X) > self.max_archive:\n                M = max(300, int(6 * self.dim))\n                idxs = np.argsort(np.asarray(self.y))[:M]\n                keep = list(idxs)\n                # add some diverse random others\n                others = np.setdiff1d(np.arange(len(self.X)), idxs)\n                if others.size > 0:\n                    pick = min(len(others), M)\n                    selo = rng.choice(others, size=pick, replace=False)\n                    keep.extend(list(selo))\n                keep = np.unique(keep)\n                self.X = [self.X[i] for i in keep]\n                self.y = [self.y[i] for i in keep]\n\n        # final micro-polish: covariance-adaptive sampling around best\n        remaining = self.budget - evals[0]\n        if remaining > 0:\n            sigma = 0.06 * np.linalg.norm(span)\n            # adapt covariance slightly towards recent path\n            C = self.cov + 1e-12 * np.eye(self.dim)\n            try:\n                L = np.linalg.cholesky(C)\n            except Exception:\n                L = np.linalg.cholesky(np.eye(self.dim) * (sigma ** 2) + 1e-12 * np.eye(self.dim))\n            for _ in range(remaining):\n                z = rng.normal(size=self.dim)\n                cand = np.clip(x_best + sigma * (L @ z), lb, ub)\n                fv = eval_and_record(cand)\n                if fv is None:\n                    break\n                if fv < f_best:\n                    step = cand - x_best\n                    if np.linalg.norm(step) > 1e-14:\n                        s = step / (np.linalg.norm(step) + 1e-12)\n                        self.path = self.path_decay * self.path + (1.0 - self.path_decay) * s\n                        R = np.outer(s, s)\n                        self.cov = (1.0 - self.cov_lr) * self.cov + self.cov_lr * R\n                        self.cov = 0.5 * (self.cov + self.cov.T) + 1e-12 * np.eye(self.dim)\n                    f_best = fv; x_best = cand.copy()\n                    sigma = max(1e-6, sigma * 0.95)\n                else:\n                    sigma = min(0.5 * np.linalg.norm(span), sigma * 1.02)\n\n        return float(f_best), np.asarray(x_best).copy()", "configspace": "", "generation": 22, "feedback": "The algorithm FractalSurrogateExplorerRefined scored 0.640 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4a31f3da-67fd-4693-826d-b72eeeb45833"], "operator": null, "metadata": {"performance_data": [{"fid": 1, "iid": 1, "dim": 10, "auc": 0.9428823333752303}, {"fid": 1, "iid": 2, "dim": 10, "auc": 0.9400843557691109}, {"fid": 1, "iid": 3, "dim": 10, "auc": 0.9402189928275592}, {"fid": 2, "iid": 1, "dim": 10, "auc": 0.8912834409763102}, {"fid": 2, "iid": 2, "dim": 10, "auc": 0.9037607765864892}, {"fid": 2, "iid": 3, "dim": 10, "auc": 0.8900673004991035}, {"fid": 3, "iid": 1, "dim": 10, "auc": 0.15562470926271432}, {"fid": 3, "iid": 2, "dim": 10, "auc": 0.5272890330244873}, {"fid": 3, "iid": 3, "dim": 10, "auc": 0.15861033593947538}, {"fid": 4, "iid": 1, "dim": 10, "auc": 0.1516007203226185}, {"fid": 4, "iid": 2, "dim": 10, "auc": 0.11454460828014923}, {"fid": 4, "iid": 3, "dim": 10, "auc": 0.1600429005806029}, {"fid": 5, "iid": 1, "dim": 10, "auc": 0.9393710967033946}, {"fid": 5, "iid": 2, "dim": 10, "auc": 0.9396967236158404}, {"fid": 5, "iid": 3, "dim": 10, "auc": 0.9394661980301632}], "aucs": [0.9428823333752303, 0.9400843557691109, 0.9402189928275592, 0.8912834409763102, 0.9037607765864892, 0.8900673004991035, 0.15562470926271432, 0.5272890330244873, 0.15861033593947538, 0.1516007203226185, 0.11454460828014923, 0.1600429005806029, 0.9393710967033946, 0.9396967236158404, 0.9394661980301632]}, "task_prompt": "\nYou are a Python expert working on a new optimization algorithm. You can use numpy v2 and some other standard libraries.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should work on different instances of noiseless unconstrained functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-ES-BBOB-4", "seed": 4}
